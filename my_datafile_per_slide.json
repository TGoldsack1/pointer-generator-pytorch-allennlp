{"article_lines": ["The cameras are co-labelled with the station they are viewing, for example camera 4 is viewing station 4. The CPU is housed within the CPPS infrastructure and is connected to the PROFINET via RJ45 cable. Each camera is mounted such that it is positioned to view the whole of the individual conveyor and multiple pal- lets that are transferred along the conveyor.", "Vision tracking system. The objective of the vision track- ing system was to create a low cost solution to identify, accurately track and transmit the position of the pallets to the CPPS DT in real-time. The inputs to the vision", "https://uk.sagepub.com/en-gb/journals-permissions https://doi.org/10.1177/09544054211002464 journals.sagepub.com/home/pib http://crossmark.crossref.org/dialog/?doi=10.1177%2F09544054211002464&domain=pdf&date_stamp=2021-03-12"], "summary_lines": "PowerPoint Prentation"}
{"article_lines": ["The iFactory, housed with the Intelligent Manufacturing Systems31 Center at Windsor, Canada is a reconfigurable manufacturing plant focused on sys- tems learning, re-configurable processes and product design and customisation. The Experimental and Digital Factory32 at the Chemnitz University of Technology consists of networked laboratories used for teaching various aspects of manufacturing processes. The Smart Factory KL33 at the Technical University of Kaiserslautern consists of a testbed with many of the key communication technologies used within this research such as RFID and networked systems. The facility demonstrates a liquid soap manufacturing sys- tem. The Smart Factory,34 at the Fraunhofer Project Center at MTA SZATKI in Hungary, is a compact production facility which explores physical and virtual manufacturing processes. The Smart Production Laboratory (Smart Lab)35 at Aalborg University con- sists of reconfigurable cells interconnected with con- veyor systems with research focused on emerging", "digital technologies such as virtual commissioning. Both the MTA SZATKI and Aalborg plants initially focused research on the cyber physical production system and both will subsequently benefit from applied digital twin research. Other similar modular systems include I4.0 Lab36 at Politecnico di Milano, SmartPro4.037 at Zurich University of Applied Sciences, Swiss Smart Factory38", "at the Switzerland Innovation Park, Bern and the Industry 4.0 Pilot Factory (I40PF)39 at the Technical University of Vienna. These centres all offer a multitude of smart manufacturing capabilities but have yet to take the CPPS or Digital Twin research to publication. And finally, in terms of production line tracking, a computer integrated manufacturing scenario40 implemented a vision based tracking system to a production line teach- ing demonstrator which resulted in position updates every 25 s to a cloud based system for visualisation."], "summary_lines": "Mindsphere Lab \nManufacturing Intelligence"}
{"article_lines": ["Within this research, the DT is termed a \u2018Simulation DT\u2019, the systems\u2019 behaviour is represented through Discrete Event Simulation (DES). It is concerned with the movement of parts through the system which is dri- ven by discrete events such as operations upon the part. It is not be concerned with simulating the intricacies of the operations themselves and the physics involved. This can be considered the level of abstraction of the DT. The usefulness of this type of DT for a factory, for example, comes though the virtual replica (simulation) of the factory being actively linked to the physical fac- tory floor. The DT can hence monitor the factory, identify issues or sub-optimal behaviour, re-optimise the factory virtually (within the simulation) and, upon approval, implement these changes back into the physi- cal factory via connected control systems, all performed within decision critical times.", "The focus of this research has been to demonstrate a proof of concept system for Industry 4.0 research. The next step is to integrate this information with the MES in order to optimise product launch scheduling and prove the utility of real-time plant information to opti- mise resource planning. In the immediate future, the research effort will extend the current level of simula- tion and focus on a multi-level DT of the FESTO sys- tem with the aim of DT at each level in the automation pyramid (see Figure 1).", "Abstract The adoption of Industry 4.0 technologies within the manufacturing and process industries is widely accepted to have benefits for production cycles, increase system flexibility and give production managers more options on the production line through reconfigurable systems. A key enabler in Industry 4.0 technology is the rise in Cyber-Physical Systems (CPS) and Digital Twins (DTs). Both technologies connect the physical to the cyber world in order to generate smart manufac- turing capabilities. State of the art research accurately describes the frameworks, challenges and advantages surrounding these technologies but fails to deliver on testbeds and case studies that can be used for development and validation. This research demonstrates a novel proof of concept Industry 4.0 production system which lays the foundations for future research in DT technologies, process optimisation and manufacturing data analytics. Using a connected system of com- mercial off-the-shelf cameras to retrofit a standard programmable logic controlled production process, a digital simula- tion is updated in real time to create the DT. The system can identify and accurately track the product through the production cycle whilst updating the DT in real-time. The implemented system is a lightweight, low cost, customable and scalable design solution which provides a testbed for practical Industry 4.0 research both for academic and industrial research purposes."], "summary_lines": "\u2022 s have become a growth area of \nrearch over the last 10 years. \n\u2022 AMRC defines a digital twin as: A live digital \ncoupling of the state of a physical ast of \n to a virtual reprentation with a \n\u2022 Aim was to create a  using a \ntypical Discrete Event Simulation software.\n\u2022 The project was supported by Siemens, \nFesto and the Department of Automatic \nControl and Systems Engineering (ACSE) at \nthe University of Sheffield.\n\u2022 The project was split into three phas."}
{"article_lines": ["A DES model representing the \u2018as-is\u2019 simulation DT of the cell was created using Siemens Tecnomatix PlantSim software.43 A DES model was used as it has the ability to use measured data in order to accurately model the future behaviour and states of the production process. The model captures the physical and digital behaviour of the cell. The model uses a functional model to represent the individual stations and conveyor system overlaid with a CAD model as viewed in Figure 5. Combined the system gives a realistic visualisation of the real-world system (as shown in Figure 6). The DES model can display RFID Information (Pallet ID), num- ber and position of parts in the system at a given time based on touch sensors and RFID tag detection when in station, cycle times of a production process duration at any given station and goods-in/goods-out. PlantSim reads the real-time data from the eight PLCs in the phys- ical cell via OPC-UA. Each PLC hosts an OPC-UA ser- ver with one for each station. It is through this OPC-UA server that the real-time vision system data is hosted.", "Vision tracking system. The objective of the vision track- ing system was to create a low cost solution to identify, accurately track and transmit the position of the pallets to the CPPS DT in real-time. The inputs to the vision", "has been implemented to provide accurate real-time product identification and position throughout the plant. The real-time information provides inputs to a visual discrete event model via an OPC-UA server which results in an accurate real-time DT and visual representation of the industrial process. This low cost, reconfigurable and scalable technology is an excellent testbed for future Industry 4.0 research."], "summary_lines": "\u2022 Develop simulation model of current system.\n\u2022 Model developed using Siemens Tecnomatix Plant Simulation.\nI\n\u2022 Connect the model to live data from the physical cell.\n\u2022 Prove monitoring capability.\nII\n\u2022 Part tracking using vision system.\n\u2022 sation of an order from MES."}
{"article_lines": ["All eight modules consist of a pallet carrying con- veyor system, a series of sensors (including an RFID sensor), a PLC and in-built OPC-UA servers as shown in Figure 4. The layout is shown in Figure 3. The appli- cation modules are controlled by Siemens S7 ET200SP PLCs with Siemens TP700 touch panel human machine interfaces (HMIs). The PLCs are each connected to a PROFINET via RJ45 connections. The PROFINET communicates with the local PC which hosts the MES via an OPC-UA server for production orders, system status and maintenance functions. The two bridge mod- ules are controlled by FESTO PLCs using CoDeSys software connected to the local PC via an OPC-UA", "The six application modules each represent a part of a standard industrial manufacturing process. The spe- cific function of the CPPS Lab is a mobile phone pro- duction line where upper and lower panels of a mobile phone case are joined together and heat treated before being removed from the conveyor system for unload- ing. Numbered pallets move around the production line; stopping at each station to carry out the respective function. Each pallet contains an RFID chip that is read by sensors situated at each station for pallet identi- fication. The application modules consist of the follow- ing functions (in order) \u2013 Magazine application (front panel), height measuring module, magazine application (back panel), pneumatic press, tunnel heater and finally an output module for physical removal from the pro- duction line.", "digital technologies such as virtual commissioning. Both the MTA SZATKI and Aalborg plants initially focused research on the cyber physical production system and both will subsequently benefit from applied digital twin research. Other similar modular systems include I4.0 Lab36 at Politecnico di Milano, SmartPro4.037 at Zurich University of Applied Sciences, Swiss Smart Factory38"], "summary_lines": "\u2022  that we looked to twin was Festo Modular Cyber \n\u2022 Fest CP Lab designed to simulate mobile phone production facility.\n\u2022 Two modules, 4 s and 4 PLCs per \n\u2022 6 ing s ud Siemens PLCs with two transfer bridges \nusing  Codesys PLCs.\n\u2022 Pallets transferred using a pallet transfer robot (Robotino)"}
{"article_lines": ["https://uk.sagepub.com/en-gb/journals-permissions https://doi.org/10.1177/09544054211002464 journals.sagepub.com/home/pib http://crossmark.crossref.org/dialog/?doi=10.1177%2F09544054211002464&domain=pdf&date_stamp=2021-03-12", "40. Konstantindis FK, Gasteratos A andMouroutsos SG. Vision- based product tracking method for cyber-physical production systems in Industry 4.0. In: 2018 IEEE international conference on imaging systems and techniques (IST), Krakow, Poland, 16\u201318 October 2018, pp.1\u20136. NewYork: IEEE.", "score. A common approach is to use the IOU (Intersection Over Union) which means that if the bounding box in next frame overlaps the bounding box in the current frame then it is likely the same object. The algorithm can tell if an object is new, if it has moved, or if it is stationary, as shown in Figure 11."], "summary_lines": ""}
{"article_lines": ["The challenge within the CPS context is to use the available data in a smart manner to improve productiv- ity, accurately predict system response and reduce sys- tem downtime.20 Therefore, the same information that is used to update the MES (PLC and M2M data) can be used to update the DT. One source of real-time data", "All eight modules consist of a pallet carrying con- veyor system, a series of sensors (including an RFID sensor), a PLC and in-built OPC-UA servers as shown in Figure 4. The layout is shown in Figure 3. The appli- cation modules are controlled by Siemens S7 ET200SP PLCs with Siemens TP700 touch panel human machine interfaces (HMIs). The PLCs are each connected to a PROFINET via RJ45 connections. The PROFINET communicates with the local PC which hosts the MES via an OPC-UA server for production orders, system status and maintenance functions. The two bridge mod- ules are controlled by FESTO PLCs using CoDeSys software connected to the local PC via an OPC-UA", "Within this research, the DT is termed a \u2018Simulation DT\u2019, the systems\u2019 behaviour is represented through Discrete Event Simulation (DES). It is concerned with the movement of parts through the system which is dri- ven by discrete events such as operations upon the part. It is not be concerned with simulating the intricacies of the operations themselves and the physics involved. This can be considered the level of abstraction of the DT. The usefulness of this type of DT for a factory, for example, comes though the virtual replica (simulation) of the factory being actively linked to the physical fac- tory floor. The DT can hence monitor the factory, identify issues or sub-optimal behaviour, re-optimise the factory virtually (within the simulation) and, upon approval, implement these changes back into the physi- cal factory via connected control systems, all performed within decision critical times."], "summary_lines": "\u2022 8 s each had a PLC connected to an \n\u2022 Plant Simulation connects via OPC UA \n\u2022 Monitor parts enter/leave each .\n\u2022 Record ing times bad on nsor \n\u2022 Feedback real  times into"}
{"article_lines": ["is programmable logic controller (PLC).21 Significant process data can be accessed from the PLC such as counters for throughput metrics or from radio- frequency identification (RFID).22\u201324 tags on pallets. Pallets are load carrying components of a production line that transport the product around the plant during the various phases of production. The RFID tags con- tain electronically stored information and use electro- magnetic fields to enable automatic identification and tracking whilst attached to objects. The RFID tags located in the pallets can be read by RFID sensors at strategic points around the production system. The use of RFID tags to assist tracking in manufacturing assembly lines has been a proven capability for many years.25,26 Placing RFID sensors at strategic locations around a manufacturing plant, such as at stations and loading/unloading platforms, then the MES can be kept up-to-date with the state of the production plant. The data from RFID tags can be sent to the MES either via PLC communication or through one of the many M2M communication methods. When RFID object detection is set far at a distance from the sensor then signal strength can lead to difficulty in detection therefore methods have combined RFID with com- puter vision techniques27,28 to improve detection and tracking. The technologies described above (RFID, PLC, Fast M2M communications) are all key compo- nents to a successful CPPS and DT.", "In the Yolo detector, two lists are generated, a track- ing list and a detection list. A cost function gives a weight to each score and stores these in a matrix. The maximum and minimum weights are 1 and 0, respec- tively. A score of 1 would represent an exact match of a bounding box in two frames. The matrix describes the mapping between the detections and the tracks. The Hungarian algorithm uses bipartite graph theory to calculate each detection. The individual detections and now mapped to the individual tracks. Each cycle generates matched detections, unmatched detections and unmatched tracks. Multiple pallet tracking can be seen in Figure 12.", "A cost array is generated and then calculated to assign predictions to tracks. The Hungarian algo- rithm49,50 (also known as Kuhn-Munkres algorithm) is then applied to assign predictions to tracks by minimis- ing the cost. The Hungarian algorithm is used for two purposes, one, to map each observed detection to a track, and two, for multiple object tracking."], "summary_lines": "\u2022 The parts could be monitored entering and leaving the  the \nnsors xBG1 and xG1_BG9.\n\u2022 Recorded ing time distributions for each . \n\u2022 Each carrier was tracked using RFID tag that was read in a each \nData type Description\nxBG1 Boolean Part entering/leaving  \nxG1_BG9 Boolean Actuator switch that \nholds/releas pallet at  \niCarrierID Integer RFID tag number\nrActVal Real Temperature value for heat"}
{"article_lines": ["Hardware. The vision tracking system consists of four D435 Intel\ufffdRealSense\ufffd depth cameras connected to a Dell Optiplex 5050 CPU (Intel i5-7500 3.4GHz, 8GB 2400MHZ DDR4, Windows 10 Pro OS) via USB3 cables. The D435 cameras are configured for depth and colour streaming which provide intrinsic and extrinsic calibration capabilities.44 The D435 cameras have excellent low light sensitivity due to the global shutter sensor and a wide field of view which makes them suitable for a range of industrial environments. The mounted positions can be see in Figure 7 and the corresponding four video feeds can be seen in Figure 8.", "The cameras are co-labelled with the station they are viewing, for example camera 4 is viewing station 4. The CPU is housed within the CPPS infrastructure and is connected to the PROFINET via RJ45 cable. Each camera is mounted such that it is positioned to view the whole of the individual conveyor and multiple pal- lets that are transferred along the conveyor.", "All eight modules consist of a pallet carrying con- veyor system, a series of sensors (including an RFID sensor), a PLC and in-built OPC-UA servers as shown in Figure 4. The layout is shown in Figure 3. The appli- cation modules are controlled by Siemens S7 ET200SP PLCs with Siemens TP700 touch panel human machine interfaces (HMIs). The PLCs are each connected to a PROFINET via RJ45 connections. The PROFINET communicates with the local PC which hosts the MES via an OPC-UA server for production orders, system status and maintenance functions. The two bridge mod- ules are controlled by FESTO PLCs using CoDeSys software connected to the local PC via an OPC-UA"], "summary_lines": "\u2022 Four D435 Intel RealSen depth cameras were added to cond \n\u2022 Cameras connected to a PC via USB.\n\u2022 PC connected to PROFINET making values available from OPC UA \n\u2022 Centre of carrier position was converted to a pixel number.\n\u2022 Each time the pixel number updated a method was called to convert \nto distance along conveyor.\n\u2022 Assume pixel number changed linearly."}
{"article_lines": ["All eight modules consist of a pallet carrying con- veyor system, a series of sensors (including an RFID sensor), a PLC and in-built OPC-UA servers as shown in Figure 4. The layout is shown in Figure 3. The appli- cation modules are controlled by Siemens S7 ET200SP PLCs with Siemens TP700 touch panel human machine interfaces (HMIs). The PLCs are each connected to a PROFINET via RJ45 connections. The PROFINET communicates with the local PC which hosts the MES via an OPC-UA server for production orders, system status and maintenance functions. The two bridge mod- ules are controlled by FESTO PLCs using CoDeSys software connected to the local PC via an OPC-UA", "An integral component of a CPS is Machine to Machine communication (M2M). M2M includes any communication between machines, controllers and actuators using both wired and wireless networks. M2M is a key part of Internet of Things (IoT) net- works and there is a host of commercial and open- source protocols which have been designed for the manufacturing sector.16 MTConnect and OPC-Unified Architecture (OPC-UA) are two communication proto- cols specifically designed for industrial automation.17,18", "The six application modules each represent a part of a standard industrial manufacturing process. The spe- cific function of the CPPS Lab is a mobile phone pro- duction line where upper and lower panels of a mobile phone case are joined together and heat treated before being removed from the conveyor system for unload- ing. Numbered pallets move around the production line; stopping at each station to carry out the respective function. Each pallet contains an RFID chip that is read by sensors situated at each station for pallet identi- fication. The application modules consist of the follow- ing functions (in order) \u2013 Magazine application (front panel), height measuring module, magazine application (back panel), pneumatic press, tunnel heater and finally an output module for physical removal from the pro- duction line."], "summary_lines": "\u2022 MES for the Festo CP Lab was a 32-bit Access data ba with a GUI.\n\u2022 Siemens Plant Simulation is 64-bit.\n\u2022 Compatibility issues linking the two.\n\u2022 Number of avenues exhausted in working round this issue. \n\u2022 Ultimately no connection to the MES was made."}
{"article_lines": ["The system has the capability to detect four pallets on each conveyor, therefore each conveyor is assigned four sets of tags. The tag sets are presented in terms of ranked position order, that is, 1st, 2nd, 3rd or 4th and contain pallet ID and positional information (mm) along the conveyor. However, due to the time taken in station to read the pallet ID, only three pallets can be", "tracking system are the four video feeds of the pallets moving along the conveyors and the pallet RFID infor- mation read at each station. The output was defined to be an OPC-UA data structure with updated pallet num- ber, position and order on each conveyor. The system was designed to be stand alone which then can be used by the DT to retrieve the data as desired. It has no func- tion into the control or operation of the lab, therefore it can be turned on or off without disrupting the produc- tion process.", "Difficulties arose when the objects passed behind the struts of the modules, where all tracking algorithms failed to track objects. In order to overcome this, the Kalman Filter was used to estimate the pallets next positions based on the previous estimation and detected position of the pallet. This enables the system to track the object as it passes behind the struts."], "summary_lines": "Order sation\n\u2022 Could still prove order optimisation \u2013 hypothetical order.\n\u2022 Order contained 3 part variations.\n\u2022 6 orders were placed for part different part variations and quantities.\n\u2022 4 pallets running at any one time\n\u2022 Genetic algorithm ud to determine optimum work order to reduce \noverall production time."}
{"article_lines": ["The six application modules each represent a part of a standard industrial manufacturing process. The spe- cific function of the CPPS Lab is a mobile phone pro- duction line where upper and lower panels of a mobile phone case are joined together and heat treated before being removed from the conveyor system for unload- ing. Numbered pallets move around the production line; stopping at each station to carry out the respective function. Each pallet contains an RFID chip that is read by sensors situated at each station for pallet identi- fication. The application modules consist of the follow- ing functions (in order) \u2013 Magazine application (front panel), height measuring module, magazine application (back panel), pneumatic press, tunnel heater and finally an output module for physical removal from the pro- duction line.", "The system has the capability to detect four pallets on each conveyor, therefore each conveyor is assigned four sets of tags. The tag sets are presented in terms of ranked position order, that is, 1st, 2nd, 3rd or 4th and contain pallet ID and positional information (mm) along the conveyor. However, due to the time taken in station to read the pallet ID, only three pallets can be", "All eight modules consist of a pallet carrying con- veyor system, a series of sensors (including an RFID sensor), a PLC and in-built OPC-UA servers as shown in Figure 4. The layout is shown in Figure 3. The appli- cation modules are controlled by Siemens S7 ET200SP PLCs with Siemens TP700 touch panel human machine interfaces (HMIs). The PLCs are each connected to a PROFINET via RJ45 connections. The PROFINET communicates with the local PC which hosts the MES via an OPC-UA server for production orders, system status and maintenance functions. The two bridge mod- ules are controlled by FESTO PLCs using CoDeSys software connected to the local PC via an OPC-UA"], "summary_lines": "Part Variants Router\n          Pallet Transfer\n             Movement but no"}
{"article_lines": ["In the Yolo detector, two lists are generated, a track- ing list and a detection list. A cost function gives a weight to each score and stores these in a matrix. The maximum and minimum weights are 1 and 0, respec- tively. A score of 1 would represent an exact match of a bounding box in two frames. The matrix describes the mapping between the detections and the tracks. The Hungarian algorithm uses bipartite graph theory to calculate each detection. The individual detections and now mapped to the individual tracks. Each cycle generates matched detections, unmatched detections and unmatched tracks. Multiple pallet tracking can be seen in Figure 12.", "score. A common approach is to use the IOU (Intersection Over Union) which means that if the bounding box in next frame overlaps the bounding box in the current frame then it is likely the same object. The algorithm can tell if an object is new, if it has moved, or if it is stationary, as shown in Figure 11.", "A cost array is generated and then calculated to assign predictions to tracks. The Hungarian algo- rithm49,50 (also known as Kuhn-Munkres algorithm) is then applied to assign predictions to tracks by minimis- ing the cost. The Hungarian algorithm is used for two purposes, one, to map each observed detection to a track, and two, for multiple object tracking."], "summary_lines": "sation Results"}
{"article_lines": ["In the Yolo detector, two lists are generated, a track- ing list and a detection list. A cost function gives a weight to each score and stores these in a matrix. The maximum and minimum weights are 1 and 0, respec- tively. A score of 1 would represent an exact match of a bounding box in two frames. The matrix describes the mapping between the detections and the tracks. The Hungarian algorithm uses bipartite graph theory to calculate each detection. The individual detections and now mapped to the individual tracks. Each cycle generates matched detections, unmatched detections and unmatched tracks. Multiple pallet tracking can be seen in Figure 12.", "In this implementation of the Hungarian algorithm, the cost function is based on the Euclidean distance in the x axis between detected object position in the Yolo net- work and the updated state estimate from the Kalman Filter. Previously assigned tracks are updated and new tracks are created for any unassigned detections. Tracks that have expired or not assigned for 15 frames are deleted.", "score. A common approach is to use the IOU (Intersection Over Union) which means that if the bounding box in next frame overlaps the bounding box in the current frame then it is likely the same object. The algorithm can tell if an object is new, if it has moved, or if it is stationary, as shown in Figure 11."], "summary_lines": "sation Results\nd work orderOriginal work order"}
{"article_lines": ["\ufffd Darknet/YOLO v3 tiny47\u2013 the used DNN. The model was crosstrained to allow it to identify the pallets. The OpenCV matrices are then passed to an OpenCV DNN. This is running the Darknet Yolo v3 Tiny model crosstrained on video footage of the conveyors in order to detect the pallets.", "A DES model representing the \u2018as-is\u2019 simulation DT of the cell was created using Siemens Tecnomatix PlantSim software.43 A DES model was used as it has the ability to use measured data in order to accurately model the future behaviour and states of the production process. The model captures the physical and digital behaviour of the cell. The model uses a functional model to represent the individual stations and conveyor system overlaid with a CAD model as viewed in Figure 5. Combined the system gives a realistic visualisation of the real-world system (as shown in Figure 6). The DES model can display RFID Information (Pallet ID), num- ber and position of parts in the system at a given time based on touch sensors and RFID tag detection when in station, cycle times of a production process duration at any given station and goods-in/goods-out. PlantSim reads the real-time data from the eight PLCs in the phys- ical cell via OPC-UA. Each PLC hosts an OPC-UA ser- ver with one for each station. It is through this OPC-UA server that the real-time vision system data is hosted.", "tracking system are the four video feeds of the pallets moving along the conveyors and the pallet RFID infor- mation read at each station. The output was defined to be an OPC-UA data structure with updated pallet num- ber, position and order on each conveyor. The system was designed to be stand alone which then can be used by the DT to retrieve the data as desired. It has no func- tion into the control or operation of the lab, therefore it can be turned on or off without disrupting the produc- tion process."], "summary_lines": "\u2022 The DES simulation model was linked to the Festo CP Lab via OPC UA.\n\u2022 information was ud for live monitoring of parts.\n\u2022 Process times were also recorded from the nsor and fed into the \npredictive \n\u2022 Cameras were ud to track parts along the conveyors on the cond \n\u2022  was jumpy within the model due to read-write and \nmethod execution times before live position changed.\n was possible.\n\u2022 sation of production order was still proven."}
{"article_lines": ["https://uk.sagepub.com/en-gb/journals-permissions https://doi.org/10.1177/09544054211002464 journals.sagepub.com/home/pib http://crossmark.crossref.org/dialog/?doi=10.1177%2F09544054211002464&domain=pdf&date_stamp=2021-03-12", "This project was a collaboration between the Department of Automatic Control and Systems Engineering and the Manufacturing Intelligence Department at the Advanced Manufacturing Research Centre, University of Sheffield. We would like to thank Dr Jamie Smith for his support with Tecnomatix. We would also like to thank Mike Smart, Joe Bell and Adam Jarmin from Inspec Solutions for their technical support throughout this project. The authors also acknowledge the support of the Royal Academy of Engineering under the Research Chairs and Senior Research Fellowships scheme (RCSRF1718/5/41). Professor Ashutosh Tiwari is Airbus/RAEng Research Chair in Digitisation for Manufacturing at the University of Sheffield.", "The author(s) disclosed receipt of the following finan- cial support for the research, authorship, and/or publi- cation of this article: This work was supported by the Higher Education Innovation Funding at the University of Sheffield Knowledge Exchange. Grant No. X/013194."], "summary_lines": "For further information plea contact or \nj.k.smith@amrc.co.uk"}
{"article_lines": ["Figure 11. Multiple object tracking showing (1) stationary object, (2) and (3) moving objects and (4) new object detected. The images shows the relationship between the bounding boxes and the detected objects in two successive frames.", "In the Yolo detector, two lists are generated, a track- ing list and a detection list. A cost function gives a weight to each score and stores these in a matrix. The maximum and minimum weights are 1 and 0, respec- tively. A score of 1 would represent an exact match of a bounding box in two frames. The matrix describes the mapping between the detections and the tracks. The Hungarian algorithm uses bipartite graph theory to calculate each detection. The individual detections and now mapped to the individual tracks. Each cycle generates matched detections, unmatched detections and unmatched tracks. Multiple pallet tracking can be seen in Figure 12.", "tracking system are the four video feeds of the pallets moving along the conveyors and the pallet RFID infor- mation read at each station. The output was defined to be an OPC-UA data structure with updated pallet num- ber, position and order on each conveyor. The system was designed to be stand alone which then can be used by the DT to retrieve the data as desired. It has no func- tion into the control or operation of the lab, therefore it can be turned on or off without disrupting the produc- tion process."], "summary_lines": "Order sation\n\tPart Variants Router\n\tsation Results\n\tsation Results"}
{"article_lines": ["[1] S. P. Ruemler, K. E. Zimmerman, N. W. Hartman, T. Hedberg, and A. Barnard Feeny, \u201cPromoting Model-Based Definition to Establish a Complete Product Definition,\u201d J. Manuf. Sci. Eng., vol. 139, no. 5, p. 051008, 2016.", "[6] A.  Trainer  and  A.  B.  Feeney,  \u201cGaps  Analysis  of  Integrating  Product  Design  , Manufacturing , and Quality Data in The Supply Chain Using Model-Based Definition,\u201d in ASME Int Conf Manuf Sci Eng, 2016.", "(NI). In particular, the hardware consisted of an NI Industrial Smart Camera (type: NI-ISC-1780)  with a resolution of 640 x 480 pixels. An external ring light from Spectrum Illumination (type:  MRL5.5) was mounted around the camera to provide uniform lighting conditions during the  inspection process. The smart camera came with a Power and I/O Accessory that was used to  simplify the power and I/O configurations for controlling the external ring light."], "summary_lines": "PowerPoint Presentation"}
{"article_lines": ["[6] A.  Trainer  and  A.  B.  Feeney,  \u201cGaps  Analysis  of  Integrating  Product  Design  , Manufacturing , and Quality Data in The Supply Chain Using Model-Based Definition,\u201d in ASME Int Conf Manuf Sci Eng, 2016.", "The output of the Design Engineering  process(es) which represents a  component or assembly in a 3D Digital  format, as well as critical information  required to manufacture or assemble it  associated to the model and it\u00e2\u20ac\u2122s  features. Used within or outside of an  organisation to manufacture and/or  procure the component or assembly. May  be seen as a replacement for traditional  Engineering Drawings.", "[1] S. P. Ruemler, K. E. Zimmerman, N. W. Hartman, T. Hedberg, and A. Barnard Feeny, \u201cPromoting Model-Based Definition to Establish a Complete Product Definition,\u201d J. Manuf. Sci. Eng., vol. 139, no. 5, p. 051008, 2016."], "summary_lines": "(Sept \nModel Based Enterprise\nIntegrated Manufacturing Group"}
{"article_lines": ["A survey was distributed to AMRC partners to establish the state of Model Based Definition  within the engineering sector. The objectives of this survey were to establish how MBD is  currently defined within industry such that the AMRC can align its research objectives, and to  identify areas where research potential into the implementation of MBD exists. While the  survey was focussed on MBD, many of the questions were designed such that they could be of  use in the implementation of a manufacturing process utilising MBE.", "A key component of the MBE capability development process was to implement a demonstrator that could showcase the AMRC\u2019s current interpretation of the MBD/E process, and to support  the AMRC\u2019s demonstration stand at MACH 2020.", "This CATAPULT project was initiated to allow the AMRC to develop a more robust understanding of MBD and MBE, what benefits these can deliver to industry, and how these potential benefits  can be realised by both top tier companies and their supply chain. The project also identified  areas where development is required in the practical implementation of MBD and MBE  approaches, and specifically explored the current state of the implementation of MBD within  industry. Finally, it delivered a practical demonstration of a MBE process."], "summary_lines": "Project Introduction\nCapability development for IMG \u2013 aiming to establish what Model Based Enterprise (MBE) is, the benefits \nit can bring and how it can be used.\nTo tie in with MACH stand \u2013 show off the concept of the \u2018digital thread\u2019. \n1. Establish a definition for Model Based Definition/Enterprise\n2. Find out how it is currently used, how industry perceives it and what are the perceived advantages \n3. Identify where the AMRC could usefully contribute to research and development around the subject\n4. Produce a demonstrator of an MBD/E implementation for the MACH 2020 trade show"}
{"article_lines": ["Model Based Definition (MBD) describes the use of a 3D model as sole reference for all design  data for a component or assembly. This can be extended by Model Based Enterprise (MBE),  which is the use of a digital model as the sole reference for all stages across the manufacturing process. The use of these techniques has the potential for delivering cost, quality and time  savings into a manufacturing process, but their use is not pervasive within the Engineering  community.", "Model Based Enterprise (MBE) extends the MBD methodology from a process or product to the  entire workflow of an organisation, using the 3D model generated as a single source of truth to  which relevant data is appended. As such, the model may be used as a common reference for  processes including design, manufacturing, inspection, marketing and any other processes that may require product engagement.", "Broadly speaking, Model Based Enterprise (MBE) is the use of a 3D geometry as the single  reference for the production of a part or assembly, Model Based Definition (MBD) is the use of a single 3D model as the design reference only. That is, a model designed through MBD will be  annotated with all information required for manufacture, such as GD&T data and bills of  material. In an MBE process, this model is passed between departments, suppliers and clients  without the necessity for traditional 2D paper drawings and may be used in processes such as  manufacturing and inspection."], "summary_lines": "What is \u2018Model Based\u2026\u2019?\n\u2018Elevator pitch\u2019 \u2013 MBD/E\nUse of a 3D model as a \u2018single-source-of-truth\u2019 for the manufacturing process \u2013 linking all processes via a \ndigital thread to generate improvements in cost and quality.\nTwo definitions covered by the project:\n\u2022 MBD (Model Based Definition) \u2013 The use of a single 3D model (\u201csingle source of truth\u201d), based on a \ncommon framework, as the sole design reference for a product.\n\u2022 MBE (Model Based Enterprise) \u2013 The use of a 3D model as the sole reference for multiple manufacturing \nprocesses across a product\u2019s manufacturing cycle.\nOther \u2018Model Based\u2019 approaches\n\u2022 Model Based Systems Engineering (MBSE) \u2013 The use of domain models to exchange information, in a \nmanner similar to that defined by Model Based Enterprise.\n\u2022 Model Based Design, Model Based Engineering (MBD, MBE) \u2013 The systematic use of computational \nmodels to deliver time and cost savings during the design and test phases of product development."}
{"article_lines": ["Figure   2 .1 shows the conversion from a traditional 2D drawing as a source of information to a 3D model. The same information is retained in the model but is better presented, with only  relevant information shown to the user via an interactive 3D PDF. The most obvious benefit to  this is the intuitive viewing and presentation of the model, with this having clear applications in the generation of digital work instructions. However, the key benefit to the storage of data in a  single model is that there is the potential for a significant decrease in product manufacturing  time, arising as a result of fewer conflicts between design changes. This leads to faster analysis and approval of any changes, more intuitive instructions during manufacture and the potential  for two way data exchange being used to make in-process improvements.", "This ties in well with the ethos of the digital thread, in that the provision of information via a  single 3D model allows easy and customisable access to the most up-to-date data. Developing  a reliable system for presenting this to an operator would enable the introduction of digital  work instructions with up-to-date notes and inspection data, as well as information about  defects and the tolerancing state of previous parts in the assembly.", "Model Based Definition (MBD) is a manufacturing methodology that utilises the 3D CAD model  of the product as the sole design reference. This differs from the common method of using 2D  drawings as the primary source of information about a product, with supplemental 3D models  being used for design and development work. The goal of this approach is to reduce the  amount of superfluous documents and drawings, improve the consistency of the data used,  and to enable better virtualization of a product or process. However, it does not necessarily  remove traditional 2D drawings from the design process if they are the most suitable  presentation format of the product information \u2013 it simply removes them as the primary source  of information."], "summary_lines": "\u2022 Eliminates conflicts between CAD/3D models\n\u2022 More responsive engineering change process.\n\u2022 Only need to manage one type of file, saving time and server \n\u2022 Can be used to provide a more intuitive display of design \n\u2022 Suited to automating inspection, particularly for when \n\u2022 Captures design intent early in the process, allows early \n\u2022 Ensures that all end users of the model data have the same up-\nto-date information at all times"}
{"article_lines": ["Respondents were particularly nervous about software capability and user training, with  specific concerns highlighted including lack of functionality and the cost of software. Although  software is available that implements the MBD standards described in section 3.1, it is seen as  expensive and limited in its functionality \u2013 representing potential hurdles to for companies to  overcome in their implementation of MBD. User training is an issue across manufacturing, and  it would be beneficial to develop methods to overcome skills gaps in companies looking to  implement MBD. This work may be based around developing more accessible software, novel  frameworks for the implementation of such an approach, improving universality of software  functionality and other similar steps.", "The benefits of MBD have been demonstrated in specific processes and are recognised by  companies. However, a variety of barriers remain, primarily related to the consistency of data  passed between processes and companies, resistance to change from operators and the  storage requirements over the long term. Some of these issues, such as consistency of data,  are being addressed in the latest literature.", "Most software packages have broadly similar functionality [4]. However, there are some  notable exceptions. For example, most packages output data conforming with the STEP 242  standard, although a few (such as the Siemens NX MBD plug-in discussed above) output in the  ANSI QIF standard. In terms of implementation this standard should deliver comparable results  to STEP 242, but there may be problems related to interoperability where more than one  software package is used.  This highlights the necessity for careful consideration of process  requirements prior to the selection of any software to be used for Model Based Definition."], "summary_lines": "\u2022 Software discrepancies could lead to inconsistent \n\u2022 Supply chain may provide or require 2D drawings\n\u2022 Legacy products may make its implementation prohibitive\n\u2022 Resistance to change from operators \n\u2022 Inconsistent MBD/E frameworks lead to inconsistent PMI\n\u2022 Different users may require different software and"}
{"article_lines": ["[12] L. Yin, D. Tang, Q. Wang, I. Ullah, and H. Zhang, \u201cEngineering Change Management of Product Design Using Model-Based Definition Technology,\u201d J. Comput. Inf. Sci. Eng., vol. 17, no. 4, p. 041006, 2017.", "[6] A.  Trainer  and  A.  B.  Feeney,  \u201cGaps  Analysis  of  Integrating  Product  Design  , Manufacturing , and Quality Data in The Supply Chain Using Model-Based Definition,\u201d in ASME Int Conf Manuf Sci Eng, 2016.", "[3] V.  Quintana,  L.  Rivest,  R.  Pellerin,  F.  Venne,  and  F.  Kheddouci,  \u201cWill  Model-based Definition  replace  engineering  drawings  throughout  the  product  lifecycle?  A  global perspective from aerospace industry,\u201d Comput. Ind., vol. 61, no. 5, pp. 497\u2013508, 2010."], "summary_lines": ""}
{"article_lines": ["[12] L. Yin, D. Tang, Q. Wang, I. Ullah, and H. Zhang, \u201cEngineering Change Management of Product Design Using Model-Based Definition Technology,\u201d J. Comput. Inf. Sci. Eng., vol. 17, no. 4, p. 041006, 2017.", "[6] A.  Trainer  and  A.  B.  Feeney,  \u201cGaps  Analysis  of  Integrating  Product  Design  , Manufacturing , and Quality Data in The Supply Chain Using Model-Based Definition,\u201d in ASME Int Conf Manuf Sci Eng, 2016.", "[3] V.  Quintana,  L.  Rivest,  R.  Pellerin,  F.  Venne,  and  F.  Kheddouci,  \u201cWill  Model-based Definition  replace  engineering  drawings  throughout  the  product  lifecycle?  A  global perspective from aerospace industry,\u201d Comput. Ind., vol. 61, no. 5, pp. 497\u2013508, 2010."], "summary_lines": ""}
{"article_lines": ["[12] L. Yin, D. Tang, Q. Wang, I. Ullah, and H. Zhang, \u201cEngineering Change Management of Product Design Using Model-Based Definition Technology,\u201d J. Comput. Inf. Sci. Eng., vol. 17, no. 4, p. 041006, 2017.", "[6] A.  Trainer  and  A.  B.  Feeney,  \u201cGaps  Analysis  of  Integrating  Product  Design  , Manufacturing , and Quality Data in The Supply Chain Using Model-Based Definition,\u201d in ASME Int Conf Manuf Sci Eng, 2016.", "[3] V.  Quintana,  L.  Rivest,  R.  Pellerin,  F.  Venne,  and  F.  Kheddouci,  \u201cWill  Model-based Definition  replace  engineering  drawings  throughout  the  product  lifecycle?  A  global perspective from aerospace industry,\u201d Comput. Ind., vol. 61, no. 5, pp. 497\u2013508, 2010."], "summary_lines": ""}
{"article_lines": ["The output of the Design Engineering  process(es) which represents a  component or assembly in a 3D Digital  format, as well as critical information  required to manufacture or assemble it  associated to the model and it\u00e2\u20ac\u2122s  features. Used within or outside of an  organisation to manufacture and/or  procure the component or assembly. May  be seen as a replacement for traditional  Engineering Drawings.", "An MBD-based methodology broadly consists of annotating a 3D model with any information  relevant to the manufacture of the product. Known as PMI (Product Manufacturing Information)  this can consist of any of the traditional information stored in a 2D drawing, such as  dimensions, tolerances and material. This is related to the Common Information Model, which is a model that contains information relevant to all stages of the product\u2019s manufacturing process (i.e. design, manufacture and inspection). There are a number of factors to consider during the  definition of the Common Information Model and PMI, with these being listed below [1]:", "Model Based Definition (MBD) describes the use of a 3D model as sole reference for all design  data for a component or assembly. This can be extended by Model Based Enterprise (MBE),  which is the use of a digital model as the sole reference for all stages across the manufacturing process. The use of these techniques has the potential for delivering cost, quality and time  savings into a manufacturing process, but their use is not pervasive within the Engineering  community."], "summary_lines": "\u201cThe use of 3D models and associated data to define \ncomponents and processes\u201d\n\u201cAero primes flowing their drawing completion \ninto their \u201d\n\u201cThe output of the Design Engineering process(es) \nwhich represents a component or assembly in a 3D \nDigital format, as well as critical information required \nto manufacture or assemble it associated to the \nmodel and it\u2019s features. Used within or outside of an \norganisation to manufacture and/or procure the \ncomponent or assembly. May be seen as a \nreplacement for traditional Engineering Drawings.\u201d\n\u201cUsing 3d cad model directly in manufacture and \ninspection rather than creating a separate drawing\u201d\n\u201cTypical characteristics of a part represented in 3D \nrather than 2D/text\u201d"}
{"article_lines": ["[12] L. Yin, D. Tang, Q. Wang, I. Ullah, and H. Zhang, \u201cEngineering Change Management of Product Design Using Model-Based Definition Technology,\u201d J. Comput. Inf. Sci. Eng., vol. 17, no. 4, p. 041006, 2017.", "[6] A.  Trainer  and  A.  B.  Feeney,  \u201cGaps  Analysis  of  Integrating  Product  Design  , Manufacturing , and Quality Data in The Supply Chain Using Model-Based Definition,\u201d in ASME Int Conf Manuf Sci Eng, 2016.", "[3] V.  Quintana,  L.  Rivest,  R.  Pellerin,  F.  Venne,  and  F.  Kheddouci,  \u201cWill  Model-based Definition  replace  engineering  drawings  throughout  the  product  lifecycle?  A  global perspective from aerospace industry,\u201d Comput. Ind., vol. 61, no. 5, pp. 497\u2013508, 2010."], "summary_lines": ""}
{"article_lines": ["[12] L. Yin, D. Tang, Q. Wang, I. Ullah, and H. Zhang, \u201cEngineering Change Management of Product Design Using Model-Based Definition Technology,\u201d J. Comput. Inf. Sci. Eng., vol. 17, no. 4, p. 041006, 2017.", "[6] A.  Trainer  and  A.  B.  Feeney,  \u201cGaps  Analysis  of  Integrating  Product  Design  , Manufacturing , and Quality Data in The Supply Chain Using Model-Based Definition,\u201d in ASME Int Conf Manuf Sci Eng, 2016.", "[3] V.  Quintana,  L.  Rivest,  R.  Pellerin,  F.  Venne,  and  F.  Kheddouci,  \u201cWill  Model-based Definition  replace  engineering  drawings  throughout  the  product  lifecycle?  A  global perspective from aerospace industry,\u201d Comput. Ind., vol. 61, no. 5, pp. 497\u2013508, 2010."], "summary_lines": ""}
{"article_lines": ["[12] L. Yin, D. Tang, Q. Wang, I. Ullah, and H. Zhang, \u201cEngineering Change Management of Product Design Using Model-Based Definition Technology,\u201d J. Comput. Inf. Sci. Eng., vol. 17, no. 4, p. 041006, 2017.", "[6] A.  Trainer  and  A.  B.  Feeney,  \u201cGaps  Analysis  of  Integrating  Product  Design  , Manufacturing , and Quality Data in The Supply Chain Using Model-Based Definition,\u201d in ASME Int Conf Manuf Sci Eng, 2016.", "[3] V.  Quintana,  L.  Rivest,  R.  Pellerin,  F.  Venne,  and  F.  Kheddouci,  \u201cWill  Model-based Definition  replace  engineering  drawings  throughout  the  product  lifecycle?  A  global perspective from aerospace industry,\u201d Comput. Ind., vol. 61, no. 5, pp. 497\u2013508, 2010."], "summary_lines": ""}
{"article_lines": ["[12] L. Yin, D. Tang, Q. Wang, I. Ullah, and H. Zhang, \u201cEngineering Change Management of Product Design Using Model-Based Definition Technology,\u201d J. Comput. Inf. Sci. Eng., vol. 17, no. 4, p. 041006, 2017.", "[6] A.  Trainer  and  A.  B.  Feeney,  \u201cGaps  Analysis  of  Integrating  Product  Design  , Manufacturing , and Quality Data in The Supply Chain Using Model-Based Definition,\u201d in ASME Int Conf Manuf Sci Eng, 2016.", "[3] V.  Quintana,  L.  Rivest,  R.  Pellerin,  F.  Venne,  and  F.  Kheddouci,  \u201cWill  Model-based Definition  replace  engineering  drawings  throughout  the  product  lifecycle?  A  global perspective from aerospace industry,\u201d Comput. Ind., vol. 61, no. 5, pp. 497\u2013508, 2010."], "summary_lines": ""}
{"article_lines": ["It is clear that there is little understanding of the use of MBD across a range of industries. While multiple surveys have been conducted to establish how and why MBD is and isn\u2019t used, these  have tended to be limited in scope and with a small population size. While there is considerable research that examines perceived barriers to MBD integration, a better understanding of the  attitude of potential industrial users to the process would provide an indication of where  research should be focused to ensure that companies are able to take maximum advantage of  the potential benefits delivered MBD process.", "This implies that companies do not believe they can leverage the full benefit of MBD if the  supply chain does not adopt it, and are concerned about the investment required to achieve  MBD functionality in terms of software and training costs. Cost is particularly relevant for  aerospace given the large number of legacy products and their associated storage  requirements, and this is reflected in the number of respondents who said number of legacy  products was their main concern, as a proportion of those who listed it as a general concern.", "Further to this, as MBD is very heavily software driven, the end user of the engineering  drawings needs to have the proper software tools at their disposal and need to be trained to  use it. This leads to an increase of responsibility which can overstrain the users [4]. Further, to  obtain the best value from MBD, designers require proper training in annotation to properly  utilise MBD [6]."], "summary_lines": "\u2022 There is considerable interest within the aero sector in MBD as a design tool, with understanding of \nthe concept being better within the aero industry than others\n\u2022 As well as being identified as delivering efficiencies on its own, MBD was also cited as being a useful tool \nfor the implementation of other augmentative technologies\n\u2022 Software cost and  appears to generally be a secondary concern\n\u2022 Manufacturers see the most significant limitations to MBD being around how to integrate it into their \nexisting processes, covering a broad range of topics from legacy products to staff training\n\u2022 Linked to the above point, STEP is far from the dominant CAD format, implying that adoption of the STEP \n242 format would be hindered by existing design processes\n\u2022 Efforts to improve MBD capability development within the Engineering community should have a cross-\n\u2022 Development of MBD applications should focus more on accessibility to stakeholders, including staff and \n\u2022 While the STEP 242 standard provides a framework for the integration of manufacturing data to a 3D \nmodel, STEP is not completely pervasive, and some work should focus on how to leverage MBD where \norganizations are not able to convert to the use of the STEP 242 format"}
{"article_lines": ["As part of this process, annotation of the part with inspection data was shown, along with the  application of MBD to the generation of Digital Work Instructions (DWIs). Finally, extending the  demonstrator to cover aspects of MBE, the attachment of inspection reports, COSHH sheets  and method statements to the model and their presentation to the user was demonstrated.", "Research also looks at the creation of a framework for incorporating inspection into the MBD  process without the use of QIF, by generating inspection instructions for Coordinate  Measurement Machines (CMMs) [11]. The information from these routines can then be fed into  the model and visualised or used to inform future processes by adjusting future programs  accordingly, or running alternative automated processes depending on the inspection results.", "Finally a demonstrator was developed to show an application of an MBE-based approach for an  inspection process, described within this report. The demonstrator consisted of a KUKA iiwa  fitted with a camera, this being used to inspect the hole geometry of some motor retention  brackets. This inspection information was attached to the 3D model of the brackets, along with  various other types of data to provide a reference for other potential processes."], "summary_lines": "Implementation \u2013 Demonstration\nDemonstrates an MBE implementation using an inspection routine as an example\nConsists of a UI on a touchscreen and cobot with a camera attached\nUser is able to call up fully annotated GD&T, attach relevant files (COSHH, RAMS etc) to a 3D PDF and use \ndata contained within to allocate appropriate staff and resources\nInspection routine is triggered and the camera is used to inspect mounting brackets"}
{"article_lines": ["As part of this process, annotation of the part with inspection data was shown, along with the  application of MBD to the generation of Digital Work Instructions (DWIs). Finally, extending the  demonstrator to cover aspects of MBE, the attachment of inspection reports, COSHH sheets  and method statements to the model and their presentation to the user was demonstrated.", "Research also looks at the creation of a framework for incorporating inspection into the MBD  process without the use of QIF, by generating inspection instructions for Coordinate  Measurement Machines (CMMs) [11]. The information from these routines can then be fed into  the model and visualised or used to inform future processes by adjusting future programs  accordingly, or running alternative automated processes depending on the inspection results.", "Finally a demonstrator was developed to show an application of an MBE-based approach for an  inspection process, described within this report. The demonstrator consisted of a KUKA iiwa  fitted with a camera, this being used to inspect the hole geometry of some motor retention  brackets. This inspection information was attached to the 3D model of the brackets, along with  various other types of data to provide a reference for other potential processes."], "summary_lines": "Implementation \u2013 Demonstration\nDemonstrates an MBE implementation using an inspection routine as an example\nConsists of a UI on a touchscreen and cobot with a camera attached\nUser is able to call up fully annotated GD&T, attach relevant files (COSHH, RAMS etc) to a 3D PDF and use \ndata contained within to allocate appropriate staff and resources\nInspection routine is triggered and the camera is used to inspect mounting brackets"}
{"article_lines": ["As part of this process, annotation of the part with inspection data was shown, along with the  application of MBD to the generation of Digital Work Instructions (DWIs). Finally, extending the  demonstrator to cover aspects of MBE, the attachment of inspection reports, COSHH sheets  and method statements to the model and their presentation to the user was demonstrated.", "Research also looks at the creation of a framework for incorporating inspection into the MBD  process without the use of QIF, by generating inspection instructions for Coordinate  Measurement Machines (CMMs) [11]. The information from these routines can then be fed into  the model and visualised or used to inform future processes by adjusting future programs  accordingly, or running alternative automated processes depending on the inspection results.", "Finally a demonstrator was developed to show an application of an MBE-based approach for an  inspection process, described within this report. The demonstrator consisted of a KUKA iiwa  fitted with a camera, this being used to inspect the hole geometry of some motor retention  brackets. This inspection information was attached to the 3D model of the brackets, along with  various other types of data to provide a reference for other potential processes."], "summary_lines": "Implementation \u2013 Demonstration\nDemonstrates an MBE implementation using an inspection routine as an example\nConsists of a UI on a touchscreen and cobot with a camera attached\nUser is able to call up fully annotated GD&T, attach relevant files (COSHH, RAMS etc) to a 3D PDF and use \ndata contained within to allocate appropriate staff and resources\nInspection routine is triggered and the camera is used to inspect mounting brackets"}
{"article_lines": ["[12] L. Yin, D. Tang, Q. Wang, I. Ullah, and H. Zhang, \u201cEngineering Change Management of Product Design Using Model-Based Definition Technology,\u201d J. Comput. Inf. Sci. Eng., vol. 17, no. 4, p. 041006, 2017.", "[6] A.  Trainer  and  A.  B.  Feeney,  \u201cGaps  Analysis  of  Integrating  Product  Design  , Manufacturing , and Quality Data in The Supply Chain Using Model-Based Definition,\u201d in ASME Int Conf Manuf Sci Eng, 2016.", "[3] V.  Quintana,  L.  Rivest,  R.  Pellerin,  F.  Venne,  and  F.  Kheddouci,  \u201cWill  Model-based Definition  replace  engineering  drawings  throughout  the  product  lifecycle?  A  global perspective from aerospace industry,\u201d Comput. Ind., vol. 61, no. 5, pp. 497\u2013508, 2010."], "summary_lines": ""}
{"article_lines": ["As part of this process, annotation of the part with inspection data was shown, along with the  application of MBD to the generation of Digital Work Instructions (DWIs). Finally, extending the  demonstrator to cover aspects of MBE, the attachment of inspection reports, COSHH sheets  and method statements to the model and their presentation to the user was demonstrated.", "A key component of the MBE capability development process was to implement a demonstrator that could showcase the AMRC\u2019s current interpretation of the MBD/E process, and to support  the AMRC\u2019s demonstration stand at MACH 2020.", "The demonstrator presented an MBE implementation focussed on the inspection of motor  retention brackets for an electrified Caterham kit car, one of these being shown in Figure   5 .3  mounted to a custom fixture manufactured by Metlase. It consisted of a camera mounted to a  collaborative robot, which would inspect a fixtured part after a request made via a user  interface."], "summary_lines": "Implementation \u2013 Demonstration"}
{"article_lines": ["The output of the Design Engineering  process(es) which represents a  component or assembly in a 3D Digital  format, as well as critical information  required to manufacture or assemble it  associated to the model and it\u00e2\u20ac\u2122s  features. Used within or outside of an  organisation to manufacture and/or  procure the component or assembly. May  be seen as a replacement for traditional  Engineering Drawings.", "This ties in well with the ethos of the digital thread, in that the provision of information via a  single 3D model allows easy and customisable access to the most up-to-date data. Developing  a reliable system for presenting this to an operator would enable the introduction of digital  work instructions with up-to-date notes and inspection data, as well as information about  defects and the tolerancing state of previous parts in the assembly.", "would define it, in their own words. By distilling these responses to a set of key principles, it  was hoped that the AMRC would be able to use a definition for further work that aligned with  industry expectations."], "summary_lines": "Thank you. For further information please contact or \n(Sept"}
{"article_lines": ["As part of this process, annotation of the part with inspection data was shown, along with the  application of MBD to the generation of Digital Work Instructions (DWIs). Finally, extending the  demonstrator to cover aspects of MBE, the attachment of inspection reports, COSHH sheets  and method statements to the model and their presentation to the user was demonstrated.", "Model Based Definition (MBD) describes the use of a 3D model as sole reference for all design  data for a component or assembly. This can be extended by Model Based Enterprise (MBE),  which is the use of a digital model as the sole reference for all stages across the manufacturing process. The use of these techniques has the potential for delivering cost, quality and time  savings into a manufacturing process, but their use is not pervasive within the Engineering  community.", "Finally a demonstrator was developed to show an application of an MBE-based approach for an  inspection process, described within this report. The demonstrator consisted of a KUKA iiwa  fitted with a camera, this being used to inspect the hole geometry of some motor retention  brackets. This inspection information was attached to the 3D model of the brackets, along with  various other types of data to provide a reference for other potential processes."], "summary_lines": "Project Introduction\n\tWhat is \u2018Model Based\u2026\u2019?\n\tImplementation \u2013 Demonstration\n\tImplementation \u2013 Demonstration\n\tImplementation \u2013 Demonstration\n\tImplementation \u2013 Demonstration"}
{"article_lines": ["1 Introduction \t2 Electric Panel \t2.1 Background \t2.2 Process Analysis \t2.3 Hardware and Software \t2.3.1 Development Software \t2.3.2 Database \t2.3.3 Augmented Reality and Mixed Reality \t2.3.4 Multimeter", "3 Pipe Assembly Leak Test \t3.1 Background \t3.2 Process Analysis \t3.3 Hardware and Software \t3.3.1 Development Software \t3.3.2 Database \t3.3.3 Augmented Reality \t3.3.4 Nutrunner \t3.3.5 Leak Test Instrument", "Figure 16 provides a top-level architectural view of the data flow between the components of the AR solution. Similar to the electric panel solution, the AR device acts as a message broker between the smart tool, leak test instrument, database, and user. Raw data from each device is parsed into a user-friendly format before being displayed. Figure 17 depicts the application flow described in the sections above."], "summary_lines": "PowerPoint Presentation"}
{"article_lines": ["As part of the AMSCI project, Laing O\u2019Rourke wishes to investigate the adoption of augmented reality into existing manufacturing processes at Crown House Technologies.  Laing O\u2019Rourke have identified two processes for the pilot scheme to demonstrate and quantify the benefits of augmented reality adoption as part of their assembly, verification, and testing processes.", "Executive summary The purpose of this document is to conclude the deliverables of the Laing O\u2019Rourke Digitally Assisted Testing work package as part of the wider AMSCI project. The AMRC developed two augmented reality applications to demonstrate and quantify the benefits of augmented reality adoption in assembly, verification, and testing processes.", "At  Crown  House  Technologies,  a  datasheet  accompanies  each  manufacturing  assembly outlining  the  manufacturing  stages,  current  progress,  and  completion  timestamps.  Each assembly  must  undergo  various  factory  acceptance  tests  to  ensure  it  meets  the  relevant industry standards; the paper test results are added to the datasheet."], "summary_lines": "Copyright \u00a9 The University of Sheffield / May-17 \n with Boeing Template detail: .PPTW Revision 2 (May \nLaing O\u2019Rourke Technology \nAR Electric Cabinet Testing\nIntegrate Manufacturing Group"}
{"article_lines": ["[14] X.-T.  Yan, C.  Jiang, B.  Eynard,  and International Conference on Advanced Design and Manufacture (2008), Advanced design and manufacture to gain a competitive edge\u202f: new manufacturing techniques and their role in improving enterprise performance. Springer, 2008.", "Heads up displays have been widely used in the aerospace industry to provide pilots with critical  information  while  minimising distraction  from their  primary  object.  With the birth of wearable technology, there is a growing  adoption  into  the  manufacturing space to provide just-in-time information to engineers and technicians [10].", "As part of the AMSCI project, Laing O\u2019Rourke wishes to investigate the adoption of augmented reality into existing manufacturing processes at Crown House Technologies.  Laing O\u2019Rourke have identified two processes for the pilot scheme to demonstrate and quantify the benefits of augmented reality adoption as part of their assembly, verification, and testing processes."], "summary_lines": "A world-leading cluster of industry-focused manufacturing R&D Centre's \nand supporting facilities. Originated as a collaboration between Boeing \nand the University of Sheffield. Now recognised as a global centre of \nexence, and a model for research centres worldwide.\nDeveloping new means, methods, tools and techniques to \nadvance manufacturing technology.\nSetup in 2001, the  is a faculty within the University of Sheffield and \npart of HVM Catapult network.\nThe  - Introduction"}
{"article_lines": ["3 Pipe Assembly Leak Test \t3.1 Background \t3.2 Process Analysis \t3.3 Hardware and Software \t3.3.1 Development Software \t3.3.2 Database \t3.3.3 Augmented Reality \t3.3.4 Nutrunner \t3.3.5 Leak Test Instrument", "Crown House Technologies manufacture the electric control panel for the AMF modular plant room. The modular panel design uses a single schematic to accommodate plant rooms with one to  six  pumps.  The electric  panel  houses  both  the  electrical  supply  and the  electronic control for the pumps. Once installed into the plant room module, the panel must be tested and comply with an electrical installation test (BS7671), electrical installation condition report, and a continuity test to ensure the panel has been correctly linked to the external units. The tests occur during the factory acceptance testing, before the module is installed on-site. For the pilot scheme, the continuity test was identified as the first process to adopt augmented reality.", "At  Crown  House  Technologies,  a  datasheet  accompanies  each  manufacturing  assembly outlining  the  manufacturing  stages,  current  progress,  and  completion  timestamps.  Each assembly  must  undergo  various  factory  acceptance  tests  to  ensure  it  meets  the  relevant industry standards; the paper test results are added to the datasheet."], "summary_lines": "Technology  (TRL)\nPhase MRL State of Development\nProduction Implementation\n9 Full production process qualified for full range of parts and full metrics achieved\n8 Full production process qualified for full range of parts\n7 Capability and rate confirmed\n6 Process optimised for production rate on production equipment\n5 Basic capability demonstrated\nTechnology assessment and \n4 Production validated in lab environment\n3 Experimental proof of concept completed\n2 Application and validity of concept validated or demonstrated\n1 Concept proposed with scientific validation\nThe  - Technology"}
{"article_lines": ["Using three delivery methods, the user is provided with various levels of information that can be utilised or ignored, depending on competency and familiarity of the task. During testing and through user feedback, it was observed that users referred to the virtual window mainly for the terminal  location  images  (see  Figure  11),  and heavily  relied  on  the  overlay  and  heads-up display to guide them through the process. Three of the AR testers noted that adding a visual highlight on the overlay for the location of the terminal would further increase the ease of the task.", "1 Introduction \t2 Electric Panel \t2.1 Background \t2.2 Process Analysis \t2.3 Hardware and Software \t2.3.1 Development Software \t2.3.2 Database \t2.3.3 Augmented Reality and Mixed Reality \t2.3.4 Multimeter", "The first application guides an engineer through the continuity testing of an electric panel, which is required as part of factory acceptance testing. The application use a mixture of heads- up-display  (HUD),  virtual  monitors,  and  a  mixed  reality  overlay  to  provide  the  user  with information  of  varying  degrees  of  detail  and  guidance.  Using  wireless  communication,  the user\u2019s augmented reality headset communicates with wireless tools and a backend database to fetch the appropriate test procedures and record the results."], "summary_lines": "Factory 2050 DAA - Overview\nWhat themes does Digitally Assisted \n\u2022 Data / Informs Visualisation.\n\u2022 Process / Task Verification.\n\u2022 System (MES / PLM) Integration.\n\u2022 Layout & Optimisation\n\u2022 Operator Work Instructions.\n\u2022 Just in Time Information"}
{"article_lines": ["Tablets are at TRL 9 and are widely used in the manufacturing industry. A tablet-based solution would  act  as  an  intermediate  step  to  augmented  reality  adoption.  By  combining  the augmented overlay and virtual  window into a single screen-based user interface, a solution could be developed to utilise the benefits of automation provided by the current solution\u2019s centralised storage and connected smart tools.", "The first application guides an engineer through the continuity testing of an electric panel, which is required as part of factory acceptance testing. The application use a mixture of heads- up-display  (HUD),  virtual  monitors,  and  a  mixed  reality  overlay  to  provide  the  user  with information  of  varying  degrees  of  detail  and  guidance.  Using  wireless  communication,  the user\u2019s augmented reality headset communicates with wireless tools and a backend database to fetch the appropriate test procedures and record the results.", "Heads up displays have been widely used in the aerospace industry to provide pilots with critical  information  while  minimising distraction  from their  primary  object.  With the birth of wearable technology, there is a growing  adoption  into  the  manufacturing space to provide just-in-time information to engineers and technicians [10]."], "summary_lines": "Factory 2050 DAA - Overview\nThrough which medium?\n\u2022 Wearable Technology.\n\u2022 Head Mounted Devices.\n\u2022 Optical Projection.\n\u2022 Immersive Technologies.\nHow is this delivered?\n\u2022 3D Environments.\n\u2022  Overlays.\n\u2022 Intuitive Handheld GUI\u2019s.\n\u2022 Digital Platforms."}
{"article_lines": ["Augmented reality requires information about the user\u2019s physical  surroundings to provide a contextual overlay. This is captured through various sensors such as monocular and binocular cameras, IR depth cameras, and inertial measurement units (IMU). Post processing on sensor feedback helps to locate the user and identify objects in view. With a virtual model of an object or environment and an understanding of the user\u2019s location, it is possible to align the digital world to the physical world, creating an overlay as seen in Error: Reference source not found. An augmented overlay allows for object highlighting as seen in Figure 6.", "The  application  was  developed  to  provide the  use  with  digital  work  instructions  and real-time  feedback  through  a  heads  up display (HUD). The Vuzix M300 (Figure 12) monocular  AR headset  is  recommended to deliver the work instructions hands-free, but the application  has been developed to run on both handheld and wearable android device alike.", "[10] T. P. Caudell and D. W. Mizell, \u201cAugmented reality: an application of heads-up display technology  to  manual  manufacturing  processes,\u201d  in  Proceedings  of  the  Twenty-Fifth Hawaii International Conference on System Sciences, 1992, pp. 659\u2013669 vol.2."], "summary_lines": "Copyright \u00a9 The University of Sheffield / May-17  with Boeing.                \nTechnology that superimposes a computer-generated image on a \nHUD (direct overlay) World overlay"}
{"article_lines": ["[1] S. Gnanasundaram and A. Shrivastava,  Information storage and management\u202f: storing, managing,  and  protecting  digital  information  in  classic,  virtualized,  and  cloud environments. Wiley, 2012.", "Heads up displays have been widely used in the aerospace industry to provide pilots with critical  information  while  minimising distraction  from their  primary  object.  With the birth of wearable technology, there is a growing  adoption  into  the  manufacturing space to provide just-in-time information to engineers and technicians [10].", "[10] T. P. Caudell and D. W. Mizell, \u201cAugmented reality: an application of heads-up display technology  to  manual  manufacturing  processes,\u201d  in  Proceedings  of  the  Twenty-Fifth Hawaii International Conference on System Sciences, 1992, pp. 659\u2013669 vol.2."], "summary_lines": "Copyright \u00a9 The University of Sheffield / May-17  with Boeing.                \nReal and virtual world merging through sensor \n\u2022 Environment cameras"}
{"article_lines": ["The AMRC with Boeing has exercised due care in conducting this report but has not, apart from as  specifically  stated,  independently  verified  information  provided  by  others.  No  other warranty, express or implied, is made in relation to the contents of this report. Therefore, the AMRC  with  Boeing  assumes  no  liability  for  any  loss  resulting  from  errors,  omissions,  or misrepresentations made by others. Any recommendations, opinions or findings stated in this report are based on circumstances and facts as they existed at the time the AMRC with Boeing performed the work. Any changes in such circumstances and facts upon which this report is based  may  adversely  affect  any  recommendations,  opinions  or  findings  contained  in  this report.  Where experiments have been carried out,  these have been restricted to a level of detail required to achieve the stated objectives of the statement of work. This work has been undertaken in accordance with the AMRC with Boeing\u2019s quality system.", "Heads up displays have been widely used in the aerospace industry to provide pilots with critical  information  while  minimising distraction  from their  primary  object.  With the birth of wearable technology, there is a growing  adoption  into  the  manufacturing space to provide just-in-time information to engineers and technicians [10].", "The health and safety implications of using wearable technology over an extended period of time is currently unknown. The AMRC, in conjunction with EPSRC [22] and Cranfield University [23], are currently conducting research into the human factors and ergonomics of wearable technology and will be publishing a paper in late 2017."], "summary_lines": "Copyright \u00a9 The University of Sheffield / May-17  with Boeing.                \nAR Benefits and Capabilities\n\u2022 Visual enhancement"}
{"article_lines": ["Crown House Technologies manufacture the electric control panel for the AMF modular plant room. The modular panel design uses a single schematic to accommodate plant rooms with one to  six  pumps.  The electric  panel  houses  both  the  electrical  supply  and the  electronic control for the pumps. Once installed into the plant room module, the panel must be tested and comply with an electrical installation test (BS7671), electrical installation condition report, and a continuity test to ensure the panel has been correctly linked to the external units. The tests occur during the factory acceptance testing, before the module is installed on-site. For the pilot scheme, the continuity test was identified as the first process to adopt augmented reality.", "The electric panel used for the testing will not be connected to any externally components; therefore,  the  continuity  test  will  not  include  an  insulation  resistance  test  or  wire  length measure. A visual check will be conducted as part of the test and requires the tester to check for visible damage or loose connectors.", "At  Crown  House  Technologies,  a  datasheet  accompanies  each  manufacturing  assembly outlining  the  manufacturing  stages,  current  progress,  and  completion  timestamps.  Each assembly  must  undergo  various  factory  acceptance  tests  to  ensure  it  meets  the  relevant industry standards; the paper test results are added to the datasheet."], "summary_lines": "Copyright \u00a9 The University of Sheffield / May-17  with Boeing.                \nElectric Cabinet Testing\nFactory Acceptance Testing\n\u2022 Electrical installation test (BS7671)"}
{"article_lines": ["The AMRC with Boeing has exercised due care in conducting this report but has not, apart from as  specifically  stated,  independently  verified  information  provided  by  others.  No  other warranty, express or implied, is made in relation to the contents of this report. Therefore, the AMRC  with  Boeing  assumes  no  liability  for  any  loss  resulting  from  errors,  omissions,  or misrepresentations made by others. Any recommendations, opinions or findings stated in this report are based on circumstances and facts as they existed at the time the AMRC with Boeing performed the work. Any changes in such circumstances and facts upon which this report is based  may  adversely  affect  any  recommendations,  opinions  or  findings  contained  in  this report.  Where experiments have been carried out,  these have been restricted to a level of detail required to achieve the stated objectives of the statement of work. This work has been undertaken in accordance with the AMRC with Boeing\u2019s quality system.", "Heads up displays have been widely used in the aerospace industry to provide pilots with critical  information  while  minimising distraction  from their  primary  object.  With the birth of wearable technology, there is a growing  adoption  into  the  manufacturing space to provide just-in-time information to engineers and technicians [10].", "[1] S. Gnanasundaram and A. Shrivastava,  Information storage and management\u202f: storing, managing,  and  protecting  digital  information  in  classic,  virtualized,  and  cloud environments. Wiley, 2012."], "summary_lines": "Copyright \u00a9 The University of Sheffield / May-17  with Boeing.                \nMarker Cabinet"}
{"article_lines": ["Heads up displays have been widely used in the aerospace industry to provide pilots with critical  information  while  minimising distraction  from their  primary  object.  With the birth of wearable technology, there is a growing  adoption  into  the  manufacturing space to provide just-in-time information to engineers and technicians [10].", "[1] S. Gnanasundaram and A. Shrivastava,  Information storage and management\u202f: storing, managing,  and  protecting  digital  information  in  classic,  virtualized,  and  cloud environments. Wiley, 2012.", "The AMRC with Boeing has exercised due care in conducting this report but has not, apart from as  specifically  stated,  independently  verified  information  provided  by  others.  No  other warranty, express or implied, is made in relation to the contents of this report. Therefore, the AMRC  with  Boeing  assumes  no  liability  for  any  loss  resulting  from  errors,  omissions,  or misrepresentations made by others. Any recommendations, opinions or findings stated in this report are based on circumstances and facts as they existed at the time the AMRC with Boeing performed the work. Any changes in such circumstances and facts upon which this report is based  may  adversely  affect  any  recommendations,  opinions  or  findings  contained  in  this report.  Where experiments have been carried out,  these have been restricted to a level of detail required to achieve the stated objectives of the statement of work. This work has been undertaken in accordance with the AMRC with Boeing\u2019s quality system."], "summary_lines": "Copyright \u00a9 The University of Sheffield / May-17  with Boeing.                \nTransition to Digital\n\u2022 s\n\u2022 Work instruction delivery"}
{"article_lines": ["[1] S. Gnanasundaram and A. Shrivastava,  Information storage and management\u202f: storing, managing,  and  protecting  digital  information  in  classic,  virtualized,  and  cloud environments. Wiley, 2012.", "Tablets are at TRL 9 and are widely used in the manufacturing industry. A tablet-based solution would  act  as  an  intermediate  step  to  augmented  reality  adoption.  By  combining  the augmented overlay and virtual  window into a single screen-based user interface, a solution could be developed to utilise the benefits of automation provided by the current solution\u2019s centralised storage and connected smart tools.", "Heads up displays have been widely used in the aerospace industry to provide pilots with critical  information  while  minimising distraction  from their  primary  object.  With the birth of wearable technology, there is a growing  adoption  into  the  manufacturing space to provide just-in-time information to engineers and technicians [10]."], "summary_lines": "Copyright \u00a9 The University of Sheffield / May-17  with Boeing.                \nTransition to Digital\nSheet\ns\n\u2022 Diagram \uf0e0 SQL Server"}
{"article_lines": ["Heads up displays have been widely used in the aerospace industry to provide pilots with critical  information  while  minimising distraction  from their  primary  object.  With the birth of wearable technology, there is a growing  adoption  into  the  manufacturing space to provide just-in-time information to engineers and technicians [10].", "The  current  solution  relies  upon  an  accurate  overlay  to  prevent  incorrect  component highlighting.  The degree of  accuracy  required  for  the  continuity  testing  process  is  not  yet robust enough for live industrial use. Research is being conducted at the AMRC to investigate the latest marker and markerless tracking in an effort to increase the reliability of augmented overlay technology. It is recommended that a tablet based adaptation is developed as an initial step towards augmented reality adoption. A tablet based solution would provide the benefit of digital  work  instructions,  relevant  information  retrieval,  but  substitutes  virtual  model highlighting for an augmented overlay (see Figure 11).", "[10] T. P. Caudell and D. W. Mizell, \u201cAugmented reality: an application of heads-up display technology  to  manual  manufacturing  processes,\u201d  in  Proceedings  of  the  Twenty-Fifth Hawaii International Conference on System Sciences, 1992, pp. 659\u2013669 vol.2."], "summary_lines": "Copyright \u00a9 The University of Sheffield / May-17  with Boeing.                \nTransition to Digital\n\u2022 Measurements + Laser Scan \uf0e0 3DS Max"}
{"article_lines": ["Heads up displays have been widely used in the aerospace industry to provide pilots with critical  information  while  minimising distraction  from their  primary  object.  With the birth of wearable technology, there is a growing  adoption  into  the  manufacturing space to provide just-in-time information to engineers and technicians [10].", "The AMRC with Boeing has exercised due care in conducting this report but has not, apart from as  specifically  stated,  independently  verified  information  provided  by  others.  No  other warranty, express or implied, is made in relation to the contents of this report. Therefore, the AMRC  with  Boeing  assumes  no  liability  for  any  loss  resulting  from  errors,  omissions,  or misrepresentations made by others. Any recommendations, opinions or findings stated in this report are based on circumstances and facts as they existed at the time the AMRC with Boeing performed the work. Any changes in such circumstances and facts upon which this report is based  may  adversely  affect  any  recommendations,  opinions  or  findings  contained  in  this report.  Where experiments have been carried out,  these have been restricted to a level of detail required to achieve the stated objectives of the statement of work. This work has been undertaken in accordance with the AMRC with Boeing\u2019s quality system.", "[1] S. Gnanasundaram and A. Shrivastava,  Information storage and management\u202f: storing, managing,  and  protecting  digital  information  in  classic,  virtualized,  and  cloud environments. Wiley, 2012."], "summary_lines": "Copyright \u00a9 The University of Sheffield / May-17  with Boeing.                \nTransition to Digital\nWork Instruction Delivery"}
{"article_lines": ["The AMRC with Boeing has exercised due care in conducting this report but has not, apart from as  specifically  stated,  independently  verified  information  provided  by  others.  No  other warranty, express or implied, is made in relation to the contents of this report. Therefore, the AMRC  with  Boeing  assumes  no  liability  for  any  loss  resulting  from  errors,  omissions,  or misrepresentations made by others. Any recommendations, opinions or findings stated in this report are based on circumstances and facts as they existed at the time the AMRC with Boeing performed the work. Any changes in such circumstances and facts upon which this report is based  may  adversely  affect  any  recommendations,  opinions  or  findings  contained  in  this report.  Where experiments have been carried out,  these have been restricted to a level of detail required to achieve the stated objectives of the statement of work. This work has been undertaken in accordance with the AMRC with Boeing\u2019s quality system.", "[1] S. Gnanasundaram and A. Shrivastava,  Information storage and management\u202f: storing, managing,  and  protecting  digital  information  in  classic,  virtualized,  and  cloud environments. Wiley, 2012.", "Heads up displays have been widely used in the aerospace industry to provide pilots with critical  information  while  minimising distraction  from their  primary  object.  With the birth of wearable technology, there is a growing  adoption  into  the  manufacturing space to provide just-in-time information to engineers and technicians [10]."], "summary_lines": "Copyright \u00a9 The University of Sheffield / May-17  with Boeing.                \n\u2022 Autom document creation\n\u2022 Cross-skilling workforce"}
{"article_lines": ["With reference to NASA\u2019s TRL scale [8], the continuity testing AR solution is currently at TRL 4; the  HoloLens  application  has  been tested in  lab  environment.  There  are  several  unknown factors which must be understood before the solution can mature to TRL 5. Firstly, with the HoloLens  and other  such augmented  reality  devices  currently  in  their  infancy,  the  human factors are widely unknown and as discussed, require further research before such devices can be deployed in industry. Secondly, a continuous and more accurate orientation solution must be implemented to remove the requirement for manual adjustments. Finally, at a current cost of  \u00a34,529 per  unit,  and  without  the  industry  standard  safety  features  of  PPE,  the  current", "The health and safety implications of using wearable technology over an extended period of time is currently unknown. The AMRC, in conjunction with EPSRC [22] and Cranfield University [23], are currently conducting research into the human factors and ergonomics of wearable technology and will be publishing a paper in late 2017.", "The electric panel used for the testing will not be connected to any externally components; therefore,  the  continuity  test  will  not  include  an  insulation  resistance  test  or  wire  length measure. A visual check will be conducted as part of the test and requires the tester to check for visible damage or loose connectors."], "summary_lines": "Copyright \u00a9 The University of Sheffield / May-17  with Boeing.                \nHoloLens tested in lab environment: \n\u2022 Human factor unknown\n\u2022 Overlay robustness unknown\n\u2022 Device cost (Commercial -"}
{"article_lines": ["[11] Bosch, \u201cNexo cordless Wi-Fi nutrunner - Bosch Rexroth USA.\u201d [Online]. Available: https:// www.boschrexroth.com/en/us/products/product-groups/tightening-technology/nexo- cordless-wi-fi-nutrunner/index. [Accessed: 18-Aug-2017].", "The AMRC with Boeing has exercised due care in conducting this report but has not, apart from as  specifically  stated,  independently  verified  information  provided  by  others.  No  other warranty, express or implied, is made in relation to the contents of this report. Therefore, the AMRC  with  Boeing  assumes  no  liability  for  any  loss  resulting  from  errors,  omissions,  or misrepresentations made by others. Any recommendations, opinions or findings stated in this report are based on circumstances and facts as they existed at the time the AMRC with Boeing performed the work. Any changes in such circumstances and facts upon which this report is based  may  adversely  affect  any  recommendations,  opinions  or  findings  contained  in  this report.  Where experiments have been carried out,  these have been restricted to a level of detail required to achieve the stated objectives of the statement of work. This work has been undertaken in accordance with the AMRC with Boeing\u2019s quality system.", "as  required.  Some connected tools  can also receive  input,  allowing them to be  controlled externally. A wirelessly connected multimeter would be able to provide continuity results back to a listening device, which can format and store the feedback data in a database."], "summary_lines": "Twitter: @the\nFor further information please contact or \nCopyright \u00a9 The University of Sheffield / May-17  with \nBoeing."}
{"article_lines": ["1 Introduction \t2 Electric Panel \t2.1 Background \t2.2 Process Analysis \t2.3 Hardware and Software \t2.3.1 Development Software \t2.3.2 Database \t2.3.3 Augmented Reality and Mixed Reality \t2.3.4 Multimeter", "At  Crown  House  Technologies,  a  datasheet  accompanies  each  manufacturing  assembly outlining  the  manufacturing  stages,  current  progress,  and  completion  timestamps.  Each assembly  must  undergo  various  factory  acceptance  tests  to  ensure  it  meets  the  relevant industry standards; the paper test results are added to the datasheet.", "1. All six testers will have an understanding of how to assemble an endcap and use the required tools. Each tester will read a three page instruction manual on the process of assembly and the order and stages in which bolts are tightened."], "summary_lines": "The  - Introduction\n\tThe  - Technology \n\tFactory 2050 DAA - Overview\n\tFactory 2050 DAA - Overview\n\tAR Benefits and Capabilities\n\tElectric Cabinet Testing\n\tTransition to Digital\n\tTransition to Digital\n\tTransition to Digital\n\tTransition to Digital"}
{"article_lines": ["1 Introduction \t2 Electric Panel \t2.1 Background \t2.2 Process Analysis \t2.3 Hardware and Software \t2.3.1 Development Software \t2.3.2 Database \t2.3.3 Augmented Reality and Mixed Reality \t2.3.4 Multimeter", "3 Pipe Assembly Leak Test \t3.1 Background \t3.2 Process Analysis \t3.3 Hardware and Software \t3.3.1 Development Software \t3.3.2 Database \t3.3.3 Augmented Reality \t3.3.4 Nutrunner \t3.3.5 Leak Test Instrument", "Figure 16 provides a top-level architectural view of the data flow between the components of the AR solution. Similar to the electric panel solution, the AR device acts as a message broker between the smart tool, leak test instrument, database, and user. Raw data from each device is parsed into a user-friendly format before being displayed. Figure 17 depicts the application flow described in the sections above."], "summary_lines": "PowerPoint Presentation"}
{"article_lines": ["The AMRC with Boeing has exercised due care in conducting this report but has not, apart from as  specifically  stated,  independently  verified  information  provided  by  others.  No  other warranty, express or implied, is made in relation to the contents of this report. Therefore, the AMRC  with  Boeing  assumes  no  liability  for  any  loss  resulting  from  errors,  omissions,  or misrepresentations made by others. Any recommendations, opinions or findings stated in this report are based on circumstances and facts as they existed at the time the AMRC with Boeing performed the work. Any changes in such circumstances and facts upon which this report is based  may  adversely  affect  any  recommendations,  opinions  or  findings  contained  in  this report.  Where experiments have been carried out,  these have been restricted to a level of detail required to achieve the stated objectives of the statement of work. This work has been undertaken in accordance with the AMRC with Boeing\u2019s quality system.", "Executive summary The purpose of this document is to conclude the deliverables of the Laing O\u2019Rourke Digitally Assisted Testing work package as part of the wider AMSCI project. The AMRC developed two augmented reality applications to demonstrate and quantify the benefits of augmented reality adoption in assembly, verification, and testing processes.", "The health and safety implications of using wearable technology over an extended period of time is currently unknown. The AMRC, in conjunction with EPSRC [22] and Cranfield University [23], are currently conducting research into the human factors and ergonomics of wearable technology and will be publishing a paper in late 2017."], "summary_lines": "Confidential.  Copyright \u00a9 The University of Sheffield / May-17 \nAMRC with Boeing Template detail: AMRC.PPTW Revision 2 (May \nLaing O\u2019Rourke Technology \nPressure Leak ing\nIntegrate Manufacturing Group"}
{"article_lines": ["Augmented reality requires information about the user\u2019s physical  surroundings to provide a contextual overlay. This is captured through various sensors such as monocular and binocular cameras, IR depth cameras, and inertial measurement units (IMU). Post processing on sensor feedback helps to locate the user and identify objects in view. With a virtual model of an object or environment and an understanding of the user\u2019s location, it is possible to align the digital world to the physical world, creating an overlay as seen in Error: Reference source not found. An augmented overlay allows for object highlighting as seen in Figure 6.", "Tablets are at TRL 9 and are widely used in the manufacturing industry. A tablet-based solution would  act  as  an  intermediate  step  to  augmented  reality  adoption.  By  combining  the augmented overlay and virtual  window into a single screen-based user interface, a solution could be developed to utilise the benefits of automation provided by the current solution\u2019s centralised storage and connected smart tools.", "Accurate  head  and  environment  tracking are  essential  to  provide  the  overlay  as components in the electric panel are very close  together  and  as  narrow  as  10mm. The  Microsoft  HoloLens  was  selected  as the AR delivery device for this solution due to its  unique spatial  mapping capabilities and multiple input methods. The HoloLens uses an IMU and four cameras to track the user\u2019s environment and accurately tracks head  movement.  The  HoloLens  is controlled through gaze, voice, and hand gestures. The standard HoloLens gestures will be used to navigate the application; a list  of  the gestures and how to perform them can be found at [7]."], "summary_lines": "Real and virtual world merging through sensor \n\u2022 Touch Button Navigation\n\u2022 Head-Tracking and GPS\n\u2022 Operates on Android"}
{"article_lines": ["Augmented reality (AR) provides additional context and information to a user\u2019s view through various methods of light projection (the preceding trade study describes AR and the various delivery methods and tools). For augmented reality to deliver relevant information, data must first be captured, processed, stored, and retrieved.", "Executive summary The purpose of this document is to conclude the deliverables of the Laing O\u2019Rourke Digitally Assisted Testing work package as part of the wider AMSCI project. The AMRC developed two augmented reality applications to demonstrate and quantify the benefits of augmented reality adoption in assembly, verification, and testing processes.", "From  the  resulting  data  and  observations  made  during  the  testing  process,  the  use  of augmented reality to assist in the continuity process shows a time saving of 57.7% over using paper schematics and paper-based recording. Augmented reality also showed a decrease in mistakes of 87% and eliminated missed tests for both continuity and visual tests."], "summary_lines": "AR Benefits and Capabilities\n\u2022 Visual enhancement"}
{"article_lines": ["Building  modules  often  require  pipework  to  transport  various  liquids  and  gases  across  a building. The pipework for a module is built in sections, which are assembled together before leak tested. Leak testing is performed through air pressurisation and evaluating pressure decay over time. The normal operating pressure is exceeded during testing to ensure the assembly's integrity, even when over pressurised.", "The leak testing process can be split into three main tasks: part acquisition, endcap assembly, and leak testing. Part acquisition begins with identifying the quantity and size of the endcaps required to block all open ends of an assembly. Each assembly carries a unique ID number; this can  be  used  to  correctly  identify  the  assembly,  and  querying  a  database  for  the  endcap information. Associating a unique ID with the endcaps and gaskets will provide a method of validation during part selection, further reducing the risk of incorrect assembly.", "The second application guides a user through the assembly of a pipe section in preparation for leak testing and stores the test results in a backend database. A monocular HUD provides the user with just-in-time assembly instructions and utilises a wirelessly connected torque wrench to minimise errors and verify assembly."], "summary_lines": "Pipework Pressure ing\n\u2022 Unique login via head mounted display (HMD)\n\u2022  unique individual pipework assemblies \n\u2022 Loads complete job-list from database for particular \n\u2022 Loads and verifies required  components\n1) User Login 2) Assembly"}
{"article_lines": ["3 Pipe Assembly Leak Test \t3.1 Background \t3.2 Process Analysis \t3.3 Hardware and Software \t3.3.1 Development Software \t3.3.2 Database \t3.3.3 Augmented Reality \t3.3.4 Nutrunner \t3.3.5 Leak Test Instrument", "The  second  stage  requires  the  correct  installation  of  the  endcaps  on  the  assembly  in preparation for leak testing. For a correct seal, the flange must be sealed with the correct sized gasket, and tightened with incremental torque in the correct order [9]. Digital work instructions can provide just-in-time information, ensuring an assembly process is completed in the correct order. For a system to provide the correct step at the right time, a feedback loop is required. The unique ID of each endcap can be used to display the correct work instructions, number of bolts,  torque  level,  and  order  of  tightening.  Smart  tools  provide  an  automated  method of setting torque based on the work instructions and provide feedback when the target torque has been reached.", "Building  modules  often  require  pipework  to  transport  various  liquids  and  gases  across  a building. The pipework for a module is built in sections, which are assembled together before leak tested. Leak testing is performed through air pressurisation and evaluating pressure decay over time. The normal operating pressure is exceeded during testing to ensure the assembly's integrity, even when over pressurised."], "summary_lines": "\u2022 Animation of complete  \n\u2022 Live connected tool readout\n\u2022 Required torque verification\n\u2022 Pre-programmed leak test\n4) Assembly 5) Bolt"}
{"article_lines": ["At  Crown  House  Technologies,  a  datasheet  accompanies  each  manufacturing  assembly outlining  the  manufacturing  stages,  current  progress,  and  completion  timestamps.  Each assembly  must  undergo  various  factory  acceptance  tests  to  ensure  it  meets  the  relevant industry standards; the paper test results are added to the datasheet.", "[13] L.  Kester,  P.  A.  Kirschner,  and  J.  J.  G.  van  Merri\u00ebnboer,  \u201cJust-in-time  information presentation: Improving learning a troubleshooting skill,\u201d  Contemp. Educ. Psychol., vol. 31, no. 2, pp. 167\u2013185, Apr. 2006.", "[10] T. P. Caudell and D. W. Mizell, \u201cAugmented reality: an application of heads-up display technology  to  manual  manufacturing  processes,\u201d  in  Proceedings  of  the  Twenty-Fifth Hawaii International Conference on System Sciences, 1992, pp. 659\u2013669 vol.2."], "summary_lines": ""}
{"article_lines": ["Executive summary The purpose of this document is to conclude the deliverables of the Laing O\u2019Rourke Digitally Assisted Testing work package as part of the wider AMSCI project. The AMRC developed two augmented reality applications to demonstrate and quantify the benefits of augmented reality adoption in assembly, verification, and testing processes.", "Building  modules  often  require  pipework  to  transport  various  liquids  and  gases  across  a building. The pipework for a module is built in sections, which are assembled together before leak tested. Leak testing is performed through air pressurisation and evaluating pressure decay over time. The normal operating pressure is exceeded during testing to ensure the assembly's integrity, even when over pressurised.", "To understand the effects of using augmented reality to assist the continuity testing procedure, a series of controlled tests will be conducted. Specifically, the testing procedure will focus on understanding the impact on continuity test completion time and testing accuracy."], "summary_lines": "AR Benefits and Capabilities\n\tPipework Pressure ing"}
{"article_lines": ["A large array of these photo-electric cells on the chip provides each pixel of the  image. The electron \u201clevel\u201d can then be converted into a value, and these pixel  values and positions are put together to create a whole digital image. All the vision  systems looked at work on using same technology.", "Figure 8 : Programming Interface for OpenCV Program (showing on pattern detection  on paper) ................................................................................................................. 18", "This functionality greatly decreases the time and programming resources needed to  perform vision tasks. However, it is still necessary to build a program for this  framework to run within. Using the SimpleShapeChecker example, in order to  perform this code, it has to be called in the program (by an event such as a timed- trigger or a user selection in a GUI), and passed a filtered image from a file or  camera feed to work with."], "summary_lines": ""}
{"article_lines": ["5.3 Hardware Selection  Use of vision systems has been driven by the development of digital photo-electric  light sensors. These take in light, concentrated onto a small chip through a lens, and  convert it into electrons in a photo-electric cell on the chip. The chips vary in  manufacturing techniques, leading to two main sensor types; complementary metal- oxide semiconductor (CMOS) and charge-coupled device (CCD). They work in the  same general way, with CMOS taking less power and producing higher resolution  images, but with more noise and therefore lower image quality than CCD.", "There are different cameras within the Cognex range for different applications. For  this project, the AMRC decided on an Intellect Machine Vision camera, using an  Intellect 554 camera (covered below in 5.3.1 Commercial Cameras) and the Intellect  v1.5.1 software. This software uses a set of graphical tools that are dropped onto the", "For the CotS camera system, we looked specifically at webcams. Webcams are low- cost computer cameras, typically for video conferencing purposes. They are typically  small and require very low power to run. Internally, they are similar to many other  low-cost digital cameras, although they usually lack complex optical lensing to reduce  cost, and are built to stream video images as easily as high-resolution stills."], "summary_lines": "\u00a9AMC 2012\u00a9AMC 2013\u00a9AMC 2012"}
{"article_lines": ["There is a wide range of open source and commercial vision systems available on  the market. Due to limited time and to ensure the project was kept in budget, a  limited number of open-source software packages and commercial-off-the-shelf  hardware had to be picked to look at in detail. To  down select from the wider range  of choices, a review was conducted of available solutions. The visible features, pros,  and cons were weighed up to decide what would represent the most useful solution  for the project.", "There are different cameras within the Cognex range for different applications. For  this project, the AMRC decided on an Intellect Machine Vision camera, using an  Intellect 554 camera (covered below in 5.3.1 Commercial Cameras) and the Intellect  v1.5.1 software. This software uses a set of graphical tools that are dropped onto the", "5.3 Hardware Selection  Use of vision systems has been driven by the development of digital photo-electric  light sensors. These take in light, concentrated onto a small chip through a lens, and  convert it into electrons in a photo-electric cell on the chip. The chips vary in  manufacturing techniques, leading to two main sensor types; complementary metal- oxide semiconductor (CMOS) and charge-coupled device (CCD). They work in the  same general way, with CMOS taking less power and producing higher resolution  images, but with more noise and therefore lower image quality than CCD."], "summary_lines": "\u00a9AMC 2012\u00a9AMC 2013\u00a9AMC 2012\u00a9AMC 2013\nOpen-Source Visionystems\nAdrian irst Dr Alberto Zorcolo\nPresentation Contents\n\u2022 What are vision systems their \n\u2022 Goals for the project\n\u2022 Overview of  hardware \n\u2022ummary of results\n\u2022  urther Work"}
{"article_lines": ["5.3 Hardware Selection  Use of vision systems has been driven by the development of digital photo-electric  light sensors. These take in light, concentrated onto a small chip through a lens, and  convert it into electrons in a photo-electric cell on the chip. The chips vary in  manufacturing techniques, leading to two main sensor types; complementary metal- oxide semiconductor (CMOS) and charge-coupled device (CCD). They work in the  same general way, with CMOS taking less power and producing higher resolution  images, but with more noise and therefore lower image quality than CCD.", "To combat this, industrial vision software often comes with advanced built-in features  that allow a relatively inexperienced user to filter difficult camera feeds and images to  reliably make out features and perform a task (i.e. guide a robot, measure a part).  This software is often designed to be used in tandem with industrial specification  cameras which are guaranteed to perform as specified and give a usable image for  inspecting.", "Vision systems are used extensively in automation as a flexible form of metrology  and sensing, allowing a system to have a view of work akin to a human\u2019s view on a  task. However solutions are often challenging to implement for several reasons, such  as variations in lighting, reflective nature of surfaces, and limitations placed by  camera accuracy and calibration."], "summary_lines": "\u00a9AMC 2012\u00a9AMC 2013\u00a9AMC 2012\u00a9AMC 2013\nSystems?\u2022 Machine Visionystems are sensors to \nallow machines to \u201csee\u201d (much like a \n\u2022 Most based around digital camera \n\u2022 Industrial use often focuses on \nInspection Alignment/Guidance.Object to inspect\n input"}
{"article_lines": ["There are different cameras within the Cognex range for different applications. For  this project, the AMRC decided on an Intellect Machine Vision camera, using an  Intellect 554 camera (covered below in 5.3.1 Commercial Cameras) and the Intellect  v1.5.1 software. This software uses a set of graphical tools that are dropped onto the", "5.3 Hardware Selection  Use of vision systems has been driven by the development of digital photo-electric  light sensors. These take in light, concentrated onto a small chip through a lens, and  convert it into electrons in a photo-electric cell on the chip. The chips vary in  manufacturing techniques, leading to two main sensor types; complementary metal- oxide semiconductor (CMOS) and charge-coupled device (CCD). They work in the  same general way, with CMOS taking less power and producing higher resolution  images, but with more noise and therefore lower image quality than CCD.", "Unlike the preceding two systems, the AMRC had to use OpenCV as a framework to  develop our own vision system software. This lead to a longer development time as  the GUI and underlying code had to be created by the AMRC. In addition, the AMRC  was unfamiliar with OpenCV so it had to be learnt from scratch. The AMRC found the  online documentation (at the time of the project) to be reasonably deep and  comprehensive. Examples could be found that made some of the less intuitive  elements easier to understand. However initial setup of the system on a PC  (involving changing the PC\u2019s environmental variables) was more convoluted that any  of the other systems tested. The C++ programming environment also required more  work and time to add a GUI to than the C# equivalent in AForge.NET (although a it  could be encompassed in to pre-existing GUI frameworks if available). Overall this  was the most time consuming system to setup of the 4."], "summary_lines": "\u00a9AMC 2012\u00a9AMC 2013\u00a9AMC 2012\u00a9AMC 2013\nWhat are the existing: \nlimitations?\u2022 Visionystems"}
{"article_lines": ["Outside of vision systems, camera technology for general consumers has been  driven particularly rapidly to the point where many modern devices have high-spec  cameras with  high pixel counts developed at a low cost. This has led to a great drive  in improving the quality and cost of digital camera systems, leading to high quality  and very low cost cameras being widely available. Combined with this, many open- source vision solutions have been developed by programmers over recent years.  Open-source software is created under open-source licenses online by any  developers who want to take part. This makes it free to use, and provides the entire  code base for modification if needed.", "5.3 Hardware Selection  Use of vision systems has been driven by the development of digital photo-electric  light sensors. These take in light, concentrated onto a small chip through a lens, and  convert it into electrons in a photo-electric cell on the chip. The chips vary in  manufacturing techniques, leading to two main sensor types; complementary metal- oxide semiconductor (CMOS) and charge-coupled device (CCD). They work in the  same general way, with CMOS taking less power and producing higher resolution  images, but with more noise and therefore lower image quality than CCD.", "There are different cameras within the Cognex range for different applications. For  this project, the AMRC decided on an Intellect Machine Vision camera, using an  Intellect 554 camera (covered below in 5.3.1 Commercial Cameras) and the Intellect  v1.5.1 software. This software uses a set of graphical tools that are dropped onto the"], "summary_lines": "\u00a9AMC 2012\u00a9AMC 2013\u00a9AMC 2012\u00a9AMC 2013\nPrinciples behind the \nprojectecent popular technologies such as \nsmartphones have driven fast commercial \ndevelopment massive cost reduction \niPhone 5s: sbnation.com\naspberry Pi: .bit-tech.net\nMS Kinect: microsoft.com\nCode image: netdna-cdn.com\nInternet computing has lead to lots of \nrapid, free-to-licence Open-Source \n development\u2026"}
{"article_lines": ["The purpose of this project was to determine if the recent drive in camera technology  paired with stable versions of open-source vision software could provide a decent  testing bed for vision systems and the find a way to grade the quality of these  systems against each other and the proven industrial equivalents.", "From these criteria, the AMRC proceeded to develop a set of tests that would give a  measurable result. From this, a comparison of the performance of each system in  given categories could be performed.", "7 Testing  The first stage of the testing was to find a suitable part on which to perform the tests  detailed in the testing plan. For this, a part with known feature sizes was required.  The parts chosen were measured earlier on the AMRC\u2019s CMM.  This is a  measurement device with a stated accuracy of 5 microns. This was the most  accurate and reliable part that could be obtained with the budget of the project. 5  microns on the scale of the part represented far less than the accuracy the cameras  could obtain based on the pixel density of the sensors and the area they would be  focusing on. Therefore, the CMM results (and the related CAD) could sensibly be  used as a reasonable baseline measurement to which the camera based  measurements could be compared. This part can be seen below (Figure 10) with  markings for the various parts that inspections would be carried out on:"], "summary_lines": "\u00a9AMC 2012\u00a9AMC 2013\u00a9AMC 2012\u00a9AMC 2013\n\u2022 o test the  viability of \nopen-source vision \n\u2022 o test the  viability of \ncommercial camera \n\u2022 o gauge the performance \nusefulness of these; providing \nadvantages, disadvantages, \nperformance metrics."}
{"article_lines": ["There are different cameras within the Cognex range for different applications. For  this project, the AMRC decided on an Intellect Machine Vision camera, using an  Intellect 554 camera (covered below in 5.3.1 Commercial Cameras) and the Intellect  v1.5.1 software. This software uses a set of graphical tools that are dropped onto the", "5.3 Hardware Selection  Use of vision systems has been driven by the development of digital photo-electric  light sensors. These take in light, concentrated onto a small chip through a lens, and  convert it into electrons in a photo-electric cell on the chip. The chips vary in  manufacturing techniques, leading to two main sensor types; complementary metal- oxide semiconductor (CMOS) and charge-coupled device (CCD). They work in the  same general way, with CMOS taking less power and producing higher resolution  images, but with more noise and therefore lower image quality than CCD.", "Two cameras were selected: the Microsoft LifeCam Studio and the Trust Spotlight.  These represented the top-end and lower-end of typical home and office digital  webcams at the time of the project. These are shown in Table 3 below:"], "summary_lines": "\u00a9AMC 2012\u00a9AMC 2013\u00a9AMC 2012\u00a9AMC 2013\nSelected for project (off-the-shelf)\n-Microsoft ifeCam (~\u00a390)\n-rustpot (~\u00a310)\nrom AMC for comparison/use (industrial-use)\n-Unibrain (~\u00a32000 + lens)\n( industrial )\n+ (~\u00a34000 + lens)"}
{"article_lines": ["There is a wide range of open source and commercial vision systems available on  the market. Due to limited time and to ensure the project was kept in budget, a  limited number of open-source software packages and commercial-off-the-shelf  hardware had to be picked to look at in detail. To  down select from the wider range  of choices, a review was conducted of available solutions. The visible features, pros,  and cons were weighed up to decide what would represent the most useful solution  for the project.", "5.1 Review of Open-source Software  Firstly, an overall search was undertaken to see what software could be found online  and what features were available, with the results collated for this review.     During this search, it became obvious that open-source solutions weren\u2019t available  fully-featured as a software package and user interface akin to the industrial  equivalents the AMRC had used previously. Instead, most open-source vision code  was available in the form of a programming library. This means that rather than a  fixed program with a pre-made interface, they consist of add-ons for a programming  language. These add-ons aim to provide the complex functions of vision software  without the programming effort; instead allowing for a simpler line of code in your  chosen language to call a library function that does the complex work load and  returns the results in a set format for your program to use.    Several libraries were found at varying degrees of functionality and completeness.  These were narrowed down into a set of options that were still supported by  developers and could fit the majority of our needs for a vision package in the project.  Once this sub-set of libraries had been decided upon, they were looked at in greater  detail to ascertain which ones would be worth devoting programming time to using  fully in experimentation. The available literature on each was looked at and the  resulting expected capacity of the library was recorded. The results of this are  displayed in Table 1 and Table 2 below. (The industrial solutions available to the  AMRC at the time have been included for comparison purposes).", "The final testing box size was determined by the optical range of the cameras and  lens available in the project, and by the size of the parts which were available to  measure. Taking these into account, a testing rig was built out of wood to a size of  300x250x600mm, which would allow the parts to fit and all the cameras to be  mounted with room to spare for cabling and lighting."], "summary_lines": "\u00a9AMC 2012\u00a9AMC 2013\u00a9AMC 2012\u00a9AMC 2013\neviewA review of available open-source \n was done to narrow down a \nselection to test with, based on what \nthey claimed to do how complete \n2 Openource ibraries were selected:"}
{"article_lines": ["5.1 Review of Open-source Software  Firstly, an overall search was undertaken to see what software could be found online  and what features were available, with the results collated for this review.     During this search, it became obvious that open-source solutions weren\u2019t available  fully-featured as a software package and user interface akin to the industrial  equivalents the AMRC had used previously. Instead, most open-source vision code  was available in the form of a programming library. This means that rather than a  fixed program with a pre-made interface, they consist of add-ons for a programming  language. These add-ons aim to provide the complex functions of vision software  without the programming effort; instead allowing for a simpler line of code in your  chosen language to call a library function that does the complex work load and  returns the results in a set format for your program to use.    Several libraries were found at varying degrees of functionality and completeness.  These were narrowed down into a set of options that were still supported by  developers and could fit the majority of our needs for a vision package in the project.  Once this sub-set of libraries had been decided upon, they were looked at in greater  detail to ascertain which ones would be worth devoting programming time to using  fully in experimentation. The available literature on each was looked at and the  resulting expected capacity of the library was recorded. The results of this are  displayed in Table 1 and Table 2 below. (The industrial solutions available to the  AMRC at the time have been included for comparison purposes).", "There is a wide range of open source and commercial vision systems available on  the market. Due to limited time and to ensure the project was kept in budget, a  limited number of open-source software packages and commercial-off-the-shelf  hardware had to be picked to look at in detail. To  down select from the wider range  of choices, a review was conducted of available solutions. The visible features, pros,  and cons were weighed up to decide what would represent the most useful solution  for the project.", "programming style and approach the AMRC encountered using each system, to give  the time taken some context. It is worth noting that the Open Source systems will  naturally take more time and effort, as they are libraries while the 2 industrial systems  are already programmed into pre-made suites."], "summary_lines": "\u00a9AMC 2012\u00a9AMC 2013\u00a9AMC 2012\u00a9AMC 2013\nProgramming of Open \n\u2022 Initially planned to find full open source \n\u2022 Only programming libraries available\n\u2022 Programmed multi-purpose  to"}
{"article_lines": ["6.3 Testing Rig  In order to run fair tests, a rig was required to control the conditions the test would  take place in. As the AMRC was interested in the vision properties of the system,  conditions that affect image quality were the most important to control (such as  lighting conditions).", "For the purposes of the AMRC tests, the inspections were setup separately. Within  the software, multiple sets of inspection tools can be stored as Jobs. When the  correct ID is transmitted, that particular job (and therefore set of inspection tools) are  called.", "7 Testing  The first stage of the testing was to find a suitable part on which to perform the tests  detailed in the testing plan. For this, a part with known feature sizes was required.  The parts chosen were measured earlier on the AMRC\u2019s CMM.  This is a  measurement device with a stated accuracy of 5 microns. This was the most  accurate and reliable part that could be obtained with the budget of the project. 5  microns on the scale of the part represented far less than the accuracy the cameras  could obtain based on the pixel density of the sensors and the area they would be  focusing on. Therefore, the CMM results (and the related CAD) could sensibly be  used as a reasonable baseline measurement to which the camera based  measurements could be compared. This part can be seen below (Figure 10) with  markings for the various parts that inspections would be carried out on:"], "summary_lines": "\u00a9AMC 2012\u00a9AMC 2013\u00a9AMC 2012\u00a9AMC 2013\nProcedure\u2022 ole location \nAll  done on pre-measured parts in a controlled-ing test rig"}
{"article_lines": ["5.3 Hardware Selection  Use of vision systems has been driven by the development of digital photo-electric  light sensors. These take in light, concentrated onto a small chip through a lens, and  convert it into electrons in a photo-electric cell on the chip. The chips vary in  manufacturing techniques, leading to two main sensor types; complementary metal- oxide semiconductor (CMOS) and charge-coupled device (CCD). They work in the  same general way, with CMOS taking less power and producing higher resolution  images, but with more noise and therefore lower image quality than CCD.", "There are different cameras within the Cognex range for different applications. For  this project, the AMRC decided on an Intellect Machine Vision camera, using an  Intellect 554 camera (covered below in 5.3.1 Commercial Cameras) and the Intellect  v1.5.1 software. This software uses a set of graphical tools that are dropped onto the", "Unlike the preceding two systems, the AMRC had to use OpenCV as a framework to  develop our own vision system software. This lead to a longer development time as  the GUI and underlying code had to be created by the AMRC. In addition, the AMRC  was unfamiliar with OpenCV so it had to be learnt from scratch. The AMRC found the  online documentation (at the time of the project) to be reasonably deep and  comprehensive. Examples could be found that made some of the less intuitive  elements easier to understand. However initial setup of the system on a PC  (involving changing the PC\u2019s environmental variables) was more convoluted that any  of the other systems tested. The C++ programming environment also required more  work and time to add a GUI to than the C# equivalent in AForge.NET (although a it  could be encompassed in to pre-existing GUI frameworks if available). Overall this  was the most time consuming system to setup of the 4."], "summary_lines": "\u00a9AMC 2012\u00a9AMC 2013\u00a9AMC 2012\u00a9AMC 2013"}
{"article_lines": ["Both industrial cameras use the older, more proven CCD technology for sensors.  They run at lower resolutions than the CMOS based CotS cameras, but the CCD  sensor produces lower noise images. They also both feature proper lens mounts,  and require a lens to be specified and fitted before any measurement tasks.", "Since the Cognex Intellect software (detailed in Table 4) is tied to a camera, it was  necessary to use a Cognex Intellect DVT 554 camera. It is the top of the Intellect  DVT range along with the identical resolution DVT554C (colour version of the  camera), although this is no longer Cognex\u2019s newest range of camera.", "There are different cameras within the Cognex range for different applications. For  this project, the AMRC decided on an Intellect Machine Vision camera, using an  Intellect 554 camera (covered below in 5.3.1 Commercial Cameras) and the Intellect  v1.5.1 software. This software uses a set of graphical tools that are dropped onto the"], "summary_lines": "\u00a9AMC 2012\u00a9AMC 2013\u00a9AMC 2012\u00a9AMC 2013\nesults\nCamera DV UniBrain ifeCam rust\nAvg. rror N/A 4.303% 4.431% 5.445%\nAforge.N\nCognex Intellect (only works with DV )\nCamera DV UniBrain ifeCam rust\nAvg. rror N/A N/A 3.514% 4.389%\nCamera DV UniBrain ifeCam rust\nAvg. rror N/A 1.641% 1.645% 2.172%\nCamera DV UniBrain ifeCam rust\nAvg. rror 2.015% N/A N/A N/A"}
{"article_lines": ["This project\u2019s results show a similar set of pros and cons regarding CotS camera  hardware. The cameras tested that showed excellent performance-to-cost, were  better at colour imaging than most industrial cameras and offered very high pixel  density comparatively.  However, they were less accurate devices without  considerable effort applied to the image, required more tuning to match any given  industrial environment, and crucially lacked the ability to change lenses.", "Overall, both industrial and open-source of vision systems have shown different  strengths that would be useful in a variety of contexts. The results show that open- source solutions can give good, and theoretically equal, accuracy results when  compared to industrial solutions. They can also do this at a no up-front price.  However, they also have notable weaknesses compared to industrial solutions, since  they require considerable more time and effort to program, and don\u2019t offer the  guaranteed reliability industrial software does.", "Outside of vision systems, camera technology for general consumers has been  driven particularly rapidly to the point where many modern devices have high-spec  cameras with  high pixel counts developed at a low cost. This has led to a great drive  in improving the quality and cost of digital camera systems, leading to high quality  and very low cost cameras being widely available. Combined with this, many open- source vision solutions have been developed by programmers over recent years.  Open-source software is created under open-source licenses online by any  developers who want to take part. This makes it free to use, and provides the entire  code base for modification if needed."], "summary_lines": "\u00a9AMC 2012\u00a9AMC 2013\u00a9AMC 2012\u00a9AMC 2013\n\u2022 Open-source libraries can give good results at low \n\u2022 \u2026but are far more difficult to program \nunreliable next to true industrial solutions\n\u2022 ow cost cameras offer excellent performance-to-\ncost better colour pixel density\u2026\n\u2022 \u2026but are noticeably less accurate, require more \ntuning to an  lack lens-changing \ncapacity  Overall: ow-cost, open source vision \nsolutions offer good value for money for \nresearch,  prototyping \nsolutions; but require extensive \nprogramming lack the reliability for \nactual industrial use"}
{"article_lines": ["There are several possible branches of work that could be done to extend this  project. Firstly, both the open-source software coding and the testing procedure  developed could be used to create standards for future vision system creation within  the AMRC. These standards could consist of coding standards to ensure vision code  is easily understood and cross-compatible across developers, and testing standards  to produce a fixed set of tests and metrics that could be used to determine the  suitability of vision systems to tasks before fully integrating them.    Secondly, the testing done in this project could be move onto more complex tasks  that aren\u2019t yet commonly used in industrial applications. An example of this would be  stereo-vision, a relatively new technique that uses 2 camera feeds and software to  produce 3D inspection data on a scene or object. Many of these more complex tasks  have become possible due to the rapid development of computer processing  capability; therefore they are at a lower TRL level and might be less well developed  in industrially approved systems, making open-source based solutions more viable.    Thirdly, the testing done could be extended over more cameras and possibly other  camera-related systems. New sensors such as the Microsoft Kinect and the Leap are  not strictly vision systems, but often contain cameras in conjunction with other  sensors based on similar technology (such as a CMOS or CCD light sensor tuned to  pick up on infra-red light) to perform similar inspections. This could also tie into the  previous point, as most of these sensors are based on adding 3D inspection  functionality without the complex and time consuming code required in stereo vision  systems.    Finally, if an open-source vision solution were to be used in an industrial test-bed, it  would need to be properly integrated into a machine. Industrial vision systems have  this functionality, but it would need to be developed for the open-source derived  software. Several network and communication systems exist for machines, and  developing a middleware that would be able to turn results from a low-cost camera  into a format that could be transmitted efficiently over the majority of these networks  would be a useful development for rapid testing and deployment of these systems in  the future.", "7 Testing  The first stage of the testing was to find a suitable part on which to perform the tests  detailed in the testing plan. For this, a part with known feature sizes was required.  The parts chosen were measured earlier on the AMRC\u2019s CMM.  This is a  measurement device with a stated accuracy of 5 microns. This was the most  accurate and reliable part that could be obtained with the budget of the project. 5  microns on the scale of the part represented far less than the accuracy the cameras  could obtain based on the pixel density of the sensors and the area they would be  focusing on. Therefore, the CMM results (and the related CAD) could sensibly be  used as a reasonable baseline measurement to which the camera based  measurements could be compared. This part can be seen below (Figure 10) with  markings for the various parts that inspections would be carried out on:", "Cognex is a set of vision solutions developed by the Cognex Corporation. They  consist of a range of industrial cameras with built-in micro-processors and bespoke  software to setup the inspections these cameras are to carry out. While the hardware  is often more expensive than an equivalent camera, the advantage of this setup is  that the micro-processor on the camera can carry out the inspection. This removes  the need for more expensive and potentially unreliable computer hardware and any  attached wiring. The range of software is also simple to use and user friendly,  providing a GUI for setting up inspections and tweaking the parameters."], "summary_lines": "\u00a9AMC 2012\u00a9AMC 2013\u00a9AMC 2012\u00a9AMC 2013\nPossible urther Work\n\u2022 o optimise standardise Open-\nsource produced  for \n\u2022 of advanced features (e.g. \n\u2022 Investigating performance of other \ncommercial sensors (e.g. Kinect, eap)\n\u2022 Investigate integration with industrial \nhardware controllers"}
{"article_lines": ["There are different cameras within the Cognex range for different applications. For  this project, the AMRC decided on an Intellect Machine Vision camera, using an  Intellect 554 camera (covered below in 5.3.1 Commercial Cameras) and the Intellect  v1.5.1 software. This software uses a set of graphical tools that are dropped onto the", "5.3 Hardware Selection  Use of vision systems has been driven by the development of digital photo-electric  light sensors. These take in light, concentrated onto a small chip through a lens, and  convert it into electrons in a photo-electric cell on the chip. The chips vary in  manufacturing techniques, leading to two main sensor types; complementary metal- oxide semiconductor (CMOS) and charge-coupled device (CCD). They work in the  same general way, with CMOS taking less power and producing higher resolution  images, but with more noise and therefore lower image quality than CCD.", "Unlike the preceding two systems, the AMRC had to use OpenCV as a framework to  develop our own vision system software. This lead to a longer development time as  the GUI and underlying code had to be created by the AMRC. In addition, the AMRC  was unfamiliar with OpenCV so it had to be learnt from scratch. The AMRC found the  online documentation (at the time of the project) to be reasonably deep and  comprehensive. Examples could be found that made some of the less intuitive  elements easier to understand. However initial setup of the system on a PC  (involving changing the PC\u2019s environmental variables) was more convoluted that any  of the other systems tested. The C++ programming environment also required more  work and time to add a GUI to than the C# equivalent in AForge.NET (although a it  could be encompassed in to pre-existing GUI frameworks if available). Overall this  was the most time consuming system to setup of the 4."], "summary_lines": "\u00a9AMC 2012\u00a9AMC 2013\u00a9AMC 2012\nAdrian irst"}
{"article_lines": ["Regarding the feature criteria looked at it was assumed that, even if it was possible  to make the library perform the listed feature though programming, if it wasn\u2019t a core  components of the library to begin with it would be marked as not available. This was  in order to provide a boundary between obtained software and AMRC programmed  work.", "There are several possible branches of work that could be done to extend this  project. Firstly, both the open-source software coding and the testing procedure  developed could be used to create standards for future vision system creation within  the AMRC. These standards could consist of coding standards to ensure vision code  is easily understood and cross-compatible across developers, and testing standards  to produce a fixed set of tests and metrics that could be used to determine the  suitability of vision systems to tasks before fully integrating them.    Secondly, the testing done in this project could be move onto more complex tasks  that aren\u2019t yet commonly used in industrial applications. An example of this would be  stereo-vision, a relatively new technique that uses 2 camera feeds and software to  produce 3D inspection data on a scene or object. Many of these more complex tasks  have become possible due to the rapid development of computer processing  capability; therefore they are at a lower TRL level and might be less well developed  in industrially approved systems, making open-source based solutions more viable.    Thirdly, the testing done could be extended over more cameras and possibly other  camera-related systems. New sensors such as the Microsoft Kinect and the Leap are  not strictly vision systems, but often contain cameras in conjunction with other  sensors based on similar technology (such as a CMOS or CCD light sensor tuned to  pick up on infra-red light) to perform similar inspections. This could also tie into the  previous point, as most of these sensors are based on adding 3D inspection  functionality without the complex and time consuming code required in stereo vision  systems.    Finally, if an open-source vision solution were to be used in an industrial test-bed, it  would need to be properly integrated into a machine. Industrial vision systems have  this functionality, but it would need to be developed for the open-source derived  software. Several network and communication systems exist for machines, and  developing a middleware that would be able to turn results from a low-cost camera  into a format that could be transmitted efficiently over the majority of these networks  would be a useful development for rapid testing and deployment of these systems in  the future.", "8.1 Setup Challenge  Outside of the results listed, there were other elements of the setup and usability of  the solutions that, while without measurable metrics, are important in deciding the  viability of a vision system for use in a project. To offer some viewpoint on the  complexity of setting up the vision solutions used here, a simple record of the time  taken to setup each system was noted down, as well as  comments on the"], "summary_lines": "Systems?\n\tWhat are the existing:  limitations?\n\tPrinciples behind the project\n\teview\n\tProgramming of Open \n\tProcedure\n\tesults\n\tPossible urther Work\n\tAdrian irst"}
{"article_lines": ["As a part of an engineering doctorate program in 2016 at the Nuclear AMRC a Low Cost Remote Support system was  developed. This system allowed a Bi-directional link to allow communication between a Manufacturing Cell and Virtual  Environment. The system used a large frame to house an array of Microsoft Kinect sensors, to detect the location of  work inside the cell, and projectors to display work instructions and information on to a wing skin. The Kinect sensors  were used to show the positon of the worker in the virtual environment, which could be viewed by a person in Virtual  Reality (VR) and he could then point virtual lasers at regions of interest (ROI) on the physical wing through the  projectors. While this was a good  attempt and implementation to solve  the specific problem, it was also  limited in number of ways. The frame  for the Kinect sensors and projectors  was large and heavy meaning it was  far from a mobile solution, it was a  bespoke solution to a particular  problem, it required a calibration  process and if it was to be extended  to a different scenario it would  require an entire virtual scene to be  created for that specific problem.", "With the release of Microsoft\u2019s Augmented Reality (AR) device, the HoloLens, a Cyber-Physical presence in a scenario  was now possible. The software utilises the camera suite of the HoloLens to allow the supporter to virtually draw onto  user\u2019s view to show them what they need to do, whilst talking to the user to explain the tasks simultaneously. While  enabling the supporter to see what the user sees using the camera of the HoloLens does provide an improvement to  the support service as it contextualises the problem for the supporter, is also somewhat limited. A certain degree of  instructions between the user and the supporter is still required for the supporter to be able to see exactly what they  need to, interactions remain limited, the HoloLens is not widely available, the device is expensive at \u00a34529 for a  consumer model, and the program itself is not extendable in any way. Figure 2-1 depicts an example scenario of the  software being used. Here the supporter, in this case a father, is showing his daughter how to fit a U-bend.", "The work performed for this report was developing a remote support system for physical assets and processes that  improved upon the current offerings in industry, namely phoning a call centre or arranging a callout. Some previous  attempts were taken into consideration, Microsoft\u2019s Skype for HoloLens and previous work carried out at the AMRC as  a part of a Doctorate Engineering program. The system developed used mobile AR and VR technologies produced by  Google, Tango and Daydream respectively."], "summary_lines": "PowerPoint Presentation"}
{"article_lines": ["Remote Support is used across a wide range of different industries, for many different types of  products. These services have typically operated in a similar fashion throughout their existence,  but recent advancements in a number of fields could potentially allow these services to finally  move forward and provide more accurate help.  1.1 Current Offerings", "At time of writing, and to the best knowledge of the author, there have been two attempts at  trying to improve remote support services for physical products and processes. One is an offering  from Microsoft integrated into their HoloLens device and the latter was work carried out as a part  of an doctorate engineering program between Rolls-Royce and the Nuclear AMRC.", "There are a wide number of reasons why these services are less than ideal. Firstly, they are almost always conducted  over the phone or via an internet chat system. This is an issue because it relies on the user, whom is very rarely  knowledgeable about the product they are having problems with, being able to articulate the issue they are having  well enough so that the supporter can build a mental picture in their head of the scenario, determine likely causes and  solutions to the problems, and then feed this information back to the user for them to carry out the fixes. Secondly,  these services are often outsourced to different countries, which can potentially affect the quality of the connection,  and it means that the supporter you are speaking to will probably not have the same first language as you, which  could potentially introduce language barriers to the situation. Lastly, the item you are having trouble with could be  one of tens, hundred or maybe even thousands of items produced by that company, meaning that it is very unlikely  that the supporter will be an expert or have specialist knowledge about the particular product you are having  problems with."], "summary_lines": "Se\u00e1n Wilson and Jacob Senior"}
{"article_lines": ["With the release of Microsoft\u2019s Augmented Reality (AR) device, the HoloLens, a Cyber-Physical presence in a scenario  was now possible. The software utilises the camera suite of the HoloLens to allow the supporter to virtually draw onto  user\u2019s view to show them what they need to do, whilst talking to the user to explain the tasks simultaneously. While  enabling the supporter to see what the user sees using the camera of the HoloLens does provide an improvement to  the support service as it contextualises the problem for the supporter, is also somewhat limited. A certain degree of  instructions between the user and the supporter is still required for the supporter to be able to see exactly what they  need to, interactions remain limited, the HoloLens is not widely available, the device is expensive at \u00a34529 for a  consumer model, and the program itself is not extendable in any way. Figure 2-1 depicts an example scenario of the  software being used. Here the supporter, in this case a father, is showing his daughter how to fit a U-bend.", "ABG3016-RP-171128 Released 3 of 12  Confidential. \u00a9 2018 AMRC with Boeing.                                                                                             Template \u2013 AMRC.RP Revision 14 (June 2017)", "ABG3016-RP-171128 Released 2 of 12  Confidential. \u00a9 2018 AMRC with Boeing.                                                                                             Template \u2013 AMRC.RP Revision 14 (June 2017)"], "summary_lines": ""}
{"article_lines": ["For the Remote Support system implemented in this work, three salient qualities were identified  to ensure that it was an improvement over current attempts. It then needed to be determined  what technologies allowed these aspects to be implemented into the system, and then the system  could be produced.", "This level of presence has benefitted the computer industry by being able to provide a more efficient and a more  precise way to help end users solve their problems and make much better use of expert\u2019s time by removing the need  to travel to each client individually. Essentially, the direct control of the system alleviated most of the problems  associated with the remote aspect of the support. The main focus of this work was to investigate whether or not it is  possible to bring this presence to physical products and processes, as being able to see, walk around and interact with  the problematic product would allow for an improved support service.", "Firstly, and most importantly, this system needed to be general. It was paramount that anyone could use this system  for any product or scenario. This was one of the benefits of the HoloLens system and one of the drawbacks to the Low  Cost Remote Support solution. Secondly, it needed to be a mobile solution so that support could provide on a wide  range of physical items and scenarios, for example underneath a car, looking at an entire plane wing or in tight and  confined spaces. Lastly, the system needed to be accessible, so using devices that people are already familiar with and  not bespoke solutions to particular problems. Generally speaking, when Remote Support is needed it is rarely going to  be in a pristine laboratory or factory, it would be needed at the inconvenient times and in the inconvenient places,  which was something that was attempted to be captured in these three qualities."], "summary_lines": "Computer \n\u2022 Contextualises proms\n\u2022 Analyse the system \n\u2022 Guidance through solution\nCan this be adapted for products"}
{"article_lines": ["As a part of an engineering doctorate program in 2016 at the Nuclear AMRC a Low Cost Remote Support system was  developed. This system allowed a Bi-directional link to allow communication between a Manufacturing Cell and Virtual  Environment. The system used a large frame to house an array of Microsoft Kinect sensors, to detect the location of  work inside the cell, and projectors to display work instructions and information on to a wing skin. The Kinect sensors  were used to show the positon of the worker in the virtual environment, which could be viewed by a person in Virtual  Reality (VR) and he could then point virtual lasers at regions of interest (ROI) on the physical wing through the  projectors. While this was a good  attempt and implementation to solve  the specific problem, it was also  limited in number of ways. The frame  for the Kinect sensors and projectors  was large and heavy meaning it was  far from a mobile solution, it was a  bespoke solution to a particular  problem, it required a calibration  process and if it was to be extended  to a different scenario it would  require an entire virtual scene to be  created for that specific problem.", "With the release of Microsoft\u2019s Augmented Reality (AR) device, the HoloLens, a Cyber-Physical presence in a scenario  was now possible. The software utilises the camera suite of the HoloLens to allow the supporter to virtually draw onto  user\u2019s view to show them what they need to do, whilst talking to the user to explain the tasks simultaneously. While  enabling the supporter to see what the user sees using the camera of the HoloLens does provide an improvement to  the support service as it contextualises the problem for the supporter, is also somewhat limited. A certain degree of  instructions between the user and the supporter is still required for the supporter to be able to see exactly what they  need to, interactions remain limited, the HoloLens is not widely available, the device is expensive at \u00a34529 for a  consumer model, and the program itself is not extendable in any way. Figure 2-1 depicts an example scenario of the  software being used. Here the supporter, in this case a father, is showing his daughter how to fit a U-bend.", "This level of presence has benefitted the computer industry by being able to provide a more efficient and a more  precise way to help end users solve their problems and make much better use of expert\u2019s time by removing the need  to travel to each client individually. Essentially, the direct control of the system alleviated most of the problems  associated with the remote aspect of the support. The main focus of this work was to investigate whether or not it is  possible to bring this presence to physical products and processes, as being able to see, walk around and interact with  the problematic product would allow for an improved support service."], "summary_lines": "Cyber-Physical Presence"}
{"article_lines": ["There are a wide number of reasons why these services are less than ideal. Firstly, they are almost always conducted  over the phone or via an internet chat system. This is an issue because it relies on the user, whom is very rarely  knowledgeable about the product they are having problems with, being able to articulate the issue they are having  well enough so that the supporter can build a mental picture in their head of the scenario, determine likely causes and  solutions to the problems, and then feed this information back to the user for them to carry out the fixes. Secondly,  these services are often outsourced to different countries, which can potentially affect the quality of the connection,  and it means that the supporter you are speaking to will probably not have the same first language as you, which  could potentially introduce language barriers to the situation. Lastly, the item you are having trouble with could be  one of tens, hundred or maybe even thousands of items produced by that company, meaning that it is very unlikely  that the supporter will be an expert or have specialist knowledge about the particular product you are having  problems with.", "Most prominently it is improving the quality of the remote support services that a company could provide, which  would result in an improved customer experience with the company. It improves these services by providing a more  precise method to identify a problem, explain why it\u2019s wrong and provide solutions to it. As the system is based off  using smart phones, which the vast majority of people already know how to use, no special training is required to use  the system whereas if it was Hololens and HTC Vive based, this may be a requirement. It is far more cost effective  when compared to the original solution that was going to be implemented using the two aforementioned devices. It is  a completely mobile solution so you don\u2019t need to be in an office or a VR space to provide the support, the supporter  could be anywhere so long as they had a reliable internet connection to their device. It also means that the support  could be provided at any time too, so long as they have the device with them. This system is also completely general,  so long as there is an item to scan, support could be provided for it.", "Firstly, and most importantly, this system needed to be general. It was paramount that anyone could use this system  for any product or scenario. This was one of the benefits of the HoloLens system and one of the drawbacks to the Low  Cost Remote Support solution. Secondly, it needed to be a mobile solution so that support could provide on a wide  range of physical items and scenarios, for example underneath a car, looking at an entire plane wing or in tight and  confined spaces. Lastly, the system needed to be accessible, so using devices that people are already familiar with and  not bespoke solutions to particular problems. Generally speaking, when Remote Support is needed it is rarely going to  be in a pristine laboratory or factory, it would be needed at the inconvenient times and in the inconvenient places,  which was something that was attempted to be captured in these three qualities."], "summary_lines": "Low Cost \n\u2022 Bi-directional communication\n\u2022 Costly to extend use case\n\u2022 Not a  solution"}
{"article_lines": ["With the release of Microsoft\u2019s Augmented Reality (AR) device, the HoloLens, a Cyber-Physical presence in a scenario  was now possible. The software utilises the camera suite of the HoloLens to allow the supporter to virtually draw onto  user\u2019s view to show them what they need to do, whilst talking to the user to explain the tasks simultaneously. While  enabling the supporter to see what the user sees using the camera of the HoloLens does provide an improvement to  the support service as it contextualises the problem for the supporter, is also somewhat limited. A certain degree of  instructions between the user and the supporter is still required for the supporter to be able to see exactly what they  need to, interactions remain limited, the HoloLens is not widely available, the device is expensive at \u00a34529 for a  consumer model, and the program itself is not extendable in any way. Figure 2-1 depicts an example scenario of the  software being used. Here the supporter, in this case a father, is showing his daughter how to fit a U-bend.", "ABG3016-RP-171128 Released 3 of 12  Confidential. \u00a9 2018 AMRC with Boeing.                                                                                             Template \u2013 AMRC.RP Revision 14 (June 2017)", "ABG3016-RP-171128 Released 2 of 12  Confidential. \u00a9 2018 AMRC with Boeing.                                                                                             Template \u2013 AMRC.RP Revision 14 (June 2017)"], "summary_lines": ""}
{"article_lines": ["To solve the generalisation problem, a VR scene needed to  be dynamically generated for a contextualised system. To  be able to do this, a device needed to be used that allowed  a detailed 3D scan of the problematic physical asset to be  produced. There are a number of different commercially  available products that would allow this to be performed,  from camera and depth sensor solutions such as the  Microsoft Kinect (1) and the HoloLens (2) to stereoscopic  cameras such as the Zed Space (3). The results from the  devices ranged from unusable such as one produced by the  HoloLens, to very good produced by the Kinect Sensor,  however due to mobility issues with the Kinect needing to  be power and a PC to be connected to it had to be ruled  out. As the quality of the scan was so good, there was a  brief discussion about how to make the system mobile,  however then bespoke hardware would have been created  for the system which was something was to be specifically  avoided. These device at least proved that it is possible to  produce a detailed 3D scan of a physical asset that could  then be used in a VR scene. The main issue was the second  salient aspect, mobility.", "With so much data being produced by users and companies in today\u2019s civilization, one of the latest trends in the tech  sector is Data Visualisation. Two of the relatively new and popular methods are AR and VR, while these have been  possible on specifically made devices such as the Google Glass, Microsoft HoloLens and the HTC Vive, for a few years,  only recently has it been possible to incorporate them onto mobile devices, due to advancements in mobile CPU and  GPU speeds and power.", "The solution needed to be mobile so that it could be taken and used in any given scenario, which would not have been  possible if the user was tethered to a PC, like when the Kinect was used. The HoloLens is mobile device, however as  can be seen from Figure 3-1, which is a scan of a Caterham, the poor quality of the cameras meant that it was  unsuitable for the needs of this project. A mobile device that is familiar to almost everyone and has recently become  capable of performing the tasks required in this system is the Smart Phone."], "summary_lines": "alising \nVR scene must be dynamically generated for a contextualised"}
{"article_lines": ["With the release of Microsoft\u2019s Augmented Reality (AR) device, the HoloLens, a Cyber-Physical presence in a scenario  was now possible. The software utilises the camera suite of the HoloLens to allow the supporter to virtually draw onto  user\u2019s view to show them what they need to do, whilst talking to the user to explain the tasks simultaneously. While  enabling the supporter to see what the user sees using the camera of the HoloLens does provide an improvement to  the support service as it contextualises the problem for the supporter, is also somewhat limited. A certain degree of  instructions between the user and the supporter is still required for the supporter to be able to see exactly what they  need to, interactions remain limited, the HoloLens is not widely available, the device is expensive at \u00a34529 for a  consumer model, and the program itself is not extendable in any way. Figure 2-1 depicts an example scenario of the  software being used. Here the supporter, in this case a father, is showing his daughter how to fit a U-bend.", "With so much data being produced by users and companies in today\u2019s civilization, one of the latest trends in the tech  sector is Data Visualisation. Two of the relatively new and popular methods are AR and VR, while these have been  possible on specifically made devices such as the Google Glass, Microsoft HoloLens and the HTC Vive, for a few years,  only recently has it been possible to incorporate them onto mobile devices, due to advancements in mobile CPU and  GPU speeds and power.", "that leads up to the messages or program crashes occurring. The supporter is then able to analyse the user\u2019s  computer, look at and directly manipulate the system to attempt to solve the issue and all the while they can be  talking to the user, explaining what he\u2019s doing as he\u2019s doing it, which means that if the problem reoccurs in the future  the user would have a much better chance of solving it themselves."], "summary_lines": "alising"}
{"article_lines": ["The work performed for this report was developing a remote support system for physical assets and processes that  improved upon the current offerings in industry, namely phoning a call centre or arranging a callout. Some previous  attempts were taken into consideration, Microsoft\u2019s Skype for HoloLens and previous work carried out at the AMRC as  a part of a Doctorate Engineering program. The system developed used mobile AR and VR technologies produced by  Google, Tango and Daydream respectively.", "With so much data being produced by users and companies in today\u2019s civilization, one of the latest trends in the tech  sector is Data Visualisation. Two of the relatively new and popular methods are AR and VR, while these have been  possible on specifically made devices such as the Google Glass, Microsoft HoloLens and the HTC Vive, for a few years,  only recently has it been possible to incorporate them onto mobile devices, due to advancements in mobile CPU and  GPU speeds and power.", "Since their inception in the early 1980s, mobile phones manufacturers have been driven by consumer demand for the  latest and greatest handsets. This has forced the manufacturers to keep innovating and pushing the boundaries of  mobile technology, whether they are pushing for the highest resolution screen, fastest chipset, most powerful  graphical capabilities or unique features that make the devices stand out from the crowd. These devices have also  become sensor rich with a plethora of different sensors integrated, meaning that a wealth of information is readily  available to be accessed by developers on these systems."], "summary_lines": "Mobility and Accessibility\n\u2022   21 hours talk time\n\u2022 Audio and Video calls\n\u2022 Front and Rear HD Cameras\n\u2022 2.7 million pixel display\n\u2022 Inertial Measurement Unit\n\u2022 30 minutes talk time"}
{"article_lines": ["The solution needed to be mobile so that it could be taken and used in any given scenario, which would not have been  possible if the user was tethered to a PC, like when the Kinect was used. The HoloLens is mobile device, however as  can be seen from Figure 3-1, which is a scan of a Caterham, the poor quality of the cameras meant that it was  unsuitable for the needs of this project. A mobile device that is familiar to almost everyone and has recently become  capable of performing the tasks required in this system is the Smart Phone.", "With so much data being produced by users and companies in today\u2019s civilization, one of the latest trends in the tech  sector is Data Visualisation. Two of the relatively new and popular methods are AR and VR, while these have been  possible on specifically made devices such as the Google Glass, Microsoft HoloLens and the HTC Vive, for a few years,  only recently has it been possible to incorporate them onto mobile devices, due to advancements in mobile CPU and  GPU speeds and power.", "The scanning of objects was occasionally difficult, particularly on matte black objects like the tyre tread, the Asus  Zenfone AR does have a specialized tri camera system, however if the comments by Apple and Google (10) about AR  are taken to be as serious as they say, these types of camera systems will be essential to improving the quality of their  AR platforms in the future. Daydream is also only available on Android devices however if Apple plan to support VR on  their phones in the future then cross platform operation should be feasible."], "summary_lines": "Mobile Augmented Reality (AR)\nhappen, it will happen \nin a big way, and we \nwill wonder when it \nlived without it. Like \nphone today.\u201d \u2013 Tim"}
{"article_lines": ["The solution needed to be mobile so that it could be taken and used in any given scenario, which would not have been  possible if the user was tethered to a PC, like when the Kinect was used. The HoloLens is mobile device, however as  can be seen from Figure 3-1, which is a scan of a Caterham, the poor quality of the cameras meant that it was  unsuitable for the needs of this project. A mobile device that is familiar to almost everyone and has recently become  capable of performing the tasks required in this system is the Smart Phone.", "As a part of an engineering doctorate program in 2016 at the Nuclear AMRC a Low Cost Remote Support system was  developed. This system allowed a Bi-directional link to allow communication between a Manufacturing Cell and Virtual  Environment. The system used a large frame to house an array of Microsoft Kinect sensors, to detect the location of  work inside the cell, and projectors to display work instructions and information on to a wing skin. The Kinect sensors  were used to show the positon of the worker in the virtual environment, which could be viewed by a person in Virtual  Reality (VR) and he could then point virtual lasers at regions of interest (ROI) on the physical wing through the  projectors. While this was a good  attempt and implementation to solve  the specific problem, it was also  limited in number of ways. The frame  for the Kinect sensors and projectors  was large and heavy meaning it was  far from a mobile solution, it was a  bespoke solution to a particular  problem, it required a calibration  process and if it was to be extended  to a different scenario it would  require an entire virtual scene to be  created for that specific problem.", "The work performed for this report was developing a remote support system for physical assets and processes that  improved upon the current offerings in industry, namely phoning a call centre or arranging a callout. Some previous  attempts were taken into consideration, Microsoft\u2019s Skype for HoloLens and previous work carried out at the AMRC as  a part of a Doctorate Engineering program. The system developed used mobile AR and VR technologies produced by  Google, Tango and Daydream respectively."], "summary_lines": "(software)"}
{"article_lines": ["Virtual Reality is also a focus for some mobile phone companies like Google and Samsung, producing the Cardboard  and Gear VR systems. While this is taking a very simplistic approach to VR, it\u2019s also much more accessible than more  sophisticated fully fledged VR systems like the Oculus Rift (6) and the HTC Vive (7), with the aim of putting VR into the  hands of everybody. The \u2018top-end\u2019 of the mobile VR market is the Daydream (6) system also produced by Google. This  is an updated and refined version of their earlier Cardboard device. It has a 6 DOF controller for interacting with and  moving around the virtual scene. By simply inserting a mobile phone into the headset, the Daydream software is  launched and this allows the user to be in a VR environment. Using the phones Inertial Measurement Unit, orientation  tracking is possible. Phones that can be used in for Daydream applications must meet certain performance  requirements to ensure the apps will run on the phone, and this standardisation across hardware and software is  being driven by Google themselves. It\u2019s also small, transportable and relatively low cost, satisfying the second and  third salient qualities discussed above.", "With so much data being produced by users and companies in today\u2019s civilization, one of the latest trends in the tech  sector is Data Visualisation. Two of the relatively new and popular methods are AR and VR, while these have been  possible on specifically made devices such as the Google Glass, Microsoft HoloLens and the HTC Vive, for a few years,  only recently has it been possible to incorporate them onto mobile devices, due to advancements in mobile CPU and  GPU speeds and power.", "With the release of Microsoft\u2019s Augmented Reality (AR) device, the HoloLens, a Cyber-Physical presence in a scenario  was now possible. The software utilises the camera suite of the HoloLens to allow the supporter to virtually draw onto  user\u2019s view to show them what they need to do, whilst talking to the user to explain the tasks simultaneously. While  enabling the supporter to see what the user sees using the camera of the HoloLens does provide an improvement to  the support service as it contextualises the problem for the supporter, is also somewhat limited. A certain degree of  instructions between the user and the supporter is still required for the supporter to be able to see exactly what they  need to, interactions remain limited, the HoloLens is not widely available, the device is expensive at \u00a34529 for a  consumer model, and the program itself is not extendable in any way. Figure 2-1 depicts an example scenario of the  software being used. Here the supporter, in this case a father, is showing his daughter how to fit a U-bend."], "summary_lines": "Mobile Virtual Reality (VR)\napplications for VR \nas you can think of,"}
{"article_lines": ["AR has, over the past year or two, become a major focus for the big hitters in the Mobile Phone sector. Both Apple  and Google who together have over 99% of the mobile OS market share (1), have made firm statements about how  they view the importance of Augmented Reality in the near future, producing platforms such as ARkit and ARCore  respectively. These software development kits are enabling developers to produce AR based apps for their devices.  Google have also been developing a much more powerful AR software development kit called Tango (5). Due to its  sophistication it does need a certain suite of sensors, specifically a Tri-Camera system that has a standard camera, one  dedicated to motion tracking and a depth sensor. This enables the phone to perform fairly sophisticated and advanced  AR techniques such as Motion Tracking and Area Learning.", "Since their inception in the early 1980s, mobile phones manufacturers have been driven by consumer demand for the  latest and greatest handsets. This has forced the manufacturers to keep innovating and pushing the boundaries of  mobile technology, whether they are pushing for the highest resolution screen, fastest chipset, most powerful  graphical capabilities or unique features that make the devices stand out from the crowd. These devices have also  become sensor rich with a plethora of different sensors integrated, meaning that a wealth of information is readily  available to be accessed by developers on these systems.", "Everybody has been there at one point or another, some kind of purchased product, for whatever reason, no longer  works, doesn\u2019t go together properly or is just simply broken. The instruction manual has been consulted, which was  no use, Google returned no useful information and all troubleshooting methods have been exhausted to no avail;  there remains one option: Calling support."], "summary_lines": "Google"}
{"article_lines": ["Virtual Reality is also a focus for some mobile phone companies like Google and Samsung, producing the Cardboard  and Gear VR systems. While this is taking a very simplistic approach to VR, it\u2019s also much more accessible than more  sophisticated fully fledged VR systems like the Oculus Rift (6) and the HTC Vive (7), with the aim of putting VR into the  hands of everybody. The \u2018top-end\u2019 of the mobile VR market is the Daydream (6) system also produced by Google. This  is an updated and refined version of their earlier Cardboard device. It has a 6 DOF controller for interacting with and  moving around the virtual scene. By simply inserting a mobile phone into the headset, the Daydream software is  launched and this allows the user to be in a VR environment. Using the phones Inertial Measurement Unit, orientation  tracking is possible. Phones that can be used in for Daydream applications must meet certain performance  requirements to ensure the apps will run on the phone, and this standardisation across hardware and software is  being driven by Google themselves. It\u2019s also small, transportable and relatively low cost, satisfying the second and  third salient qualities discussed above.", "With so much data being produced by users and companies in today\u2019s civilization, one of the latest trends in the tech  sector is Data Visualisation. Two of the relatively new and popular methods are AR and VR, while these have been  possible on specifically made devices such as the Google Glass, Microsoft HoloLens and the HTC Vive, for a few years,  only recently has it been possible to incorporate them onto mobile devices, due to advancements in mobile CPU and  GPU speeds and power.", "The solution needed to be mobile so that it could be taken and used in any given scenario, which would not have been  possible if the user was tethered to a PC, like when the Kinect was used. The HoloLens is mobile device, however as  can be seen from Figure 3-1, which is a scan of a Caterham, the poor quality of the cameras meant that it was  unsuitable for the needs of this project. A mobile device that is familiar to almost everyone and has recently become  capable of performing the tasks required in this system is the Smart Phone."], "summary_lines": "Mobile \nAugmented Reality Virtual Reality\n\u2022 Adding \n\u2022 Voice communication"}
{"article_lines": ["With the release of Microsoft\u2019s Augmented Reality (AR) device, the HoloLens, a Cyber-Physical presence in a scenario  was now possible. The software utilises the camera suite of the HoloLens to allow the supporter to virtually draw onto  user\u2019s view to show them what they need to do, whilst talking to the user to explain the tasks simultaneously. While  enabling the supporter to see what the user sees using the camera of the HoloLens does provide an improvement to  the support service as it contextualises the problem for the supporter, is also somewhat limited. A certain degree of  instructions between the user and the supporter is still required for the supporter to be able to see exactly what they  need to, interactions remain limited, the HoloLens is not widely available, the device is expensive at \u00a34529 for a  consumer model, and the program itself is not extendable in any way. Figure 2-1 depicts an example scenario of the  software being used. Here the supporter, in this case a father, is showing his daughter how to fit a U-bend.", "ABG3016-RP-171128 Released 3 of 12  Confidential. \u00a9 2018 AMRC with Boeing.                                                                                             Template \u2013 AMRC.RP Revision 14 (June 2017)", "ABG3016-RP-171128 Released 2 of 12  Confidential. \u00a9 2018 AMRC with Boeing.                                                                                             Template \u2013 AMRC.RP Revision 14 (June 2017)"], "summary_lines": ""}
{"article_lines": ["Most prominently it is improving the quality of the remote support services that a company could provide, which  would result in an improved customer experience with the company. It improves these services by providing a more  precise method to identify a problem, explain why it\u2019s wrong and provide solutions to it. As the system is based off  using smart phones, which the vast majority of people already know how to use, no special training is required to use  the system whereas if it was Hololens and HTC Vive based, this may be a requirement. It is far more cost effective  when compared to the original solution that was going to be implemented using the two aforementioned devices. It is  a completely mobile solution so you don\u2019t need to be in an office or a VR space to provide the support, the supporter  could be anywhere so long as they had a reliable internet connection to their device. It also means that the support  could be provided at any time too, so long as they have the device with them. This system is also completely general,  so long as there is an item to scan, support could be provided for it.", "The scanning of objects was occasionally difficult, particularly on matte black objects like the tyre tread, the Asus  Zenfone AR does have a specialized tri camera system, however if the comments by Apple and Google (10) about AR  are taken to be as serious as they say, these types of camera systems will be essential to improving the quality of their  AR platforms in the future. Daydream is also only available on Android devices however if Apple plan to support VR on  their phones in the future then cross platform operation should be feasible.", "There are a wide number of reasons why these services are less than ideal. Firstly, they are almost always conducted  over the phone or via an internet chat system. This is an issue because it relies on the user, whom is very rarely  knowledgeable about the product they are having problems with, being able to articulate the issue they are having  well enough so that the supporter can build a mental picture in their head of the scenario, determine likely causes and  solutions to the problems, and then feed this information back to the user for them to carry out the fixes. Secondly,  these services are often outsourced to different countries, which can potentially affect the quality of the connection,  and it means that the supporter you are speaking to will probably not have the same first language as you, which  could potentially introduce language barriers to the situation. Lastly, the item you are having trouble with could be  one of tens, hundred or maybe even thousands of items produced by that company, meaning that it is very unlikely  that the supporter will be an expert or have specialist knowledge about the particular product you are having  problems with."], "summary_lines": "and \n\u2022 Improves customer services\n\u2022 Completely \n\u2022 Technologies central to Industry 4.0\n\u2022 Scan occasionally struggles\n\u2022  requires specialised camera system\n\u2022  only on Android"}
{"article_lines": ["Firstly, voice is currently transmitted through the standard cellular network capabilities of the mobile device. Voice  calling can be integrated into the application using voice over IP (VOIP), providing greater control, allowing multiple  participants, and removing the reliance on a cellular network. A log in system would also be able to record who the  supporter and supportee were, the time of the support session and the issues raised and fixed during the session  which could be recorded into a database for future reference and potentially data tracking and analysis of the system  for further improvements. Additional attachments could be added to each marker, for example documentation  regarding the problematic part of the product. 3D Geometry of different items could be imported to show how the  product should look. Freeform drawing could be implemented so either user could draw directly on the virtual object.  Reports could automatically be generated based on the session between the user and the supporter. Multiple  Collaboration could be included so many people could enter the VR scene to collaborate and come up with solutions  to the problem. Lastly, with Offline Capabilities could be used so neither party has to wait until the other is free for  support to be given.", "The work performed for this report was developing a remote support system for physical assets and processes that  improved upon the current offerings in industry, namely phoning a call centre or arranging a callout. Some previous  attempts were taken into consideration, Microsoft\u2019s Skype for HoloLens and previous work carried out at the AMRC as  a part of a Doctorate Engineering program. The system developed used mobile AR and VR technologies produced by  Google, Tango and Daydream respectively.", "Remote Support is used across a wide range of different industries, for many different types of  products. These services have typically operated in a similar fashion throughout their existence,  but recent advancements in a number of fields could potentially allow these services to finally  move forward and provide more accurate help.  1.1 Current Offerings"], "summary_lines": "\u2022 Additional attachments\n\u2022 Import 3D Geometry\n\u2022 Multiple collaboration\n\u2022 Offline capabilities\nJacob Senior: j.senior@amrc.co.uk\nSe\u00e1n Wilson: s.wilson@amrc.co.uk"}
{"article_lines": ["With so much data being produced by users and companies in today\u2019s civilization, one of the latest trends in the tech  sector is Data Visualisation. Two of the relatively new and popular methods are AR and VR, while these have been  possible on specifically made devices such as the Google Glass, Microsoft HoloLens and the HTC Vive, for a few years,  only recently has it been possible to incorporate them onto mobile devices, due to advancements in mobile CPU and  GPU speeds and power.", "The solution needed to be mobile so that it could be taken and used in any given scenario, which would not have been  possible if the user was tethered to a PC, like when the Kinect was used. The HoloLens is mobile device, however as  can be seen from Figure 3-1, which is a scan of a Caterham, the poor quality of the cameras meant that it was  unsuitable for the needs of this project. A mobile device that is familiar to almost everyone and has recently become  capable of performing the tasks required in this system is the Smart Phone.", "For the final salient quality, Accessibility, was important because of the need to keep the system useable to as many  people as possible without needing special training, and it also needed to be not a bespoke solution or piece of  equipment for a particular problem. With the sheer versatility demonstrated by smart phone app stores, it was clear  that Accessibility was also encapsulated in mobile phones. As they are so prevalent in society, the vast majority of  people already know how to operate one and use Apps for them, so a mobile phone became the ideal platform to  develop the Remote Support system on."], "summary_lines": "Cyber-Physical Presence\n\talising \n\talising \n\tMobility and Accessibility\n\tMobile Augmented Reality (AR)\n\tMobile Virtual Reality (VR)\n\tMobile \n\t and"}
{"article_lines": ["The information in this document is the property of AI Project Foreground IP Ownersthe AMRC subject to the terms of the  AROPCQA \u2013 Augmented Reality for Operative Productivity & Continuous Quality Analysis, grant offer letter 105096", "5 Additional image processing \t6 Hardware and software integration \t7 Demonstrators results and observations: \t7.1 Hardware and software configuration (EIP) \t7.2 Hardware and software configuration (Factory 2050) \t7.3 Results \t7.4 Discussion \t7.5 Demonstrator on Thames Tideway Blackfriars panel", "Opportunities to improve the accuracy and confidence of the AI models, by training on data from the production environment and using a different camera and camera mount hasve been discussed.  and also Aadditional features that can be added to the inspection tool to improve productivity and quality, such as using a tablet device to capture images not visible to the overhead camera, automated shape checking of the panel hasve also been discussed.  has also been to improve productivity have identified and discussed."], "summary_lines": "Innovate UK \u2013 AROPCQA   Augmented Reality for Operative Productivity and Continuous Quality Analysis    Session Title  Speaker names"}
{"article_lines": ["The information in this document is the property of AI Project Foreground IP Ownersthe AMRC subject to the terms of the  AROPCQA \u2013 Augmented Reality for Operative Productivity & Continuous Quality Analysis, grant offer letter 105096", "An inspection tool based on artificial intelligence (AI) has been developed to demonstrate the improvement in efficiency that can be achieved in the quality assurance process for inspecting precast panels. With data extracted from the Industry Foundation Classes (IFC) files and an image of the panel captured by an overhead camera, the demonstrated web inspection tool wascan becan be used to confirm the presence of the precast embeds and also verify that they are  in the correct locationwithin the acceptable tolerance\u00a0level.", "This report describes the inspection tool the web application developed by the AMRC in order to digitise and improve the efficiency of the current quality assurance process at EIPimprove the efficiency in the quality assurance process at EIP."], "summary_lines": "Innovate UK \u2013 AROPCQA\nAugmented Reality for Operative Productivity and Continuous Quality Analysis"}
{"article_lines": ["5 Additional image processing \t6 Hardware and software integration \t7 Demonstrators results and observations: \t7.1 Hardware and software configuration (EIP) \t7.2 Hardware and software configuration (Factory 2050) \t7.3 Results \t7.4 Discussion \t7.5 Demonstrator on Thames Tideway Blackfriars panel", ".  The  aim  of  this  report  is  to  provide  the  reader  with  an  understanding  of  the  various technologies (hardware and software) used to develop the inspection tool, and also provide details  about  the  different  features  of  the  inspection  tool.  The  results  from  the  two demonstrators, at AMRC Factory 2050 and EIP, haves also been discussed and further work to improve the inspection tool haves been stated.", "Figure 1-1 EIP operator capturing images of precast embeds to verify that the correct number of embeds are present on a given panel. Existing processes for quality assurance within offsite manufacture are \u2026 and time consuming, prone to errors etc. This report describes the web application developed by the AMRC in order to improve the  efficiency in the quality assurance process at EIP. The aim of this report is to provide the reader with an understanding of the various  technologies (hardware and software) used to develop the inspection tool, and also provide  details about the different features of the web application. The demonstrator results has also  been discussed and further work to improve the tool has been stated.  Section 3 describes in detail the development of the backbone of the web based inspection  tool, which is an AI method called deep learning.  Section 4 includes details about the various features of the web application and why they are  required in order to make the quality assurance and inspection process more efficient.  Section 5 is about the additional image processing required in order for checking whether the  precast embeds are within tolerance, and also to distinguish between similar looking items. Section 6 describes how the hardware and the software connect and integrate with each other.  Section 7 reports on the findings of the two demonstrations performed at AMRC Factory 2050  and Explore Industrial Park, ways to improve on them have been discussed.  Section 8 is about the future work that can be done to make the inspection tool better and  further its capabilities. It also includes ways to simplify some of the processes."], "summary_lines": "1. Current quality assurance methods\n2. Solution overview\n3. AI \u2013 Deep learning\n4. Development of deep learning s \n5. Features of the web inspection tool\n6. Demonstrator at AMRC Factory 2050\n7. Demonstrator at Explore Industrial Park \n8. Results and discussion\nPresentation content"}
{"article_lines": ["Executive summary Current quality assurance methods for inspecting panels at EIP is a multi-stage process, which involves  operators  capturing  images  of  every  single  precast  embed with  a  mobile  device, before and after the rebar cage is placed in the mould, and senior members of the quality team validating it later on. This is a very manual process which is time consuming and prone to errors,  and therefore presents an opportunity to reduce costs and increase product  quality through automation..", "Currently, a multi-stage quality assurance process is in place at EIP to verify and validate the presence of  the precast  embeds,  particularly  the anchor  pins.  The current  process can be summarised as follow:", "Figure 1-1 EIP operator capturing images of precast embeds to verify that the correct number of embeds are present on a given panel. Existing processes for quality assurance within offsite manufacture are \u2026 and time consuming, prone to errors etc. This report describes the web application developed by the AMRC in order to improve the  efficiency in the quality assurance process at EIP. The aim of this report is to provide the reader with an understanding of the various  technologies (hardware and software) used to develop the inspection tool, and also provide  details about the different features of the web application. The demonstrator results has also  been discussed and further work to improve the tool has been stated.  Section 3 describes in detail the development of the backbone of the web based inspection  tool, which is an AI method called deep learning.  Section 4 includes details about the various features of the web application and why they are  required in order to make the quality assurance and inspection process more efficient.  Section 5 is about the additional image processing required in order for checking whether the  precast embeds are within tolerance, and also to distinguish between similar looking items. Section 6 describes how the hardware and the software connect and integrate with each other.  Section 7 reports on the findings of the two demonstrations performed at AMRC Factory 2050  and Explore Industrial Park, ways to improve on them have been discussed.  Section 8 is about the future work that can be done to make the inspection tool better and  further its capabilities. It also includes ways to simplify some of the processes."], "summary_lines": "\uf0a7 Operators  need to confirm that \nall the precast  as per the design file are \non the panel before it is casted. \n\uf0a7 Current quality assurance method used at Explore \nIndustrial Park is a multi stage process that \ninvolves operators capturing  of every \nsingle precast embed with a mobile device before \nand after the rebar cage is put into the mould \n\uf0a7 This process is manual, time consuming and \nCurrent quality assurance method"}
{"article_lines": ["Currently, a multi-stage quality assurance process is in place at EIP to verify and validate the presence of  the precast  embeds,  particularly  the anchor  pins.  The current  process can be summarised as follow:", "Executive summary Current quality assurance methods for inspecting panels at EIP is a multi-stage process, which involves  operators  capturing  images  of  every  single  precast  embed with  a  mobile  device, before and after the rebar cage is placed in the mould, and senior members of the quality team validating it later on. This is a very manual process which is time consuming and prone to errors,  and therefore presents an opportunity to reduce costs and increase product  quality through automation..", "This report describes the inspection tool the web application developed by the AMRC in order to digitise and improve the efficiency of the current quality assurance process at EIPimprove the efficiency in the quality assurance process at EIP."], "summary_lines": "Current quality assurance method"}
{"article_lines": ["Two different image processing routines, which vary slightly, are used to distinguish the two sets of similar looking embeds. The From a high-level view, the image processing routine to distinguish between them involves converting the area enclosed by the bounding box into an HSV image,  finding  the  distinctive  feature  colour  of  the  precast  embed,  applying  multiple thresholds  and  computing  the  area  of  the  region  of  interest.  Once  the  area  has  been calculated, it can be compared against a set value to know its true type. A method which is less prone to errors and better has been discussed in Section 78.", "Using the combined output from two, the count and location of the precast embeds can be verified and validated. Section 3 describes in detail the development of the backbone of the web based inspection tool, which is an AI method called deep learning.", "These items vary slightly in size, and the AI models do not perform well in differentiating them. Hence, they are treated as a single class by the model and post processing is applied when they are detected in order to distinguish between them."], "summary_lines": "Compare the two using \nAI and report back on"}
{"article_lines": ["Deep  learning  is  a  subset  of  artificial  intelligence  and  is  inspired  by  our  brain\u2019s  own interconnected network of neurons. In image based deep learning, tThe earlier layers learn simple features such as edges, whereas the deeper layers learn more complex features about the  object.  It  is  widely  used  in  a  variety  of  applications,  from  self-driving  cars,  image classification to voice recognition.", "Deep learning is preferred over traditional computer vision because it takes away the time consuming  part  of  defining  the  features  and  doing  feature  engineering,  and  it  learns  the significant visual features of an object during the training process..  Deep learning also helps generalise  better  and  deal  with  a  range  of  different  conditions  such  as  different  lighting, orientation, colour etc.", "The performance of deep learning models increases with more  training data, and thisa limited dataset can be doneartificially expanded by performing data augmentation. Data augmentation is  a  technique  by  which  the  existing  data  can be  used to  create  new data  with  different transformations such as horizontal and vertical flipping, adding blur and noise to the image, and a change of colour. This Data augmentation helps generalise the model even better and makes it adaptable to changes in lighting, noise, rotation etc. Data augmentation is performed on both the the training set and test to gather more data."], "summary_lines": "\uf0a7 Deep learning is a sub of AI \n\uf0a7 Inspired by the human brain\u2019s network of neurons\n\uf0a7 Recently gained widespread usage due to the huge amount of data \nand  power available \n\uf0a7 Currently used in day to day life from voice assistant such as Alexa, \nSiri to self driving cars\n\uf0a7 Advantage over classical computer vision:\n\u2013 No future engineering required\n\u2013 Network learns the important features (shape, size, colour) of an object \n\u2013 Network learns to generalise in different conditions, orientation etc. \n\uf0a7 The problem in this case is an object detection problem\n\uf0a7 Object detection s draws a bounding box around the object and \nArtificial intelligence - Deep learning"}
{"article_lines": ["The camera  is  connected to  the  ceiling  on a Unistrut  frame using  a  tripod  head.  It  wasis connected to the pod PC by a Gigabit  Ethernet  (GigE)  cable.  A  tablet  computerdevice wasis then connected to the PC via a local hotspot, which allows theed inspection tool to be accessed using the tablet\u2019s web browser.", "With  the  current  method  in  place,  not  only  are  the  shop  floor  operators  spending  hours collecting the evidence, but it also takes the senior members of the quality team another hour to go through all the evidence collected and then validate it.", "2. Using  the  embedment\u2019s  expected  position  from  the  CAD,  the  search  area  for  a particular  embed  can  be  narrowed  down  to  allow  for  the  AI  models  to  look  for  a particular item in a smaller search space. By doing this, the AI model can report back to the user the  confidence score  of  the  expected item type in  that  particular  location rather than the precast embed with the highest score. This is likely to improve the models performance."], "summary_lines": ""}
{"article_lines": ["Detecting the precast embeds can be classed as a supervised learning problem. Supervised learning  methods  are  designed  to  learn  by  example,  where  the  labels  (ground  truth)  are available for all training and testing samples. In this case,, where the ground truths (position and the type of embed) are available along with the inputs (image of the panel)..", "The algorithm of choice was YOLOv4, which is a state of the art model for object detection. The models were trained using a pre-trained model on the Common Objects in Context (COCO) dataset. This method of using a pre-trained model as a starting point for a model on a second task is called transfer learning. Transfer learning speeds up the training process and also helps the model learn better in the new domain.", "Note that the training and test set for the other precast embeds models is larger compared to the anchor pins dataset as there are 6 different types of precast embeds. For the anchor pin model,  the  training  set  was around 84% of  the  total  labelled data,  whereas for  the  other precast embed model, the training set was around 89% of the total labelled data. It is generally recommended to have at least 80% of the labelled data in the training set, as this helps the model learn and generalise better."], "summary_lines": "\uf0a7 Training is the process of the  learning to map the inputs (image of \nthe panel with precast )  to the output (localising the object and \n\uf0a7 YOLO (You Only Look Once) v4 used to train the , which achieves the \noptimal speed and accuracy needed to perform automated inspection \n\uf0a7 Transfer learning on s pre-trained on COCO (Common Object in \nContext) data used. \n\uf0a7 Two different object detection s: anchor pins and other precast \nTraining and testing the \nEmbed type Training"}
{"article_lines": ["Based  on  this  evaluation  metric,  the  model  is  tuned  by  the  process  of  hyperparameters optimisation. Hyperparameters are values which are used to control the learning process, and that which affects the weights and biases in a neural network. Some of the hyperparameters that were tuned to improve the performance were:", "1. Learning rate: This hyper parameter controls the step size of how much the parameters (weights  and biases)  are being adjusted during the learning process.  It  has  a value between 0 and 1. Choosing a very low learning rate slows down the training process, because the updates to the parameters are very small. On the other hand, choosing a number very close to 1 can result in an unstable training process.", "2. Batch size: This hyper  parameter controls the number of training samples that will be propagated through the network before the model\u2019s weights and biases are updated. Having a very small batch size is computationally expensive and will result in a longer training time, whereas having a very large batch size results in significant degradation of the model\u2019s ability to generalise."], "summary_lines": "\uf0a7 Model hyperparameters tuned until desired accuracy is achieved. \n\u2013 Hyperparameters are properties that govern the training process\n\uf0a7 Hyper parameters tuned: Learning rate and batch size \n\uf0a7 Mean average precision metric used to evaluate the s \n\u2013 Compares the ground-truth bounding box ( by the user) to the detected box \n(predicted by the ) and returns a score. \nTraining and testing the  \nLearning rate vs loss"}
{"article_lines": ["Another equally important step is the processing of the IFC files. The IFC file of a particular panel contains data such as the CAD of the panel, along with the position and count of the different precast embeds. This data needs to be processed and extracted in order to be used in the inspection tool.", "Once the user logs in, a list of all the available IFC files is presented. This list shows the IFC files that has already been processed and are ready to be used for the inspection routine. A CAD image of the selected panel is then also shown to the user upon selecting a particular IFC file.", "Once the inspection routine has been completed, a PDF report is generated upon request. This includes the date and time of the report, CAD image of the panel, inference images of both the anchor pins and the other precast embeds. It also includes the BOM list, which displays the name, the CAD count and the actual count of the precast embeds. This report is then saved tonto  a  folder,  from where  it  is  then  directly  uploaded  to  Trimble  Connect  using  Trimble Connect Sync. An example of the PDF report is in the Appendix IVII"], "summary_lines": "\uf0a7 Industry Foundation Class (IFC) file contains data such as the CAD of the panel, \nalong with the position and position of the different precast \n\uf0a7 needs to be extracted in order to use the data for embed count and \nposition verification \n\uf0a7 Python script developed automatically extracts relevant  extracted \nfrom the IFC, once a new file is available. \nProcessing of files\nSimplified CAD image \nCAD Image extracted Array with"}
{"article_lines": ["2 BackgroundPrimary software-based tools This section describes in detail the two primary tools on which the inspection tool is based upon: deep learning and processing Industry Foundation Classes (IFC) files of the panels.", "The frontend of the application is developed using HTML, JavaScript and CSS. Once the Python program is running on the PC, the inspection tool can be accessed from a web browser by any device connected to the PC.  A sequence diagram of the entire inspection process is listed available in Appendix I. A full list of all the dependencies and libraries required is in Appendix III", "Figure 1-1 EIP operator capturing images of precast embeds to verify that the correct number of embeds are present on a given panel. Existing processes for quality assurance within offsite manufacture are \u2026 and time consuming, prone to errors etc. This report describes the web application developed by the AMRC in order to improve the  efficiency in the quality assurance process at EIP. The aim of this report is to provide the reader with an understanding of the various  technologies (hardware and software) used to develop the inspection tool, and also provide  details about the different features of the web application. The demonstrator results has also  been discussed and further work to improve the tool has been stated.  Section 3 describes in detail the development of the backbone of the web based inspection  tool, which is an AI method called deep learning.  Section 4 includes details about the various features of the web application and why they are  required in order to make the quality assurance and inspection process more efficient.  Section 5 is about the additional image processing required in order for checking whether the  precast embeds are within tolerance, and also to distinguish between similar looking items. Section 6 describes how the hardware and the software connect and integrate with each other.  Section 7 reports on the findings of the two demonstrations performed at AMRC Factory 2050  and Explore Industrial Park, ways to improve on them have been discussed.  Section 8 is about the future work that can be done to make the inspection tool better and  further its capabilities. It also includes ways to simplify some of the processes."], "summary_lines": "Inspection web application overview\nOperator using tablet \nPC at the pod used for \ncapture panel \nfrom"}
{"article_lines": ["The frontend of the application is developed using HTML, JavaScript and CSS. Once the Python program is running on the PC, the inspection tool can be accessed from a web browser by any device connected to the PC.  A sequence diagram of the entire inspection process is listed available in Appendix I. A full list of all the dependencies and libraries required is in Appendix III", "This report describes the inspection tool the web application developed by the AMRC in order to digitise and improve the efficiency of the current quality assurance process at EIPimprove the efficiency in the quality assurance process at EIP.", "An inspection tool based on artificial intelligence (AI) has been developed to demonstrate the improvement in efficiency that can be achieved in the quality assurance process for inspecting precast panels. With data extracted from the Industry Foundation Classes (IFC) files and an image of the panel captured by an overhead camera, the demonstrated web inspection tool wascan becan be used to confirm the presence of the precast embeds and also verify that they are  in the correct locationwithin the acceptable tolerance\u00a0level."], "summary_lines": "Web application features: Select panel to inspect"}
{"article_lines": ["Since the panels can vary widely in size and the camera field of view can include areas other than the panel itself, it is important to focus only on the panel and remove the surrounding areas from the image.  This decreases the chances of the AI models detecting similar looking items outside the panel, and thus reduces false positives. Cropping the image also allows for faster processing and inference, as there are fewer pixels to process compared to the whole field of view.", "Using the CAD overlay tool, a 2D depiction of the panel is overlaid on top of the image, thus helping the operator in making sure that  the orientation matches,  this  is  a critical  step to match the coordinate systems in the reference image and IFC file. The operator can rotate the image until the orientation matches.", "An image processing routine was developed using Python, which uses thresholding has been developed using Python to automatically identify the boundary of the panel and crop it. In the case where this is not accurate, the operator also has the ability to manually crop the image by drawing a box around it, if needed."], "summary_lines": "\uf0a7 Ability to manually or automatically crop the panel from the frame \nWeb application features: Cropping, rotating image and CAD overlay"}
{"article_lines": ["Using the AI models developed as discussed in Section 3, the operator can choose whether to detect cast in items first or embeds first. With the current version of the web application, both anchor pins detection and other precast embeds detections need to be done in order to move onto the next step.", "5 Additional image processing  Once the precast embeds have been identified using the trained models, it is important to find the centres of the precast embeds in order to verify whether they are in their correct position and within the tolerance. The centre of the bounding boxes are not a good indicator of their actual  centres,  as  the  bounding boxes are often not  very tight  and often includes regions outside the boundary of the precast embed. Hence, image processing routines were developed to find the true centre of the precast embeds.", "Digitising  this  process  and  detecting  the  precast  embeds  using  AI  will  help  automate  the inspection process by verifying whether the correct number of the precast embeds are present on the panel, and also whether they are in the correct position."], "summary_lines": "Web application features: Detecting precast"}
{"article_lines": ["The BOM list displays all the precast embeds from the IFC file, along with their count in the IFC and that detected by the AI models. The count in the BOM list changes if the user manually adds or deletes items as discussed in Section 4.7", "Figure 1-1 EIP operator capturing images of precast embeds to verify that the correct number of embeds are present on a given panel. Existing processes for quality assurance within offsite manufacture are \u2026 and time consuming, prone to errors etc. This report describes the web application developed by the AMRC in order to improve the  efficiency in the quality assurance process at EIP. The aim of this report is to provide the reader with an understanding of the various  technologies (hardware and software) used to develop the inspection tool, and also provide  details about the different features of the web application. The demonstrator results has also  been discussed and further work to improve the tool has been stated.  Section 3 describes in detail the development of the backbone of the web based inspection  tool, which is an AI method called deep learning.  Section 4 includes details about the various features of the web application and why they are  required in order to make the quality assurance and inspection process more efficient.  Section 5 is about the additional image processing required in order for checking whether the  precast embeds are within tolerance, and also to distinguish between similar looking items. Section 6 describes how the hardware and the software connect and integrate with each other.  Section 7 reports on the findings of the two demonstrations performed at AMRC Factory 2050  and Explore Industrial Park, ways to improve on them have been discussed.  Section 8 is about the future work that can be done to make the inspection tool better and  further its capabilities. It also includes ways to simplify some of the processes.", "Figure 3-23-3 Labelling the different precast embeds. The dataset is imported into IBM PowerAI Vision platform and individual embeds are highlighted and categorised.  An  image  (jpg?)  is  imported  into  the  application  and individual  features  are  hilighted  and catagorised."], "summary_lines": "\uf0a7 Displays a list of all the precast , along with their count in the file and that detected by the AI s\nWeb application features: Bill of materials"}
{"article_lines": ["Dimensional Ttolerance checking ensures that the correct embed has been put in the correct position on the panel, according to the IFC file. The suggested acceptable tolerance levels are hardcoded presented to the userinto the program, but can be overridden manually by the user with the help of a built in slider. The default tolerance levels are in accordance with the ones stated in the quality documents used at EIP.", "Figure  4-194-20 Tolerance checking for an individual item. Note the orange dot is the item centre and the blue dot corresponds to the position in the IFC file. A message is displayed to the user to alert if a part is within tolerance or not.", "Digitising  this  process  and  detecting  the  precast  embeds  using  AI  will  help  automate  the inspection process by verifying whether the correct number of the precast embeds are present on the panel, and also whether they are in the correct position."], "summary_lines": "\uf0a7 Dimensional tolerance checking ensures that the correct  has \nbeen put in the correct position on the panel\nWeb application features: Tolerance checking"}
{"article_lines": ["In order to account for the AI object detection not being fully accurate in detecting the embeds, the operator has the option to manually add items by drawing a bounding box around it, or to delete  a  falsely  detected  item  by  selecting  the  desired  bounding  box  and  updating  the changes. The changes are then reflected back in the BOM list.", "13.  Embeds which look similar, such as the ones discussed in Section 5, can be colour coded. This will make it easier for the object detection model to distinguish between them  and  also  reduce  the  amount  of  post  processing  required,  thus  reducing  the chances of error.", "9 Conclusions This report states how an AI based inspection tool has the potential to increase productivity and improve efficiency in the quality inspection process at EIP. By using the data extracted from the IFC files and an image captured by an overhead camera, AI models can be used to detect the precast embeds and infer whether the correct number of each different type of embed is present on the panel. This can also be used to confirm the position of each embed and check whether they are within tolerance."], "summary_lines": "\uf0a7 AI object detection may introduce false positives or false negatives\n\uf0a7 In such cases, operator can remove a detected object or add an undetected object manually, by drawing a box around it\nWeb application features: Adding and deleting extra"}
{"article_lines": ["Once the inspection routine has been completed, a PDF report is generated upon request. This includes the date and time of the report, CAD image of the panel, inference images of both the anchor pins and the other precast embeds. It also includes the BOM list, which displays the name, the CAD count and the actual count of the precast embeds. This report is then saved tonto  a  folder,  from where  it  is  then  directly  uploaded  to  Trimble  Connect  using  Trimble Connect Sync. An example of the PDF report is in the Appendix IVII", "8 Future work \t9 Conclusions \tAppendix I \u2013 Sequence diagram for web application \tAppendix II Examples of training and test set images \tAppendix III \u2013 Dependencies/ libraries required \tAppendix IV Automatically generated PDF report", "TheThe  Python  parserprogram  used  for  processing  the  IFC  files  runs  continuously  in  the background and watches tracks for changes in a particular folder. Doing so allows for an IFC file to  be  processed as  soon as  it  is  downloaded  from Trimble  Connect.  When a new IFC file downloaded from Trimble Connect (discussed in Section 4.1) is available on that particular folder, the IFC file is parsed processed automatically and the relevant data is extracted, ready to be used in the inspection tool."], "summary_lines": "\uf0a7 PDF inspection report automatically \ngeneration upon completion of inspection\n\uf0a7 PDF report includes:\n\u2013 Date and time of report \n\u2013 Field for operator\u2019s name \n\u2013 CAD image of the panel \n\u2013 2 inference  (anchor pins and other \n\u2013 Bill of material list \n\u2013 Empty fields for comments and signature\n\uf0a7 This is automatically uploaded back to \nWeb application features: Automatically PDFs"}
{"article_lines": ["5 Additional image processing  Once the precast embeds have been identified using the trained models, it is important to find the centres of the precast embeds in order to verify whether they are in their correct position and within the tolerance. The centre of the bounding boxes are not a good indicator of their actual  centres,  as  the  bounding boxes are often not  very tight  and often includes regions outside the boundary of the precast embed. Hence, image processing routines were developed to find the true centre of the precast embeds.", "Two different image processing routines, which vary slightly, are used to distinguish the two sets of similar looking embeds. The From a high-level view, the image processing routine to distinguish between them involves converting the area enclosed by the bounding box into an HSV image,  finding  the  distinctive  feature  colour  of  the  precast  embed,  applying  multiple thresholds  and  computing  the  area  of  the  region  of  interest.  Once  the  area  has  been calculated, it can be compared against a set value to know its true type. A method which is less prone to errors and better has been discussed in Section 78.", "Figure  4-194-20 Tolerance checking for an individual item. Note the orange dot is the item centre and the blue dot corresponds to the position in the IFC file. A message is displayed to the user to alert if a part is within tolerance or not."], "summary_lines": "\uf0a7 Once an embed has been detected, its centre needs to be located\n\uf0a7 The centres of the bounding box is not a good indicator of its centre\n\uf0a7 OpenCV image  routines developed to find the centre \n\u2013 Convert the bounded area into a HSV image \n\u2013 Apply multiple thresholds \n\u2013 Find the distinctive colour of the precast embed\n\u2013 Find centre of the remaining section\n\uf0a7 Image  routines also used to distinguish similar looking items\n\u2013 Calculate area of the section and classify accordingly\nPost   for tolerance checking and distinguish similar items"}
{"article_lines": ["A Basler  acA5472-5gc industrial  camera  was attached  to  the  ceiling  at  EIP  at  a  height  of approximately 15m, looking down at bay 12. This camera has a resolution of 20 megapixels, which captured enough detail in the scene to allow the inspection process. A 25 mm lens is used which allows for a field of view of 7.8m x 5.2m. The lens on the camera can be swapped to accommodate for more or less field of view.", "A Basler  acA5472-5gc industrial  camera  was attached  to  the  ceiling  at  EIP  at  a  height  of approximately 15m, looking down at bay 12. This camera has a resolution of 20 megapixels, and a, which captured enough detail in the scene to allow the inspection process. A 25 mm lens is used which allows for a field of view of 7.8m x 5.2m from 15m height.  The lens on the camera can be swapped to accommodate for more or less field of view.", "6 Hardware and software integration A  Basler  acA5472-5gc  industrial  camera  is  attached  to  the  ceiling  at  EIP  at  a  height  of approximately 15m, looking down at bay 12. This camera has a resolution of 20 megapixels, which captured enough detail in the scene to allow the inspection process. A 25 mm lens is used which allows for a field of view of 7.8m x 5.2m. The lens on the camera can be swapped to accommodate for more or less field of view."], "summary_lines": "\u2013 Basler ace industrial camera to the ceiling at approximately \n\u2013 20mp camera with a 25mm lens, which allows a field of view of 7.8m x"}
{"article_lines": ["Figure 1-1 EIP operator capturing images of precast embeds to verify that the correct number of embeds are present on a given panel. Existing processes for quality assurance within offsite manufacture are \u2026 and time consuming, prone to errors etc. This report describes the web application developed by the AMRC in order to improve the  efficiency in the quality assurance process at EIP. The aim of this report is to provide the reader with an understanding of the various  technologies (hardware and software) used to develop the inspection tool, and also provide  details about the different features of the web application. The demonstrator results has also  been discussed and further work to improve the tool has been stated.  Section 3 describes in detail the development of the backbone of the web based inspection  tool, which is an AI method called deep learning.  Section 4 includes details about the various features of the web application and why they are  required in order to make the quality assurance and inspection process more efficient.  Section 5 is about the additional image processing required in order for checking whether the  precast embeds are within tolerance, and also to distinguish between similar looking items. Section 6 describes how the hardware and the software connect and integrate with each other.  Section 7 reports on the findings of the two demonstrations performed at AMRC Factory 2050  and Explore Industrial Park, ways to improve on them have been discussed.  Section 8 is about the future work that can be done to make the inspection tool better and  further its capabilities. It also includes ways to simplify some of the processes.", "3. Due to the height at which the camera is attached, thedistance of the panel from the camera, the number of pixels in the region of interest (the panel) is around 10 times more for the AMRC Factory 2050 demonstrator compared to the EIP demonstrator. A higher resolution camera will  be preferable for the EIP demonstrator in order to get better quality images.", "This report describes the inspection tool the web application developed by the AMRC in order to digitise and improve the efficiency of the current quality assurance process at EIPimprove the efficiency in the quality assurance process at EIP."], "summary_lines": "Video of EIP demonstrator"}
{"article_lines": ["The same camera (Basler acA5472-5gc) as the one used for the EIP demonstrator was used at AMRC Factory 2050. The height of the frame where the camera was attached is 3m. Using a 16mm lens,  this  allowed a field of  view of  2.4m x 1.6m from the height  the camera was attached", "3. Due to the height at which the camera is attached, thedistance of the panel from the camera, the number of pixels in the region of interest (the panel) is around 10 times more for the AMRC Factory 2050 demonstrator compared to the EIP demonstrator. A higher resolution camera will  be preferable for the EIP demonstrator in order to get better quality images.", "A Basler  acA5472-5gc industrial  camera  was attached  to  the  ceiling  at  EIP  at  a  height  of approximately 15m, looking down at bay 12. This camera has a resolution of 20 megapixels, which captured enough detail in the scene to allow the inspection process. A 25 mm lens is used which allows for a field of view of 7.8m x 5.2m. The lens on the camera can be swapped to accommodate for more or less field of view."], "summary_lines": "\uf0a7 AMRC Factory 2050 demonstrator: \n\u2013 Same Basler ace camera as EIP demonstrator \n\u2013 Using a 16mm lens gives a field of view of 2.4m x 1.6m"}
{"article_lines": ["Another test carried out on an actual  production panel for the Thames Tideway Blackfriars project at EIP resulted in the model being able to accurately detect 92% of the anchor pins. This  particular  model  was trained on a very small  dataset  (66 images)  captured at  AMRC Factory 2050, and was trained using the previously trained anchor pin model for the ESJ project as a starting point.", "Two demonstrations, one at AMRC Factory 2050 and the other at EIP were carried out in order to assess the accuracy of the AI models in correctly detecting the precast embeds. The panel used for demonstration was a miniature version of the panels produced for Edinburgh Saint James (ESJ) project. The models were trained using images captured at AMRC Factory 2050. It was found that the AMRC Factory 2050 demonstrator had an accuracy of 97% and 95% for the anchor pins and the other precast embeds respectively. On testing the performance of the AI models at EIP using the same panel, the accuracy dropped to 70% and 73% for the anchor pins and other precast embeds respectively. These results can be improved by stabilising the vision system and also training the models on data captured in the production environment.", "The new model for the new type of anchor pin had 66 images in the training set and 21 images in the test set, captured at AMRC Factory 2050. Because of the similar nature of the object detection task with respect to the ESJ anchor pins, the model trained on the anchor pins for ESJ were used as a starting point and applying transfer learning, the new model was trained. Upon testing on a single image as shown in Fig 6-7, this model had an accuracy of 92% in detecting the pins. This model can be made more accurate by improving the vision system and training the models from data captured in the production environment, as discussed in Section 6.4"], "summary_lines": "\uf0a7 Panels for ESJ were no longer in production during the demonstrator phase of this project\n\uf0a7 New deep learning  trained for Thames Tideway Blackfriars panel anchor pins \n\uf0a7 Models trained using data captured at Factory 2050\n\u2013 66 training , 21 test . \n\u2013 Transfer learning applied\n\uf0a7 Achieved an accuracy of 92% on the single image it has been tested on. \nDemonstration on Thames Tideway Blackfriars panel (EIP)"}
{"article_lines": ["1 Introduction Panels  for  construction  projects  such as  Edinburgh Saint  James (ESJ)  and Thames Tideway Blackfriars are produced at EIP. The presence of the required embeds on these panels need to be verified and validated before the concrete is poured in and the panel is casted.", "5 Additional image processing \t6 Hardware and software integration \t7 Demonstrators results and observations: \t7.1 Hardware and software configuration (EIP) \t7.2 Hardware and software configuration (Factory 2050) \t7.3 Results \t7.4 Discussion \t7.5 Demonstrator on Thames Tideway Blackfriars panel", "Another test carried out on an actual  production panel for the Thames Tideway Blackfriars project at EIP resulted in the model being able to accurately detect 92% of the anchor pins. This  particular  model  was trained on a very small  dataset  (66 images)  captured at  AMRC Factory 2050, and was trained using the previously trained anchor pin model for the ESJ project as a starting point."], "summary_lines": "Demonstration on Thames Tideway panels"}
{"article_lines": ["Confidence is the score output of an AI model about how confident it is with its decision about an  object  classification.  Average  confidence  is  calculated  as: Confidence of correctlydetected embeds Number of embeds correctlydetected", "Two demonstrators,  one at  Advanced  Manufacturing  Research  Centre  Factory  2050 (AMRC Factory 2050) and Explore Industrial Park (EIP) were set up to test the developed inspection tool. It was found that the AI models were able to correctly identify the precast embeds at AMRC  Factory  2050  with  approximately  95%  accuracy,  while  around  70%  of  the  precast embeds could be identified at EIP.", "Two demonstrations, one at AMRC Factory 2050 and the other at EIP were carried out in order to assess the accuracy of the AI models in correctly detecting the precast embeds. The panel used for demonstration was a miniature version of the panels produced for Edinburgh Saint James (ESJ) project. The models were trained using images captured at AMRC Factory 2050. It was found that the AMRC Factory 2050 demonstrator had an accuracy of 97% and 95% for the anchor pins and the other precast embeds respectively. On testing the performance of the AI models at EIP using the same panel, the accuracy dropped to 70% and 73% for the anchor pins and other precast embeds respectively. These results can be improved by stabilising the vision system and also training the models on data captured in the production environment."], "summary_lines": "\uf0a7 Two metrics used: confidence and accuracy \n\u2013 Accuracy: How the s performed in localising and classifying the objects. \n\u2013 Confidence: Score output of an AI  about how confident it is with its decision about an object classification\n            Average accuracy:    \n             Average confidence:  \nResults: EIP and AMRC Factory 2050 demonstrators.  \nDemonstrator Location s \nAMRC Factory 2050 97% 95%\nExplore Industrial Park 70% 73%\nDemonstrator Location s \nAMRC Factory 2050 89% 90%\nExplore Industrial Park 65% 70%"}
{"article_lines": ["2. Upon investigation, it was found that the camera at EIP was not fully stable and was vibrating  due  to  gantry  crane  movements  and  windother  structural  vibrations.  This affects the image quality, and deep learning models tend to be susceptible to blur and noise distortions. Using an anti-vibration camera mount will help stabilise the camera. results in the image not being clear, which affects the model\u2019s performance.", "3. Due to the height at which the camera is attached, thedistance of the panel from the camera, the number of pixels in the region of interest (the panel) is around 10 times more for the AMRC Factory 2050 demonstrator compared to the EIP demonstrator. A higher resolution camera will  be preferable for the EIP demonstrator in order to get better quality images.", "The same camera (Basler acA5472-5gc) as the one used for the EIP demonstrator was used at AMRC Factory 2050. The height of the frame where the camera was attached is 3m. Using a 16mm lens,  this  allowed a field of  view of  2.4m x 1.6m from the height  the camera was attached"], "summary_lines": "\uf0a7 Reasons for differences in demonstrator results\n\u2013 Models were trained solely on data gathered from AMRC Factory 2050\n\u2013 The camera at EIP was not stable and vibrating due to gantry crane movements and other structural \n\u2013 Differences in number of pixels in the region of interest due to the height at which the camera is"}
{"article_lines": ["Recommendations on how to improve the accuracy of the AI models by stabilising the vision system, using a higher resolution camera has been stated. Additional features that can be added to the inspection tool to further enhance its capabilities such as the ability to detect embeds not visible to the overhead camera using a tablet device, checking the shape of the panel,  colour  coding  similar  looking  embeds  haves  also  been  discussed.   With  further development in training the AI models with the right data and improvement in the hardware, this tool can be deployed on the shop floor at EIP.", "Opportunities to improve the accuracy and confidence of the AI models, by training on data from the production environment and using a different camera and camera mount hasve been discussed.  and also Aadditional features that can be added to the inspection tool to improve productivity and quality, such as using a tablet device to capture images not visible to the overhead camera, automated shape checking of the panel hasve also been discussed.  has also been to improve productivity have identified and discussed.", "5 Additional image processing  Once the precast embeds have been identified using the trained models, it is important to find the centres of the precast embeds in order to verify whether they are in their correct position and within the tolerance. The centre of the bounding boxes are not a good indicator of their actual  centres,  as  the  bounding boxes are often not  very tight  and often includes regions outside the boundary of the precast embed. Hence, image processing routines were developed to find the true centre of the precast embeds."], "summary_lines": "\uf0a7 Training on  from the production environment. \n\uf0a7 Narrowing down the embed search area\n\uf0a7 Using tablet camera to detect  on the vertical wall\n\u2013 Allow detection of  not visible to overhead camera\n\uf0a7 Adding QR codes to automatically crop the panel \n\u2013 Minimise error while manual or auto cropping\n\uf0a7 CAD with dimensions mark-up to help in embed placement \n\uf0a7 Automated shape checking of the panel\n\uf0a7 Colour coding similar looking \n\u2013 Minimise post  required to distinguish between them. \n\uf0a7 Read tolerance  from files\n\uf0a7 Direction arrows to reposition embed \n\uf0a7 Modifications in camera system\n\u2013 Higher resolution camera \n\u2013 Anti vibration camera mount"}
{"article_lines": ["Executive summary Current quality assurance methods for inspecting panels at EIP is a multi-stage process, which involves  operators  capturing  images  of  every  single  precast  embed with  a  mobile  device, before and after the rebar cage is placed in the mould, and senior members of the quality team validating it later on. This is a very manual process which is time consuming and prone to errors,  and therefore presents an opportunity to reduce costs and increase product  quality through automation..", "Opportunities to improve the accuracy and confidence of the AI models, by training on data from the production environment and using a different camera and camera mount hasve been discussed.  and also Aadditional features that can be added to the inspection tool to improve productivity and quality, such as using a tablet device to capture images not visible to the overhead camera, automated shape checking of the panel hasve also been discussed.  has also been to improve productivity have identified and discussed.", "Recommendations on how to improve the accuracy of the AI models by stabilising the vision system, using a higher resolution camera has been stated. Additional features that can be added to the inspection tool to further enhance its capabilities such as the ability to detect embeds not visible to the overhead camera using a tablet device, checking the shape of the panel,  colour  coding  similar  looking  embeds  haves  also  been  discussed.   With  further development in training the AI models with the right data and improvement in the hardware, this tool can be deployed on the shop floor at EIP."], "summary_lines": "\u201cInnovative, directly solving a real problem that we have, \nand will make things easier and save us a lot of time. It\u2019s \nalso giving us the certainty that critical to safety and quality \nitems are there. This will have a huge benefit in making the \ninspection process safer, more efficient and more assured\u201d\nFeedback from EIP quality team"}
{"article_lines": ["Additional  image  processing  is  also  required  when  distinguishing  between  similar  looking precast embeds, such as distinguishing between the HTA-5030P-A4-150 and HTA-5030P-A4- 200 HTA 150/200 and. Another set of similar looks embeds are the HFV9_3260_GROUT_SLEEVE and BWT-GENERIC-FACE-HALFEN-DOWEL , as shown in Figure 5-1.  Halfen Dowel/ Grout sleeve.", ".  The  aim  of  this  report  is  to  provide  the  reader  with  an  understanding  of  the  various technologies (hardware and software) used to develop the inspection tool, and also provide details  about  the  different  features  of  the  inspection  tool.  The  results  from  the  two demonstrators, at AMRC Factory 2050 and EIP, haves also been discussed and further work to improve the inspection tool haves been stated.", "4 Web application \t4.1 Syncing with Trimble Connect \t4.2 Selecting a particular IFC file: \t4.3 Cropping, rotating and orientating: \t4.4 Detecting precast embeds \t4.5 Bill of materials list \t4.6 Tolerance checking \t4.7 Adding and deleting extra embeds \t4.8 Automatically generated inspection report"], "summary_lines": "r.w.scott@sheffield.ac.uk\nb.baruah@sheffield.ac.uk"}
{"article_lines": ["Figure 1-1 EIP operator capturing images of precast embeds to verify that the correct number of embeds are present on a given panel. Existing processes for quality assurance within offsite manufacture are \u2026 and time consuming, prone to errors etc. This report describes the web application developed by the AMRC in order to improve the  efficiency in the quality assurance process at EIP. The aim of this report is to provide the reader with an understanding of the various  technologies (hardware and software) used to develop the inspection tool, and also provide  details about the different features of the web application. The demonstrator results has also  been discussed and further work to improve the tool has been stated.  Section 3 describes in detail the development of the backbone of the web based inspection  tool, which is an AI method called deep learning.  Section 4 includes details about the various features of the web application and why they are  required in order to make the quality assurance and inspection process more efficient.  Section 5 is about the additional image processing required in order for checking whether the  precast embeds are within tolerance, and also to distinguish between similar looking items. Section 6 describes how the hardware and the software connect and integrate with each other.  Section 7 reports on the findings of the two demonstrations performed at AMRC Factory 2050  and Explore Industrial Park, ways to improve on them have been discussed.  Section 8 is about the future work that can be done to make the inspection tool better and  further its capabilities. It also includes ways to simplify some of the processes.", "Executive summary Current quality assurance methods for inspecting panels at EIP is a multi-stage process, which involves  operators  capturing  images  of  every  single  precast  embed with  a  mobile  device, before and after the rebar cage is placed in the mould, and senior members of the quality team validating it later on. This is a very manual process which is time consuming and prone to errors,  and therefore presents an opportunity to reduce costs and increase product  quality through automation..", "3. Due to the height at which the camera is attached, thedistance of the panel from the camera, the number of pixels in the region of interest (the panel) is around 10 times more for the AMRC Factory 2050 demonstrator compared to the EIP demonstrator. A higher resolution camera will  be preferable for the EIP demonstrator in order to get better quality images."], "summary_lines": "Current quality assurance method\n\tVideo of EIP demonstrator\n\tDemonstration on Thames Tideway panels"}

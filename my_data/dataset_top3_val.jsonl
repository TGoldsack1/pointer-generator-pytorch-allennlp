{"article_lines": ["A Fully Statistical Approach To Natural Language Interfaces", "We present a natural language interface system which is based entirely on trained statistical models .", "The system consists of three stages of processing parsing , semantic interpretation , and discourse .", "Each of these stages is modeled as a statistical process .", "The models are fully integrated , resulting in an end to end system that maps input utterances into meaning representation frames .", "A recent trend in natural language processing has been toward a greater emphasis on statistical approaches , beginning with the success of statistical part of speech tagging programs Church 1988 , and continuing with other work using statistical part of speech tagging programs , such as BBN PLUM Weischedel et al . 1993 and NYU Proteus Grishman and Sterling 1993 .", "More recently , statistical methods have been applied to domain specific semantic parsing Miller et al . 1994 , and to the more difficult problem of wide coverage syntactic parsing Magerman 1995 .", "Nevertheless , most natural language systems remain primarily rule based , and even systems that do use statistical techniques , such as AT T Chronus Levin and Pieraccini 1995 , continue to require a significant rule based component .", "Development of a complete end to end statistical understanding system has been the focus of several ongoing research efforts , including Miller et al . 1995 and Koppelman et al .", "In this paper , we present such a system .", "The overall structure of our approach is conventional , consisting of a parser , a semantic interpreter , and a discourse module .", "The implementation and integration of these elements is far less conventional .", "Within each module , every processing step is assigned a probability value , and very large numbers of alternative theories are pursued in parallel .", "The individual modules are integrated through an n best paradigm , in which many theories are passed from one stage to the next , together with their associated probability scores .", "The meaning of a sentence is determined by taking the highest scoring theory from among the n best possibilities produced by the final stage in the model .", "Some key advantages to statistical modeling techniques are sufficient to provide the system with examples specifying the correct parses for a set of training examples .", "There is no need to specify an exact set of rules or a detailed procedure for producing such parses . are principled techniques for estimating the gradations .", "The system is thus free to pursue unusual theories , while remaining aware of the fact that they are unlikely .", "In the event that a more likely theory exists , then the more likely theory is selected , but if no more likely interpretation can be found , the unlikely interpretation is accepted .", "The focus of this work is primarily to extract sufficient information from each utterance to give an appropriate response to a user's request .", "A variety of problems regarded as standard in computational linguistics , such as quantification , reference and the like , are thus ignored .", "To evaluate our approach , we trained an experimental system using data from the Air Travel Information ATIS domain Bates et al . 1990 ; Price 1990 .", "The selection of ATIS was motivated by three concerns .", "First , a large corpus of ATIS sentences already exists and is readily available .", "Second , ATIS provides an existing evaluation methodology , complete with independent training and test corpora , and scoring programs .", "Finally , evaluating on a common corpus makes it easy to compare the performance of the system with those based on different approaches .", "We have evaluated our system on the same blind test sets used in the ARPA evaluations Pallett et al . 1995 , and present a preliminary result at the conclusion of this paper .", "The remainder of the paper is divided into four sections , one describing the overall structure of our models , and one for each of the three major components of parsing , semantic interpretation and discourse .", "Given a string of input words W and a discourse history H , the task of a statistical language understanding system is to search among the many possible discourse dependent meanings MD for the most likely meaning Mo Mo arg max P MD I W , H .", "Directly modeling P MD I W , H is difficult because the gap that the model must span is large .", "A common approach in non statistical natural language systems is to bridge this gap by introducing intermediate representations such as parse structure and pre discourse sentence meaning .", "Introducing these intermediate levels into the statistical framework gives where T denotes a semantic parse tree , and Ms denotes prediscourse sentence meaning .", "This expression can be simplified by introducing two independence assumptions Now , since P W is constant for any given word string , the problem of finding meaning MD that maximizes We now introduce a third independence assumption 3 .", "The probability of words W does not depend on meaning Ms , given that parse T is known .", "This assumption is justified because the word tags in our parse representation specify both semantic and syntactic class information .", "Under this assumption Finally , we assume that most of the probability mass for each discourse dependent meaning is focused on a single parse tree and on a single pre discourse meaning .", "Under this Viterbi assumption , the summation operator can be replaced by the maximization operator , yielding This expression corresponds to the computation actually performed by our system which is shown in Figure 1 .", "Processing proceeds in three stages These parses , together with their probability scores , are passed to the semantic interpretation model .", "The constrained space of candidate parses T received from the parsing model , combined with the full space of possible pre discourse meanings Ms , is searched for n best candidates according to the measure P Ms , T P W IT .", "These pre discourse meanings , together with their associated probability scores , are passed to the discourse model .", "The constrained space of candidate pre discourse meanings Ms received from the semantic interpretation model , combined with the full space of possible postdiscourse meanings MD , is searched for the single candidate that maximizes P MD I H , M s P Ms , T P W IT , conditioned on the current history H . The discourse history is then updated and the post discourse meaning is returned .", "We now proceed to a detailed discussion of each of these three stages , beginning with parsing .", "Our parse representation is essentially syntactic in form , patterned on a simplified head centered theory of phrase structure .", "In content , however , the parse trees are as much semantic as syntactic .", "Specifically , each parse node indicates both a semantic and a syntactic class excepting a few types that serve purely syntactic functions .", "Figure 2 shows a sample parse of a typical ATIS sentence .", "The semantic , syntactic character of this representation offers several advantages top wh question process semantic labels identify the basic units of meaning , while syntactic structures help identify relationships between those units .", "The parsing model is a probabilistic recursive transition network similar to those described in Miller et al . 1994 and Seneff 1992 .", "The probability of a parse tree T given a word string W is rewritten using Bayes rule as Since P W is constant for any given word string , candidate parses can be ranked by considering only the product P T P W I 7 .", "The probability P 7 is modeled by state transition probabilities in the recursive transition network , and P W I 7 is modeled by word transition probabilities .", "P location pp I arrivaL vp head , arrival vp is the probability of a location pp following an arrivaMvphead within an arrivallvp constituent . probability along the path corresponding to T . Transition probabilities are estimated directly by observing occurrence and transition frequencies in a training corpus of annotated parse trees .", "These estimates are then smoothed to overcome sparse data limitations .", "The semantic syntactic parse labels , described above , provide a further advantage in terms of smoothing for cases of undertrained probability estimates , the model backs off to independent syntactic and semantic probabilities as follows where A . is estimated as in Placeway et al . 1993 .", "Backing off to independent semantic and syntactic probabilities potentially provides more precise estimates than the usual strategy of backing off directly form bigram to unigram models .", "In order to explore the space of possible parses efficiently , the parsing model is searched using a decoder based on an adaptation of the Earley parsing algorithm Earley 1970 .", "This adaptation , related to that of Stolcke 1995 , involves reformulating the Earley algorithm to work with probabilistic recursive transition networks rather than with deterministic production rules .", "For details of the decoder , see Miller 1996 .", "Both pre discourse and post discourse meanings in our current system are represented using a simple frame representation .", "Figure 3 shows a sample semantic frame corresponding to the parse in Figure 2 .", "Recall that the semantic interpreter is required to compute P M s , T P W IT .", "The conditional word probability P W IT has already been computed during the parsing phase and need not be recomputed .", "The current problem , then , is to compute the prior probability of meaning Ms and parse T occurring together .", "Our strategy is to embed the instructions for constructing Ms directly into parse T , resulting in an augmented tree structure .", "For example , the instructions needed to create the frame shown in Figure 3 are These instructions are attached to the parse tree at the points indicated by the circled numbers see Meanings Ms are decomposed into two parts the frame type FT , and the slot fillers S . The frame type is always attached to the topmost node in the augmented parse tree , while the slot filling instructions are attached to nodes lower down in the tree .", "Except for the topmost node , all parse nodes are required to have some slot filling operation .", "For nodes that do not directly trigger any slot fill operation , the special operation null is attached .", "The probability P Ms , 7 is then Obviously , the prior probabilities P FT can be obtained directly from the training data .", "To compute P T I FT , each of the state transitions from the previous parsing model are simply rescored conditioned on the frame type .", "The new state transition probabilities are To compute P S I FT , 7 , we make the independence assumption that slot filling operations depend only on the frame type , the slot operations already performed , and on the local parse structure around the operation .", "This local neighborhood consists of the parse node itself , its two left siblings , its two right siblings , and its four immediate ancestors .", "Further , the syntactic and semantic components of these nodes are considered independently .", "Under these assumptions , the probability of a slot fill operation is and the probability P S I FT , 7 is simply the product of all such slot fill operations in the augmented tree .", "Transition probabilities are estimated from a training corpus of augmented trees .", "Unlike probabilities in the parsing model , there obviously is not sufficient training data to estimate slot fill probabilities directly .", "Instead , these probabilities are estimated by statistical decision trees similar to those used in the Spatter parser Magerman 1995 .", "Unlike more common decision tree classifiers , which simply classify sets of conditions , statistical decision trees give a probability distribution over all possible outcomes .", "Statistical decision trees are constructed in a two phase process .", "In the first phase , a decision tree is constructed in the standard fashion using entropy reduction to guide the construction process .", "This phase is the same as for classifier models , and the distributions at the leaves are often extremely sharp , sometimes consisting of one outcome with probability 1 , and all others with probability 0 .", "In the second phase , these distributions are smoothed by mixing together distributions of various nodes in the decision tree .", "As in Magerman 1995 , mixture weights are determined by deleted interpolation on a separate block of training data .", "Searching the interpretation model proceeds in two phases .", "In the first phase , every parse T received from the parsing model is rescored for every possible frame type , computing P T I F7 our current model includes only a half dozen different types , so this computation is tractable .", "Each of these theories is combined with the corresponding prior probability P FT yielding P FT P T I PT .", "The n best of these theories are then passed to the second phase of the interpretation process .", "This phase searches the space of slot filling operations using a simple beam search procedure .", "For each combination of FT and T , the beam search procedure considers all possible combinations of fill operations , while pruning partial theories that fall beneath the threshold imposed by the beam limit .", "The surviving theories are then combined with the conditional word probabilities P W I 7 , computed during the parsing model .", "The final result of these steps is the n best set of candidate pre discourse meanings , scored according to the measure P Ms , T P W IT .", "The discourse module computes the most probable postdiscourse meaning of an utterance from its pre discourse meaning and the discourse history , according to the measure Because pronouns can usually be ignored in the ATIS domain , our work does not treat the problem of pronominal reference .", "Our probability model is instead shaped by the key discourse problem of the ATIS domain , which is the inheritance of constraints from context .", "This inheritance phenomenon , similar in spirit to one anaphora , is illustrated by the following dialog SYSTEM2 displays Boston to Denver flights for Tuesday In USER2 , it is obvious from context that the user is asking about flights whose ORIGIN is BOSTON and whose DESTINATION is DENVER , and not all flights between any two cities .", "Constraints are not always inherited , however .", "For example , in the following continuation of this dialogue USER3 Show me return flights from Denver to Boston , it is intuitively much less likely that the user means the quot ; on Tuesday quot ; constraint to continue to apply .", "The discourse history H simply consists of the list of all postdiscourse frame representations for all previous utterances in the current session with the system .", "These frames are the source of candidate constraints to be inherited .", "For most utterances , we make the simplifying assumption that we need only look at the last i . e . most recent frame in this list , which we call M . The statistical discourse model maps a 23 element input vector X onto a 23 element output vector Y .", "These vectors have the following interpretations The 23 elements in vectors X and Y correspond to the 23 possible slots in the frame schema .", "Each element in X can have one of five values , specifying the relationship between the filler of the corresponding slot in Mp and Ms INITIAL slot filled in Ms but not in Mp TACIT slot filled in Mp but not in Ms REITERATE slot filled in both MA and Ms ; value the same CHANGE slot filled in both Mp and Ms ; value different IRRELEVANT slot not filled in either Mp or Ms Output vector Y is constructed by directly copying all fields from input vector X except those labeled TACIT .", "These direct copying operations are assigned probability 1 .", "For fields labeled TACIT , the corresponding field in Y is filled with either INHERITED or NOT INHERITED .", "The probability of each of these operations is determined by a statistical decision tree model .", "The discourse model contains 23 such statistical decision trees , one for each slot position .", "An ordering is imposed on the set of frame slots , such that inheritance decisions for slots higher in the order are conditioned on the decisions for slots lower in the order .", "The probability P Y I X is then the product of all 23 statistical models to additional linguistic phenomena such as quantification and anaphora resolution . decision probabilities The discourse model is trained from a corpus annotated with both pre discourse and post discourse semantic frames .", "Corresponding pairs of input and output X , Y vectors are computed from these annotations , which are then used to train the 23 statistical decision trees .", "The training procedure for estimating these decision tree models is similar to that used for training the semantic interpretation model .", "Searching the discourse model begins by selecting a meaning frame Mp from the history stack H , and combining it with each pre discourse meaning Ms received from the semantic interpretation model .", "This process yields a set of candidate input vectors X .", "Then , for each vector X , a search process exhaustively constructs and scores all possible output vectors Y according to the measure P Y I X this computation is feasible because the number of TACIT fields is normally small .", "These scores are combined with the pre discourse scores P Ms , T P W IT , already computed by the semantic interpretation process .", "This computation yields The highest scoring theory is then selected , and a straightforward computation derives the final meaning frame MD from output vector Y .", "We have trained and evaluated the system on a common corpus of utterances collected from naive users in the ATIS domain .", "In this test , the system was trained on approximately 4000 ATIS 2 and ATIS 3 sentences , and then evaluated on the December 1994 test material which was held aside as a blind test set .", "The combined system produced an error rate of 21 . 6 .", "Work on the system is ongoing , however , and interested parties are encouraged to contact the authors for more recent results .", "We have presented a fully trained statistical natural language interface system , with separate models corresponding to the classical processing steps of parsing , semantic interpretation and discourse .", "Much work remains to be done in order to refine the statistical modeling techniques , and to extend the", "We wish to thank Robert Ingria for his effort in supervising the annotation of the training corpus , and for his helpful technical suggestions .", "This work was supported by the Advanced Research Projects Agency and monitored by the Office of Naval Research under Contract No .", "NO0014 91 C 0115 , and by Ft . Huachuca under Contract Nos .", "DABT63 94 C 0061 and DABT63 94C 0063 .", "The content of the information does not necessarily reflect the position or the policy of the Government and no official endorsement should be inferred ."], "summary_lines": ["A Fully Statistical Approach To Natural Language Interfaces\n", "We present a natural language interface system which is based entirely on trained statistical models.\n", "The system consists of three stages of processing: parsing, semantic interpretation, and discourse.\n", "Each of these stages is modeled as a statistical process.\n", "The models are fully integrated, resulting in an end-to-end system that maps input utterances into meaning representation frames.\n", "Our approach is fully supervised and produces a final meaning representation in SQL.\n", "We compute the probability that a constituent such as Atlanta filled a semantic slot such as Destination in a semantic frame for air travel.\n"]}
{"article_lines": ["Factored Language Models And Generalized Parallel Backoff", "We introduce factored language models FLMs and generalized parallel backoff GPB .", "An FLM represents words as bundles of features e . g . , morphological classes , stems , data driven clusters , etc .", ", and induces a probability model covering sequences of bundles rather than just words .", "GPB extends standard backoff to general conditional probability tables where variables might be heterogeneous types , where no obvious natural temporal backoff order exists , and where multiple dynamic backoff strategies are allowed .", "These methodologies were implemented during the JHU 2002 workshop as extensions to the SRI language modeling toolkit .", "This paper provides initial perplexity results on both CallHome Arabic and on Penn Treebank Wall Street Journal articles .", "Significantly , FLMs with GPB can produce bigrams with significantly lower perplexity , sometimes lower than highly optimized baseline trigrams .", "In a multi pass speech recognition context , where bigrams are used to create first pass bigram lattices or N best lists , these results are highly relevant .", "The art of statistical language modeling LM is to create probability models over words and sentences that tradeoff statistical prediction with parameter variance .", "The field is both diverse and intricate Rosenfeld , 2000 ; Chen and Goodman , 1998 ; Jelinek , 1997 ; Ney et al . , 1994 , with many different forms of LMs including maximumentropy , whole sentence , adaptive and cache based , to name a small few .", "Many models are simply smoothed conditional probability distributions for a word given its preceding history , typically the two preceding words .", "In this work , we introduce two new methods for language modeling factored language model FLM and generalized parallel backoff GPB .", "An FLM considers a word as a bundle of features , and GPB is a technique that generalized backoff to arbitrary conditional probability tables .", "While these techniques can be considered in isolation , the two methods seem particularly suited to each other in particular , the method of GPB can greatly facilitate the production of FLMs with better performance .", "In a factored language model , a word is viewed as a vector of k factors , so that wt f1t , f2t , . . . , fKt .", "Factors can be anything , including morphological classes , stems , roots , and other such features in highly inflected languages e . g . , Arabic , German , Finnish , etc .", ", or data driven word classes or semantic features useful for sparsely inflected languages e . g . , English .", "Clearly , a two factor FLM generalizes standard class based language models , where one factor is the word class and the other is words themselves .", "An FLM is a model over factors , i . e . , p ft K ft1 F'1K that can be factored as a product of probabilities of the form p f f1 , f2 , . . . , fN .", "Our task is twofold 1 find an appropriate set of factors , and 2 induce an appropriate statistical model over those factors i . e . , the structure learning problem in graphical models Bilmes , 2003 ; Friedman and Koller , 2001 .", "An individual FLM probability model can be seen as a directed graphical model over a set of N 1 random variables , with child variable F and N parent variables F1 through FN if factors are words , then F Wt and Fi Wt i .", "Two features make an FLM distinct from a standard language model 1 the variables F , F1 , . . . , FN can be heterogeneous e . g . , words , word clusters , morphological classes , etc .", "; and 2 there is no obvious natural e . g . , temporal backoff order as in standard wordbased language models .", "With word only models , backoff proceeds by dropping first the oldest word , then the next oldest , and so on until only the unigram remains .", "In p f f1 , f2 , .", ". . , fN , however , many of the parent variables might be the same age .", "Even if the variables have differing seniorities , it is not necessarily best to drop the oldest variable first .", "We introduce the notion of a backoff graph Figure 1 to depict this issue , which shows the various backoff paths from the all parents case top graph node to the unigram bottom graph node .", "Many possible backoff paths could be taken .", "For example , when all variables are words , the path A B E H corresponds to trigram with standard oldest first backoff order .", "The path A D G H is a reverse time backoff model .", "This can be seen as a generalization of lattice based language modeling Dupont and Rosenfeld , 1997 where factors consist of words and hierarchically derived word classes .", "In our GPB procedure , either a single distinct path is chosen for each gram or multiple parallel paths are used simultaneously .", "In either case , the set of backoff path s that are chosen are determined dynamically at run time based on the current values of the variables .", "For example , a path might consist of nodes A BCD EF G where node A backs off in parallel to the three nodes BCD , node B backs off to nodes EF , C backs off to E , and D backs off to F .", "This can be seen as a generalization of the standard backoff equation .", "In the two parents case , this becomes where dN f , f1 , f2 is a standard discount determining the smoothing method , pML is the maximum likelihood distribution , \u03b1 f1 , f2 are backoff weights , and g f , f1 , f2 is an arbitrary non negative backofffunction of its three factor arguments .", "Standard backoff occurs with g f , f1 , f2 pBO f f1 , but the GPB procedures can be obtained by using different g functions .", "For example , g f , f1 , f2 pBO f f2 corresponds to a different backoff path , and parallel backoff is obtained by using an appropriate g see below .", "As long as g is non negative , the backoff weights are defined as follows This equation is non standard only in the denominator , where one may no longer sum over the factors f only with counts greater than T . This is because g is not necessarily a distribution i . e . , does not sum to unity .", "Therefore , backoff weight computation can indeed be more expensive for certain g functions , but this appears not to be prohibitive as demonstrated in the next few sections .", "During the recent 2002 JHU workshop Kirchhoff et al . , 2003 , significant extensions were made to the SRI language modeling toolkit Stolcke , 2002 to support arbitrary FLMs and GPB procedures .", "This uses a graphicalmodel like specification language , and where many different backoff functions 19 in total were implemented .", "Other features include 1 all SRILM smoothing methods at every node in a backoff graph ; 2 graph level skipping ; and 3 up to 32 possible parents e . g . , 33 gram .", "Two of the backoff functions are in the three parents case where call this g2 where N is the count function .", "Implemented backoff functions include maximum min normalized counts backoff probabilities , products , sums , mins , maxs , weighted averages , and geometric means .", "GPB FLMs were applied to two corpora and their perplexity was compared with standard optimized vanilla biand trigram language models .", "In the following , we consider as a bigram a language model with a temporal history that includes information from no longer than one previous time step into the past .", "Therefore , if factors are deterministically derivable from words , a bigram might include both the previous words and previous factors as a history .", "From a decoding state space perspective , any such bigram would be relatively cheap .", "In CallHome Arabic , words are accompanied with deterministically derived factors morphological class M , stems S , roots R , and patterns P .", "Training data consisted of official training portions of the LDC CallHome ECA corpus plus the CallHome ECA supplement 100 conversations .", "For testing we used the official 1996 evaluation set .", "Results are given in Table 1 and show perplexity for 1 the baseline 3 gram ; 2 a FLM 3 gram using morphs and stems ; 3 a GPB FLM 3 gram using morphs , stems and backoff function g1 ; 4 the baseline 2 gram ; 5 an FLM 2 gram using morphs ; 6 an FLM 2 gram using morphs and stems ; and 7 an GPB FLM 2 gram using morphs and stems .", "Backoffpath s are depicted by listing the parent number s in backoff order .", "As can be seen , the FLM alone might increase perplexity , but the GPB FLM decreases it .", "Also , it is possible to obtain a 2 gram with lower perplexity than the optimized baseline 3 gram .", "The Wall Street Journal WSJ data is from the Penn Treebank 2 tagged 88 89 WSJ collection .", "Word and POS tag information Tt was extracted .", "The sentence order was randomized to produce 5 fold crossvalidation results using 4 5 1 5 training testing sizes .", "Other factors included the use of a simple deterministic tagger obtained by mapping a word to its most frequent tag Ft , and word classes obtained using SRILM s ngram class tool with 50 Ct and 500 Dt classes .", "Results are given in Table 2 .", "The table shows the baseline 3 gram and 2 gram perplexities , and three GPB FLMs .", "Model A uses the true by hand tag information from the Treebank .", "To simulate conditions during first pass decoding , Model B shows the results using the most frequent tag , and Model C uses only the two data driven word classes .", "As can be seen , the bigram perplexities are significantly reduced relative to the baseline , almost matching that of the baseline trigram .", "Note that none of these reduced perplexity bigrams were possible without using one of the novel backoff functions .", "The improved perplexity bigram results mentioned above should ideally be part of a first pass recognition step of a multi pass speech recognition system .", "With a bigram , the decoder search space is not large , so any appreciable LM perplexity reductions should yield comparable word error reductions for a fixed set of acoustic scores in a fzrstpass .", "For N best or lattice generation , the oracle error should similarly improve .", "The use of an FLM with GPB in such a first pass , however , requires a decoder that supports such language models .", "Therefore , FLMs with GPB will be incorporated into GMTK Bilmes , 2002 , a general purpose graphical model toolkit for speech recognition and language processing .", "The authors thank Dimitra Vergyri , Andreas Stolcke , and Pat Schone for useful discussions during the JHU 02 workshop ."], "summary_lines": ["Factored Language Models And Generalized Parallel Backoff\n", "We introduce factored language models (FLMs) and generalized parallel backoff (GPB).\n", "An FLM represents words as bundles of features (e.g. , morphological classes, stems, data-driven clusters, etc.), and induces a probability model covering sequences of bundles rather than just words.\n", "GPB extends standard backoff to general conditional probability tables where variables might be heterogeneous types, where no obvious natural (temporal) backoff order exists, and where multiple dynamic backoff strategies are allowed.\n", "These methodologies were implemented during the JHU 2002 workshop as extensions to the SRI language modeling toolkit.\n", "This paper provides initial perplexity results on both CallHome Arabic and on Penn Treebank Wall Street Journal articles.\n", "Significantly, FLMs with GPB can produce bigrams with significantly lower perplexity, sometimes lower than highly-optimized baseline trigrams.\n", "In a multi-pass speech recognition context, where bigrams are used to create first-pass bigram lattices or N-best lists, these results are highly relevant.\n", "We show that factored language models are able to outperform standard n-gram techniques in terms of perplexity.\n", "A factored language model (FLM) is based on a representation of words as feature vectors and can utilize a variety of additional information sources in addition to words, such as part-of-speech (POS) information, morphological information, or semantic features, in a unified and principled framework.\n"]}
{"article_lines": ["Word Association Norms Mutual Information And Lexicography", "1982 for constructing language models for applications in speech recognition .", "Smadja in press discusses the separation between collocates in a very similar way .", "This definition y a rectangular window .", "It might be interesting to consider alternatives e . g . a triangular window or a decaying exponential that would weight words less and less as they are separated by more and more words .", "Other windows are also possible .", "For example , Hindle Church et al . 1989 has used a syntactic parser to select words in certain constructions of interest .", "Although the Good Turing Method Good 1953 is more than 35 years old , it is still heavily cited .", "For example , Katz 1987 uses the in order to estimate trigram probabilities in the recognizer .", "The Good Turing Method is helpful for trigrams that have not been seen very often in the training corpus .", "The last unclassified line , .", ". shoppers anywhere from 50 .", ". raises interesting problems .", "Syntactic quot ; chunking quot ; shows that , in spite its co occurrence of line does not belong here .", "An intriguing exercise , given the lookup table we are trying construct , is how to guard against false inferences such as that since tagged PERSON , here count as either a LOCATION .", "Accidental coincidences of this kind do not have a significant effect on the measure , however , although they do serve as a reminder of the probabilistic nature of the findings .", "The word also occurs significantly in the table , but on closer it is clear that this use of to time as something like a commodity or resource , not as part of a time adjunct .", "Such are the pitfalls of lexicography obvious when they are pointed out .", "The term word association is used in a very particular sense in the psycholinguistic literature .", "Generally speaking , subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor .", "We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena , ranging from semantic relations of the doctor nurse type content word content word to lexico syntactic co occurrence constraints between verbs and prepositions content word function word .", "This paper will propose an objective measure based on the information theoretic notion of mutual information , for estimating word association norms from computer readable corpora .", "The standard method of obtaining word association norms , testing a few thousand subjects on a few hundred words , is both costly and unreliable .", "The proposed measure , the association ratio , estimates word association norms directly from computer readable corpora , making it possible to estimate norms for tens of thousands of words .", "It is common practice in linguistics to classify words not only on the basis of their meanings but also on the basis of their co occurrence with other words .", "Running through the whole Firthian tradition , for example , is the theme that quot ; You shall know a word by the company it keeps quot ; Firth , 1957 .", "On the one hand , bank co occurs with words and expression such as money , notes , loan , account , investment , clerk , official , manager , robbery , vaults , working in a , its actions , First National , of England , and so forth .", "On the other hand , we find bank co occurring with river , swim , boat , east and of course West and South , which have acquired special meanings of their own , on top of the , and of the Rhine .", "Hanks 1987 , p . 127 The search for increasingly delicate word classes is not new .", "In lexicography , for example , it goes back at least to the quot ; verb patterns quot ; described in Hornby's Advanced Learner's Dictionary first edition 1948 .", "What is new is that facilities for the computational storage and analysis of large bodies of natural language have developed significantly in recent years , so that it is now becoming possible to test and apply informal assertions of this kind in a more rigorous way , and to see what company our words do keep .", "The proposed statistical description has a large number of potentially important applications , including a constraining the language model both for speech recognition and optical character recognition OCR , b providing disambiguation cues for parsing highly ambiguous syntactic structures such as noun compounds , conjunctions , and prepositiona 1 phrases , c retrieving texts from large databases e . g . newspapers , patents , d enhancing the productivity of computational linguists in compiling lexicons of lexicosyntactic facts , and e enhancing the productivity of lexicographers in identifying normal and conventional usage .", "Consider the optical character recognizer OCR application .", "Suppose that we have an OCR device as in Kahan et al . 1987 , and it has assigned about equal probability to having recognized farm and form , where the context is either 1 federal credit or 2 some of .", "The proposed association measure can make use of the fact that farm is much more likely in the first context and form is much more likely in the second to resolve the ambiguity .", "Note that alternative disambiguation methods based on syntactic constraints such as part of speech are unlikely to help in this case since both form and farm are commonly used as nouns .", "Word association norms are well known to be an important factor in psycholinguistic research , especially in the area of lexical retrieval .", "Generally speaking , subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor .", "Some results and implications are summarized from reaction time experiments in which subjects either a classified successive strings of letters as words and nonwords , or b pronounced the strings .", "Both types of response to words e . g .", "BUTTER were consistently faster when preceded by associated words e . g .", "BREAD rather than unassociated words e . g .", "NURSE Meyer etal .", "1975 , p . 98 Much of this psycholinguistic research is based on empirical estimates of word association norms as in Palermo and Jenkins 1964 , perhaps the most influential study of its kind , though extremely small and somewhat dated .", "This study measured 200 words by asking a few thousand subjects to write down a word after each of the 200 words to be measured .", "Results are reported in tabular form , indicating which words were written down , and by how many subjects , factored by grade level and sex .", "The word doctor , for example , is reported on pp .", "98 100 to be most often associated with nurse , followed by sick , health , medicine , hospital , man , sickness , lawyer , and about 70 more words .", "We propose an alternative measure , the association ratio , for measuring word association norms , based on the information theoretic concept of mutual information . '", "The proposed measure is more objective and less costly than the subjective method employed in Palermo and Jenkins 1964 .", "The association ratio can be scaled up to provide robust estimates of word association norms for a large portion of the language .", "Using the association ratio measure , the five most associated words are , in order dentists , nurses , treating , treat , and hospitals .", "What is quot ; mutual information ? quot ; According to Fano 1961 , if two points words , x and y , have probabilities P x and P y , then their mutual information , I x , y , is defined to be Informally , mutual information compares the probability of observing x and y together the joint probability with the probabilities of observing x and y independently chance .", "If there is a genuine association between x and y , then the joint probability P x , y will be much larger than chance P x P y , and consequently I x , y 0 .", "If there is no interesting relationship between x and y , then P x , y P x P y , and thus , I x , y 0 .", "If x and y are in complementary distribution , then P x , y will be much less than P x P y , forcing I x , y 0 .", "In our application , word probabilities P x and P y are estimated by counting the number of observations of x and y in a corpus , f x and f y , and normalizing by N , the size of the corpus .", "Our examples use a number of different corpora with different sizes 15 million words for the 1987 AP corpus , 36 million words for the 1988 AP corpus , and 8 . 6 million tokens for the tagged corpus .", "Joint probabilities , P x , y , are estimated by counting the number of times that xis followed by y in a window of w words , f , x , y , and normalizing by N . The window size parameter allows us to look at different scales .", "Smaller window sizes will identify fixed expressions idioms such as bread and butter and other relations that hold over short ranges ; larger window sizes will highlight semantic concepts and other relationships that hold over larger scales .", "Table 1 may help show the contrast . 2 In fixed expressions , such as bread and butter and drink and drive , the words of interest are separated by a fixed number of words and there is very little variance .", "In the 1988 AP , it was found that the two words are always exactly two words apart whenever they are found near each other within five words .", "That is , the mean separation is two , and the variance is zero .", "Compounds also have very fixed word order little variance , but the average separation is closer to one word rather than two .", "In contrast , relations such as man woman are less fixed , as indicated by a larger variance in their separation .", "The nearly zero value for the mean separation for man women indicates the words appear about equally often in either order .", "Lexical relations come in several varieties .", "There are some like refraining from that are fairly fixed , others such as coming from that may be separated by an argument , and still others like keeping from that are almost certain to be separated by an argument .", "The ideal window size is different in each case .", "For the remainder of this paper , the window size , w , will be set to five words as a compromise ; this setting is large enough to show some of the constraints between verbs and arguments , but not so large that it would wash out constraints that make use of strict adjacency . 3 Since the association ratio becomes unstable when the counts are very small , we will not discuss word pairs with f x , y 5 .", "An improvement would make use of t scores , and throw out pairs that were not significant .", "Unfortunately , this requires an estimate of the variance off x , y , which goes beyond the scope of this paper .", "For the remainder of this paper , we will adopt the simple but arbitrary threshold , and ignore pairs with small counts .", "Technically , the association ratio is different from mutual information in two respects .", "First , joint probabilities are supposed to be symmetric P x , y P y , x , and thus , mutual information is also symmetric I x , y I y , x .", "However , the association ratio is not symmetric , since f x , y encodes linear precedence .", "Recall thatf x , y denotes the number of times that word x appears before y in the window of w words , not the number of times the two words appear in either order .", "Although we could fix this problem by redefining f x , y to be symmetric by averaging the matrix with its transpose , we have decided not to do so , since order information appears to be very interesting .", "Notice the asymmetry in the pairs in Table 2 computed from 44 million words of 1988 AP text , illustrating a wide variety of biases ranging from sexism to syntax .", "Second , one might expect f x , y f x and f x , y f y , but the way we have been counting , this needn't be the case if x and y happen to appear several times in the window .", "For example , given the sentence , quot ; Library workers were prohibited from saving books from this heap of ruins , quot ; which appeared in an AP story on April 1 , 1988 , f prohibited 1 and f prohibited , from 2 .", "This problem can be fixed by dividing f x , y by w 1 which has the consequence of subtracting log2 w 1 2 from our association ratio scores .", "This adjustment has the addif x , y f y , x doctors nurses 99 10 man woman 256 56 doctors lawyers 29 19 bread butter 15 1 save life 129 11 save money 187 11 save from 176 18 supposed to 1188 25 tional benefit of assuring that f x , y f x f Y N . When x , y is large , the association ratio produces very credible results not unlike those reported in Palermo and Jenkins 1964 , as illustrated in Table 3 .", "In contrast , when x , y 0 , the pairs are less interesting .", "As a very rough rule of thumb , we have observed that pairs with x , y 3 tend to be interesting , and pairs with smaller x , y are generally not .", "One can make this statement precise by calibrating the measure with subjective measures .", "Alternatively , one could make estimates of the variance and then make statements about confidence levels , e . g . with 95 confidence , P x , y P x P y .", "If x , y 0 , we would predict that x and y are in complementary distribution .", "However , we are rarely able to observe I x , y 0 because our corpora are too small and our measurement techniques are too crude .", "Suppose , for example , that both x and y appear about 10 times per million words of text .", "Then , P x P y 10 5 and chance is P x P x 10 1 .", "Thus , to say that x , y is much less than 0 , we need to say that P x , y is much less than 10 1 , a statement that is hard to make with much confidence given the size of presently available corpora .", "In fact , we cannot easily observe a probability less than 1 N , 10 7 , and therefore it is hard to know if x , y is much less than chance or not , unless chance is very large .", "In fact , the pair a . .", ". doctors in Table 3 , appears significantly less often than chance .", "But to justify this statement , we need to compensate for the window size which shifts the score downward by 2 . 0 , e . g . from 0 . 96 down to 1 . 04 , and we need to estimate the standard deviation , using a method such as Good 1953 . 4", "Although the psycholinguistic literature documents the significance of noun noun word associations such as doctor nurse in considerable detail , relatively little is said about associations among verbs , function words , adjectives , and other non nouns .", "In addition to identifying semantic relations of the doctor nurse variety , we believe the association ratio can also be used to search for interesting lexicosyntactic relationships between verbs and typical arguments adjuncts .", "The proposed association ratio can be viewed as a formalization of Sinclair's argument How common are the phrasal verbs with set ?", "Set is particularly rich in making combinations with words like about , in , up , out , on , off , and these words are themselves very common .", "How likely is set off to occur ?", "Both are frequent words set occurs approximately 250 times in a million words and off occurs approximately 556 times in a million words .", "T he question we are asking can be roughly rephrased as follows how likely is off to occur immediately after set ? .", "This is 0 . 00025 x 0 . 00055 P x P y , which gives us the tiny figure of 0 . 0000001375 .", "The assumption behind this calculation is that the words are distributed at random in a text at chance , in our terminology .", "It is obvious to a linguist that this is not so , and a rough measure of how much set and off attract each other is to compare the probability with what actually happens .", "Set off occurs nearly 70 times in the 7 . 3 million word corpus P x , y 70 7 . 3 x 106 P x P y .", "That is enough to show its main patterning and it suggests that in currently held corpora there will be found sufficient evidence for the description of a substantial collection of phrases .", "Sinclair 1987c , pp .", "Using Sinclair's estimates P set 250 x 10 6 , P off 556 x 10 6 , and P set , off 70 7 . 3 x 106 , we would estimate the mutual information to be I set ; off log2 P set , off 1 P set P off 6 . 1 .", "In the 1988 AP corpus N 44 , 344 , 077 , we estimate P set 13 , 046 N , P off 20 , 693 N , and P set , off 463 N .", "Given these estimates , we would compute the mutual information to be I set ; off 6 . 2 .", "In this example , at least , the values seem to be fairly comparable across corpora .", "In other examples , we will see some differences due to sampling .", "Sinclair's corpus is a fairly balanced sample of mainly British text ; the AP corpus is an unbalanced sample of American journalese .", "This association between set and off is relatively strong ; the joint probability is more than 26 64 times larger than chance .", "The other particles that Sinclair mentions have association ratios that can be seen in Table 4 .", "The first three , set up , set off and set out , are clearly associated ; the last three are not so clear .", "As Sinclair suggests , the approach is well suited for identifying the phrasal verbs , at least in certain cases .", "Phrasal verbs involving the preposition to raise an interesting problem because of the possible confusion with the infinitive marker to .", "We have found that if we first tag every word in the corpus with a part of speech using a method such as Church 1988 , and then measure associations between tagged words , we can identify interesting contrasts between verbs associated with a following preposition to in and verbs associated with a following infinitive marker to to .", "Part of speech notation is borrowed from Francis and Kucera 1982 ; in preposition ; to infinitive marker ; vb bare verb ; vbg verb ing ; vbd verb ed ; vbz verb s ; vbn verb en .", "The association ratio identifies quite a number of verbs associated in an interesting way with to ; restricting our attention to pairs with a score of 3 . 0 or more , there are 768 verbs associated with the preposition to in and 551 verbs with the infinitive marker to to .", "The ten verbs found to be most associated before to in are Thus , we see there is considerable leverage to be gained by preprocessing the corpus and manipulating the inventory of tokens .", "Hindle Church et al . 1989 has found it helpful to preprocess the input with the Fidditch parser Hindle 1983a , 1983b to identify associations between verbs and arguments , and postulate semantic classes for nouns on this basis .", "Hindle's method is able to find some very interesting associations , as Tables 5 and 6 demonstrate .", "After running his parser over the 1988 AP corpus 44 million words , Hindle found N 4 , 112 , 943 subject verb object SVO triples .", "The mutual information between a verb and its object was computed from these 4 million triples by counting how often the verb and its object were found in the same triple and dividing by chance .", "Thus , for example , disconnect V and telephone 0 have a joint probability of 7IN .", "In this case , chance is 84 N x 481 N because there are 84 SVO triples with the verb disconnect , and 481 SVO triples with the object telephone .", "The mutual information is log2 7N 84 x 481 9 . 48 .", "Similarly , the mutual information for drink V beer' 0 is 9 . 9 log2 29N 660 x 195 .", "drink IV and beer I 0 are found in 660 and Computational Linguistics Volume 16 , Number 1 , March 1990 25 Kenneth Church and Patrick Hanks Word Association Norms , Mutual Information , and Lexicography readers , which introduced an element of selectivity and so inevitably distortion rare words and uses were collected but common uses of common words were not , or on small corpora of only a million words or so , which are reliably informative for only the most common uses of the few most frequent words of English .", "A million word corpus such as the Brown Corpus is reliable , roughly , for only some uses of only some of the forms of around 4000 dictionary entries .", "But standard dictionaries typically contain twenty times this number of entries .", "The computational tools available for studying machinereadable corpora are at present still rather primitive .", "These are concordancing programs see Figure 1 , which are basically KWIC key word in context ; Aho et al . 1988 indexes with additional features such as the ability to extend the context , sort leftward as well as rightward , and so on .", "There is very little interactive software .", "In a typical situation in the lexicography of the 1980s , a lexicographer is given the concordances for a word , marks up the printout with colored pens to identify the salient senses , and then writes syntactic descriptions and definitions .", "Although this technology is a great improvement on using human readers to collect boxes of citation index cards the method Murray used in constructing The Oxford English Dictionary a century ago , it works well if there are no more than a few dozen concordance lines for a word , and only two or three main sense divisions .", "In analyzing a complex word such as take , save , or from , the lexicographer is trying to pick out significant patterns and subtle distinctions that are buried in literally thousands of concordance lines pages and pages of computer printout .", "The unaided human mind simply cannot discover all the signifi195 SVO triples , respectively ; they are found together in 29 of these triples .", "This application of Hindle's parser illustrates a second example of preprocessing the input to highlight certain constraints of interest .", "For measuring syntactic constraints , it may be useful to include some part of speech information and to exclude much of the internal structure of noun phrases .", "For other purposes , it may be helpful to tag items and or phrases with semantic labels such as person , place , time , body part , bad , and so on .", "Large machine readable corpora are only just now becoming available to lexicographers .", "Up to now , lexicographers have been reliant either on citations collected by human Table 6 .", "What Can You Do to a Telephone ?", "Verb Object Mutual Info Joint Freq sit_bylV telephone 0 11 . 78 7 disconnectIV telephone 0 9 . 48 7 answerIV telephone 0 8 . 80 98 hang_up1V telephone 0 7 . 87 3 tap1V telephone 0 7 . 69 15 pick_upIV telephone 0 5 . 63 11 return V telephone 0 5 . 01 19 be_bylV telephone 0 4 . 93 2 spotIV telephone 0 4 . 43 2 repeat1V telephone 0 4 . 39 3 placelV telephone 0 4 . 23 7 receivelV telephone 0 4 . 22 28 installIV telephone 0 4 . 20 2 be_onIV telephone 0 4 . 05 15 come_tolV telephone 0 3 . 63 6 uselV telephone 0 3 . 59 29 operatelV telephone 0 3 . 16 4 rs Sunday , calling for greater economic reforms to maniac ion asserted that quot ; the Postal Service could Then , she mid , the family hopes to e out of work steelworker , quot ; because that doesn't quot ; We suspend reality when we say we'll scientists has won the first round in an effort to about three children ma mining town who plot to GM executives say the shutdowns will Innen , as receiver , instructed officials to try to The package , which is to newly enhanced image as the moderate who moved to million offer from chaimun Victor Posner to help after telling a delivery room doctor not to try to h birthday Tuesday , cheered by those who fought to at he had formed an alliance with Moslem rebels to Basically we could We worked for a year to their estimative mirrors , just like in wanime , to ant of many who risked their own lives in order to We must increase the amount Americans save China front poverty . save enormous sums of money in contracting out individual c save enough for a down payment on a home . save jobs , that costs jobs .", "quot ; save money by spending 10 , 000 in wages for a public workt save one of Egypt's great treasures , the decaying tomb of ft save the quot ; pit ponies quot ; doomed lobe slaughtered . save the automaker 500 million a year in operating costs a save the company rather than liquidate it and then declared save the country nearly 2 billion , also includes a program save the country . save the financially troubled company , but said Posner till save the infant by inserting a tube in its throat to help i save the majestic Beaux Arts architectural masterpiece . save the nation from communism . save the operating costs of the Persbings and ground launch save the site at enormous expense to at , quot ; mid Leveillee . save diem from drunken Yankee brawlers , quot ; Tam sank save those who were passengers .", "quot ; cant patterns , let alone group them and rank them in order of importance .", "The AP 1987 concordance to save is many pages long ; there are 666 lines for the base form alone , and many more for the inflected forms saved , saves , saving , and savings .", "In the discussion that follows , we shall , for the sake of simplicity , not analyze the inflected forms and we shall only look at the patterns to the right of save see Table 7 .", "It is hard to know what is important in such a concordance and what is not .", "For example , although it is easy to see from the concordance selection in Figure 1 that the word quot ; to quot ; often comes before quot ; save quot ; and the word quot ; the quot ; often comes after quot ; save , quot ; it is hard to say from examination of a concordance alone whether either or both of these co occurrences have any significance .", "Two examples will illustrate how the association ratio measure helps make the analysis both quicker and more accurate .", "The association ratios in Table 7 show that association norms apply to function words as well as content words .", "For example , one of the words significantly associated with save is from .", "Many dictionaries , for example Webster's Ninth New Collegiate Dictionary Merriam Webster , make no explicit mention of from in the entry for save , although British learners' dictionaries do make specific mention of from in connection with save .", "These learners' dictionaries pay more attention to language structure and collocation than do American collegiate dictionaries , and lexicographers trained in the British tradition are often fairly skilled at spotting these generalizations .", "However , teasing out such facts and distinguishing true intuitions from false intuitions takes a lot of time and hard work , and there is a high probability of inconsistencies and omissions .", "Which other verbs typically associate with from , and where does save rank in such a list ?", "The association ratio identified 1530 words that are associated with from ; 911 of them were tagged as verbs .", "The first 100 verbs are refrain vb , gleaned vbn , stems vbz , stemmed vbd , stemming vbg , ranging vbg , stemmed vbn , ranged vbn , derived vbn , ranged vbd , extort vb , graduated vbd , barred vbn , benefiting vbg , benefitted vbn , benefited vbn , excused vbd , arising vbg , range vb , exempts vbz , suffers vbz , exempting vbg , benefited vbd , prevented vbd 7 . 0 , seeping vbg , barred vbd , prevents vbz , suffering vbg , excluded vbn , marks vbz , profiting vbg , recovering vbg , discharged vbn , rebounding vbg , vary vb , exempted vbn , separate vb , banished vbn , withdrawing vbg , ferry vb , prevented vbn , profit vb , bar vb , excused vbn , bars vbz , benefit vb , emerges vbz , emerge vb , varies vbz , differ vb , removed vbn , exempt vb , expelled vbn , withdraw vb , stem vb , separated vbn , judging vbg , adapted vbn , escaping vbg , inherited vbn , differed vbd , emerged vbd , withheld vbd , leaked vbn , strip vb , resulting vbg , discourage vb , prevent vb , withdrew vbd , prohibits vbz , borrowing vbg , preventing vbg , prohibit vb , resulted vbd 6 . 0 , preclude vb , divert vb , distinguish vb , pulled vbn , fell vbn , varied vbn , emerging vbg , suffer vb , prohibiting vbg , extract vb , subtract vb , recover vb , paralyzed vbn , stole vbd , departing vbg , escaped vbn , prohibited vbn , forbid vb , evacuated vbn , reap vb , barring vbg , removing vbg , stolen vbn , receives vbz .", "Save .", ". from is a good example for illustrating the advantages of the association ratio .", "Save is ranked 319th in this list , indicating that the association is modest , strong enough to be important 21 times more likely than chance , but not so strong that it would pop out at us in a concordance , or that it would be one of the first things to come to mind .", "If the dictionary is going to list save .", ". from , then , for consistency's sake , it ought to consider listing all of the more important associations as well .", "Of the 27 bare verbs tagged vb' in the list above , all but seven are listed in Collins Cobuild English Language Dictionary as occurring with from .", "However , this dictionary does not note that vary , ferry , strip , divert , forbid , and reap occur with from .", "If the Cobuild lexicographers had had access to the proposed measure , they could possibly have obtained better coverage at less cost .", "Having established the relative importance of save .", ". from , and having noted that the two words are rarely Computational Linguistics Volume 16 , Number 1 , March 1990 27 Kenneth Church and Patrick Hanks Word Association Norms , Mutual Information , and Lexicography adjacent , we would now like to speed up the labor intensive task of categorizing the concordance lines .", "Ideally , we would like to develop a set of semi automatic tools that would help a lexicographer produce something like Figure 2 , which provides an annotated summary of the 65 concordance lines for save .", ". from . 5 The save .", ". from pattern occurs in about 10 of the 666 concordance lines for save .", "Traditionally , semantic categories have been only vaguely recognized , and to date little effort has been devoted to a systematic classification of a large corpus .", "Lexicographers have tended to use concordances impressionistically ; semantic theorists , AI ers , and others have concentrated on a few interesting examples , e . g . bachelor , and have not given much thought to how the results might be scaled up .", "With this concern in mind , it seems reasonable to ask how well these 65 lines for save .", ". from fit in with all other uses of save A laborious concordance analysis was undertaken to answer this question .", "When it was nearing completion , we noticed that the tags that we were inventing to capture the generalizations could in most cases have been suggested by looking at the lexical items listed in the association ratio table for save .", "For example , we had failed to notice the significance of time adverbials in our analysis of save , and no dictionary records this .", "Yet it should be rescuers who helped save the toddler PERSON from an abandoned well LOCI will be feted with a parade while anempting to save two drowning boys PERSONI from a turbulem BAD creek LOC m Ohio LOC member states to help save the PEC INST from possible bankruptcy ECOM RADI this pane . should be sought quot ; to save the company CORKINST from bankruptcy ECONNBAD1 . law was necessary to save the country NATION INSTE from disaster BAD . operation quot ; to save the nation NATION INSTB from Communism BADUOLITICALI were not needed to save the system from bankruptcy ECON tBADI . his efforts to save the world lNST from the likes of Lothar and the Spider Woman give them the money to save the dogs ANIMAL from being destroyed DESTRUCT , program intended to save the giant birds ANIMAL from ereinction DESTRUCT , UNCLASSIFIED 10 concordance lines walnut and ash trees to save them from the axes and saws of a logging company . after the attack to save the ship from a terrible BAD fire , Navy reports concluded Thursday . certificates that would save shoppers PERSON anywhere from 50 MONEY1 NUMBER to 500 MONEY NU Figure 2 Some AP 1987 Concordance Lines to quot ; save .", ". from , quot ; Roughly Sorted into Categories . clear from the association ratio table above that annually and month6 are commonly found with save .", "More detailed inspection shows that the time adverbials correlate interestingly with just one group of save objects , namely those tagged MONEY .", "The AP wire is full of discussions of saving 1 . 2 billion per month ; computational lexicography should measure and record such patterns if they are general , even when traditional dictionaries do not .", "As another example illustrating how the association ratio tables would have helped us analyze the save concordance lines , we found ourselves contemplating the semantic tag ENV IRONMENT to analyze lines such as the trend to save the forests ENV it's our turn to save the lake ENV , joined a fight to save their forests ENV , can we get busy to save the planet ENV ?", "If we had looked at the association ratio tables before labeling the 65 lines for save .", ". from , we might have noticed the very large value for save .", ". forests , suggesting that there may be an important pattern here .", "In fact , this pattern probably subsumes most of the occurrences of the quot ; save ANIMAL quot ; pattern noticed in Figure 2 .", "Thus , these tables do not provide semantic tags , but they provide a powerful set of suggestions to the lexicographer for what needs to be accounted for in choosing a set of semantic tags .", "It may be that everything said here about save and other words is true only of 1987 American journalese .", "Intuitively , however , many of the patterns discovered seem to be good candidates for conventions of general English .", "A future step would be to examine other more balanced corpora and test how well the patterns hold up .", "We began this paper with the psycholinguistic notion of word association norm , and extended that concept toward the information theoretic definition of mutual information .", "This provided a precise statistical calculation that could be applied to a very large corpus of text to produce a table of associations for tens of thousands of words .", "We were then able to show that the table encoded a number of very interesting patterns ranging from doctor .", ". nurse to save .", ". from .", "We finally concluded by showing how the patterns in the association ratio table might help a lexicographer organize a concordance .", "In point of fact , we actually developed these results in basically the reverse order .", "Concordance analysis is still extremely labor intensive and prone to errors of omission .", "The ways that concordances are sorted don't adequately support current lexicographic practice .", "Despite the fact that a concordance is indexed by a single word , often lexicographers actually use a second word such as from or an equally common semantic concept such as a time adverbial to decide how to categorize concordance lines .", "In other words , they use two words to triangulate in on a word sense .", "This triangulation approach clusters concordance lines together into word senses based primarily on usage distributional evidence , as opposed to intuitive notions of meaning .", "Thus , the question of what is a word sense can be addressed with syntactic methods symbol pushing , and need not address semantics interpretation , even though the inventory of tags may appear to have semantic values .", "The triangulation approach requires quot ; art . quot ; How does the lexicographer decide which potential cut points are quot ; interesting quot ; and which are merely due to chance ?", "The proposed association ratio score provides a practical and objective measure that is often a fairly good approximation to the quot ; art . quot ; Since the proposed measure is objective , it can be applied in a systematic way over a large body of material , steadily improving consistency and productivity .", "But on the other hand , the objective score can be misleading .", "The score takes only distributional evidence into account .", "For example , the measure favors set .", ". for over set .", ". down ; it doesn't know that the former is less interesting because its semantics are compositional .", "In addition , the measure is extremely superficial ; it cannot cluster words into appropriate syntactic classes without an explicit preprocess such as Church's parts program or Hindle's parser .", "Neither of these preprocesses , though , can help highlight the quot ; natural quot ; similarity between nouns such as picture and photograph .", "Although one might imagine a preprocess that would help in this particular case , there will probably always be a class of generalizations that are obvious to an intelligent lexicographer , but lie hopelessly beyond the objectivity of a computer .", "Despite these problems , the association ratio could be an important tool to aid the lexicographer , rather like an index to the concordances .", "It can help us decide what to look for ; it provides a quick summary of what company our words do keep ."], "summary_lines": ["Word Association Norms Mutual Information And Lexicography\n", "The term word association is used in a very particular sense in the psycholinguistic literature.\n", "(Generally speaking, subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor.)\n", "We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word) to lexico-syntactic co-occurrence constraints between verbs and prepositions (content word/function word).\n", "This paper will propose an objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora.\n", "(The standard method of obtaining word association norms, testing a few thousand :mbjects on a few hundred words, is both costly and unreliable.)\n", "The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, making it possible to estimate norms for tens of thousands of words.\n", "In our work, the significance of an association (x, y) is measured by the mutual information I (x, y) ,i.e. the probability of observing x and y together, compared with the probability of observing x and y independently.\n"]}
{"article_lines": ["Online Learning Of Approximate Dependency Parsing Algorithms", "In this paper we extend the maximum spanning tree MST dependency parsing framework of McDonald et al . 2005c to incorporate higher order feature representations and allow dependency structures with multiple parents per word .", "We show that those extensions can make the MST framework computationally intractable , but that the intractability can be circumvented with new approximate parsing algorithms .", "We conclude with experiments showing that discriminative online learning using those approximate algorithms achieves the best reported parsing accuracy for Czech and Danish .", "Dependency representations of sentences Hudson , 1984 ; Me l\u02c7cuk , 1988 model head dependent syntactic relations as edges in a directed graph .", "Figure 1 displays a dependency representation for the sentence John hit the ball with the bat .", "This sentence is an example of a projective or nested tree representation , in which all edges can be drawn in the plane with none crossing .", "Sometimes a non projective representations are preferred , as in the sentence in Figure 2 . 1 In particular , for freer word order languages , non projectivity is a common phenomenon since the relative positional constraints on dependents is much less rigid .", "The dependency structures in Figures 1 and 2 satisfy the tree constraint they are weakly connected graphs with a unique root node , and each non root node has a exactly one parent .", "Though trees are more common , some formalisms allow for words to modify multiple parents Hudson , 1984 .", "Recently , McDonald et al . 2005c have shown that treating dependency parsing as the search for the highest scoring maximum spanning tree MST in a graph yields efficient algorithms for both projective and non projective trees .", "When combined with a discriminative online learning algorithm and a rich feature set , these models provide state of the art performance across multiple languages .", "However , the parsing algorithms require that the score of a dependency tree factors as a sum of the scores of its edges .", "This first order factorization is very restrictive since it only allows for features to be defined over single attachment decisions .", "Previous work has shown that conditioning on neighboring decisions can lead to significant improvements in accuracy Yamada and Matsumoto , 2003 ; Charniak , 2000 .", "In this paper we extend the MST parsing framework to incorporate higher order feature representations of bounded size connected subgraphs .", "We also present an algorithm for acyclic dependency graphs , that is , dependency graphs in which a word may depend on multiple heads .", "In both cases parsing is in general intractable and we provide novel approximate algorithms to make these cases tractable .", "We evaluate these algorithms within an online learning framework , which has been shown to be robust with respect approximate inference , and describe experiments displaying that these new models lead to state of the art accuracy for English and the best accuracy we know of for Czech and Danish .", "Dependency tree parsing as the search for the maximum spanning tree MST in a graph was proposed by McDonald et al . 2005c .", "This formulation leads to efficient parsing algorithms for both projective and non projective dependency trees with the Eisner algorithm Eisner , 1996 and the Chu Liu Edmonds algorithm Chu and Liu , 1965 ; Edmonds , 1967 respectively .", "The formulation works by defining the score of a dependency tree to be the sum of edge scores , where x x1 xn is an input sentence and y a dependency tree for x .", "We can view y as a set of tree edges and write i , j E y to indicate an edge in y from word xi to word xj .", "Consider the example from Figure 1 , where the subscripts index the nodes of the tree .", "The score of this tree would then be , We call this first order dependency parsing since scores are restricted to a single edge in the dependency tree .", "The score of an edge is in turn computed as the inner product of a high dimensional feature representation of the edge with a corresponding weight vector , This is a standard linear classifier in which the weight vector w are the parameters to be learned during training .", "We should note that f i , j can be based on arbitrary features of the edge and the input sequence x .", "Given a directed graph G V , E , the maximum spanning tree MST problem is to find the highest scoring subgraph of G that satisfies the tree constraint over the vertices V .", "By defining a graph in which the words in a sentence are the vertices and there is a directed edge between all words with a score as calculated above , McDonald et al . 2005c showed that dependency parsing is equivalent to finding the MST in this graph .", "Furthermore , it was shown that this formulation can lead to state of the art results when combined with discriminative learning algorithms .", "Although the MST formulation applies to any directed graph , our feature representations and one of the parsing algorithms Eisner s rely on a linear ordering of the vertices , namely the order of the words in the sentence .", "Restricting scores to a single edge in a dependency tree gives a very impoverished view of dependency parsing .", "Yamada and Matsumoto 2003 showed that keeping a small amount of parsing history was crucial to improving parsing performance for their locally trained shift reduce SVM parser .", "It is reasonable to assume that other parsing models might benefit from features over previous decisions .", "Here we will focus on methods for parsing second order spanning trees .", "These models factor the score of the tree into the sum of adjacent edge pair scores .", "To quantify this , consider again the example from Figure 1 .", "In the second order spanning tree model , the score would be , Here we use the second order score function s i , k , j , which is the score of creating a pair of adjacent edges , from word xi to words xk and xj .", "For instance , s 2 , 4 , 5 is the score of creating the edges from hit to with and from hit to ball .", "The score functions are relative to the left or right of the parent and we never score adjacent edges that are on different sides of the parent for instance , there is no s 2 , 1 , 4 for the adjacent edges from hit to John and ball .", "This independence between left and right descendants allow us to use a O n3 second order projective parsing algorithm , as we will see later .", "We write s xi , , xj when xj is the first left or first right dependent of word xi .", "For example , s 2 , , 4 is the score of creating a dependency from hit to ball , since ball is the first child to the right of hit .", "More formally , if the word xi0 has the children shown in this picture , This second order factorization subsumes the first order factorization , since the score function could just ignore the middle argument to simulate first order scoring .", "The score of a tree for secondorder parsing is now where k and j are adjacent , same side children of i in the tree y .", "The second order model allows us to condition on the most recent parsing decision , that is , the last dependent picked up by a particular word , which is analogous to the the Markov conditioning of in the Charniak parser Charniak , 2000 .", "For projective MST parsing , the first order algorithm can be extended to the second order case , as was noted by Eisner 1996 .", "The intuition behind the algorithm is shown graphically in Figure 3 , which displays both the first order and secondorder algorithms .", "In the first order algorithm , a word will gather its left and right dependents independently by gathering each half of the subtree rooted by its dependent in separate stages .", "By splitting up chart items into left and right components , the Eisner algorithm only requires 3 indices to be maintained at each step , as discussed in detail elsewhere Eisner , 1996 ; McDonald et al . , 2005b .", "For the second order algorithm , the key insight is to delay the scoring of edges until pairs 2 order non proj approx x , s Sentence x x0 . . . xn , x0 root Weight function s i , k , j R of dependents have been gathered .", "This allows for the collection of pairs of adjacent dependents in a single stage , which allows for the incorporation of second order scores , while maintaining cubictime parsing .", "The Eisner algorithm can be extended to an arbitrary mth order model with a complexity of O nm 1 , for m 1 .", "An mth order parsing algorithm will work similarly to the second order algorithm , except that we collect m pairs of adjacent dependents in succession before attaching them to their parent .", "Unfortunately , second order non projective MST parsing is NP hard , as shown in appendix A .", "To circumvent this , we designed an approximate algorithm based on the exact O n3 second order projective Eisner algorithm .", "The approximation works by first finding the highest scoring projective parse .", "It then rearranges edges in the tree , one at a time , as long as such rearrangements increase the overall score and do not violate the tree constraint .", "We can easily motivate this approximation by observing that even in non projective languages like Czech and Danish , most trees are primarily projective with just a few non projective edges Nivre and Nilsson , 2005 .", "Thus , by starting with the highest scoring projective tree , we are typically only a small number of transformations away from the highest scoring non projective tree .", "The algorithm is shown in Figure 4 .", "The expression y i j denotes the dependency graph identical to y except that xi s parent is xi instead shows how h1 creates a dependency to h3 with the second order knowledge that the last dependent of h1 was h2 .", "This is done through the creation of a sibling item in part B .", "In the first order model , the dependency to h3 is created after the algorithm has forgotten that h2 was the last dependent . of what it was in y .", "The test tree y is true iff the dependency graph y satisfies the tree constraint .", "In more detail , line 1 of the algorithm sets y to the highest scoring second order projective tree .", "The loop of lines 2 16 exits only when no further score improvement is possible .", "Each iteration seeks the single highest scoring parent change to y that does not break the tree constraint .", "To that effect , the nested loops starting in lines 4 and 5 enumerate all i , j pairs .", "Line 6 sets y' to the dependency graph obtained from y by changing xj s parent to xi .", "Line 7 checks that the move from y to y' is valid by testing that xj s parent was not already xi and that y' is a tree .", "Line 8 computes the score change from y to y' .", "If this change is larger than the previous best change , we record how this new tree was created lines 9 10 .", "After considering all possible valid edge changes to the tree , the algorithm checks to see that the best new tree does have a higher score .", "If that is the case , we change the tree permanently and re enter the loop .", "Otherwise we exit since there are no single edge switches that can improve the score .", "This algorithm allows for the introduction of non projective edges because we do not restrict any of the edge changes except to maintain the tree property .", "In fact , if any edge change is ever made , the resulting tree is guaranteed to be nonprojective , otherwise there would have been a higher scoring projective tree that would have already been found by the exact projective parsing algorithm .", "It is not difficult to find examples for which this approximation will terminate without returning the highest scoring non projective parse .", "It is clear that this approximation will always terminate there are only a finite number of dependency trees for any given sentence and each iteration of the loop requires an increase in score to continue .", "However , the loop could potentially take exponential time , so we will bound the number of edge transformations to a fixed value M . It is easy to argue that this will not hurt performance .", "Even in freer word order languages such as Czech , almost all non projective dependency trees are primarily projective , modulo a few nonprojective edges .", "Thus , if our inference algorithm starts with the highest scoring projective parse , the best non projective parse only differs by a small number of edge transformations .", "Furthermore , it is easy to show that each iteration of the loop takes O n2 time , resulting in a O n3 Mn2 runtime algorithm .", "In practice , the approximation terminates after a small number of transformations and we do not need to bound the number of iterations in our experiments .", "We should note that this is one of many possible approximations we could have made .", "Another reasonable approach would be to first find the highest scoring first order non projective parse , and then re arrange edges based on second order scores in a similar manner to the algorithm we described .", "We implemented this method and found that the results were slightly worse .", "Kromann 2001 argued for a dependency formalism called Discontinuous Grammar and annotated a large set of Danish sentences using this formalism to create the Danish Dependency Treebank Kromann , 2003 .", "The formalism allows for a word to have multiple parents .", "Examples include verb coordination in which the subject or object is an argument of several verbs , and relative clauses in which words must satisfy dependencies both inside and outside the clause .", "An example is shown in Figure 5 for the sentence He looks for and sees elephants .", "Here , the pronoun He is the subject for both verbs in the sentence , and the noun elephants the corresponding object .", "In the Danish Dependency Treebank , roughly 5 of words have more than one parent , which breaks the single parent or tree constraint we have previously required on dependency structures .", "Kromann also allows for cyclic dependencies , though we deal only with acyclic dependency graphs here .", "Though less common than trees , dependency graphs involving multiple parents are well established in the literature Hudson , 1984 .", "Unfortunately , the problem of finding the dependency structure with highest score in this setting is intractable Chickering et al . , 1994 .", "To create an approximate parsing algorithm for dependency structures with multiple parents , we start with our approximate second order nonprojective algorithm outlined in Figure 4 .", "We use the non projective algorithm since the Danish Dependency Treebank contains a small number of non projective arcs .", "We then modify lines 7 10 of this algorithm so that it looks for the change in parent or the addition of a new parent that causes the highest change in overall score and does not create a cycle2 .", "Like before , we make one change per iteration and that change will depend on the resulting score of the new tree .", "Using this simple new approximate parsing algorithm , we train a new parser that can produce multiple parents .", "In this section , we review the work of McDonald et al . 2005b for online large margin dependency parsing .", "As usual for supervised learning , we assume a training set T xt , yt Tt 1 , consisting of pairs of a sentence xt and its correct dependency representation yt .", "The algorithm is an extension of the Margin Infused Relaxed Algorithm MIRA Crammer and Singer , 2003 to learning with structured outputs , in the present case dependency structures .", "Figure 6 gives pseudo code for the algorithm .", "An online learning algorithm considers a single training instance for each update to the weight vector w . We use the common method of setting the final weight vector as the average of the weight vectors after each iteration Collins , 2002 , which has been shown to alleviate overfitting .", "On each iteration , the algorithm considers a single training instance .", "We parse this instance to obtain a predicted dependency graph , and find the smallest norm update to the weight vector w that ensures that the training graph outscores the predicted graph by a margin proportional to the loss of the predicted graph relative to the training graph , which is the number of words with incorrect parents in the predicted tree McDonald et al . , 2005b .", "Note that we only impose margin constraints between the single highest scoring graph and the correct graph relative to the current weight setting .", "Past work on tree structured outputs has used constraints for the k best scoring tree McDonald et al . , 2005b or even all possible trees by using factored representations Taskar et al . , 2004 ; McDonald et al . , 2005c .", "However , we have found that a single margin constraint per example leads to much faster training with a negligible degradation in performance .", "Furthermore , this formulation relates learning directly to inference , which is important , since we want the model to set weights relative to the errors made by an approximate inference algorithm .", "This algorithm can thus be viewed as a large margin version of the perceptron algorithm for structured outputs Collins 2002 .", "Online learning algorithms have been shown to be robust even with approximate rather than exact inference in problems such as word alignment Moore , 2005 , sequence analysis Daum e and Marcu , 2005 ; McDonald et al . , 2005a and phrase structure parsing Collins and Roark , 2004 .", "This robustness to approximations comes from the fact that the online framework sets weights with respect to inference .", "In other words , the learning method sees common errors due to where y' arg maxy , s xt , y' ; w i approximate inference and adjusts weights to correct for them .", "The work of Daum e and Marcu 2005 formalizes this intuition by presenting an online learning framework in which parameter updates are made directly with respect to errors in the inference algorithm .", "We show in the next section that this robustness extends to approximate dependency parsing .", "The score of adjacent edges relies on the definition of a feature representation f i , k , j .", "As noted earlier , this representation subsumes the first order representation of McDonald et al . 2005b , so we can incorporate all of their features as well as the new second order features we now describe .", "The old first order features are built from the parent and child words , their POS tags , and the POS tags of surrounding words and those of words between the child and the parent , as well as the direction and distance from the parent to the child .", "The second order features are built from the following conjunctions of word and POS identity predicates xi pos , xk pos , xj pos xk pos , xj pos xk word , xj word xk word , xj pos xk pos , xj word where xi pos is the part of speech of the ith word in the sentence .", "We also include conjunctions between these features and the direction and distance from sibling j to sibling k . We determined the usefulness of these features on the development set , which also helped us find out that features such as the POS tags of words between the two siblings would not improve accuracy .", "We also ignored features over triples of words since this would explode the size of the feature space .", "We evaluate dependencies on per word accuracy , which is the percentage of words in the sentence with the correct parent in the tree , and on complete dependency analysis .", "In our evaluation we exclude punctuation for English and include it for Czech and Danish , which is the standard .", "To create data sets for English , we used the Yamada and Matsumoto 2003 head rules to extract dependency trees from the WSJ , setting sections 2 21 as training , section 22 for development and section 23 for evaluation .", "The models rely on part of speech tags as input and we used the Ratnaparkhi 1996 tagger to provide these for the development and evaluation set .", "These data sets are exclusively projective so we only compare the projective parsers using the exact projective parsing algorithms .", "The purpose of these experiments is to gauge the overall benefit from including second order features with exact parsing algorithms , which can be attained in the projective setting .", "Results are shown in Table 1 .", "We can see that there is clearly an advantage in introducing second order features .", "In particular , the complete tree metric is improved considerably .", "For the Czech data , we used the predefined training , development and testing split of the Prague Dependency Treebank Haji\u02c7c et al . , 2001 , and the automatically generated POS tags supplied with the data , which we reduce to the POS tag set from Collins et al . 1999 .", "On average , 23 of the sentences in the training , development and test sets have at least one non projective dependency , though , less than 2 of total edges are actually non projective .", "Results are shown in Table 2 .", "McDonald et al . 2005c showed a substantial improvement in accuracy by modeling nonprojective edges in Czech , shown by the difference between two first order models .", "Table 2 shows that a second order model provides a comparable accuracy boost , even using an approximate non projective algorithm .", "The second order nonprojective model accuracy of 85 . 2 is the highest reported accuracy for a single parser for these data .", "Similar results were obtained by Hall and N ov ak 2005 85 . 1 accuracy who take the best output of the Charniak parser extended to Czech and rerank slight variations on this output that introduce non projective edges .", "However , this system relies on a much slower phrase structure parser as its base model as well as an auxiliary reranking module .", "Indeed , our second order projective parser analyzes the test set in 16m32s , and the non projective approximate parser needs 17m03s to parse the entire evaluation set , showing that runtime for the approximation is completely dominated by the initial call to the second order projective algorithm and that the post process edge transformation loop typically only iterates a few times per sentence .", "For our experiments we used the Danish Dependency Treebank v1 . 0 .", "The treebank contains a small number of inter sentence and cyclic dependencies and we removed all sentences that contained such structures .", "The resulting data set contained 5384 sentences .", "We partitioned the data into contiguous 80 20 training testing splits .", "We held out a subset of the training data for development purposes .", "We compared three systems , the standard second order projective and non projective parsing models , as well as our modified second order non projective model that allows for the introduction of multiple parents Section 3 .", "All systems use gold standard part of speech since no trained tagger is readily available for Danish .", "Results are shown in Figure 3 .", "As might be expected , the nonprojective parser does slightly better than the projective parser because around 1 of the edges are non projective .", "Since each word may have an arbitrary number of parents , we must use precision and recall rather than accuracy to measure performance .", "This also means that the correct training loss is no longer the Hamming loss .", "Instead , we use false positives plus false negatives over edge decisions , which balances precision and recall as our ultimate performance metric .", "As expected , for the basic projective and nonprojective parsers , recall is roughly 5 lower than precision since these models can only pick up at most one parent per word .", "For the parser that can introduce multiple parents , we see an increase in recall of nearly 3 absolute with a slight drop in precision .", "These results are very promising and further show the robustness of discriminative online learning with approximate parsing algorithms .", "We described approximate dependency parsing algorithms that support higher order features and multiple parents .", "We showed that these approximations can be combined with online learning to achieve fast parsing with competitive parsing accuracy .", "These results show that the gain from allowing richer representations outweighs the loss from approximate parsing and further shows the robustness of online learning algorithms with approximate inference .", "The approximations we have presented are very simple .", "They start with a reasonably good baseline and make small transformations until the score of the structure converges .", "These approximations work because freer word order languages we studied are still primarily projective , making the approximate starting point close to the goal parse .", "However , we would like to investigate the benefits for parsing of more principled approaches to approximate learning and inference techniques such as the learning as search optimization framework of Daum e and Marcu , 2005 .", "This framework will possibly allow us to include effectively more global features over the dependency structure than those in our current second order model .", "This work was supported by NSF ITR grants 0205448 ."], "summary_lines": ["Online Learning Of Approximate Dependency Parsing Algorithms\n", "In this paper we extend the maximum spanning tree (MST) dependency parsing framework of McDonald et al. (2005c) to incorporate higher-order feature representations and allow dependency structures with multiple parents per word.\n", "We show that those extensions can make the MST framework computationally intractable, but that the intractability can be circumvented with new approximate parsing algorithms.\n", "We conclude with experiments showing that discriminative online learning using those approximate algorithms achieves the best reported parsing accuracy for Czech and Danish.\n", "We propose a second-order graph-based dependency parsing model which incorporates features from the two kinds of subtrees.\n", "We use the Viterbi decoding algorithm to achieve O (n3) parsing time.\n", "We show that non-projective dependency parsing with horizontal Markovization is FNP-hard.\n", "We define a second-order dependency parsing model in which interactions between adjacent siblings are allowed.\n"]}
{"article_lines": ["A Decoder For Syntax Based Statistical MT", "This paper describes a decoding algorithm for a syntax based translation model Yamada and Knight , 2001 .", "The model has been extended to incorporate phrasal translations as presented here .", "In contrast to a conventional word to word statistical model , a decoder for the syntaxbased model builds up an English parse tree given a sentence in a foreign language .", "As the model size becomes huge in a practical setting , and the decoder considers multiple syntactic structures for each word alignment , several pruning techniques are necessary .", "We tested our decoder in a Chinese to English translation system , and obtained better results than IBM Model 4 .", "We also discuss issues concerning the relation between this decoder and a language model .", "A statistical machine translation system based on the noisy channel model consists of three components a language model LM , a translation model TM , and a decoder .", "For a system which translates from a foreign language to English , the LM gives a prior probability P and the TM gives a channel translation probability P .", "These models are automatically trained using monolingual for the LM and bilingual for the TM corpora .", "A decoder then finds the best English sentence given a foreign are not simple probability tables but are parameterized models , a decoder must conduct a search over the space defined by the models .", "For the IBM models defined by a pioneering paper Brown et al . , 1993 , a decoding algorithm based on a left to right search was described in Berger et al . , 1996 .", "Recently Yamada and Knight , 2001 introduced a syntax based TM which utilized syntactic structure in the channel input , and showed that it could outperform the IBM model in alignment quality .", "In contrast to the IBM models , which are word to word models , the syntax based model works on a syntactic parse tree , so the decoder builds up an English parse tree given a sentencein a foreign language .", "This paper describes an algorithm for such a decoder , and reports experimental results .", "Other statistical machine translation systems such as Wu , 1997 and Alshawi et al . , 2000 also produce a tree given a sentence .", "Their models are based on mechanisms that generate two languages at the same time , so an English tree is obtained as a subproduct of parsing .", "However , their use of the LM is not mathematically motivated , since their models do not decompose into P and unlike the noisy channel model .", "Section 2 briefly reviews the syntax based TM , and Section 3 describes phrasal translation as an extension .", "Section 4 presents the basic idea for decoding .", "As in other statistical machine translation systems , the decoder has to cope with a huge search sentence that maximizes P , which also maximizes P according to Bayes rule .", "A different decoder is needed for different choices of LM and TM .", "Since P and P space .", "Section 5 describes how to prune the search space for practical decoding .", "Section 6 shows experimental results .", "Section 7 discusses LM issues , and is followed by conclusions .", "The syntax based TM defined by Yamada and Knight , 2001 assumes an English parse tree as a channel input .", "The channel applies three kinds of stochastic operations on each node reordering children nodes , inserting an optional extra word to the left or right of the node , and translating leaf words . 1 These operations are independent of each other and are conditioned on the features , , of the node .", "Figure 1 shows an example .", "The child node sequence of the top node VB is reordered from PRP VB1 VB2 into PRP VB2 VB1 as seen in the second tree Reordered .", "An extra word ha is inserted at the leftmost node PRP as seen in the third tree Inserted .", "The English word He under the same node is translated into a foreign word kare as seen in the fourth tree Translated .", "After these operations , the channel emits a foreign word sentenceby taking the leaves of the modified tree .", "Formally , the channel probability P is where , , and is a sequence of leaf words of a tree transformed byfrom .", "The model tables , , and are called the r table , n table , and t table , respectively .", "These tables contain the probabilities of the channel operations , , conditioned by the features , , .", "In Figure 1 , the r table specifies the probability of having the second tree Reordered given the first tree .", "The n table specifies the probability of having the third tree Inserted given the second tree .", "The t table specifies the probability of having the fourth tree Translated given the third tree .", "The probabilities in the model tables are automatically obtained by an EM algorithm using pairs of channel input and channel output as a training corpus .", "Usually a bilingual corpus comes as pairs of translation sentences , so we need to parse the corpus .", "As we need to parse sentences on the channel input side only , many X to English translation systems can be developed with an English parser alone .", "The conditioning features , , can be anything that is available on a tree , however they should be carefully selected not to cause datasparseness problems .", "Also , the choice of features may affect the decoding algorithm .", "In our experiment , a sequence of the child node label was used for , a pair of the node label and the parent label was used for , and the identity of the English word is used for .", "For example , PPRP VB2 VB1PRP VB1 VB2 for the top node in Figure 1 .", "Similarly for the node PRP , Pright , haVB PRPand Pkarehe .", "More detailed examples are found in Yamada and Knight , 2001 .", "In Yamada and Knight , 2001 , the translationis a 1 to 1 lexical translation from an English wordto a foreign word , i . e . , .", "To allow non 1 to 1 translation , such as for idiomatic phrases or compound nouns , we extend the model as follows .", "First we use fertility as used in IBM models to allow 1 to N mapping .", "For N to N mapping , we allow direct translation of an English phrase to a foreign phrase at non terminal tree nodes as if is non terminal .", "In practice , the phrase lengths , are limited to reduce the model size .", "In our experiment Section 5 , we restricted them as , to avoid pairs of extremely different lengths .", "This formula was obtained by randomly sampling the length of translation pairs .", "See Yamada , 2002 for details .", "Our statistical MT system is based on the noisychannel model , so the decoder works in the reverse direction of the channel .", "Given a supposed channel output e . g . , a French or Chinese sentence , it will find the most plausible channel input an English parse tree based on the model parameters and the prior probability of the input .", "In the syntax based model , the decoder s task is to find the most plausible English parse tree given an observed foreign sentence .", "Since the task is to build a tree structure from a string of words , we can use a mechanism similar to normal parsing , which builds an English parse tree from a string of English words .", "Here we need to build an English parse tree from a string of foreign e . g . , French or Chinese words .", "To parse in such an exotic way , we start from an English context free grammar obtained from the training corpus , 2 and extend the grammar to incorporate the channel operations in the translation model .", "For each non lexical rule in the original English grammar such as VP VB NP PP , we supplement it with reordered rules e . g .", "VP NP PP VB , VP NP VB PP , etc . and associate them with the original English order and the reordering probability from the r table .", "Similarly , rules such as VP VP X and X word are added for extra word insertion , and they are associated with a probability from the n table .", "For each lexical rule in the English grammar , we add rules such as englishWord foreignWord with a probability from the t table .", "Now we can parse a string of foreign words and build up a tree , which we call a decoded tree .", "An example is shown in Figure 2 .", "The decoded tree is built up in the foreign language word order .", "To obtain a tree in the English order , we apply the reverse of the reorder operation back reordering using the information associated to the rule expanded by the r table .", "In Figure 2 , the numbers in the dashed oval near the top node shows the original english order .", "Then , we obtain an English parse tree by removing the leaf nodes foreign words from the backreordered tree .", "Among the possible decoded trees , we pick the best tree in which the product of the LM probability the prior probability of the English tree and the TM probability the probabilities associated pairs of English parse trees and foreign sentences . with the rules in the decoded tree is the highest .", "The use of an LM needs consideration .", "Theoretically we need an LM which gives the prior probability of an English parse tree .", "However , we can approximate it with an n gram LM , which is wellstudied and widely implemented .", "We will discuss this point later in Section 7 .", "If we use a trigram model for the LM , a convenient implementation is to first build a decodedtree forest and then to pick out the best tree using a trigram based forest ranking algorithm as described in Langkilde , 2000 .", "The ranker uses two leftmost and rightmost leaf words to efficiently calculate the trigram probability of a subtree , and finds the most plausible tree according to the trigram and the rule probabilities .", "This algorithm finds the optimal tree in terms of the model probability but it is not practical when the vocabulary size and the rule size grow .", "The next section describes how to make it practical .", "We use our decoder for Chinese English translation in a general news domain .", "The TM becomes very huge for such a domain .", "In our experiment see Section 6 for details , there are about 4M non zero entries in the trained table .", "About 10K CFG rules are used in the parsed corpus of English , which results in about 120K non lexical rules for the decoding grammar after we expand the CFG rules as described in Section 4 .", "We applied the simple algorithm from Section 4 , but this experiment failed no complete translations were produced .", "Even four word sentences could not be decoded .", "This is not only because the model size is huge , but also because the decoder considers multiple syntactic structures for the same word alignment , i . e . , there are several different decoded trees even when the translation of the sentence is the same .", "We then applied the following measures to achieve practical decoding .", "The basic idea is to use additional statistics from the training corpus . beam search We give up optimal decoding by using a standard dynamic programming parser with beam search , which is similar to the parser used in Collins , 1999 .", "A standard dynamicprogramming parser builds upnonterminal , inputsubstring tuples from bottom up according to the grammar rules .", "When the parsing cost3 comes only from the features within a subtree TM cost , in our case , the parser will find the optimal tree by keeping the single best subtree for each tuple .", "When the cost depends on the features outside of a subtree , we need to keep all the subtrees for possible different outside features boundary words for the trigram LM cost to obtain the optimal tree .", "Instead of keeping all the subtrees , we only retain subtrees within a beam width for each input substring .", "Since the outside features are not considered for the beam pruning , the optimality of the parse is not guaranteed , but the required memory size is reduced . t table pruning Given a foreign Chinese sentence to the decoder , we only consider English wordsfor each foreign word such that P is high .", "In addition , only limited part of speech labels are considered to reduce the number of possible decoded tree structures .", "Thus we only use the top 5 , pairs ranked by Notice that P is a model parameter , and that Pand P are obtained from the parsed training corpus . phrase pruning We only consider limited pairs Section 2 .", "The pair must appear more than once in the Viterbi alignments4 of the training corpus .", "Then we use the top 10 pairs ranked similarly to t table pruning above , except we replace PP with P and use trigrams to estimate P .", "By this pruning , we effectively remove junk phrase pairs , most of which come from misaligned sentences or untranslated phrases in the training corpus . r table pruning To reduce the number of rules for the decoding grammar , we use the top N rules ranked by PrulePreord so that PrulePreord , where Pruleis a prior probability of the rule in the original English order found in the parsed English corpus , and Preordis the reordering probability in the TM .", "The product is a rough estimate of how likely a rule is used in decoding .", "Because only a limited number of reorderings are used in actual translation , a small number of rules are highly probable .", "In fact , among a total of 138 , 662 reorder expanded rules , the most likely 875 rules contribute 95 of the probability mass , so discarding the rules which contribute the lower 5 of the probability mass efficiently eliminates more than 99 of the total rules . zero fertility words An English word may be translated into a null zero length foreign word .", "This happens when the fertility , and such English word called a zero fertility word must be inserted during the decoding .", "The decoding parser is modified to allow inserting zero fertility words , but unlimited insertion easily blows up the memory space .", "Therefore only limited insertion is allowed .", "Observing the Viterbi alignments of the training corpus , the top 20 frequent zero fertility words5 cover over 70 of the cases , thus only those are allowed to be inserted .", "Also we use syntactic context to limit the insertion .", "For example , a zero fertility word in is inserted as IN when PP IN NP A rule is applied .", "Again , observing the Viterbi alignments , the top 20 frequent contexts cover over 60 of the cases , so we allow insertions only in these contexts .", "This kind of context sensitive insertion is possible because the decoder builds a syntactic tree .", "Such selective insertion by syntactic context is not easy for The pruning techniques shown above use extra statistics from the training corpus , such as P , P , and Prule .", "These statistics may be considered as a part of the LM P , and such syntactic probabilities are essential when we mainly use trigrams for the LM .", "In this respect , the pruning is useful not only for reducing the search space , but also improving the quality of translation .", "We also use statistics from the Viterbi alignments , such as the phrase translation frequency and the zero fertility context frequency .", "These are statistics which are not modeled in the TM .", "The frequency count is essentially a joint probability P , while the TM uses a conditional probability P .", "Utilizing statistics outside of a model is an important idea for statistical machine translation in general .", "For example , a decoder in Och and Ney , 2000 uses alignment template statistics found in the Viterbi alignments .", "This section describes results from our experiment using the decoder as described in the previous section .", "We used a Chinese English translation corpus for the experiment .", "After discarding long sentences more than 20 words in English , the English side of the corpus consisted of about 3M words , and it was parsed with Collins parser Collins , 1999 .", "Training the TM took about 8 hours using a 54 node unix cluster .", "We selected 347 short sentences less than 14 words in the reference English translation from the held out portion of the corpus , and they were used for evaluation .", "Table 1 shows the decoding performance for the test sentences .", "The first system ibm4 is a reference system , which is based on IBM Model4 .", "The second and the third syn and syn nozf are our decoders .", "Both used the same decoding algorithm and pruning as described in the previous sections , except that syn nozf allowed no zero fertility insertions .", "The average decoding speed was about 100 seconds6 per sentence for both syn and syn nozf .", "As an overall decoding performance measure , we used the BLEU metric Papineni et al . , 2002 .", "This measure is a geometric average of n gram accuracy , adjusted by a length penalty factor LP . 7 The n gram accuracy in percentage is shown in Table 1 as P1 P2 P3 P4 for unigram bigram trigram 4 gram .", "Overall , our decoder performed better than the IBM system , as indicated by the higher BLEU score .", "We obtained better n gram accuracy , but the lower LP score penalized the overall score .", "Interestingly , the system with no explicit zero fertility word insertion syn nozf performed better than the one with zerofertility insertion syn .", "It seems that most zerofertility words were already included in the phrasal translations , and the explicit zero fertility word insertion produced more garbage than expected words .", "To verify that the pruning was effective , we relaxed the pruning threshold and checked the decoding coverage for the first 92 sentences of the test data .", "Table 2 shows the result .", "On the left , the r table pruning was relaxed from the 95 level to 98 or 100 .", "On the right , the t table pruning was relaxed from the top 5 , pairs to the top 10 or top 20 pairs .", "The system r95 and w5 are identical to syn nozf in Table 1 .", "When r table pruning was relaxed from 95 to 98 , only about half 47 92 of the test sentences were decoded , others were aborted due to lack of memory .", "When it was further relaxed to 100 i . e . , no pruning was done , only 20 sentences were decoded .", "Similarly , when the t table pruning threshold was relaxed , fewer sentences could be decoded due to the memory limitations .", "Although our decoder performed better than the if , and LP if , where , , is the system output length , andis the reference length .", "IBM system in the BLEU score , the obtained gain was less than what we expected .", "We have thought the following three reasons .", "First , the syntax of Chinese is not extremely different from English , compared with other languages such as Japanese or Arabic .", "Therefore , the TM could not take advantage of syntactic reordering operations .", "Second , our decoder looks for a decoded tree , not just for a decoded sentence .", "Thus , the search space is larger than IBM models , which might lead to more search errors caused by pruning .", "Third , the LM used for our system was exactly the same as the LM used by the IBM system .", "Decoding performance might be heavily influenced by LM performance .", "In addition , since the TM assumes an English parse tree as input , a trigram LM might not be appropriate .", "We will discuss this point in the next section .", "Phrasal translation worked pretty well .", "Figure 3 shows the top 20 frequent phrase translations observed in the Viterbi alignment .", "The leftmost column shows how many times they appeared .", "Most of them are correct .", "It even detected frequent sentenceto sentence translations , since we only imposed a relative length limit for phrasal translations Section 3 .", "However , some of them , such as the one with in cantonese , are wrong .", "We expected that these junk phrases could be eliminated by phrase pruning Section 5 , however the junk phrases present many times in the corpus were not effectively filtered out .", "The BLEU score measures the quality of the decoder output sentences .", "We were also interested in the syntactic structure of the decoded trees .", "The leftmost tree in Figure 4 is a decoded tree from the syn nozf system .", "Surprisingly , even though the decoded sentence is passable English , the tree structure is totally unnatural .", "We assumed that a good parse tree gives high trigram probabilities .", "But it seems a bad parse tree may give good trigram probabilities too .", "We also noticed that too many unary rules e . g .", "NPB PRN were used .", "This is because the reordering probability is always 1 .", "To remedy this , we added CFG probabilities PCFG in the decoder search , i . e . , it now looks for a tree which maximizes PtrigramPcfgPTM .", "The CFG probability was obtained by counting the rule frequency in the parsed English side of the training corpus .", "The middle of Figure 4 is the output for the same sentence .", "The syntactic structure now looks better , but we found three problems .", "First , the BLEU score is worse 0 . 078 .", "Second , the decoded trees seem to prefer noun phrases .", "In many trees , an entire sentence was decoded as a large noun phrase .", "Third , it uses more frequent node reordering than it should .", "The BLEU score may go down because we weighed the LM trigram and PCFG more than the TM .", "For the problem of too many noun phrases , we thought it was a problem with the corpus .", "Our training corpus contained many dictionary entries , and the parliament transcripts also included a list of participants names .", "This may cause the LM to prefer noun phrases too much .", "Also our corpus contains noise .", "There are two types of noise .", "One is sentence alignment error , and the other is English parse error .", "The corpus was sentence aligned by automatic software , so it has some bad alignments .", "When a sentence was misaligned , or the parse was wrong , the Viterbi alignment becomes an over reordered tree as it picks up plausible translation word pairs first and reorders trees to fit them .", "To see if it was really a corpus problem , we selected a good portion of the corpus and re trained the r table .", "To find good pairs of sentences in the corpus , we used the following 1 Both English and Chinese sentences end with a period .", "2 The English word is capitalized at the beginning .", "3 The sentences do not contain symbol characters , such as colon , dash etc , which tend to cause parse errors .", "4 The Viterbi ratio8 is more than the average of the pairs which satisfied the first three conditions .", "Using the selected sentence pairs , we retrained only the r table and the PCFG .", "The rightmost tree in Figure 4 is the decoded tree using the re trained TM .", "The BLEU score was improved 0 . 085 , and the tree structure looks better , though there are still problems .", "An obvious problem is that the goodness of syntactic structure depends on the lexical choices .", "For example , the best syntactic structure is different if a verb requires a noun phrase as object than it is if it does not .", "The PCFG based LM does not handle this .", "At this point , we gave up using the PCFG as a component of the LM .", "Using only trigrams obtains the best result for the BLEU score .", "However , the BLEU metric may not be affected by the syntactic aspect of translation quality , and as we saw in Figure 4 , we can improve the syntactic quality by introducing the PCFG using some corpus selection techniques .", "Also , the pruning methods described in Section 5 use syntactic statistics from the training corpus .", "Therefore , we are now investigating more sophisticated LMs such as Charniak , 2001 which incorporate syntactic features and lexical information .", "We have presented a decoding algorithm for a syntax based statistical machine translation .", "The translation model was extended to incorporate phrasal translations .", "Because the input of the channel model is an English parse tree , the decoding algorithm is based on conventional syntactic parsing , and the grammar is expanded by the channel operations of the TM .", "As the model size becomes huge in a practical setting , and the decoder considers multiple syntactic structures for a word alignment , efficient pruning is necessary .", "We applied several pruning techniques and obtained good decoding quality and coverage .", "The choice of the LM is an important issue in implementing a decoder for the syntaxbased TM .", "At present , the best result is obtained by using trigrams , but a more sophisticated LM seems promising .", "This work was supported by DARPA ITO grant N66001 00 1 9814 ."], "summary_lines": ["A Decoder For Syntax-Based Statistical MT\n", "This paper describes a decoding algorithm for a syntax-based translation model (Yamada and Knight, 2001).\n", "The model has been extended to incorporate phrasal translations as presented here.\n", "In contrast to a conventional word-to-word statistical model, a decoder for the syntax-based model builds up an English parse tree given a sentence in a foreign language.\n", "As the model size becomes huge in a practical setting, and the decoder considers multiple syntactic structures for each word alignment, several pruning techniques are necessary.\n", "We tested our decoder in a Chinese-to-English translation system, and obtained better results than IBM Model 4.\n", "We also discuss issues concerning the relation between this decoder and a language model.\n", "We propose a syntax-based decoder that restrict word reordering based on reordering operations on syntactic parse-trees of the input sentence.\n"]}
{"article_lines": ["Labeled Pseudo Projective Dependency Parsing With Support Vector Machines", "We use SVM classifiers to predict the next action of a deterministic parser that builds labeled projective dependency graphs in an incremental fashion .", "Non projective dependencies are captured indirectly by projectivizing the training data for the classifiers and applying an inverse transformation to the output of the parser .", "We present evaluation results and an error analysis focusing on Swedish and Turkish .", "The CoNLL X shared task consists in parsing texts in multiple languages using a single dependency parser that has the capacity to learn from treebank data .", "Our methodology for performing this task is based on four essential components All experiments have been performed using MaltParser Nivre et al . , 2006 , version 0 . 4 , which is made available together with the suite of programs used for pre and post processing . 1", "The parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing by Nivre 2003 and extended to labeled dependency parsing by Nivre et al . 2004 .", "The algorithm builds a labeled dependency graph in one left to right pass over the input , using a stack to store partially processed tokens and adding arcs using four elementary actions where top is the token on top of the stack and next is the next token Although the parser only derives projective graphs , the fact that graphs are labeled allows non projective dependencies to be captured using the pseudoprojective approach of Nivre and Nilsson 2005 .", "Another limitation of the parsing algorithm is that it does not assign dependency labels to roots , i . e . , to tokens having HEAD 0 .", "To overcome this problem , we have implemented a variant of the algorithm that starts by pushing an artificial root token with ID 0 onto the stack .", "Tokens having HEAD 0 can now be attached to the artificial root in a RIGHT ARC r action , which means that they can be assigned any label .", "Since this variant of the algorithm increases the overall nondeterminism , it has only been used for the data sets that include informative root labels Arabic , Czech , Portuguese , Slovene .", "History based parsing models rely on features of the derivation history to predict the next parser action .", "The features used in our system are all symbolic and extracted from the following fields of the data representation FORM , LEMMA , CPOSTAG , POSTAG , FEATS , and DEPREL .", "Features of the type DEPREL have a special status in that they are extracted during parsing from the partially built dependency graph and may therefore contain errors , whereas all the other features have gold standard values during both training and parsing . 2 Based on previous research , we defined a base model to be used as a starting point for languagespecific feature selection .", "The features of this model are shown in Table 1 , where rows denote tokens in a parser configuration defined relative to the stack , the remaining input , and the partially built dependency graph , and where columns correspond to data fields .", "The base model contains twenty features , but note that the fields LEMMA , CPOS and FEATS are not available for all languages .", "We use support vector machines3 to predict the next parser action from a feature vector representing the history .", "More specifically , we use LIBSVM Chang and Lin , 2001 with a quadratic kernel K xZ , xj yxT xj r 2 and the built in one versus all strategy for multi class classification .", "Symbolic features are converted to numerical features using the standard technique of binarization , and we split values of the FEATS field into its atomic components . 4 For some languages , we divide the training data into smaller sets , based on some feature s normally the CPOS or POS of the next input token , which may reduce training times without a significant loss in accuracy Yamada and Matsumoto , 2003 .", "To avoid too small training sets , we pool together categories that have a frequency below a certain threshold t . Pseudo projective parsing was proposed by Nivre and Nilsson 2005 as a way of dealing with non projective structures in a projective data driven parser .", "We projectivize training data by a minimal transformation , lifting non projective arcs one step at a time , and extending the arc label of lifted arcs using the encoding scheme called HEAD by Nivre and Nilsson 2005 , which means that a lifted arc is assigned the label rTh , where r is the original label and h is the label of the original head in the nonprojective dependency graph .", "Non projective dependencies can be recovered by applying an inverse transformation to the output of the parser , using a left to right , top down , breadthfirst search , guided by the extended arc labels rTh assigned by the parser .", "This technique has been used without exception for all languages .", "Since the projective parsing algorithm and graph transformation techniques are the same for all data sets , our optimization efforts have been focused on feature selection , using a combination of backward and forward selection starting from the base model described in section 2 . 2 , and parameter optimization for the SVM learner , using grid search for an optimal combination of the kernel parameters y and r , the penalty parameter C and the termination criterion c , as well as the splitting feature s and the frequency threshold t . Feature selection and parameter optimization have to some extent been interleaved , but the amount of work done varies between languages .", "The main optimization criterion has been labeled attachment score on held out data , using ten fold cross validation for all data sets with 100k tokens or less , and an 80 20 split into training and devtest sets for larger datasets .", "The number of features in the optimized models varies from 16 Turkish to 30 Spanish , but the models use all fields available for a given language , except that FORM is not used for Turkish only LEMMA .", "The SVM parameters fall into the following ranges \u03b3 0 . 12 0 . 20 ; r 0 . 0 0 . 6 ; C 0 . 1 0 . 7 ; c 0 . 01 1 . 0 .", "Data has been split on the POS of the next input token for Czech t 200 , German t 1000 , and Spanish t 1000 , and on the CPOS of the next input token for Bulgarian t 1000 , Slovene t 600 , and Turkish t 100 .", "For the remaining languages , the training data has not been split at all .", "5 A dry run at the end of the development phase gave a labeled attachment score of 80 . 46 over the twelve required languages .", "Table 2 shows final test results for each language and for the twelve required languages together .", "The total score is only 0 . 27 percentage points below the score from the dry run , which seems to indicate that models have not been overfitted to the training data .", "The labeled attachment score varies from 91 . 65 to 65 . 68 but is above average for all languages .", "We have the best reported score for Japanese , Swedish and Turkish , and the score for Arabic , Danish , Dutch , Portuguese , Spanish , and overall does not differ significantly from the best one .", "The unlabeled score is less competitive , with only Turkish having the highest reported score , which indirectly indicates that the integration of labels into the parsing process primarily benefits labeled accuracy .", "An overall error analysis is beyond the scope of this paper , but we will offer a few general observations before we turn to Swedish and Turkish , focusing on recall and precision of root nodes , as a reflection of global syntactic structure , and on attachment score as a function of arc length .", "If we start by considering languages with a labeled attachment score of 85 or higher , they are characterized by high precision and recall for root nodes , typically 95 90 , and by a graceful degradation of attachment score as arcs grow longer , typically 95 90 85 , for arcs of length 1 , 2 and 3 6 .", "Typical examples are Bulgarian Simov et al . , 2005 ; Simov and Osenova , 2003 , Chinese Chen et al . , 2003 , Danish Kromann , 2003 , and Swedish Nilsson et al . , 2005 .", "Japanese Kawata and Bartels , 2000 , despite a very high accuracy , is different in that attachment score drops from 98 to 85 , as we go from length 1 to 2 , which may have something to do with the data consisting of transcribed speech with very short utterances .", "A second observation is that a high proportion of non projective structures leads to fragmentation in the parser output , reflected in lower precision for roots .", "This is noticeable for German Brants et al . , 2002 and Portuguese Afonso et al . , 2002 , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech B ohmov a et al . , 2003 , Dutch van der Beek et al . , 2002 and Slovene D\u02c7zeroski et al . , 2006 , where root precision drops more drastically to about 69 , 71 and 41 , respectively , and root recall is also affected negatively .", "On the other hand , all three languages behave like high accuracy languages with respect to attachment score .", "A very similar pattern is found for Spanish Civit Torruella and MartiAntonin , 2002 , although this cannot be explained by a high proportion of non projective structures .", "One possible explanation in this case may be the fact that dependency graphs in the Spanish data are sparsely labeled , which may cause problem for a parser that relies on dependency labels as features .", "The results for Arabic Haji\u02c7c et al . , 2004 ; Smr\u02c7z et al . , 2002 are characterized by low root accuracy as well as a rapid degradation of attachment score with arc length from about 93 for length 1 to 67 for length 2 .", "By contrast , Turkish Oflazer et al . , 2003 ; Atalay et al . , 2003 exhibits high root accuracy but consistently low attachment scores about 88 for length 1 and 68 for length 2 .", "It is noteworthy that Arabic and Turkish , being typological outliers , show patterns that are different both from each other and from most of the other languages .", "A more fine grained analysis of the Swedish results reveals a high accuracy for function words , which is compatible with previous studies Nivre , 2006 .", "Thus , the labeled F score is 100 for infinitive markers IM and subordinating conjunctions UK , and above 95 for determiners DT .", "In addition , subjects SS have a score above 90 .", "In all these cases , the dependent has a configurationally defined but not fixed position with respect to its head .", "Arguments of the verb , such as objects DO , IO and predicative complements SP , have a slightly lower accuracy about 85 labeled F score , which is due to the fact that they compete in the same structural positions , whereas adverbials labels that end in A have even lower scores often below 70 .", "The latter result must be related both to the relatively fine grained inventory of dependency labels for adverbials and to attachment ambiguities that involve prepositional phrases .", "The importance of this kind of ambiguity is reflected also in the drastic difference in accuracy between noun pre modifiers AT F 97 and noun post modifiers ET F Pz 75 .", "Finally , it is worth noting that coordination , which is often problematic in parsing , has high accuracy .", "The Swedish treebank annotation treats the second conjunct as a dependent of the first conjunct and as the head of the coordinator , which seems to facilitate parsing . 6 The attachment of the second conjunct to the first CC has a labeled F score above 80 , while the attachment of the coordinator to the second conjunct has a score well above 90 .", "In Turkish , very essential syntactic information is contained in the rich morphological structure , where concatenated suffixes carry information that in other languages may be expressed by separate words .", "The Turkish treebank therefore divides word forms into smaller units , called inflectional groups IGs , and the task of the parser is to construct dependencies between IGs , not primarily between word forms Eryi git and Oflazer , 2006 .", "It is then important to remember that an unlabeled attachment score of 75 . 8 corresponds to a word to word score of 82 . 7 , which puts Turkish on a par with languages like Czech , Dutch and Spanish .", "Moreover , when we break down the results according to whether the head of a dependency is part of a multiple IG word or a complete single IG word , we observe a highly significant difference in accuracy , with only 53 . 2 unlabeled attachment score for multiple IG heads versus 83 . 7 for single IG heads .", "It is hard to say at this stage whether this means that our methods are ill suited for IG based parsing , or whether it is mainly a case of sparse data for multiple IG words .", "When we break down the results by dependency type , we can distinguish three main groups .", "The first consists of determiners and particles , which have an unlabeled attachment score over 80 and which are found within a distance of 1 1 . 4 IGs from their head . 7 The second group mainly contains subjects , objects and different kinds of adjuncts , with a score in the range 60 80 and a distance of 1 . 8 5 . 2 IGs to their head .", "In this group , information about case and possessive features of nominals is important , which is found in the FEATS field in the data representation .", "We believe that one important explanation for our relatively good results for Turkish is that we break down the FEATS information into its atomic components , independently of POS and CPOS tags , and let the classifier decide which one to use in a given situation .", "The third group contains distant dependencies , such as sentence modifiers , vocatives and appositions , which have a much lower accuracy .", "The evaluation shows that labeled pseudo projective dependency parsing , using a deterministic parsing algorithm and SVM classifiers , gives competitive parsing accuracy for all languages involved in the 7Given that the average IG count of a word is 1 . 26 in the treebank , this means that they are normally adjacent to the head word . shared task , although the level of accuracy varies considerably between languages .", "To analyze in depth the factors determining this variation , and to improve our parsing methods accordingly to meet the challenges posed by the linguistic diversity , will be an important research goal for years to come .", "We are grateful for the support from T UB ITAK The Scientific and Technical Research Council of Turkey and the Swedish Research Council .", "We also want to thank Atanas Chanev for assistance with Slovene , the organizers of the shared task for all their hard work , and the creators of the treebanks for making the data available ."], "summary_lines": ["Labeled Pseudo-Projective Dependency Parsing With Support Vector Machines\n", "We use SVM classifiers to predict the next action of a deterministic parser that builds labeled projective dependency graphs in an incremental fashion.\n", "Non-projective dependencies are captured indirectly by projectivizing the training data for the classifiers and applying an inverse transformation to the output of the parser.\n", "We present evaluation results and an error analysis focusing on Swedish and Turkish.\n", "Our pseudo-projective approach transforms non-projective training trees to projective ones but encode the information necessary to make the inverse transformation in the DEPREL, so that this inverse transformation can also be carried out on the test trees (Nivre et al, 2006).\n"]}
{"article_lines": ["Language Independent Named Entity Recognition Combining Morphological And Contextual Evidence", "Identifying and classifying personal , geographic , institutional or other names in a text is an important task for numerous applications .", "This paper describes and evaluates a language independent bootstrapping algorithm based on iterative learning and re estimation of contextual and morphological patterns captured in hierarchically smoothed trie models .", "The algorithm learns from unannotated text and achieves competitive performance when trained on a very short labelled name list with no other required language specific information , tokenizers or tools .", "The ability to determine the named entities in a text has been established as an important task for several natural language processing areas , including information retrieval , machine translation , information extraction and language understanding .", "For the 1995 Message Understanding Conference MUC 6 , a separate named entity recognition task was developed and the best systems achieved impressive accuracy with an F measure approaching 95 .", "What should be underlined here is that these systems were trained for a specific domain and a particular language English , typically making use of hand coded rules , taggers , parsers and semantic lexicons .", "Indeed , most named entity recognizers that have been published either use tagged text , perform syntactical and morphological analysis or use semantic information for contextual clues .", "Even the systems that do not make use of extensive knowledge about a particular language , such as Nominator Choi et al . , 1997 , still typically use large data files containing lists of names , exceptions , personal and organizational identifiers .", "Our aim has been to build a maximally languageindependent system for both named entity identification and classification , using minimal information about the source language .", "The applicability of AI style algorithms and supervised methods is limited in the multilingual case because of the cost of knowledge databases and manually annotated corpora .", "Therefore , a much more suitable approach is to consider an EM style bootstrapping algorithm .", "In terms of world knowledge , the simplest and most relevant resource for this task is a database of known names .", "For each entity class to be recognized and tagged , it is assumed that the user can provide a short list order of one hundred of unambiguous examples seeds .", "Of course the more examples provided , the better the results , but what we try to prove is that even with minimal knowledge good results can be achieved .", "Additionally some basic particularities of the language should be known capitalization if it exists and is relevant some languages do not make use of capitalization ; in others , such as German , the capitalization is not of great help , allowable word separators if they exist , and a few frequent exceptions like the pronoun quot ; I quot ; in English .", "Although such information can be utilised if present , it is not required , and no other assumptions are made in the general model .", "The algorithm relies on both word internal and contextual clues as relatively independent evidence sources that drive the bootstrapping algorithm .", "The first category refers to the morphological structure of the word and makes use of the paradigm that for certain classes of entities some prefixes and suffixes are good indicators .", "For example , knowing that quot ; Maria quot ; , quot ; Marinela quot ; and quot ; Maricica quot ; are feminine first names in Romanian , the same classification may be a good guess for quot ; Mariana quot ; , based on common prefix .", "Suffixes are typically even more informative , for example quot ; escu quot ; is an almost perfect indicator of a last name in Romanian , the same applies to quot ; wski quot ; in Polish , quot ; ovic quot ; and quot ; ivic quot ; in SerboCroatian , quot ; son quot ; in English etc .", "Such morphological information is automatically learned during bootstrapping .", "Contextual patterns e . g .", "quot ; Mr . quot ; , quot ; in quot ; and quot ; mayor of quot ; in left context are also clearly crucial to named entity identification and classification , especially for names that do not follow a typical morphological pattern for their word class , are of foreign origin or polysemous for example , many places or institutions are named after persons , such as quot ; Washington quot ; or quot ; Madison quot ; , or , in some cases , vice versa quot ; Ion Popescu Topolog quot ; is the name of a Romanian writer , who added to his name the name of the river quot ; Topolog quot ; .", "Clearly , in many cases , the context for only one occurrence of a new word and its morphological information is not enough to make a decision .", "But , as noted in Katz 1996 , a newly introduced entity will be repeated , quot ; if not for breaking the monotonous effect of pronoun use , then for emphasis and clarity quot ; .", "Moreover , he claims that the number of instances of the new entity is not associated with the document length but with the importance of the entity with regard to the subject discourse .", "We will use this property in conjunction with the one sense per discourse tendency noted by Gale , Church and Yarowsky 1992b , who showed that words strongly tend to exhibit only one sense in a document discourse .", "By gathering contextual information about the entity from each of its occurrences in the text and using morphological clues as well , we expect to classify entities more effectively than if they are considered in isolation , especially those that are very important with regard to the subject .", "When analyzing large texts , a segmentation phase should be considered , so that all the instances of a name in a segment have a high probability of belonging to the same class , and thus the contextual information for all instances within a segment can be used jointly when making a decision .", "Since the precision of the segmentation is not critical , a language independent segmentation system like the one presented by Amithay , Richmond and Smith 1997 is adequately reliable for this task .", "There are two basic alternatives for handling a text .", "The first one is to tokenize it and classify the individual tokens or group of tokens .", "This alternative works for languages that use word separators such as spaces or punctuation , where a relatively simple set of separator patterns can adequately tokenize the text .", "The second alternative is to classify entities simply with respect to a given starting and ending character position , without knowing the word boundaries , but just the probability that can be learned automatically of a boundary given the neighboring contexts .", "This second alternative works for languages like Chinese , where no separators between the words are typically used .", "Since for the first class of languages we can define a priori probabilities for boundaries that will match the actual separators , this second approach represents a generalization of the one using tokenized text .", "However , the first method , in which the text is tokenized , presents the advantage that statistics for both tokens and types can be kept and , as the results show , the statistics for types seem to be more reliable than those for tokens .", "Using the second method , there is no single definition of quot ; type quot ; , given that there are multiple possible boundaries for each token instance , but there are ways to gather statistics , such as considering what we may call quot ; probable types quot ; according to the boundary probabilities or keeping statistics on sistrings semi infinite strings .", "Some other advantages and disadvantages of the two methods will be discussed below .", "Before describing the algorithm , we will present a brief overview of some of its goals Three important concepts are used in our model 2 . 1 Trie structures are used for both morphological and contextual information Tries provide an effective , efficient and flexible data structure for storing both contextual and morphological patterns and statistics .", "First , they are very compact representations .", "Second , they support a natural hierarchical smoothing procedure for distributional class statistics .", "We consider characterbased tries , in which each node contains a probability distribution when working with tokenized text , two distributions are considered in each node , one for tokens and one for types .", "The distribution stored at each node contain the probability of each name class given the history ending at that node .", "Each distribution also has two standard classes , named quot ; questionable quot ; unassigned probability mass in terms of entity classes , to be motivated below and quot ; non entity quot ; .", "To simplify the notations , we will refer to a start and end point bounded portion of text being analyzed in order to determine if it represents a named entity or not as a token .", "Two tries are used for context left and right and two for internal morphological patterns of tokens .", "Figure 1 shows an example of a morphological prefix trie , which stores the characters of tokens from left to right from given starting points with optional word boundaries indicated by quot ; quot ; .", "Suffix tries typically more informative have equivalent structure but reversed direction .", "The left and right context tries have the same structure as well , but the list of links refers now to the tokens which have the particular context represented by the path from the root to the current node .", "For right context , the letters are introduced in the trie in normal order , for left context they are considered in the reversed order in our example , quot ; Anda quot ; has as left context quot ; dna xela quot ; .", "Similarly , nodes of the context tries contain links to the tokens that occurred in the particular contexts defined by the paths .", "Two bipartite graph structures are created in this way by these links .", "For reasons that will be explained later , raw counts are kept for the distributions .", "The probability of a token context as being in or indicating a class is computed along the whole path from the root to the terminal node of the token context .", "In this way , effective smoothing is realized for rare tokens or contexts .", "Considering a token context formed from characwhere Ai E 0 , 1 and E Ai 1 It is reasonable to expect that smaller lambdas should correspond to smaller indices , or even that Ai A2 An .", "In order to keep the number of parameters low , we used the following model where a , 13 E 0 , 1 , i3 having a small value The symbol F is used instead of P since we have raw distributions frequencies and a normalization step is needed to compute the final probability distribution .", "A simpler model can use just one parameter setting 3 an , but this has limited flexibility in optimizing the hierarchical inheritance the probability of a class given the first letter is often not very informative for some languages such as English or Romanian or , by contrast , may be extremely important for others e . g .", "Japanese .", "The basic concept of this bootstrapping procedure is to iteratively leverage relatively independent sources of information .", "Beginning with some seed names for each class , the algorithm learns contextual patterns that are indicative for those classes and then iteratively learns new class members and word internal morphological clues .", "Through this cycle , probability distributions for class given token , prefix suffix or context are incrementally refined .", "More details are given when describing stage 2 of the algorithm . opposed to the classical maximum entropy principle When faced with a highly skewed observed class distribution for which there is little confidence due to small sample size , a typical response to this uncertainty in statistical machine learning systems is to backoff or smooth to the more general class distribution , which is typically more uniform .", "Unfortunately , this representation is difficult to distinguish from a conditional distribution based on a very large sample and hence estimated with confidence that just happens to have a similar fairly uniform true distribution .", "One would like a representation that does not obscure this distinction , and represents the uncertainty of the distribution separately .", "We resolve this problem while retaining a single probability distribution over classes by adding a separate quot ; questionable quot ; or unassigned cell that reflects the uncertainty of the distribution .", "Probability mass continues to be distributed among the remaining class cells proportional to the observed distribution in the 'data , but with a total sum 1 that reflects the confidence in the distribution and is equal to 1 P qiiestionable .", "This approach has the advantage of explicitly representing the uncertainty in a given class distribution , facilitating the further development of an interactive system , while retaining a single probability distribution that simplifies trie architecture and model combination .", "Incremental learning essentially becomes the process of gradually shifting probability mass from questionable uncertain to one of the primary categories .", "The algorithm can be divided into five stages , which are summarized below .", "Stage 0 build the initial training list of class representatives Stage 1 read the text and build the left and right morphological and context tries Stage 2 introduce the training information in the tries and re estimate the distributions by bootstrapping Stage 3 identify and classify the named entities in the text using competing classifiers Stage 4 update the entity and context training space , using the new extracted information Stage 0 This stage is performed once for each language task and consists of defining the classes and filling in the initial class seed data with examples provided by the user .", "The list of class training names should be as unambiguous as possible and ideally also relatively common .", "It is also necessary to have a relatively large unannotated text for bootstrapping the contextual models and classifying new named entities .", "Examples of such training seeds and text for Romanian language are given in Tables 1 and 21 .", "For the primary experiments reported in this paper , we have studied a relatively difficult 3 way named entity partition between First given names , Last family names and Place 'names .", "The first two tend to be relatively hard to distinguish in most languages .", "A 1The text refers to the mayor of a small town of Alba county , who was so drunk while officiating at a wedding that he shook the bride's hand and kissed the groom . simpler person place based distinction more comparable to the MUC 6 EMAMEX task is evaluated in Table 3 d .", "Target Evaluation Text labels not used for training Primarul comunei place Rosia Montana place judetul place Alba place fname David fname lname Botar lname a intrat in legenda datorita unor intimplari de a dreptul penibile , relatate in quot ; Evenimentul zilei quot ; .", "Practic , primul gospodar al celei mai bogate comune in aur din place Muntii Apuseni place este mai tot timpul beat crita , drept pentru care , la oficierea unei casatorii , a sarutat mina mirelui , a strins mina miresei si a intocmit certificat de deces in locul celui de casatorie .", "Recent , fname Andrei fname lname Paunescu lname fiul poetului , a intentionat sa achizitioneze gospodaria unei bucurestence care se stabilise de o vreme in place Rosia Montana place La primarie Ins . , turmentatul primar 1 a trimis pe fiul lui fname Adrian fname lname Paunescu lname sa i cumpere ceva de baut , pentru a se putea concentra indeajuns asupra hirtiilor tranzactiei imobiliare .", "There are two ways to start this stage , either by tokenizing the text or considering it in raw form .", "When tokenization is used , each token is inserted in the two morphological tries one that keeps the letters of the tokens in the normal prefix order , another that keeps the letter in the reverse suffix order .", "For each letter on the path , the raw distributions are changed by adding the a priori probability of the token belonging to each class language dependent information may be used here .", "For example , in the case of Indo European languages , if the token starts with an upper case letter , we add 1 full count all probability mass to the quot ; questionable quot ; sum , as this entity is initially fully ambiguous .", "If the token starts with lower case and hence is an unlikely name in this case we add the bulk of the probability mass 5 e . g . d ?", "; 0 . 9 to quot ; non entity quot ; and the remainder 1 5 to quot ; questionable quot ; otherwise unassigned .", "Other language specific orthographic clues could potentially affect this initial probability mass assignment .", "When no tokenization is applied , we have to consider possible starting and ending points .", "Therefore , the strings which , for simplicity , we will refer as well as tokens introduced in the prefix morphological trie and the ones introduced in the suffix trie may differ .", "The left context of each token is introduced , letters in reverse order , in the left context trie , with pointers to the token in the morphlogical prefix trie ; the right context of each token is introduced , in normal order , in the right context trie , keeping pointers to the token in the suffix trie .", "The distributions along the paths are modified according to the a priori distribution of the targeted token .", "This stage is the core bootstrapping phase of the algorithm .", "In essence , as contextual models become better estimated , they identify additional named entities with increasing confidence , allowing reestimation and improvement of the internal morphological models .", "The additional training data that this yields allows the contextual models to be augmented and reestimated , and the cycle continues until convergence .", "One approach to this bootstrapping process is to use a standard continuous EM ExpectationMaximization family of algorithms Baum , 1972 ; Dempster et al . , 1977 .", "The proposed approach outlined below is a discrete variant that is much less computationally intensive , and has the advantage of distinguishing between unknown probability distributions and those which are simply evenly distributed .", "The approach is conservative in that it only utilizes the class estimations for newly classified data in the retraining process if the class probability passes a confidence threshold , as defined below .", "The concept of confidence threshold can be captured through the following definitions of dominant and semi dominant .", "Let us consider a discrete finite probability distribution P pi , . . , pn We say that P has a dominant if there is an i in 1 . . . n such that pi 0 . 5 , or in other words if We say that P has an a semi dominant with respect to an event k , where c 1 , if it does not have k as dominant and there exist i in 1 . . . n such that A few comments about these definitions are necessary it can be easily observed that not every distribution has a dominant , even though it has a maximum value .", "The second definition , of a semidominant , makes sense if we consider a particular event k that is not relevant or the result cannot be measured .", "By removing this event and normalizing the rest of the values , we obtain a new distribution of size n 1 having an a dominant .", "The core of stage 2 is the bootstrapping procedure .", "The known names either from the original training list or otherwise learned data are inserted sequentially into the morphological tries , modifying the probability distributions of the nodes on the paths accordingly the data structure is illustrated in Figures 1 and 2 .", "If the new distribution in one of the nodes on the path of a known token gains a dominant for example quot ; place quot ; then the effect of this change is propagated by reestimating other node distributions given this change .", "Each distribution on the context paths in which that token occurred in the text is modified , by subtracting from the quot ; questionable quot ; mass a quantity proportional to the number of times the respective token was found in that context and adding it to the dominant position e . g .", "quot ; place quot ; mass .", "For the newly obtained distributions that gained a dominant in our example quot ; place quot ; in the context trie , the bootstrapping procedure is called for all tokens that occurred in that context , and so on , recursively .", "Here it is very important that we consider raw distributions and not normalize them .", "For example , if word quot ; Mariana quot ; occurs x times with the right context quot ; merge quot ; meaning quot ; goes quot ; and the distribution for quot ; niariana quot ; has now been identified with the dominant quot ; first name quot ; , then x units from the quot ; questionable quot ; mass can be moved to quot ; first name quot ; mass along the path of quot ; merge quot ; in the right context trie .", "If semi dominants are used instead of dominants then we have to account for the fact that the semi dominants may change over time , so the probability mass must be moved either from quot ; questionable quot ; position or previous semi dominant position , if a semi dominant state has been reached before .", "It may be easily observed that stage 2 has a sequential characteristic , because the updating is done after reading each name incrementally .", "When using dominants the order does not affect the process , because of the fact that once a dominant state is reached , it cannot change to another dominant state in the future probability mass is moved only from quot ; questionable quot ; .", "In the case of semi dominants , the data ordering in the training file does influence the learning procedure .", "The more conservative strategy of using dominants rather then semi dominants has , on the other hand , the disadvantage of cancelling or postponing the utilisation of many words .", "For example , if both quot ; questionable quot ; and quot ; first name quot ; have 49 of the mass then subsequent reestimation iterations are not initiated for this data , even though the alternative name classes are very unlikely .", "Considering those advantages and disadvantages , we used the less conservative semi dominant approach as the default model .", "In this stage the text is re analysed sequentially , and for each token given a start end point pair a decision is made .", "Here the bipartite structure of the two pairs of tries has a central role during stage 2 , the left context and prefix tries interact with each other and so do the right context and suffix tries , but there's no interference between the two pairs during the bootstrapping stage .", "Therefore , for each instance of a token in the text , four classifiers are available , a different one given by each trie .", "The decision with regard to the presence of an entity and its classification is made by combining them .", "Comparative trials indicate that higher performance is achieved by initially having the classifiers vote .", "Results indicate that the most accurate classifications are obtained from the two independently bootstrapped morphological tries they incorporate the morphological information about the token to be classified , and , during the bootstrapping , they also incorporate information from all the contexts in which the token occurred .", "If the two agree they have semi dominants and they are the same then the corresponding class is returned .", "Otherwise , agreement is tested between other paired independent classifiers in order of empirically measured reliability .", "If no agreement is found , then a simple linear combination of all four is considered for the decision .", "This approach yields 6 higher F measure than the simple interpolation of classifiers for the default parameters .", "Stage 4 The newly classified tokens and contexts are saved for future use as potential seed data in subsequent named entity classification on new texts .", "The basic measures for evaluation of this work are precision and recall .", "Precision P represents the percentage of the entities that the system recognized which are actually correct .", "Recall R represents the percentage of the correct named entities in the text that the system identified .", "Both measures are incorporated in the F measure , F 2PRAP R .", "It would be inappropriate to compare the results of a language independent system with the ones designed for only one language .", "As Day and Palmer 1997 observed , quot ; the fact that existing systems perform extremely well on mixed case English newswire corpora is certainly related to the years of research and organized evaluations on this specific task in this language .", "It is not clear what resources are required to adapt systems to new languages . quot ; It is important to mention that the F measure for the human performance on this task is about 96 , Sundheim 1995 .", "Our experiments on Romanian text were consistent with this figure .", "In order to obtain a baseline performance for this method we considered the performance of a system that tags only the examples found in one of the the original training wordlists .", "We consider this to be a plausible lower bound measure if the training words have not been selected from the test text .", "Day and Palmer 1997 showed that a baseline Fmeasure score for the ENAMEX task varies from 21 . 2 for English to 73 . 2 for Chinese .", "It is important to mention that , when they computed these figures , they trained their language independent system on large annotated corpora e . g . the Wall Street Journal for English .", "The fact that the precision obtained by the baseline approach is not 100 indicates that the seed training names for each class are not completely unambiguous , and that a certain degree of ambiguity is generally unavoidable in this case , mainly because of the interference between first names and last names .", "Another significant performance measure is forced classification accuracy , where the entities have been previously identified in the text and the only task is selecting their name class .", "To obtain baseline performance for this measure , we considered a system that uses the original training word labels if there is an exact match , with all other entities labeled with a default quot ; last name quot ; tag , the most common class in all languages studied .", "The baseline accuracy was measured at 61 . 18 for Romanian .", "System accuracies range from 77 . 12 to 91 . 76 on this same data .", "The results shown in Table 3 were obtained for a Romanian text having 12320 words , from which 438 were entities , using a training seed set of 300 names 115 first names , 125 last names , and 60 city country names .", "The baseline measures and default system a are as described above .", "In configuration b , the based parameters of the system have been optimized for Romanian , using greedy search on an independent development test devtest set , yielding a slight increase in F measure .", "Configuration c used the default parameters , but the more conservative quot ; dominant quot ; criterion was utilized , clearly favoring precision at the expense of recall .", "Configuration d , which is relevant for the ENAMEX task , represents the performance of the system when classes quot ; first name quot ; and quot ; last name quot ; are combined into quot ; person quot ; whenever two or more such entities are adjacent , we consider the whole group as a quot ; person quot ; entity .", "Configuration e shows contrastive performance when using standard continuous EM smoothing on the same data and data structures .", "Table 4 shows system performance for 5 fairly diverse languages Romanian , English , Greek , Turkish and Hindi .", "The initial 4 rows provide some basic details on the training data available for each language .", "Note that when annotators were generating the lists of 150 300 seed words , they had access to a development test from which to extract samples , but they were not constrained to this text and could add additional ones from memory .", "Furthermore , it was quite unpredictable how many contexts would actually be found for a given word in the development texts , as some appeared several times and many did not appear at all .", "Thus the total number of contextual matches for the seed words was quite variable , from 113 249 , and difficult to control .", "It is also the case that not all additional contexts bring comparable new benefit , as many secondary instances of the same word in a given related document collection tend to have similar or identical surrounding contexts to the first instance e . g .", "quot ; Mayor of XXX quot ; or quot ; XXX said quot ; , so in general it is quite difficult to control the actual training information content just by the number of raw seed word types that are annotated .", "For each of these languages , 5 levels of information sources are evaluated .", "The baseline case is as previously described for Table 3 .", "The context only case restricts system training to the two left and right contextual tries , ignoring the prefix suffix morphological information .", "The morphology only case , in contrast , restricts the system to only the two prefix and suffix morphological models .", "These can be estimated from the 3 training wordlists 150 300 words total , but without an independent source of information e . g . context via which bootstrapping can iterate , there is no available path by which these models can learn the behaviour of previously unseen affixes and conquer new territory .", "Thus the model is entirely static on just the initial training data .", "For the same reasons , the context only model is also static .", "In this case there is a possible bootstrapping path using alternating left and right context to expand coverage to new contexts , but this tends to be not robust and was not pursued .", "Interestingly , recall for morphology only is typically much higher than in the context only case .", "The reason for this is that the morphology models are full hierarchically smoothed character tries rather than word token tries , and hence have much denser initial statistics for small training data sets , proving greater partial matching potential for previously unseen words .", "In an effort to test the contribution of the full iterative boostrapping , the quot ; context and morphology only quot ; results , are based on the combination of all 4 tries , but Without any bootstrapping .", "Thus they are trained exclusively on the 150 300 training examples .", "Performance for the combined sources is in all cases greater than for the morphology or context source used alone .", "Furthermore , the full iterative bootstrapping clearly yields substantial improvement over the static models , almost exclusively in the form of increased recall and its corresponding boost the the F measure .", "Cross language analysis yields further insight .", "First , recall is much higher for the 4 languages in which case is explicitly marked and is a clue for named entity identification Romanian , English , Greek and Turkish than for a language like Hindi , where there are no case distinctions and hence any word could potentially be a named entity .", "A language such as German would be roughly in the middle , where lower case words have low probability as named entities , but capitalized words are highly ambiguous between common and proper nouns .", "Because approximately 96 of words in the Hindi text are not named entities , without additional orthographic clues the prior probability for quot ; non entity quot ; is so strong that the morphological or contextual evidence in favor of one of the named entity classes must be very compelling to overcome this bias .", "With only 50 training words per context this is difficult , and in the face of such strong odds against any of the named entity classes the conservative nature of the learning algorithm only braves an entity label correctly for 38 more words than the baseline model .", "In contrast , its performance on entity classification rather than identification , measured by forced choice accuracy in labelling the given entities , is comparable to all the other languages , with 79 accuracy relative to the 62 baseline . 2 Figure 3 demonstrates that the performance of the algorithm is highly sensitive to the size of the training data .", "Based on Romanian , the first graph shows that as the size of the raw text for bootstrapping increases , F measure performance increases roughly logrithmically , due almost exclusively to increases in precision .", "Approximately the same number of unique entities are being identified , but due to the increased number of examples of each , their classification is more accurate .", "This is a very encouraging trend , as the web and other online sources provides virtually unlimited raw text in most major languages , and substantial on line text for virtually all languages .", "So extrapolating far beyond the 10K word level is relatively low cost and very feasible .", "The second graph shows that F measure performance also increases roughly logrithmically with the total length of the seed wordlists in the range 40300 .", "This increase is due entirely to improved recall , which doubles over this small range .", "This trend suggests that there is considerable benefit to be gained by additional human annotation , or seed wordlist acquisition from existing online lexicons .", "However , relative to case of raw text acquisition , such additional annotations tend to be much costlier , and there is a clear cost benefit tradeoff to further investment in annotation .", "In summary , however , these evaluation results are satisfying in that they a show clear and consistent trends across several diverse languages , b show clear trends for improvement as training resources grow , and c show that comparable and robust classification results can be achieved on this diversity of languages .", "For future work , natural next steps include incorporating a language independent word segmentation phase like the one proposed by Amitay , Richmond and Smith 1997 , to improve the performance on large texts .", "Different statistics can be pre computed for different languages and language families and stored in external files .", "For example , the a priori probability of a named entity given the set of characteristics of its representation in the text , such as position , capitalization , and relative position of other entities e . g .", "first name followed by last name .", "A further step is the implementation of a supervised active learning system based on the present algorithm , in which the most relevant words for future disambiguation is presented to the user to be classified and the feedback used for bootstrapping .", "The selection of candidate examples for tagging would be based on both the unassigned probability mass and the frequency of occurrence .", "Active learning strategies Lewis and Gale , 1994 are a natural path for efficiently selecting contexts for human annotation .", "This paper has presented an algorithm for the minimally supervised learning of named entity recognizers given short name lists as seed data typically 40100 example words per entity class .", "The algorithm uses hierarchically smoothed trie structures for modeling morphological and contextual probabilities effectively in a language independent framework , overcoming the need for fixed token boundaries or history lengths .", "The combination of relatively independent morphological and contextual evidence sources in an iterative bootstrapping framework converges upon a successful named entity recognizer , achieving a competitive 70 . 5 75 . 4 F measure measuring both named entity identification and classification when applied to Romanian text .", "Fixed k way classification accuracy on given entities ranges between 73 79 on 5 diverse languages for a difficult firstname lastname place partition , and approaches 92 accuracy for the simpler person place discrimination .", "These results were achieved using only unannotated training texts , with absolutely no required language specific information , tokenizers or other tools , and requiring no more than 15 minutes total human effort in training for short wordlist creation The observed robust and consistent performance and very rapid , low cost rampup across 5 quite different languages shows the potential for further successful and diverse applications of this work to new languages and domains .", "The authors would like to thank Eric Brill , Radu Florian , Shanka , r Kumar , Murat Saraclar , Dimitra Vergyri and Jun Wu for both their feedback on this work and their help in annotating the named entity data for the languages studied ."], "summary_lines": ["Language Independent Named Entity Recognition Combining Morphological And Contextual Evidence\n", "Identifying and classifying personal, geographic, institutional or other names in a text is an important task for numerous applications.\n", "This paper describes and evaluates a language-independent bootstrapping algorithm based on iterative learning and re-estimation of contextual and morphological patterns captured in hierarchically smoothed trie models.\n", "The algorithm learns from unannotated text and achieves competitive performance when trained on a very short labelled name list with no other required language-specific information, tokenizers or tools.\n", "We consider one sense per discourse phenomenon, the tendency of terms to have a fixed meaning within a single document.\n", "We take a character-level approach to named entity recognition (NER) using prefix and suffix tries.\n", "The bootstrapping stage uses the initial or current entity assignments to estimate the class conditional distributions for both entities and contexts along their trie paths, and then re-estimates the distributions of the contexts/entity-candidates to which they are linked, recursively, until all accessible nodes are reached.\n"]}
{"article_lines": ["Named Entity Recognition With Character Level Models", "We discuss two named entity recognition models which use characters and character grams either exclusively or as an important part of their data representation .", "The first model is a character level HMM with minimal context information , and the second model is a maximum entropy conditional markov model with substantially richer context features .", "Our best model achieves an overall F of 86 . 07 on the English test data 92 . 31 on the development data .", "This number represents a 25 error reduction over the same model without word internal substring features .", "For most sequence modeling tasks with word level evaluation , including named entity recognition and part ofspeech tagging , it has seemed natural to use entire words as the basic input features .", "For example , the classic HMM view of these two tasks is one in which the observations are words and the hidden states encode class labels .", "However , because of data sparsity , sophisticated unknown word models are generally required for good performance .", "A common approach is to extract word internal features from unknown words , for example suffix , capitalization , or punctuation features Mikheev , 1997 , Wacholder et al . , 1997 , Bikel et al . , 1997 .", "One then treats the unknown word as a collection of such features .", "Having such unknown word models as an add on is perhaps a misplaced focus in these tasks , providing correct behavior on unknown words is typically the key challenge .", "Here , we examine the utility of taking character sequences as a primary representation .", "We present two models in which the basic units are characters and character grams , instead of words and word phrases .", "Earlier papers have taken a character level approach to named entity recognition NER , notably Cucerzan and Yarowsky 1999 , which used prefix and suffix tries , though to our knowledge incorporating all character grams is new .", "In section 2 , we discuss a character level HMM , while in section 3 we discuss a sequence free maximum entropy maxent classifier which uses gram substring features .", "Finally , in section 4 we add additional features to the maxent model , and chain these models into a conditional markov model CMM , as used for tagging Ratnaparkhi , 1996 or earlier NER work Borthwick , 1999 .", "Figure 1 shows a graphical model representation of our character level HMM .", "Characters are emitted one at a time , and there is one state per character .", "Each state s identity depends only on the previous state .", "Each character s identity depends on both the current state and on the previous characters .", "In addition to this HMM view , it may also be convenient to think of the local emission models as type conditional gram models .", "Indeed , the character emission model in this section is directly based on the gram proper name classification engine described in Smarr and Manning , 2002 .", "The primary addition is the state transition chaining , which allows the model to do segmentation as well as classification .", "When using character level models for word evaluated tasks , one would not want multiple characters inside a single word to receive different labels .", "This can be avoided in two ways by explicitly locking state transitions inside words , or by careful choice of transition topology .", "In our current implementation , we do the latter .", "Each state is a pair where is an entity type such as PERSON , and including an other type and indicates the length of time the system has been in state .", "Therefore , a state like PERSON , 2 indicates the second letter inside a person phrase .", "The final letter of a phrase is a following space we insert one if there is none and the state is a special final state like PERSON , F .", "Additionally , once reaches our gram history order , it stays there .", "We then use empirical , unsmoothed estimates for statestate transitions .", "This annotation and estimation enforces consistent labellings in practice .", "For example , PERSON , 2 can only transition to the next state PERSON , 3 or the final state PERSON , F .", "Final states can only transition to beginning states , like other , 1 .", "For emissions , we must estimate a quantity of the form , for example , . 1 We use an gram model of order . 2 The gram estimates are smoothed via deleted interpolation .", "Given this model , we can do Viterbi decoding in the standard way .", "To be clear on what this model does and does not capture , we consider a few examples indicates a space .", "First , we might be asked for .", "In this case , we know both that we are in the middle of a location that begins with Denv and also that the preceding context was to .", "In essence , encoding into the state lets us distinguish the beginnings of phrases , which lets us model trends like named entities all the classes besides other generally starting with capital letters in English .", "Second , we may be asked for quantities like , which allows us to model the ends of phrases .", "Here we have a slight complexity by the notation , one would expect such emissions to have probability 1 , since nothing else can be emitted from a final state .", "In practice , we have a special stop symbol in our n gram counts , and the probability of emitting a space from a final state is the probability of the n gram having chosen the stop character . 3 models .", "The value was the empirically optimal order .", "3This can be cleaned up conceptually by considering the entire process to have been a hierarchical HMM Fine et al . , 1998 , where the gram model generates the entire phrase , followed by a tier pop up to the phrase transition tier .", "Using this model , we tested two variants , one in which preceding context was discarded for example , was turned into , and another where context was used as outlined above .", "For comparison , we also built a first order word level HMM ; the results are shown in table 1 .", "We give F both per category and overall .", "The word level model and the context disabled character level model are intended as a rough minimal pair , in that the only information crossing phrase boundaries was the entity type , isolating the effects of character vs word level modeling a more precise minimal pair is examined in section 3 .", "Switching to the character model raised the overall score greatly , from 74 . 5 to 82 . 2 .", "On top of this , context helped , but substantially less , bringing the total to 83 . 2 .", "We did also try to incorporate gazetteer information by adding gram counts from gazetteer entries to the training counts that back the above character emission model .", "However , this reduced performance by 2 . 0 with context on .", "The supplied gazetteers appear to have been built from the training data and so do not increase coverage , and provide only a flat distribution of name phrases whose empirical distributions are very spiked .", "Given the amount of improvement from using a model backed by character grams instead of word grams , the immediate question is whether this benefit is complementary to the benefit from features which have traditionally been of use in word level systems , such as syntactic context features , topic features , and so on .", "To test this , we constructed a maxent classifier which locally classifies single words , without modeling the entity type sequences . 4 These local classifiers map a feature representation of each word position to entity types , such as PERSON . 5 We present a hill climb over feature sets for the English development set data in table 2 .", "First , we tried only the local word as a feature ; the result was that each word was assigned its most common class in the training data .", "The overall F score was 52 . 29 , well below the official CoNLL baseline of 71 . 18 . 6 We next added gram features ; specifically , we framed each word with special start and end symbols , and then added every contiguous substring to the feature list .", "Note that this subsumes the entire word features .", "Using the substring features alone scored 73 . 10 , already breaking the the phrase based CoNLL baseline , though lower than the no context HMM , which better models the context inside phrases .", "Adding a current tag feature gave a score of 74 . 17 .", "At this point , the bulk of outstanding errors were plausibly attributable to insufficient context information .", "Adding even just the previous and next words and tags as atomic features raised performance to 82 . 39 .", "More complex , joint context features which paired the current word and tag with the previous and next words and tags raised the score further to 83 . 09 , nearly to the level of the HMM , still without actually having any model of previous classification decisions .", "In order to include state sequence features , which allow the classifications at various positions to interact , we have to abandon classifying each position independently .", "Sequence sensitive features can be included by chaining our local classifiers together and performing joint inference , i . e . , by building a conditional markov model CMM , also known as a maximum entropy markov model McCallum et al . , 2000 .", "Previous classification decisions are clearly relevant for example the sequence Grace Road is a single location , not a person s name adjacent to a location which is the erroneous output of the model in section 3 .", "Adding features representing the previous classification decision raised the score 2 . 35 to 85 . 44 .", "We found knowing that the previous word was an other wasn t particularly useful without also knowing its part of speech e . g . , a preceding preposition might indicate a location .", "Joint tag sequence features , along with longer distance sequence and tag sequence features , gave 87 . 21 .", "The remaining improvements involved a number of other features which directly targetted observed error types .", "These features included letter type pattern features for example 20 month would become d x for digitlowercase and Italy would become Xx for mixed case .", "This improved performance substantially , for example allowing the system to detect ALL CAPS regions .", "Table 3 shows an example of a local decision for Grace in the context at Grace Road , using all of the features defined to date .", "Note that the evidence against Grace as a name completely overwhelms the gram and word preference for PERSON .", "Other features included secondprevious and second next words when the previous or next words were very short and a marker for capitalized words whose lowercase forms had also been seen .", "The final system also contained some simple error driven postprocessing .", "In particular , repeated sub elements usually last names of multi word person names were given type PERSON , and a crude heuristic restoration of B prefixes was performed .", "In total , this final system had an F score of 92 . 31 on the English development set .", "Table 4 gives a more detailed breakdown of this score , and also gives the results of this system on the English test set , and both German data sets .", "The primary argument of this paper is that character substrings are a valuable , and , we believe , underexploited source of model features .", "In an HMM with an admittedly very local sequence model , switching from a word model to a character model gave an error reduction of about 30 .", "In the final , much richer chained maxent setting , the reduction from the best model minus gram features to the reported best model was about 25 smaller , but still substantial .", "This paper also again demonstrates how the ease of incorporating features into a discriminative maxent model allows for productive feature engineering ."], "summary_lines": ["Named Entity Recognition With Character-Level Models\n", "We discuss two named-entity recognition models which use characters and character n-grams either exclusively or as an important part of their data representation.\n", "The first model is a character-level HMM with minimal context information, and the second model is a maximum-entropy conditional markov model with substantially richer context features.\n", "Our best model achieves an overall F1 of 86.07% on the English test data (92.31% on the development data).\n", "This number represents a 25% error reduction over the same model without word-internal (substring) features.\n", "We find that the introduction of character n-gram features improved the overall F1 score by over 20%.\n"]}
{"article_lines": ["Labeled LDA A supervised topic model for credit attribution in multi labeled corpora", "A significant portion of the world s text is tagged by readers on social bookmarkwebsites . attribution an inherent problem in these corpora because most pages have multiple tags , but the tags do not always apply with equal specificity across the whole document .", "Solving the credit attribution problem requires associating each word in a document with the most appropriate tags and vice versa .", "This introduces a topic model that constrains Latent Dirichlet Allocation by defining a one to one correspondence between LDA s latent topics and user tags .", "This allows Labeled LDA to directly learn word tag correspondences .", "We demonstrate Labeled LDA s improved expressiveness over traditional LDA with visualizations of a corpus of tagged web from Labeled LDA outperforms SVMs by more than 3 to 1 when extracting tag specific document snippets .", "As a multi label text classifier , our model is competitive with a discriminative baseline on a variety of datasets .", "From news sources such as Reuters to modern community web portals like del . icio . us , a significant proportion of the world s textual data is labeled with multiple human provided tags .", "These collections reflect the fact that documents are often about more than one thing for example , a news story about a highway transportation bill might naturally be filed under both transportation and politics , with neither category acting as a clear subset of the other .", "Similarly , a single web page in del . icio . us might well be annotated with tags as diverse as arts , physics , alaska , and beauty .", "However , not all tags apply with equal specificity across the whole document , opening up new opportunities for information retrieval and corpus analysis on tagged corpora .", "For instance , users who browse for documents with a particular tag might prefer to see summaries that focus on the portion of the document most relevant to the tag , a task we call tag specific snippet extraction .", "And when a user browses to a particular document , a tag augmented user interface might provide overview visualization cues highlighting which portions of the document are more or less relevant to the tag , helping the user quickly access the information they seek .", "One simple approach to these challenges can be found in models that explicitly address the credit attribution problem by associating individual words in a document with their most appropriate labels .", "For instance , in our news story about the transportation bill , if the model knew that the word highway went with transportation and that the word politicians went with politics , more relevant passages could be extracted for either label .", "We seek an approach that can automatically learn the posterior distribution of each word in a document conditioned on the document s label set .", "One promising approach to the credit attribution problem lies in the machinery of Latent Dirichlet Allocation LDA Blei et al . , 2003 , a recent model that has gained popularity among theoreticians and practitioners alike as a tool for automatic corpus summarization and visualization .", "LDA is a completely unsupervised algorithm that models each document as a mixture of topics .", "The model generates automatic summaries of topics in terms of a discrete probability distribution over words for each topic , and further infers per document discrete distributions over topics .", "Most importantly , LDA makes the explicit assumption that each word is generated from one underlying topic .", "Although LDA is expressive enough to model multiple topics per document , it is not appropriate for multi labeled corpora because , as an unsupervised model , it offers no obvious way of incorporating a supervised label set into its learning procedure .", "In particular , LDA often learns some topics that are hard to interpret , and the model provides no tools for tuning the generated topics to suit an end use application , even when time and resources exist to provide some document labels .", "Several modifications of LDA to incorporate supervision have been proposed in the literature .", "Two such models , Supervised LDA Blei and McAuliffe , 2007 and DiscLDA Lacoste Julien et al . , 2008 are inappropriate for multiply labeled corpora because they limit a document to being associated with only a single label .", "Supervised LDA posits that a label is generated from each document s empirical topic mixture distribution .", "DiscLDA associates a single categorical label variable with each document and associates a topic mixture with each label .", "A third model , MM LDA Ramage et al . , 2009 , is not constrained to one label per document because it models each document as a bag of words with a bag of labels , with topics for each observation drawn from a shared topic distribution .", "But , like the other models , MM LDA s learned topics do not correspond directly with the label set .", "Consequently , these models fall short as a solution to the credit attribution problem .", "Because labels have meaning to the people that assigned them , a simple solution to the credit attribution problem is to assign a document s words to its labels rather than to a latent and possibly less interpretable semantic space .", "This paper presents Labeled LDA L LDA , a generative model for multiply labeled corpora that marries the multi label supervision common to modern text datasets with the word assignment ambiguity resolution of the LDA family of models .", "In contrast to standard LDA and its existing supervised variants , our model associates each label with one topic in direct correspondence .", "In the following section , L LDA is shown to be a natural extension of both LDA by incorporating supervision and Multinomial Naive Bayes by incorporating a mixture model .", "We demonstrate that L LDA can go a long way toward solving the credit attribution problem in multiply labeled documents with improved interpretability over LDA Section 4 .", "We show that L LDA s credit attribution ability enables it to greatly outperform supFigure 1 Graphical model of Labeled LDA unlike standard LDA , both the label set \u039b as well as the topic prior \u03b1 influence the topic mixture e . port vector machines on a tag driven snippet extraction task on web pages from del . icio . us Section 6 .", "And despite its generative semantics , we show that Labeled LDA is competitive with a strong baseline discriminative classifier on two multi label text classification tasks Section 7 .", "Labeled LDA is a probabilistic graphical model that describes a process for generating a labeled document collection .", "Like Latent Dirichlet Allocation , Labeled LDA models each document as a mixture of underlying topics and generates each word from one topic .", "Unlike LDA , L LDA incorporates supervision by simply constraining the topic model to use only those topics that correspond to a document s observed label set .", "The model description that follows assumes the reader is familiar with the basic LDA model Blei et al . , 2003 .", "Let each document d be represented by a tuple consisting of a list of word indices w d w1 , . . . , wNd and a list of binary topic presence absence indicators \u039b d l1 , . . . , lK where each wz E 1 , . . . , V and each lk E 0 , 1 .", "Here Nd is the document length , V is the vocabulary size and K the total number of unique labels in the corpus .", "We set the number of topics in Labeled LDA to be the number of unique labels K in the corpus .", "The generative process for the algorithm is found in Table 1 .", "Steps 1 and 2 drawing the multinomial topic distributions over vocabulary 3k for each topic k , from a Dirichlet prior 77 remain the same as for traditional LDA see Blei et al . , 2003 , page 4 .", "The traditional LDA model then draws a multinomial mixture distribution e d over all K topics , for each document d , from a Dirichlet prior \u03b1 .", "However , we would like to restrict e d to be defined only over the topics that correspond to , C3k is a vector consisting of the parameters of the multinomial distribution corresponding to the kth topic , \u03b1 are the parameters of the Dirichlet topic prior and 77 are the parameters of the word prior , while \u03a6k is the label prior for topic k . For the meaning of the projection matrix L d , please refer to Eq 1 . its labels A d .", "Since the word topic assignments zi see step 9 in Table 1 are drawn from this distribution , this restriction ensures that all the topic assignments are limited to the document s labels .", "Towards this objective , we first generate the document s labels A d using a Bernoulli coin toss for each topic k , with a labeling prior probability \u03a6k , as shown in step 5 .", "Next , we define the vector of document s labels to be X d k \u039b d k 1 .", "This allows us to define a document specific label projection matrix L d of size Md K for each document d , where Md X d , as follows For each row i 1 , . . . , Md and column j 1 , . . . , K In other words , the ith row of L d has an entry of 1 in column j if and only if the ith document label A d i is equal to the topic j , and zero otherwise .", "As the name indicates , we use the L d matrix to project the parameter vector of the Dirichlet topic prior \u03b1 \u03b11 , . . . , \u03b1K T to a lower dimensional vector \u03b1 d as follows Clearly , the dimensions of the projected vector correspond to the topics represented by the labels of the document .", "For example , suppose K 4 and that a document d has labels given by A d 0 , 1 , 1 , 0 which implies X d 2 , 3 , then L d Then , 0 d is drawn from a Dirichlet distribution with parameters \u03b1 d L d \u03b1 \u03b12 , \u03b13 T i . e . , with the Dirichlet restricted to the topics 2 and 3 .", "This fulfills our requirement that the document s topics are restricted to its own labels .", "The projection step constitutes the deterministic step 6 in Table 1 .", "The remaining part of the model from steps 7 through 10 are the same as for regular LDA .", "The dependency of 0 on both \u03b1 and A is indicated by directed edges from A and \u03b1 to 0 in the plate notation in Figure 1 .", "This is the only additional dependency we introduce in LDA s representation please compare with Figure 1 in Blei et al . , 2003 .", "In most applications discussed in this paper , we will assume that the documents are multiply tagged with human labels , both at learning and inference time .", "When the labels A d of the document are observed , the labeling prior \u03a6 is d separated from the rest of the model given A d .", "Hence the model is same as traditional LDA , except the constraint that the topic prior \u03b1 d is now restricted to the set of labeled topics X d .", "Therefore , we can use collapsed Gibbs sampling Griffiths and Steyvers , 2004 for training where the sampling probability for a topic for position i in a document d in Labeled LDA is given by where nwi i , j is the count of word wi in topic j , that does not include the current assignment zi , a missing subscript or superscript e . g . n i , j indicates a summation over that dimension , and 1 is a vector of 1 s of appropriate dimension .", "Although the equation above looks exactly the same as that of LDA , we have an important distinction in that , the target topic j is restricted to belong to the set of labels , i . e . , j X d .", "Once the topic multinomials , C3 are learned from the training set , one can perform inference on any new labeled test document using Gibbs sampling restricted to its tags , to determine its per word label assignments z .", "In addition , one can also compute its posterior distribution \u03b8 over topics by appropriately normalizing the topic assignments z .", "It should now be apparent to the reader how the new model addresses some of the problems in multi labeled corpora that we highlighted in Section 1 .", "For example , since there is a one to one correspondence between the labels and topics , the model can display automatic topical summaries for each label k in terms of the topic specific distribution \u03b2k .", "Similarly , since the model assigns a label zz to each word wz in the document d automatically , we can now extract portions of the document relevant to each label k it would be all words wz E w d such that zz k .", "In addition , we can use the topic distribution \u03b8 d to rank the user specified labels in the order of their relevance to the document , thereby also eliminating spurious ones if necessary .", "Finally , we note that other less restrictive variants of the proposed L LDA model are possible .", "For example , one could consider a version that allows topics that do not correspond to the label set of a given document with a small probability , or one that allows a common background topic in all documents .", "We did implement these variants in our preliminary experiments , but they did not yield better performance than L LDA in the tasks we considered .", "Hence we do not report them in this paper .", "The derivation of the algorithm so far has focused on its relationship to LDA .", "However , Labeled LDA can also be seen as an extension of the event model of a traditional Multinomial Naive Bayes classifier McCallum and Nigam , 1998 by the introduction of a mixture model .", "In this section , we develop the analogy as another way to understand L LDA from a supervised perspective .", "Consider the case where no document in the collection is assigned two or more labels .", "Now for a particular document d with label ld , Labeled LDA draws each word s topic variable zz from a multinomial constrained to the document s label set , i . e . zz ld for each word position i in the document .", "During learning , the Gibbs sampler will assign each zz to ld while incrementing Old wz , effectively counting the occurences of each word type in documents labeled with ld .", "Thus in the singly labeled document case , the probability of each document under Labeled LDA is equal to the probability of the document under the Multinomial Naive Bayes event model trained on those same document instances .", "Unlike the Multinomial Naive Bayes classifier , Labeled LDA does not encode a decision boundary for unlabeled documents by comparing P w d ld to P w d ld , although we discuss using Labeled LDA for multilabel classification in Section 7 .", "Labeled LDA s similarity to Naive Bayes ends with the introduction of a second label to any document .", "In a traditional one versus rest Multinomial Naive Bayes model , a separate classifier for each label would be trained on all documents with that label , so each word can contribute a count of 1 to every observed label s word distribution .", "By contrast , Labeled LDA assumes that each document is a mixture of underlying topics , so the count mass of single word instance must instead be distributed over the document s observed labels .", "Social bookmarking websites contain millions of tags describing many of the web s most popular and useful pages .", "However , not all tags are uniformly appropriate at all places within a document .", "In the sections that follow , we examine mechanisms by which Labeled LDA s credit assignment mechanism can be utilized to help support browsing and summarizing tagged document collections .", "To create a consistent dataset for experimenting with our model , we selected 20 tags of medium to high frequency from a collection of documents dataset crawled from del . icio . us , a popular social bookmarking website Heymann et al . , 2008 .", "From that larger dataset , we selected uniformly at random four thousand documents that contained at least one of the 20 tags , and then filtered each document s tag set by removing tags not present in our tag set .", "After filtering , the resulting corpus averaged 781 non stop words per document , with each document having 4 distinct tags on average .", "In contrast to many existing text datasets , our tagged corpus is highly multiply labeled almost 90 of of the documents have more than one tag .", "For comparison , less than one third of the news documents in the popular RCV1 v2 collection of newswire are multiply labeled .", "We will refer to this collection of data as the del . icio . us tag dataset .", "A first question we ask of Labeled LDA is how its topics compare with those learned by traditional LDA on the same collection of documents .", "We ran our implementations of Labeled LDA and LDA on the del . icio . us corpus described above .", "Both are based on the standard collapsed Gibbs sampler , with the constraints for Labeled LDA implemented as in Section 2 .", "Figure 2 Comparison of some of the 20 topics learned on del . icio . us by Labeled LDA left and traditional LDA right , with representative words for each topic shown in the boxes .", "Labeled LDA s topics are named by their associated tag .", "Arrows from right to left show the mapping of LDA topics to the closest Labeled LDA topic by cosine similarity .", "Tags not shown are design , education , english , grammar , history , internet , language , philosophy , politics , programming , reference , style , writing .", "Figure 2 shows the top words associated with 20 topics learned by Labeled LDA and 20 topics learned by unsupervised LDA on the del . icio . us document collection .", "Labeled LDA s topics are directly named with the tag that corresponds to each topic , an improvement over standard practice of inferring the topic name by inspection Mei et al . , 2007 .", "The topics learned by the unsupervised variant were matched to a Labeled LDA topic highest cosine similarity .", "The topics selected are representative compared to Labeled LDA , unmodified LDA allocates many topics for describing the largest parts of the The Elements of Style , William Strunk , Jr .", "Asserting that one must first know the rules to break them , this classic reference book is a must have for any student and conscientious writer .", "Intended for use in which the practice of composition is combined with the study of literature , it gives in brief space the principal requirements of plain English style and concentratesattention on the rules of usage and principles of composition most commonly violated . corpus and under represents tags that are less uncommon of the 20 topics learned , LDA learned multiple topics mapping to each of five tags web , culture , and computer , reference , and politics , all of which were common in the dataset and learned no topics that aligned with six tags books , english , science , history , grammar , java , and philosophy , which were rarer .", "In addition to providing automatic summaries of the words best associated with each tag in the corpus , Labeled LDA s credit attribution mechanism can be used to augment the view of a single document with rich contextual information about the document s tags .", "Figure 3 shows one web document from the collection , a page describing a guide to writing English prose .", "The 10 most common tags for that document are writing , reference , english , grammar , style , language , books , book , strunk , and education , the first eight of which were included in our set of 20 tags .", "In the figure , each word that has high posterior probability from one tag has been annotated with that tag .", "The red words come from the style tag , green from the grammar tag , blue from the reference tag , and black from the education tag .", "In this case , the model does very well at assigning individual words to the tags that , subjectively , seem to strongly imply the presence of that tag on this page .", "A more polished rendering could add subtle visual cues about which parts of a page are most appropriate for a particular set of tags .", "Tag Labeled LDA LDA Topic ID book image pdf review library posted read copyright books title works water map human life work science time world years sleep windows file version linux computerfree system software mac comment god jesus people gospel bible reply lord religion written applications spring open web java pattern eclipse development ajax people day link posted time comments back music jane permalink comments read nice post great april blog march june wordpress news information service web online project site free search home web images design content java css website articles page learning jun quote pro views added check anonymous card core power ghz life written jesus words made man called mark john person fact name house light radio media photography news music travel cover game review street public art health food city history science books L LDA this classic reference book is a must have for any student and conscientious writer .", "Intended for SVM the rules of usage and principles of composition most commonly violated .", "Search CONTENTS Bibliographic language L LDA the beginning of a sentence must refer to the grammatical subject 8 .", "Divide words at SVM combined with the study of literature , it gives in brief space the principal requirements of", "Another natural application of Labeled LDA s credit assignment mechanism is as a means of selecting snippets of a document that best describe its contents from the perspective of a particular tag .", "Consider again the document in Figure 3 .", "Intuitively , if this document were shown to a user interested in the tag grammar , the most appropriate snippet of words might prefer to contain the phrase rules of usage , whereas a user interested in the term style might prefer the title Elements of Style . To quantitatively evaluate Labeled LDA s performance at this task , we constructed a set of 29 recently tagged documents from del . icio . us that were labeled with two or more tags from the 20 tag subset , resulting in a total of 149 document , tag pairs .", "For each pair , we extracted a 15 word window with the highest tag specific score from the document .", "Two systems were used to score each window Labeled LDA and a collection of onevs rest SVMs trained for each tag in the system .", "L LDA scored each window as the expected probability that the tag had generated each word .", "For SVMs , each window was taken as its own document and scored using the tag specific SVM s un thresholded scoring function , taking the window with the most positive score .", "While a complete solution to the tag specific snippet extraction quality as extracted by L LDA and SVM .", "The center column is the number of document tag pairs for which a system s snippet was judged superior .", "The right column is the number of snippets for which all three annotators were in complete agreement numerator in the subset of document scored by all three annotators denominator . problem might be more informed by better linguistic features such as phrase boundaries , this experimental setup suffices to evaluate both kinds of models for their ability to appropriately assign words to underlying labels .", "Figure 3 shows some example snippets output by our system for this document .", "Note that while SVMs did manage to select snippets that were vaguely on topic , Labeled LDA s outputs are generally of superior subjective quality .", "To quantify this intuition , three human annotators rated each pair of snippets .", "The outputs were randomly labeled as System A or System B , and the annotators were asked to judge which system generated a better tag specific document subset .", "The judges were also allowed to select neither system if there was no clear winner .", "The results are summarized in Table 2 .", "L LDA was judged superior by a wide margin of the 149 judgments , L LDA s output was selected as preferable in 72 cases , whereas SVM s was selected in only 21 .", "The difference between these scores was highly significant p . 001 by the sign test .", "To quantify the reliability of the judgments , 51 of the 149 document tag pairs were labeled by all three annotators .", "In this group , the judgments were in substantial agreement , 1 with Fleiss Kappa at . 63 .", "Further analysis of the triply annotated subset yields further evidence of L LDA s advantage over SVM s 33 of the 51 were tag page pairs where L LDA s output was picked by at least one annotator as a better snippet although L LDA might not have been picked by the other annotators .", "And of those , 24 were unanimous in that all three judges selected L LDA s output .", "By contrast , only 10 of the 51 were tag page pairs where SVMs output was picked by at least one annotator , and of those , only 2 were selected unanimously .", "In the preceding section we demonstrated how Labeled LDA s credit attribution mechanism enabled effective modeling within documents .", "In this section , we consider whether L LDA can be adapted as an effective multi label classifier for documents as a whole .", "To answer that question , we applied a modified variant of L LDA to a multi label document classification problem given a training set consisting of documents with multiple labels , predict the set of labels appropriate for each document in a test set .", "Multi label classification is a well researched problem .", "Many modern approaches incorporate label correlations e . g . , Kazawa et al . 2004 , Ji et al .", "Others , like our algorithm are based on mixture models such as Ueda and Saito 2003 .", "However , we are aware of no methods that trade off label specific word distributions with document specific label distributions in quite the same way .", "In Section 2 , we discussed learning and inference when labels are observed .", "In the task of multilabel classification , labels are available at training time , so the learning part remains the same as discussed before .", "However , inferring the best set of labels for an unlabeled document at test time is more complex it involves assessing all label assignments and returning the assignment that has the highest posterior probability .", "However , this is not straight forward , since there are 2K possible label assignments .", "To make matters worse , the support of \u03b1 A d is different for different label assignments .", "Although we are in the process of developing an efficient sampling algorithm for this inference , for the purposes of this paper we make the simplifying assumption that the model reduces to standard LDA at inference , where the document is free to sample from any of the K topics .", "This is a reasonable assumption because allowing the model to explore the whole topic space for each document is similar to exploring all possible label assignments .", "The document s most likely labels can then be inferred by suitably thresholding its posterior probability over topics .", "As a baseline , we use a set of multiple one vsrest SVM classifiers which is a popular and extremely competitive baseline used by most previous papers see Kazawa et al . , 2004 ; Ueda and Saito , 2003 for instance .", "We scored each model based on Micro F1 and Macro F1 as our evaluation measures Lewis et al . , 2004 .", "While the former allows larger classes to dominate its results , the latter assigns an equal weight to all classes , providing us complementary information .", "We ran experiments on a corpus from the Yahoo directory , modeling our experimental conditions on the ones described in Ji et al . , 2008 . 2 We considered documents drawn from 8 top level categories in the Yahoo directory , where each document can be placed in any number of subcategories .", "The results were mixed , with SVMs ahead on one measure Labeled LDA beat SVMs on five out of eight datasets on MacroF1 , but didn t win on any datasets on MicroF1 .", "Results are presented in Table 3 .", "Because only a processed form of the documents was released , the Yahoo dataset does not lend itself well to error analysis .", "However , only 33 of the documents in each top level category were applied to more than one sub category , so the credit assignment machinery of L LDA was unused for the majority of documents .", "We therefore ran an artificial second set of experiments considering only those documents that had been given more than one label in the training data .", "On these documents , the results were again mixed , but Labeled LDA comes out ahead .", "For MacroF1 , L LDA beat SVMs on four datasets , SVMs beat L LDA on one dataset , and three were a statistical tie . 3 On MicroF1 , L LDA did much better than on the larger subset , outperforming on four datasets with the other four a statistical tie .", "It is worth noting that the Yahoo datasets are skewed by construction to contain many documents with highly overlapping content because each collection is within the same super class such as Arts , Business , etc . , each sub categories of the named Yahoo directory categories .", "Numbers in parentheses are standard deviations across runs .", "L LDA outperforms SVMs on 5 subsets with MacroF1 , but on no subsets with MicroF1 . vocabularies will naturally overlap a great deal .", "L LDA s credit attribution mechanism is most effective at partitioning semantically distinct words into their respective label vocabularies , so we expect that Labeled LDA s performance as a text classifier would improve on collections with more semantically diverse labels .", "We also applied our method to text classification on the del . icio . us dataset , where the documents are naturally multiply labeled more than 89 and where the tags are less inherently similar than in the Yahoo subcategories .", "Therefore we expect Labeled LDA to do better credit assignment on this subset and consequently to show improved performance as a classifier , and indeed this is the case .", "We evaluated L LDA and multiple one vs rest SVMs on 4000 documents with the 20 tag subset described in Section 3 .", "L LDA and multiple one vs rest SVMs were trained on the first 80 of documents and evaluated on the remaining 20 , with results averaged across 10 random permutations of the dataset .", "The results are shown in Table 4 .", "We tuned the SVMs shared cost parameter C 10 . 0 and selected raw term frequency over tf idf weighting based on 4 fold cross validation on 3 , 000 documents drawn from an independent permutation of the data .", "For L LDA , we tuned the shared parameters of threshold and proportionality constants in word and topic priors .", "L LDA and SVM have very similar performance on MacroF1 , while L LDA substantially outperforms on MicroF1 .", "In both cases , L LDA s improvement is statistically significantly by a 2 tailed paired t test at 95 confidence . multi label text classification for predicting 20 tags on del . icio . us data .", "L LDA outperforms SVMs significantly on both metrics by a 2 tailed , paired t test at 95 confidence .", "One of the main advantages of L LDA on multiply labeled documents comes from the model s document specific topic mixture \u03b8 .", "By explicitly modeling the importance of each label in the document , Labeled LDA can effective perform some contextual word sense disambiguation , which suggests why L LDA can outperform SVMs on the del . icio . us dataset .", "As a concrete example , consider the excerpt of text from the del . icio . us dataset in Figure 5 .", "The document itself has several tags , including design and programming .", "Initially , many of the likelihood probabilities p wllabel for the content words in this excerpt are higher for the label programming than design , including content , client , CMS and even designed , while design has higher likelihoods for just website and happy .", "However , after performing inference on this document using L LDA , the inferred document probability for design p design is much higher than it is for programming .", "In fact , the higher probability for the tag more than makes up the difference in the likelihood for all the words except CMS Content Management System , so underline words are generated from the design tag ; red dashed underline from the programming tag .", "By themselves , most words used here have a higher probability in programming than in design .", "But because the document as a whole is more about design than programming incorporating words not shown here , inferring the document s topic mixture \u03b8 enables L LDA to correctly re assign most words . that L LDA correctly infers that most of the words in this passage have more to do with design than programming .", "This paper has introduced Labeled LDA , a novel model of multi labeled corpora that directly addresses the credit assignment problem .", "The new model improves upon LDA for labeled corpora by gracefully incorporating user supervision in the form of a one to one mapping between topics and labels .", "We demonstrate the model s effectiveness on tasks related to credit attribution within documents , including document visualizations and tagspecific snippet extraction .", "An approximation to Labeled LDA is also shown to be competitive with a strong baseline multiple one vs rest SVMs for multi label classification .", "Because Labeled LDA is a graphical model in the LDA family , it enables a range of natural extensions for future investigation .", "For example , the current model does not capture correlations between labels , but such correlations might be introduced by composing Labeled LDA with newer state of the art topic models like the Correlated Topic Model Blei and Lafferty , 2006 or the Pachinko Allocation Model Li and McCallum , 2006 .", "And with improved inference for unsupervised \u039b , Labeled LDA lends itself naturally to modeling semi supervised corpora where labels are observed for only some documents .", "This project was supported in part by the President of Stanford University through the IRiSS Initiatives Assessment project ."], "summary_lines": ["Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora\n", "A significant portion of the world's text is tagged by readers on social bookmarking websites.\n", "Credit attribution is an inherent problem in these corpora because most pages have multiple tags, but the tags do not always apply with equal specificity across the whole document.\n", "Solving the credit attribution problem requires associating each word in a document with the most appropriate tags and vice versa.\n", "This paper introduces Labeled LDA, a topic model that constrains Latent Dirichlet Allocation by defining a one-to-one correspondence between LDA's latent topics and user tags.\n", "This allows Labeled LDA to directly learn word-tag correspondences.\n", "We demonstrate Labeled LDA's improved expressiveness over traditional LDA with visualizations of a corpus of tagged web pages from del.icio.us.\n", "Labeled LDA outperforms SVMs by more than 3 to 1 when extracting tag-specific document snippets.\n", "As a multi-label text classifier, our model is competitive with a discriminative baseline on a variety of datasets.\n", "L-LDA extends standard LDA to include supervision for specific target categories, and the generative process includes a second observed variable, i.e. each document is explicitly labeled with a target category.\n"]}
{"article_lines": ["The ICSI Meeting Recorder Dialog Act MRDA Corpus", "2 Data We describe a new corpus of over 180 , 000 handannotated dialog act tags and accompanying adjacency pair annotations for roughly 72 hours of speech from 75 naturally occurring meetings .", "We provide a brief summary of the annotation system and labeling procedure , inter annotator reliability statistics , overall distributional statistics , a description of auxiliary files distributed with the corpus , and information on how to obtain the data .", "We describe a new corpus of over 180 , 000 handannotated dialog act tags and accompanying adjacency pair annotations for roughly 72 hours of speech from 75 naturally occurring meetings .", "We provide a brief summary of the annotation system and labeling procedure , inter annotator reliability statistics , overall distributional statistics , a description of auxiliary files distributed with the corpus , and information on how to obtain the data .", "Natural meetings offer rich opportunities for studying a variety of complex discourse phenomena .", "Meetings contain regions of high speaker overlap , affective variation , complicated interaction structures , abandoned or interrupted utterances , and other interesting turn taking and discourse level phenomena .", "In addition , meetings that occur naturally involve real topics , debates , issues , and social dynamics that should generalize more readily to other real meetings than might data collected using artificial scenarios .", "Thus meetings pose interesting challenges to descriptive and theoretical models of discourse , as well as to researchers in the speech recognition community 4 , 7 , 9 , 13 , 14 , 15 .", "We describe a new corpus of hand annotated dialog acts and adjacency pairs for roughly 72 hours of naturally occurring multi party meetings .", "The meetings were recorded at the International Computer Science Institute ICSI as part of the ICSI Meeting Recorder Project 9 .", "Word transcripts and audio files from that corpus are available through the Linguistic Data Consortium LDC .", "In this paper , we provide a first description of the meeting recorder dialog act MRDA corpus , a companion set of annotations that augment the word transcriptions with discourse level segmentations , dialog act DA information , and adjacency pair information .", "The corpus is currently available online for research purposes 16 , and we plan a future release through the LDC .", "The ICSI Meeting Corpus data is described in detail in 9 .", "It consists of 75 meetings , each roughly an hour in length .", "There are 53 unique speakers in the corpus , and an average of about 6 speakers per meeting .", "Reflecting the makeup of the Institute , there are more male than female speakers 40 and 13 , respectively .", "There are a28 native English speakers , although many of the nonnative English speakers are quite fluent .", "Of the 75 meetings , 29 are meetings of the ICSI meeting recorder project itself , 23 are meetings of a research group focused on robustness in automatic speech recognition , 15 involve a group discussing natural language processing and neural theories of language , and 8 are miscellaneous meeting types .", "The last set includes 2 very interesting meetings involving the corpus transcribers as participants example included in 16 .", "Annotation involved three types of information marking of DA segment boundaries , marking of DAs themselves , and marking of correspondences between DAs adjacency pairs , 12 .", "Each type of annotation is described in detail in 7 .", "Segmentation methods were developed based on separating out speech regions having different discourse functions , but also paying attention to pauses and intonational grouping .", "To distinguish utterances that are prosodically one unit but which contain multiple DAs , we use a pipe bar in the annotations .", "This allows the researcher to either split or not split at the bar , depending on the research goals .", "We examined existing annotation systems , including 1 , 2 , 5 , 6 , 8 , 10 , 11 , for similarity to the style of interaction in the ICSI meetings .", "We found that SWBD DAMSL 11 , a system adapted from DAMSL 6 , provided a fairly good fit .", "Although our meetings were natural , and thus had real agenda items , the dialog was less like human human or human machine task oriented dialog added in MRDA .", "Tags in italics are based on the SWBD DAMSL version but have had meanings modified for MRDA .", "The ordering of tags in the table is explained as follows In the mapping of DAMSL tags to SWBD DAMSL tags in the SWBDDAMSL manual , tags were ordered in categories such as Communication Status , Information Requests , and so on .", "In the mapping of MRDA tags to SWBD DAMSL tags here , we have retained the same overall ordering of tags within the table , but we do not explicitly mark the higher level SWBD DAMSL categories in order to avoid confusion , since categorical structure differs in the two systems see 7 .", "e . g . , 1 , 2 , 10 and more like human human casual conversation 5 , 6 , 8 , 11 .", "Since we were working with English rather than Spanish , and did not view a large tag set as a problem , we preferred 6 , 11 over 5 , 8 for this work .", "We modified the system in 11 a number of ways , as indicated in Figure 1 and as explained further in 7 .", "The MRDA system requires one general tag per DA , and attaches a variable number of following specific tags .", "Excluding nonlabelable cases , there are 11 general tags and 39 specific tags .", "There are two disruption forms , , two types of indecipherable utterances x , and a non DA tag to denote rising tone rt .", "An interface allowed annotators to play regions of speech , modify transcripts , and enter DA and adjacency pair information , as well as other comments .", "Meetings were divided into 10 minute chunks ; labeling time averaged about 3 hours per chunk , although this varied considerably depending on the complexity of the dialog .", "An example from one of the meetings is shown in Figure 2 as an illustration of some of the types of interactions we observe in the corpus .", "Audio files and additional sample excerpts are available from 16 .", "In addition to the obvious high degree of overlap roughly one third of all words are overlapped note the explicit struggle for the floor indicated by the two failed floor grabbers fg by speakers c5 and c6 .", "Furthermore , 6 of the 19 total utterances express some form of agreement or disagreement arp , aa , and nd with previous utterances .", "Also , of the 19 utterances within the excerpt , 9 are incomplete due to interruption by another talker , as is typical of many regions in the corpus showing high speaker overlap .", "We find in related work that regions of high overlap correlate with high speaker involvement , or hot spots 15 .", "The example also provides a taste of the frequency and complexity of adjacency pair information .", "For example , within only half a minute , speaker c5 has interacted with speakers c3 and c6 , and speaker c6 has interacted with speakers c2 and c5 .", "We computed interlabeler reliability among the three labelers for both segmentation into DA units and DA labeling , using randomly selected excerpts from the 75 labeled meetings .", "Since agreement on DA segmentation does not appear to have standard associated metrics in the literature , we developed our own approach .", "The philosophy is that any difference in words at the beginning and or end of a DA could result in a different label for that DA , and the more words that are mismatched , the more likely the difference in label .", "As a very strict measure of reliability , we used the following approach 1 Take one labeler s transcript as a reference .", "2 Look at each other labeler s words .", "For each word , look at the utterance it comes from and see if the reference has the exact same utterance .", "3 If it does , there is a match .", "Match every word in the utterance , and then mark the matched utterance in the reference so it cannot be matched again this prevents felicitous matches due to identical repeated words .", "4 Repeat this process for each word in each reference labeler pair , and rotate to the next labeler as the reference .", "Note that this metric requires perfect matching of the full utterance a word is in for that word to be matched .", "For example in the following case , labelers agree on 3 segmentation locations , but the agreement on our metric is only 0 . 14 , since only 1 of 7 words is matched Overall segmentation results on this metric are provided by labeler pair in Table 1 .", "We examined agreement on DA labels using the Kappa statistic 3 , which adjusts for chance agreement .", "Because of the large number of unique full label combinations , we report Kappa values in Table 2 using various class mappings distributed with the corpus .", "Values are shown by labeler pair .", "The overall value of Kappa for our basic , six way classmap Map1 is 0 . 80 , representing good agreement for this type of task .", "We provide basic statistics based on the dialog act labels for the 75 meetings .", "If we ignore the tag marking rising intonation rt , since this is not a DA tag , we find 180 , 218 total tags .", "Table 3 shows the distribution of the tags in more detail .", "If instead we look at only the 11 obligatory general tags , for which there is one per DA , and if we split labels at the pipe bar , the total is 113 , 560 excluding tags that only include a disruption label .", "The distribution of general tags is shown in Table 4 .", "We include other useful information with the corpus .", "Word level time information is available , based on alignments from an automatic speech recognizer .", "Annotator comments are also provided .", "We suggest various ways to group the large set of labels into a smaller set of classes , depending on the research focus .", "Finally , the corpus contains information that may be useful in for developing automatic modeling of prosody , such as hand marked annotation of rising intonation .", "We thank Chuck Wooters , Don Baron , Chris Oei , and Andreas Stolcke for software assistance , Ashley Krupski for contributions to the annotation scheme , Andrei Popescu Belis for analysis and comments on a release of the 50 meetings , and Barbara Peskin and Jane Edwards for general advice and feedback .", "This work was supported by an ICSI subcontract to the University of Washington on a DARPA Communicator project , ICSI NSF ITR Award IIS 0121396 , SRI NASA Award NCC2 1256 , SRI NSF IRI 9619921 , an SRI DARPA ROAR project , an ICSI award from the Swiss National Science Foundation through the research network IM2 , and by the EU Framework 6 project on Augmented Multi party Interaction AMI .", "The views are those of the authors and do not represent the views of the funding agencies ."], "summary_lines": ["The ICSI Meeting Recorder Dialog Act (MRDA) Corpus\n", "We describe a new corpus of over 180,000 hand-annotated dialog act tags and accompanying adjacency pair annotations for roughly 72 hours of speech from 75 naturally-occurring meetings.\n", "We provide a brief summary of the annotation system and labeling procedure, inter-annotator reliability statistics, overall distributional statistics, a description of auxiliary files distributed with the corpus, and information on how to obtain the data.\n"]}
{"article_lines": ["A Practical Part Of Speech Tagger", "We present an implementation of a part of speech tagger based on a hidden Markov model .", "The methodology enables robust and accurate tagging with few resource requirements .", "Only a lexicon and some unlabeled training text are required .", "Accuracy exceeds 96 .", "We describe implementation strategies and optimizations which result in high speed operation .", "Three applications for tagging are described phrase recognition ; word sense disambiguation ; and grammatical function assignment .", "1 Desiderata Many words are ambiguous in their part of speech .", "For example , quot ; tag quot ; can be a noun or a verb .", "However , when a word appears in the context of other words , the ambiguity is often reduced in quot ; a tag is a part of speech label , quot ; the quot ; tag quot ; can only be a noun .", "A tagger is a system that uses context to assign parts of speech to words .", "Automatic text tagging is an important first step in discovering the linguistic structure of large text corpora .", "Part of speech information facilitates higher level analysis , such as recognizing noun phrases and other patterns in text .", "For a tagger to function as a practical component in a language processing system , we believe that a tagger must be corpora contain ungrammatical constructions , isolated phrases such as titles , and nonlinguistic data such as tables .", "Corpora are also likely to contain words that are unknown to the tagger .", "It is desirable that a tagger deal gracefully with these situations . a tagger is to be used to analyze arbitrarily large corpora , it must be efficient performing in time linear in the number of words tagged .", "Any training required should also be fast , enabling rapid turnaround with new corpora and new text genres .", "A should attempt to assign the correct part of speech tag to every word encountered .", "A should be able to take advantage of linguistic insights .", "One should be able to correct errors by supplying appropriate priori quot ; hints . quot ; It should be possible to give different hints for different corpora . effort required to retarget a tagger to new corpora , new tagsets , and new languages should be minimal .", "2 Methodology 2 . 1 Background Several different approaches have been used for building text taggers .", "Greene and Rubin used a rule based approach in the TAGGIT program Greene and Rubin , 1971 , which was an aid in tagging the Brown corpus Francis and KuEera , 1982 .", "TAGGIT disambiguated 77 of the corpus ; the rest was done manually over a period of several years .", "More recently , Koskenniemi also used a rule based approach implemented with finite state machines Koskenniemi , 1990 .", "Statistical methods have also been used e . g . , DeRose , Garside al . , These provide the capability of resolving ambiguity on the basis of most likely interpretation .", "A form of Markov model has been widely used that assumes that a word depends probabilistically on just its part of speech category , which in turn depends solely on the categories of the preceding two words .", "Two types of training i . e . , parameter estimation have been used with this model .", "The first makes use of a tagged training corpus .", "Derouault and Merialdo use a bootstrap method for training Derouault and Merialdo , 1986 .", "At first , a relatively small amount of text is manually tagged and used to train a partially accurate model .", "The model is then used to tag more text , and the tags are manually corrected and then used to retrain the model .", "Church uses the tagged Brown corpus for training Church , 1988 .", "These models involve probabilities for each word in the lexicon , so large tagged corpora are required for reliable estimation .", "The second method of training does not require a tagged training corpus .", "In this situation the Baum Welch algorithm also known as the forward backward algorithm can be used Baum , 1972 .", "Under this regime the model is a Markov model as state transitions i . e . , part of speech categories are assumed to be unobservable .", "Jelinek has used this method for training a text tagger Jelinek , 1985 .", "Parameter smoothing can be conachieved using the method of interpolawhich weighted estimates are taken from secondand first order models and a uniform probability distribution Jelinek and Mercer , 1980 .", "Kupiec used word equivclasses referred to here as classes on parts of speech , to pool data from individual words Kupiec , 1989b .", "The most common words are still represented individually , as sufficient data exist for robust estimation .", "133 However all other words are represented according to the set of possible categories they can assume .", "In this manner , the vocabulary of 50 , 000 words in the Brown corpus can be reduced to approximately 400 distinct ambiguity classes Kupiec , 1992 .", "To further reduce the number of parameters , a first order model can be employed this assumes that a word's category depends only on the immediately preceding word's category .", "In Kupiec , 1989a , networks are used to selectively augment the context in a basic firstorder model , rather than using uniformly second order dependencies .", "2 . 2 Our approach We next describe how our choice of techniques satisfies the listed in section 1 .", "The use of an complete flexibility in the choice of training corpora .", "Text from any desired domain can be used , and a tagger can be tailored for use with a particular text database by training on a portion of that database .", "Lexicons containing alternative tag sets can be easily accommodated without any need for re labeling the training corpus , affording further flexibility in the use of specialized tags .", "As the resources required are simply a lexicon and a suitably large sample of ordinary text , taggers can be built with minimal effort , even for other languages , such as French e . g . , Kupiec , 1992 .", "The use of ambiguity classes and a first order model reduces the number of parameters to be estimated without significant reduction in accuracy discussed in section 5 .", "This also enables a tagger to be reliably trained using only moderate amounts of text .", "We have produced reasonable results training on as few as 3 , 000 sentences .", "Fewer parameters also reduce the time required for training .", "Relatively few ambiguity classes are sufficient for wide coverage , so it is unlikely that adding new words to the lexicon requires retraining , as their ambiguity classes are already accommodated .", "Vocabulary independence is achieved by predicting categories for words not in the lexicon , using both context and suffix information .", "Probabilities corresponding to category sequences that never occurred in the training data are assigned small , non zero values , ensuring that the model will accept any sequence of tokens , while still providing the most likely tagging .", "By using the fact that words are typically associated with only a few part ofspeech categories , and carefully ordering the computation , the algorithms have linear complexity section 3 . 3 .", "3 Hidden Markov Modeling The hidden Markov modeling component of our tagger is implemented as an independent module following the specgiven in Levinson et with special attention to space and time efficiency issues .", "Only first order modeling is addressed and will be presumed for the remainder of this discussion .", "3 . 1 Formalism brief , an a doubly stochastic process that generates sequence of symbols Sl , S2 , , 1 i where W is some finite set of possible symbols , by composing an underlying Markov process with a state dependent symbol generator i . e . , a Markov process with noise . '", "Th Markov process captures the notion of sequence depen and is described by a set of a matrix c probabilities A 1 a , the probability of moving from state i to state of initial probabilities H 70 1 i is the probability of starting in state i .", "The symbol ger erator is a state dependent measure on V described by of symbol probabilities 1 j M IW I is the probability symbol given that the Markov process is i In part of speech tagging , we will model word order di pendency through an underlying Markov process that 01 erates in terms of lexical tags , ' yet we will only be ab to observe the sets of tags , or ambiguity classes , that ai possible for individual words .", "The ambiguity class of eac word is the set of its permitted parts of speech , only or of which is correct in context .", "Given the parameters . 4 , Markov modeling allows us to compute ti most probable sequence of state transitions , and hence a mostly likely sequence of lexical tags , corresponding to of ambiguity classes .", "In the following , identified with the number of possible . tags , and W wil the set of all ambiguity classes .", "Applying an HMM consists of two tasks estimating ti parameters A , a training set ; ar computing the most likely sequence of underlying sta transitions given new observations .", "Maximum likeliho estimates that is , estimates that maximize the probabili of the training set can be found through application of z ternating expectation in a procedure known as the Baur Welch , or forward backward , algorithm Baum , 1972 . proceeds by recursively defining two sets of probabiliti the forward probabilities eft 1 .", "i bi St i t T 1 , for all the backward prob bilities , i3 i 1 t 1 , 1 . 1 1 for all forward probabili the joint probability of the sequence up to tir t , S2 , , the event that the Markov pr is in state i at time the backwa is the probability of seeing the sequen , ST that the Markov process is state i at time t . It follows that the probability of t entire sequence is N N", "Many words are ambiguous in their part of speech .", "For example , quot ; tag quot ; can be a noun or a verb .", "However , when a word appears in the context of other words , the ambiguity is often reduced in quot ; a tag is a part of speech label , quot ; the word quot ; tag quot ; can only be a noun .", "A part of speech tagger is a system that uses context to assign parts of speech to words .", "Automatic text tagging is an important first step in discovering the linguistic structure of large text corpora .", "Part of speech information facilitates higher level analysis , such as recognizing noun phrases and other patterns in text .", "For a tagger to function as a practical component in a language processing system , we believe that a tagger must be Robust Text corpora contain ungrammatical constructions , isolated phrases such as titles , and nonlinguistic data such as tables .", "Corpora are also likely to contain words that are unknown to the tagger .", "It is desirable that a tagger deal gracefully with these situations .", "Efficient If a tagger is to be used to analyze arbitrarily large corpora , it must be efficient performing in time linear in the number of words tagged .", "Any training required should also be fast , enabling rapid turnaround with new corpora and new text genres .", "Accurate A tagger should attempt to assign the correct part of speech tag to every word encountered .", "Tunable A tagger should be able to take advantage of linguistic insights .", "One should be able to correct systematic errors by supplying appropriate a priori quot ; hints . quot ; It should be possible to give different hints for different corpora .", "Reusable The effort required to retarget a tagger to new corpora , new tagsets , and new languages should be minimal .", "Several different approaches have been used for building text taggers .", "Greene and Rubin used a rule based approach in the TAGGIT program Greene and Rubin , 1971 , which was an aid in tagging the Brown corpus Francis and KuEera , 1982 .", "TAGGIT disambiguated 77 of the corpus ; the rest was done manually over a period of several years .", "More recently , Koskenniemi also used a rule based approach implemented with finite state machines Koskenniemi , 1990 .", "Statistical methods have also been used e . g . , DeRose , 1988 , Garside et al . , 1987 .", "These provide the capability of resolving ambiguity on the basis of most likely interpretation .", "A form of Markov model has been widely used that assumes that a word depends probabilistically on just its part of speech category , which in turn depends solely on the categories of the preceding two words .", "Two types of training i . e . , parameter estimation have been used with this model .", "The first makes use of a tagged training corpus .", "Derouault and Merialdo use a bootstrap method for training Derouault and Merialdo , 1986 .", "At first , a relatively small amount of text is manually tagged and used to train a partially accurate model .", "The model is then used to tag more text , and the tags are manually corrected and then used to retrain the model .", "Church uses the tagged Brown corpus for training Church , 1988 .", "These models involve probabilities for each word in the lexicon , so large tagged corpora are required for reliable estimation .", "The second method of training does not require a tagged training corpus .", "In this situation the Baum Welch algorithm also known as the forward backward algorithm can be used Baum , 1972 .", "Under this regime the model is called a hidden Markov model HMM , as state transitions i . e . , part of speech categories are assumed to be unobservable .", "Jelinek has used this method for training a text tagger Jelinek , 1985 .", "Parameter smoothing can be conveniently achieved using the method of deleted interpolation in which weighted estimates are taken from secondand first order models and a uniform probability distribution Jelinek and Mercer , 1980 .", "Kupiec used word equivalence classes referred to here as ambiguity classes based on parts of speech , to pool data from individual words Kupiec , 1989b .", "The most common words are still represented individually , as sufficient data exist for robust estimation .", "However all other words are represented according to the set of possible categories they can assume .", "In this manner , the vocabulary of 50 , 000 words in the Brown corpus can be reduced to approximately 400 distinct ambiguity classes Kupiec , 1992 .", "To further reduce the number of parameters , a first order model can be employed this assumes that a word's category depends only on the immediately preceding word's category .", "In Kupiec , 1989a , networks are used to selectively augment the context in a basic firstorder model , rather than using uniformly second order dependencies .", "We next describe how our choice of techniques satisfies the criteria listed in section 1 .", "The use of an HMM permits complete flexibility in the choice of training corpora .", "Text from any desired domain can be used , and a tagger can be tailored for use with a particular text database by training on a portion of that database .", "Lexicons containing alternative tag sets can be easily accommodated without any need for re labeling the training corpus , affording further flexibility in the use of specialized tags .", "As the resources required are simply a lexicon and a suitably large sample of ordinary text , taggers can be built with minimal effort , even for other languages , such as French e . g . , Kupiec , 1992 .", "The use of ambiguity classes and a first order model reduces the number of parameters to be estimated without significant reduction in accuracy discussed in section 5 .", "This also enables a tagger to be reliably trained using only moderate amounts of text .", "We have produced reasonable results training on as few as 3 , 000 sentences .", "Fewer parameters also reduce the time required for training .", "Relatively few ambiguity classes are sufficient for wide coverage , so it is unlikely that adding new words to the lexicon requires retraining , as their ambiguity classes are already accommodated .", "Vocabulary independence is achieved by predicting categories for words not in the lexicon , using both context and suffix information .", "Probabilities corresponding to category sequences that never occurred in the training data are assigned small , non zero values , ensuring that the model will accept any sequence of tokens , while still providing the most likely tagging .", "By using the fact that words are typically associated with only a few part ofspeech categories , and carefully ordering the computation , the algorithms have linear complexity section 3 . 3 .", "The hidden Markov modeling component of our tagger is implemented as an independent module following the specification given in Levinson et al . , 1983 , with special attention to space and time efficiency issues .", "Only first order modeling is addressed and will be presumed for the remainder of this discussion .", "In brief , an HMM is a doubly stochastic process that generates sequence of symbols S Sl , S2 , , ST , S , EW 1 i T , where W is some finite set of possible symbols , by composing an underlying Markov process with a state dependent symbol generator i . e . , a Markov process with noise . '", "Th Markov process captures the notion of sequence depen dency and is described by a set of N states , a matrix c transition probabilities A la , 11 1 i , j N where a , is the probability of moving from state i to state j , and vector of initial probabilities H 70 1 i N where 71 is the probability of starting in state i .", "The symbol ger erator is a state dependent measure on V described by matrix of symbol probabilities B kik 1 j N an 1 k M where M IW I and kik is the probability generating symbol sk given that the Markov process is i state j . 2 In part of speech tagging , we will model word order di pendency through an underlying Markov process that 01 erates in terms of lexical tags , ' yet we will only be ab to observe the sets of tags , or ambiguity classes , that ai possible for individual words .", "The ambiguity class of eac word is the set of its permitted parts of speech , only or of which is correct in context .", "Given the parameters . 4 , and II , hidden Markov modeling allows us to compute ti most probable sequence of state transitions , and hence a mostly likely sequence of lexical tags , corresponding to sequence of ambiguity classes .", "In the following , N can identified with the number of possible . tags , and W wil the set of all ambiguity classes .", "Applying an HMM consists of two tasks estimating ti model parameters A , B and H from a training set ; ar computing the most likely sequence of underlying sta transitions given new observations .", "Maximum likeliho estimates that is , estimates that maximize the probabili of the training set can be found through application of z ternating expectation in a procedure known as the Baur Welch , or forward backward , algorithm Baum , 1972 . proceeds by recursively defining two sets of probabiliti the forward probabilities where ot1 i ribi 51 for all i ; and the backward prob bilities , where OT j 1 for all j .", "The forward probabili oet i is the joint probability of the sequence up to tir t , S1 , S2 , , St , and the event that the Markov pr cess is in state i at time t . Similarly , the backwa probability Ot j is the probability of seeing the sequen St i , St 2 , , ST given that the Markov process is state i at time t . It follows that the probability of t entire sequence is for any tin the range 1 t T 1 . 3 'For an introduction to hidden Markov modeling see F biner and Juang , 1986 .", "Given an initial choice for the parameters A , B , and H the expected number of transitions , , from state i to state j conditioned on the observation sequence S may be computed as follows rescale .", "One approach premultiplies the a and 0 probabilities with an accumulating product depending on t Levinson et al . , 1983 .", "Let ei1 i ai i and define and 1 .", "Y1 001 0 .", "5 In summary , to find maximum likelihood estimates for A , B , and H , via the Baum Welch algorithm , one chooses some starting values , applies equations 3 5 to compute new values , and then iterates until convergence .", "It can be shown that this algorithm will converge , although possibly to a non global maximum Baum , 19721 .", "Once a model has been estimated , selecting the most likely underlying sequence of state transitions corresponding to an observation S can be thought of as a maximization over all sequences that might generate S . An efficient dynamic programming procedure , known as the Viterbi algorithm Viterbi , 1967 , arranges for this computation to proceed in time proportional to T . Suppose V v t 1 t T is a state sequence that generates S , then the probability that V generates S is , To find the most probable such sequence we start by defining 01 i rabi Si for 1 i N and then perform the recursion for 2 t T and 1 j N . The crucial observation is that for each time t and each state i one need only consider the most probable sequence arriving at state i at time t . The probability of the most probable sequence is maxi , N OT i while the sequence itself can be reconstructed by defining v T max N OT i and v t 1 Ike qt for T t 2 .", "The Baum Welch algorithm equations 1 5 and the Viterbi algorithm equation 6 involve operations on products of numbers constrained to be between 0 and 1 .", "Since these products can easily underflow , measures must be taken to Now define t i cicit i and use a' in place of a in equation 1 to define for the next iteration Note that Ein_i eet i 1 for 1 t T . Similarly , let OT i OT i and define t i ct4t i for T t 1 where The scaled backward and forward probabilities , and 0 , can be exchanged for the unscaled probabilities in equations 3 5 without affecting the value of the ratios .", "To see this , note that t i Cat i and 0t i where Now , in terms of the scaled probabilities , equation 5 , for example , can be seen to be unchanged A slight difficulty occurs in equation 3 that can be cured by the addition of a new term , ct i , in each product of the upper sum Numerical instability in the Viterbi algorithm can be ameliorated by operating on a logarithmic scale Levinson et al . , 1983 .", "That is , one maximizes the log probability of each sequence of state transitions , Care must be taken with zero probabilities .", "However , this can be elegantly handled through the use of IEEE negative infinity P754 , 1981 .", "As can be seen from equations 1 5 , the time cost of training is 0 TN2 .", "Similarly , as given in equation 6 , the Viterbi algorithm is also 0 TN2 .", "However , in part of speech tagging , the problem structure dictates that the matrix of symbol probabilities B is sparsely populated .", "That is , 0 if the ambiguity class corresponding to symbol j includes the part of speech tag associated with state i .", "In practice , the degree of overlap between ambiguity classes is relatively low ; some tokens are assigned unique tags , and hence have only one non zero symbol probability .", "The sparseness of B leads one to consider restructuring equations 1 6 so a check for zero symbol probability can obviate the need for further computation .", "Equation 1 is already conveniently factored so that the dependence on bj St i is outside the inner sum .", "Hence , if k is the average number of non zero entries in each row of B , the cost of computing equation 1 can be reduced to 0 kTN .", "Equations 2 4 can be similarly reduced by switching the order of iteration .", "For example , in equation 2 , rather than for a given t computing ot i for each i one at a time , one can accumulate terms for all i in parallel .", "The net effect of this rewriting is to place a b3 St i 0 check outside the innermost iteration .", "Equations 3 and 4 submit to a similar approach .", "Equation 5 is already only 0 N .", "Hence , the overall cost of training can be reduced to 0 kTN , which , in our experience , amounts to an order of magnitude speedup .", "4 The time complexity of the Viterbi algorithm can also be reduced to 0 kTN by noting that b3 S can be factored out of the maximization of equation 6 .", "Adding up the sizes of the probability matrices A , B , and H , it is easy to see that the storage cost for directly representing one model is proportional to N N M 1 .", "Running the Baum Welch algorithm requires storage for the sequence of observations , the a and 3 probabilities , the vector c , and copies of the A and B matrices since the originals cannot be overwritten until the end of each iteration .", "Hence , the grand total of space required for training is proportional to T 2N T N M 1 .", "Since N and M are fixed by the model , the only parameter that can be varied to reduce storage costs is T . Now , adequate training requires processing from tens of thousands to hundreds of thousands of tokens Kupiec , 1989a .", "The training set can be considered one long sequence , it which case T is very large indeed , or it can be broken up into a number of smaller sequences at convenient boundaries .", "In first order hidden Markov modeling , the stochastic process effectively restarts at unambiguous tokens , such as sentence and paragraph markers , hence these tokens are convenient points at which to break the training set .", "If the Baum Welch algorithm is run separately from the same starting point on each piece , the resulting trained models must be recombined in some way .", "One obvious approach is simply to average .", "However , this fails if any two 'An equivalent approach maintains a mapping from states i to non zero symbol probabilities and simply avoids , in the inner iteration , computing products which must be zero Kupiec , 1992 . states are indistinguishable in the sense that they had the same transition probabilities and the same symbol probabilities at start , because states are then not matched across trained models .", "It is therefore important that each state have a distinguished role , which is relatively easy to achieve in part of speech tagging .", "Our implementation of the Baum Welch algorithm breaks up the input into fixed sized pieces of training text .", "The Baum Welch algorithm is then run separately on each piece and the results are averaged together .", "Running the Viterbi algorithm requires storage for the sequence of observations , a vector of current maxes , a scratch array of the same size , and a matrix of i , b indices , for a total proportional to T N 2 T and a grand total including the model of T N N M T 3 .", "Again , N and M are fixed .", "However , T need not be longer than a single sentence , since , as was observed above , the HMM , and hence the Viterbi algorithm , restarts at sentence boundaries .", "An HMM for part of speech tagging can be tuned in a variety of ways .", "First , the choice of tagset and lexicon determines the initial model .", "Second , empirical and a priori information can influence the choice of starting values for the Baum Welch algorithm .", "For example , counting instances of ambiguity classes in running text allows one to assign non uniform starting probabilities in A for a particular tag's realization as a particular ambiguity class .", "Alternatively , one can state a priori that a particular ambiguity class is most likely to be the reflection of some subset of its component tags .", "For example , if an ambiguity class consisting of the open class tags is used for unknown words , one may encode the fact that most unknown words are nouns or proper nouns by biasing the initial probabilities in B .", "Another biasing of starting values can arises from noting that some tags are unlikely to be followed by others .", "For example , the lexical item quot ; to quot ; maps to an ambiguity class containing two tags , infinitive marker and to aspreposition , neither of which occurs in any other ambiguity class .", "If nothing more were stated , the HMM would have two states which were indistinguishable .", "This can be remedied by setting the initial transition probabilities from infinitive marker to strongly favor transitions to such states as verb uninflected and adverb .", "Our implementation allows for two sorts of biasing of starting values ambiguity classes can be annotated with favored tags ; and states can be annotated with favored transitions .", "These biases may be specified either as sets or as set complements .", "Biases are implemented by replacing the disfavored probabilities with a small constant machine epsilon and redistributing mass to the other possibilities .", "This has the effect of disfavoring the indicated outcomes without disallowing them ; sufficient converse data can rehabilitate these values .", "In support of this and other work , we have developed a system architecture for text access Cutting et al . , 1991 .", "This architecture defines five components for such systems corpus , which provides text in a generic manner ; analysis , which extracts terms from the text ; index which stores term occurrence statistics ; and search , which utilizes these statistics to resolve queries .", "The part of speech tagger described here is implemented as an analysis module .", "Figure 1 illustrates the overall architecture , showing the tagger analysis implementation in detail .", "The tagger itself has a modular architecture , isolating behind standard protocols those elements which may vary , enabling easy substitution of alternate implementations .", "Also illustrated here are the data types which flow between tagger components .", "As an analysis implementation , the tagger must generate terms from text .", "In this context , a term is a word stem annotated with part of speech .", "Text enters the analysis sub system where the first processing module it encounters is the tokenizer , whose duty is to convert text a sequence of characters into a sequence of tokens .", "Sentence boundaries are also identified by the tokenizer and are passed as reserved tokens .", "The tokenizer subsequently passes tokens to the lexicon .", "Here tokens are converted into a set of stems , each annotated with a part of speech tag .", "The set of tags identifies an ambiguity class .", "The identification of these classes is also the responsibility of the lexicon .", "Thus the lexicon delivers a set of stems paired with tags , and an ambiguity class .", "The training module takes long sequences of ambiguity classes as input .", "It uses the Baum Welch algorithm to produce a trained HMM , an input to the tagging module .", "Training is typically performed on a sample of the corpus at hand , with the trained HMM being saved for subsequent use on the corpus at large .", "The tagging module buffers sequences of ambiguity classes between sentence boundaries .", "These sequences are disambiguated by computing the maximal path through the HMM with the Viterbi algorithm .", "Operating at sentence granularity provides fast throughput without loss of accuracy , as sentence boundaries are unambiguous .", "The resulting sequence of tags is used to select the appropriate stems .", "Pairs of stems and tags are subsequently emitted .", "The tagger may function as a complete analysis component , providing tagged text to search and indexing components , or as a sub system of a more elaborate analysis , such as phrase recognition .", "The problem of tokenization has been well addressed by much work in compilation of programming languages .", "The accepted approach is to specify token classes with regular expressions .", "These may be compiled into a single deterministic finite state automaton which partitions character streams into labeled tokens Aho et al . , 1986 , Lesk , 19751 .", "In the context of tagging , we require at least two token classes sentence boundary and word .", "Other classes may include numbers , paragraph boundaries and various sorts of punctuation e . g . , braces of various types , commas .", "However , for simplicity , we will henceforth assume only words and sentence boundaries are extracted .", "Just as with programming languages , with text it is not always possible to unambiguously specify the required token classes with regular expressions .", "However the addition of a simple lookahead mechanism which allows specification of right context ameliorates this Aho et al . , 1986 , Lesk , 1975 .", "For example , a sentence boundary in English text might be identified by a period , followed bywhitespace , followed by an uppercase letter .", "However the uppercase letter must not be consumed , as it is the first component of the next token .", "A lookahead mechanism allows us to specify in the sentence boundary regular expression that the final character matched should not be considered a part of the token .", "This method meets our stated goals for the overall system .", "It is efficient , requiring that each character be examined only once modulo lookahead .", "It is easily parameterizable , providing the expressive power to concisely define accurate and robust token classes .", "The lexicon module is responsible for enumerating parts of speech and their associated stems for each word it is given .", "For the English word quot ; does , quot ; the lexicon might return quot ; do , verb quot ; and quot ; doe , plural noun . quot ; It is also responsible for identifying ambiguity classes based upon sets of tags .", "We have employed a three stage implementation First , we consult a manually constructed lexicon to find stems and parts of speech .", "Exhaustive lexicons of this sort are expensive , if not impossible , to produce .", "Fortunately , a small set of words accounts for the vast majority of word occurences .", "Thus high coverage can be obtained without prohibitive effort .", "Words not found in the manually constructed lexicon are generally both open class and regularly inflected .", "As a second stage , a language specific method can be employed to guess ambiguity classes for unknown words .", "For many languages e . g . , English and French , word suffixes provide strong cues to words' possible categories .", "Probabalistic predictions of a word's category can be made by analyzing suffixes in untagged text Kupiec , 1992 , Meteer et al . , 1991 .", "As a final stage , if a word is not in the manually constructed lexicon , and its suffix is not recognized , a default ambiguity class is used .", "This class typically contains all the open class categories in the language .", "Dictionaries and suffix tables are both efficiently implementable as letter trees , or tries Knuth , 1973 , which require that each character of a word be examined only once during a lookup .", "In this section , we detail how our tagger meets the desiderata that we outlined in section 1 .", "The system is implemented in Common Lisp Steele , 1990 .", "All timings reported are for a Sun SPARCStation2 .", "The English lexicon used contains 38 tags M 38 and 174 ambiguity classes N 174 .", "Training was performed on 25 , 000 words in articles selected randomly from Grolier's Encyclopedia .", "Five iterations of training were performed in a total time of 115 CPU seconds .", "Following is a time breakdown by component Training average pseconds per token tokenizer lexicon 1 iteration 5 iterations total 640 400 680 3400 4600 Tagging was performed on 115 , 822 words in a collection of articles by the journalist Dave Barry .", "This required a total of of 143 CPU seconds .", "The time breakdown for this was as follows Tagging average pseconds per token tokenizer lexicon Viterbi total 604 388 233 1235 It can be seen from these figures that training on a new corpus may be accomplished in a matter of minutes , and that tens of megabytes of text may then be tagged per hour .", "When using a lexicon and tagset built from the tagged text of the Brown corpus Francis and KuEera , 1982 , training on one half of the corpus about 500 , 000 words and tagging the other , 96 of word instances were assigned the correct tag .", "Eight iterations of training were used .", "This level of accuracy is comparable to the best achieved by other taggers Church , 1988 , Merialdo , 1991 .", "The Brown Corpus contains fragments and ungrammaticalities , thus providing a good demonstration of robustness .", "A tagger should be tunable , so that systematic tagging errors and anomalies can be addressed .", "Similarly , it is important that it be fast and easy to target the tagger to new genres and languages , and to experiment with different tagsets reflecting different insights into the linguistic phenomena found in text .", "In section 3 . 5 , we describe how the HMM implementation itself supports tuning .", "In addition , our implementation supports a number of explicit parameters to facilitate tuning and reuse , including specification of lexicon and training corpus .", "There is also support for a flexible tagset .", "For example , if we want to collapse distinctions in the lexicon , such as those between positive , comparative , and superlative adjectives , we only have to make a small change in the mapping from lexicon to tagset .", "Similarly , if we wish to make finer grain distinctions than those available in the lexicon , such as case marking on pronouns , there is a simple way to note such exceptions .", "We have used the tagger in a number of applications .", "We describe three applications here phrase recognition ; word sense disambiguation ; and grammatical function assignment .", "These projects are part of a research effort to use shallow analysis techniques to extract content from unrestricted text .", "We have constructed a system that recognizes simple phrases when given as input the sequence of tags for a sentence .", "There are recognizers for noun phrases , verb groups adverbial phrases , and prepositional phrases .", "Each of these phrases comprises a contiguous sequence of tags that sat is . fies a simple grammar .", "For example , a noun phrase can be a unary sequence containing a pronoun tag or an arbitrar . ily long sequence of noun and adjective tags , possibly pre . ceded by a determiner tag and possibly with an embeddec possessive marker .", "The longest possible sequence is founc e . g . , quot ; the program committee quot ; but not quot ; the program' Conjunctions are not recognized as part of any phrase ; for example , in the fragment quot ; the cats and dogs , quot ; quot ; the cats quot ; and quot ; dogs quot ; will be recognized as two noun phrases .", "Prepositional phrase attachment is not performed at this stage of processing .", "This approach to phrase recognition in some cases captures only parts of some phrases ; however , our approach minimizes false positives , so that we can rely on the recognizers' results .", "Part of speech tagging in and of itself is a useful tool in lexical disambiguation ; for example , knowing that quot ; dig quot ; is being used as a noun rather than as a verb indicates the word's appropriate meaning .", "But many words have multiple meanings even while occupying the same part of speech .", "To this end , the tagger has been used in the implementation of an experimental noun homograph disambiguation algorithm Hearst , 1991 .", "The algorithm known as CatchWord performs supervised training over a large text corpus , gathering lexical , orthographic , and simple syntactic evidence for each sense of the ambiguous noun .", "After a period of training , Catch Word classifies new instances of the noun by checking its context against that of previously observed instances and choosing the sense for which the most evidence is found .", "Because the sense distinctions made are coarse , the disambiguation can be accomplished without the expense of knowledge bases or inference mechanisms .", "Initial tests resulted in accuracies of around 90 for nouns with strongly distinct senses .", "This algorithm uses the tagger in two ways i to determine the part of speech of the target word filtering out the non noun usages and ii as a step in the phrase recognition analysis of the context surrounding the noun .", "The phrase recognizers also provide input to a system , Sopa Sibun , 1991 , which recognizes nominal arguments of verbs , specifically , Subject , Object , and Predicative Arguments .", "Sopa does not rely on information such as arity or voice specific to the particular verbs involved .", "The first step in assigning grammatical functions is to partition the tag sequence of each sentence into phrases .", "The phrase types include those mentioned in section 6 . 1 , additional types to account for conjunctions , complementizers , and indicators of sentence boundaries , and an quot ; unknown quot ; type .", "After a sentence has been partitioned , each simple noun phrase is examined in the context of the phrase to its left and the phrase to its right .", "On the basis of this local context and a set of rules , the noun phrase is marked as a syntactic Subject , Object , Predicative , or is not marked at all .", "A label of Predicative is assigned only if it can be determined that the governing verb group is a form of a predicating verb e . g . , a form of quot ; be quot ; .", "Because this cannot always be determined , some Predicatives are labeled Objects .", "If a noun phrase is labeled , it is also annotated as to whether the governing verb is the closest verb group to the right or to the left .", "The algorithm has an accuracy of approximately 80 in assigning grammatical functions .", "We would like to thank Marti Hearst for her contributions to this paper , Lauri Karttunen and Annie Zaenen for their work on lexicons , and Kris Halvorsen for supporting this project ."], "summary_lines": ["A Practical Part-Of-Speech Tagger\n", "We present an implementation of a part-of-speech tagger based on a hidden Markov model.\n", "The methodology enables robust and accurate tagging with few resource requirements.\n", "Only a lexicon and some unlabeled training text are required.\n", "Accuracy exceeds 96%.\n", "We describe implementation strategies and optimizations which result in high-speed operation.\n", "Three applications for tagging are described: phrase recognition; word sense disambiguation; and grammatical function assignment.\n", "Our semi-supervised model makes use of both labeled training text and some amount of unlabeled text.\n", "We train statistical models using unlabeled data with the expectation maximization algorithm.\n", "We report very high results (96% on the Brown corpus) for unsupervised POS tagging using Hidden Markov Models (HMMs) by exploiting hand-built tag dictionaries and equivalence classes.\n"]}
{"article_lines": ["Collective Information Extraction With Relational Markov Networks", "Most information extraction IE systems treat separate potential extractions as independent .", "However , in many cases , considering influences potential extractions could improve overall accuracy .", "Statistical methods on models , such as random fields have been shown to be an effective approach to learning accurate IE systems .", "We present a new IE method that employs Relational Markov Networks a generalization of CRFs , which can represent arbitrary dependencies between extractions .", "This allows for quot ; collective information extraction quot ; that exploits the mutual influence between possible extractions .", "Experiments on learning to extract protein names from biomedical text demonstrate the advantages of this approach .", "Information extraction IE , locating references to specific types of items in natural language documents , is an important task with many practical applications .", "Since IE systems are difficult and time consuming to construct , most recent research has focused on empirical techniques that automatically construct information extractors by training on supervised corpora Cardie , 1997 ; Califf , 1999 .", "One of the current best empirical approaches to IE is conditional random fields CRF's Lafferty et al . , 2001 .", "CRF's are a restricted class of undirected graphical models Jordan , 1999 designed for sequence segmentation tasks such as IE , part of speech POS tagging Lafferty et al . , 2001 , and shallow parsing Sha and Pereira , 2003 .", "In a recent follow up to previously published experiments comparing a large variety of IE learning methods including HMM , SVM , MaxEnt , and rule based methods on the task of tagging references to human proteins in Medline abstracts Bunescu et al . , 2004 , CRF's were found to significantly out perform competing techniques .", "As typically applied , CRF's , like almost all IE methods , assume separate extractions are independent and treat each potential extraction in isolation .", "However , in many cases , considering influences between extractions can be very useful .", "For example , in our protein tagging task , repeated references to the same protein are common .", "If the context surrounding one occurrence of a phrase is very indicative of it being a protein , then this should also influence the tagging of another occurrence of the same phrase in a different context which is not indicative of protein references .", "Relational Markov Networks RMN's Taskar et al . , 2002 are a generalization of CRF's that allow for collective classification of a set of related entities by integrating information from features of individual entities as well as the relations between them .", "Results on classifying connected sets of web pages have verified the advantage of this approach Taskar et al . , 2002 .", "In this paper , we present an approach to collective information extraction using RMN's that simultaneously extracts all of the information from a document by exploiting the textual content and context of each relevant substring as well as the document relationships between them .", "Experiments on human protein tagging demonstrate the advantages of collective extraction on several annotated corpora of Medline abstracts .", "Given a collection of documents D , we associate with each document dE Da set of candidate entities d . E , in our case a restricted set of token sequences from the document .", "Each entity e E d . E is characterized by a predefined set of boolean features e . F .", "This set of features is the same for all candidate entities , and it can be assimilated with the relational database definition of a table .", "One particular feature is e . label which is set to 1 if e is considered a valid extraction , and 0 otherwise .", "In this document model , labels are the only hidden features , and the inference procedure will try to find a most probable assignment of values to labels , given the current model parameters .", "Each document is associated with an undirected graphical model , with nodes corresponding directly to entity features , one node for each feature of each candidate entity in the document .", "The set of edges is created by matching clique templates against the entire set of entities d . E .", "A clique template is a procedure that finds all subsets of entities satisfying a given constraint , after which , for each entity subset , it connects a selected set of feature nodes so that they form a clique .", "Formally , there is a set of clique templates C , with each template c E C specified by Given a set , E , of nodes , Mc E C 2E consists of subsets of entities whose feature nodes S , are to be connected in a clique .", "In previous applications of RMNs , the selected subsets of entities for a given template have the same size ; however , our clique templates may match a variable number of entities .", "The set 5 , may contain the same feature from different entities .", "Usually , for each entity in the matching set , its label is included in Sc .", "All these will be illustrated with examples in Sections 4 and 5 where the clique templates used in our model are described in detail .", "Depending on the number of hidden labels in we define two categories of clique templates After the graph model for a document d has been completed with cliques from all templates , the probability distribution over the random field of hidden entity labels d . Y given the observed features d . X is computed as The above distribution presents the RMN as a Markov random field MRF with the clique templates as a method for tying potential values across different cliques in the graphical model .", "Like most entity names , almost all proteins in our data are base noun phrases or parts of them .", "Therefore , such substrings are used to determine candidate entities .", "To avoid missing options , we adopt a very broad definition of base noun phrase .", "Definition 1 A base noun phrase is a maximal contiguous sequence of tokens whose POS tags are from quot ; JJ quot ; , quot ; VBN quot ; , quot ; VBG quot ; , quot ; POS quot ; , quot ; NN quot ; , quot ; NNS quot ; , quot ; NNP quot ; , quot ; NNPS quot ; , quot ; CD quot ; , quot ; quot ; , and whose last word the head is tagged either as a noun , or a number .", "Candidate extractions consist of base NPs , augmented with all their contiguous subsequences headed by a noun or number .", "The set of features associated with each candidate is based on the feature templates introduced in Collins , 2002 , used there for training a ranking algorithm on the extractions returned by a maximum entropy tagger .", "Many of these features use the concept of word type , which allows a different form of token generalization than POS tags .", "The short type of a word is created by replacing any maximal contiguous sequences of capital letters with 'A' , of lowercase letters with 'a' , and of digits with '0' .", "For example , the word TGF 1 would be mapped to type A 0 .", "Consequently , each token position i in a candidate extraction provides three types of information the word itself wi , its POS tag t , and its short type si .", "The full set of features types is listed in Table 1 , where we consider a generic elabel \u03c6HD enzyme elabel \u03c6PF A0_a . . . \u03c6SF A0_a . . . \u03c6SF a Note that the factor graph above has an equivalent RMN graph consisting of a one node clique only , on which it is hard to visualize the various potentials involved .", "There are cases where different factor graphs may yield the same underlying RMN graph , which makes the factor graph representation preferable .", "Global clique templates enable us to model hypothesized influences between entities from the same document .", "They connect the label nodes of two or more entities , which , in the factor graph , translates into potential nodes connected to at least two label nodes .", "In our experiments we use three global templates Overlap Template OT No two entity names overlap in the text i . e if the span of one entity is Si , el and the span of another entity is 82 , e2 , and Si 82 , then el 82 .", "Repeat Template RT If multiple entities in the same document are repetitions of the same name , their labels tend to have the same value i . e . most of them are protein names , or most of them are not protein names .", "Later we discuss situations in which repetitions of the same protein name are not tagged as proteins , and design an approach to handle this .", "Acronym Template AT It is common convention that a protein is first introduced by its long name , immediately followed by its short form acronym in parentheses .", "The definition of a candidate extraction from Section 3 leads to many overlapping entities .", "For example , 'glutathione S transferase' is a base NP , and it generates five candidate extractions 'glutathione' , 'glutathione S' , 'glutathione S transferase' , 'S transferase' , and 'transferase' .", "If 'gintathione S transferase' has label value 1 , because the other four entities overlap with it , they should all have label value 0 .", "This type of constraint is enforced by the overlap template whose M operator matches any two overlapping candidate entities , and which connects their label nodes specified in S through a potential node with a potential function cb that allows at most one of them to have label value 1 , as illustrated in Table 2 .", "Continuing with the previous example , because 'gintathione S' and 'S transferase' are two overlapping entities , the factor graph model will contain an overlap potential node connected to the label nodes of these two entities .", "An alternative solution for the overlap template is to create a potential node for each token position that is covered by at least two candidate entities in the document , and connect it to their label nodes .", "The difference in this case is that the potential node will be connected to a variable number of entity label nodes .", "However this second approach has the advantage of creating fewer potential nodes in the document factor graph , which results in faster inference .", "We could specify the potential for the repeat template in a similar 2 by 2 table , this time leaving the table entries to be learned , given that it is not a hard constraint .", "However we can do better by noting that the vast majority of cases where a repeated protein name is not also tagged as a protein happens when it is part of a larger phrase that is tagged .", "For example , 'HDAC1 enzyme' is a protein name , therefore 'HDAC1' is not tagged in this phrase , even though it may have been tagged previously in the abstract where it was not followed by 'enzyme' .", "We need a potential that allows two entities with the same text to have different labels if the entity with label value 0 is inside another entity with label value 1 .", "But a candidate entity may be inside more than one quot ; including quot ; entity , and the number of including entities may vary from one candidate extraction to another .", "Using the example from Section 5 . 1 , the candidate entity 'glutathione' is included in two other entities 'glutathione S' and 'glutathione S transferase' .", "In order to instantiate potentials over variable number of label nodes , we introduce a logical OR clique template that matches a variable number of entities .", "When this template matches a subset of entities el , e2 , . . . , en , it will create an auxiliary OR entity e , , with a single feature e . 1 abel .", "The potential function is set so that it assigns a non zero potential only when e , . 1 abel el . 1 abel V e2 . 1abel V . . . V en . 1 abel .", "The cliques are only created as needed , e . g . when the auxiliary OR variable is required by repeat and acronym clique templates .", "Figure 3 shows the factor graph for a samverges , it gives a good approximation to the correct marginals .", "The algorithm works by altering the belief at each label node by repeatedly passing messages between the node and all potential nodes connected to it Kschischang et al . , 2001 .", "As many of the label nodes are indirectly connected through potential nodes instantiated by global templates , their belief values will propagate in the graph and mutually influence each other , leading in the end to a collective labeling decision .", "The time complexity of computing messages from a potential node to a label node is exponential in the number of label nodes attached to the potential .", "Since this quot ; fan in quot ; can be large for OR potential nodes , this step required optimization .", "Fortunately , due to the special form of the OR potential , and the normalization before each message passing step , we were able to develop a linear time algorithm for this special case .", "Details are omitted due to limited space .", "Following a maximum likelihood estimation , we shall use the log linear representation of potentials where A is a vector of binary features , one for each configuration of values for X , and K . Let w be the concatenated vector of all potential parameters wc .", "One approach to finding the maximum likelihood solution for w is to use a gradient based method , which requires computing the gradient of the log likelihood with respect to potential parameters wc .", "It can be shown that this gradient is equal with the difference between the empirical counts of fc and their expectation under the current set of parameters w . This expectation is expensive to compute , since it requires summing over all possible configurations of candidate entity labels from a given document .", "To circumvent this complexity , we use Collins' voted perceptron approach Collins , 2002 , which approximates the full expectation of fc with the fc counts for the most likely labeling under the current parameters , w . In all our experiments , the perceptron was run for 50 epochs , with a learning rate set at 0 . 01 .", "We have tested the RMN approach on two datasets that have been hand tagged for human protein names .", "The first dataset is Yapexl which consists of 200 Medline abstracts .", "Of these , 147 have been randomly selected by posing a query containing the Mesh terms protein binding , interaction , and molecular to Medline , while the rest of 53 have been extracted randomly from the GENIA corpus Collier et al . , 1999 .", "It contains a total of 3713 protein references .", "The second dataset is Aimed2 which has been previously used for training the protein interaction extraction systems in Bunescu et al . , 2004 .", "It consists of 225 Medline abstracts , of which 200 are known to describe interactions between human proteins , while the other 25 do not refer to any interaction .", "There are 4084 protein references in this dataset .", "We compared the performance of three systems LT RMN is the RMN approach using local templates and the overlap template , GLT RMN is the full RMN approach , using both local and global templates , and CRF , which uses a CRF for labeling token sequences .", "We used the CRF implementation from McCallum , 2002 with the set of tags and features used by the MaximumEntropy tagger described in Bunescu et al . , 2004 .", "All Medline abstracts were tokenized and then POS tagged using Brill's tagger Brill , 1995 .", "Each extracted protein name in the test data was compared to the human tagged data , with the positions taken into account .", "Two extractions are considered a match if they consist of the same character sequence in the same position in the text .", "Results are shown in Tables 3 and 4 which give average precision , recall , and F measure using 10 fold cross validation .", "These tables show that , in terms of Fmeasure , the use of global templates for modto improve a Maximum Entropy tagger ; however , these features do not fully capture the mutual influence between the labels of acronyms and their long forms , or between entity repetitions .", "In particular , they only allow earlier extractions in a document to influence later ones and not vice versa .", "The RMN approach handles these and potentially other mutual influences between entities in a more complete , probabilistically sound manner .", "We have presented an approach to collective information extraction that uses Relational Markov Networks to reason about the mutual influences between multiple extractions .", "A new type of clique template the logical OR template was introduced , allowing a variable number of relevant entities to be used by other clique templates .", "Soft correlations between repetitions and acronyms and their long form in the same document have been captured by global clique templates , allowing for local extraction decisions to propagate and mutually influence each other .", "Regarding future work , a richer set of features for the local templates would likely improve performance .", "Currently , LT RMN's accuracy is still significantly less than CRF's , which limits the performance of the full system .", "Another limitation is the approximate inference used by both RMN methods .", "The number of factor graphs for which the sum product algorithm did not converge was non negligible , and our approach stopped after a fix number of iterations .", "Besides exploring improvements to loopy belief propagation that increase computational cost Yedidia et al . , 2000 , we intend to examine alternative approximate inference methods .", "This work was partially supported by grants IIS 0117308 and IIS 0325116 from the NSF ."], "summary_lines": ["Collective Information Extraction With Relational Markov Networks\n", "Most information extraction (IE) systems treat separate potential extractions as independent.\n", "However, in many cases, considering influences between different potential extractions could improve overall accuracy.\n", "Statistical methods based on undirected graphical models, such as conditional random fields (CRFs), have been shown to be an effective approach to learning accurate IE systems.\n", "We present a new IE method that employs Relational Markov Networks (a generalization of CRFs), which can represent arbitrary dependencies between extractions.\n", "This allows for \"collective information extraction\" that exploits the mutual influence between possible extractions.\n", "Experiments on learning to extract protein names from biomedical text demonstrate the advantages of this approach.\n", "We present AImed, a corpus for the evaluation of PPI extraction systems.\n"]}
{"article_lines": ["Building a Large Annotated Corpus of English The Penn Treebank Mitchell P . Marcus University of Pennsylvania Mary Ann Marcinkiewicz University of Pennsylvania Beatrice Santorini t Northwestern University 1 .", "Introduction There is a growing consensus that significant , rapid progress can be made in both text understanding and spoken language understanding by investigating those phenom ena that occur most centrally in naturally occurring unconstrained materials and by attempting to automatically extract information about language from very large cor pora .", "Such corpora re beginning to serve as important research tools for investigators in natural anguage processing , speech recognition , and integrated spoken language systems , as well as in theoretical linguistics .", "Annotated corpora promise to be valu able for enterprises as diverse as the automatic onstruction of statistical models for the grammar of the written and the colloquial spoken language , the development of explicit formal theories of the differing grammars of writing and speech , the investi gation of prosodic phenomena in speech , and the evaluation and comparison of the adequacy of parsing models .", "In this paper , we review our experience with constructing one such large annotated corpus the Penn Treebank , a corpus 1consisting of over 4 . 5 million words of American English .", "During the first three year phase of the Penn Treebank Project 1989 1992 , this corpus has been annotated for part of speech POS information .", "In addition , over half of it has been annotated for skeletal syntactic structure .", "These materials are available to members of the Linguistic Data Consortium ; for details , see Section 5 . 1 .", "The paper is organized as follows .", "Section 2 discusses the POS tagging task .", "After outlining the considerations that informed the design of our POS tagset and pre senting the tagset itself , we describe our two stage tagging process , in which text is first assigned POS tags automatically and then corrected by human annotators .", "Section 3 briefly presents the results of a comparison between entirely manual and semi automated tagging , with the latter being shown to be superior on three counts speed , consistency , and accuracy .", "In Section 4 , we turn to the bracketing task .", "Just as with the tagging task , we have partially automated the bracketing task the output of ?", "Department of Computer and Information Sciences , University of Pennsylvania , Philadelphia , PA 19104 . f Department of Linguistics , Northwestern University , Evanston , IL 60208 . Department of Computer and Information Sciences , University of Pennsylvania , Philadelphia , PA 19104 .", "1 A distinction is sometimes made between a corpus as a carefully structured set of materials gathered together to jointly meet some design principles , and a collection , which may be much more opportunistic n construction .", "We acknowledge that from this point of view , the raw materials of the Penn Treebank form a collection .", "1993 Association for Computational Linguistics Computational Linguistics Volume 19 , Number 2 the POS tagging phase is automatically parsed and simplified to yield a skeletal syn tactic representation , which is then corrected by human annotators .", "After presenting the set of syntactic tags that we use , we illustrate and discuss the bracketing process .", "In particular , we will outline various factors that affect the speed with which annotators are able to correct bracketed structures , a task that not surprisingly is considerably more difficult than correcting POS tagged text .", "Finally , Section 5 describes the com position and size of the current Treebank corpus , briefly reviews some of the research projects that have relied on it to date , and indicates the directions that the project is likely to take in the future .", "Part of Speech Tagging 2 . 1 A Simplified POS Tagset for English The POS tagsets used to annotate large corpora in the past have traditionally been fairly extensive .", "The pioneering Brown Corpus distinguishes 87 simple tags Francis 1964 ; Francis and Ku era 1982 and allows the formation of compound tags ; thus , the contraction I m is tagged as PPSS BEM PPSS for non third person nominative per sonal pronoun and BEM for am , m .", "2 Subsequent projects have tended to elaborate the Brown Corpus tagset .", "For instance , the Lancaster Oslo Bergen LOB Corpus uses about 135 tags , the Lancaster UCREL group about 165 tags , and the London Lund Cor pus of Spoken English 197 tags .", "3 The rationale behind developing such large , richly articulated tagsets is to approach the ideal of providing distinct codings for all classes of words having distinct grammatical behaviour Garside , Leech , and Sampson 1987 , p . 167 .", "2 . 1 . 1 Recoverability .", "Like the tagsets just mentioned , the Penn Treebank tagset is based on that of the Brown Corpus .", "However , the stochastic orientation of the Penn Tree bank and the resulting concern with sparse data led us to modify the Brown Corpus tagset by paring it down considerably .", "A key strategy in reducing the tagset was to eliminate redundancy by taking into account both lexical and syntactic information .", "Thus , whereas many POS tags in the Brown Corpus tagset are unique to a particular lexical item , the Penn Treebank tagset strives to eliminate such instances of lexical re dundancy .", "For instance , the Brown Corpus distinguishes five different forms for main verbs the base form is tagged VB , and forms with overt endings are indicated by appending D for past tense , G for present participle gerund , N for past participle , and Z for third person singular present .", "Exactly the same paradigm is recognized for have , but have regardless of whether it is used as an auxiliary or a main verb is as signed its own base tag HV .", "The Brown Corpus further distinguishes three forms of do the base form DO , the past tense DOD , and the third person singular present DOZ , 4 and eight forms of be the five forms distinguished for regular verbs as well as the irregular forms am BEM , are BER , and was BEDZ .", "By contrast , since the distinctions between the forms of VB on the one hand and the forms of BE , DO , and HV on the other are lexically recoverable , they are eliminated in the Penn Treebank , as shown in Table 1 .", "5 2 Counting both simple and compound tags , the Brown Corpus tagset contains 187 tags .", "3 A useful overview of the relation of these and other tagsets to each other and to the Brown Corpus tagset is given in Appendix B of Garside , Leech , and Sampson 1987 .", "4 The gerund and the participle of do are tagged VBG and VBN in the Brown Corpus , respectively presumably because they are never used as auxiliary verbs in American English .", "5 The irregular present ense forms am and are are tagged as VBP in the Penn Treebank see Section 2 . 1 . 3 , just like any other non third person singular present ense form .", "314 Mitchell P . Marcus et al .", "Building a Large Annotated Corpus of English Table 1 Elimination of lexically recoverable distinctions .", "sing VB be VB do VB have VB sings VBZ is VBZ does VBZ has VBZ sang VBD was VBD did VBD had VBD singing VBG being VBG doing VBG having VBG sung VBN been VBN done VBN had VBN A second example of lexical recoverability concerns those words that can precede articles in noun phrases .", "The Brown Corpus assigns a separate tag to pre qualifiers quite , rather , such , pre quantifiers all , half , many , nary and both .", "The Penn Treebank , on the other hand , assigns all of these words to a single category PDT predeterminer .", "Further examples of lexically recoverable categories are the Brown Corpus categories PPL singular eflexive pronoun and PPLS plural reflexive pronoun , which we col lapse with PRP personal pronoun , and the Brown Corpus category RN nominal adverb , which we collapse with RB adverb .", "Beyond reducing lexically recoverable distinctions , we also eliminated certain POS distinctions that are recoverable with reference to syntactic structure .", "For instance , the Penn Treebank tagset does not distinguish subject pronouns from object pronouns even in cases where the distinction is not recoverable from the pronouns form , as with you , since the distinction is recoverable on the basis of the pronouns position in the parse tree in the parsed version of the corpus .", "Similarly , the Penn Treebank tagset conflates ubordinating conjunctions with prepositions , tagging both categories as IN .", "The distinction between the two categories i not lost , however , since subor dinating conjunctions can be recovered as those instances of IN that precede clauses , whereas prepositions are those instances of IN that precede noun phrases or preposi tional phrases .", "We would like to emphasize that the lexical and syntactic recoverability inherent in the POS tagged version of the Penn Treebank corpus allows end users to employ a much richer tagset han the small one described in Section 2 . 2 if the need arises .", "2 . 1 . 2 Consistency .", "As noted above , one reason for eliminating a POS tag such as RN nominal adverb is its lexical recoverability .", "Another important reason for doing so is consistency .", "For instance , in the Brown Corpus , the deictic adverbs there and now are always tagged RB adverb , whereas their counterparts here and then are inconsistently tagged as RB adverb or RN nominal adverb even in identical syntactic ontexts , such as after a preposition .", "It is clear that reducing the size of the tagset reduces the chances of such tagging inconsistencies .", "2 . 1 . 3 Syntactic Function .", "A further difference between the Penn Treebank and the Brown Corpus concerns the significance accorded to syntactic ontext .", "In the Brown Corpus , words tend to be tagged independently of their syntactic function .", "6 For in stance , in the phrase the one , one is always tagged as CD cardinal number , whereas 6 An important exception is there , which the Brown Corpus tags as EX existential there when it is used as a formal subject and as RB adverb when it is used as a locative adverb .", "In the case of there , we did not pursue our strategy oftagset reduction toits logical conclusion , which would have implied tagging existential there as NN common noun .", "315 Computational Linguistics Volume 19 , Number 2 in the corresponding plural phrase the ones , ones is always tagged as NNS plural com mon noun , despite the parallel function of one and ones as heads of the noun phrase .", "By contrast , since one of the main roles of the tagged version of the Penn Treebank corpus is to serve as the basis for a bracketed version of the corpus , we encode a words syntactic function in its POS tag whenever possible .", "Thus , one is tagged as NN singular common noun rather than as CD cardinal number when it is the head of a noun phrase .", "Similarly , while the Brown Corpus tags both as ABX pre quantifier , double conjunction , regardless of whether it functions as a prenominal modifier both the boys , a postnominal modifier the boys both , the head of a noun phrase both of the boys or part of a complex coordinating conjunction both boys and girls , the Penn Treebank tags both differently in each of these syntactic ontexts as PDT predeter miner , RB adverb , NNS plural common noun and coordinating conjunction CC , respectively .", "There is one case in which our concern with tagging by syntactic function has led us to bifurcate Brown Corpus categories rather than to collapse them namely , in the case of the uninflected form of verbs .", "Whereas the Brown Corpus tags the bare form of a verb as VB regardless of whether it occurs in a tensed clause , the Penn Treebank tagset distinguishes VB infinitive or imperative from VBP non third person singular present ense .", "2 . 1 . 4 Indeterminacy .", "A final difference between the Penn Treebank tagset and all other tagsets we are aware of concerns the issue of indeterminacy both POS ambiguity in the text and annotator uncertainty .", "In many cases , POS ambiguity can be resolved with reference to the linguistic context .", "So , for instance , in Katharine Hepburns witty line Grant can be outspoken but not by anyone I know , the presence of the by phrase forces us to consider outspoken as the past participle of a transitive derivative of speak outspeak rather than as the adjective outspoken .", "However , even given explicit criteria for assigning POS tags to potentially ambiguous words , it is not always possible to assign a unique tag to a word with confidence .", "Since a major concern of the Treebank is to avoid requiring annotators to make arbitrary decisions , we allow words to be associated with more than one POS tag .", "Such multiple tagging indicates either that the words part of speech simply cannot be decided or that the annotator is unsure which of the alternative tags is the correct one .", "In principle , annotators can tag a word with any number of tags , but in practice , multiple tags are restricted to a small number of recurring two tag combinations JJINN adjective or noun as prenominal modifier , JJIVBG adjective or gerund present participle , JJ VBN adjective or past participle , NNIVBG noun or gerund , and RBIRP adverb or particle .", "2 . 2 The POS Tagset The Penn Treebank tagset is given in Table 2 .", "It contains 36 POS tags and 12 other tags for punctuation and currency symbols .", "A detailed description of the guidelines governing the use of the tagset is available in Santorini 1990 .", "7 2 . 3 The POS Tagging Process The tagged version of the Penn Treebank corpus is produced in two stages , using a combination of automatic POS assignment and manual correction .", "7 In versions of the tagged corpus distributed before November 1992 , singular proper nouns , plural proper nouns , and personal pronouns were tagged as NP , NPS , and PP , respectively .", "The current tags NNP , NNPS , and PRP were introduced in order to avoid confusion with the syntactic tags NP noun phrase and PP prepositional phrase see Table 3 .", "316 Mitchell P . Marcus et al .", "Building a Large Annotated Corpus of English Table 2 The Penn Treebank POS tagset .", "CC Coordinating conjunction 25 .", "CD Cardinal number 26 .", "UH Interjection 3 .", "DT Determiner 27 .", "VB Verb , base form 4 .", "EX Existential there 28 .", "VBD Verb , past tense 5 .", "FW Foreign word 29 .", "VBG Verb , gerund present 6 .", "IN Preposition subordinating participle conjunction 30 .", "VBN Verb , past participle 7 .", "JJ Adjective 31 .", "VBP Verb , non 3rd ps .", "JJR Adjective , comparative 32 .", "VBZ Verb , 3rd ps .", "JJS Adjective , superlative 33 .", "WDT wh determiner 10 .", "LS List item marker 34 .", "WP wh pronoun 11 .", "WP Possessive wh pronoun 12 .", "NN Noun , singular or mass 36 .", "WRB wh adverb 13 .", "NNS Noun , plural 37 .", "Pound sign 14 .", "NNP Proper noun , singular 38 .", "Dollar sign 15 .", "NNPS Proper noun , plural 39 . . Sentence final punctuation 16 .", "PDT Predeterminer 40 . , Comma 17 .", "POS Possessive nding 41 . Colon , semi colon 18 .", "PRP Personal pronoun 42 .", "Left bracket character 19 .", "PP Possessive pronoun 43 .", "Right bracket character 20 .", "RB Adverb 44 .", "Straight double quote 21 .", "RBR Adverb , comparative 45 .", "Left open single quote 22 .", "RBS Adverb , superlative 46 .", "Left open double quote 23 .", "RP Particle 47 .", "Right close single quote 24 .", "SYM Symbol mathematical or scientific 48 .", "Right close double quote 2 . 3 . 1 Automated Stage .", "During the early stages of the Penn Treebank project , the initial automatic POS assignment was provided by PARTS Church 1988 , a stochastic algorithm developed at AT T Bell Labs .", "PARTS uses a modif ied version of the Brown Corpus tagset close to our own and assigns POS tags with an error rate of 3 5 .", "The output of PARTS was automatically tokenized 8 and the tags assigned by PARTS were automatically mapped onto the Penn Treebank tagset .", "This mapp ing introduces about 4 error , since the Penn Treebank tagset makes certain distinctions that the PARTS tagset does not .", "9 A sample of the resulting tagged text , which has an error rate of 7 9 , is shown in Figure 1 .", "More recently , the automatic POS assignment is provided by a cascade of stochastic and rule driven taggers developed on the basis of our early experience .", "Since these taggers are based on the Penn Treebank tagset , the 4 error rate introduced as an artefact of mapping from the PARTS tagset o ours is eliminated , and we obtain error rates of 2 6 .", "2 . 3 . 2 Manual Correction Stage .", "The result of the first , automated stage of POS tagging is given to annotators to correct .", "The annotators use a mouse based package written 8 In contrast to the Brown Corpus , we do not allow compound tags of the sort illustrated above for Im .", "Rather , contractions and the Anglo Saxon genitive of nouns are automatically split into their component morphemes , and each morpheme is tagged separately .", "Thus , childrens is tagged children NNS s POS , and wont is tagged wo MD nt RB .", "9 The two largest sources of mapping error are that the PARTS tagset distinguishes neither infinitives from non third person singular present tense forms of verbs , nor prepositions from particles in cases like run up a hill and run up a bill .", "317 Computational Linguistics Volume 19 , Number 2 Battle tested NNP industrial JJ managers NNS here RB always RB buck VB up IN nervous JJ newcomers NNS with IN the DT tale NN of IN the DT first JJ of IN their PP countrymen NNS to TO visit VB Mexico NNP , , a DT boatload NN of IN samurai NNS warriors NNS blown VBN ashore RB 375 CD years NNS ago RB . .", "From IN the DT beginning NN , , it PRP took VBD a DT man NN with IN extraordinary JJ qualities NNS to TO succeed VB in IN Mexico NNP , , says VBZ Kimihide NNP Takimura NNP , , president NN of IN Mitsui NNS group NN s POS Kensetsu NNP Engineering NNP Inc . NNP unit NN . .", "Figure 1 Sample tagged text before correction .", "Battle tested NNP JJ industrial JJ managers NNS here RB always RB buck VB VBP up IN RP nervous JJ newcomers NNS with IN the DT tale NN of IN the DT first JJ of IN their PP countrymen NNS to TO visit VB Mexico NNP , , a DT boatload NN of IN samurai NNS FW warriors NNS blown VBN ashore RB 375 CD years NNS ago RB . .", "From IN the DT beginning NN , , it PRP took VBD a DT man NN with IN extraordinary JJ qualities NNS to TO succeed VB in IN Mexico NNP , , says VBZ Kimihide NNP Takimura NNP , , president NN of IN Mitsui NNS NNP group NN s POS Kensetsu NNP Engineering NNP Inc . NNP unit NN . .", "Figure 2 Sample tagged text after correction .", "in GNU Emacs Lisp , which is embedded within the GNU Emacs editor Lewis et al .", "The package allows annotators to correct POS assignment errors by positioning the cursor on an incorrectly tagged word and then entering the desired correct ag or sequence of multiple tags .", "The annotators input is automatically checked against the list of legal tags in Table 2 and , if valid , appended to the original word tag pair separated by an asterisk .", "Appending the new tag rather than replacing the old tag allows us to easily identify recurring errors at the automatic POS assignment s age .", "We believe that the confusion matrices that can be extracted from this information should also prove useful in designing better automatic taggers in the future .", "The result of this second stage of POS tagging is shown in Figure 2 .", "Finally , in the distribution version of the tagged corpus , any incorrect tags assigned at the first , automatic stage are removed .", "The learning curve for the POS tagging task takes under a month at 15 hours a week , and annotation speeds after a month exceed 3 , 000 words per hour .", "Two Modes of AnnotationwAn Experiment To determine how to maximize the speed , inter annotator consistency , and accuracy of POS tagging , we performed an experiment a the very beginning of the project o com pare two alternative modes of annotation .", "In the first annotation mode tagging , annotators tagged unannotated text entirely by hand ; in the second mode correct ing , they verified and corrected the output of PARTS , modified as described above .", "318 Mitchell P . Marcus et al .", "Building a Large Annotated Corpus of English This experiment showed that manual tagging took about twice as long as correcting , with about twice the inter annotator disagreement rate and an error rate that was about 50 higher .", "Four annotators , all with graduate training in linguistics , participated in the exper iment .", "All completed a training sequence consisting of 15 hours of correcting followed by 6 hours of tagging .", "The training material was selected from a variety of nonfiction genres in the Brown Corpus .", "All the annotators were familiar with GNU Emacs at the outset of the experiment .", "Eight 2 , 000 word samples were selected from the Brown Cor pus , two each from four different genres two fiction , two nonfiction , none of which any of the annotators had encountered in training .", "The texts for the correction task were automatically tagged as described in Section 2 . 3 .", "Each annotator first manually tagged four texts and then corrected four automatically tagged texts .", "Each annotator completed the four genres in a different permutation .", "A repeated measures analysis of annotation speed with annotator identity , genre , and annotation mode tagging vs . correcting as classification variables howed a sig nificant annotation mode effect p . 05 .", "No other effects or interactions were signif icant .", "The average speed for correcting was more than twice as fast as the average speed for tagging 20 minutes vs . 44 minutes per 1 , 000 words .", "Median speeds per 1 , 000 words were 22 vs . 42 minutes .", "A simple measure of tagging consistency is inter annotator disagreement rate , the rate at which annotators disagree with one another over the tagging of lexical tokens , expressed as a percentage of the raw number of such disagreements over the number of words in a given text sample .", "For a given text and n annotators , there are disagreement ratios one for each possible pair of annotators .", "Mean inter annotator disagreement was 7 . 2 for the tagging task and 4 . 1 for the correcting task with me dians 7 . 2 and 3 . 6 , respectively .", "Upon examination , a disproportionate amount of disagreement i the correcting case was found to be caused by one text that contained many instances of a cover symbol for chemical and other formulas .", "In the absence of an explicit guideline for tagging this case , the annotators had made different decisions on what part of speech this cover symbol represented .", "When this text is excluded from consideration , mean inter annotator disagreement for the correcting task drops to 3 . 5 , with the median unchanged at 3 . 6 .", "Consistency , while desirable , tells us nothing about he validity of the annotators corrections .", "We therefore compared each annotators output not only with the output of each of the others , but also with a benchmark version of the eight texts .", "This benchmark version was derived from the tagged Brown Corpus by 1 mapping the original Brown Corpus tags onto the Penn Treebank tagset and 2 carefully hand correcting the revised version in accordance with the tagging conventions in force at the time of the experiment .", "Accuracy was then computed as the rate of disagreement between each annotators results and the benchmark version .", "The mean accuracy was 5 . 4 for the tagging task median 5 . 7 and 4 . 0 for the correcting task median 3 . 4 .", "Excluding the same text as above gives a revised mean accuracy for the correcting task of 3 . 4 , with the median unchanged .", "We obtained a further measure of the annotators accuracy by comparing their error rates to the rates at which the raw output of Churchs PARTS program appropri ately modified to conform to the Penn Treebank tagset disagreed with the benchmark version .", "The mean disagreement rate between PARTS and the benchmark version was 319 Computational Linguistics Volume 19 , Number 2 9 . 6 , while the corrected version had a mean disagreement rate of 5 . 4 , as noted above .", "The annotators were thus reducing the error rate by about 4 . 2 .", "Bracketing 4 . 1 Basic Methodology The methodology for bracketing the corpus is completely parallel to that for tagging hand correction of the output of an errorful automatic process .", "Fidditch , a deterministic parser developed by Donald Hindle first at the University of Pennsylvania nd sub sequently at AT T Bell Labs Hindle 1983 , 1989 , is used to provide an initial parse of the material .", "Annotators then hand correct he parsers output using a mouse based interface implemented in GNU Emacs Lisp .", "Fidditch has three properties that make it ideally suited to serve as a preprocessor to hand correction ?", "Fidditch always provides exactly one analysis for any given sentence , so that annotators need not search through multiple analyses .", "Fidditch never attaches any constituent whose role in the larger structure it cannot determine with certainty .", "In cases of uncertainty , Fidditch chunks the input into a string of trees , providing only a partial structure for each sentence .", "Fidditch has rather good grammatical coverage , so that the grammatical chunks that it does build are usually quite accurate .", "Because of these properties , annotators do not need to rebracket much of the parsers output a relatively time consuming task .", "Rather , the annotators main task is to glue together the syntactic hunks produced by the parser .", "Using a mouse based interface , annotators move each unattached chunk of structure under the node to which it should be attached .", "Notational devices allow annotators to indicate uncertainty concerning constituent labels , and to indicate multiple attachment sites for ambiguous modifiers .", "The bracketing process is described in more detail in Section 4 . 3 .", "4 . 2 The Syntactic Tagset Table 3 shows the set of syntactic tags and null elements that we use in our skeletal bracketing .", "More detailed information on the syntactic tagset and guidelines concern ing its use are to be found in Santorini and Marcinkiewicz 1991 .", "Although different in detail , our tagset is similar in delicacy to that used by the Lancaster Treebank Project , except hat we allow null elements in the syntactic anno tation .", "Because of the need to achieve a fairly high output per hour , it was decided not to require annotators to create distinctions beyond those provided by the parser .", "Our approach to developing the syntactic tagset was highly pragmatic and strongly influenced by the need to create a large body of annotated material given limited hu man resources .", "Despite the skeletal nature of the bracketing , however , it is possible to make quite delicate distinctions when using the corpus by searching for combinations of structures .", "For example , an SBAR containing the word to immediately before the VP will necessarily be infinitival , while an SBAR containing a verb or auxiliary with a 10 We would like to emphasize that the percentage given for the modified output of PARTS does not represent an error rate for PARTS .", "It reflects not only true mistakes in PARTS performance , but also the many and important differences in the usage of Penn Treebank POS tags and the usage of tags in the original Brown Corpus material on which PARTS was trained .", "320 Mitchell P . Marcus et al .", "Building a Large Annotated Corpus of English Table 3 The Penn Treebank syntactic tagset .", "X Null elements 2 .", "NIL Adjective phrase Adverb phrase Noun phrase Prepositional phrase Simple declarative clause Clause introduced by subordinating conjunction or 0 see below Direct question introduced by wh word or wh phrase Declarative sentence with subject aux inversion Subconstituent of SBARQ excluding wh word or wh phrase Verb phrase wh adverb phrase wh noun phrase wh prepositional phrase Constituent of unknown or uncertain category Understood subject of infinitive or imperative Zero variant of that in subordinate clauses Trace marks position where moved wh constituent is interpreted Marks position where preposition is interpreted in pied piping contexts tense feature will necessarily be tensed .", "To take another example , so called that clauses can be identified easily by searching for SBARs containing the word that or the null element 0 in initial position .", "As can be seen from Table 3 , the syntactic tagset used by the Penn Treebank in cludes a variety of null elements , a subset of the null elements introduced by Fidditch .", "While it would be expensive to insert null elements entirely by hand , it has not proved overly onerous to maintain and correct hose that are automatically provided .", "We have chosen to retain these null elements because we believe that they can be exploited in many cases to establish a sentences predicate argument structure ; at least one recipient of the parsed corpus has used it to bootstrap the development of lexicons for partic ular NLP projects and has found the presence of null elements to be a considerable aid in determining verb transitivity Robert Ingria , personal communication .", "While these null elements correspond more directly to entities in some grammatical theories than in others , it is not our intention to lean toward one or another theoretical view in producing our corpus .", "Rather , since the representational framework for grammatical structure in the Treebank is a relatively impoverished flat context free notation , the eas iest mechanism to include information about predicate argument structure , although indirectly , is by allowing the parse tree to contain explicit null items .", "4 . 3 Sample Bracketing Output Below , we illustrate the bracketing process for the first sentence of our sample text .", "Figure 3 shows the output of Fidditch modified slightly to include our POS tags .", "As Figure 3 shows , Fidditch leaves very many constituents unattached , labeling them as ?", ", and its output is perhaps better thought of as a string of tree fragments than as a single tree structure .", "Fidditch only builds structure when this is possible for a purely syntactic parser without access to semantic or pragmatic information , and it 321 Computational Linguistics Volume 19 , Number 2 s NP ?", "Figure 3 NBAK ADJP ADJ Battle tested JJ ADJ industrial JJ NPL managers NNS ?", "ADV here KB ?", "ADV always KB AUX TNS VP VPKES buck VBP ?", "PP PKEP up KP NP NBAR ADJ nervous JJ NPL newcomers NNS ?", "PP PREP with IN NP DART the DT NBAK N tale NN PP of PKEP NP DART the DT NBAK ADJP ADJ first JJ ?", "PP of PREP NP PROS their PP NBAK NPL countrymen NNS ?", "S NP PRO AUX to TNS VP V visit VB NP PNP Mexico NNP ?", "MID , , ?", "NP IAKT a DT NBAK N boatload NN PP of PKEP NP NBAK NPL warriors NNS VP VPPKT blown VBN ?", "ADV ashore KB NP NBAR CARD 375 CD NPL years NNS ADV ago KB FIN . .", "Sample bracketed text full structure provided by Fidditch .", "always errs on the side of caution .", "Since determining the correct attachment point of prepositional phrases , relative clauses , and adverbial modifiers almost always requires extrasyntactic information , Fidditch pursues the very conservative strategy of always leaving such constituents unattached , even if only one attachment point is syntacti cally possible .", "However , Fidditch does indicate its best guess concerning a fragments attachment site by the fragments depth of embedding .", "Moreover , it attaches preposi tional phrases beginning with of if the preposition immediately follows a noun ; thus , tale of . . . and boatload of . . . are parsed as single constituents , while first of . . . is not .", "Since Fidditch lacks a large verb lexicon , it cannot decide whether some constituents serve as adjuncts or arguments and hence leaves subordinate clauses such as infini 322 Mitchell P . Marcus et al .", "Building a Large Annotated Corpus of English tives as separate fragments .", "Note further that Fidditch creates adjective phrases only when it determines that more than one lexical item belongs in the ADJP .", "Finally , as is well known , the scope of conjunctions and other coordinate structures can only be determined given the richest forms of contextual information ; here again , Fidditch simply turns out a string of tree fragments around any conjunction .", "Because all de cisions within Fidditch are made locally , all commas which often signal conjunction must disrupt he input into separate chunks .", "The original design of the Treebank called for a level of syntactic analysis compa rable to the skeletal analysis used by the Lancaster Treebank , but a limited experiment was performed early in the project o investigate the feasibility of providing greater levels of structural detail .", "While the results were somewhat unclear , there was ev idence that annotators could maintain a much faster rate of hand correction if the parser output was simplified in various ways , reducing the visual complexity of the tree representations and eliminating a range of minor decisions .", "The key results of this experiment were as follows ?", "Annotators take substantially longer to learn the bracketing task than the POS tagging task , with substantial increases in speed occurring even after two months of training .", "Annotators can correct he full structure provided by Fidditch at an average speed of approximately 375 words per hour after three weeks and 475 words per hour after six weeks .", "Reducing the output from the full structure shown in Figure 3 to a more skeletal representation similar to that used by the Lancaster UCREL Treebank Project increases annotator productivity by approximately 100 200 words per hour .", "It proved to be very difficult for annotators to distinguish between a verbs arguments and adjuncts in all cases .", "Allowing annotators to ignore this distinction when it is unclear attaching constituents high increases productivity by approximately 150 200 words per hour .", "Informal examination of later annotation showed that forced distinctions cannot be made consistently .", "As a result of this experiment , the originally proposed skeletal representation was adopted , without a forced distinction between arguments and adjuncts .", "Even after extended training , performance varies markedly by annotator , with speeds on the task of correcting skeletal structure without requiring a distinction between arguments and adjuncts ranging from approximately 750 words per hour to well over 1 , 000 words per hour after three or four months experience .", "The fastest annotators work in bursts of well over 1 , 500 words per hour alternating with brief rests .", "At an average rate of 750 words per hour , a team of five part time annotators annotating three hours a day should maintain an output of about 2 . 5 million words a year of treebanked sentences , with each sentence corrected once .", "It is worth noting that experienced annotators can proofread previously corrected material at very high speeds .", "A parsed subcorpus of over I million words was recently proofread at an average speed of approximately 4 , 000 words per annotator per hour .", "At this rate of productivity , annotators are able to find and correct gross errors in parsing , but do not have time to check , for example , whether they agree with all prepositional phrase attachments .", "323 Computational Linguistics Volume 19 , Number 2 S NP ADJP Battle tested industrial managers ?", "always VP buck ?", "PP up NP nervous newcomers ?", "PP with NP the tale PP of NP the ADJP first ?", "PP of NP their countrymen ?", "S NP to VP visit NP Mexico ?", "NP a boatload PP of NP warriors VP blown ?", "ashore NP 375 years ?", "Figure 4 Sample bracketed text after simplification , before correction .", "The process that creates the skeletal representations to be corrected by the anno tators simplifies and flattens the structures shown in Figure 3 by removing POS tags , nonbranching lexical nodes , and certain phrasal nodes , notably NBAR .", "The output of the first automated stage of the bracketing task is shown in Figure 4 .", "Annotators correct his simplified structure using a mouse based interface .", "Their primary job is to glue fragments ogether , but they must also correct incorrect parses and delete some structure .", "Single mouse clicks perform the following tasks , among others .", "The interface correctly reindents the structure whenever necessary .", "Attach constituents labeled ? .", "This is done by pressing down the appropriate mouse button on or immediately after the ? , moving the mouse onto or immediately after the label of the intended parent and releasing the mouse .", "Attaching constituents automatically deletes their ?", "Promote a constituent up one level of structure , making it a sibling of its current parent .", "Delete a pair of constituent brackets .", "324 Mitchell R Marcus et al .", "Building a Large Annotated Corpus of English S NP Battle tested industrial managers here always VP buck up NP nervous newcomers PP with NP the tale PP of NP NP the ADJP first PP of NP S NP to VP visit NP VP 1 .", "Figure 5 Sample bracketed text after correction .", "their countrymen NP Mexico NP a boatload PP of NP NP warriors VP I blown ashore ADVP NP 375 years ago pseudo attach ?", "Create a pair of brackets around a constituent .", "This is done by typing a constituent tag and then sweeping out the intended constituent with the mouse .", "The tag is checked to assure that it is a legal label .", "Change the label of a constituent .", "The new tag is checked to assure that it is legal .", "The bracketed text after correction is shown in Figure 5 .", "The fragments are now connected together into one rooted tree structure .", "The result is a skeletal analysis in that much syntactic detail is left unannotated .", "Most prominently , all internal structure of the NP up through the head and including any single word post head modifiers is left unannotated .", "As noted above in connection with POS tagging , a major goal of the Treebank project is to allow annotators only to indicate structure of which they were certain .", "The Treebank provides two notational devices to ensure this goal the X constituent label and so called pseudo attachment .", "The X constituent label is used if an annotator is sure that a sequence of words is a major constituent but is unsure of its syntactic category ; in such cases , the annotator simply brackets the sequence and labels it X .", "The second notational device , pseudo attachment , hastwo primary uses .", "On the one hand , 325 Computational Linguistics Volume 19 , Number 2 it is used to annotate what Kay has called permanent predictable ambiguities , allowing an annotator to indicate that a structure isglobally ambiguous even given the surrounding context annotators always assign structure to a sentence on the basis of its context .", "An example of this use of pseudo attachment is shown in Figure 5 , where the participial phrase blown ashore 375 years ago modifies either warriors or boatload , but there is no way of settling the question both attachments mean exactly the same thing .", "In the case at hand , the pseudo attachment notation indicates that the annotator of the sentence thought hat VP 1 is most likely a modifier of warriors , but that it is also possible that it is a modifier of boatload .", "11A second use of pseudo attachment is toallow annotators to represent the underlying position of extraposed elements ; in addition to being attached in its superficial position in the tree , the extraposed constituent is pseudo attached within the constituent to which it is semantically related .", "Note that except for the device of pseudo attachment , the skeletal analysis of the Treebank is entirely restricted to simple context free trees .", "The reader may have noticed that the ADJP brackets in Figure 4 have vanished in Figure 5 .", "For the sake of the overall efficiency of the annotation task , we leave all ADJP brackets in the simplified structure , with the annotators expected to remove many of them during annotation .", "The reason for this is somewhat complex , but provides a good example of the considerations that come into play in designing the details of annotation methods .", "The first relevant fact is that Fidditch only outputs ADJP brackets within NPs for adjective phrases containing more than one lexical item .", "To be consistent , he final structure must contain ADJP nodes for all adjective phrases within NPs or for none ; we have chosen to delete all such nodes within NPs under normal circumstances .", "This does not affect the use of the ADJP tag for predicative adjective phrases outside of NPs .", "In a seemingly unrelated guideline , all coordinate structures are annotated in the Treebank ; such coordinate structures are represented by Chomsky adjunction when the two conjoined constituents bear the same label .", "This means that if an NP contains coordinated adjective phrases , then an ADJP tag will be used to tag that coordination , even though simple ADJPs within NPs will not bear an APJP tag .", "Experience has shown that annotators can delete pairs of brackets extremely quickly using the mouse based tools , whereas creating brackets is a much slower operation .", "Because the coordination of adjectives i quite common , it is more efficient o leave in ADJP labels , and delete them if they are not part of a coordinate structure , than to reintroduce them if necessary .", "Progress to Date 5 . 1 Composition and Size of Corpus Table 4 shows the output of the Penn Treebank project at the end of its first phase .", "All the materials listed in Table 4 are available on CD ROM to members of the Linguistic Data Consortium .", "12About 3 million words of POS tagged material and a small sam piing of skeletally parsed text are available as part of the first Association for Com putational Linguistics Data Collection Initiative CD ROM , and a somewhat larger subset of materials is available on cartridge tape directly from the Penn Treebank Project .", "For information , contact he first author of this paper or send e mail to tree bank unagi . cis . upenn . edu .", "11 This use of pseudo attachment is identical to its original use in Churchs parser Church 1980 .", "12 Contact he Linguistic Data Consortium , 441 Williams Hall , University of Pennsylvania , Philadelphia PA 19104 6305 , or send e mail to ldc unagi . cis . upenn . edu for more information .", "326 Mitchell P . Marcus et al .", "Building a Large Annotated Corpus of English Table 4 Penn Treebank as of 11 92 .", "Description Tagged for Skeletal Part of Speech Parsing Tokens Tokens Dept .", "of Energy abstracts Dow Jones Newswire stories Dept .", "of Agriculture bulletins Library of America texts MUC 3 messages IBM Manual sentences WBUR radio transcripts ATIS sentences Brown Corpus , retagged 231 , 404 231 , 404 3 , 065 , 776 1 , 061 , 166 78 , 555 78 , 555 105 , 652 105 , 652 111 , 828 111 , 828 89 , 121 89 , 121 11 , 589 11 , 589 19 , 832 19 , 832 1 , 172 , 041 1 , 172 , 041 Total 4 , 885 , 798 2 , 881 , 188 Some comments on the materials included ?", "Department of Energy abstracts are scientific abstracts from a variety of disciplines .", "All of the skeletally parsed Dow Jones Newswire materials are also available as digitally recorded read speech as part of the DARPA WSJ CSR1 corpus , available through the Linguistic Data Consortium .", "The Department of Agriculture materials include short bulletins on such topics as when to plant various flowers and how to can various vegetables and fruits .", "The Library of America texts are 5 , 000 10 , 000 word passages , mainly book chapters , from a variety of American authors including Mark Twain , Henry Adams , Willa Cather , Herman Melville , W . E . B . Dubois , and Ralph Waldo Emerson .", "The MUC 3 texts are all news stories from the Federal News Service about terrorist activities in South America .", "Some of these texts are translations of Spanish news stories or transcripts of radio broadcasts .", "They are taken from training materials for the Third Message Understanding Conference .", "The Brown Corpus materials were completely retagged by the Penn Treebank project starting from the untagged version of the Brown Corpus Francis 1964 .", "The IBM sentences are taken from IBM computer manuals ; they are chosen to contain a vocabulary of 3 , 000 words , and are limited in length .", "The ATIS sentences are transcribed versions of spontaneous sentences collected as training materials for the DARPA Air Travel Information System project .", "The entire corpus has been tagged for POS information , at an estimated error rate 327 Computational Linguistics Volume 19 , Number 2 of approximately 3 .", "The POS tagged version of the Library of America texts and the Department of Agriculture bulletins have been corrected twice each by a different annotator , and the corrected files were then carefully adjudicated ; we estimate the error rate of the adjudicated version at well under 1 .", "Using a version of PARTS retrained on the entire preliminary corpus and adjudicating between the output of the retrained version and the preliminary version of the corpus , we plan to reduce the error rate of the final version of the corpus to approximately 1 .", "All the skeletally parsed materials have been corrected once , except for the Brown materials , which have been quickly proofread an additional time for gross parsing errors .", "5 . 2 Future Direct ions A large number of research efforts , both at the University of Pennsylvania nd else where , have relied on the output of the Penn Treebank Project o date .", "A few examples already in print a number of projects investigating stochastic parsing have used either the POS tagged materials Magerman and Marcus 1990 ; Brill et al .", "1990 ; Brill 1991 or the skeletally parsed corpus Weischedel et al .", "1991 ; Pereira and Schabes 1992 .", "The POS tagged corpus has also been used to train a number of different POS taggers in cluding Meteer , Schwartz , and Weischedel 1991 , and the skeletally parsed corpus has been used in connection with the development of new methods to exploit intonational cues in disambiguating the parsing of spoken sentences Veilleux and Ostendorf 1992 .", "The Penn Treebank has been used to bootstrap the development of lexicons for particu lar applications Robert Ingria , personal communication and is being used as a source of examples for linguistic theory and psychological modelling e . g .", "To aid in the search for specific examples of grammatical phenomena using the Treebank , Richard Pito has developed tgrep , a tool for very fast context free pattern matching against he skeletally parsed corpus , which is available through the Linguistic Data Consortium .", "While the Treebank is being widely used , the annotation scheme mployed has a variety of limitations .", "Many otherwise clear argument adjunct relations in the corpus are not indicated because of the current Treebanks essentially context free represen tation .", "For example , there is at present no satisfactory representation for sentences in which complement oun phrases or clauses occur after a sentential level adverb .", "Either the adverb is trapped within the VP , so that the complement can occur within the VP where it belongs , or else the adverb is attached to the S , closing off the VP and forcing the complement to attach to the S . This trapping problem serves as a limitation for groups that currently use Treebank material semiautomatically to derive lexicons for particular applications .", "For most of these problems , however , solutions are possible on the basis of mechanisms already used by the Treebank Project .", "For example , the pseudo attachment no ation can be extended to indicate a variety of crossing depen dencies .", "We have recently begun to use this mechanism to represent various kinds of dislocations , and the Treebank annotators themselves have developed a detailed proposal to extend pseudo attachment to a wide range of similar phenomena .", "A variety of inconsistencies in the annotation scheme used within the Treebank have also become apparent with time .", "The annotation schemes for some syntactic categories should be unified to allow a consistent approach to determining predicate argument structure .", "To take a very simple example , sentential adverbs attach under VP when they occur between auxiliaries and predicative ADJPs , but attach under S when they occur between auxiliaries and VPs .", "These structures need to be regularized .", "As the current Treebank has been exploited by a variety of users , a significant number have expressed a need for forms of annotation richer than provided by the projects first phase .", "Some users would like a less skeletal form of annotation of surface 328 Mitchell P . Marcus et al .", "Building a Large Annotated Corpus of English grammatical structure , expanding the essentially context free analysis of the current Penn Treebank to indicate a wide variety of noncontiguous structures and dependen cies .", "A wide range of Treebank users now strongly desire a level of annotation that makes explicit some form of predicate argument structure .", "The desired level of rep resentation would make explicit the logical subject and logical object of the verb , and would indicate , at least in clear cases , which subconstituents serve as arguments of the underlying predicates and which serve as modifiers .", "During the next phase of the Treebank project , we expect o provide both a richer analysis of the existing corpus and a parallel corpus of predicate argument structures .", "This will be done by first enriching the annotation of the current corpus , and then automatically extracting predicate argument structure , at the level of distinguishing logical subjects and objects , and distinguishing arguments from adjuncts for clear cases .", "Enrichment will be achieved by automatically transforming the current Penn Treebank into a level of structure close to the intended target , and then completing the conversion by hand .", "Acknowledgments The work reported here was partially supported by DARPA grant No .", "N0014 85 K0018 , byDARPA and AFOSR jointly under grant No .", "AFOSR 90 0066 and by ARO grant No .", "DAAL 03 89 C0031 PRI .", "Seed money was provided by the General Electric Corporation under grant No .", "We gratefully acknowledge this support .", "We would also like to acknowledge the contribution of the annotators who have worked on the Penn Treebank Project Florence Dong , Leslie Dossey , Mark Ferguson , Lisa Frank , Elizabeth Hamilton , Alissa Hinckley , Chris Hudson , Karen Katz , Grace Kim , Robert MacIntyre , Mark Parisi , Britta Schasberger , Victoria Tredinnick and Matt Waters ; in addition , Rob Foye , David Magerman , Richard Pito and Steven Shapiro deserve our special thanks for their administrative and programming support .", "We are grateful to AT T Bell Labs for permission to use Kenneth Churchs PARTS part of speech labeler and Donald Hindles Fidditch parser .", "Finally , we would like to thank Sue Marcus for sharing with us her statistical expertise and providing the analysis of the time data of the experiment reported in Section 3 .", "The design of that experiment is due to the first two authors ; they alone are responsible for its shortcomings .", "References Brill , Eric 1991 .", "Discovering the lexical features of a language .", "In Proceedings , 29th Annual Meeting of the Association for Computational Linguistics .", "Brill , Eric ; Magerman , David ; Marcus , Mitchell P . ; and Santorini , Beatrice 1990 .", "Deducing linguistic structure from the statistics of large corpora .", "In Proceedings , DARPA Speech and Natural Language Workshop .", "June 1990 , 275 282 .", "Church , Kenneth W . 1980 .", "Memory limitations in natural language processing .", "Masters dissertation , Massachusetts Institute of Technology , Cambridge MA .", "Church , Kenneth W . 1988 .", "A stochastic parts program and noun phrase parser for unrestricted text .", "In Proceedings , Second Conference on Applied Natural Language Processing .", "Francis , W . Nelson 1964 .", "A standard sample of present day English for use with digital computers .", "Report o the U . S Office of Education on Cooperative Research Project No .", "Brown University , Providence RI .", "Francis , W . Nelson , and Ku era , Henry 1982 .", "Frequency Analysis of English Usage Lexicon and Grammar .", "Houghton Mifflin .", "Garside , Roger ; Leech , Geoffrey ; and Sampson , Geoffrey 1987 .", "The Computational Analysis of English A Corpus Based Approach .", "Hindle , Donald 1983 .", "User manual for Fidditch .", "Technical memorandum 7590 142 , Naval Research Laboratory .", "Hindle , Donald 1989 .", "Acquiring disambiguation rules from text .", "In Proceedings , 27th Annual Meeting of the Association for Computational Linguistics .", "Lewis , Bil ; LaLiberte , Dan ; and the GNU Manual Group 1990 .", "The GNU Emacs Lisp reference manual .", "Free Software Foundation , Cambridge , MA .", "Magerman , David , and Marcus , Mitchell P . 1990 .", "Parsing a natural language using 329 Computational Linguistics Volume 19 , Number 2 mutual information statistics .", "In Proceedings of AAAI 90 .", "Meteer , Marie ; Schwartz , Richard ; and Weischedel , Ralph 1991 .", "Studies in part of speech labelling .", "In Proceedings , Fourth DARPA Speech and Natural Language Workshop .", "Niv , Michael 1991 .", "Syntactic disambiguation .", "In The Penn Review of Linguistics , 14 , 120 126 .", "Pereira , Fernando , and Schabes , Yves 1992 .", "Inside outside r estimation from partially bracketed corpora .", "In Proceedings , 30th Annual Meeting of the Association for Computational Linguistics .", "Santorini , Beatrice 1990 .", "Part of speech tagging guidelines for the Penn Treebank Project .", "Technical report MS CIS 90 47 , Department of Computer and Information Science , University of Pennsylvania .", "Santorini , Beatrice , and Marcinkiewicz , Mary Ann 1991 .", "Bracketing uidelines for the Penn Treebank Project .", "Unpublished manuscript , Department of Computer and Information Science , University of Pennsylvania .", "Veilleux , N . M . , and Ostendorf , Mari 1992 .", "Probabilistic parse scoring based on prosodic features .", "In Proceedings , Fifth DARPA Speech and Natural Language Workshop .", "Weischedel , Ralph ; Ayuso , Damaris ; Bobrow , R . ; Boisen , Sean ; Ingria , Robert ; and Palmucci , Jeff 1991 .", "Partial parsing a report of work in progress .", "In Proceedings , Fourth DARPA Speech and Natural Language Workshop ."], "summary_lines": ["Building A Large Annotated Corpus Of English: The Penn Treebank\n"]}
{"article_lines": ["An Annotation Scheme For Free Word Order Languages", "We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non configurational languages .", "Since the requirements for such a formalism differ from those posited for configurational languages , several features have been added , influencing the architecture of the scheme .", "The resulting scheme reflects a stratificational notion of language , and makes only minimal assumpabout the interrelation of the particu lar representational strata .", "The work reported in this paper aims at providing syntactically annotated corpora treebanks' for stochastic grammar induction .", "In particular , we focus on several methodological issues concerning the annotation of non configurational languages .", "In section 2 , we examine the appropriateness of existing annotation schemes .", "On the basis of these considerations , we formulate several additional requirements .", "A formalism complying with these requirements is described in section 3 .", "Section 4 deals with the treatment of selected phenomena .", "For a description of the annotation tool see section 5 .", "Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods .", "Realworld texts annotated with different strata of linguistic information can be used for grammar induction .", "The data drivenness of this approach presents a clear advantage over the traditional , idealised notion of competence grammar .", "Corpora annotated with syntactic structures are commonly referred to as trctbank . 5 .", "Existing treebank annotation schemes exhibit a fairly uniform architecture , as they all have to meet the same basic requirements , namely Descriptivity Grammatical phenomena are to be described rather than explained .", "Theory independence Annotations should not be influenced by theory specific considerations .", "Nevertheless , different theory specific representations shall be recoverable from the annotation , cf .", "Marcus et al . , 1994 .", "Multi stratal representation Clear separation of different description levels is desirable .", "Data drivenness The scheme must provide representational means for all phenomena occurring in texts .", "Disambiguation is based on human processing skills cf .", "Marcus et . al . , 1994 , Sampson , 1995 , Black et . al .", "The typical treebank architecture is as follows Structures A context free backbone is augmented with trace filler representations of non local dependencies .", "The underlying argument SirlteilITC is not represented directly , but can be recovered from the tree and trace filler annotations .", "Syntactic category is encoded in node labels .", "Grammatical functions constitute a complex label system cf .", "Bies et al . , 1995 , Sampson , 1995 .", "Part of Speech is annotated at word level .", "Thus the context free constituent backbone plays a pivotal role in the annotation scheme .", "Due to the substantial differences between existing models of constituent structure , the question arises of how the theory independencf requirement , can be satisfied .", "At , this point the importance of the underlying argument structure is emphasised cf .", "Lehmann et al . , 1996 , Marcus et al . , 1994 , Sampson , 1995 .", "Treebanks of the format , described in the above section have been designed for English .", "Therefore , the solutions they offer are not always optimal for other language types .", "As for free word order languages , the following features may cause problems sition between the two poles .", "In light of these facts , serious difficulties can be expected arising from the structural component of the existing formalisms .", "Due to the frequency of discontinuous constituents in non configurational languages , the filler trace mechanism would be used very often , yielding syntactic trees fairly different from the underlying predicate argument structures .", "Consider the German sentence 1 daran wird ihn Anna erkennen , di er weint at it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below The fairly short sentence contains three non local dependencies , marked by co references between traces and the corresponding nodes .", "This hybrid representation makes the structure less transparent , and therefore more difficult to annotate .", "Apart from this rather technical problem , two further arguments speak against phrase structure as the structural pivot of the annotation scheme Finally , the structural handling of free word order means stating well formedness constraints on structures involving many trace filler dependencies , which has proved tedious .", "Since most methods of handling discontinuous constituents make the formalism more powerful , the efficiency of processing deteriorates , too .", "An alternative solution is to make argument structure the main structural component of the formalism .", "This assumption underlies a growing number of recent syntactic theories which give up the context free constituent backbone , cf .", "McCawley , 1987 , Dowty , 1989 , Reape , 1993 , Kathol and Pollard , 1995 .", "These approaches provide an adequate explanation for several issues problematic for phrase structure grammars clause union , extraposition , diverse second position phenomena .", "Argument structure can be represented in terms of unordered trees with crossing branches .", "In order to reduce their ambiguity potential , rather simple , 'flat' trees should be employed , while more information can be expressed by a rich system of function labels .", "Furthermore , the required theory independence means that the form of syntactic trees should not reflect theory specific assumptions , e . g . every syntactic structure has a unique head .", "Thus , notions such as head should be distinguished at the level of syntactic functions rather than structures .", "This requirement speaks against the traditional sort of dependency trees , in which heads a , re represented as non terminal nodes , cf .", "Hudson , 1984 .", "A tree meeting these requirements is given below Adv V NP NP V CPL NP V damn wird ihn Anna erkennen , dais er 'vein !", "Such a word order independent representation has the advantage of all structural information being encoded in a single data structure .", "A uniform representation of local and non local dependencies makes the structure more transparent' .", "We distinguish the following levels of representation 'A context free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node .", "Argument structure , represented in terms of unordered trees .", "Grammatical functions , encoded in edge labels , e . g .", "SB subject , MO modifier , HD head .", "Syntactic categories , expressed by category labels assigned to non terminal nodes and by part of speech tags assigned to terminals .", "A structure for 2 is shown in fig .", "2 schade , daB kein Arzt anwesend ist , der pity that no doctor present is who sich a . uskennt is competent 'Pity that no competent doctor is here' Note that the root node does not have a head descendant HD as the sentence is a predicative construction consisting of a subject SB and a predicate PD without a copula .", "The subject is itself a sentence in which the copula zA does occur and is assigned the tag HD' .", "The tree resembles traditional constituent structures .", "The difference is its word order independence structural units quot ; phrases quot ; need not be contiguous substrings .", "For instance , the extraposed relative clause RC is still treated as part of the subject NP .", "As the annotation scheme does not distinguish different bar levels or any similar intermediate categories , only a small set of node labels is needed currently 16 tags , S , NP , AP . . . .", "Due to the rudimentary character of the argument structure representations , a great deal of information has to be expressed by grammatical functions .", "Their further classification must reflect different kinds of linguistic information morphology e . g . , case , inflection , category , dependency type complementation vs . modification , thematic role , etc . '", "However , there is a trade off between the granularity of information encoded in the labels and the speed and accuracy of annotation .", "In order to avoid inconsistencies , the corpus is annotated in two stages basic annotation and nfirtellte714 .", "While in the first phase each annotator has to annotate structures as well as categories and functions , the refinement call be done separately for each representation level .", "During the first phase , the focus is on annotating correct structures and a coarse grained classification of grammatical functions , which represent the following areas of information Dependency type complements are further classified according to features such as category and case clausal complements OC , accusative objects OA , datives DA , etc .", "Modifiers are assigned the label MO further classification with respect to thematic roles is planned .", "Separate labels are defined for dependencies that do not fit the complement modifier dichotomy , e . g . , pre GL and postnominal genitives GR .", "Headed and non headed structures are distinguished by the presence or absence of a branch labeled HD .", "Morphological information Another set of labels represents morphological information .", "PM stands for morphological particle , a label for German infinitival Z7t and superlative am .", "Separable verb prefixes are labeled SVP .", "During the second annotation stage , the annotation is enriched with information about thematic roles , quantifier scope and anaphoric reference .", "As already mentioned , this is done separately for each of the three information areas .", "A phrase or a lexical item can perform multiple functions in a sentence .", "Consider qui verbs where the subject of the infinitival VP is not realised syntactically , but co referent with the subject or object . of the matrix equi verb 3 er bat mich zu kommen he asked me to come mich is the understood subject . of kommt , n .", "In such cases , an additional edge is drawn from the embedded VP node to the controller , thus changing the syntactic tree into a graph .", "We call such additional edges secondary links and represent them as dotted lines , see fig .", "4 , showing the structure of 3 .", "As theory independence is one of our objectives , the annotation scheme incorporates a number of widely accepted linguistic analyses , especially in the area of verbal , adverbial and adjectival syntax .", "However , some other standard analysts turn out to be problematic , mainly due to the partial , idealised character of competence grammars , which often marginalise or ignore such important . phenomena . as 'deficient' e . g . headless constructions , appositions , temporal expressions , etc .", "In the following paragraphs , we give annotations for a number of such phenomena . .", "Most linguistic theories treat NPs as structures headed by a unique lexical item noun .", "However , this idealised model needs several additional assumptions in order to account for such important phenomena as complex nominal NP components cf .", "4 or nominalised adjectives cf .", "In 4 , different theories make different headedness predictions .", "In 5 , either a lexical nominalisation rule for the adjective Gliickliche is stipulated , or the existence of an empty nominal head .", "Moreover , the so called DP analysis views the article der as the head of the phrase .", "Further differences concern the attachment of the degree modifier sehr .", "Because of the intended theory independence of the scheme , we annotate only the common minimum .", "We distinguish an NP kernel consisting of determiners , adjective phrases and nouns .", "All components of this kernel are assigned the label NK and treated as sibling nodes .", "The difference between the particular NK's lies in the positional and part of speech information , which is also sufficient to recover theory specific structures from our underspecified' representations .", "For instance , the first , determiner among the NK's can be treated as the specifier of the phrase .", "The head of the phrase can be determined in a similar way according to theory specific assumptions .", "In addition , a . number of clear cut NP cornponents can be defined outside that , juxtapositional kernel pre and postnominal genitives GL , GR , relative clauses RC , clausal and sentential complements OC .", "They are all treated as siblings of NK's regardless of their position in situ or extraposed .", "Adjunct attachment often gives rise to structural ambiguities or structural uncertainty .", "However , full or partial disambiguation takes place in context , and the annotators do not consider unrealistic readings .", "In addition , we have adopted a simple convention for those cases in which context information is insufficient , for total disambiguation the highest possible attachment , site is chosen .", "A similar convention ha . s been adopted for constructions in which scope ambiguities have syntactic effects but a one to one correspondence between scope and attachment . does not seem reasonable , cf . focus particles such as only or also .", "If the scope of such a word does not directly correspond to a tree node , the word is attached to the lowest node dominating all subconstituents appearing in its scope .", "A problem for the rudimentary argument . structure representations is the use of incomplete structures in natural language , i . e . phenomena such as coordination and ellipsis .", "Since a precise structural description of non constituent coordination would require a . rich inventory of incomplete phrase types , we have agreed on a sort of unde . rspe . cified representations the coordinated units are assigned structures in which missing lexical material is not represented at the level of primary links .", "Fig .", "3 shows the representation of the sentence 6 sic wurde von preuBischen Truppen besetzt she was by Prussian troops occupied und 1887 dem preuliischen Staat angegliedert and 1887 to the Prussian state incorporated 'it was occupied by Prussian troops and incorporated into Prussia in 1887' The category of the coordination is labeled CVP here , where C stands for coordination , and VP for the actual category .", "This extra marking makes it easy to distinguish between 'normal' and coordinated categories .", "Multiple coordination as well as enumerations are annotated in the same way .", "An explicit coordinating conjunction need not be present .", "Structure sharing is expressed using secondary links .", "The development of linguistically interpreted corpora presents a laborious and time consuming task .", "In order to make the annotation process more efficient , extra effort has been put . into the development of an annotation tool .", "The tool supports immediate graphical feedback and automatic error checking .", "Since our scheme permits crossing edges , visualisation as bracketing and indentation would be insufficient . .", "Instead , the complete structure should be represented .", "The tool should also permit a convenient handling of node and edge labels .", "In particular , variable tagsets and label collections should be allowed .", "As the need for certain functionalities becomes obvious with growing annotation experience , we have decided to implement the tool in two stages .", "In the first phase , the main functionality for building and displaying unordered trees is supplied .", "In the second phase , secondary links and additional structural functions are supported .", "The implementation of the first phase as described in the following paragraphs is completed .", "As keyboard input , is more efficient than mouse input cf .", "Lehmann et al . , 1996 most effort , has been put in developing an efficient keyboard interface .", "Menus are supported as a useful way of getting help on commands and labels .", "In addition to pure annotation , we can attach comments to structures .", "Figure 1 shows a screen dump of the tool .", "The largest part of the window contains the graphical representation of the structure being annotated .", "The following commands are available The three tagsets used by the annotation tool for words , phrases , and edges are variable and are stored together with the corpus .", "This allows easy modification if needed .", "The tool checks the appropriateness of the input .", "For the implementation , we used Tcl Tk Version 4 . 1 .", "The corpus is stored in a SQL database .", "The degree of automation increases with the amount of data available .", "Sentences annotated in previous steps are used as training material for further processing .", "We distinguish five degrees of automation So far , about 1100 sentences of our corpus have been annotated .", "This amount of data suffices as training material to reliably assign the grammatical functions if the user determines the elements of a phrase and its type step 1 of the list above .", "Grammatical functions are assigned using standard statistical part of speech tagging methods cf . e . g .", "Cutting et al . , 1992 and Feldweg , 1995 .", "For a phrase Q with children of type T . . . , Ta and grammatical functions G . . . , GA . , we use the lexical probabilities and the contextual trigram probabilities The lexical and contextual probabilities are determined separately for each type of phrase .", "During annotation , the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure , i . e . , we calculate argma . x11 PQ Ti 1Z 1 , Ti . 2 PQ Gi ITi .", "To keep the human annotator from missing errors made by the tagger , we additionally calculate the strongest competitor for each label G . If its probability is close to the winner closeness is defined by a threshold on the quotient , the assignment is regarded as unreliable , and the annotator is asked to confirm the assignment .", "For evaluation , the already annotated sentences were divided into two disjoint sets , one for training 90 of the corpus , the other one for testing 10 .", "The procedure was repeated 10 times with different . partitionings .", "The tagger rates 90 of all assignments as reliable and carries them out fully automatically .", "Accuracy for these cases is 97 .", "Most errors are due to wrong identification of the subject and different kinds of objects in sentences and VPs .", "Accuracy of the unreliable 10 of assignments is 75 , i . e . , the annotator has to alter the choice in 1 of 4 cases when asked for confirmation .", "Overall accuracy of the tagger is 95 .", "Owing to the partial automation , the average annotation efficiency improves by 25 from around 4 minutes to 3 minutes per sentence .", "As the annotation scheme described in this paper focusses on annotating argument structure rather than constituent trees , it differs from existing treebanks in several aspects .", "These differences can be illustrated by a comparison with the Penn Treebank annotation scheme .", "The following features of our formalism are then of particular importance The current tagset comprises only 16 node labels and 34 function tags , yet a . finely grinned classification will take place in the near future .", "We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order .", "In general , the resulting interpreted data also are closer to semantic annotation and more neutral with respect to particular syntactic theories .", "As modern linguistics is also becoming more aware of the importance of larger sets of naturally occurring data . , interpreted corpora . are a . valuable resource for theoretical and descriptive linguistic research .", "In addition the approach provides empirical material for psycholinguistic investigation , since preferences for the choice of certain syntactic constructions , linea . rizations , and attachments that have been observed in online experiments of language production and comprehension can now be put in relation with the frequency of these alternatives in larger amounts of texts .", "Syntactically annotated corpora of German have been missing until now .", "In the second phase of the project Verbmobil a . treebank for 30 , 000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created .", "We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation .", "Since the combinatorics of syntactic constructions creates a . demand for very large corpora . , efficiency of annotation is an important . criterion for the success of the developed methodology and tools .", "Our annotation tool supplies efficient manipulation and immediate visualization of argument structures .", "Partial automation included in the current version significantly reduces the manna . 1 effort .", "Its extension is subject to further investigations .", "This work is part of the DFG Sonderforschungsbereich 378 Rcsource Adaptim Cogniiivc Proccsses , We wish to thank Tania Avgustinova , Berthold Crysmann , Lars Konieczny , Stephan Oepen , Karel Oliva . , Christian Weil3 and two anonymous reviewers for their helpful comments on the content of this paper .", "We also wish to thank Robert MacIntyre and Ann Taylor for valuable discussions on the Penn Treebank annotation .", "Special thanks go t , o Oliver Plaehn , who implemented the annotation tool , and to our fearless annotators Roland Hendriks , Kerstin Klockner , Thomas Schulz , and Bernd Paul Simon ."], "summary_lines": ["An Annotation Scheme For Free Word Order Languages\n", "We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.\n", "Since the requirements for such a formalism differ from those posited for configurational languages, several features have been added, influencing the architecture of the scheme.\n", "The resulting scheme reflects a stratificational notion of language, and makes only minimal assumptions about the interrelation of the particular representational strata.\n", "We release the NEGRA corpus, a hand parsed corpus of German newspaper text containing approximately 20,000 sentences.\n"]}
{"article_lines": ["CCGbank A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank", "article presents an algorithm for translating the Penn Treebank into a corpus of Combinatory Categorial Grammar CCG derivations augmented with local and long range word word dependencies .", "The resulting corpus , CCGbank , includes 99 . 4 of the sentences in the Penn Treebank .", "It is available from the Linguistic Data Consortium , and has been used to train widecoverage statistical parsers that obtain state of the art rates of dependency recovery .", "In order to obtain linguistically adequate CCG analyses , and to eliminate noise and inconsistencies in the original annotation , an extensive analysis of the constructions and annotations in the Penn Treebank was called for , and a substantial number of changes to the Treebank were necessary .", "We discuss the implications of our findings for the extraction of other linguistically expressive grammars from the Treebank , and for the design offuture treebanks .", "This article presents an algorithm for translating the Penn Treebank into a corpus of Combinatory Categorial Grammar CCG derivations augmented with local and long range word word dependencies .", "The resulting corpus , CCGbank , includes 99 . 4 of the sentences in the Penn Treebank .", "It is available from the Linguistic Data Consortium , and has been used to train widecoverage statistical parsers that obtain state of the art rates of dependency recovery .", "In order to obtain linguistically adequate CCG analyses , and to eliminate noise and inconsistencies in the original annotation , an extensive analysis of the constructions and annotations in the Penn Treebank was called for , and a substantial number of changes to the Treebank were necessary .", "We discuss the implications of our findings for the extraction of other linguistically expressive grammars from the Treebank , and for the design offuture treebanks .", "In order to understand a newspaper article , or any other piece of text , it is necessary to construct a representation of its meaning that is amenable to some form of inference .", "This requires a syntactic representation which is transparent to the underlying semantics , making the local and long range dependencies between heads , arguments , and modifiers explicit .", "It also requires a grammar that has sufficient coverage to deal with the vocabulary and the full range of constructions that arise in free text , together with a parsing model that can identify the correct analysis among the many alternatives that such a wide coverage grammar will generate even for the simplest sentences .", "Given our current machine learning techniques , such parsing models typically need to be trained on relatively large treebanks that is , text corpora hand labeled with detailed syntactic structures .", "Because such annotation requires linguistic expertise , and is therefore difficult to produce , we are currently limited to at most a few treebanks per language .", "One of the largest and earliest such efforts is the Penn Treebank Marcus , Santorini , and Marcinkiewicz 1993 ; Marcus et al . 1994 , which contains a one million word subcorpus of Wall Street Journal text that has become the de facto standard training and test data for statistical parsers .", "Its annotation , which is based on generic phrasestructure grammar with coindexed traces and other null elements indicating non local dependencies and function tags on nonterminal categories providing a limited degree of syntactic role information , is designed to facilitate the extraction of the underlying predicate argument structure .", "Statistical parsing on the Penn Treebank has made great progress by focusing on the machine learning or algorithmic aspects Magerman 1994 ; Ratnaparkhi 1998 ; Collins 1999 ; Charniak 2000 ; Henderson 2004 ; McDonald , Crammer , and Pereira 2005 .", "However , this has often resulted in parsing models and evaluation measures that are both based on reduced representations which simplify or ignore the linguistic information represented by function tags and null elements in the original Treebank .", "One exception is Collins 1999 , whose Model 2 includes a distinction between arguments and adjuncts , and whose Model 3 additionally captures wh movement in relative clauses with a GPSG like slash feature passing mechanism .", "The reasons for this shift away from linguistic adequacy are easy to trace .", "The very healthy turn towards quantitative evaluation interacts with the fact that just about every dimension of linguistic variation exhibits a Zipfian distribution , where a very small proportion of the available alternatives accounts for most of the data .", "This creates a temptation to concentrate on capturing the few high frequency cases at the top end of the distribution , and to ignore the long tail of rare events such as non local dependencies .", "Despite the fact that these occur in a large number of sentences , they affect only a small number of words , and have thus a small impact on overall dependency recovery .", "Although there is now a sizable literature on trace and function tag insertion algorithms Blaheta and Charniak 2000 ; Johnson 2002 ; Campbell 2004 , and integrated parsing with function tags or null elements Dienes and Dubey 2003a , 2003b ; Merlo and Musillo 2005 ; Gabbard , Kulick , and Marcus 2006 , such approaches typically require additional pre or postprocessing steps that are likely to add further noise and errors to the parser output .", "A completely integrated approach that is based on a syntactic representation which allows direct recovery of the underlying predicate argument structure might therefore be preferable .", "Such representations are provided by grammar formalisms that are more expressive than simple phrase structure grammar , like Lexical Functional Grammar LFG Kaplan and Bresnan 1982 , Head driven Phrase Structure Grammar HPSG Pollard and Sag 1994 , Tree Adjoining Grammar TAG Joshi and Schabes 1992 , Minimalist Program related Grammars Stabler 2004 , or Combinatory Categorial Grammar CCG Steedman 1996 , 2000 .", "However , until very recently , only handwritten grammars , which lack the wide coverage and robustness of Treebank parsers , were available for these formalisms Butt et al . 1999 ; XTAG group 1999 ; Copestake and Flickinger 2000 ; OpenCCG1 White and Baldridge 2003 ; White 2006 .", "Because treebank annotation for individual formalisms is prohibitively expensive , there have been a number of efforts to extract TAGs , LFGs , and , more recently , HPSGs , from the Penn Treebank Xia 1999 ; Chen and Vijay Shanker 2000 ; Xia , Palmer , and Joshi 2000 ; Xia 2001 ; Cahill et al . 2002 ; Miyao , Ninomiya , and Tsujii 2004 ; O Donovan et al .", "2005 ; Shen and Joshi 2005 ; Chen , Bangalore , and Vijay Shanker 2006 .", "Statistical parsers that are trained on these TAG and HPSG corpora have been presented by Chiang 2000 and Miyao and Tsujii 2005 , whereas the LFG parsing system of Cahill et al . 2004 uses a postprocessing step on the output of a Treebank parser to recover predicate argument dependencies .", "In this article we present an algorithmic method for obtaining a corpus of CCG derivations and dependency structures from the Penn Treebank , together with some observations that we believe carry wider implications for similar attempts with other grammar formalisms and corpora .", "Earlier versions of the resulting corpus , CCGbank , have already been used to build a number of wide coverage statistical parsers Clark , Hockenmaier , and Steedman 2002 ; Hockenmaier and Steedman 2002 ; Hockenmaier 2003b , 2003a ; Clark and Curran 2004 , 2007 , which recover both local and long range dependencies directly and in a single pass .", "CCG is a linguistically expressive , but efficiently parseable , lexicalized grammar formalism that was specifically designed to provide a base generative account of coordinate and relativized constructions like the following CCG directly captures the non local dependencies involved in these and other constructions , including control and raising , via an enriched notion of syntactic types , without the need for syntactic movement , null elements , or traces .", "It also provides a surface compositional syntax semantics interface , in which monotonic rules of semantic composition are paired one to one with rules of syntactic composition .", "The corresponding predicate argument structure or logical form can therefore be directly obtained from any derivation if the semantic interpretation of each lexical entry is known .", "In this article and in CCGbank , we approximate such semantic interpretations with dependency graphs that include most semantically relevant non anaphoric local and long range dependencies .", "Although certain decisions taken by the builders of the original Penn Treebank mean that the syntactic derivations that can be obtained from the Penn Treebank are not always semantically correct as we will discuss , subsequent work by Bos et al . 2004 and Bos 2005 has demonstrated that the output of parsers trained on CCGbank can also be directly translated into logical forms such as Discourse Representation Theory structures Kamp and Reyle 1993 , which can then be used as input to a theorem prover in applications like question answering and textual entailment recognition .", "Translating the Treebank into this more demanding formalism has revealed certain sources of noise and inconsistency in the original annotation that have had to be corrected in order to permit induction of a linguistically correct grammar .", "Because of this preprocessing , the dependency structures in CCGbank are likely to be more consistent than those extracted directly from the Treebank via heuristics such as those given by Magerman 1994 and Collins 1999 , and therefore may also be of immediate use for dependency based approaches .", "However , the structure of certain constructions , such as compound nouns or fragments , is deliberately underspecified in the Penn Treebank .", "Although we have attempted to semi automatically restore the missing structure wherever possible , in many cases this would have required additional manual annotation , going beyond the scope of our project .", "We suspect that these properties of the original Treebank will affect any similar attempt to extract dependency structures or grammars for other expressive formalisms .", "The Penn Treebank is the earliest and still the largest corpus of its kind ; we hope that our experiences will extend its useful life , and help in the design of future treebanks .", "Combinatory Categorial Grammar CCG was originally developed as a near contextfree theory of natural language grammar , with a very free definition of derivational structure adapted to the analysis of coordination and unbounded dependency without movement or deletion transformations .", "It has been successfully applied to the analysis of coordination , relative clauses and related constructions , intonation structure , binding and control , and quantifier scope alternation , in a number of languages see Steedman and Baldridge 2006 for a recent review .", "Extensions of CCG to other languages and word orders are discussed by Hoffman 1995 , Kang 1995 , Bozsahin 1998 , Komagata 1999 , Steedman 2000 , Trechsel 2000 , Baldridge 2002 , and C ak\u0131c\u0131 2005 .", "The derivations in CCGbank follow the analyses of Steedman 1996 , 2000 , except where noted .", "Categorial Grammars are strongly lexicalized , in the sense that the grammar is entirely defined by a lexicon in which words and other lexical items are associated with one or more specific categories which completely define their syntactic behavior .", "The set of categories consists of basic categories e . g . , S , NP , PP and complex categories of the form X Y or X Y , representing functors with basic or complex argument category Y and result category X . Functor categories of the form X Y expect their argument Y to its right , whereas those of the form X Y expect Y to their left . 2 These functor categories encode subcategorization information , that is , the number and directionality of expected arguments .", "English intransitive verbs and verb phrases have the category S NP they take a subject NP to their left as argument and yield a sentence .", "English transitive verbs have the category S NP NP they take an object NP to their right to yield a verb phrase S NP , which in turn takes a subject NP to its left to form a sentence S . Each syntactic category also has a corresponding semantic interpretation here given as a A expression .", "Hence , the lexical entry for ditransitive give can be written as follows 3 In our translation algorithm , we use simple word word dependency structures to approximate the underlying semantic interpretation .", "A universal set of syntactic combinatory rules defines how constituents can be combined .", "All variants of categorial grammar since Ajdukiewicz 1935 and Bar Hillel 1953 include function application , where a functor X Y or X Y is applied to an argument Y These rules give rise to derivations like the following 4 This derivation is isomorphic to a traditional context free derivation tree like the following the semantics is omitted CCG additionally introduces a set of rule schemata based on the combinators of combinatory logic Curry and Feys 1958 , which enable succinct analyses of extraction and coordination constructions .", "It is a distinctive property of CCG that all syntactic rules are purely type driven , unlike traditional structure dependent transformations .", "Composition and substitution allow two functors to combine into another functor , whereas type raising is a unary rule that exchanges the roles of functor and argument For example , the following is the derivation of a relative clause related to 4 We will see further examples of their use later .", "Such rules induce additional derivational ambiguity , even in canonical sentences like 4 .", "However , our translation algorithm yields normal form derivations Hepple and Morrill 1989 ; Wittenburg and Wall 1991 ; K onig 1994 ; Eisner 1996 , which use composition and type raising only when syntactically necessary .", "For coordination , we will use a binarized version of the following ternary rule schema 5 For further explanation and linguistics and computational motivation for this theory of grammar , the reader is directed to Steedman 1996 , 2000 .", "The syntactic derivations in CCGbank are accompanied with bilexical head dependency structures , which are defined in terms of the lexical heads of functor categories and their arguments .", "The derivation in 6 corresponds to the following dependency structure , which includes the long range dependency between give and money The dependency structures in CCGbank are intended to include all non anaphoric local and long range dependencies relevant to determining semantic predicate argument relations , and hence approximate more fine grained semantic representations .", "In this , they differ crucially from the bilexical surface dependencies used by the parsing models of Collins 1999 and Charniak 2000 and returned by the dependency parser of McDonald , Crammer , and Pereira 2005 .", "In order to obtain such non local dependencies , certain types of lexical category such as relative pronouns or raising and control verbs require additional coindexation information described subsequently .", "We believe that CCGbank s extensive annotation of non local predicate argument dependencies is one of its most useful features for researchers using other expressive grammar formalisms , including LFG , HPSG , and TAG , facilitating comparisons in terms of error analyses of particular constructions or types of dependency , such as non subject extracted relative clauses .", "Because these dependency structures provide a suitable approximation of the underlying semantics , and because each interpretation unambiguously corresponds to one dependency structure but may be obtained from multiple , equivalent , derivations , we furthermore follow Lin 1998 and Carroll , Minnen , and Briscoe 1999 in regarding them as a fairer , and ultimately more useful , standard against which to evaluate the output of parsers trained on CCGbank than the syntactic derivations themselves .", "The Wall Street Journal subcorpus of the Penn Treebank contains about 50 , 000 sentences , or 1 million words , annotated with part of speech tags and phrase structure trees These trees are relatively flat modals and auxiliaries introduce a new VP level , whereas verb modifiers and arguments typically appear all at the same level , as sisters of the main verb .", "A similarly flat annotation style is adopted at the sentence level .", "NPs are flat as well , with all complex modifiers appearing at the same NP level , and compound nouns typically lacking any internal structure .", "The translation algorithm needs to identify syntactic heads , and has to distinguish between complements and modifiers .", "In the Treebank , this information is not explicit .", "Although some non terminal nodes carry additional function tags , such as SBJ subject or TMP temporal modifier , truly problematic cases such as prepositional phrases are often marked with tags such as CLR closely related or DIR direction , which are not always reliable or consistent indicators that a constituent is a modifier or an argument .", "The Treebank uses various types of null elements and traces to encode non local dependencies .", "These are essential for our algorithm since they make it possible to obtain correct CCG derivations for relative clauses , wh questions , and coordinate constructions such as right node raising .", "Their treatment is discussed in Sections 6 . 2 and 6 . 3 .", "In order to obtain CCG derivations from the Penn Treebank , we need to define a mapping from phrase structure trees to CCG derivations , including a treatment of the null elements in the Treebank .", "We also need to modify the Treebank where its syntactic analyses differ from CCG , and clean up certain sources of noise that would otherwise result in incorrect CCG derivations .", "We will begin by ignoring null elements , and assume that Penn Treebank trees are entirely consistent with CCG analyses .", "The basic algorithm then consists of four steps Similar algorithms for phrase structure trees without traces or other null elements have been suggested by Buszkowski and Penn 1990 and Osborne and Briscoe 1998 .", "We illustrate this basic algorithm using the previous example 9 .", "Then we will extend this algorithm to deal with coordination , and introduce a modification to cope with the fact that certain word classes , such as participials , can act as modifiers of a large number of constituent types .", "Section 5 summarizes the most important preprocessing steps that were necessary to obtain the desired CCG analyses from the Treebank trees .", "Section 6 extends this basic algorithm to deal with the null elements in the Treebank .", "First , the constituent type of each node head h , complement c , or adjunct a is determined , using heuristics adapted from Magerman 1994 and Collins 1999 , which take the label of a node and its parent into account . 6 We assume that NP daughters of VPs are complements , unless they carry a function tag such as LOC , DIR , TMP , and so on , but treat all PPs as adjuncts unless they carry the CLR function tag .", "In our example , we therefore treat passing as transitive , even though it should subcategorize for the PP This binarization process inserts dummy nodes into the tree such that all children to the left of the head branch off in a right branching tree , and then all children to the right of the head branch off in a left branching tree . 7 We assign CCG categories to the nodes in this binary tree in the following manner 4 . 3 . 1 The Root Node .", "The category of the root node is determined by the label of the root of the Treebank tree e . g . , VP S NP , S , SINV , SQ S . 8 If the root node has the category S , it typically carries a feature that distinguishes different types of sentences , such as declaratives S dcl , wh questions S wq , yes no questions S q , or fragments S frg .", "In our running example , the root is S dcl , because its Treebank label is S , and its head word , the auxiliary , has the POS tag VBZ .", "4 . 3 . 2 Head and Complement .", "The category of a complement child is defined by a similar mapping from Treebank labels to categories , for example , NP NP , PP PP . 9 The CCG category of the head is a function which takes the category of the complement as argument and returns the category of the parent node .", "The direction of the slash is given by the position of the complement relative to the head The VP that is headed by the main verb passing is a complement of the auxiliary .", "Because the POS tag of passing is VBG , the CCG category of the complement VP is S ng NP present participle and the lexical category of is is therefore S dcl NP S ng NP is just passing the buck to young people Other VP features include to to infinitival , b bare infinitival , S pt past participle , pss passive , or ng present participle .", "4 . 3 . 3 Head and Adjunct .", "According to the Treebank annotation and the assumptions of the algorithm , our example has two VP adjuncts the adverb just , and , because of its DIR function tag , the PP to young people .", "In both cases , the adjunct category depends on the category of the parent , and the category of the head child is copied from the parent Given a parent category C , the category of an adjunct child is a unary functor C' C' if the adjunct child is to the left of the head child a premodifier , or C' C' if it is to the right Function composition reduces the number of lexical categories of adjuncts . of the head a postmodifier .", "In most cases , the category C' is equal to the parent category C without any features such as dcl , ng , and so forth , and the modifier combines with the head via simple function application .", "As shown in Figure 1 , in many cases , a more elegant and general analysis can be obtained if we allow modifiers to compose with the head .", "For example , regularly has the category S NP S NP in sentences such as I visit certain places regularly , because it modifies the verb phrase visit certain places , which has the category S dcl NP .", "But in the corresponding relative clause places that I visit regularly or with heavy NP shift I visit regularly certain places in Europe , regularly modifies visit , that is , a constituent with category S dcl NP NP .", "Without function composition , the category of regularly would have to be S NP NP S NP NP , but crossed composition allows the ordinary category S NP S NP to also work in this case .", "Therefore , if the parent and head category C is of the form X , the algorithm strips off all outermost forward arguments and syntactic features from C to obtain C' .", "Similarly , if C is of the form X , all outermost backward arguments and syntactic features are stripped off from C to obtain C' .", "4 . 3 . 4 Head and Punctuation Mark .", "With the exception of some dashes and parentheses see Section 4 , the category of a punctuation mark is identical to its POS tag , and the head has the same category as its parent .", "4 . 3 . 5 The Final Derivation .", "Figure 2 shows the complete CCG derivation of our example .", "The category assignment procedure corresponds to a top down normal form derivation , which almost always uses function application .", "In the basic case presented here , composition is only used to provide a uniform analysis of adjuncts .", "Long range dependencies represented in the Penn Treebank by traces such as T and RNR require extensions to the basic algorithm , which result in derivations that make use of typeraising , composition , and occasionally substitution rules like those in 5 wherever syntactically necessary .", "We defer explanation of these rules until Section 6 , which presents the constructions that motivate them .", "Finally , we need to obtain the word word dependencies which approximate the underlying predicate argument structure .", "This is done by a bottom up procedure , which simply retraces the steps in the CCG derivation that we have now obtained .", "The CCG derivation with corresponding dependencies and dependency graph for example 9 .", "All categories in CCGbank , including results and arguments of complex categories , are associated with a corresponding list of lexical heads .", "This list can be empty in the case of yet uninstantiated arguments of functor categories , or it can consist of one or more tokens .", "Lexical categories have one lexical head , the word itself for example , He for the first NP , and is for the S dcl NP S b NP .", "All dependencies are defined in terms of the heads of lexical functor categories and of their arguments .", "In order to distinguish the slots filled by different arguments , we number the arguments of complex lexical categories from left to right in the category notation that is , from innermost to outermost argument in a purely applicative derivation , for example , S ng NP1 NP2 , or S b NP1 S to NP 2 NP3 .", "In lexical functor categories such as that of the auxiliary , S dcl NP S b NP , the lexical head of all result categories S dcl NP and S dcl is identical to the lexical head of the entire category i . e . , is .", "But in functor categories that represent modifiers , such as the adverb S NP S NP , the head of the result the modified verb phrase comes from the argument the unmodified verb phrase .", "We use indices on the categories to represent this information S NP i S NP i .", "In CCGbank , modifier categories are easily identified by the fact that they are of the form X X or X X . . . with either or , where X does not have any of the features described previously , such as dcl , b .", "Similarly , determiners the take a noun N , buck as argument to form a non bare noun phrase whose lexical head comes from the noun NP nb i Ni .", "Thus , the lexical head of the noun phrase the buck is buck , not the .", "We also use this coindexation mechanism for lexical categories that project nonlocal dependencies .", "For instance , the category of the auxiliary , S dcl NP S ng NP , mediates a dependency between the subject He and the main verb passing .", "Like all lexical categories of auxiliaries , modals and subject raising verbs , the head of the subject NP is coindexed with the head of subject inside the VP argument S dcl NPi S ng NPi .", "The set of categories that project such dependencies is not acquired automatically , but is given as a list of category templates to the algorithm which creates the actual dependency structures .", "A complete list of the lexical entries in sections 02 21 of the Treebank which use this coindexation mechanism to project nonlocal dependencies is given in the CCGbank manual Hockenmaier and Steedman 2005 .", "We believe that in practice this mechanism is largely correct , even though it is based on the fundamentally flawed assumption that all lexical categories that have the same syntactic type project the same dependencies .", "It may be possible to use the indices on the PRO null elements 1 in the Treebank to identify and resolve ambiguous cases ; we leave this to future research . 10 Function application and composition typically result in the instantiation of the lexical head of an argument of some functor category , and therefore create new dependencies , whereas coordination creates a new category whose lexical head lists are concatenations of the head lists of the conjuncts .", "When the S ng NP1 NP2 passing is combined with the NP the buck , the lexical head of the NP2 is instantiated with buck .", "Similarly , when the adverb just S NP1 S NP 2 is applied to passing the buck , a dependency between just and passing is created However , because S NP1 S NP 2 is a modifier category , the head of the resulting S ng NP is passing , not just and no dependency is established between just and its NP1 .", "In the next step , this S ng NP is combined with the auxiliary S dcl NP1 S ng NP 2 .", "The NP in the S ng NP 2 argument of the auxiliary unifies with the uninstantiated NP1 argument of passing .", "Because the NP in the S ng NP 2 is also coindexed with the subject NP1 of the auxiliary , the NP of the resulting S dcl NP now has two unfilled dependencies to the subject NP1 of is and passing .", "When the entire verb phrase is combined with the subject , He fills both slots Figure 2 shows the resulting CCG derivation and the corresponding list of word word dependencies for our example sentence .", "It is the latter structure that we claim approximates for present purposes the predicate argument structure or interpretation of the sentence , and provides the gold standard against which parsers can be evaluated .", "In order to deal with coordination , both the tree binarization and the category assignment have to be modified .", "In CCGbank , coordination is represented by the following binary rule schemata , rather than the ternary rule 7 compare to Steedman 1989 11 In order to obtain this analysis from Treebank trees , a separate node that spans only the conjuncts and the conjunction or punctuation marks comma , semicolon is inserted if necessary .", "Identifying the conjuncts often requires a considerable amount of preprocessing .", "These trees are then transformed into strictly right branching binary trees .", "The dummy nodes inserted during binarization receive the same category as the conjuncts , but additionally carry a feature conj An additional modification of the grammar is necessary to deal with unlike coordinate phrases UCP , namely , coordinate constructions where the conjuncts do not belong to the same syntactic category Such constructions are difficult for any formalism .", "This phenomenon could be handled elegantly with a feature hierarchy over categories as proposed by Copestake 2002 , Villavicencio 2002 , and McConville 2007 .", "Because the induction of such a hierarchy was beyond the scope of our project , we modify our grammar slightly , and allow the algorithm to use instantiations of a special coordination rule schema , such as This enables us to analyze the previous example as In CCG , all language specific information is associated with the lexical categories of words .", "There are many syntactic regularities associated with word classes , however , which may potentially generate a large number of lexical entries for each item in that class .", "One particularly frequent example of this is clausal adjuncts .", "Figure 3 illustrates how the basic algorithm described above leads to a proliferation of adjunct categories .", "For example , a past participle such as used would receive a different category in a reduced relative like Figure 3 a from its standard category S pss NP S to NP .", "As a consequence , modifiers of used would also receive different categories depending on what occurrence of used they modify .", "This is undesirable , because we are only guaranteed to acquire a complete lexicon if we have seen all participles and their possible modifiers in all their possible surface positions .", "Similar regularities have been recognized and given a categorial analysis by Carpenter 1992 , who advocates lexical rules to account for the use of predicatives as adjuncts .", "In a statistical model , the parameters for such lexical rules are difficult to estimate .", "We therefore follow the approach of Aone and Wittenburg 1990 and implement these type changing Type changing rules reduce the number of lexical category types required for complex adjuncts . operations in the derivational syntax , where these generalizations are captured in a few rules .", "If these rules apply recursively to their own output , they can generate an infinite set of category types , leading to a shift in generative power from context free to recursively enumerable Carpenter 1991 , 1992 .", "Like Aone and Wittenburg , we therefore consider only a finite number of instantiations of these type changing rules , namely those which arise when we extend the category assignment procedure in the following way For any sentential or verb phrase modifier an adjunct with label S or SBAR with null complementizer , or VP to which the original algorithm assigns category X X , apply the following type changing rule given in bottom up notation in reverse where S is the category that this constituent obtains if it is treated like a head node by the basic algorithm .", "S has the appropriate verbal features , and can be S NP or S NP .", "Some of the most common type changing rules are the following , for various types of reduced relative modifier Hockenmaier and Steedman CCGbank In order to obtain the correct predicate argument structure , the heads of corresponding arguments in the input and output category are unified as indicated by coindexation .", "In written English , certain types of NP extraposition require a comma before or after the extraposed noun phrase Factories booked 236 . 74 billion in orders in September , NP nearly the same 18 as the 236 . 79 billion in August Because any predicative noun phrase could be used in this manner , this construction is also potentially problematic for the coverage of our grammar and lexicon .", "However , the fact that a comma is required allows us to use a small number of binary type changing rules which do not project any dependencies , such as", "The translation algorithm presumes that the trees in the Penn Treebank map directly to the desired CCG derivations .", "However , this is not always the case , either because of noise in the Treebank annotation , differences in linguistic analysis , or because CCG , like any other expressive linguistic formalism , requires information that is not present in the Treebank analysis .", "Before translation , a number of preprocessing steps are therefore required .", "Disregarding the most common preprocessing step the insertion of a noun level , which is required in virtually all sentences , preprocessing affects almost 43 of all sentences .", "Here we summarize the most important preprocessing steps for those constructions that do not involve non local dependencies .", "Preprocessing steps required for constructions involving non local dependencies i . e . , traces or null elements in the Treebank are mentioned in Section 6 .", "Remaining problems are discussed in Section 7 .", "More detailed and complete descriptions can be found in the CCGbank manual .", "Annotation errors and inconsistencies in the Treebank affect the quality of any extracted grammar or lexicon .", "This is especially true for formalisms with an extended domain of locality , such as TAG or CCG , where a single elementary tree or lexical category may contain information that is distributed over a number of distinct phrase structure rules .", "Part of Speech Tagging Errors .", "Ratnaparkhi 1996 estimates a POS tagging error rate of 3 in the Treebank .", "The translation algorithm is sensitive to these errors and inconsistencies , because POS tagging errors can lead to incorrect categories or to incorrect features on verbal categories e . g . , when a past participle is wrongly tagged as past tense .", "For instance , if a simple past tense form occurs in a verb phrase which itself is the daughter of a verb phrase whose head is an inflected verb , it is highly likely that it should be a past participle instead .", "Using the verb form itself and the surrounding context , we have attempted to correct such errors automatically .", "In 7 of all sentences , our algorithm modifies at least one POS tag .", "Quotation Marks .", "Although not strictly coming under the heading of noise , quotation marks cause a number of problems for the translation algorithm .", "Although it is tempting to analyze them similarly to parentheticals , quotations often span sentence boundaries , and consequently quotation marks appear to be unbalanced at the sentence level .", "We therefore decided to eliminate them during the preprocessing stage .", "Unlike a hand written grammar , the grammar that is implicit in a treebank has to cover all constructions that occur in the corpus .", "Expressive formalisms such as CCG provide explicit analyses that contain detailed linguistic information .", "For example , CCG derivations assign a lexical head to every constituent and define explicit functor argument relations between constituents .", "In a phrase structure grammar , analyses can be much coarser , and may omit more fine grained structures if they are assumed to be implicit in the given analysis .", "Furthermore , constructions that are difficult to analyze do not need to be given a detailed analysis .", "In both cases , the missing information has to be added before a Treebank tree can be translated into CCG .", "If the missing structure is implicit in the Treebank analysis , this step is relatively straightforward , but constructions such as parentheticals , multiword expressions , and fragments require careful reanalysis in order to avoid lexical coverage problems and overgeneration .", "Detecting Coordination .", "Although the Treebank does not explicitly indicate coordination , it can generally be inferred from the presence of a conjunction .", "However , in list like nominal coordinations , the conjuncts are only separated by commas or semicolons , and may be difficult to distinguish from appositives .", "There are also a number of verb phrase or sentential coordinations in the Treebank where shared arguments or modifiers simply appear at the same level as conjuncts and the conjunction 12 In CCG , the conjuncts and conjunction form a separate constituent .", "In 1 . 8 of all sentences , additional preprocessing is necessary to obtain this structure .", "Noun Phrases and Quantifier Phrases .", "In the Penn Treebank , non recursive noun phrases have remarkably little internal structure NP DT the NNP Dutch VBG publishing NN group 20 Some , but not all , of the structure that is required to obtain a linguistically adequate analysis can be inferred semi automatically .", "The CCGbank grammar distinguishes noun phrases , NP , from nouns , N , and treats determiners the as functions from nouns Hockenmaier and Steedman CCGbank to noun phrases NP nb N .", "Therefore , we need to insert an additional noun level , which also includes the adjuncts Dutch and publishing , which receive both the category N N However , because nominal compounds in the Treebank have no internal bracketing , we always assume a right branching analysis , and are therefore not able to obtain the correct dependencies for cases such as lung cancer deaths .", "QPs quantifier phrases are another type of constituent where the Treebank annotation lacks internal structure We use a number of heuristics to identify the internal structure of these constituents for example , to detect conjuncts and prepositions .", "The above example is then re bracketed Fragments .", "1 . 24 of the sentences in the Penn Treebank correspond to or contain fragmentary utterances labeled FRAG , for which no proper analysis could be given FRAGs are often difficult to analyze , and the annotation is not very consistent .", "The CCGbank manual lists heuristics that we used to infer additional structure .", "For example , if a node is labeled FRAG , and there is only one daughter and potentially an end of sentence punctuation mark , as in the first example , we treat the tree as if it was labeled with the label of its daughter NP in this case .", "Parentheticals .", "Parentheticals are insertions that are often enclosed in parentheses , or preceded by a dash .", "Unless the parenthetical element itself is of a type that could be a modifier by itself e . g . , a PP , we assume that the opening parenthesis or first dash takes the parenthetical element as argument and yields a modifier of the appropriate type NP NP the third highest PP LOC in the developing world This results in the following derivation , which ignores the fact that parentheses are usually balanced Nunberg 1990 the third highest in the developing world We use a similar treatment for other constituents that appear after colons and dashes , such as sentence final appositives , or parentheticals that are not marked as PRN .", "Overall , these changes affect 8 . 7 of all sentences .", "Multi Word Expressions .", "Under the assumption that every constituent has a lexical head that corresponds to an individual orthographic word , multi word expressions require an analysis where one of the items subcategorizes for a specific syntactic type that can only correspond to the other lexical item .", "We only attempted an analysis for expressions that are either very frequent or where the multi word expression has a different subcategorization behavior from the head word of the expression .", "This includes some closed class items described in the CCGbank manual , including connectives e . g . , as if , as though , because of , comparatives so ADJ that , too ADJ to , at least most . . . X , monetary expressions , and dates , affecting 23 . 8 of all sentences .", "Additionally , there are a number of constructions whose Treebank annotation differs from the standard CCG analysis for linguistic reasons .", "This includes small clauses , as well as pied piping , subject extraction from embedded sentences and argument cluster coordination discussed in Section 6 .", "Small Clauses .", "The Treebank treats constructions such as the following as small clauses Pollard and Sag 1992 and Steedman 1996 argue against this analysis on the basis of extractions like what does the country want forgiven , which suggest that these cases should rather be treated as involving two complements .", "We eliminate the small clause , and transform the trees such that the verb takes both NP children of the small clause as complements , thereby obtaining the lexical category S dcl NP NP NP for makes .", "Because our current grammar treats predicative NPs like ordinary NPs , we are not able to express the relationship between it and supplier , or between pool and hostage .", "A correct analysis would assign a functor category S nom NP or perhaps NP prd NP to predicative NP arguments of verbs like makes , not only in these examples , but also in copular sentences and appositives .", "The other case where small clauses are used in the Treebank includes absolute with and though constructions with the limit in effect .", "Here , we also assume that the subordinating conjunction takes the individual constituents in the small clause as complements , and with obtains therefore the category S S PP NP .", "Again , a predicative analysis of the PP might be desirable in order to express the dependencies between limit and in effect .", "Eliminating small clauses affects 8 . 2 of sentences .", "The treatment of non local dependencies is one of the most important points of difference between grammar formalisms .", "The Treebank uses a large inventory of null element types and traces , including coindexation to represent long range dependencies .", "Hockenmaier and Steedman CCGbank Because standard Treebank parsers use probabilistic versions of context free grammar , they are generally trained and tested on a version of the Treebank in which these null elements and indices are deleted or ignored , or , in the case of Collin s 1999 Model 3 , only partially captured .", "Non local dependencies are therefore difficult to recover from their output .", "In CCG , long range dependencies are represented without null elements or traces , and coindexation is restricted to arguments of the same lexical functor category .", "Although this mechanism is less expressive than the potentially unrestricted coindexation used in the Treebank , it allows parsers to recover non anaphoric long range dependencies directly , without the need for further postprocessing or trace insertion .", "Passive .", "In the Treebank , the surface subject of a passive sentence is coindexed with a null element in direct object position Our translation algorithm uses the presence of the null element to identify passive mode , but ignores it otherwise , assigning the CCG category S pss NP to noted . 13 The dependency between the subject and the participial is mediated through the lexical category of the copula , S dcl NPi S pss NPi with the standard semantics apax . px . 14 In order to reduce lexical ambiguity and deal with data sparseness , we treat optional by PPs which contain the logical subject NP LGS as adjuncts rather than arguments of the passive participle . 15 Here is the resulting CCG derivation , together with its dependency structure 13 In the case of verbs like pay for , which take a PP argument , the null element appears within the PP .", "In order to obtain the correct lexical category of paid , S pss NP PP NP , we treat the null element like an argument of the preposition and percolate it up to the PP level .", "14 We assume that the fact that the subject NP argument of passive participials with category S pss NP identifies the patient , rather than agent , is represented in the semantic interpretation of noted , for example , ax . noted'x one , where one is simply a placeholder for a bindable argument , like the relational grammarians ch\u02c6omeur relation .", "15 Extractions such as Who was he paid by require the by PP to be treated as an argument , and it would in fact be better to use a lexical rule to generate S pss NP PP by from S pss NP and vice versa .", "Infinitival and Participial VPs , Gerunds .", "In the Treebank , participial phrases , gerunds , imperatives , and to VP arguments are annotated as sentences with a null subject We treat these like verb phrases S NP with the appropriate feature b , to , ng , or pt , depending on the part of speech tag of the verb .", "Control and Raising .", "CCGbank does not distinguish between control and raising .", "In the Treebank , subject control and subject raising verbs e . g . , want and seem also take an S complement with a null subject that is coindexed with the subject of the main clause We ignore the coindexation in the Treebank , and treat all control verbs as non arbitrary control .", "As indicated by the index i , we assume that all verbs which subcategorize for a verb phrase complement and take no direct object mediate a dependency between their subject and their complement .", "Because the copula and to mediate similar dependencies between their subjects and complements , but do not fill their own subject dependencies , Japanese has the following dependencies In the Treebank , object raising verbs wants half the debt forgiven take a small clause argument with non empty subject .", "Following our treatment of small clauses see Section 5 . 3 we modify this tree so that we obtain the lexical category S dcl NP S pss NPi NPi for wanted , which mediates the dependency between debt and forgiven . 16 Extraposition of Appositives .", "Appositive noun phrases can be extraposed out of a sentence or verb phrase , resulting in an anaphoric dependency .", "The Penn Treebank analyzes these as adverbial small clauses with a coindexed null subject We also treat these appositives as sentential modifiers .", "However , the corresponding CCG derivation deliberately omits the dependency between dummies and drivers 17 This derivation uses one of the special binary type changing rules see Section 4 . 6 that takes into account that these appositives can only occur adjacent to commas .", "The Penn Treebank analyzes wh questions , relative clauses , topicalization of complements , tough movement , cleft , and parasitic gaps in terms of movement .", "These constructions are frequent The entire Treebank contains 16 , 056 T traces , including 8 , 877 NP traces , 4 , 120 S traces , 2 , 465 ADVP traces , 422 PP traces , and 210 other T traces .", "Sections 02 21 39 , 604 sentences contain 5 , 288 full subject relative clauses , as well as 459 full and 873 reduced object relative clauses .", "The dependencies involved in these constructions , however , are difficult to obtain from the output of standard parsers such as Collins 1999 or Charniak 2000 , and require additional postprocessing that may introduce further noise and errors .", "In those cases where the trace corresponds to a moved argument , the corresponding long range dependencies can be recovered directly from the correct CCG derivation .", "In the Treebank , the moved constituent is coindexed with a trace T , which is inserted at the extraction site 17 We regard this type of dependency as anaphoric rather than syntactic , on the basis of its immunity to such syntactic restrictions as subject islands .", "CCG has a similarly uniform analysis of these constructions , albeit one that does not require syntactic movement .", "In the CCG derivation of the example , the relative pronoun has the category NPi NPi S dcl NPi whereas the verb bought just bears the standard transitive category S dcl NP NP .", "The subject NP and the incomplete VP combine via type raising and forward composition into an S dcl NP , which the relative pronoun then takes as its argument The coindexation on the lexical category of the relative pronoun guarantees that the missing object unifies with the modified NP , and we obtain the desired dependencies This analysis of movement in terms of functors over incomplete constituents allows CCG to use the same category for the verb when its arguments are extracted as when they are in situ .", "This includes not only relative clauses and wh questions , but also piedpiping , tough movement , topicalization , and clefts .", "For our translation algorithm , the T traces are essential They indicate the presence of a long range dependency for a particular argument of the verb , and allow us to use a mechanism similar to GPSG s slash feature passing Gazdar et al . 1985 , so that long range dependencies are represented in the gold standard dependency structures of the test and training data .", "This is crucial to correctly inducing and evaluating grammars and parsers for any expressive formalism , including TAG , GPSG , HPSG , LFG , and MPG .", "A detailed description of this mechanism and of our treatment of other constructions that use T traces can be found in the CCGbank manual .", "This algorithm works also if there is a coordinate structure within the relative clause such that there are two T traces the interest rates they pay T on their deposits and charge T on their loans , resulting in the following long range dependencies that the verb takes the VP and the NP argument in reversed order and change the tree accordingly before translation , resulting in the correct CCG analysis We obtain the following long range dependencies Because our grammar does not use Baldridge s 2002 modalities or Steedman s 1996 equivalent rule based restrictions , which prohibit this category from applying to in situ NPs , this may lead to overgeneralization .", "However , such examples are relatively frequent There are 97 instances of S .", "NP NP S dcl NP in sections 02 21 , and to omit this category would reduce coverage and recovery of long range extractions .", "By percolating the T trace up to the SQ level in a similar way to relative clauses and treating Which as syntactic head of the WHNP , we obtain the desired CCG analysis We coindex the head of the extracted NP with that of the noun cars S wq S q NPi Ni , and the subject of do with the subject of its complement S q S b NP1 NPi to obtain the following dependencies In this example , we need to rebracket the Treebank tree so that details of forms a constituent , 18 apply a special rule to assign the category NP NP NP to the preposition , and combine it via type raising and composition with details .", "This constituent is then treated as an argument of the relative pronoun With appropriate coindexation NP NPi S dcl NPj NP NPi j , we obtain the following non local dependencies 19 Because adjuncts generally do not extract unboundedly , 20 the corresponding traces which account for 20 of all T traces can be ignored by the translation procedure .", "Instead , the dependency between when and dropped is directly established by the fact that dropped is the head of the complement S dcl wh extraction , which use the same lexical categories as for in situ complements , they also provide an analysis of right node raising constructions without introducing any new lexical categories .", "In the Treebank analysis of right node raising , the shared constituent is coindexed with two RNR traces in both of its canonical positions We need to alter the translation algorithm slightly to deal with RNR traces in a manner essentially equivalent to the earlier treatment of T wh traces .", "Details are in the CCGbank manual .", "The CCG derivation for the above example is as follows The right node raising dependencies are as follows Our algorithm works also if the shared constituent is an adjunct , or if two conjoined noun phrases share the same head , which is also annotated with RNR traces .", "Although there are only 209 sentences with RNR traces in the entire Treebank , right node raising is actually far more frequent , because RNR traces are not used when the conjuncts consist of single verb tokens .", "The Treebank contains 349 VPs in which a verb form VB is immediately followed by a conjunction CC and another verb form , and has an NP sister without any coindexation or function tag .", "In CCGbank , sections 02 21 alone contain 444 sentences with verbal or adjectival right node raising .", "Right node raising is also marked in the Penn Treebank using RNR traces for parasitic gap constructions such as the following These sentences require rules based on the substitution combinator S Steedman 1996 .", "Our treatment of right node raising traces deals with the first case correctly , via the backward crossing rule S , and allows us to obtain the following correct dependencies The second type of parasitic gap , 44b , would be handled equally correctly by the forward substitution rule S , since the PPs are both arguments .", "Unfortunately , as we saw in Section 3 , the Treebank classifies such PPs as directional adverbials , hence we translate them as adjuncts and lose such examples , of which there are at least three more , all also involving from and to As in the case of leftward extraction , including such long range dependencies in the dependency structure is crucial to correct induction and evaluation of all expressive grammar formalisms .", "Although no leftward extracting parasitic gaps appear to occur in the Treebank , our grammar and model predicts examples like the following , and will cover them when encountered Conflict which the system was held to cause , rather than resolve .", "47 6 . 4 . 1 Argument Cluster Coordination .", "If two VPs with the same head are conjoined , the second verb can be omitted .", "The Treebank encodes these constructions as a VPcoordination in which the second VP lacks a verb .", "The daughters of the second conjunct are coindexed with the corresponding elements in the first conjunct using a index In the CCG account of this construction , 5 million right away and additional amounts in the future form constituents argument clusters , which are then coordinated .", "These constituents are obtained by type raising and composing the arguments in each conjunct , yielding a functor which takes a verb with the appropriate category to its left to yield a verb phrase Dowty 1988 ; Steedman 1985 .", "Then the argument clusters are conjoined , and combine with the verb via function application 21 This construction is one in which the CCGbank head dependency structure shown subsequently fails to capture the full set of predicate argument structure relations that would be implicit in a full logical form That is , the dependency structure does not express the fact that right away takes scope over 5 million and in future over additional amounts , rather than the other way around .", "However , this information is included in the full surface compositional semantic interpretation that is built by the combinatory rules .", "Because the Treebank constituent structure does not correspond to the CCG analysis , we need to transform the tree before we can translate it .", "During preprocessing , we create a copy of the entire argument cluster which corresponds to the constituent structure of the CCG analysis .", "During normal category assignment , we use the first conjunct in its original form to obtain the correct categories of all constituents .", "In a later stage , we use type raising and composition to combine the constituents within each argument cluster .", "For a detailed description of this algorithm and a number of variations on the original Treebank annotation that we did not attempt to deal with , the interested reader is referred to the CCGbank manual .", "There are 226 instances of argument cluster coordination in the entire Penn Treebank .", "The algorithm delivers a correct CCG derivation for 146 of these .", "Translation failures are due to the fact that the algorithm can at present only deal with this construction if the two conjuncts are isomorphic in structure , which is not always the case .", "This is unfortunate , because CCG is particularly suited for this construction .", "However , we believe that it would be easier to manually reannotate those sentences that are not at present translated than to try to adapt the algorithm to deal with all of them individually .", "PP CLR 2 to the general manager This construction cannot be handled with the standard combinatory rules of CCG that are assumed for English .", "Instead , Steedman 2000 proposes an analysis of gapping that uses a unification based decomposition rule .", "Categorial decomposition allows a category type to be split apart into two subparts , and is used to yield an analysis of gapping that is very similar to that of argument cluster coordination 22 22 It is only the syntactic types that are decomposed or recovered in this way the corresponding semantic entities and in particular the interpretation for the gapped verb group can talk must be available from the left conjunct s information structure , via anaphora .", "That is , decomposition adds very little to the categorial information available from the right conjunct , except to make the syntactic types yield an S . The real work is done in the semantics .", "Because the derivation is not a tree anymore , and the decomposed constituents do not correspond to actual constituents in the surface string , this analysis is difficult to represent in a treebank .", "The 107 sentences that contain sentential gapping are therefore omitted in the current version of CCGbank , even though special coordination rules that mimic the decomposition analysis are conceivable .", "Besides the cases discussed herein , the Treebank contains further kinds of null elements , all of which the algorithm ignores .", "The null element ICH Insert Constituent Here , which appears 1 , 240 times , is used for extraposition of modifiers .", "Like ellipsis , this is a case of a semantic dependency which we believe to be anaphoric , and therefore not reflected in the syntactic category .", "For this reason we treat any constituent that is coindexed with an ICH as an adjunct .", "The null element PPA Permanent Predictable Ambiguity , 26 occurrences is used for genuine attachment ambiguities .", "Since the Treebank manual states that the actual constituent should be attached at the more likely attachment site , we chose to ignore any PPA null element .", "Our algorithm also ignores the null element ?", ", which occurs 582 times , and indicates a missing predicate or a piece thereof Marcus , Santorini , and Marcinkiewicz 1993 .", "It is used for VP ellipsis , and can also occur in conjunction with a VP pro form do You either believe he can do it or you don t ?", ", or in comparatives the total was far higher than expected ?", ". 23 We can now define the complete translation algorithm , including the modifications necessary to deal with traces and argument clusters 23 We believe that both conjuncts in the first example are complete sentences which are related anaphorically .", "Therefore , the syntactic category of do is S dcl NP , not S dcl NP VP .", "In the second example , ?", "indicates a semantic argument of expected that we do not reflect in the syntactic category .", "The successive steps have the following more detailed character preprocessTree Correct tagging errors , ensure the constituent structure conforms to the CCG analysis .", "Eliminate quotes .", "Create copies of coordinated argument clusters that correspond to the CCG analysis . determineConstituentTypes For each node , determine its constituent type head , complement , adjunct , conjunction , a constituent that is coindexed with a RNR trace , spurious null element , or argument cluster . makeBinary Binarize the tree . percolateTraces Determine the CCG category of T and RNR traces in complement position , and percolate them up to the appropriate level in the tree . assignCategories Assign CCG categories to nodes in the tree , starting at the root node .", "Nodes that are coindexed with RNR traces receive the category of the corresponding traces .", "Argument clusters are ignored in this step . treatArgumentClusters Assign categories to argument clusters . cutTracesAndUnaryRules Cut out constituents that are not part of the CCG derivation , such as traces , null elements , and the copy of the first conjunct in argument cluster coordination .", "Eliminate resulting unary projections of the form X X . verifyDerivation Discard those trees for which the algorithm does not produce a valid CCG derivation .", "In most cases , this is due to argument cluster coordination that is not annotated in a way that our algorithm can deal with . assignDependencies coindex specific classes of lexical categories to project non local dependencies , and generate the word word dependencies that constitute the underlying predicate argument structure .", "In a number of cases , missing structure or a necessary distinction between different constructions needed to inform the translation is missing , and cannot be inferred deterministically from the Treebank analysis without further manual re annotation .", "We discuss these residual problems here , because they are likely to present obstacles to the extraction of linguistically adequate grammars in any formalism .", "Our translation algorithm requires a distinction between complements and adjuncts .", "In many cases , this distinction is easily read off the Treebank annotation , but it is in general an open linguistic problem McConnell Ginet 1982 .", "Because the Treebank annotation does not explicitly distinguish between complements and adjuncts , researchers typically develop their own heuristics see , for example , Kinyon and Prolo 2002 .", "For prepositional phrases , we rely on the CLR closely related function tag to identify complements , although it is unclear whether the Treebank annotators were able to use this tag consistently .", "Not all PP arguments seem to have this function tag , and some PPs that have this tag may have been better considered adjuncts For TAG , Chen , Bangalore , and Vijay Shanker 2006 show that different heuristics yield grammars that differ significantly in size , coverage , and linguistic adequacy .", "We have not attempted such an investigation .", "In a future version of CCGbank , it may be possible to follow Shen and Joshi 2005 in using the semantic roles of the Proposition Bank Palmer , Gildea , and Kingsbury 2005 to distinguish arguments and adjuncts .", "Particle verb constructions are difficult to identify in the Treebank , because particles can be found as PRT , ADVP CLR , and ADVP .", "Therefore , verbs in the CCGbank grammar do not subcategorize for particles , which are instead treated as adverbial modifiers .", "Compound nouns are often inherently ambiguous , and in most cases , the Treebank does not specify their internal structure In order to obtain the correct analysis , manual re annotation would be required .", "Because this was not deemed feasible within our project , compound nouns are simply translated into strictly right branching binary trees , which yields the correct analysis in some , but not all , cases .", "This eschews the computational problem that a grammar for compound nouns induces all possible binary bracketings , but is linguistically incorrect .", "A similar problem arises in compound nouns that involve internal coordination We include the following linguistically incorrect rule in our grammar , which yields a default dependency structure corresponding to N N coordination conj N N 56 This rule allows us to translate the above tree as follows N N cotton N N conj and N N fibers The Treebank markup of NP appositives is indistinguishable from that of NP lists Therefore , our current grammar does not distinguish between appositives and NP coordination , even though appositives should be analyzed as predicative modifiers .", "This leads to a reduction of ambiguity in the grammar , but is semantically incorrect Our current grammar does not implement number agreement which is , however , represented in the POS tags .", "One problem that prevented us from including number agreement is the above mentioned inability to distinguish NP lists and appositives .", "In the Penn Treebank , all relative clauses are attached at the noun phrase level .", "This is semantically undesirable , because a correct interpretation of restrictive relative clauses can only be obtained if they modify the noun , whereas non restrictive relative clauses are noun phrase modifiers .", "Because this distinction requires manual inspection on a caseby case basis , we were unable to modify the Treebank analysis .", "Thus , all CCGbank relative pronouns have categories of the form NPi NPi S NPi , rather than Ni Ni S NPi .", "This will make life difficult for those trying to provide a Montague style semantics for relative modifiers .", "Like most other problems that we were not able to overcome , this limitation of the Treebank ultimately reflects the sheer difficulty of providing a consistent and reliable annotation for certain linguistic phenomena , such as modifier scope .", "7 . 7 . 1 Heavy NP Shift .", "In English , noun phrase arguments can be shifted to the end of the sentence if they become too heavy . This construction was studied extensively by Ross 1967 .", "The CCG analysis Steedman 1996 uses backward crossed composition to provide an analysis where brings has its canonical lexical category VP PP NP Because the Penn Treebank does not indicate heavy NP shift , the corresponding CCGbank derivation does not conform to the desired analysis , and requires additional lexical categories which may lead to incorrect overgeneralizations 24 This will also be a problem in using the Penn Treebank or CCGbank for any theory of grammar that treats heavy NP shift as extraction or movement .", "Coverage , Size , and Evaluation Here we first examine briefly the coverage of the translation algorithm on the entire Penn Treebank .", "Then we examine the CCG grammar and lexicon that are obtained from CCGbank .", "Although the grammar of CCG is usually thought of as consisting only of the combinatory rule schemata such as 3 and 5 , we are interested here in the instantiation of these rules , in which the variables X and Y are bound to values such as S and NP , because statistical parsers such as Hockenmaier and Steedman s 2002 or Clark and Curran s 2004 are trained on counts of such instantiations .", "We report our results on sections 02 21 , the standard training set for Penn Treebank parsers , and use section 00 to evaluate coverage of the training set on unseen data .", "Sections 02 21 contains 39 , 604 sentences 929 , 552 words tokens , whereas section 00 consists of 1 , 913 sentences 45 , 422 words tokens .", "CCGbank contains 48 , 934 99 . 44 of the 49 , 208 sentences in the entire Penn Treebank .", "The missing 274 sentences could not be automatically translated to CCG .", "This includes 107 instances of sentential gapping , a construction our algorithm does not cover see Section 6 . 4 . 2 , and 66 instances of non sentential gapping , or argument cluster coordination see Section 6 . 4 . 1 .", "The remaining translation failures include trees that consist of sequences of NPs that are not separated by commas , some fragments , and a small number of constructions involving long range dependencies , such as wh extraction , parasitic gaps , or argument cluster coordinations where the translation did not yield a valid CCG derivation because a complement had been erroneously identified as an adjunct .", "24 Backward crossed composition is also used by Steedman 1996 , 2000 and Baldridge 2002 to account for constraints on preposition stranding in English .", "Because this rule in its unrestricted form leads to overgeneralization , Baldridge restricts crossing rules via the x modality .", "The current version of CCGbank does not implement modalities , but because the grammar that is implicit in CCGbank only consists of particular seen rule instantiations , it may not be affected by such overgeneration problems .", "A CCG lexicon specifies the lexical categories of words , and therefore contains the entire language specific grammar .", "Here , we examine the size and coverage of the lexicon that consists of the word category pairs that occur in CCGbank .", "This lexicon could be used by any CCG parser , although morphological generalization which is beyond the scope of the present paper and ways to treat unknown words are likely to be necessary to obtain a more complete lexicon .", "Number of Entries .", "The lexicon extracted from sections 02 21 has 74 , 669 entries for 44 , 210 word types or 929 , 552 word tokens .", "Many words have only a small number of categories , but because a number of frequent closed class items have a large number of categories see Table 1 , the expected number of lexical categories per token is 19 . 2 .", "Number and Growth of Lexical Category Types .", "How likely is it that we have observed the complete inventory of category types in the English language ?", "There are 1 , 286 lexical category types in sections 02 21 .", "Figure 4 examines the growth of the number of lexical category types as a function of the amount of data translated into CCG .", "The log log plot The growth of lexical category types and rule instantiations sections 02 21 .", "A log log plot of the rank order and frequency of the lexical category types left and instantiations of combinatory rules right in CCGbank . of the rank order and frequency of the lexical categories in Figure 5 indicates that the underlying distribution is roughly Zipfian , with a small number of very frequent categories and a long tail of rare categories .", "We note 439 categories that occur only once , and only 556 categories occur five times or more .", "Inspection suggests that although some of the category types that occur only once are due to noise or annotation errors , most are correct and are in fact required for certain constructions .", "Typical examples of rare but correct and necessary categories are relative pronouns in pied piping constructions , or verbs which take expletive subjects .", "Lexical Coverage on Unseen Data .", "The lexicon extracted from sections 02 21 contains the necessary categories as determined by our translation algorithm for 94 . 0 of all tokens in section 00 42 , 707 out of 45 , 422 .", "The missing entries that would be required for the remaining 6 of tokens fall into two classes 1 , 728 , or 3 . 8 , correspond to completely unknown words that do not appear at all in section 02 21 , whereas the other 2 . 2 of tokens do appear in the training set , but not with the categories required in section 00 .", "All statistical parsers have to be able to accept unknown words in their input , regardless of the underlying grammar formalism .", "Typically , frequency information for rare words in the training data is used to estimate parameters for unknown words and when these rare or unknown words are encountered during parsing , additional information may be obtained from a POS tagger Collins 1997 .", "However , in a lexicalized formalism such as CCG , there is the additional problem of missing lexical entries for known words .", "Because lexical categories play such an essential role in CCG , even a small fraction of missing lexical entries can have a significant effect on coverage , since the parser will not be able to obtain the correct analysis for any sentence that contains such a token .", "Hockenmaier and Steedman 2002 show that this lexical coverage problem does in practice have a significant impact on overall parsing accuracy .", "However , because many of the known words with missing entries do not appear very often in the training data , Hockenmaier 2003a demonstrates that this problem can be partially alleviated if the frequency threshold below which rare words are treated as unseen is set to a much higher value than for standard Treebank parsers .", "An alternative approach , advocated by Clark and Curran 2004 , is to use a supertagger which predicts lexical CCG categories in combination with a discriminative parsing model .", "Size and Growth of Instantiated Syntactic Rule Set .", "Statistical CCG parsers such as Hockenmaier and Steedman 2002 or Clark and Curran 2004 are trained on counts of specific instantiations of combinatory rule schemata by category types .", "It is therefore instructive to consider the frequency distribution of these category instantiated rules .", "The grammar for sections 02 21 has 3 , 262 instantiations of general syntactic combinatory rules like those in 3 with specific categories .", "Of these , 1146 appear only once , and 2 , 027 appear less than five times .", "Although there is some noise , many of the CCG rules that appear only once are linguistically correct and should be used by the parser .", "They include certain instantiations of type raising , coordination , or punctuation rules , or rules involved in argument cluster coordinations , pied piping constructions , or questions , all of which are rare in the Wall Street Journal .", "As can be seen from Figure 5 , the distribution of rule frequencies is again roughly Zipfian , with the 10 most frequent rules accounting for 59 . 2 of all rule instantiations 159 rules account for 95 ; 591 rules for 99 .", "The growth of rule instantiations is shown in Figure 4 .", "If function tags are ignored , the grammar for the corresponding sections of the original Treebank contains 12 , 409 phrase structure rules , out of which 6 , 765 occur only once Collins 1999 .", "These rules also follow a Zipfian distribution Gaizauskas 1995 .", "The fact that both category types and rule instances are also Zipfian for CCGbank , despite its binarized rules , shows that the phenomenon is not just due to the Treebank annotation with its very flat rules .", "Syntactic Rule Coverage on Unseen Data .", "Syntactic rule coverage for unseen data is almost perfect 51 , 932 of the 51 , 984 individual rule instantiations in section 00 corresponding to 844 different rule types have been observed in section 02 21 .", "Out of the 52 missing rule instantiation tokens corresponding to 38 rule types , because one rule appears 13 times in one sentence , six involve coordination , and three punctuation .", "One missing rule is an instance of substitution caused by a parasitic gap .", "Two missing rules are instances of type raised argument types combining with a verb of a rare type .", "This paper has presented an algorithm which translates Penn Treebank phrase structure trees into CCG derivations augmented with word word dependencies that approximate the underlying predicate argument structure .", "In order to eliminate some of the noise in the original annotation and to obtain linguistically adequate derivations that conform to the correct analyses proposed in the literature , considerable preprocessing was necessary .", "Even though certain mismatches between the syntactic annotations in the Penn Treebank and the underlying semantics remain , and will affect any similar attempt to obtain expressive grammars from the Treebank , we believe that CCGbank , the resulting corpus , will be of use to the computational linguistics community in the following ways .", "CCGbank has already enabled the creation of several robust and accurate wide coverage CCG parsers , including Hockenmaier and Steedman 2002 , Clark , Hockenmaier , and Steedman 2002 , Hockenmaier 2003b , and Clark and Curran 2004 , 2007 .", "Although the construction of full logical forms was beyond the scope of this project , CCGbank can also be seen as a resource which may enable the automatic construction of full semantic interpretations by wide coverage parsers .", "Unlike most Penn Treebank parsers , such as Collins 1999 or Charniak 2000 , these CCGbank parsers return not only syntactic derivations , but also local and long range dependencies , including those that arise under relativization and coordination .", "Although these dependencies are only an approximation of the full semantic interpretation that can in principle be obtained from a CCG , they may prove useful for tasks such as summarization and question answering Clark , Steedman , and Curran 2004 .", "Furthermore , Bos et al . 2004 and Bos 2005 have demonstrated that the output of CCGbank parsers can be successfully translated into Kamp and Reyle s 1993 Discourse Representation Theory structures , to support question answering and the textual entailment task Bos and Markert 2005 .", "We hope that these results can be ported to other corpora and other similarly expressive grammar formalisms .", "We also hope that our experiences will be useful in designing guidelines for future treebanks .", "Although implementational details will differ across formalisms , similar problems and questions to those that arose in our work will be encountered in any attempt to extract expressive grammars from annotated corpora .", "Because CCGbank preserves most of the linguistic information in the Treebank in a somewhat less noisy form , we hope that others will find it directly helpful for inducing grammars and statistical parsing models for other linguistically expressive formalisms .", "There are essentially three ways in which this might work .", "For lexicalized grammars , it may in some cases be possible to translate the subcategorization frames in the CCG lexicon directly into the target theory .", "For type logical grammars Moortgat 1988 ; Morrill 1994 ; Moot 2003 , this is little more than a matter of transducing the syntactic types for the lexicon into the appropriate notation .", "For formalisms like LTAG , the relation is more complex , but the work of Joshi and Kulick 1996 , who unfold CCG categories into TAG elementary trees via partial proof trees , and Shen and Joshi 2005 , who define LTAG spines that resemble categories , suggest that this is possible .", "Transduction into HPSG signs is less obvious , but also seems possible in principle .", "A second possibility is to transduce CCGbank itself into a form appropriate to the target formalism .", "There seems to be a similar ordering over alternative formalisms from straightforward to less straightforward for this approach .", "We would also expect that dependency grammars Mel \u02c7cuk and Pertsov 1987 ; Hudson 1984 and parsers McDonald , Crammer , and Pereira 2005 could be trained and tested with little extra work on the dependencies in CCGbank .", "Finally , we believe that existing methods for translating the Penn Treebank from scratch into other grammar formalisms will benefit from including preprocessing similar to that described here .", "As some indication of the relative ease with which these techniques transfer , we offer the observation that the 900K word German Tiger dependency corpus has recently been translated into CCG using very similar techniques by Hockenmaier 2006 , and C ak\u0131c\u0131 2005 has derived a Turkish lexicon from the a similarly preprocessed version of the METU Sabanc \u0131 Turkish dependency treebank Oflazer et al . 2003 .", "A fundamental assumption behind attempts at the automatic translation of syntactically annotated corpora into different grammatical formalisms such as CCG , TAG , HPSG , or LFG is that the analyses that are captured in the original annotation can be mapped directly or , at least , without too much additional work into the desired analyses in the target formalism .", "This can only hold if all constructions that are treated in a similar manner in the original corpus are also treated in a similar manner in the target formalism .", "For the Penn Treebank , our research and the work of others Xia 1999 ; Chen and Vijay Shanker 2004 ; Chiang 2000 ; Cahill et al . 2002 have shown that such a correspondence exists in most cases .", "Although the output of most current Treebank parsers is linguistically impoverished , the Treebank annotation itself is not .", "It is precisely the linguistic richness and detail of the original annotation in particular , the additional information present in the null elements and function tags that are ignored by most other parsers that has made the creation of CCGbank possible .", "The translation process would have been easier if some of the annotation had been more explicit and precise as in the case of VP coordination , where preprocessing was required to identify the conjuncts , or in NP coordination , where we were not able to distinguish NP lists from appositives and consistent most importantly in identifying adjuncts and arguments .", "An important conclusion that follows for the builders of future treebanks is that the tradition established by the Penn Treebank of including all linguistically relevant dependencies should be continued , with if anything even closer adherence to semantically informed linguistic insights into predicate argument structural relations .", "Our results also indicate that corpora of at least the order of magnitude of the Penn Treebank are necessary to obtain grammars and parsers that are sufficiently expressive , robust , and wide in coverage to recover these relations completely .", "We would like to thank our colleagues in Edinburgh and Philadelphia in particular Jason Baldridge , Johan Bos , Stephen Clark , James Curran , Michael White , Mitch Marcus , Ann Bies , Martha Palmer , and Aravind Joshi for numerous conversations and feedback on the corpus .", "We would also like to thank the Linguistic Data Consortium for their help in publishing CCGbank , and the Computational Linguistics reviewers for their extensive comments on earlier versions of this paper .", "We gratefully acknowledge the financial support provided by EPSRC grant GR M96889 .", "JH also acknowledges support by an EPSRC studentship and the Edinburgh Language Technology Group , and by NSF ITR grant 0205456 at the University of Pennsylvania .", "MJS acknowledges support from the Scottish Enterprise Edinburgh Stanford Link NSF IIS 041628 R39058 and EU IST grant PACOPLUS FP6 2004 IST 4 27657 ."], "summary_lines": ["CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank\n", "This article presents an algorithm for translating the Penn Treebank into a corpus of Combinatory Categorial Grammar (CCG) derivations augmented with local and long-range word\u2013word dependencies.\n", "The resulting corpus, CCGbank, includes 99.4% of the sentences in the Penn Treebank.\n", "It is available from the Linguistic Data Consortium, and has been used to train wide coverage statistical parsers that obtain state-of-the-art rates of dependency recovery.\n", "In order to obtain linguistically adequate CCG analyses, and to eliminate noise and inconsistencies in the original annotation, an extensive analysis of the constructions and annotations in the Penn Treebank was called for, and a substantial number of changes to the Treebank were necessary.\n", "We discuss the implications of our findings for the extraction of other linguistically expressive grammars from the Treebank, and for the design of future treebanks.\n", "The CCGbank-style dependency is a directed graph of head-child relations labelled with the head's lexical category and the argument slot filled by the child.\n", "CCGbank is a corpus of CCG derivations that was semiautomatically converted from the Wall Street Journal section of the Penn treebank.\n"]}
{"article_lines": ["Japanese Dependency Analysis Using Cascaded Chunking", "In this paper , we propose a new statistical Japanese dependency parser using a cascaded chunking model .", "Conventional Japanese statistical dependency parsers are mainly based on a probabilistic model , which is not always efficient or scalable .", "We propose a new method that is simple and efficient , since it parses a sentence deterministically only deciding whether the current segment modifies the segment on its immediate right hand side .", "Experiments using the Kyoto University Corpus show that the method outperforms previous systems as well as improves the parsing and training efficiency .", "Dependency analysis has been recognized as a basic process in Japanese sentence analysis , and a number of studies have been proposed .", "Japanese dependency structure is usually defined in terms of the relationship between phrasal units called bunsetsu segments hereafter segments .", "Most of the previous statistical approaches for Japanese dependency analysis Fujio and Matsumoto , 1998 ; Haruno et al . , 1999 ; Uchimoto et al . , 1999 ; Kanayama et al . , 2000 ; Uchimoto et al . , 2000 ; Kudo and Matsumoto , 2000 are based on a probabilistic model consisting of the following two steps .", "First , they estimate modification probabilities , in other words , how probable one segment tends to modify another .", "Second the optimal combination of dependencies is searched from the all candidates dependencies .", "Such a probabilistic model is not always efficient since it needs to calculate the probabilities for all possible dependencies and creates n n 1 2 where n is the number of segments in a sentence training examples per sentence .", "In addition , the probabilistic model assumes that each pairs of dependency structure is independent .", "In this paper , we propose a new Japanese dependency parser which is more efficient and simpler than the probabilistic model , yet performs better in training and testing on the Kyoto University Corpus .", "The method parses a sentence deterministically only deciding whether the current segment modifies segment on its immediate right hand side .", "Moreover , it does not assume the independence constraint between dependencies", "This section describes the general formulation of the probabilistic model for parsing which has been applied to Japanese statistical dependency analysis .", "First of all , we define a sentence as a sequence of segments B b1 , b2 . . . , bm and its syntactic structure as a sequence of dependency patterns D Dep 1 , Dep 2 , . . . , Dep m 1 , where Dep i j means that the segment bi depends on modifies segment bj .", "In this framework , we assume that the dependency sequence D satisfies the following two constraints .", "Statistical dependency analysis is defined as a searching problem for the dependency pattern D that maximizes the conditional probability P D B of the input sequence under the above mentioned constraints .", "If we assume that the dependency probabilities are mutually independent , P D B can be rewritten as modifies bj . fzj is an n dimensional feature vector that represents various kinds of linguistic features related to the segments bz and bj .", "We obtain Dbest argmaxD P D B taking into all the combination of these probabilities .", "Generally , the optimal solution Dbest can be identified by using bottom up parsing algorithm such as CYK algorithm .", "The problem in the dependency structure analysis is how to estimate the dependency probabilities accurately .", "A number of statistical and machine learning approaches , such as Maximum Likelihood estimation Fujio and Matsumoto , 1998 , Decision Trees Haruno et al . , 1999 , Maximum Entropy models Uchimoto et al . , 1999 ; Uchimoto et al . , 2000 ; Kanayama et al . , 2000 , and Support Vector Machines Kudo and Matsumoto , 2000 , have been applied to estimate these probabilities .", "In order to apply a machine learning algorithm to dependency analysis , we have to prepare the positive and negative examples .", "Usually , in a probabilistic model , all possible pairs of segments that are in a dependency relation are used as positive examples , and two segments that appear in a sentence but are not in a dependency relation are used as negative examples .", "Thus , a total of n n 1 2 training examples where n is the number of segments in a sentence must be produced per sentence .", "In the probabilistic model , we have to estimate the probabilities of each dependency relation .", "However , some machine learning algorithms , such as SVMs , cannot estimate these probabilities directly .", "Kudo and Matsumoto 2000 used the sigmoid function to obtain pseudo probabilities in SVMs .", "However , there is no theoretical endorsement for this heuristics .", "Moreover , the probabilistic model is not good in its scalability since it usually requires a total of n n 1 2 training examples per sentence .", "It will be hard to combine the probabilistic model with some machine learning algorithms , such as SVMs , which require a polynomial computational cost on the number of given training examples .", "In this paper , we introduce a new method for Japanese dependency analysis , which does not require the probabilities of dependencies and parses a sentence deterministically .", "The proposed method can be combined with any type of machine learning algorithm that has classification ability .", "The original idea of our method stems from the cascaded chucking method which has been applied in English parsing Abney , 1991 .", "Let us introduce the basic framework of the cascaded chunking parsing method We apply this cascaded chunking parsing technique to Japanese dependency analysis .", "Since Japanese is a head final language , and the chunking can be regarded as the creation of a dependency between two segments , we can simplify the process of Japanese dependency analysis as follows Figure 1 shows an example of the parsing process with the cascaded chunking model .", "The input for the model is the linguistic features related to the modifier and modifiee , and the output from the model is either of the tags D or O .", "In training , the model simulates the parsing algorithm by consulting the correct answer from the training annotated corpus .", "During the training , positive D and negative O examples are collected .", "In testing , the model consults the trained system and parses the input with the cascaded chunking algorithm .", "We think this proposed cascaded chunking model has the following advantages compared with the traditional probabilistic models .", "If we use the CYK algorithm , the probabilistic model requires O n3 parsing time , where n is the number of segments in a sentence . .", "On the other hand , the cascaded chunking model requires O n2 in the worst case when all segments modify the rightmost segment .", "The actual parsing time is usually lower than O n2 , since most of segments modify segment on its immediate right hand side .", "Furthermore , in the cascaded chunking model , the training examples are extracted using the parsing algorithm itself .", "The training examples required for the cascaded chunking model is much smaller than that for the probabilistic model .", "The model reduces the training cost significantly and enables training using larger amounts of annotated corpus .", "No assumption on the independence between dependency relations The probabilistic model assumes that dependency relations are independent .", "However , there are some cases in which one cannot parse a sentence correctly with this assumption .", "For example , coordinate structures cannot be always parsed with the independence constraint .", "The cascaded chunking model parses and estimates relations simultaneously .", "This means that one can use all dependency relations , which have narrower scope than that of the current focusing relation being considered , as feature sets .", "We describe the details in the next section .", "The cascaded chunking model can be combined with any machine learning algorithm that works as a binary classifier , since the cascaded chunking model parses a sentence deterministically only deciding whether or not the current segment modifies the segment on its immediate right hand side .", "Probabilities of dependencies are not always necessary for the cascaded chunking model .", "Linguistic features that are supposed to be effective in Japanese dependency analysis are head words and their parts of speech tags , functional words and inflection forms of the words that appear at the end of segments , distance between two segments , existence of punctuation marks .", "As those are solely defined by the pair of segments , we refer to them as the static features .", "Japanese dependency relations are heavily constrained by such static features since the inflection forms and postpositional particles constrain the dependency relation .", "However , when a sentence is long and there are more than one possible dependency , static features , by themselves cannot determine the correct dependency .", "To cope with this problem , Kudo and Matsumoto 2000 introduced a new type of features called dynamic features , which are created dynamically during the parsing process .", "For example , if some relation is determined , this modification relation may have some influence on other dependency relation .", "Therefore , once a segment has been determined to modify another segment , such information is kept in both of the segments and is added to them as a new feature .", "Specifically , we take the following three types of dynamic features in our experiments .", "He her warm heart be moved A .", "The segments which modify the current candidate modifiee .", "boxes marked with A in Figure 2 B .", "The segments which modify the current candidate modifier .", "boxes marked with B in Figure 2 C . The segment which is modified by the current candidate modifiee .", "boxes marked with C in Figure 2", "Although any kind of machine learning algorithm can be applied to the cascaded chunking model , we use Support Vector Machines Vapnik , 1998 for our experiments because of their state of the art performance and generalization ability .", "SVM is a binary linear classifier trained from the samples , each of which belongs either to positive or negative class as follows x1 , y1 , . . . , xl , yl xi E Rn , yi E 1 , 1 , where xi is a feature vector of the i th sample represented by an n dimensional vector , and yi is the class positive 1 or negative 1 class label of the i th sample .", "SVMs find the optimal separating hyperplane w x b based on the maximal margin strategy .", "The margin can be seen as the distance between the critical examples and the separating hyperplane .", "We omit the details here , the maximal margin strategy can be realized by the following optimization problem", "Furthermore , SVMs have the potential to carry out non linear classifications .", "Though we leave the details to Vapnik , 1998 , the optimization problem can be rewritten into a dual form , where all feature vectors appear as their dot products .", "By simply substituting every dot product of xi and xj in dual form with a Kernel function K xi , xj , SVMs can handle non linear hypotheses .", "Among many kinds of Kernel functions available , we will focus on the dth polynomial kernel K xi , xj xi xj 1 d . Use of d th polynomial kernel functions allows us to build an optimal separating hyperplane which takes into account all combinations of features up to d .", "We used the following two annotated corpora for our experiments .", "This data set consists of the Kyoto University text corpus Version 2 . 0 Kurohashi and Nagao , 1997 .", "We used 7 , 958 sentences from the articles on January 1st to January 7th as training examples , and 1 , 246 sentences from the articles on January 9th as the test data .", "This data set was used in Uchimoto et al . , 1999 ; Uchimoto et al . , 2000 and Kudo and Matsumoto , 2000 .", "In order to investigate the scalability of the cascaded chunking model , we prepared larger data set .", "We used all 38 , 383 sentences of the Kyoto University text corpus Version 3 . 0 .", "The training and test data were generated by a two fold cross validation .", "The feature sets used in our experiments are shown in Table 1 .", "The static features are basically taken from Uchimoto s list Uchimoto et al . , 1999 .", "Head Word HW is the rightmost content word in the segment .", "Functional Word FW is set as follows FW the rightmost functional word , if there is a functional word in the segment FW the rightmost inflection form , if there is a predicate in the segment FW same as the HW , otherwise .", "The static features include the information on existence of brackets , question marks and punctuation marks , etc .", "Besides , there are features that show the relative relation of two segments , such as distance , and existence of brackets , quotation marks and punctuation marks between them .", "For a segment X and its dynamic feature Y where Y is of type A or B , we set the Functional Representation FR feature of X based on the FW of X X FW as follows FR lexical form of X FW if POS of X FW is particle , adverb , adnominal or conjunction FR inflectional form ofX FW ifX FW has an inflectional form .", "FR the POS tag ofX FW , otherwise .", "For a segment X and its dynamic feature C , we set POS tag and POS subcategory of the HW of X .", "All our experiments are carried out on AlphaSever 8400 21164A 500Mhz for training and Linux PentiumIII 1GHz for testing .", "We used a third degree polynomial kernel function , which is exactly the same setting in Kudo and Matsumoto , 2000 .", "Performance on the test data is measured using dependency accuracy and sentence accuracy .", "Dependency accuracy is the percentage of correct dependencies out of all dependency relations .", "Sentence accuracy is the percentage of sentences in which all dependencies are determined correctly .", "The results for the new cascaded chunking model as well as for the previous probabilistic model based on SVMs Kudo and Matsumoto , 2000 are summarized in Table 2 .", "We cannot employ the experiments for the probabilistic model using large dataset , since the data size is too large for our current SVMs learning program to terminate in a realistic time period .", "Even though the number of training examples used for the cascaded chunking model is less than a quarter of that for the probabilistic model , and the used feature set is the same , dependency accuracy and sentence accuracy are improved using the cascaded chunking model 89 . 09 89 . 29 , 46 . 17 47 . 53 .", "The time required for training and parsing are significantly reduced by applying the cascaded chunking model 336h . 8h , 2 . 1sec . 0 . 5sec . .", "As can be seen Table 2 , the cascaded chunking model is more accurate , efficient and scalable than the probabilistic model .", "It is difficult to apply the probabilistic model to the large data set , since it takes no less than 336 hours 2 weeks to carry out the experiments even with the standard data set , and SVMs require quadratic or more computational cost on the number of training examples .", "For the first impression , it may seems natural that higher accuracy is achieved with the probabilistic model , since all candidate dependency relations are used as training examples .", "However , the experimental results show that the cascaded chunking model performs better .", "Here we list what the most significant contributions are and how well the cascaded chunking model behaves compared with the probabilistic model .", "The probabilistic model is trained with all candidate pairs of segments in the training corpus .", "The problem of this training is that exceptional dependency relations may be used as training examples .", "For example , suppose a segment which appears to right hand side of the correct modifiee and has a similar content word , the pair with this segment becomes a negative example .", "However , this is negative because there is a better and correct candidate at a different point in the sentence .", "Therefore , this may not be a true negative example , meaning that this can be positive in other sentences .", "In addition , if a segment is not modified by a modifier because of cross dependency constraints but has a similar content word with correct modifiee , this relation also becomes an exception .", "Actually , we cannot ignore these exceptions , since most segments modify a segment on its immediate right hand side .", "By using all candidates of dependency relation as the training examples , we have committed to a number of exceptions which are hard to be trained upon .", "Looking in particular on a powerful heuristics for dependency structure analysis A segment tends to modify a nearer segment if possible , it will be most important to train whether the current segment modifies the segment on its immediate right hand side .", "The cascaded chunking model is designed along with this heuristics and can remove the exceptional relations which has less potential to improve performance .", "Figure 3 shows the relationship between the size of the training data and the parsing accuracy .", "This figure also shows the accuracy with and without the dynamic features .", "Generally , the results with the dynamic feature set is better than the results without it .", "The dynamic features constantly outperform static features when the size of the training data is large .", "In most cases , the improvements is considerable .", "Table 3 summarizes the performance without some dynamic features .", "From these results , we can conclude that all dynamic features are effective in improving the performance .", "Table 4 summarizes recent results on Japanese dependency analysis .", "Uchimoto et al . 2000 report that using the Kyoto University Corpus for their training and testing , they achieve around 87 . 93 accuracy by building statistical model based on the Maximum Entropy framework .", "They extend the original probabilistic model , which learns only two class ; modify and not modify , to the one that learns three classes ; between , modify and beyond .", "Their model can also avoid the influence of the exceptional dependency relations .", "Using same training and test data , we can achieve accuracy of 89 . 29 .", "The difference is considerable .", "Kanayama et al . 2000 use an HPSG based Japanese grammar to restrict the candidate dependencies .", "Their model uses at most three candidates restricted by the grammar as features ; the nearest , the second nearest , and the farthest from the modifier .", "Thus , their model can take longer context into account , and disambiguate complex dependency relations .", "However , the features are still static , and dynamic features are not used in their model .", "We cannot directly compare their model with ours because they use a different corpus , EDR corpus , which is ten times as large as the corpus we used .", "Nevertheless , they reported an accuracy 88 . 55 , which is worse than our model .", "Haruno et al . 99 report that using the EDR Corpus for their training and testing , they achieve around 85 . 03 accuracy with Decision Tree and Boosting .", "Although Decision Tree can take combinations of features as SVMs , it easily overfits on its own .", "To avoid overfitting , Decision Tree is usually used as an weak learner for Boosting .", "Combining Boosting technique with Decision Tree , the performance may be improved .", "However , Haruno et al . 99 report that the performance with Decision Tree falls down when they added lexical entries with lower frequencies as features even using Boosting .", "We think that Decision Tree requires a careful feature selection for achieving higher accuracy .", "We presented a new Japanese dependency parser using a cascaded chunking model which achieves 90 . 46 accuracy using the Kyoto University Corpus .", "Our model parses a sentence deterministically only deciding whether the current segment modifies the segment on its immediate right hand side .", "Our model outperforms the previous probabilistic model with respect to accuracy and efficiency .", "In addition , we showed that dynamic features significantly contribute to improve the performance ."], "summary_lines": ["Japanese Dependency Analysis Using Cascaded Chunking\n", "In this paper, we propose a new statistical Japanese dependency parser using a cascaded chunking model.\n", "Conventional Japanese statistical dependency parsers are mainly based on a probabilistic model, which is not always efficient or scalable.\n", "We propose a new method that is simple and efficient, since it parses a sentence deterministically only deciding whether the current segment modifies the segment on its immediate right hand side.\n", "Experiments using the Kyoto University Corpus show that the method outperforms previous systems as well as improves the parsing and training efficiency.\n", "Our cascaded chunking model does not require the probabilities of dependencies and parses a sentence deterministically.\n"]}
{"article_lines": ["MITRE DESCRIPTION OF THE ALEMBIC SYSTEM USED FOR MUC 6 John Aberdeen , John Burger , David Day , Lynette Hirschman , Patricia Robinson , and Marc Vilain The MITRE Corporation 202 Burlington Rd .", "Bedford , MA 01730 aberdeen , john , clay , lynette , parann , mbv mitre . org As with several other veteran Muc participants , MITRES Alembic system has undergone a major trans formation in the past two years .", "The genesis of this transformation occurred during a dinner conversation at the last Muc conference , MUC 5 .", "At that time , several of us reluctantly admitted that our major impediment towards improved performance was reliance on then standard linguistic models of syntax .", "We knew we would need an alternative to traditional linguistic grammars , even to the somewhat non traditional categoria l pseudo parser we had in place at the time .", "The problem was , which alternative ?", "The answer came in the form of rule sequences , an approach Eric Brill originally laid out in his work o n part of speech tagging 5 , 7 .", "Rule sequences now underlie all the major processing steps in Alembic .", "part of speech tagging , syntactic analysis , inference , and even some of the set fill processing in the Template Elemen t task TE .", "We have found this approach to provide almost an embarrassment of advantages , speed an d accuracy being the most externally visible benefits .", "In addition , most of our rule sequence processors ar e trainable , typically from small samples .", "The rules acquired in this way also have the characteristic that the y allow one to readily mix hand crafted and machine learned elements .", "We have exploited this opportunity t o apply both machine learned and hand crafted rules extensively , choosing in some instances to run sequence s that were primarily machine learned , and in other cases to run sequences that were entirely crafted by hand .", "ALEMBICS OVERALL ARCHITECTUR E For all the changes that the system has undergone , the coarse architecture of the Muc 6 version ofAlembic is remarkably close to that of its predecessors .", "As illustrated in Fig .", "r , below , processing is still divided int o three main steps a UNIX and c based preprocess , a Lisp based syntactic analysis , and a Lisp based inferenc e phase .", "Beyond these coarse grain similarities , the system diverges significantly from earlier incarnations .", "We replaced our categorial grammar pseudo parser , as suggested above .", "We also redesigned the preprocess fro m the ground up .", "Only the inferential back end of the system is largely unchanged .", "The internal module by module architecture of the current Alembic is illustrated in Fig .", "The central innovation in the system is its approach to syntactic analysis , which is now performed through a sequence of phrase finding rules that are processed by a simple interpreter .", "The interpreter has somewhat less recognition power than a finite state machine , and operates by successively relabeling the input according t o the rule actions ? more on this below .", "In support of the syntactic phrase finder , or phraser as we call it , the input text must be tagged for part of speech .", "This part of speech tagging is the principal role of the UNIX preprocess , and it is itself supported by a number of pretaggers e . g . , for labeling dates and title words and zoners e . g . , for word tokenization , sentence boundary determination and headline segmentation .", "The phrases that are parsed by the phraser are subsequently mapped to facts in the inferential database , a mapping mediated by a simple semantic interpreter .", "We then exploit inference to instantiate domai n Syntactic analysis Lisp Tractable inference Lisp IL u u NE markup TE templates 5T templates Figure 1 Coarse grained system architecture .", "UNIX preprocess 141 Template Printing gazetteer Figure 2 Processing modules in Alembic .", "constraints and resolve restricted classes of coreference .", "The inference system also supports equality reasonin g by congruence closure , and this equality machinery is in turn exploited to perform TE specific processing , i n particular , acronym and alias merging .", "Finally , the template generation module forms the final TE and S T output by a roughly one to one mapping from facts in the inferential database to templates .", "THE PREPROCESSORS As noted above , the UNIX based portion of the system is primarily responsible for part of speech tagging .", "Prior to the part of speech tagger , however , a text to be processed by Alembic passes through severa l preprocess stages ; each preprocessor enriches the text by means of SGML tags .", "All of these preprocess components are implemented with LEX the lexical analyzer generator and are very fast .", "An initial preprocessor , the punctoker , makes decisions about word boundaries that are not coincident with whitespace .", "It tokenizes abbreviations e . g . , Dr .", ", and decides when sequences of punctuation and alphabeti c characters are to be broken up into several lexemes e . g . , Singapore based .", "The punctoker wraps LEX tags around text where necessary to indicate its decisions , as in the following Singapore L EX pos JJ based LEX As this example suggests , in some cases , the punctoker guides subsequent part of speech tagging by addin g a part of speech attribute to the LEX tags that it emits .", "The parasenter zones text for paragraph and sentence boundaries , the former being unnecessary for Muc 6 .", "The sentence tagging component is both simple and conservative .", "If any end of sentence punctuation has no t been explained by the punctoker as part of a lexeme , as in abbreviations , it is taken to indicate a sentenc e boundary .", "The parasenter is also intended to filter lines in the text body that begin with ?", "but see our error analysis below .", "A separate hl taggeris invoked to zone sentence like constructs in the headline field .", "The preprocess includes specialized phrase taggers .", "The title tagger marks personal titles , making distinc tions along the lines drawn by the NE and ST tasks .", "Included are personal honorifics Dr . , Ms .", "; military an d religious titles Vicar , Sgt .", "; corporate posts CEO , chairman ; and profession words analyst , spokesperson .", "The date tagger identifies TIMEX phrases .", "It uses a lex based scanner as a front end for tokenizing and typing its input ; then a pattern matching engine finds the actual date phrases .", "The date tagger is fast , sinc e the pattern matcher itself is highly optimized , and since the lex based front end does not actually tokenize th e input or fire the pattern matcher unless it suspects that a date phrase may be occurring in the text .", "TE Processin g Acronyms Aliases H UNIX pre process Zoning , Pre tagging , fart of speech tagging t Phraser NE Rules TE Rules CorpNP Rules 5 , T Rule_ 142 Both the date and title tagger can tag a phrase as either I a single SGML element , or 2 individual lexemes , with special attributes that indicate the beginning and end of the matrix phrase , as i n LEX post start chief LEX LEX post mid executive LEX LEX post end officer LEX We adopted this LEX based phrase encoding so as to simplify and speed up the input scanner of the part of speech tagger .", "In addition , a phrases LEX tags can encode parts of speech to help guide the p o s tagger .", "THE PART OF SPEECH TAGGER Our part of speech tagger is closest among the components of our Muc 6 system to Brills original work o n rule sequences S , 6 , 7 .", "The tagger is in fact a re implementation of Brills widely disseminated system , wit h various speed and maintainability improvements .", "Most of the rule sequences that drive the tagger were automatically learned from hand tagged corpora , rather than hand crafted by human engineers .", "However , the rules are in a human understandable form , and thus hand crafted rules can easily be combined with automatically learned rules , a property which we exploited in the Muc 6 version of Alembic .", "The tagger operates on text that has been lexicalized through pre processing .", "The following , for example , is how a sample walkthrough sentence is passed to the part of speech tagger .", "Note how punctuation has bee n tokenized , and Mr . has been identified as a title and assigned the part of speech NNP proper noun .", "5 Even so lex , lex LEX pos NNP ttl WHOLE Mr . LEX Dooner is on the prowl for more creative talen t and is interested in acquiring a hot agency lex . lex 5 The part of speech tagger first assigns initial parts of speech by consulting a large lexicon .", "The lexicon maps words to their most frequently occurring tag in the training corpus .", "Words that do not appear in th e lexicon are assigned a default tag of NN common noun or NNP proper noun , depending on capitalization .", "For unknown words , after a default tag is assigned , lexical rules apply to improve the initial guess .", "These rules operate principally by inspecting the morphology of words .", "For example , an early rule in the lexical rul e sequence retags unknown words ending in ly with the 10 tag adverb .", "In the sentence above , the only unknown word Dooner is not subject to retagging by lexical rules ; in fact , the default NNP tag assignment i s correct .", "Lexical rules play a larger role when the default tagging lexicon is less complete than our own , which we generated from the whole Brown Corpus plus 3 million words of Wall Street Journal text .", "For example , in our experiments tagging Spanish texts for which we had much smaller lexica , we have found that lexica l rules play a larger role this can also be partially attributed to the more inflected nature of Spanish .", "After the initial tagging , contextual rules apply in an attempt to further fix errors in the tagging .", "These rules reassign a words tag on the basis of neighboring words and their tags .", "In this sentence , more changes from its initial JJR comparative adjective to RBR comparative adverb .", "Note that this change is arguably erroneous , depending on how one reads the scope of more .", "This tagging is changed by the following rule , which roughly reads change word W from JJR to ROR if the the word to Ws immediate right is tagged JJ JJR RBR nexttag JJ Table 1 , below , illustrates the tagging process .", "The sample sentence is on the first line ; its initial lexicon based tagging is on the second line ; the third line shows the final tagging produced by the contextual rules .", "In controlled experiments , we measured the taggers accuracy on Wall Street Journal text at 95 . 1 based on a training set of140 , 000 words .", "The production version of the tagger , which we used for Muc 6 , relies on the Even so .", "Mr . Dooner Is on the prowl for more creative talent and Is Interested In acquiring a hot agency rb rb nnp NNP vbz In dt nn in JJR jj nn cc vbz jj In vim dt jj n n rb rb nnp NNP vbz In dt nn in RBR jj nn cc vbz jj In vim dt jj n n Table 1 Tagging a text with the lexicon line 2 and contextual rules line 3 .", "Note the defaul t lexicon assignment of nnp to Dooner and the rule based correction of more .", "143 learned rules from Brills release 1 . 1 148 lexical rules , 283 contextual rules , for which Brill has measure d accuracies that are 2 3 percentage points higher than in our own smaller scale experiments .", "For MUC 6 , we combined these rules with 19 hand crafted contextual rules that correct residual tagging errors that were especially detrimental to our NE performance .", "Tagger throughput is around 3000 words sec .", "THE PHRASER The Alembic phrase finder , or phraser for short , performs the bulk of the systems syntactic analysis .", "As noted above , it has somewhat less recognition power than a finite state machine , and as such shares many characteristics of pattern matching systems , such as CIRCUS 10 or FASTUS 2 .", "Where it differs from these systems is in being driven by rule sequences .", "We have experimented with both automatically learned rul e sequences and hand crafted ones .", "In the system we fielded for Muc 6 , we ended up running entirely with hand crafted sequences , as they outperformed the automatically learned rules .", "How the phraser works The phraser process operates in several steps .", "First , a set of initial phrasing functions is applied to all of the sentences to be analyzed .", "These functions are responsible for seeding the sentences with likely candidate phrases of various kinds .", "This seeding process is driven by word lists , part of speech information , and pre taggings provided by the preprocessors .", "Initial phrasing produces a number of phrase structures , many o f which have the initial null labeling none , while some have been assigned an initial label e . g . , num .", "The following example shows a sample sentence from the walkthrough message after initial phrasing .", "Yesterday , none McCann none made official what had been widely anticipated ttl Mr . ttl none James none , num 57 num years old , is stepping down as post chief executive officer post o n date July 1 date and will retire as post chairman post at the end of the year .", "The post , ttl , and date phrases were identified by the title and date taggers .", "Mr . James num tagged age is identified on the basis of part of speech information , as is the organization name McCann .", "Once the initial phrasing has taken place , the phraser proceeds with phrase identification proper .", "This is driven by a sequence of phrase finding rules .", "Each rule in the sequence is applied in turn against all of the phrases in all the sentences under analysis .", "If the antecedents of the rule are satisfied by a phrase , then th e action indicated by the rule is executed immediately .", "The action can either change the label of the satisfyin g phrase , grow its boundaries , or create new phrases .", "After the nth rule is applied in this way against every phrase in all the sentences , the n lth rule is applied in the same way , until all rules have been applied .", "After all of the rules have been applied , the phraser is done .", "It is important to note that the search strategy in the phraser differs significantly from that in standar d parsers .", "In standard parsing , one searches for any and all rules whose antecedents might apply given the stat e of the parsers chart all these rules become candidates for application , and indeed they all are applie d modulo higher order search control .", "In our phraser , only the current rule in a rule sequence is tested the rule is applied wherever this test succeeds , and the rule is never revisited at any subsequent stage of processing .", "After the final rule of a sequence is run , no further processing occurs .", "The language of phraser rule s The language of the phraser rules is as simple as their control strategy .", "Rules can test lexemes to the left and right of the phrase , or they can look at the lexemes in the phrase .", "Tests in turn can be part of speech queries , literal lexeme matches , tests for the presence of neighboring phrases , or the application of predicates that are evaluated by invoking a Lisp procedure .", "There are several reasons for keeping this rule languag e simple .", "In the case of hand crafted rules , it facilitates the process of designing a rule sequence .", "In the case of machine learned rules , it restricts the size of the search space on each epoch of the learning regimen , thus making it tractable .", "In either case , the overall processing power derives as much from the fact that the rule s are sequenced , and feed each other in turn , as it does from the expressiveness of the rule language .", "144 To make this clearer , consider a simple named entity rule that Is applied to identifying persons .", "clef phraser label none left 1 phrase ttl label action person This rule changes the label of a phrase from none to person if the phrase is bordered on its left by a ttl phrase .", "On the sample sentence , this rule causes the following relabeling of the phrase around James .", "Yesterday , none McCann none made official what had been widely anticipated ttl Mr . ttl person James person , num 57 num years old , is stepping down as post chief executiv e officer post on date July 1 date and will retire as post chairman post at the end of the year .", "Once this rule has run , the labelings it instantiates become available as input to subsequent rules in th e sequence , e . g . , rules that attach the title to the person in Mr . James , that attach the age apposition , and s o forth .", "Phraser rules do make mistakes , but as with other sequence based processors , the phraser applies later rules in a sequence to patch errors made by earlier rules .", "In the walkthrough message , for example , Amarati Purls is identified as an organization , which ultimately leads to an incorrect org tag for Martin Purls , since this persons name shares a common substring with the organization name .", "However , rules that find personal names occur later in our named entities sequence than those which find organizations , thus allowing th e phraser to correctly relabel Martin Purls as a person on the basis of a test for common first names .", "Rule sequences for MUG6 For MUC 6 , Alembic relies on three sequences of phraser rules , divided roughly into rules for generatin g NE specific phrases , those for finding TE related phrases , and those for ST phrases .", "The division is only rough , as the NE sequence yields some number of TE related phrases as a side effect of searching for named entities .", "To illustrate this process , consider the following walkthrough sentence , as tagged by the NE rule sequence .", "But the bragging rights to org Coke org s ubiquitous advertising belongs to org Creative Artists Agency org , the big location Hollywood location talent agency .", "The org label on Creative Artists Agency was set by a predicate that tests for org keywords like Agency .", "Coke was found to be an org elsewhere in the document , and the label was then percolated .", "Finally , the location label on Hollywood was set by a predicate that inspects the tried and not so true TIPSTER gazetteer .", "What is important to note about these NE phraser rules is that they do not rely on a large database o f known company names .", "Instead , the rules are designed to recognize organization names in almost complet e absence of any information about particular organization names with the sole exception of a few acronyms such as IBM , GM , etc .", "This seems to auger well for the ability to apply Alembic to different application tasks .", "Proceeding beyond named entities , the phraser next applies its TE specific rule sequence .", "This sequence performs manipulations that resemble NP parsing , e . g . , attaching locational modifiers .", "In addition , a subsequence of TE rules concentrates on recognizing potential organization descriptors .", "These rules generate so called corpnp phrases , that is noun phrases that are headed by an organizational common noun such a s agency , maker , and of course company .", "The rules expand these heads leftwards to incorporate lexemes that satisfy a set of part of speech constraints .", "One such phrase , for example , is in the sample sentence above .", "But the bragging rights to org Coke org s ubiquitous advertising belongs to org org Creative Artist s Agency org , corpnp the big location Hollywood location talent agency corpnp org After corpnp phrases have been marked , another collection of TE rules associates these phrases with neigh boring org phrases .", "In this case such a phrase is found two places to the left on the other side of a comma , so a new org phrase is created which spans both the original org phrase and its corpnp neighbor .", "Note that these rule sequences encode a semantic grammar .", "Organizationally headed noun phrases are labeled as org , regardless of whether they are simple proper names or more complex constituents such as th e 145 TOTAL SLOT SCORES SLOT POS ACTT COR PAR INCI SPU MIS NONI REC PRE UND OVG ERR SU B enamex 938 9911 881 0 01 110 57 01 94 89 6 11 16 0 type 938 9911 775 0 1061 110 57 01 83 78 6 11 26 1 2 text 938 9911 840 0 411 110 57 01 90 85 6 11 20 5 subto 1876 19821 1615 0 1471 220 114 01 86 81 6 11 23 8 ALL OB 2286 24061 1993 0 1631 250 130 01 87 83 6 10 21 8 MATCHD 2156 21561 1993 0 1631 0 0 01 92 92 0 0 8 8 P R 2P R P 2R F MEASURES 84 . 95 83 . 67 86 . 2 8 TASK SUBCATEGORIZATION SCORES SLOT POS ACTT COR PAR INC SPU MIS NONI REC PRE UND OVG ERR SUB Enamex organi 454 4931 392 0 281 73 34 0I 86 80 7 15 26 7 person 373 3641 292 0 601 12 21 0I 78 80 6 3 24 17 locati 111 1341 91 0 181 25 2 0I 82 68 2 19 33 1 6 Figure4 Performance of rules learned for theENAMEXportion of theNEtask unofficialscore org corpnp apposition above .", "This semantic characteristic of the phraser grammar is clearer still with ST rules .", "These rules are responsible for finding phrases denoting events relevant to the MUC 6 scenario templates .", "For the succession scenario , this consists of a few key phrase types , most salient among them job a post at an org , job in and job out fully specified successions and post in and post out partially specified successions .", "The following example shows the ST phrases parsed out of a key sentence from the walkthrough message .", "Yesterday , person McCann person made official what had been widely anticipated post out person person ttl Mr .", "ttl person James person person , age num 57 num years old age person , is stepping down a s post chief executive officer post post out .", "The post out phrase encodes the resignation of a person in a post .", "Note that in the formal evaluation we failed to find a more correct job out phrase , which should have included McCann .", "This happened because we did not successfully identify McCann as an organization , thus precluding the formation of the job out phrase .", "Learning Phrase Rules We have applied the same general error reduction learning approach that Brill designed for generating part of speech rules to the problem of learning phraser rules in support of the NE task .", "The official version of Alembic for MUC 6 did not use any of the rule sequences generated by this phrase rule learner , but we hav e since generated unofficial scores .", "In these runs we used phrase rules that had been learned for the ENAMEX expressions only ? we still used the hand coded pre processors and phraser rules for recognizing TIMEX and NUMEX phrases .", "Our performance on this task is shown in Fig .", "These rules yield six fewer points o f P R than the hand coded ENAMEX rules ? still an impressive result for machine learned rules .", "Interestingly , the bulk of the additional error in the machine learned rules is not with the hard organization names , but with person names OR If , LP 14 and locations AR I2 , AP 18 .", "1 We put about one staff week of work into the sT task , during which we experienced steep hill climbing on the training set .", "Never theless , we felt that the maturity of our sT processing was sufficiently questionable to preclude participating in the official evaluation .", "The present discussion should be taken in this light , i . e . , with the understanding that it was not officially evaluated atMuc 6 .", "146 PHRASE INTERPRETATION AND INFERENC E The inference component is central to all processing beyond phrase identification .", "It has three roles .", "As a representational substrate , it records propositions encoding the semantics of parsed phrases ; ?", "As an equational system , it allows initially distinct semantic individuals to be equated to each other , an d allows propositions about these individuals to be merged through congruence closure .", "As a limited inference system , it allows domain specific and general constraints to be instantiate d through carefully controlled forward chaining .", "Phrase Interpretation Facts enter the propositional database as the result of phrase interpretation .", "The phrase interpreter is controlled by a small set of Lisp interpretation functions , roughly one for each phrase type .", "Base level phrases , i . e .", "phrases with no embedded phrases , are mapped to unary interpretations .", "The phras e person IZobert L . James person , for example is mapped to the following propositional fact .", "Note the pers 01 term in this proposition it designates the semantic individual denoted by the phrase , and is generated in the process of interpretation .", "person pers 01 Complex phrases , those with embedded phrases , are typically interpreted as conjuncts of simple r interpretations the exception being NP coordination , as in chairman and chief executive .", "Consider the phrase Mr . James , 57 years old which is parsed by the phraser as follows .", "Note in particular that the overal l person age apposition is itself parsed as a person phrase .", "person person Mr .", "James person , age num 57 num years old age person The treatment of age appositions is compositional , as is the case for the interpretation of all but a few complex phrases .", "Once again , the embedded base level phrase ends up interpreted as a unary person fact .", "The semantic account of the overall apposition ends up as a has age relation modifying pers 02 , the semanti c individual for the embedded person phrase .", "This proposition designates the semantic relationship between a person and that persons age .", "More precisely , the following facts are added to the inferential database .", "person pers 02 has age pers 02 , age 03 ha 04 age age 03 What appears to be a spare argument to the has age predicate above is the event individual for the predicate .", "Such arguments denote events themselves in this case the event of being a particular number o f years old , as opposed to the individuals participating in the events the individual and his or her age .", "This treatment is similar to the partial Davidsonian analysis of events due to Hobbs 8 .", "Note that event indi viduals are by definition only associated with relations , not unary predicates .", "As a point of clarification , note that the inference system does not encode facts at the predicate calculu s level so much as at the interpretation level made popular in such systems as the SRI core language engine 1 , 3 .", "In other words , the representation is actually a structured attribute value graph such as the following , whic h encodes the age apposition above .", "head person proxy pers 02 modifiers head has age proxy ha 04 arguments pers 02 head age proxy age 03 The first two fields correspond to the embedded phrase the head field is a semantic sort , and the proxy field holds the designator for the semantic individual denoted by the phrase .", "The interpretation encoding the 147 overall apposition ends up in the modifiers slot , an approach adopted from the standard linguistic account o f phrase modification .", "Inference in Alembic is actually performed directly on interpretation structures , an d there is no need for a separate translation from interpretations to more traditional looking propositions .", "The propositional notation is more perspicuous to the reader , and we have adopted it here .", "Finally , note that the phrase interpretation machinery maintains pointers between semantic individual s and the surface strings from which they originated .", "One of the fortunate ? if unexpected ? consequences o f the phrasers semantic grammar is that maintaining these cross references is considerably simpler than was th e case in our more linguistically inspired categorial parser of old .", "Except for the ORG_DESCRIPTOR slot , the fil l rules line up more readily with semantic notions than with syntactic considerations , e . g . , maximal projections .", "Equality reasoning Much of the strength of this inferential framework derives from its equality mechanism .", "This subcomponent allows one to make two semantic individuals co designating , i . e . , to equate them .", "Facts that formerly held of only one individual are then copied to its co designating siblings .", "This in turn enables inference that may have been previously inhibited because the necessary antecedents were distributed ove r what were then distinct individuals .", "This equality machinery is exploited at many levels in processing semantic and domain constraints .", "One of the clearest such uses is in enforcing the semantics of coreference , either definite reference or appositional coreference .", "Take for example the following phrase from the walkthrough message , which we show here a s parsed by the phraser .", "org org Creative Artists Agency org , orgnp the big location Hollywood Iocation talent agency orgnp org In propositional terms , the embedded organization is interpreted a s organization org 05 Creative Artists Agency The appositive noun phrase is interpreted a s organization org 06 the .", ". agency geo region geo 07 Hollywood has location org 06 , geo 07 hasloc 08 locational pre modifier Pressing on , the phraser parses the overall org orgnp apposition as an overarching org .", "To interpret th e apposition , the interpreter also adds the following proposition to the database .", "entity np app org 05 , org 06 e n a 09 This ultimately causes org 05 and org 06 to become co designating through the equality system , and th e following fact appears in the inferential database .", "has location org 05 , geo 07 hasloc 10 i . e . , Creative Artists Agency is located in Hollywoo d This propagation of facts from one individual CO its co designating siblings is the heart of our coreferenc e mechanism .", "Its repercussions are particularly critical to the subsequent stage of template generation .", "By propagating facts in this way , we can dramatically simplify the process of collating information into templates , since all the information relevant to , say , an individual company will have been attached to that company b y equality reasoning .", "We will touch on this point again below .", "Inference The final role of the Alembic inference component is to derive new facts through the application o f carefully controlled forward inference .", "As was the case with our MUC 5 system , the present Alembic allows only limited forward inference .", "Though the full details of this inference process are of legitimate interest i n 148 their own right , we will only note some highlights here .", "To begin with , the tractability of forward inference in this framework is guaranteed just in case the inference axioms meet a certain syntactic requirement .", "To date , all the rules we have written for even complex domains , such as the joint venture task in MUC 5 , have met this criterion .", "Aside from this theoretical bound on computation , we have found in practice that th e inference system is remarkably fast , with semantic interpretation , equality reasoning , rule application , and al l other aspects of inference together accounting for 6 7 of all processing time in Alembic .", "Details are in II .", "We exploited inference rules in several primary ways for the TE and ST tasks .", "The first class of inferenc e rules enforce so called terminological reasoning , local inference that composes the meaning of words .", "One such rule distributes the meaning of certain adjectives such as retired across coordinated titles , as in retire d chairman and CEO .", "The phrase parses as follows ; note the embedded post semantic phrase types .", "post post qua I retired post aua I post post chai rman post and post CEO post post post This particular example propositionalizes as follows , where the group construct denotes a plural individual in Landmans sense roughly a set 9 .", "title ttl 11 chairman title ttl 12 CEO group ttl 11 , ttl 12 grp 13 chairman and CEO retired ttl grp 13 retired To shift the scope of retired from the overall coordination to individual titles , the following rule applies .", "retired ttl ttl ?", "group ttl , x grp retired grp This rule yields the fact retired ttl ttl 11 , and a similar rule yields retired ttl ttl 12 .", "Other like rules distribute coordinated titles across the title holder , and so forth .", "The fact that multiple rules are needed t o distribute adjectives over coordinated noun phrases is one of the drawbacks of semantic grammars .", "On the other hand , these rules simplify semantic characteristics of distributivity by deferring questions of scope and non compositionality to a later stage , i . e . , inference .", "Interpretation procedures can thus remain composi tional , which makes them substantially simpler to write .", "Additionally , these kinds of distribution rules further contribute to collating facts relevant to template generation onto the individuals for which these facts hold .", "Of greater importance , however , is the fact that inference rules are the mechanism by which we instantiat e domain specific constraints and set up the particulars required for scenario level templates .", "Some of this information is again gained by fairly straightforward compositional means .", "For example , the phrase Walter IZawleigh Jr . , retired chairman of Mobil Corp .", "yields a succession template through the mediation of on e inference rule .", "The phrase is compositionally interpreted as organization org 14 title ttl 15 retired ttl ttl 15 job ttl 15 , org 14 , job 16 person pers 17 holds job pers 17 , job 16 h j 18 The rule that maps these propositions to a succession event i s job out pers , ttl , org ?", "holds job pers , job x job ttl , org , job retired ttl ttl When applied to the above propositions this rule yields job out pers 17 , ttl 15 , org 07 j o 19 .", "This fact is all that is required for the template generator to subsequently issue the appropriate succession event templates .", "149 The most interesting form of domain inference is not compositional of course , but based on discours e considerations .", "In the present ST task , for example , succession events are not always fully fleshed out , bu t depend for their complete interpretation on information provided earlier in the discourse .", "In the walkthrough message , this kind of contextualized interpretation is required early on Yesterday , McCann made official what had been widely anticipated Mr . James , 57 years old , is stepping down a s chief executive officer on July 1 .", "He will be succeeded by Mr . Donner , 45 .", "ST level phrasing and interpretation of this passage produces two relevant facts , a job out for the firs t clause , and a successor for the second .", "Note that although successor is normally a two place relation , its valence here is one by virtue of the phraser not finding a named person as a subject to the clause .", "person pers 20 Mr . James title ttl 21 ; chairman organization org 22 McCann job out pers 20 , ttl 21 , org 22 j o 23 person pers 24 Mr . Dooner successor pers 24 succ 25 One approach to contextualizing the succession clause in this text would require first resolving th e pronominal subject He to Mr . James and then exploiting any job change facts that held about thi s antecedent .", "An equally effective approach , and simpler , is to ignore the pronoun and reason directly from th e successor fact to any contextualizing job out fact .", "The rule that accomplishes this i s job in pers a , ttl , org ?", "successor pers a succ job out in context ?", "succ , job out x 1 job out pers b , ttl , org x 2 The mysterious looking job out in context ?", "predicate implements a simple discourse model it is true just in case its second argument is the most immediate job out fact in the context of its first argument .", "Context encoding facts are not explicitly present in the database , as their numbers would be legion , but are instantiate d on demand when a rule attempts to match such a fact .", "Note that what counts as a most immediate contex tualized fact is itself determined by a separate search procedure .", "The simple minded strategy we adopted here is to proceed backwards from the current sentence , searching for the most recent sentence containing a n occurrence of ajob out phrase , and returning the semantic individual it denotes .", "In this example , the job out in contexa predicate succeeds by binding the succ variable to j o 23 , with the rule overall yielding a job in fact .", "job in pers 24 , ttl 21 , org 22 j i 26 As with job out , this fact is mapped directly by the template generator to an incoming succession template .", "Note that this process of combining a job out and successor fact effectively achieves what is ofte n accomplished in data extraction systems by template merging .", "However , since the merging takes place in th e inferential database , with propagation of relevant facts as a side effect , the process is greatly simplified an d obviates the need for explicit template comparisons .", "One final wrinkle must be noted .", "Inference is generally a non deterministic search problem , with no firm guarantee as to whether facts will be derived in the same chronological order as the sentences which underli e the facts .", "Rules that require contextualized facts , however , crucially rely on the chronological order of th e sentences underlying these facts .", "We have thus pulled these rules out of the main fray of inference , and apply them only after all other forward chaining is complete .", "In fact , these rules are organized as a Brill style rule sequence , where each rule is allowed to run to quiescence at only one point in the sequence before the nex t rule becomes active .", "It is our hypothesis , though , that alldomain inference rules can be so organized , not jus t contextualized ones , and that by this organizational scheme , rules can be automatically learned from example .", "TASK SPECIFIC PROCESSING AND TEMPLATE GENERATIO N Aside from phrasing and inference , a relatively small ? but critical ? amount of processing is required t o perform the Muc 6 named entities and template generation tasks .", "150 For NE , little is actually required beyond careful document management and printing routines .", "TIMEX forms , introduced by the preprocessing date tagger , must be preserved through the rest of the processing pipe .", "Named entity phrases that encode an intermediate stage of NE processing must be suppressed at printout .", "Examples such as these abound , but by and large , Alembics NE output is simply a direct readout of the resul t of running the named entity phraser rules .", "Name coreference in TE Of all three tasks , TE is actually the one that explicitly requires most idiosyncratic processing beyon d phrasing and inference .", "Specifically , this task is the crucible for name coreference , i . e . , the process by which short name forms are reconciled with their originating long forms .", "This merging process takes place by iterating over the semantic individuals in the inferential database tha t are of a namable sort e . g . , person or organization .", "Every such pair of same sort individuals is compared t o determine whether one is a derivative form of the other .", "Several tests are possible .", "If the forms are identical strings , as in the frequently repeated Dooner , or McCann in th e walkthrough article , then they are merged .", "If one form is a shortening of the other , as in Mr . James for Robert L . James , then the short form is merged as an alias of the longer .", "If one form appears to be an acronym for the other , as in CAA and Creative Artist s Agency , then the forms should be merged , with the acronym designated as an alias .", "Merging two forms takes place in several steps .", "First , their respective semantic individuals are equated in the inferential database .", "This allows facts associated with one form to become propagated to the other .", "In this way , the nationality information in Japanese giant NEC becomes associated with the canonical nam e Nippon Electric Corp .", "As a second step , names that are designated as aliases are recorded as such .", "Template generation We mentioned above that the inferential architecture that we have adopted here is in good part motivate d by a desire to simplify template generation .", "Indeed , template generation consists of nothing more than reading out the relevant propositions from the database .", "For the TE task , this means identifying person and organization individuals by matching on person x o r organization y .", "For each so matching semantic individual , we create a skeletal template .", "The skeleton is initialized with name and alias strings that were attached to the semantic individuals during name merging .", "I t is further fleshed out by looking up related facts that hold of the matched individual , e . g . , has location y , z for organizations or has title x , w for persons .", "These facts are literally just read out of the database .", "Finalization routines are then invoked on the near final template to fill the ORG TYPE slot and to normalize the geographical fills of the ORG_LOCALE and ORG COUNTRY slots .", "PERFORMANCE ANALYSIS We participated in two officially scored tasks at MUC 6 , named entities and template elements .", "As noted above , we put roughly a staff week into customizing the system to handle the scenario templates task , bu t chose not to participate in the evaluation because another staff week or so would have been required to achieve performance on a par with other parts of the system .", "Overall performance On the named entity task , we obtained an official P R score of 91 . 2 , where the separate precision and recal l scores were both officially reported to be 91 .", "The overall score is remarkably close to our performance on th e 151 Formal training Official test Recall Precision Recall Precision Recall A Precis .", "A organization 86 92 84 92 2 ?", "name 76 78 77 80 1 2 alias 60 79 56 78 4 1 descriptor 27 62 16 49 11 13 type 83 90 81 89 2 1 locale 46 87 43 87 3 ?", "country 47 88 45 93 2 5 person 94 92 95 87 1 5 name 93 91 93 84 ?", "_7 alias 94 95 86 96 8 1 title 95 96 94 93 1 3 All Objects 75 86 73 85 2 1 F Measure , unrevised 80 . 21 78 . 52 1 . 69 Table 2 Slot by slot performance differences , TE task unrevised scores .", "dry run test set which served as our principal source of data for NE training and self evaluation .", "To be precise , our final dry run P R score prior to the MUC 6 evaluation run was 91 . 8 , a scant o . 6 higher than the officially measured evaluation score .", "The fact that the score dropped so little is encouraging to us .", "On the template elements task , our initial TE score was P R 78 . 5 , and our revised official score was 77 .", "Once again , this performance is encouragingly close to Alembics performance on our final self evaluation using the formal training data set .", "By the non revised metric , we achieved a performance of P R 80 . 2 on the training data , with an overall drop of 1 . 7 points ofP R between training and official test .", "Table 2 summarizes slot by slot differences between our training and test performance on the TE task .", "The major differences we noted between training and testing performance lie in the organization alias and descriptor slots , and in th e person name and alias fields ; we have marked these discrepancies with asterisks and will address their caus e later on in this document .", "Walkthrough errors In order to quantify Alembics performance on the walkthrough message , we compiled an exhaustive analysis of our errors on this text .", "This was a difficult message for us , and we scored substantially less well on this message than in average , especially on the NE task .", "To our surprise , the majority of our errors was du e not to knowledge gaps in the named entity tagger , so much as to odd bugs and incompletely thought ou t design decisions .", "Table 3 summarizes these errors .", "Entries marked with daggers t correspond to knowledg e gaps , e . g . , missing or incorrect rules ; the other entries are coding or design problems .", "Fully half the problem instances were due to string matching issues for short name forms .", "For example , by not treating embedde d mid word hyphens as white space , we failed to process McCann as a shortened form of McCann Erickson .", "Turning now to the template element task , we note that the largest fraction of TE errors are repercussions of errors committed while performing the NE task .", "In particular , the people name companies that wer e treated as persons during NE processing in turn led to spurious person templates .", "The magnitude of the NE error is mitigated by the fact that identical mentions of incorrectly tagged named entities are merged for th e sake of TE template generation , and thus do not all individually spawn spurious templates .", "Among the TE errors not arising from NE processing errors , note in particular those that occurred on the most difficult slots , ORG DESCRIPTOR , ORG_LOCALE , and ORG_COUNTRY .", "These are all due in this case to missing locational an d 15 2 Nature of the problem Problem cases Resulting errors Naive string matching McCann vs . McCann Erickson 9 inc type John Dooner vs . John J .", "Dooner Jr . 1 inc text , 1 spu type text Missing phraser patterns t Fallon McElligott ?", "treated as person 1 inc type Tasters Choice ?", "naive s prorpssing 1 spu type text Poor phraser patterns t Coca Cola Classic ?", "zealous org rule 1 spu type text Missing date patterns t the 21st century 1 mis type text Ambiguous name New York Times ?", "not an org 1 spu type text Misc .", "embarrassing bugs James in HL ?", "treated as location 1 inc . type J .", "Walter Thompson ?", "punctoker lost J .", "1 inc type , 1 inc text Table 3 NE errors on walkthrough message Nature of the problem Problem cases Resulting errors Repercussions of NE errors Walter Thompson , Fallon McElligott , 3 spu pers .", "alias McCann all treated as person John Dooner treated as two persons 2 spu pets , 1 mis pers .", "alias Coca Cola Classic treated as organization 1 inc org .", "namett , 1 inc . org alias Missing org .", "NP patternst the agency with billings of 400 million 2 mis org .", "descriptor one of the largest world wide agencies Missing location patterns t Cokes headquarters in Atlanta 1 mis org .", "locale country Org .", "type determination t Creative Artists Agency ?", "treated as gov .", "org type Acronym resolution snafu CAA vs . Creative Artists Agency 1 inc org .", "namett , 1 mis org .", "alias Enthusiastic scorer mapping New York Times spurious entity mapped to 1 inc .", "namett Fallon McElligott inc .", "entity type Table 4 TE errors on walkthrough message organizational NP phraser rules , which is consistent with trends we noted during training .", "These observation s are summarized in Table 4 .", "Once again , single daggers t mark errors attributable to knowledge gaps .", "Note also that because of lenient template mappings on the part of the scorer , a number of errors that might intuitively have been whole organization template errors turned out only to manifest themselves a s organization name errors .", "These cases are marked with double daggers tt .", "Other trend s In addition to this analysis of the single walkthrough message , we opened up some to of the test data t o inspection , and performed a rough trend estimation .", "In particular , we wanted to explain the slot by slot discrepancies we had noted between our training and test performance cf .", "We found a com bination of knowledge gaps , known design problems that had been left unaddressed by the time of the evaluation run , and some truly embarrassing bugs .", "To dispense quickly with the latter , we admit to failing to filter lines beginning with 0 in the body of the message .", "This was due to the fact that earlier training data had these lines marked with 5 tags , whereas the official test data did not .", "These 0 lines were so rare in the formal training data that we had simply no t noticed our omission .", "This primarily affected our NUMEX and TIMEX precision in the named entity task .", "153 In the template element task , our largest performance drop was on the ORG_DESRIPTOR SIOt , where we los t ii points of recall and 13 points of precision .", "This can be largely attributed to knowledge gaps in our phrase r rules for organizational noun phrases .", "In particular , we were missing a large number of head nouns that would have been required to identify relevant descriptor NPS .", "On the PERSON_NAME and PERSON_ALIAS Slots , we respectively found a 7 point drop in precision and an 8 point drop in recall .", "These were due to the same problem , a known flaw that had been left unaddressed i n the days leading to the evaluation .", "In particular , we had failed to merge short name forms that appeared i n headlines with the longer forms that appeared in the body of the message .", "For example , James in the walkthrough headline field should have been merged with Robert L . James in the body of the message .", "Because these short forms went unmerged , they in turn spawned incorrect person templates , hence the dro p in PERSON and PERSON_NAME precision .", "For the same reason , the templates that were generated for the long forms of these names ended up without their alias slot filled , accounting for the drop in PERSON_ALIAS recall .", "A similar problem held true for the ORG ALIAS slot .", "In this case , we failed both to extract organizatio n templates from the headline fields , or merge short name forms from headlines with longer forms in the tex t bodies .", "We were aware of these mis features in our handling of person and organization name templates , but had left these problems unaddressed since they seemed to have only minimal impact on the forma l training data .", "Errare humanum est .", "POST HOC EXPERIMENTS With this error analysis behind us , we pursued a number of post hoc experiments .", "Most interesting among them was a simple attempt at improving recall on organization names .", "Indeed , Alembic has only a short list of known organizations ? less than a half dozen in total .", "Virtually all of the organizations found by Alembic are recognized from first principles .", "We decided to compare this strategy with one that uses a large lexicon of organization names .", "All of the Muc 6 NE training set was used to generate a list of i , 8o8 distinct organization name strings .", "This could certainly be larger , but seemed a reasonable size .", "Nonetheless , this lexicon by itself got less than half of the organizations in the official named entity test corpus organization recall was 45 and precision 91 .", "Another interesting question is how much an organization lexicon might have helped had it been added to our rule based phrasing algorithm , not simply used by itself .", "This configuration actually decreased ou r performance slightly F score down by 0 . 5 points of P R , trading a slight increase in organization recall for a larger decrease in precision .", "The biggest problem here is due to overgeneration up from 4 to 6 , and partial matches such as the following , Kraft ENAMEX General Foods ENAMEX First E NA M EX Fidel ity ENAMEX where General Foods and Fidelity were in the training corpus for the organization lexicon , but the longer names above were not .", "Admittedly , the way we integrated the organization lexicon into Alembic was relatively naYve , thereb y leading to some of these silly precision errors .", "We believe that if we more intelligently took advantage of thi s knowledge source , we could reduce the additional precision errors almost entirely .", "In addition , we were disappointed by the fact that our exhaustive compilation only produced somewhat less than 2 , 000 organi zation names , and only led to a piffling improvement in recall .", "Perhaps had we made use of larger name lists , we might have obtained better recall improvements ? a case in point is the gargantuan Dunn Bradstree t listing exploited by Knight Ridder for their named entities tagger 41 .", "Note , however , that all but a few of the organizations that were found in both the training name list and the test data were found by Alembic from first principles anyway .", "We may thus tentatively conclude that short of being as encyclopedic as the D B listing , a larger , better integrated organization lexicon may have provided no more than a limite d improvement in F score .", "To further improve our organization tagging , it appears that we will simply have t o expend more energy writing named entity phraser rules .", "154 CONCLUDING THOUGHT S All in all , we are quite pleased with the performance of Alembic in Muc 6 .", "While we regret no t participating in the ST task , we do believe that the framework was up to it , especially in light of our TE scores .", "There were many lessons learned , and there will continue to be , as we further analyze our results and mak e improvements to the system .", "Several points stand out .", "We had hoped to avoid full NP parsing , but the definition of the ORG DESCRIPTOR slot clearly requires this , and we will need to return to larger scale parsing strategies in the future .", "We had hoped to include more machine learned phraser rules , and as the rule learner matures , we almost certainly will .", "One thing is clear to us , however , and that is that rule sequences are an extremely powerful tool .", "They were easy to hand craft and adapt to the MUC 6 requirements .", "They run fast .", "And they work well .", "References 1 Alshawi , H .", "Van Eijck , J .", "Logical forms in the core language engine .", "In Proceedings of the 27th Meeting ofthe Assoc .", "iationfor Computational Linguistics ACL 89 .", "Vancouver , E . c . , 1985 .", "2 Appelt , D . , Hobbs , J . , Bear , J . , Israel , D . , Tyson , M .", "FASTUS A finite state processor for infor mation extraction from real world text .", "In Proceedings of the 13th Intl .", "Joint Conference on Artificial Intelligence IJCAR93 .", "Chambery , 1993 .", "Vilain , M . The relation based knowledge representation of King Kong .", "SIGART Bulletin , 2 3 , 15 21 .", "4 Borkovsky , A .", "Knight Ridders value adding name finder a variation on the theme of FASTUS .", "In Proceedings ofthe 6th Message Understanding Conference Muc 6 .", "Some advances in rule based part of speech tagging .", "In Proceedings of the 12th Nationa l Conference on Artificial Intelligence AAAI 94 .", "Seattle , 1 994 .", "6 Brill , E . A corpus based approach to language learning .", "Doctoral Dissertation , Univ .", "7 Brill , E . A simple rule based part of speech tagger .", "In Proceedings of the 3rd Conference on Applied Natural Language Processing Applied ACL 92 .", "Ontological promiscuity .", "In Proceedings of the 23rd Meeting of the Association fo r Computational Linguistics ACL 85 .", "Chicago , Ill . , 1985 .", "9 Landman , F .", "Linguistics and Philosophy , 12 3 , 359 605 and 12 4 , 723 744 .", "to Lehnert , W . , McCarthy , J . , Soderland , S . , Riloff , E . , Cardie , C . , Peterson , J . , Feng , F . , Dolan , C . , Goldman , S .", "University of Massachusetts Hughes Description of the CIRCUS system as used for Muc 5 .", "In Proceedings of the 5th Message Understanding Conference MUC 5 .", "Baltimore , Md . , 1993 .", "11 Vilain , M .", "Semantic inference in natural language validating a tractable approach .", "In Proceedings of the 24th Intl .", "Joint Conference on Artificial Intelligence IJCA1 9 5 .", "Montreal , 1995 ."], "summary_lines": ["MITRE: Description Of The Alembic System Used For MUC-6\n", "As with several other veteran Muc participants, MITRE'S Alembic system has undergone a major transformation in the past two years.\n", "The genesis of this transformation occurred during a dinner conversation at the last Muc conference, MUC-5.\n", "At that time, several of us reluctantly admitted that our major impediment towards improved performance was reliance on then-standard linguistic models of syntax.\n", "We knew we would need an alternative to traditional linguistic grammars, even to the somewhat non-traditional categorial pseudo-parser we had in place at the time.\n", "The problem was, which alternative?\n", "The answer came in the form of rule sequences, an approach Eric Brill originally laid out in his work on part-of-speech tagging [5, 7].\n", "Rule sequences now underlie all the major processing steps in Alembic: part-of-speech tagging, syntactic analysis, inference, and even some of the set-fill processing in the Template Elemen t task (TE).\n", "We have found this approach to provide almost an embarrassment of advantages, speed and accuracy being the most externally visible benefits.\n", "In addition, most of our rule sequence processors are trainable, typically from small samples.\n", "The rules acquired in this way also have the characteristic that they allow one to readily mix hand-crafted and machine-learned elements.\n", "We have exploited this opportunity to apply both machine-learned and hand-crafted rules extensively, choosing in some instances to run sequences that were primarily machine-learned, and in other cases to run sequences that were entirely crafted by hand.\n", "Our typical machine learning approaches for English NE are transformation-based learning.\n"]}
{"article_lines": ["Efficient Deep Processing Of Japanese", "We present a broad coverage Japanese grammar written in the HPSG formalism with MRS semantics .", "The grammar is created for use in real world applications , such that robustness and performance issues play an important role .", "It is connected to a POS tagging and word segmentation tool .", "This grammar is being developed in a multilingual context , requiring MRS structures that are easily comparable across languages .", "Natural language processing technology has recently reached a point where applications that rely on deep linguistic processing are becoming feasible .", "Such applications e . g . message extraction systems , machine translation and dialogue understanding systems require natural language understanding , or at least an approximation thereof .", "This , in turn , requires rich and highly precise information as the output of a parse .", "However , if the technology is to meet the demands of real world applications , this must not come at the cost of robustness .", "Robustness requires not only wide coverage by the grammar in both syntax and semantics , but also large and extensible lexica as well as interfaces to preprocessing systems for named entity recognition , non linguistic structures such as addresses , etc .", "Furthermore , applications built on deep NLP technology should be extensible to multiple languages .", "This requires flexible yet well defined output structures that can be adapted to grammars of many different languages .", "Finally , for use in real world applications , NLP systems meeting the above desiderata must also be efficient .", "In this paper , we describe the development of a broad coverage grammar for Japanese that is used in an automatic email response application .", "The grammar is based on work done in the Verbmobil project Siegel 2000 on machine translation of spoken dialogues in the domain of travel planning .", "It has since been greatly extended to accommodate written Japanese and new domains .", "The grammar is couched in the theoretical framework of Head Driven Phrase Structure Grammar HPSG Pollard Sag 1994 , with semantic representations in Minimal Recursion Semantics MRS Copestake et al . 2001 .", "HPSG is well suited to the task of multilingual development of broad coverage grammars It is flexible enough analyses can be shared across languages but also tailored as necessary , and has a rich theoretical literature from which to draw analyzes and inspiration .", "The characteristic type hierarchy of HPSG also facilitates the development of grammars that are easy to extend .", "MRS is a flat semantic formalism that works well with typed feature structures and is flexible in that it provides structures that are under specified for scopal information .", "These structures give compact representations of ambiguities that are often irrelevant to the task at hand .", "HPSG and MRS have the further advantage that there are practical and useful open source tools for writing , testing , and efficiently processing grammars written in these formalisms .", "The tools we are using in this project include the LKB system Copestake 2002 for grammar development , incr tsdb Oepen Carroll 2000 for testing the grammar and tracking changes , and PET Callmeier 2000 , a very efficient HPSG parser , for processing .", "We also use the ChaSen tokenizer and POS tagger Asahara Matsumoto 2000 .", "While couched within the same general framework BPSG , our approach differs from that of Kanayama et al 2000 .", "The work described there achieves impressive coverage 83 . 7 on the EDR corpus of newspaper text with an underspecified grammar consisting of a small number of lexical entries , lexical types associated with parts of speech , and six underspecified grammar rules .", "In contrast , our grammar is much larger in terms of the number of lexical entries , the number of grammar rules , and the constraints on both , 1 and takes correspondingly more effort to bring up to that level of coverage .", "The higher level of detail allows us to output precise semantic representations as well as to use syntactic , semantic and lexical information to reduce ambiguity and rank parses .", "The fundamental notion of an BPSG is the sign .", "A sign is a complex feature structure representing information of different linguistic levels of a phrase or lexical item .", "The attributevalue matrix of a sign in the Japanese BPSG is quite similar to a sign in the LinGO English Resource Grammar henceforth ERG Flickinger 2000 , with information about the orthographical realization of the lexical sign in PHON , syntactic and semantic information in SYNSEM , information about the lexical status in LEX , nonlocal information in NONLOC , head information that goes up the tree in HEAD and information about subcategorization in SUBCAT .", "The grammar implementation is based on a system of types .", "There are 900 lexical types that define the syntactic , semantic and pragmatic properties of the Japanese words , and 188 types that define the properties of phrases and lexical rules .", "The grammar includes 50 lexical rules for inflectional and derivational morphology and 47 phrase structure rules .", "The lexicon contains 5100 stem entries .", "As the grammar is developed for use in applications , it treats a wide range of 1 We do also make use of generic lexical entries for certain parts of speech as a means of extending our lexicon .", "See section 3 below . basic constructions of Japanese .", "Only some of these phenomena can be described here .", "The structure of SUBCAT is different from the ERG SUBCAT structure .", "This is due to differences in subcategorization between Japanese and English .", "A fundamental difference is the fact that , in Japanese , verbal arguments are frequently omitted .", "For example , arguments that refer to the speaker , addressee , and other arguments that can be inferred from context are often omitted in spoken language .", "Additionally , optional verbal arguments can scramble .", "On the other hand , some arguments are not only obligatory , but must also be realized adjacent to the selecting head .", "To account for this , our subcategorization contains the attributes SAT and VAL .", "The SAT value encodes whether a verbal argument is already saturated such that it cannot be saturated again , optional or adjacent .", "VAL contains the agreement information for the argument .", "When an argument is realized , its SAT value on the mother node is specified as sat and its SYNSEM is unified with its VAL value on the subcategorizing head .", "The VAL value on the mother is none .", "Adjacency must be checked in every rule that combines heads and arguments or adjuncts .", "This is the principle of adjacency , stated as follows In a headed phrase , the SUBCAT . SAT value on the non head daughter must not contain any adjacent arguments .", "In a headcomplement structure , the SUBCAT . SAT value of the head daughter must not contain any adjacent arguments besides the nonhead daughter .", "In a head adjunct structure , the SUBCAT . SAT value of the head daughter must not contain any adjacent arguments .", "Japanese verb stems combine with endings that provide information about honorification , tense , aspect , voice and mode .", "Inflectional rules for the different types of stems prepare the verb stems for combination with the verbal endings .", "For example , the verb stem yomu must be inflected to yon to combine with the past tense ending da .", "Morphological features constrain the combination of stem and ending .", "In the above example , the inflectional rule changes the mu character to the n character and assigns the value nd morph to the morphological feature RMORPH BIND TYPE .", "The ending da selects for a verbal stem with this value .", "Endings can be combined with other endings , as in sase rare mashi ta causative potentialhonorific past , but not arbitrarily sase mashi rare ta sase ta mashi rare sase ta rare mashi ta This is accounted for with two kinds of rules which realize mutually selected elements .", "In the combination of stem and ending , the verb stem selects for the verbal ending via the head feature SPEC .", "In the case of the combination of two verbal endings , the first ending selects for the second one via the head feature MARK .", "In both cases , the right element subcategorizes for the left one via SUBCAT . VAL . SPR .", "Using this mechanism , it is possible to control the sequence of verbal endings Verb stems select verbal endings via SPEC and take no SPR , derivational morphemes like causative or potential select tense endings or other derivational morphemes via MARK and subcategorize for verb stems and or verb endings via SPR sase takes only verb stems , and tense endings take verb stems or endings as SPR and take no MARK or SPEC as they occur at the end of the sequence .", "A special treatment is needed for Japanese verbal noun light verb constructions .", "In these cases , a word that combines the qualities of a noun with those of a verb occurs in a construction with a verb that has only marginal semantic information .", "The syntactic , semantic and pragmatic information on the complex is a combination of the information of the two .", "Consider example 1 .", "The verbal noun benkyou contains subcategorization information transitive , as well as semantic information the benkyou relation and its semantic arguments .", "The light verb shi ta supplies tense information past .", "Pragmatic information can be supplied by both parts of the construction , as in the formal form o benkyou shi mashi ta .", "The rule that licenses this type of combination is the vn lightrule , a subtype of the head marker rule . study do past 'Someone has studied . '", "Japanese auxiliaries combine with verbs and provide either aspectual or perspective information or information about honorification .", "In a verb auxiliary construction , the information about subcategorization is a combination of the SUBCAT information of verb and auxiliary , depending on the type of auxiliary .", "The rule responsible for the information combination in these cases is the head specifier rule .", "We have three basic types of auxiliaries .", "The first type is aspect auxiliaries .", "These are treated as raising verbs , and include such elements as iru roughly , progressive and aru roughly , perfective , as can be seen in example 2 .", "The other two classes of auxiliaries provide information about perspective or the point of view from which a situation is being described .", "Both classes of auxiliaries add a ni dative marked argument to the argument structure of the whole predicate .", "The classes differ in how they relate their arguments to the arguments of the verb .", "One class including kureru 'give' ; see example 3 are treated as subject control verbs .", "The other class including morau 'receive' , see example 4 establishes a control relation between the nimarked argument and the embedded subject .", "Watashi ga sensei ni hon wo I NOM teacher DAT book ACC katte morat ta . buy get past 'The teacher bought me a book . '", "The careful treatment of Japanese particles is essential , because they are the most frequently occurring words and have various central functions in the grammar .", "It is difficult , because one particle can fulfill more than one function and they can co occur , but not arbitrarily .", "The Japanese grammar thus contains a type hierarchy of 44 types for particles .", "See Siegel 1999 for a more detailed description of relevant phenomena and solutions .", "Number names , such as sen kyuu hyaku juu '1910' constitute a notable exception to the general head final pattern of Japanese phrases .", "We found Smith's 1999 head medial analysis of English number names to be directly applicable to the Japanese system as well Bender 2002 .", "This analysis was easily incorporated into the grammar , despite the oddity of head positioning , because the type hierarchy of HPSG is well suited to express the partial generalizations that permeate natural language .", "On the other hand , number names in Japanese contrast sharply with number names in English in that they are rarely used without a numeral classifier .", "The grammar provides for 'true' numeral classifiers like hon , ko , and hiki , as well as formatives like en 'yen' and do 'degree' which combine with number names just like numeral classifiers do , but never serve as numeral classifiers for other nouns .", "In addition , there are a few non branching rules that allow bare number names to surface as numeral classifier phrases with specific semantic constraints .", "Spoken language and email correspondence both encode references to the social relation of the dialogue partners .", "Utterances can express social distance between addressee and speaker and third persons .", "Honorifics can even express respect towards inanimates .", "Pragmatic information is treated in the CONTEXT layer of the complex signs .", "Honorific information is given in the CONTEXT . BACKGROUND and linked to addressee and speaker anchors .", "The expression of empathy or in group vs . out group is quite prevalent in Japanese .", "One means of expressing empathy is the perspective auxiliaries discussed above .", "For example , two auxiliaries meaning roughly 'give' ageru and kureru contrast in where they place the empathy .", "In the case of ageru , it is with the giver .", "In the case of kureru , it is with the recipient .", "We model this within the sign by positing a feature EMPATHY within CONTEXT and linking it to the relevant arguments' indices .", "In the multilingual context in which this grammar has been developed , a high premium is placed on parallel and consistent semantic representations between grammars for different languages .", "Ensuring this parallelism enables the reuse of the same downstream technology , no matter which language is used as input .", "Integrating MRS representations parallel to those used in the ERG into the Japanese grammar took approximately 3 months .", "Of course , semantic work is on going , as every new construction treated needs to be given a suitable semantic representation .", "For the most part , semantic representations developed for English were straightforwardly applicable to Japanese .", "This section provides a brief overview of those cases where the Japanese constructions we encountered led to innovations in the semantic representations and or the correspondence between syntactic and semantic structures .", "Due to space limitations , we discuss these analyses in general terms and omit technical details .", "2 . l Nominalization and Verbal Nouns Nominalization is of course attested in English and across languages .", "However , it is much more prevalent in Japanese than in English , primarily because of verbal nouns .", "As noted in Section 1 . 3 above , a verbal noun like benkyou 'study' can appear in syntactic contexts requiring nouns , or , in combination with a light verb , in contexts requiring verbs .", "One possible analysis would provide two separate lexical entries , one with nominal and one with verbal semantics .", "However , this would not only be redundant missing the systematic relationship between these uses of verbal nouns but would also contradict the intuition that even in its nominal use , the arguments of benkyou are still present .", "Nihongo no benkyou wo hajimeru .", "Japanese GEN study ACC begin 'Someone begins the study of Japanese . '", "In order to capture this intuition , we opted for an analysis that essentially treats verbal nouns as underlyingly verbal .", "The nominal uses are produced by a lexical rule which nominalizes the verbal nouns .", "The semantic effect of this rule is to provide a nominal relation which introduces a variable which can in turn be bound by quantifiers .", "The nominal relation subordinates the original verbal relation supplied by the verbal noun .", "The rule is lexical as we have not yet found any cases where the verb's arguments are clearly filled by phrases in the syntax .", "If they do appear , it is with genitive marking e . g . , nihongo no in the example above .", "In order to reduce ambiguity , we leave the relationship between these genitive marked NPs and the nominalized verbal noun underspecified .", "There is nothing in the syntax to disambiguate these cases , and we find that they are better left to downstream processing , where there may be access to world knowledge .", "As noted in Section1 . 5 , the internal syntax of number names is surprisingly parallel between English and Japanese , but their external syntax differs dramatically .", "English number names can appear directly as modifiers of NPs and are treated semantically as adjectives in the ERG .", "Japanese number names can only modify nouns in combination with numeral classifiers .", "In addition , numeral classifier phrases can appear in NP positions akin to partitives in English .", "Finally , some numeral classifier like elements do not serve the modifier function but can only head phrases that fill NP positions .", "This constellation of facts required the following innovations a representation of numbers that doesn't treat them as adjectives in MRS terms , a feature structure without the ARG feature , a representation of the semantic contribution of numeral classifiers a relation between numbers and the nouns they modify , this time with an ARG feature , and a set of rules for promoting numeral classifier phrases to NPs that contribute the appropriate nominal semantics underspecified in the case of ordinary numeral classifiers or specific in the case of words like en 'yen' .", "The primary issue in the analysis of relative clauses and adjectives is the possibility of extreme ambiguity , due to several intersecting factors Japanese has rampant pro drop and does not have any relative pronouns .", "In addition , a head noun modified by a relative clause need not correspond to any gap in the relative clause , as shown by examples like the following Matsumoto 1997 head NOM better become book 'a book that makes one smarter' Therefore , if we were to posit an attributive adjective noun construction distinct from the relative clause noun possibility we would have systematic ambiguities for NPs like akai hon 'red book' , ambiguities which could never be resolved based on information in the sentence .", "Instead , we have opted for a relative clause analysis of any adjective noun combination in which the adjective could potentially be used predicatively .", "Furthermore , because of gapless relative clauses like the one cited above , we have opted for a non extraction analysis of relative clauses . 2 Nonetheless , the well formedness constraints on MRS representations require that there be 2 There is in fact some linguistic evidence for extraction in some relative clauses in Japanese see e . g . , Baldwin 2001 .", "However , we saw no practical need to allow for this possibility in our grammar , and particularly not one that would justify the increase in ambiguity .", "There is also evidence that some adjectives are true attributives and cannot be used predicatively Yamakido 2000 .", "These are handled by a separate adjective noun rule restricted to just these cases . some relationship between the head noun and the relative clause .", "We picked the topic relation for this purpose following Kuno 1973 .", "The topic relation is introduced into the semantics by the relative clause rule .", "As with main clause topics which we also give a non extraction analysis , we rely on downstream anaphora resolution to refine the relationship .", "For the most part , semantic representations and the syntax semantic interface already worked out in the ERG were directly applicable to the Japanese grammar .", "In those cases where Japanese presented problems not yet encountered or at least not yet tackled in English , it was fairly straightforward to work out suitable MRS representations and means of building them up .", "Both of these points illustrate the cross linguistic validity and practical utility of MRS representations .", "As Japanese written text does not have word segmentation , a preprocessing system is required .", "We integrated ChaSen Asahara Matsumoto 2000 , a tool that provides word segmentation as well as POS tags and morphological information such as verbal inflection .", "As the lexical coverage of ChaSen is higher than that of the HPSG lexicon , default part of speech entries are inserted into the lexicon .", "These are triggered by the part ofspeech information given by ChaSen , if there is no existing entry in the lexicon .", "These specific default entries assign a type to the word that contains features typical to its part of speech .", "It is therefore possible to restrict the lexicon to those cases where the lexical information contains more than the typical information for a certain part of speech .", "This default mechanism is often used for different kinds of names and 'ordinary' nouns , but also for adverbs , interjections and verbal nouns where we assume a default transitive valence pattern . 3 The ChaSen lexicon is extended with a domainspecific lexicon , containing , among others , names in the domain of banking .", "For verbs and adjectives , ChaSen gives information about stems and inflection that is used in a similar way .", "The inflection type is translated to an HPSG type .", "These types interact with the inflectional rules in the grammar such that the default entries are inflected just as 'known' words would be .", "In addition to the preprocessing done by ChaSen , an additional shallow preprocessing tool recognizes numbers , date expressions , addresses , email addresses , URLs , telephone numbers and currency expressions .", "The output of the preprocessing tool replaces these expressions in the string with placeholders .", "The placeholders are parsed by the grammar using special placeholder lexical entries .", "The grammar is aimed at working with realworld data , rather than at experimenting with linguistic examples .", "Therefore , robustness and performance issues play an important role .", "While grammar development is carried out in the LKB Copestake 2002 , processing both in the application domain and for the purposes of running test suites is done with the highly efficient PET parser Callmeier 2000 .", "Figures 1 and 2 show the performance of PET parsing of hand made and real data , respectively .", "One characteristic of real world data is the variety of punctuation marks that occur and the potential for ambiguity that they bring .", "In our grammar , certain punctuation marks are given lexical entries and processed by grammar rules .", "Take , for example , quotation marks .", "Ignoring them as done in most development oriented grammars and smaller grammars , leads to a significant loss of structural information 'Someone said push the button .", "quot ; The formative to is actually ambiguous between a complementizer and a conjunction .", "Since the phrase before to is a complete sentence , this string is ambiguous if one ignores the quotation marks .", "With the quotation marks , however , only the complementizer to is possible .", "Given the high degree of ambiguity inherent in broadcoverage grammars , we have found it extremely useful to parse punctuation rather than ignore it .", "The domains we have been working on like many others contain many date and number expressions .", "While a shallow tool recognizes general structures , the grammar contains rules and types to process these .", "Phenomena occurring in semi spontaneous language email correspondence , such as interjections e . g . maa 'well' , contracted verb forms e . g . tabe chatta tabete shimatta ' someone ate it all up' , fragmentary sentences e . g . bangou 1265 'number 1265' and NP fragments e . g . bangou ?", "'number ? ' must be covered as well as the 'ordinary' complete sentences found in more carefully edited text .", "Our grammar includes types , lexical entries , and grammar rules for dealing with such phenomena .", "Perhaps the most important performance issue for broad coverage grammars is ambiguity .", "At one point in the development of this grammar , the average number of readings doubled in two months of work .", "We currently have two strategies for addressing this problem First , we include a mechanism into the grammar rules that chooses left branching rules in cases of compounds , genitive modification and conjuncts , as we don t have enough lexicalsemantic information represented to choose the right dependencies in these cases . 4 Secondly , we use a mechanism for hand coding reading preferences among rules and lexical entries .", "4Consider , for example , genitive modification The semantic relationship between modifier and modifiee is dependent on their semantic properties toukyou no kaigi 'the meeting in Tokyo' , watashi no hon 'my book' .", "More lexical semantic information is needed to choose the correct parse in more complex structures , such as in watashi no toukyou no imooto My sister in Tokyo .", "Restrictions like head complement preferred to head adjunct are quite obvious .", "Others require domain specific mechanisms that shall be subject of further work .", "Stochastic disambiguation methods being developed for the ERG by the Redwoods project at Stanford University Oepen et al . 2002 should be applicable to this grammar as well .", "The grammar currently covers 93 . 4 of constructed examples for the banking domain 747 sentences and 78 . 2 of realistic email correspondence data 316 sentences , concerning requests for documents .", "During three months of work , the coverage in the banking domain increased 48 . 49 .", "The coverage of the document request data increased 51 . 43 in the following two weeks .", "We applied the grammar to unseen data in one of the covered domains , namely the FAQ site of a Japanese bank .", "The coverage was 61 .", "91 . 2 of the parses output were associated with all well formed MRSs .", "That means that we could get correct MRSs in 55 . 61 of all sentences .", "We described a broad coverage Japanese grammar , based on HPSG theory .", "It encodes syntactic , semantic , and pragmatic information .", "The grammar system is connected to a morphological analysis system and uses default entries for words unknown to the HPSG lexicon .", "Some basic constructions of the Japanese grammar were described .", "As the grammar is aimed at working in applications with real world data , performance and robustness issues are important .", "The grammar is being developed in a multilingual context , where much value is placed on parallel and consistent semantic representations .", "The development of this grammar constitutes an important test of the cross linguistic validity of the MRS formalism .", "The evaluation shows that the grammar is at a stage where domain adaptation is possible in a reasonable amount of time .", "Thus , it is a powerful resource for linguistic applications for Japanese .", "In future work , this grammar could be further adapted to another domain , such as the EDR newspaper corpus including a headline grammar .", "As each new domain is approached , we anticipate that the adaptation will become easier as resources from earlier domains are reused .", "Initial evaluation of the grammar on new domains and the growth curve of grammar coverage should bear this out ."], "summary_lines": ["Efficient Deep Processing Of Japanese\n", "We present a broad coverage Japanese grammar written in the HPSG formalism with MRS semantics.\n", "The grammar is created for use in real world applications, such that robustness and performance issues play an important role.\n", "It is connected to a POS tagging and word segmentation tool.\n", "This grammar is being developed in a multilingual context, requiring MRS structures that are easily comparable across languages.\n", "Our hand-crafted Japanese HPSG grammar, JACY, provides semantic information as well as linguistically motivated analysis of complex constructions.\n"]}
{"article_lines": ["A Simple and Effective Hierarchical Phrase Reordering Model", "While phrase based statistical machine translation systems currently deliver state of the art performance , they remain weak on word order changes .", "Current phrase reordering models can properly handle swaps between adjacent phrases , but they typically lack theability to perform the kind of long distance re orderings possible with syntax based systems .", "In this paper , we present a novel hierarchical phrase reordering model aimed at improvingnon local reorderings , which seamlessly in tegrates with a standard phrase based system with little loss of computational efficiency .", "Weshow that this model can successfully han dle the key examples often used to motivate syntax based systems , such as the rotation of a prepositional phrase around a noun phrase .", "We contrast our model with reordering models commonly used in phrase based systems , and show that our approach provides statistically significant BLEU point gains for two language pairs Chinese English 0 . 53 on MT05 and 0 . 71 on MT08 and Arabic English 0 . 55 on MT05 .", "Statistical phrase based systems Och and Ney , 2004 ; Koehn et al , 2003 have consistently delivered state of the art performance in recent machine translation evaluations , yet these systems remain weak at handling word order changes .", "The re ordering models used in the original phrase basedsystems penalize phrase displacements proportionally to the amount of nonmonotonicity , with no con sideration of the fact that some words are far more M M D S D ! ' , .", "eue nviro nme nt m inist ers hold mee tings in l uxem burg . 01 23 45 67 8 the d evel opm ent and prog ress of the regi on . D M D D b a Figure 1 Phase orientations monotone , swap , discontin uous for Chinese to English translation .", "While previouswork reasonably models phrase reordering in simple ex amples a , it fails to capture more complex reorderings , such as the swapping of ? of the region ?", "b . likely to be displaced than others e . g . , in English to Japanese translation , a verb should typically move to the end of the clause .", "Recent efforts Tillman , 2004 ; Och et al , 2004 ; Koehn et al , 2007 have directly addressed this issue by introducing lexicalized reordering models into phrase based systems , which condition reordering probabilities on the words of each phrase pair .", "These models distinguish three orientations with respect to the previous phrase ? monotone M , swap S , anddiscontinuous D ? and as such are primarily de signed to handle local re orderings of neighboring phrases .", "Fig .", "1 a is an example where such a modeleffectively swaps the prepositional phrase in Luxembourg with a verb phrase , and where the noun min isters remains in monotone order with respect to the previous phrase EU environment .", "While these lexicalized re ordering models have shown substantial improvements over unlexicalized phrase based systems , these models only have a 848limited ability to capture sensible long distance re orderings , as can be seen in Fig .", "1 b .", "The phrase of the region should swap with the rest of the noun phrase , yet these previous approaches are unable to model this movement , and assume the orientation of this phrase is discontinuous D .", "Observe that , in a shortened version of the same sentence withoutand progress , the phrase orientation would be different S , even though the shortened version has es sentially the same sentence structure .", "Coming from the other direction , such observations about phrase reordering between different languages are precisely the kinds of facts that parsing approaches to machinetranslation are designed to handle and do success fully handle Wu , 1997 ; Melamed , 2003 ; Chiang , 2005 .", "In this paper , we introduce a novel orientationmodel for phrase based systems that aims to bet ter capture long distance dependencies , and that presents a solution to the problem illustrated in Fig .", "1 b .", "In this example , our reordering modeleffectively treats the adjacent phrases the develop ment and and progress as one single phrase , and the displacement of of the region with respect to thisphrase can be treated as a swap .", "To be able iden tify that adjacent blocks e . g . , the development and and progress can be merged into larger blocks , ourmodel infers binary non linguistic trees reminis cent of Wu , 1997 ; Chiang , 2005 .", "Crucially , our work distinguishes itself from previous hierarchical models in that it does not rely on any cubic timeparsing algorithms such as CKY used in , e . g . , Chiang , 2005 or the Earley algorithm used in Watan abe et al , 2006 .", "Since our reordering model doesnot attempt to resolve natural language ambiguities , we can effectively rely on linear time shiftreduce parsing , which is done jointly with left toright phrase based beam decoding and thus intro duces no asymptotic change in running time .", "Assuch , the hierarchical model presented in this paper maintains all the effectiveness and speed advantages of statistical phrase based systems , while be ing able to capture some key linguistic phenomena presented later in this paper which have motivated the development of parsing based approaches .", "We also illustrate this with results that are significantly better than previous approaches , in particular the lexical reordering models of Moses , a widely used phrase based SMT system Koehn et al , 2007 . This paper is organized as follows the train ing of lexicalized re ordering models is described in Section 3 .", "In Section 4 , we describe how to combine shift reduce parsing with left to right beamsearch phrase based decoding with the same asymptotic running time as the original phrase based decoder .", "We finally show in Section 6 that our ap proach yields results that are significantly better thanprevious approaches for two language pairs and dif ferent test sets .", "We compare our re ordering model with related work Tillman , 2004 ; Koehn et al , 2007 using alog linear approach common to many state of the art statistical machine translation systems Och and Ney , 2004 .", "Given an input sentence f , which is to be translated into a target sentence e , the decodersearches for the most probable translation e ?", "accord ing to the following decision rule e ?", "argmax e p e f 1 argmax e J ? j 1 ? jh j f , e 2 h j f , e are J arbitrary feature functions over sentence pairs .", "These features include lexicalized re ordering models , which are parameterized as follows given an input sentence f , a sequence of target language phrases e e1 , . . .", ", en currently hypothesized by the decoder , and a phrase alignment a a1 , . . .", ", an that defines a source f ai for eachtranslated phrase ei , these models estimate the prob ability of a sequence of orientations o o1 , . . .", ", on p o e , f n ? i 1 p oi ei , f ai , ai ? 1 , ai , 3 where each oi takes values over the set of possi ble orientations O M , S , D . 1 The probability is conditioned on both ai ? 1 and ai to make sure that the label oi is consistent with the phrase alignment .", "Specifically , probabilities in these models can be 1We note here that the parameterization and terminology in Tillman , 2004 is slightly different .", "We purposely ignore thesedifferences in order to enable a direct comparison between Till man ? s , Moses ? , and our approach .", "b i . . .", "b i . .", "a b c b i s u v u v uv s s Figure 2 Occurrence of a swap according to the threeorientation models word based , phrase based , and hier archical .", "Black squares represent word alignments , and gray squares represent blocks identified by phrase extract . In a , block bi ei , fai is recognized as a swap accord ing to all three models .", "In b , bi is not recognized as a swap by the word based model .", "In c , bi is recognized as a swap only by the hierarchical model . greater than zero only if one of the following con ditions is true ? oi M and ai ? ai ? 1 1 ? oi S and ai ? ai ? 1 ? 1 ? oi D and ai ? ai ? 1 6 1At decoding time , rather than using the log probability of Eq .", "3 as single feature function , we follow the approach of Moses , which is to assign three distinct parameters ? m , ? s , ? d for the three feature functions ? fm ? ni 1 log p oi M . . .", "fs ? ni 1 log p oi S . . .", "fd ? ni 1 log p oi D . . . .", "There are two key differences between this work and previous orientation models Tillman , 2004 ; Koehn et al , 2007 1 the estimation of factors in Eq .", "3 from data ; 2 the segmentation of e and f into phrases , which is static in the case of Tillman , 2004 ; Koehn et al , 2007 , while it is dynamically updatedwith hierarchical phrases in our case .", "These differ ences are described in the two next sections .", "We present here three approaches for computingp oi ei , f ai , ai ? 1 , ai on word aligned data using rel ative frequency estimates .", "We assume here that phrase ei spans the word range s , . . .", ", t in the target sentence e and that the phrase f ai spans the range ORIENTATION MODEL oi M oi S oi D word based Moses 0 . 1750 0 . 0159 0 . 8092 phrase based 0 . 3192 0 . 0704 0 . 6104 hierarchical 0 . 4878 0 . 1004 0 . 4116Table 1 Class distributions of the three orientation mod els , estimated from 12M words of Chinese English data using the grow diag alignment symmetrization heuristic implemented in Moses , which is similar to the ? refined ?", "heuristic of Och and Ney , 2004 .", "u , . . .", ", v in the source sentence f . All phrase pairs inthis paper are extracted with the phrase extract algo rithm Och and Ney , 2004 , with maximum length set to 7 . Word based orientation model This model an alyzes word alignments at positions s ? 1 , u ? 1 and s ? 1 , v 1 in the alignment grid shown in Fig .", "2 a .", "Specifically , orientation is set to oi M if s ? 1 , u ? 1 contains a word alignment and s ? 1 , v 1 contains no word alignment .", "It is set to oi S if s ? 1 , u ? 1 contains no word alignment and s ? 1 , v 1 contains a word alignment .", "In all other cases , it is set to oi D . This procedure is exactly the same as the one implemented in Moses . 2 Phrase based orientation model The modelpresented in Tillman , 2004 is similar to the word based orientation model presented above , except that it analyzes adjacent phrases rather than specificword alignments to determine orientations .", "Specif ically , orientation is set to oi M if an adjacent phrase pair lies at s ? 1 , u ? 1 in the alignmentgrid .", "It is set to S if an adjacent phrase pair cov ers s ? 1 , v 1 as shown in Fig .", "2 b , and is set to D otherwise . Hierarchical orientation model This model analyzes alignments beyond adjacent phrases .", "Specifically , orientation is set to oi M if the phrase extract algorithm is able to extract a phrase pair at s ? 1 , u ? 1 given no constraint on maximum phrase length .", "Orientation is S if the same is true at s ? 1 , v 1 , and orientation is D otherwise . Table 1 displays overall class distributions according to the three models .", "It appears clearly that occurrences of M and S are too sparsely seen in the word based model , which assigns more than 80 of its 2http www . statmt . org moses ? n Moses . AdvancedFeatures 850 word phrase hier .", "Monotone with previous p oi M ei , f ai , ai ? 1 , ai 1 , 4 and is 0 . 223 0 . 672 0 . 942 2 , and also 0 . 201 0 . 560 0 . 948 Swap with previous p oi S ei , f ai , ai ? 1 , ai 3 ? of china 0 . 303 0 . 617 0 . 651 4 ? ?", ", he said 0 . 003 0 . 030 0 . 395 Monotone with next p oi M ei , f ai , ai 1 , ai 5 ? ? ?", ", he pointed out that 0 . 601 0 . 770 0 . 991 6 l , however , 0 . 517 0 . 728 0 . 968 Swap with next p oi S ei , f ai , ai 1 , ai 7 0 the development of 0 . 145 0 . 831 0 . 900 8 ? at the invitation of 0 . 272 0 . 834 0 . 925 Table 2 Monotone and swap probabilities for specific phrases according to the three models word , phrase , and hierarchical .", "To ensure probabilities are representative , we only selected phrase pairs that occur at least 100 times in the training data .", "probability mass to D . Conversely , the hierarchical model counts considerably less discontinuous cases , and is the only model that accounts for the fact that real data is predominantly monotone . Since D is a rather uninformative default cat egory that gives no clue how a particular phraseshould be displaced , we will also provide MT evalu ation scores in Section 6 for a set of classes that distinguishes between left and right discontinuity M , S , Dl , Dr , a choice that is admittedly more lin guistically motivated . Table 2 displays orientation probabilities for con crete examples .", "Each example was put under one of the four categories that linguistically seems thebest match , and we provide probabilities for that cat egory according to each model .", "Note that , whilewe have so far only discussed left to right reorder ing models , it is also possible to build right to leftmodels by substituting ai ? 1 with ai 1 in Eq .", "Ex amples for right to left models appear in the second half of the table .", "The table strongly suggests that the hierarchical model more accurately determinesthe orientation of phrases with respect to large contextual blocks .", "In Examples 1 and 2 , the hierarchi cal model captures the fact that coordinated clauses almost always remain in the same order , and that words should generally be forbidden to move from one side of ? and ?", "to the other side , a constraint thatis difficult to enforce with the other two reorder ing models .", "In Example 4 , the first two models completely ignore that ? he said ?", "sometimes rotates around its neighbor clause .", "Computing reordering scores during decoding with word based3 and phrase based models Tillman , 2004 is trivial , since they only make use of localinformation to determine the orientation of a new in coming block bi .", "For a left to right ordering model , bi is scored based on its orientation with respect to bi ? 1 .", "For instance , if bi has a swap orientation withrespect to the previous phrase in the current translation hypothesis , feature p oi S . . .", "becomes ac tive .", "Computing lexicalized reordering scores with the hierarchical model is more complex , since the model must identify contiguous blocks ? monotone or swapping ? that can be merged into hierarchical blocks .", "The employed method is an instance of thewell known shift reduce parsing algorithm , and re lies on a stack S of foreign substrings that have already been translated .", "Each time the decoder adds a new block to the current translation hypothesis , it shifts the source language indices of the block ontoS , then repeatedly tries reducing the top two ele ments of S if they are contiguous . 4 This parsingalgorithm was first applied in computational geome try to identify convex hulls Graham , 1972 , and its running time was shown to be linear in the length of the sequence a proof is presented in Huang et al . , 2008 , which applies the same algorithm to the binarization of SCFG rules .", "Figure 3 provides an example of the execution of this algorithm for the translation output shownin Figure 4 , which was produced by a decoder in corporating our hierarchical reordering model .", "The decoder successively pushes source language spans 1 , 2 , 3 , which are successively merged into 1 3 , and all correspond to monotone orientations . 3We would like to point out an inconsistency in Moses be tween training and testing .", "Despite the fact that Moses estimates a word based orientation model during training i . e . , it analyzes the orientation of a given phrase with respect to adjacent wordalignments , this model is then treated as a phrase based orien tation model during testing i . e . , as a model that orients phrases with respect to other phrases .", "4It is not needed to store target language indices onto thestack , since the decoder proceeds left to right , and thus suc cessive blocks are always contiguous with respect to the target language .", "851 Target phrase Source Op .", "oi Stack the russian side 1 S M hopes 2 R M 1 to 3 R M 1 2 hold 11 S D 1 3 consultations 12 R M 11 , 1 3 with iran 9 10 R S 11 12 , 1 3 on this 6 7 S D 9 12 , 1 3 issue 8 R , R M 6 7 , 9 12 , 1 3 in the near future 4 5 R , R S 6 12 , 1 3 . 13 R , A M 1 12 Figure 3 The application of the shift reduce parsing algorithm for identifying hierarchical blocks .", "This execu tion corresponds to the decoding example of Figure 4 . Operations Op .", "include shift S , reduce R , and accept A .", "The source and stack columns contain source language spans , which is the only information needed to determine whether two given blocks are contiguous .", "oi isthe label predicted by the hierarchical model by compar ing the current block to the hierarchical phrase that is at the top of the stack .", "0 12 34 56 the russi an side hope s to hold cons ultati ons with iran on this issue in the near future . . . . . . . . . . . . . . . . . .", ". . . . . . . . . . . . . . . . . h 1 h 2 h 3 Figure 4 Output of our phrase based decoder using the hierarchical model on a sentence of MT06 .", "Hierarchical phrases h1 and h2 indicate that with Iran and in the near future have a swap orientation .", "h3 indicates that ? to ?", "and ? . ?", "are monotone .", "In this particular example , distortion limit was set to 10 .", "It then encounters a discontinuity that prevents the next block 11 from being merged with 1 3 .", "As the decoder reaches the last words of the sentence in the near future , 4 5 is successively merged with 6 12 , then 1 3 , yielding a stack that contains only 1 12 .", "A nice property of this parsing algorithm is that it does not worsen the asymptotic running time of beam search decoders such as Moses Koehn , 2004a .", "Such decoders run in time O n2 , where n is the length of the input sentence .", "Indeed , each time a partial translation hypothesis is expanded intoa longer one , the decoder must perform an O n op eration in order to copy the coverage set indicating which foreign words have already been translated into the new hypothesis .", "Since this copy operationmust be executed O n times , the overall time complexity is quadratic .", "The incorporation of the shift reduce parser into such a decoder does not worsenoverall time complexity whenever the decoder expands a given partial translation into a longer hy pothesis , it simply copies its stack into the newlycreated hypothesis similarly to copying the cover age vector , this is an O n operation .", "Hence , the incorporation of the hierarchical models described in the paper into a phrase based decoder preserves the O n2 running time .", "In practice , we observe based on a set of experiments for Chinese English and Arabic English translation that our phrase based decoder is on average only 1 . 35 times slower when it is running using hierarchical reordering features and the shift reduce parser . We finally note that the decoding algorithm presented in this section can only be applied left to right if the decoder itself is operating left to right . In order to predict orientations relative to the rightto left hierarchical reordering model , we must resort to approximations at decoding time .", "We experi mented with different approximations , and the one that worked best in the experiments discussed in Section 6 is described as follows .", "First , we note that an analysis of the alignment grid often reveals that certain orientations are impossible .", "For instance , the block issue in Figure 4 can only have discontinuousorientation with respect to what comes next in En glish , since words surrounding the Chinese phrasehave already been translated .", "When several hier archical orientations are possible according to thealignment grid , we choose according to the follow ing order of preference 1 monotone , 2 swap , 3 discontinuous .", "For instance , in the case of with iranin Figure 4 , only swap and discontinuous orientations are possible monotone orientation is impossi ble because of the block hold consultations , hence we give preference to swap .", "This prediction turns out to be the correct one according to the decoding 852 steps that complete the alignment grid .", "We now analyze the system output of Figure 4 to fur ther motivate the hierarchical model , this time from the perspective of the decoder .", "We first observe that the prepositional phrase in the future should rotatearound a relatively large noun phrase headed by consultations .", "Unfortunately , localized reordering models such as Tillman , 2004 have no means of identifying that such a displacement is a swap S .", "Accord ing to these models , the orientation of in the futurewith respect to what comes previously is discontinuous D , which is an uninformative fall back category .", "By identifying h2 hold . . . issue as a hierarchical block , the hierarchical model can properly deter mine that the block in the near future should have a swap orientation . 5 Similar observations can be made regarding blocks h1 and h3 , which leads our model to predict either monotone orientation between h3and ? to ?", "and between h3 and ? . ?", "or swap orienta tion between h1 and with Iran while local models would predict discontinuous in all cases .", "Another benefit of the hierarchical model is thatits representation of phrases remains the same dur ing both training and decoding , which is not the casefor word based and phrase based reordering mod els .", "The deficiency of these local models lies in thefact that blocks handled by phrase based SMT sys tems tend to be long at training time and short attest time , which has adverse consequences on nonhierarchical reordering models .", "For instance , in Fig ure 4 , the phrase based reordering model categorizes the block in the near future as discontinuous , though if the sentence pair had been a training example , this block would count as a swap because of the ex tracted phrase on this issue .", "In our experiments , we use a re implementationof the Moses decoder Koehn et al , 2007 .", "Except for lexical reordering models , all other fea tures are standard features implemented almost5Note that the hierarchical phrase hold . . . issue is not a well formed syntactic phrase ? i . e . , it neither matches the bracketing of the verb phrase hold . . . future nor matches the noun phrase consultations . . . issue ? yet it enables sensible reordering .", "exactly as in Moses four translation features phrase based translation probabilities and lexically weighted probabilities , word penalty , phrase penalty , linear distortion , and language model score . We experiment with two language pairs Chinese to English C E and Arabic to English A E .", "For C E , we trained translation models using a subset of the Chinese English parallel data released by LDC mostly news , in particular FBIS and Xinhua News .", "This subset comprises 12 . 2M English words , and 11M Chinese words .", "Chinese words are segmented with a conditional random field CRF classifier that conforms to the Chinese Treebank CTB standard .", "The training set for our A E systems also includes mostly news parallel data released by LDC , and contains 19 . 5M English words , and 18 . 7M Arabic tokens that have been segmented using the Arabic Treebank ATB Maamouri et al , 2004 standard . 6 For our language model , we trained a 5 gram model using the Xinhua and AFP sections of the Gigaword corpus LDC2007T40 , in addition to the target side of the parallel data .", "For both C E and A E , we manually removed documents of Gigaword that were released during periods that overlap with those of our development and test sets .", "The language model was smoothed with the modified Kneser Ney algorithm , and we kept only trigrams , 4 grams , and 5 grams that respectively occurred two , three , and three times in the training data .", "Parameters were tuned with minimum error rate training Och , 2003 on the NIST evaluation set of 2006 MT06 for both C E and A E .", "Since MERTis prone to search errors , especially with large num bers of parameters , we ran each tuning experimentfour times with different initial conditions .", "This pre caution turned out to be particularly important in the case of the combined lexicalized reordering models the combination of phrase based and hierarchical discussed later , since MERT must optimize up to 26 parameters at once in these cases . 7 For testing , 6Catalog numbers for C E LDC2002E18 , LDC2003E07 , LDC2003E14 , LDC2005E83 , LDC2005T06 , LDC2006E26 , and LDC2006E8 .", "For A E LDC2007E103 , LDC2005E83 , LDC2006E24 , LDC2006E34 , LDC2006E85 , LDC2006E92 , LDC2007E06 , LDC2007E101 , LDC2007E46 , LDC2007E86 , and LDC2008E40 . 7We combine lexicalized reordering models by simply treat ing them as distinct features , which incidentally increases the number of model parameters that must be tuned with MERT .", "853 30 . 5 31 31 . 5 32 32 . 5 33 33 . 5 34 0 2 4 6 8 10 12 14 BL EU , Ch ines e E ngli sh distortion limit hierarchicalphrase based word basedbaseline 43 43 . 5 44 44 . 5 45 45 . 5 0 2 4 6 8 10 BL EU , Arabic Eng lish distortion limit hierarchicalphrase based word basedbaseline Figure 5 Performance on the Chinese English andArabic English development sets MT06 with increasing distortion limits for all lexicalized reordering mod els discussed in the paper .", "Our novel hierarchical model systematically outperforms all other models for distortion limit equal to or greater than 4 .", "The baseline is Moses with no lexicalized reordering model .", "we used the NIST evaluation sets of 2005 and 2008 MT05 and MT08 for Chinese English , and the test set of 2005 MT05 for Arabic English .", "Statistical significance is computed using the approximate randomization test Noreen , 1989 , whose application to MT evaluation Riezler and Maxwell , 2005 was shown to be less sensitive totype I errors i . e . , incorrectly concluding that im provement is significant than the perhaps more widely used bootstrap resampling method Koehn , 2004b .", "Tuning set performance is shown in Figure 5 .", "Since this paper studies various ordering models , it is interesting to first investigate how the distor LEXICALIZED REORDERING MT06 MT05 MT08 none 31 . 85 29 . 75 25 . 22 word based 32 . 96 31 . 45 25 . 86 phrase based 33 . 24 31 . 23 26 . 01 hierarchical 33 . 80 32 . 20 26 . 38 phrase based hierarchical 33 . 86 32 . 85 26 . 53 Table 3 BLEU scores uncased for Chinese Englishand the orientation categories M , S , D .", "Maximum dis tortion is set to 6 words , which is the default in Moses .", "The stars at the bottom of the tables indicate when a given hierarchical model is significantly better than all localmodels for a given development or test set signifi cance at the . 05 level ; significance at the . 01 level .", "LEXICALIZED REORDERING MT06 MT05 MT08 phrase based 33 . 79 32 . 32 26 . 32 hierarchical 34 . 01 32 . 35 26 . 58 phrase based hierarchical 34 . 36 32 . 33 27 . 03 Table 4 BLEU scores uncased for Chinese English and the orientation categories M , S , Dl , Dr .", "Since the distinction between these four categories is not available in Moses , hence we have no baseline results for this case .", "Maximum distortion is set to 6 words .", "tion limit affects performance . 8 As has been shownin previous work in Chinese English and Arabic English translation , limiting phrase displacements to six source language words is a reasonable choice . For both C E and A E , the hierarchical model is sig nificantly better p ? . 05 than either other modelsfor distortion limits equal to or greater than 6 ex cept for distortion limit 12 in the case of C E .", "Since a distortion limit of 6 works reasonably well for both language pairs and is the default in Moses , we used this distortion limit value for all test set experiments presented in this paper .", "Our main results for Chinese English are shownin Table 3 .", "It appears that hierarchical models provide significant gains over all non hierarchical models .", "Improvements on MT06 and MT05 are very sig nificant p ? . 01 .", "In the case of MT08 , significant improvement is reached through the combination ofboth phrase based and hierarchical models .", "We of ten observe substantial gains when we combine such models , presumably because we get the benefit of identifying both local and long distance swaps .", "Since most orientations in the phrase based model are discontinuous , it is reasonable to ask whether8Note that we ran MERT separately for each distinct distor tion limit .", "854 LEXICALIZED REORDERING MT06 MT05 none 44 . 03 54 . 87 word based 44 . 64 54 . 96 phrase based 45 . 01 55 . 09 hierarchical 45 . 51 55 . 50 phrase based hierarchical 45 . 64 56 . 01 Table 5 BLEU scores uncased for Arabic English and the reordering categories M , S , D .", "LEXICALIZED REORDERING MT06 MT05 phrase based 44 . 74 55 . 52 hierarchical 45 . 53 56 . 02 phrase based hierarchical 45 . 63 56 . 07 Table 6 BLEU scores uncased for Arabic English and the reordering categories M , S , Dl , Dr .", "the relatively poor performance of the phrase basedmodel is the consequence of an inadequate set of ori entation labels .", "To try to answer this question , weuse the set of orientation labels M , S , Dl , Dr de scribed in Section 3 .", "Results for this different set oforientations are shown in Table 4 .", "While the phrasebased model appears to benefit more from the distinction between left and right discontinuous , sys tems that incorporate hierarchical models remain the most competitive overall their best performance on MT06 , MT05 , and MT08 are respectively 34 . 36 , 32 . 85 , and 27 . 03 .", "The best non hierarchical models achieve only 33 . 79 , 32 . 32 , and 26 . 32 , respectively . All these differences i . e . , . 57 , . 53 , and . 71 are sta tistically significant at the . 05 level . Our results for Arabic English are shown in Ta bles 5 and 6 .", "Similarly to C E , we provide results for two orientation sets M , S , D and M , S , Dl , Dr .", "We note that the four class orientation set is overall less effective for A E than for C E .", "This is probably due to the fact that there is less probability mass in A E assigned to the D category , and thus it is less helpful to split the discontinuous category into two .", "For both orientation sets , we observe in A E that the hierarchical model significantly outperforms thelocal ordering models .", "Gains provided by the hierarchical model are no less significant than for Chinese to English .", "This positive finding is perhaps a bitsurprising , since Arabic to English translation gen erally does not require many word order changes compared to Chinese to English translation , and thistranslation task so far has seldom benefited from hierarchical approaches to MT . In our case , one possi ble explanation is that Arabic English translation is benefiting from the fact that orientation predictionsof the hierarchical model are consistent across train ing and testing , which is not the case for the otherordering models discussed in this paper see Sec tion 4 .", "Overall , hierarchical models are the most effective on the two sets their best performances on MT06 and MT05 are respectively 45 . 64 and 56 . 07 .", "The best non hierarchical models obtain only 45 . 01 and 55 . 52 respectively for the same sets .", "All thesedifferences i . e . , . 63 and . 55 are statistically signifi cant at the . 05 level .", "In this paper , we presented a lexicalized orientation model that enables phrase movements that are more complex than swaps between adjacent phrases .", "This model relies on a hierarchical structure that is builtas a by product of left to right phrase based decod ing without increase of asymptotic running time .", "Weshow that this model provides statistically signifi cant improvements for five NIST evaluation sets and for two language pairs .", "In future work , we plan to extend the parameterization of our models to not only predict phrase orientation , but also the length of each displacement as in Al Onaizan and Papineni , 2006 .", "We believe such an extension would improve translation quality in the case of larger distortionlimits .", "We also plan to experiment with discriminative approaches to estimating reordering probabil ities Zens and Ney , 2006 ; Xiong et al , 2006 , whichcould also be applied to our work .", "We think the abil ity to condition reorderings on any arbitrary featurefunctions is also very effective in the case of our hi erarchical model , since information encoded in thetrees would seem beneficial to the orientation pre diction task .", "The authors wish to thank the anonymous reviewers for their comments on an earlier draft of this paper .", "This paper is based on work funded by the Defense Advanced Research Projects Agency through IBM .", "The content does not necessarily reflect the views of the U . S . Government , and no official endorsement should be inferred ."], "summary_lines": ["A Simple and Effective Hierarchical Phrase Reordering Model\n", "While phrase-based statistical machine translation systems currently deliver state-of-the- art performance, they remain weak on word order changes.\n", "Current phrase reordering models can properly handle swaps between adjacent phrases, but they typically lack the ability to perform the kind of long-distance reorderings possible with syntax-based systems.\n", "In this paper, we present a novel hierarchical phrase reordering model aimed at improving non-local reorderings, which seamlessly integrates with a standard phrase-based system with little loss of computational efficiency.\n", "We show that this model can successfully handle the key examples often used to motivate syntax-based systems, such as the rotation of a prepositional phrase around a noun phrase.\n", "We contrast our model with reordering models commonly used in phrase-based systems, and show that our approach provides statistically significant BLEU point gains for two language pairs: Chinese-English (+0.53 on MT05 and +0.71 on MT08) and Arabic-English (+0.55 on MT05).\n", "Our hierarchical orientation model captures non-local phrase reordering by a shift reduce algorithm.\n", "We introduce a deterministic shift-reduce parser into decoding, so that the decoder always has access to the largest possible previous block, given the current translation history.\n", "We introduce three orientation models for lexicalized reordering: word-based, phrase-based and hierarchical orientation model.\n"]}
{"article_lines": ["Parsing Algorithms And Metrics", "Many different metrics exist for evaluating parsing results , including Viterbi , Crossing Brackets Rate , Zero Crossing Brackets Rate , and several others .", "However , most parsing algorithms , including the Viterbi algorithm , attempt to optimize the same metric , namely the probability of getting the correct labelled tree .", "By choosing a parsing algorithm appropriate for the evaluation metric , better performance can be achieved .", "We present two new algorithms the quot ; Labelled Recall Algorithm , quot ; which maximizes the expected Labelled Recall Rate , and the quot ; Bracketed Recall Algorithm , quot ; which maximizes the Bracketed Recall Rate .", "Experimental results are given , showing that the two new algorithms have improved performance over the Viterbi algorithm on many criteria , especially the ones that they optimize", "In corpus based approaches to parsing , one is given a treebank a collection of text annotated with the quot ; correct quot ; parse tree and attempts to find algorithms that , given unlabelled text from the treebank , produce as similar a parse as possible to the one in the treebank .", "Various methods can be used for finding these parses .", "Some of the most common involve inducing Probabilistic Context Free Grammars PCFGs , and then parsing with an algorithm such as the Labelled Tree Viterbi Algorithm , which maximizes the probability that the output of the parser the quot ; guessed quot ; tree is the one that the PCFG produced .", "This implicitly assumes that the induced PCFG does a good job modeling the corpus .", "There are many different ways to evaluate these parses .", "The most common include the Labelled Tree Rate also called the Viterbi Criterion or Exact Match Rate , Consistent Brackets Recall Rate also called the Crossing Brackets Rate , Consistent Brackets Tree Rate also called the Zero Crossing Brackets Rate , and Precision and Recall .", "Despite the variety of evaluation metrics , nearly all researchers use algorithms that maximize performance on the Labelled Tree Rate , even in domains where they are evaluating using other criteria .", "We propose that by creating algorithms that optimize the evaluation criterion , rather than some related criterion , improved performance can be achieved .", "In Section 2 , we define most of the evaluation metrics used in this paper and discuss previous approaches .", "Then , in Section 3 , we discuss the Labelled Recall Algorithm , a new algorithm that maximizes performance on the Labelled Recall Rate .", "In Section 4 , we discuss another new algorithm , the Bracketed Recall Algorithm , that maximizes performance on the Bracketed Recall Rate closely related to the Consistent Brackets Recall Rate .", "Finally , we give experimental results in Section 5 using these two algorithms in appropriate domains , and compare them to the Labelled Tree Viterbi Algorithm , showing that each algorithm generally works best when evaluated on the criterion that it optimizes .", "In this section , we first define basic terms and symbols .", "Next , we define the different metrics used in evaluation .", "Finally , we discuss the relationship of these metrics to parsing algorithms .", "Let wa denote word a of the sentence under consideration .", "Let wab denote wa wa 1 . . . tvb _ ; in particular let writ denote the entire sequence of terminals words in the sentence under consideration .", "In this paper we assume all guessed parse trees are binary branching .", "Let a parse tree T be defined as a set of triples s , t , X where s denotes the position of the first symbol in a constituent , t denotes the position of the last symbol , and X represents a terminal or nonterminal symbol meeting the following three requirements Let Tc denote the quot ; correct quot ; parse the one in the treebank and let TG denote the quot ; guessed quot ; parse the one output by the parsing algorithm .", "Let NG denote ITGI , the number of nonterminals in the guessed parse tree , and let Nc denote ITc I , the number of nonterminals in the correct parse tree .", "There are various levels of strictness for determining whether a constituent element of TG is quot ; correct . quot ; The strictest of these is Labelled Match .", "A constituent s , t , X E TG is correct according to Labelled Match if and only if s , t , X E T . In other words , a constituent in the guessed parse tree is correct if and only if it occurs in the correct parse tree .", "The next level of strictness is Bracketed Match .", "Bracketed match is like labelled match , except that the nonterminal label is ignored .", "Formally , a constituent s , t , X E TG is correct according to Bracketed Match if and only if there exists a Y such that s , t , Y E T . The least strict level is Consistent Brackets also called Crossing Brackets .", "Consistent Brackets is like Bracketed Match in that the label is ignored .", "It is even less strict in that the observed s , t , X need not be in Tc it must simply not be ruled out by any q , r , Y E Tc .", "A particular triple q , r , Y rules out s , t , X if there is no way that s , t , X and q , r , Y could both be in the same parse tree .", "In particular , if the interval s , t crosses the interval q , r , then s , t , X is ruled out and counted as an error .", "Formally , we say that s , t crosses q , r if and only ifs q t rorq s r t .", "If Tc is binary branching , then Consistent Brackets and Bracketed Match are identical .", "The following symbols denote the number of constituents that match according to each of these criteria . crossing s , t the number of constituents in TG correct according to Consistent Brackets .", "Following are the definitions of the six metrics used in this paper for evaluating binary branching trees 5 Consistent Brackets Recall Rate CING .", "It is often called the Crossing Brackets Rate .", "In the case where the parses are binary branching , this criterion is the same as the Bracketed Recall Rate .", "6 Consistent Brackets Tree Rate 1 if C NG .", "This metric is closely related to the Bracketed Tree Rate .", "In the case where the parses are binary branching , the two metrics are the same .", "This criterion is also called the Zero Crossing Brackets Rate .", "The preceding six metrics each correspond to cells in the following table Despite this long list of possible metrics , there is only one metric most parsing algorithms attempt to maximize , namely the Labelled Tree Rate .", "That is , most parsing algorithms assume that the test corpus was generated by the model , and then attempt to evaluate the following expression , where E denotes the expected value operator TG arg mTaxE 1 if Nc 1 This is true of the Labelled Tree Algorithm and stochastic versions of Earley's Algorithm Stolcke , 1993 , and variations such as those used in Picky parsing Magerman and Weir , 1992 .", "Even in probabilistic models not closely related to PCFGs , such as Spatter parsing Magerman , 1994 , expression 1 is still computed .", "One notable exception is Brill's Transformation Based Error Driven system Brill , 1993 , which induces a set of transformations designed to maximize the Consistent Brackets Recall Rate .", "However , Brill's system is not probabilistic .", "Intuitively , if one were to match the parsing algorithm to the evaluation criterion , better performance should be achieved .", "Ideally , one might try to directly maximize the most commonly used evaluation criteria , such as Consistent Brackets Recall Crossing Brackets Rate .", "Unfortunately , this criterion is relatively difficult to maximize , since it is time consuming to compute the probability that a particular constituent crosses some constituent in the correct parse .", "On the other hand , the Bracketed Recall and Bracketed Tree Rates are easier to handle , since computing the probability that a bracket matches one in the correct parse is inexpensive .", "It is plausible that algorithms which optimize these closely related criteria will do well on the analogous Consistent Brackets criteria .", "When building an actual system , one should use the metric most appropriate for the problem .", "For instance , if one were creating a database query system , such as an ATIS system , then the Labelled Tree Viterbi metric would be most appropriate .", "A single error in the syntactic representation of a query will likely result in an error in the semantic representation , and therefore in an incorrect database query , leading to an incorrect result .", "For instance , if the user request quot ; Find me all flights on Tuesday quot ; is misparsed with the prepositional phrase attached to the verb , then the system might wait until Tuesday before responding a single error leads to completely incorrect behavior .", "Thus , the Labelled Tree criterion is appropriate .", "On the other hand , consider a machine assisted translation system , in which the system provides translations , and then a fluent human manually edits them .", "Imagine that the system is given the foreign language equivalent of quot ; His credentials are nothing which should be laughed at , quot ; and makes the single mistake of attaching the relative clause at the sentential level , translating the sentence as quot ; His credentials are nothing , which should make you laugh . quot ; While the human translator must make some changes , he certainly needs to do less editing than he would if the sentence were completely misparsed .", "The more errors there are , the more editing the human translator needs to do .", "Thus , a criterion such as the Labelled Recall criterion is appropriate for this task , where the number of incorrect constituents correlates to application performance .", "Consider writing a parser for a domain such as machine assisted translation .", "One could use the Labelled Tree Algorithm , which would maximize the expected number of exactly correct parses .", "However , since the number of correct constituents is a better measure of application performance for this domain than the number of correct trees , perhaps one should use an algorithm which maximizes the Labelled Recall criterion , rather than the Labelled Tree criterion .", "The Labelled Recall Algorithm finds that tree TG which has the highest expected value for the Labelled Recall Rate , LINc where L is the number of correct labelled constituents , and Nc is the number of nodes in the correct parse .", "This can be written as follows It is not immediately obvious that the maximization of expression 2 is in fact different from the maximization of expression 1 , but a simple example illustrates the difference .", "The following grammar generates four trees with equal probability For the first tree , the probabilities of being correct are S 100 ; A 50 ; and C 25 .", "Similar counting holds for the other three .", "Thus , the expected value of L for any of these trees is 1 . 75 .", "On the other hand , the optimal Labelled Recall parse is This tree has 0 probability according to the grammar , and thus is non optimal according to the Labelled Tree Rate criterion .", "However , for this tree the probabilities of each node being correct are S 100 ; A 50 ; and 8 50 .", "The expected value of L is 2 . 0 , the highest of any tree .", "This tree therefore optimizes the Labelled Recall Rate .", "We now derive an algorithm for finding the parse that maximizes the expected Labelled Recall Rate .", "We do this by expanding expression 2 out into a probabilistic form , converting this into a recursive equation , and finally creating an equivalent dynamic programming algorithm .", "We begin by rewriting expression 2 , expanding out the expected value operator , and removing the A , B , C , D , E , F The four trees are c , which is the same for all TG , and so plays no role in the maximization .", "Now , given a PCFG with start symbol S , the following equality holds By rearranging the summation in expression 5 and then substituting this equality , we get At this point , it is useful to introduce the Inside and Outside probabilities , due to Baker 1979 , and explained by Lan and Young 1990 .", "The Inside probability is defined as e s , t , X P X Os and the Outside probability is f s , t , X P S 3 1 X n W1 wt 1 1 Note that while Baker and others have used these probabilites for inducing grammars , here they are used only for parsing .", "Let us define a new function , g s , t , X .", "Now , the definition of a Labelled Recall Parse can be rewritten as Given the matrix g s , t , X , it is a simple matter of dynamic programming to determine the parse that maximizes the Labelled Recall criterion .", "Define loop over nonterminals X let max_g maximum of g s , t , X loop over r such that s r t let best_split max of maxc s , r maxc r 1 , t maxc s , t max_g best_split ; It is clear that MAXC 1 , n contains the score of the best parse according to the Labelled Recall criterion .", "This equation can be converted into the dynamic programming algorithm shown in Figure 1 .", "For a grammar with r rules and k nonterminals , the run time of this algorithm is 0 n3 kn2 since there are two layers of outer loops , each with run time at most n , and an inner loop , over nonterminals and n . However , this is dominated by the computation of the Inside and Outside probabilities , which takes time 0 rn3 .", "By modifying the algorithm slightly to record the actual split used at each node , we can recover the best parse .", "The entry maxc 1 , n contains the expected number of correct constituents , given the model .", "The Labelled Recall Algorithm maximizes the expected number of correct labelled constituents .", "However , many commonly used evaluation metrics , such as the Consistent Brackets Recall Rate , ignore labels .", "Similarly , some grammar induction algorithms , such as those used by Pereira and Schabes 1992 do not produce meaningful labels .", "In particular , the Pereira and Schabes method induces a grammar from the brackets in the treebank , ignoring the labels .", "While the induced grammar has labels , they are not related to those in the treebank .", "Thus , although the Labelled Recall Algorithm could be used in these domains , perhaps maximizing a criterion that is more closely tied to the domain will produce better results .", "Ideally , we would maximize the Consistent Brackets Recall Rate directly .", "However , since it is time consuming to deal with Consistent Brackets , we instead use the closely related Bracketed Recall Rate .", "For the Bracketed Recall Algorithm , we find the parse that maximizes the expected Bracketed Recall Rate , BINc .", "Remember that B is the number of brackets that are correct , and Nc is the number of constituents in the correct parse .", "The algorithm for Bracketed Recall parsing is extremely similar to that for Labelled Recall parsing .", "The only required change is that we sum over the symbols X to calculate max_g , rather than maximize over them .", "We describe two experiments for testing these algorithms .", "The first uses a grammar without meaningful nonterminal symbols , and compares the Bracketed Recall Algorithm to the traditional Labelled Tree Viterbi Algorithm .", "The second uses a grammar with meaningful nonterminal symbols and performs a three way comparison between the Labelled Recall , Bracketed Recall , and Labelled Tree Algorithms .", "These experiments show that use of an algorithm matched appropriately to the evaluation criterion can lead to as much as a 10 reduction in error rate .", "In both experiments the grammars could not parse some sentences , 0 . 5 and 9 , respectively .", "The unparsable data were assigned a right branching structure with their rightmost element attached high .", "Since all three algorithms fail on the same sentences , all algorithms were affected equally .", "5 . 1 Experiment with Grammar Induced by Pereira and Schabes Method The experiment of Pereira and Schabes 1992 was duplicated .", "In that experiment , a grammar was trained from a bracketed form of the TI section of the ATIS corpus' using a modified form of the InsideOutside Algorithm .", "Pereira and Schabes then used the Labelled Tree Algorithm to select the best parse for sentences in held out test data .", "The experiment was repeated here , except that both the Labelled Tree and Labelled Recall Algorithm were run for each sentence .", "In contrast to previous research , we repeated the experiment ten times , with different training set , test set , and initial conditions each time .", "Table 1 shows the results of running this experiment , giving the minimum , maximum , mean , and standard deviation for three criteria , Consistent Brackets Recall , Consistent Brackets Tree , and sus Bracketed Recall for Pereira and Schabes Bracketed Recall .", "We also display these statistics for the paired differences between the algorithms The only statistically significant difference is that for Consistent Brackets Recall Rate , which was significant to the 2 significance level paired t test .", "Thus , use of the Bracketed Recall Algorithm leads to a 10 reduction in error rate .", "In addition , the performance of the Bracketed Recall Algorithm was also qualitatively more appealing .", "Figure 2 shows typical results .", "Notice that the Bracketed Recall Algorithm's Consistent Brackets Rate versus iteration is smoother and more nearly monotonic than the Labelled Tree Algorithm's .", "The Bracketed Recall Algorithm also gets off to a much faster start , and is generally although not always above the Labelled Tree level .", "For the Labelled Tree Rate , the two are usually very comparable .", "The replication of the Pereira and Schabes experiment was useful for testing the Bracketed Recall Algorithm .", "However , since that experiment induces a grammar with nonterminals not comparable to those in the training , a different experiment is needed to evaluate the Labelled Recall Algorithm , one in which the nonterminals in the induced grammar are the same as the nonterminals in the test set .", "For this experiment , a very simple grammar was induced by counting , using a portion of the Penn Tree Bank , version 0 . 5 .", "In particular , the trees were first made binary branching by removing epsilon productions , collapsing singleton productions , and converting n ary productions n 2 as in figure 3 .", "The resulting trees were treated as the quot ; Correct quot ; trees in the evaluation .", "Only trees with forty or fewer symbols were used in this experiment .", "A grammar was then induced in a straightforward way from these trees , simply by giving one count for each observed production .", "No smoothing was done .", "There were 1805 sentences and 38610 nonterminals in the test data .", "Table 2 shows the results of running all three algorithms , evaluating against five criteria .", "Notice that for each algorithm , for the criterion that it optimizes it is the best algorithm .", "That is , the Labelled Tree Algorithm is the best for the Labelled Tree Rate , the Labelled Recall Algorithm is the best for the Labelled Recall Rate , and the Bracketed Recall Algorithm is the best for the Bracketed Recall Rate .", "Matching parsing algorithms to evaluation criteria is a powerful technique that can be used to improve performance .", "In particular , the Labelled Recall Algorithm can improve performance versus the Labelled Tree Algorithm on the Consistent Brackets , Labelled Recall , and Bracketed Recall criteria .", "Similarly , the Bracketed Recall Algorithm improves performance versus Labelled Tree on Consistent Brackets and Bracketed Recall criteria .", "Thus , these algorithms improve performance not only on the measures that they were designed for , but also on related criteria .", "Furthermore , in some cases these techniques can make parsing fast when it was previously impractical .", "We have used the technique outlined in this paper in other work Goodman , 1996 to efficiently parse the DOP model ; in that model , the only previously known algorithm which summed over all the possible derivations was a slow Monte Carlo algorithm Bod , 1993 .", "However , by maximizing the Labelled Recall criterion , rather than the Labelled Tree criterion , it was possible to use a much simpler algorithm , a variation on the Labelled Recall Algorithm .", "Using this technique , along with other optimizations , we achieved a 500 times speedup .", "In future work we will show the surprising result that the last element of Table 3 , maximizing the Bracketed Tree criterion , equivalent to maximizing performance on Consistent Brackets Tree Zero Crossing Brackets Rate in the binary branching case , is NP complete .", "Furthermore , we will show that the two algorithms presented , the Labelled Recall Algorithm and the Bracketed Recall Algorithm , are both special cases of a more general algorithm , the General Recall Algorithm .", "Finally , we hope to extend this work to the n ary branching case .", "I would like to acknowledge support from National Science Foundation Grant IRI 9350192 , National Science Foundation infrastructure grant CDA 9401024 , and a National Science Foundation Graduate Student Fellowship .", "I would also like to thank Stanley Chen , Andrew Kehler , Lillian Lee , and Stuart Shieber for helpful discussions , and comments on earlier drafts , and the anonymous reviewers for their comments ."], "summary_lines": ["Parsing Algorithms And Metrics\n", "Many different metrics exist for evaluating parsing results, including Viterbi, Crossing Brackets Rate, Zero Crossing Brackets Rate, and several others.\n", "However, most parsing algorithms, including the Viterbi algorithm, attempt to optimize the same metric, namely the probability of getting the correct labelled tree.\n", "By choosing a parsing algorithm appropriate for the evaluation metric, better performance can be achieved.\n", "We present two new algorithms: the \"Labelled Recall Algorithm,\" which maximizes the expected Labelled Recall Rate, and the \"Bracketed Recall Algorithm,\" which maximizes the Bracketed Recall Rate.\n", "Experimental results are given, showing that the two new algorithms have improved performance over the Viterbi algorithm on many criteria, especially the ones that they optimize.\n", "We observe that the Viterbi parse is in general not the optimal parse for evaluation metrics such as f-score that are based on the number of correct constituents in a parse.\n"]}
{"article_lines": ["OntoNotes The 90 Solution", "hovy mitch martha . palmer lance . ramshaw weischedel isi . edu cis . upenn . edu colorado . edu bbn . com bbn . com We describe the OntoNotes methodology and its result , a large multilingual richly annotated corpus constructed at 90 interannotator agreement .", "An initial portion 300K words of English newswire and 250K words of Chinese newswire will be made available to the community during 2007 .", "We describe the OntoNotes methodology and its result , a large multilingual richly annotated corpus constructed at 90 interannotator agreement .", "An initial portion 300K words of English newswire and 250K words of Chinese newswire will be made available to the community during 2007 .", "Many natural language processing applications could benefit from a richer model of text meaning than the bag of words and n gram models that currently predominate .", "Until now , however , no such model has been identified that can be annotated dependably and rapidly .", "We have developed a methodology for producing such a corpus at 90 inter annotator agreement , and will release completed segments beginning in early 2007 .", "The OntoNotes project focuses on a domain independent representation of literal meaning that includes predicate structure , word sense , ontology linking , and coreference .", "Pilot studies have shown that these can all be annotated rapidly and with better than 90 consistency .", "Once a substantial and accurate training corpus is available , trained algorithms can be developed to predict these structures in new documents .", "This process begins with parse TreeBank and propositional PropBank structures , which provide normalization over predicates and their arguments .", "Word sense ambiguities are then resolved , with each word sense also linked to the appropriate node in the Omega ontology .", "Coreference is also annotated , allowing the entity mentions that are propositional arguments to be resolved in context .", "Annotation will cover multiple languages English , Chinese , and Arabic and multiple genres newswire , broadcast news , news groups , weblogs , etc .", ", to create a resource that is broadly applicable .", "The Penn Treebank Marcus et al . , 1993 is annotated with information to make predicate argument structure easy to decode , including function tags and markers of empty categories that represent displaced constituents .", "To expedite later stages of annotation , we have developed a parsing system Gabbard et al . , 2006 that recovers both of these latter annotations , the first we know of .", "A firststage parser matches the Collins 2003 parser on which it is based on the Parseval metric , while simultaneously achieving near state of the art performance on recovering function tags F measure 89 . 0 .", "A second stage , a seven stage pipeline of maximum entropy learners and voted perceptrons , achieves state of the art performance F measure 74 . 7 on the recovery of empty categories by combining a linguistically informed architecture and a rich feature set with the power of modern machine learning methods .", "The Penn Proposition Bank , funded by ACE DOD , focuses on the argument structure of verbs , and provides a corpus annotated with semantic roles , including participants traditionally viewed as arguments and adjuncts .", "The 1M word Penn Treebank II Wall Street Journal corpus has been successfully annotated with semantic argument structures for verbs and is now available via the Penn Linguistic Data Consortium as PropBank I Palmer et al . , 2005 .", "Links from the argument labels in the Frames Files to FrameNet frame elements and VerbNet thematic roles are being added .", "This style of annotation has also been successfully applied to other genres and languages .", "Word sense ambiguity is a continuing major obstacle to accurate information extraction , summarization and machine translation .", "The subtle finegrained sense distinctions in WordNet have not lent themselves to high agreement between human annotators or high automatic tagging performance .", "Building on results in grouping fine grained WordNet senses into more coarse grained senses that led to improved inter annotator agreement ITA and system performance Palmer et al . , 2004 ; Palmer et al . , 2006 , we have developed a process for rapid sense inventory creation and annotation that includes critical links between the grouped word senses and the Omega ontology Philpot et al . , 2005 ; see Section 5 below .", "This process is based on recognizing that sense distinctions can be represented by linguists in an hierarchical structure , similar to a decision tree , that is rooted in very coarse grained distinctions which become increasingly fine grained until reaching WordNet senses at the leaves .", "Sets of senses under specific nodes of the tree are grouped together into single entries , along with the syntactic and semantic criteria for their groupings , to be presented to the annotators .", "As shown in Figure 1 , a 50 sentence sample of instances is annotated and immediately checked for inter annotator agreement .", "ITA scores below 90 lead to a revision and clarification of the groupings by the linguist .", "It is only after the groupings have passed the ITA hurdle that each individual group is linked to a conceptual node in the ontology .", "In addition to higher accuracy , we find at least a threefold increase in annotator productivity .", "As part of OntoNotes we are annotating the most frequent noun and verb senses in a 300K subset of the PropBank , and will have this data available for release in early 2007 .", "Our initial goal is to annotate the 700 most frequently occurring verbs in our data , which are typically also the most polysemous ; so far 300 verbs have been grouped and 150 double annotated .", "Subcategorization frames and semantic classes of arguments play major roles in determining the groupings , as illustrated by the grouping for the 22 WN 2 . 1 senses for drive in Figure 2 .", "In addition to improved annotator productivity and accuracy , we predict a corresponding improvement in word sense disambiguation performance .", "Training on this new data , Chen and Palmer 2005 report 86 . 3 accuracy for verbs using a smoothed maximum entropy model and rich linguistic features , which is 10 higher than their earlier , stateof the art performance on ungrouped , fine grained senses .", "We follow a similar procedure for the annotation of nouns .", "The same individual who groups WordNet verb senses also creates noun senses , starting with WordNet and other dictionaries .", "We aim to double annotate the 1100 most frequent polysemous nouns in the initial corpus by the end of 2006 , while maximizing overlap with the sentences containing annotated verbs .", "Certain nouns carry predicate structure ; these include nominalizations whose structure obviously is derived from their verbal form and various types of relational nouns like father , President , and believer , that express relations between entities , often stated using of .", "We have identified a limited set of these whose structural relations can be semi automatically annotated with high accuracy .", "In standard dictionaries , the senses for each word are simply listed .", "In order to allow access to additional useful information , such as subsumption , property inheritance , predicate frames from other sources , links to instances , and so on , our goal is to link the senses to an ontology .", "This requires decomposing the hierarchical structure into subtrees which can then be inserted at the appropriate conceptual node in the ontology .", "The OntoNotes terms are represented in the 110 , 000 node Omega ontology Philpot et al . , 2005 , under continued construction and extension at ISI .", "Omega , which has been used for MT , summarization , and database alignment , has been assembled semi automatically by merging a variety of sources , including Princeton s WordNet , New Mexico State University s Mikrokosmos , and a variety of Upper Models , including DOLCE Gangemi et al . , 2002 , SUMO Niles and Pease , 2001 , and ISI s Upper Model , which are in the process of being reconciled .", "The verb frames from PropBank , FrameNet , WordNet , and Lexical Conceptual Structures Dorr and Habash , 2001 have all been included and cross linked .", "In work planned for later this year , verb and noun sense groupings will be manually inserted into Omega , replacing the current primarily WordNet derived contents .", "For example , of the verb groups for drive in the table above , G1 and G4 will be placed into the area of controlled motion , while G2 will then sort with attitudes .", "The coreference annotation in OntoNotes connects coreferring instances of specific referring expressions , meaning primarily NPs that introduce or access a discourse entity .", "For example , Elco Industries , Inc . , the Rockford , Ill . Maker of fasteners , and it could all corefer .", "Non specific references like officials in Later , officials reported . . . are not included , since coreference for them is frequently unclear .", "In addition , proper premodifiers and verb phrases can be marked when coreferent with an NP , such as linking , when the company withdrew from the bidding to the withdrawal of New England Electric .", "Unlike the coreference task as defined in the ACE program , attributives are not generally marked .", "For example , the veterinarian NP would not be marked in Baxter Black is a large animal veterinarian .", "Adjectival modifiers like American in the American embassy are also not subject to coreference .", "Appositives are annotated as a special kind of coreference , so that later processing will be able to supply and interpret the implicit copula link .", "All of the coreference annotation is being doubly annotated and adjudicated .", "In our initial English batch , the average agreement scores between each annotator and the adjudicated results were 91 . 8 for normal coreference and 94 . 2 for appositives .", "PropBank I Palmer et al . , 2005 , developed at UPenn , captures predicate argument structure for verbs ; NomBank provides predicate argument structure for nominalizations and other noun predicates Meyers et al . , 2004 .", "PropBank II annotation eventuality ID s , coarse grained sense tags , nominal coreference and selected discourse connectives is being applied to a small 100K parallel Chinese English corpus Babko Malaya et al . , 2004 .", "The OntoNotes representation extends these annotations , and allows eventual inclusion of additional shallow semantic representations for other phenomena , including temporal and spatial relations , numerical expressions , deixis , etc .", "One of the principal aims of OntoNotes is to enable automated semantic analysis .", "The best current algorithm for semantic role labeling for PropBank style annotation Pradhan et al . , 2005 achieves an F measure of 81 . 0 using an SVM .", "OntoNotes will provide a large amount of new training data for similar efforts .", "Existing work in the same realm falls into two classes the development of resources for specific phenomena or the annotation of corpora .", "An example of the former is Berkeley s FrameNet project Baker et al . , 1998 , which produces rich semantic frames , annotating a set of examples for each predicator including verbs , nouns and adjectives , and describing the network of relations among the semantic frames .", "An example of the latter type is the Salsa project Burchardt et al . , 2004 , which produced a German lexicon based on the FrameNet semantic frames and annotated a large German newswire corpus .", "A second example , the Prague Dependency Treebank Hajic et al . , 2001 , has annotated a large Czech corpus with several levels of tectogrammatical representation , including parts of speech , syntax , and topic focus information structure .", "Finally , the IL Annotation project Reeder et al . , 2004 focused on the representations required to support a series of increasingly semantic phenomena across seven languages Arabic , Hindi , English , Spanish , Korean , Japanese and French .", "In intent and in many details , OntoNotes is compatible with all these efforts , which may one day all participate in a larger multilingual corpus integration effort ."], "summary_lines": ["OntoNotes: The 90% Solution\n", "We describe the OntoNotes methodology and its result, a large multilingual richly-annotated corpus constructed at 90% interannotator agreement.\n", "An initial portion (300K words of English newswire and 250K words of Chinese newswire) will be made available to the community during 2007.\n", "Ontonotes includes a wide array of data sources like broadcast news, news wire, magazine, web text, etc.\n", "In the OntoNotes project (Hovy et al., 2006), annotators use small-scale corpus analysis to create sense inventories derived by grouping together WordNet senses, with the procedure restricted to maintain 90% inter-annotator agreement.\n"]}
{"article_lines": ["Finite State Transducers In Language And Speech Processing", "Finite state machines have been used in various domains of natural language processing .", "We consider here the use of a type of transducer that supports very efficient programs sequential transducers .", "We recall classical theorems and give new ones characterizing sequential string tostring transducers .", "Transducers that output weights also play an important role in language and speech processing .", "We give a specific study of string to weight transducers , including algorithms for determinizing and minimizing these transducers very efficiently , and characterizations of the transducers admitting determinization and the corresponding algorithms .", "Some applications of these algorithms in speech recognition are described and illustrated .", "Finite state machines have been used in various domains of natural language processing .", "We consider here the use of a type of transducer that supports very efficient programs sequential transducers .", "We recall classical theorems and give new ones characterizing sequential string tostring transducers .", "Transducers that output weights also play an important role in language and speech processing .", "We give a specific study of string to weight transducers , including algorithms for determinizing and minimizing these transducers very efficiently , and characterizations of the transducers admitting determinization and the corresponding algorithms .", "Some applications of these algorithms in speech recognition are described and illustrated .", "Finite state machines have been used in many areas of computational linguistics .", "Their use can be justified by both linguistic and computational arguments .", "Linguistically , finite automata are convenient since they allow one to describe easily most of the relevant local phenomena encountered in the empirical study of language .", "They often lead to a compact representation of lexical rules , or idioms and clich\u00e9s , that appears natural to linguists Gross 1989 .", "Graphic tools also allow one to visualize and modify automata , which helps in correcting and completing a grammar .", "Other more general phenomena , such as parsing context free grammars , can also be dealt with using finitestate machines such as RTN's Woods 1970 .", "Moreover , the underlying mechanisms in most of the methods used in parsing are related to automata .", "From the computational point of view , the use of finite state machines is mainly motivated by considerations of time and space efficiency .", "Time efficiency is usually achieved using deterministic automata .", "The output of deterministic machines depends , in general linearly , only on the input size and can therefore be considered optimal from this point of view .", "Space efficiency is achieved with classical minimization algorithms Aho , Hoperoft , and Ullman 1974 for deterministic automata .", "Applications such as compiler construction have shown deterministic finite automata to be very efficient in practice Aho , Sethi , and Ullman 1986 .", "Finite automata now also constitute a rich chapter of theoretical computer science Perrin 1990 .", "Their recent applications in natural language processing , which range from the construction of lexical analyzers Silverztein 1993 and the compilation of morphological and phonological rules Kaplan and Kay 1994 ; Karttunen , Kaplan and Zaenen 1992 to speech processing Mohri , Pereira , and Riley 1996 show the usefulness of finite state machines in many areas .", "In this paper , we provide theoretical and algorithmic bases for the use and application of the devices that support very efficient programs sequential transducers .", "We extend the idea of deterministic automata to transducers with deterministic input , that is , machines that produce output strings or weights in addition to deterministically accepting input .", "Thus , we describe methods consistent with the initial reasons for using finite state machines , in particular the time efficiency of deterministic machines , and the space efficiency achievable with new minimization algorithms for sequential transducers .", "Both time and space concerns are important when dealing with language .", "Indeed , one of the recent trends in language studies is a large increase in the size of data sets .", "Lexical approaches have been shown to be the most appropriate in many areas of computational linguistics ranging from large scale dictionaries in morphology to large lexical grammars in syntax .", "The effect of the size increase on time and space efficiency is probably the main computational problem of language processing .", "The use of finite state machines in natural language processing is certainly not new .", "The limitations of the corresponding techniques , however , are pointed out more often than their advantages , probably because recent work in this field is not yet described in computer science textbooks .", "Sequential finite state transducers are now used in all areas of computational linguistics .", "In the following sections , we give an extended description of these devices .", "We first consider string to string transducers , which have been successfully used in the representation of large scale dictionaries , computational morphology , and local grammars and syntax , and describe the theoretical bases for their use .", "In particular , we recall classical theorems and provide some new ones characterizing these transducers .", "We then consider the case of sequential string to weight transducers .", "Language models , phone lattices , and word lattices are among the objects that can be represented by these transducers , making them very interesting from the point of view of speech processing .", "We give new theorems extending the known characterizations of stringto string transducers to these transducers .", "We define an algorithm for determinizing string to weight transducers , characterize the unambiguous transducers admitting determinization , and describe an algorithm to test determinizability .", "We also give an algorithm to minimize sequential transducers that has a complexity equivalent to that of classical automata minimization and that is very efficient in practice .", "Under certain restrictions , the minimization of sequential string to weight transducers can also be performed using the determinization algorithm .", "We describe the corresponding algorithm and give the proof of its correctness in the appendix .", "We have used most of these algorithms in speech processing .", "In the last section , we describe some applications of determinization and minimization of string to weight transducers in speech recognition , illustrating them with several results that show them to be very efficient .", "Our implementation of the determinization is such that it can be used on the fly only the necessary part of the transducer needs to be expanded .", "This plays an important role in the space and time efficiency of speech recognition .", "The reduction in the size of word lattices that these algorithms provide sheds new light on the complexity of the networks involved in speech processing .", "Sequential string to string transducers are used in various areas of natural language processing .", "Both determinization Mohri 1994c and minimization algorithms Mohri 1994b have been defined for the class of p subsequential transducers , which includes sequential string to string transducers .", "In this section , the theoretical basis of the use of sequential transducers is described .", "Classical and new theorems help to indicate the usefulness of these devices as well as their characterization .", "We consider here sequential transducers , namely , transducers with a deterministic input .", "At any state of such transducers , at most one outgoing arc is labeled with a given element of the alphabet .", "Figure 1 gives an example of a sequential transducer .", "Notice that output labels might be strings , including the empty string E . The empty string is not allowed on input , however .", "The output of a sequential transducer is not necessarily deterministic .", "The one in Figure 1 is not since , for instance , two distinct arcs with output labels b leave the state 0 .", "Sequential transducers are computationally interesting because their use with a given input does not depend on the size of the transducer but only on the size of the input .", "Since using a sequential transducer with a given input consists of following the only path corresponding to the input string and in writing consecutive output labels along this path , the total computational time is linear in the size of the input , if we consider that the cost of copying out each output label does not depend on its length .", "More formally , a sequential string to string transducer T is a 7 tuple Q , i , F , E , A , 6 ' , a , with The functions 6 and o are generally partial functions a state q E Q does not necessarily admit outgoing transitions labeled on the input side with all elements of the alphabet .", "These functions can be extended to mappings from Q x E by the following classical recurrence relations Thus , a string W E E is accepted by T iff 6 i , w E F , and in that case the output of the transducer is cr i , w .", "Sequential transducers can be generalized by introducing the possibility of generating an additional output string at final states Schiitzenberger 1977 .", "The application of the transducer to a string can then possibly finish with the concatenation of such an output string to the usual output .", "Such transducers are called subsequential transducers .", "Language processing often requires a more general extension .", "Indeed , the ambiguities encountered in language ambiguity of grammars , of morphological analyzers , or that of pronunciation dictionaries , for instance cannot be taken into account when using sequential or subsequential transducers .", "These devices associate at most a single output to a given input .", "In order to deal with ambiguities , one can introduce p subsequential transducers Mohri 1994a , namely transducers provided with at most p final output strings at each final state .", "Figure 2 gives an example of a 2 subsequential transducer .", "Here , the input string w aa gives two distinct outputs aaa and aab .", "Since one cannot find any reasonable case in language in which the number of ambiguities would be infinite , p subsequential transducers seem to be sufficient for describing linguistic ambiguities .", "However , the number of ambiguities could be very large in some cases .", "Notice that 1 subsequential transducers are exactly the subsequential transducers .", "Transducers can be considered to represent mappings from strings to strings .", "As such , they admit the composition operation defined for mappings , a useful operation that allows the construction of more complex transducers from simpler ones .", "The result of the application of T2 o Ti to a string s can be computed by first considering all output strings associated with the input s in the transducer Ti , then applying T2 to all of these strings .", "The output strings obtained after this application represent the result T2 0 Ti s .", "In fact , instead of waiting for the result of the application of Ti to be completely given , one can gradually apply T2 to the output strings of Ti yet to be completed .", "This is the basic idea of the composition algorithm , which allows the transducer T2 0 Ti to be directly constructed given Ti and T2 We define sequential resp . p subsequential functions to be those functions that can be represented by sequential resp . p subsequential transducers .", "We noted previously that the result of the composition of two transducers is a transducer that can be directly constructed .", "There exists an efficient algorithm for the general case of the composition of transducers transducers subsequential or not , having transitions or not , and with outputs in E , or in E U fool x R F U fool Mohri , Pereira , and Riley 1996 .", "The following theorem gives a more specific result for the case of subsequential and p subsequential functions , which expresses their closure under composition .", "We use the expression p subsequential in two ways here .", "One means that a finite number of Example of a subsequential transducer 72 . ambiguities is admitted the closure under composition matches this case , the second indicates that this number equals exactly p . Let f E A be a sequential resp . p subsequential and g A 9 be a sequential resp . q subsequential function , then g of is sequential resp . pq subsequential .", "We prove the theorem in the general case of p subsequential transducers .", "The case of sequential transducers , first proved by Choffrut 1978 , can be derived from the general case in a trivial way .", "Let T1 be a p subsequential transducer representing f , Ti Qi , ii , h , E , A , 61 , cri , pi , and T2 Q2 i2 F2 , A , C , 6'2 , 0 2 , P2 a q subsequential transducer representing g . pi and p2 denote the final output functions of T1 and 72 , which map Fi to A P and F2 to S2 q , respectively . pi r represents , for instance , the set of final output strings at a final state r . Define the pq subsequential transducer T Q , i , F , E , ft 6 , a , P by Q Qi X Q2 i i1 i2 .", "F qi , q2 E Q qi E Fi , 82 q2 , pi qi n F2 01 , with the following transition and output functions and with the final output function defined by V qi , q2 E F , p qi , q2 0 2 q2 , pi qi p2 . 5 q2 , pi qi Clearly , according to the definition of composition , the transducer r realizes g of .", "The definition of p shows that it admits at most pq distinct output strings for a given input one .", "This ends the proof of the theorem .", "0 Figure 3 gives an example of a 1 subsequential or subsequential transducer T2 .", "The result of the composition of the transducers Ti and 12 is shown in Figure 4 .", "States in the transducer T3 correspond to pairs of states of Ti and T2 .", "The composition consists essentially of making the intersection of the outputs of Ti with the inputs of 72 .", "Transducers admit another useful operation union .", "Given an input string w , a transducer union of T1 and T2 gives the set union of the strings obtained by application of Ti to w and 12 to w . We denote by Tl T2 the union of TI and T2 .", "The following theorem specifies the type of the transducer TI 7 2 , implying in particular the closure under union of p subsequential transducers .", "It can be proved in a way similar to the composition theorem .", "Theorem 2 Let f E A be a sequential resp . p subsequential and g E A be a sequential resp . q subsequential function , then g f is 2 subsequential resp .", "p q subsequential .", "2 subsequential transducer 73 , obtained by composition of 71 and 12 .", "The union transducer Ti T2 can be constructed from Ti and 7 2 in a way close to the union of automata .", "One can indeed introduce a new initial state connected to the old initial states of Ti and T2 by transitions labeled with the empty string both on input and output .", "But the transducer obtained using this construction is not sequential , since it contains c transitions on the input side .", "There exists , however , an algorithm to construct the union of p subsequential and q subsequential transducers directly as a p q subsequential transducer .", "The direct construction consists of considering pairs of states qi , q , qi being a state of Ti or an additional state that we denote by an underscore , q2 a state of 7 2 or an additional state that we denote by an underscore .", "The transitions leaving qi , q2 are obtained by taking the union of the transitions leaving qi and q2 , or by keeping only those of qi if q2 is the underscore state , similarly by keeping only those of q2 if qi is the underscore state .", "The union of the transitions is performed in such a way that if qi and q2 both have transitions labeled with the same input label a , then only one transition labeled with a is associated to qi , q2 .", "The output label of that transition is the longest common prefix of the output transitions labeled with a leaving qi and q2 .", "See Mohri 1996b for a full description of this algorithm .", "Figure 5 shows the 2 subsequential transducer obtained by constructing the union of the transducers 7 1 and T2 this way .", "Notice that according to the theorem the result could be a priori 3 subsequential , but these two transducers share no common accepted string .", "In such cases , the resulting transducer is max p , q subsequential .", "The linear complexity of their use makes sequential or p subsequential transducers both mathematically and computationally of particular interest .", "However , not all transducers , even when they realize functions rational functions , admit an equivalent sequential or subsequential transducer .", "Consider , for instance , the function f associated with the classical transducer represented in Figure 6 ; f can be defined by 1 VW E X 1 , f w aim' if I w I is even , Owl otherwise This function is not sequential , that is , it cannot be realized by any sequential transducer .", "Indeed , in order to start writing the output associated to an input string w a or b according to whether n is even or odd , one needs to finish reading the whole input string w , which can be arbitrarily long .", "Sequential functions , namely functions that can be represented by sequential transducers do not allow such unbounded delays .", "More generally , sequential functions can be characterized among rational functions by the following theorem Let f be a rational function mapping E to L . f is sequential iff there exists a positive integer K such that The fact that not all rational functions are sequential could reduce the interest of sequential transducers .", "The following theorem , due to Elgot and Mezei 1965 , shows , however , that transducers are exactly compositions of left and right sequential transducers .", "Theorem 4 Elgot and Mezei 1965 Let f be a partial function mapping E to A . f is rational iff there exists a left sequential function 1 E S2 and a right sequential function r S 2 A such that f r 0 1 .", "Left sequential functions or transducers are those we previously defined .", "Their application to a string proceeds from left to right .", "Right sequential functions apply to strings from right to left .", "According to the theorem , considering a new sufficiently large alphabet SZ allows one to define two sequential functions 1 and r that decompose a rational function f . This result considerably increases the importance of sequential functions in the theory of finite state machines as well as in the practical use of transducers .", "Berstel 1979 gives a constructive proof of this theorem .", "Given a finite state transducer T , one can easily construct a left sequential transducer L and a right sequential transducer R such that R o L T . Intuitively , the extended alphabet Si keeps track of the local ambiguities encountered when applying the transducer from left to right .", "A distinct element of the alphabet is assigned to each of these ambiguities .", "The right sequential transducer can be constructed in such a way that these ambiguities can then be resolved from right to left .", "Figures 7 and 8 give a decomposition of the nonsequential transducer T of Figure 6 .", "The symbols of the alphabet Q xl , x2 store information about the size of the input string w . The output of L ends with x1 iff I wl is odd .", "The right sequential function R is then easy to construct .", "Mohri Transducers in Language and Speech Sequential transducers offer other theoretical advantages .", "In particular , while several important tests , such as equivalence , are undecidable with general transducers , sequential transducers have the following decidability property Theorem 5 Let T be a transducer mapping E to A .", "It is decidable whether T is sequential .", "A constructive proof of this theorem was given by Choffrut 1978 .", "An efficient polynomial algorithm for testing the sequentiability of transducers based on this proof was given by Weber and Klemm 1995 .", "Choffrut also gave a characterization of subsequential functions based on the definition of a metric on E .", "Denote by u A v the longest common prefix of two strings u and v in E .", "It is easy to verify that the following defines a metric on E The following theorem describes this characterization of subsequential functions .", "Theorem 6 Let f be a partial function mapping E to A . f is subsequential iff The notion of bounded variation can be roughly understood here as follows if d x , y is small enough , namely if the prefix that x and y share is sufficiently long compared to their lengths , then the same is true of their images by f , f x and f y .", "This theorem can be extended to describe the case of p subsequential functions by defining a metric do on A P . For any u , up and v vi , . . , vp E A P , we define Assume f p subsequential , and let T be a p subsequential transducer realizing f . A transducer Ti , 1 i p , realizing a component fi off can be obtained from T simply by keeping only one of the p outputs at each final state of T . T , is subsequential by construction , hence the component fi is subsequential .", "Then the previous theorem implies that each component fi has bounded variation , and by definition of do , f has also bounded variation .", "Conversely , if the first condition holds , a fortiori eachfi has bounded variation .", "This combined with the second condition implies that eachfi is subsequential .", "A transducer T realizing f can be obtained by taking the union of p subsequential transducers realizing each component j .", "Thus , in view of the theorem 2 , f is p subsequential .", "One can also give a characterization of p subsequential transducers irrespective of the choice of their components .", "Let dp be the semimetric defined by Let f be a rational function mapping E to 6 , P . f is p subsequential iff it has bounded variation using the semimetric dip on A P .", "According to the previous theorem the condition is sufficient since Conversely if f is p subsequential , let T Q , i , F , E , L , 6 , a , p be a p subsequential transducer representing f , where p p1 , .", ", pp is the output function mapping Q to A P . Let N and M be defined by We denote by Dom T the set of strings accepted by T . Let k 0 and ui , u2 E Dom T 12 such that d ui , u2 k .", "Then , there exists u E E such that Thus , f has bounded variation using dip .", "This ends the proof of the theorem .", "We briefly mentioned several theoretical and computational properties of sequential and p subsequential transducers .", "These devices are used in many areas of computational linguistics .", "In all those areas , the determinization algorithm can be used to obtain a p subsequential transducer Mohri 1996b , and the minimization algorithm to reduce the size of the p subsequential transducer used Mohri 1994b .", "The composition , union , and equivalence algorithms for subsequential transducers are also useful in many applications .", "Mohri Transducers in Language and Speech 2 . 4 . 1 Representation of Dictionaries .", "Very large scale dictionaries can be represented by p subsequential dictionaries because the number of entries and that of the ambiguities they contain are finite .", "The corresponding representation offers fast look up since the recognition does not depend on the size of the dictionary but only on that of the input string considered .", "The minimization algorithm for sequential and p subsequential transducers allows the size of these devices to be reduced to the minimum .", "Experiments have shown that these compact and fast look up representations for large natural language dictionaries can be efficiently obtained .", "As an example , a French morphological dictionary of about 21 . 2 Mb can be compiled into a p subsequential transducer of 1 . 3 Mb , in a few minutes Mohri 1996b .", "2 . 4 . 3 Syntax .", "Finite state machines are also currently used to represent local syntactic constraints Silberztein 1993 ; Roche 1993 ; Karlsson et al . 1995 ; Mohri 1994d .", "Linguists can conveniently introduce local grammar transducers that can be used to disambiguate sentences .", "The number of local grammars for a given language and even for a specific domain can be large .", "The local grammar transducers are mostly p subsequential .", "Determinization and minimization can then be used to make the use of local grammar transducers more time efficient and to reduce their size .", "Since p subsequential transducers are closed under composition , the result of the composition of all local grammar transducers is a p subsequential transducer .", "The equivalence of local grammars can also be tested using the equivalence algorithm for sequential transducers .", "For a more detailed overview of the applications of sequential string to string transducers to language processing , see Mohri 1996a .", "Because they are so time and space efficient , sequential transducers will likely be used increasingly often in natural language processing as well as in other connected fields .", "In the following , we consider the case of string to weight transducers , which are also used in many areas of computational linguistics .", "We consider string to weight transducers , namely transducers with input strings and output weights .", "These transducers are used in various domains , such as language modeling , representation of word or phonetic lattices , etc . , in the following way one reads and follows a path corresponding to a given input string and outputs a number obtained by combining the weights along this path .", "In most applications to natural language processing , the weights are simply added along the path , since they are interpreted as negative logarithms of probabilities .", "In case the transducer is not sequential , that is , when it does not have a deterministic input , one proceeds in the same way for all the paths corresponding to the input string .", "In natural language processing , specifically in speech processing , one keeps the minimum of the weights associated to these paths .", "This corresponds to the Viterbi approximation in speech recognition or in other related areas for which hidden Markov models HMM's are used .", "In all such applications , one looks for the best path , i . e . , the path with the minimum weight .", "In this section , we give the definition of string to weight transducers and other definitions useful for the presentation of the theorems of the following sections .", "In addition to the output weights of the transitions , string to weight transducers are provided with initial and output weights .", "For instance , when used with the input string ab , the transducer in Figure 9 outputs 5 1 2 3 11 , 5 being the initial and 3 the final weight .", "More formally , a string to weight transducer T is defined by T Q , E , I , F , E , A , p with One can define for T a transition partial function S mapping Q x E to 2Q by V q , a E Q x E , q , a fq' I 3x E q , a , x , q' e El and an output function a mapping E to R . by A path 7 in T from q E Q to q' c Q is a set of successive transitions from q to q' 7r q0 , ao , x0 , qi , , qm i , am_i , xm_i , qm , with Vi E 0 , M 11 , q , 1 E 6 q11at .", "We can extend the definition of a to paths by a x xoxi x i .", "We denote by 7r e q q' the set of paths from q to q' labeled with the input string w . The definition of S can be extended to Q x E by V q , EQ x E , 5 q , w q' path 7r in T , 7r E q q' Mohri Transducers in Language and Speech and to 2 2 x E , by For q , w , q' E QxExQ such that there exists a path from q to q' labeled with w , we define 9 q , w , q' as the minimum of the outputs of all paths from q to q' with input w A successful path in T is a path from an initial state to a final state .", "A string w E E is accepted by T if there exists a successful path labeled with w w E S I , w n F . The output corresponding to an accepted string w is then obtained by taking the minimum of the outputs of all successful paths with input label w A transducer T is said to be trim if all states of T belong to a successful path .", "String toweight transducers clearly realize functions mapping E to 12 . .", "Since the operations we need to consider are addition and min , and since 74 U oo , min , , oo , 0 is a semiring , we call these functions formal power series . '", "We adopt the terminology and notation used in formal language theory Berstel and Reutenauer 1988 ; Kuich and Salomaa 1986 ; Salomaa and Soittola 1978 The fundamental theorem of Schutzenberger 1961 , analogous to Kleene's theorem for formal languages , states that a formal power series S is rational iff it is recognizable , that is , realizable by a string to weight transducer .", "The semiring R . U e , min , , 00 , 0 used in many optimization problems is called the tropical semiring . 3 So , the functions we consider here are more precisely rational power series over the tropical semiring .", "A string to weight transducer T is said to be unambiguous if for any given string w there exists at most one successful path labeled with w . In the following , we examine , more specifically , efficient string to weight transducers subsequential transducers .", "A transducer is said to be subsequential if its input is deterministic , that is if at any state there exists at most one outgoing transition labeled with a given element of the input alphabet E . Subsequential string to weight transducers are sometimes called weighted automata , or weighted acceptors , or probabilistic automata , or distance automata .", "Our terminology is meant to favor the functional view of these devices , which is the view that we consider here .", "Not all string to weight transducers are subsequential but we define an algorithm to determinize nonsubsequential transducers when possible .", "More formally a string to weight subsequential transducer T Q , i , F , E , 5 , a , A , p is an 8 tuple , with A string w E E is accepted by a subsequential transducer T if there exists f E F such that 5 i , w f .", "The output associated to w is then A cr i , w p f .", "We will use the following definition for characterizing the transducers that admit determinization .", "Two states q and q' of a string to weight transducer T Q , I , F , E , 6 , a , A , p , not necessarily subsequential , are said to be twins if In other words , q and q' are twins if , when they can be reached from the initial state by the same string u , the minimum outputs of loops at q and q' labeled with any string v are identical .", "We say that T has the twins property when any two states q and q' of T are twins .", "Notice that according to the definition , two states that do not have cycles with the same string v are twins .", "In particular , two states that do not belong to any cycle are necessarily twins .", "Thus , an acyclic transducer has the twins property In the following section , we consider subsequential power series in the tropical semiring , that is , functions that can be realized by subsequential string to weight transducers .", "Many rational power series defined on the tropical semiring considered in practice are subsequential , in particular , acyclic transducers represent subsequential power series .", "Mohri Transducers in Language and Speech We introduce a theorem giving an intrinsic characterization of subsequential power series irrespective of the transducer realizing them .", "We then present an algorithm that allows one to determinize some string to weight transducers .", "We give a general presentation of the algorithm since it can be used with many other semirings , in particular , with string to string transducers and with transducers whose output labels are pairs of strings and weights .", "We then use the twins property to define a set of transducers to which the determinization algorithm applies .", "We give a characterization of unambiguous transducers admitting determinization , and then use this characterization to define an algorithm to test if a given transducer can be determinized .", "We also present a very efficient minimization algorithm that applies to subsequential string to weight transducers .", "In many cases , the determinization algorithm can also be used to minimize a subsequential transducer ; we describe this use of the algorithm and give the related proofs in the appendix .", "Recall that one can define a metric on E by where we denote by u A v the longest common prefix of two strings u and v in E .", "The definition we gave for subsequential power series depends on the transducers representing them .", "The theorem that follows gives an intrinsic characterization of subsequential power series . '", "Theorem 9 Let S be a rational power series defined on the tropical semiring .", "S is subsequential iff it has bounded variation .", "Proof Assume that S is subsequential .", "Let T Q , i , F , E , 6 , 0 , A , p be a subsequential transducer .", "8 denotes the transition function associated with 7 , a its output function , and A and p the initial and final weight functions .", "Let L be the maximum of the lengths of all output labels of T and R the upper bound of all output differences at final states and define M as M L R . Let ui , u2 be in E 2 .", "By definition of d , there exists U E E such that This proves that S is M Lipschitzian5 and a fortiori that it has bounded variation .", "Conversely , suppose that S has bounded variation .", "Since S is rational , according to the theorem of Schtitzenberger 1961 it is recognizable and therefore there exists a string to weight transducer 7 Q , I , F , E , 6 , a , p realizing S . As in the case of string to string transducers , one can show that any transducer admits an equivalent trim unambiguous transducer .", "So , without loss of generality we can assume T trim and unambiguous .", "Furthermore , we describe in the next sections a determinization algorithm .", "We show that this algorithm applies to any transducer that has the twins property Thus , in order to show that S is subsequentiable , it is sufficient to show that T has the twins property Consider two states q and q' of T and let u , v E E 2 be such that Since T is trim there exists w , w' E E 2 such that 6 q , w n F 0 and 6 q , w' n F 0 .", "Notice that Vk 0 , d uvkw , uvkw' d w , w1 Thus , since S has bounded variation Algorithm for the determinization of a transducer Ti representing a power series defined on the semiring S , G , 6 , 1 .", "Hence We describe in this section an algorithm for constructing a subsequential transducer 72 Q2 , 2 , F2 , E , 62 , 0 2 , A2 , 192 equivalent to a given nonsubsequential one TI E , I , F1 , Ei , A1 , P1 .", "The algorithm extends our determinization algorithm for stringto string transducers representing p subsequential functions to the case of transducers outputting weights Mohri 1994c .", "Figure 10 gives the pseudocode of the algorithm .", "We present the algorithm in the general case of a semiring S , ED , 0 , 0 , 1 on which the transducer Ti is defined .", "Indeed , the algorithm we are describing here applies as well to transducers representing power series defined on many other semirings . 6 We describe the algorithm in the case of the tropical semiring .", "For the tropical semiring , one can replace ED by min and 0 by in the pseudocode of Figure 10 . 7 The algorithm is similar to the powerset construction used for the determinization of automata .", "However , since the outputs of two transitions bearing the same input label might differ , one can only output the minimum of these outputs in the resulting transducer , therefore one needs to keep track of the residual weights .", "Hence , the subsets q2 that we consider here are made of pairs q , x of states and weights .", "The initial weight A2 Of 72 is the minimum of all the initial weights of TI line 2 .", "The initial state i2 is a subset made of pairs i , x , where i is an initial state of and x A1 i A2 line 3 .", "We use a queue Q to maintain the set of subsets q2 yet to be examined , as in the classical powerset construction . '", "Initially , Q contains only the subset i2 .", "The subsets q2 are the states of the resulting transducer . q2 is a final state of 7 2 iff it contains at least one pair q , x , with q a final state of ri lines 7 8 .", "The final output associated to q2 is then the minimum of the final outputs of all the final states in q2 combined with their respective residual weight line 9 .", "For each input label a such that there exists at least one state q of the subset q2 admitting an outgoing transition labeled with a , one outgoing transition leaving q2 with the input label a is constructed lines 10 14 .", "The output 0 2 q2 , a of this transition is the minimum of the outputs of all the transitions with input label a that leave a state in the subset q2 , when combined with the residual weight associated to that state line 11 .", "The destination state 62 q2 , a of the transition leaving q2 is a subset made of pairs q' , x' , where q' is a state of Ti that can be reached by a transition labeled with a , and x' the corresponding residual weight line 12 . x' is computed by taking the minimum of all the transitions with input label a that leave a state q of q2 and reach q' , when combined with the residual weight of q minus the output weight C72 q2 , a .", "Finally , 62 q2 , a is enqueued in Q iff it is a new subset .", "We denote by n1 t the destination state of a transition t E Ei .", "Hence n1 t q' , if t q , a , x , q' E Ei .", "The sets r q2 , a , of q2 , a , and v q2 , a used in the algorithm are defined by q2 , a denotes the set of pairs q , x , elements of the subset q2 , having transitions labeled with the input a .", "7 q2 , a denotes the set of triples q , x , t where q , x is a pair in q2 such that q admits a transition with input label a . v q2 , a is the set of states q' that can be reached by transitions labeled with a from the states of the subset q2 .", "The algorithm is illustrated in Figures 11 and 12 .", "Notice that the input ab admits several outputs in pi 1 1 2 , 1 3 4 , 3 3 6 , 3 5 8 .", "Only one of these outputs 2 , the smallest is kept in the determinized transducer 12 , since in the tropical semiring one is only interested in the minimum outputs for any given string .", "Notice that several transitions might reach the same state with a priori different residual weights .", "Since one is only interested in the best path , namely the path corresponding to the minimum weight , one can keep the minimum of these weights for a given state element of a subset line 11 of the algorithm of Figure 10 .", "In the next section , we give a set of transducers TI for which the determinization algorithm terminates .", "The following theorem shows the correctness of the algorithm when it terminates .", "Transducer it2 obtained by power series determinization of Theorem 10 Assume that the determinization algorithm terminates , then the resulting transducer 72 is equivalent to r1 .", "We denote by Oi q , w , q' the minimum of the outputs of all paths from q to q' .", "By construction we have We define the residual output associated to q in the subset 62 i2 , w as the weight c q , w associated to the pair containing q in 82 i2 , w .", "It is not hard to show by induction on I wl that the subsets constructed by the algorithm are the sets 62 i2 , W E E , such that Notice that the size of a subset never exceeds I Qi I card 62 i2 , w I Qi .", "A state q belongs at most to one pair of a subset , since for all paths reaching q , only the minimum of the residual outputs is kept .", "Notice also that , by definition of mm , in any subset there exists at least one state q with a residual output c q , w equal to 0 .", "A string w is accepted by Ti iff there exists q E F1 such that q E 61 Ii , w .", "Using equations 15 , it is accepted if 62 i2 , w contains a pair q , c q , w with q E Fi .", "This is exactly the definition of the final states F2 line 7 .", "So 7 1 and T2 accept the same set of strings .", "Let w E E be a string accepted by ri and T2 .", "The definition of p2 in the algorithm of figure 10 , line 9 , gives The power series determinization algorithm is equivalent to the usual determinization of automata when the initial weight , the final weights , and all output labels are equal to 0 .", "The subsets considered in the algorithm are then exactly those obtained in the powerset determinization of automata , all residual outputs c q , w being equal to 0 .", "Both space and time complexity of the determinization algorithm for automata are exponential .", "There are minimal deterministic automata with exponential size with respect to an equivalent nondeterministic one .", "A fortiori the complexity of the determinization algorithm in the weighted case we just described is also exponential .", "However , in some cases in which the degree of nondeterminism of the initial transducer is high , the determinization algorithm turns out to be fast and the resulting transducer has fewer states than the initial one .", "We present examples of such cases , which appear in speech recognition , in the last section .", "We also present a minimization algorithm that allows the size of subsequential transducers representing power series to be reduced .", "The complexity of the application of subsequential transducers is linear in the size of the string to which it applies .", "This property makes it worthwhile to use the power series determinization to speed up the application of transducers .", "Not all transducers can be determinized using the power series determinization .", "In the following section , we define a set of transducers that admit determinization , and characterize unambiguous transducers that admit the application of the algorithm .", "Since determinization does not apply to all transducers , it is important to be able to test the determinizability of a transducer .", "We present , in the next section , an algorithm to test this property in the case of unambiguous trim transducers .", "The proofs of some of the theorems in the next two sections are complex ; they can be skipped on first reading .", "There are transducers with which determinization does not halt , but rather generates an infinite number of subsets .", "We define determinizable transducers as those transducers with which the algorithm terminates .", "We first show that a large set of transducers Mohri Transducers in Language and Speech admit determinization , then give a characterization of unambiguous transducers admitting determinization .", "In what follows , the states of the transducers considered will be assumed to be accessible from the initial one .", "The following lemma will be useful in the proof of the theorems .", "Lemma 1 Let T Q , E , I , F , E , A , p be a string to weight transducer , 7r E p q a path in T from the state p E Q to q E Q , and 7r' E p' q' a path from p' E Q to q' E Q both labeled with the input string w E E .", "Assume that the lengths of it and 7r' are greater than We also define A and p by V ii , t2 E h x A ii , i2 A2 i2 , V fj , , , f2 E Fi x F2 , P fi , f2 Pi fi , P2 f2 .", "Consider the cross product of T with itself , T x T . Let it and 713 be two paths in T with lengths greater than 1Q12 1 , m 1Q12 1 is a path in T x T with length greater than 1Q12 1 .", "Since T x T has exactly 1Q12 states , H admits at least one cycle at some state p1 , p'i labeled with a non empty input string u2 .", "This shows the existence of the factorization above and proves the lemma .", "0 Let TI Q1 , E , 111F1 , Ei , Ai , p1 be a string to weight transducer defined on the tropical semirirtg .", "If 7 1 has the twins property then it is determinizable .", "Proof Assume that 7 has the twins property .", "If the determinization algorithm does not halt , there exists at least one subset of 2 , q0 , , q , , such that the algorithm generates an infinite number of distinct weighted subsets qo , c0 , q .", "Then we have necessarily m 1 .", "Indeed , we mentioned previously that in any subset there exists at least one state qi with a residual output c , 0 .", "If the subset contains only one state go , then co 0 .", "So there cannot be an infinite number of distinct subsets go , co Let A C E be the set of strings w such that the states of 62 i2 , w be Iqo , , 1 .", "We have Vw E A , 62 i2 , w q0 , c qo , w , , qm , c q , n , w 1 .", "Since A is infinite , and since in each weighted subset there exists a null residual output , there exist io , 0 io m , such that c q , , w 0 for an infinite number of strings w E A .", "Without loss of generality we can assume that io 0 .", "Let B C A be the infinite set of strings w for which c q0 , w 0 .", "Since the number of subsets go , c go , w , qm , c gm , w , w E B , is infinite , there exists , 0 j m , such that c qj , w be distinct for an infinite number of strings w E B .", "Without loss of generality we can assume j 1 .", "Let C C B be an infinite set of strings w with c qi , w all distinct .", "Define R qo , qi to be the finite set of differences of the weights of paths leading to go and gi labeled with the same string w , IwIIQiI2 1 We will show that c gi , w w E C C R q0 , qi .", "This will yield a contradiction with the infinity of C , and will therefore prove that the algorithm terminates .", "Let w E C , and consider a shortest path 7ro from a state 10 E I to qo labeled with the input string w and with total cost cr 7r0 .", "Similarly consider a shortest path in from E Ito gi labeled with the input string w and with total cost cr n i .", "By definition of the subset construction we have A i1 cr 71 1 A io cr 71 0 c qi , w .", "Assume that w I 112112 1 .", "Using the lemma 1 , there exists a factorization of 7r0 and iti of the type Since in and 7T are shortest paths , we have O 7n0 o 71 01 Po , u2 , po and 0 70 c lri 01 pi , u2 , pi .", "Hence A ii 0 7r . io 7 4 c qi , w .", "By induction on 170 , we can therefore find shortest paths Ho and 1 11 from 10 to qo resp . ii to gi with length less or equal to l 2112 1 and such that A ii a 111 A io 0 11o c ql , w .", "Since c I11 cr II0 E R q0 , qi , c qi , w E R q0 , qi and C is finite .", "This ends the proof of the theorem .", "0 There are transducers that do not have the twins property and that are still determinizable .", "To characterize such transducers , more complex conditions that we will not describe here are required .", "However , in the case of trim unambiguous transducers , the twins property provides a characterization of determinizable transducers .", "Let Ti Q1 , E , 1 , F1 , Ei , Ai , Pi be a trim unambiguous string to weight transducer defined on the tropical semiring .", "Then Ti is determirtizable iff it has the twins property .", "Proof According to the previous theorem , if T . 1 has the twins property , then it is determinizable .", "Assume now that T does not have the twins property , then there exist at least two states q and q' in Q that are not twins .", "There exists u , v E E such that q , q' c SW , u , q E 61 q , v , q' E 81 qcv and 01 q , v , q 01 q' , v , q' .", "Consider the weighted subsets 52 i2 , UVk , with k E Ai , constructed by the determinization algorithm .", "A subset 62 i2 , uv quot ; contains the pairs q , c q , uvk and q' , c q' , uvk .", "We will show that these subsets are all distinct .", "This will prove that the determinization algorithm does not terminate if Ti does not have the twins property .", "Since ri is a trim unambiguous transducer , there exits only one path in Ti from I to q or to q' with input string u .", "Similarly , the cycles at q and q' labeled with v are unique .", "Thus , there exist i E I and i' E I such that Since 0 0 0 , equation 20 shows that the subsets 62 i2 , uv' are all distinct .", "0 The characterization of determinizable transducers provided by theorem 12 leads to the definition of an algorithm for testing the determinizability of trim unambiguous transducers .", "Before describing the algorithm , we introduce a lemma that shows that it suffices to examine a finite number of paths to test the twins property .", "Lemma 2 Let Ti Qi , E , Ii , Fi , Ei , Ai , Pi be a trim unambiguous string to weight transducer defined on the tropical serniring .", "Ti has the twins property iff V u , V E E 2 , 1UVI Clearly if Ti has the twins property , then 21 holds .", "Conversely , we prove that if 21 holds , then it also holds for any u , v E E 2 , by induction on I uv .", "Our proof is similar to that of Berstel 1979 for string to string transducers .", "Consider u , v E E 2 and q , q' E l 2112 such that q , q' c I I , u , q c 61 q , v , q' E q' , v .", "Assume that luvl 21 2112 1 with Ivl 0 .", "Then either lul Assume that I u I 1 21 12 1 .", "Since Ti is a trim unambiguous transducer there exists 1 2112 1 or Iv' a unique path 7r in Ti from i E Ito q labeled with the input string u , and a unique path 1Q112 1 .", "7r' from i' E Ito q' .", "In view of lemma 2 , there exist strings 141 , u2 , u3 in E , and states pi , p2 , p , and p12 such that I u2I 0 , u1u2u3 u and such that ir and ir' be factored in the following way Next , assume that lvi 1Q112 1 .", "Then according to lemma 1 , there exist strings v1 , v2 , V3 in E , and states qi , q2 , q'y and q'2 such that Iv2I 0 , v1v2v3 v and such that Ir and 71 ' be factored in the following way Let T1 E1 , A1 , 101 be a trim unambiguous string to weight transducer defined on the tropical semiring .", "There exists an algorithm to test the determinizability of Ti .", "Proof According to theorem 12 , testing the determinizability of TI is equivalent to testing for the twins property We define an algorithm to test this property Our algorithm is close to that of Weber and Klemm 1995 for testing the sequentiability of string to string transducers .", "It is based on the construction of an automaton A Q , I , F , E similar to the cross product of Ti with itself .", "Let K C 7 ? , be the finite set of real numbers defined by Mohri Transducers in Language and Speech By construction , two states qi and q2 of Q can be reached by the same string u , lul 21 2112 1 , if there exists c E K such that qi , q2 , c can be reached from in A .", "The set of such qi , q2 , c is exactly the transitive closure of I in A .", "The transitive closure of I can be determined in time linear in the size of A , 0 1 21 I E I .", "Two such states qi and q2 are not twins if there exists a path in A from eh , q2 , 0 to qi , q2 , c , with c 0 .", "Indeed , this is exactly equivalent to the existence of cycles at qi and q2 with the same input label and distinct output weights .", "According to lemma 2 , it suffices to test the twins property for strings of length less than 21 2112 1 .", "So the following gives an algorithm to test the twins property of a transducer The operations used in the algorithm computation of the transitive closure , determination of the set of states can all be done in polynomial time with respect to the size of A , using classical algorithms Aho , Hoperoft , and Ullman 1974 .", "0 This provides an algorithm for testing the twins property of an unambiguous trim transducer T . It is very useful when T is known to be unambiguous .", "In many practical cases , the transducer one wishes to determinize is ambiguous .", "It is always possible to construct an unambiguous transducer T' from T Eilenberg 1974 1976 .", "The complexity of such a construction is exponential in the worst case .", "Thus the overall complexity of the test of determinizability is also exponential in the worst case .", "Notice that if one wishes to construct the result of the determinization of T for a given input string w , one does not need to expand the whole result of the determinization , but only the necessary part of the determinized transducer .", "When restricted to a finite set the function realized by any transducer is subsequentiable , since it has bounded variation . '", "Acyclic transducers have the twins property , so they are determinizable .", "Therefore , it is always possible to expand the result of the determinization algorithm for a finite set of input strings , even if T is not determinizable .", "The determinization algorithm that we previously presented applies as well to transducers mapping strings to other semirings .", "We gave the pseudocode of the algorithm in the general case .", "The algorithm applies for instance to the real semiring R , , , . , 0 , 1 .", "One can also verify that E Uf oo , A , oo , f , where A denotes the longest common prefix operation and concatenation , oo a new element such that for any string w E E U fool , WA oo co A w w and w oo oo w oo , defines a left semiring . '", "We call this semiring the string semiring .", "The algorithm of Figure 10 used with the string semiring is exactly the determinization algorithm for subsequentiable string to string transducers , as defined by Mohri 1994c .", "The cross product of two semirings defines a semiring .", "The algorithm also applies when the semiring is the cross product of Sequential transducer 72 with outputs in E x R obtained from 31 by determinization .", "E U fool , A , oo , and R U oo , min , , 00 , 0 , which allows transducers outputting pairs of strings and weights to be determined .", "The determirtization algorithm for such transducers is illustrated in Figures 13 and 14 .", "Subsets in this algorithm are made of triples q , w , x E Q x E U oo x R . U foo , where q is a state of the initial transducer , w a residual string , and x a residual output weight .", "We here define a minimization algorithm for subsequential power series defined on the tropical semiring , which extends the algorithm defined by Mohri 1994b in the case of string to string transducers .", "For any subset L of E and any string u we define it lL by 11 One can prove that S . a power series defined on a field , is rational if it admits a finite number of independent u 1S Carlyle and Paz 1971 .", "This is the equivalent , for power series , of Nerode's theorem for regular languages .", "Mohri Transducers in Language and Speech For any subsequential power series S we can now define the following relation on E It is easy to show that Rs is an equivalence relation .", "u 1 supp S supp S defines the equivalence relation for regular languages .", "Rs is a finer relation .", "The additional condition in the definition of Rs is that the restriction of the power series 14 1S v 1S to u 1 supp S supp S is constant .", "The following lemma shows that if there exists a subsequential transducer T computing S with a number of states equal to the number of equivalence classes of Rs , then T is a minimal transducer computing f . If S is a subsequential power series defined on the tropical semiring , Rs has a finite number of equivalence classes .", "This number is bounded by the number of states of any subsequential transducer realizing S . So V u , v E E 2 , 6 i , u 6 i , v uRsv .", "This proves the lemma .", "0 The following theorem proves the existence of a minimal subsequential transducer representing S . For any subsequential function S , there exists a minimal subsequential transducer computing it .", "Its number of states is equal to the index of R . Given a subsequential power series S , we define a power series f by We then define a subsequential transducer T Q , i , F , E , 6 , a , A , p by 12 Since the index of Rs is finite , Q and F are well defined .", "The definition of 6 does not depend on the choice of the element u in U , since for any a E E , u Rs v implies ua Rs va .", "The definition of a is also independent of this choice , since by definition of Rs , if uRsv , then ua Rs va and there exists k E R . such that Vw E E , S , uaw S . yaw S , uw S , vw k . Notice that the definition of a implies that T realizes S . This ends the proof of the theorem .", "0 Given a subsequential transducer T Q , i , F , E , 6 , a , A , p , we can define for each state q E Q , d q by We define a new operation of pushing , which applies to any transducer T . In particular , if T is subsequential the result of the application of pushing to T is a new subsequential transducer T' Q , i , F , E , 6 , , A' , p' that only differs from T by its output weights in the following way According to the definition of d , we have Mohri Transducers in Language and Speech Lemma 4 Let T' be the transducer obtained from T by pushing .", "T' is a subsequential transducer which realizes the same function as T . This proves the lemma .", "0 The following theorem defines the minimization algorithm .", "Let T be a subsequential transducer realizing a power series on the tropical semiring .", "Then applying the following two operations leads to a minimal transducer .", "This minimal transducer is exactly the one defined in the proof of theorem 14 .", "The automata minimization step in the theorem consists of considering pairs of input labels and associated weights as a single label and of applying classical minimization algorithms for automata Aho , Hoperoft , and Ullman 1974 .", "We do not give the proof of the theorem ; it can be proved in a way similar to what is indicated in Mohri 1994b .", "In general , there are several distinct minimal subsequential transducers realizing the same function .", "Pushing introduces an equivalence relation on minimal transducers T Rp T' if p T p T' , where p T resp . p T' denotes the transducer obtained from T resp .", "T' by pushing .", "Indeed , if T and T' are minimal transducers realizing the same function , then p T and p T' are both equal to the unique minimal transducer equivalent to T and T' as defined in theorem 14 .", "So , two equivalent minimal transducers only differ by their output labels , they have the same topology .", "They only differ by the way the output weights are spread along the paths .", "Notice that if we introduce a new super final state 43 to which each final state q is connected by a transition of weight p q , then d q in the definition of T' is exactly the length of a shortest path from . 11 to q .", "Thus , T' can be obtained from T using the classical single source shortest paths algorithms such as that of Dijkstra Cormen , Leiserson , and Rivest 1992 . 13 lit case the transducer is acyclic , a classical linear time algorithm based on a topological sort of the graph allows one to obtain d . Once the function d is defined , the transformation of T into T' can be done in linear time , namely 0 1 21 IED , if we denote by E the set of transitions of T . The complexity of pushing is therefore linear 0 1Q1 1E1 if the transducer is acyclic .", "In the general case , the complexity of pushing is 0 1Ellog IQ' if we use classical heaps , 0 1E1 IQ' log 1 21 if we use Fibonacci heaps , and 0 1Ellog log IQ if we use the efficient implementation of priority queues by Thorup 1996 .", "In case the maximum output weight W is small , we can use the algorithm of Ahuja et al . 1988 ; the complexity of pushing is then 0 1E1 1 21 0w1 .", "In case the transducer is acyclic , we can use a specific automata minimization algorithm Revuz 1992 with linear time complexity , 0 1 21 1E1 .", "In the general case , an efficient implementation of Hoperoft's algorithm Aho , Hoperoft , and Ullman 1974 leads to 0 1ElloglQ1 .", "Thus , the overall complexity of the minimization of subsequential transducers is always as good as that of classical automata minimization 0 1 21 1El in the acyclic case , and 0 1Ellog1Q1 in the general case .", "Figures 15 to 17 illustrate the minimization algorithm .", "31 Figure 15 represents a subsequential string to weight transducer .", "Notice that the size of 31 cannot be reduced using the automata minimization . represents the transducer obtained by pushing , and Si a minimal transducer realizing the same function as 31 in the tropical semiring .", "Minimal transducer 61 obtained from 71 by automata minimization .", "The transducer obtained by this algorithm is the one defined in the proof of theorem 14 and has the minimal number of states .", "This raises the question of whether there exists a subsequential transducer with the minimal number of transitions and computing the same function as a given subsequential transducer T . The following corollary offers an answer .", "Corollary 1 A minimal subsequential transducer has also the minimal number of transitions among all subsequential transducers realizing the same function .", "This generalizes the analogous theorem that holds in the case of automata .", "The proof is similar .", "Let T be a subsequential transducer with a minimal number of transitions .", "Clearly , pushing does not change the number of transitions of T and automatan minimization , which consists of merging equivalent states , reduces or does not change this number .", "Thus , the number of transitions of the minimal transducer equivalent to T as previously defined is less or equal to that of T . This proves the corollary since , as previously pointed out , equivalent minimal transducers all have the same topology in particular , they have the same number of states and transitions .", "Given two subsequential transducers , one might wish to test their equivalence .", "The importance of this problem was pointed out by Hoperoft and Ullman 1979 , 284 .", "The following corollary addresses this question .", "Corollary 2 There exists an algorithm to determine if two subsequential transducers are equivalent .", "The algorithm of theorem 15 associates a unique minimal transducer to each subsequential transducer T . More precisely , this minimal transducer is unique up to a renumbering of the states .", "The identity of two subsequential transducers with different numbering of states can be tested in the same way as that of two deterministic automata ; for instance , by testing the equivalence of the automata and the equality of their number of states .", "An efficient algorithm for testing the equivalence of two deterministic automata is given in Aho , Hoperoft , and Ullman 1974 . 14 Since the minimization of subsequential transducers was also shown to be efficient , this proves the corollary and also the efficiency of the test of equivalence .", "0 Schiitzenberger 1961 gave an algorithm for minimizing the representation of power series , but this algorithm can only be used when the semiring considered is a field .", "In particular , it cannot be used with the tropical semiring or the string semiring used in language and speech processing , since none of these semirings is a field .", "More precisely , a recent result of Krob 1994 states that such a minimization cannot be defined for transducers defined on the tropical semiring .", "Furthermore , we implemented the algorithm of Schatzenberger 1961 and used it in the case of the semiring R . , , 0 , 1 .", "It has two important disadvantages in practice it creates many transitions , and it can generate transitions with negative weights , even if the initial machine has none .", "The negative weights cannot be interpreted in terms of probability .", "In the next section , we describe some of the applications to speech recognition of the algorithms we presented above .", "In previous sections , we gave a theoretical description of the determinization and minimization algorithms for string to weight transducers .", "Here we indicate their use in practice .", "These algorithms have interesting applications in speech recognition , some of which we briefly point out below .", "String to weight transducers are found at several stages of speech recognition .", "Phone lattices , language models , and word lattices are typically represented by such transducers .", "Weights in these graphs correspond to negative logarithms of probabilities .", "They are added along a path .", "For a given string , there might be many different paths in a transducer .", "Only the minimum of the total weights of these paths is considered relevant .", "Thus , the main operations involved in the interpretation of these transducers are addition and min , namely those of the tropical semiring .", "Thus , the algorithms we defined in the previous sections apply to speech recognition .", "The domain of the speech recognition systems above signal processing can be represented by a composition of finite state transducers outputting weights , or both strings and weights Pereira and Riley 1996 ; Mohri , Pereira , and Riley 1996 GoL 0 Co A 0 where 0 represents the acoustic observations , A the acoustic model mapping sequences of acoustic observations to context dependent phones , C the context dependency model mapping sequences of context dependent phones to context independent phones , L a pronunciation dictionary mapping sequences of phones to words , and G a language model or grammar mapping sequences of words to sentences .", "In general , this cascade of compositions cannot be explicitly expanded , because of the large size of the compositions ; an approximation method is required to search it .", "Often , a beam pruning is used only paths with weights within the beam the difference of the weights from the minimum weight so far is less than a certain predefined threshold are kept during the expansion of the cascade of composition .", "Furthermore , only the best path or a set of paths of the cascade of transducers with the lowest weights is of interest .", "A set of paths with the lowest weights can be represented by an acyclic string toweight transducer , each path of which corresponds to a sentence .", "The weight of the path can be interpreted as a negative log of the probability of that sentence given the sequence of acoustic observations utterance .", "Such acyclic string to weight transducers are called word lattices .", "For a given utterance , the word lattice obtained in such a way contains many paths labeled with the possible sentences and their associated weights .", "A word lattice often contains a lot of redundancy many paths correspond to the same sentence but with different weights .", "Word lattices can be directly searched to find the most probable sentences , those which correspond to the best paths , the paths with the smallest weights .", "Figure 18 shows a word lattice obtained in speech recognition for the 2 , 000 word ARPA ATIS Task .", "It corresponds to the following utterance Which flights leave Detroit and arrive at Saint Petersburg around nine am ?", "Clearly the lattice is complex ; it contains about 83 million paths .", "Usually , it is not enough to consider the best path of a word lattice .", "It is also necessary to correct the best path approximation by considering the n best paths , where the value of n depends on the task considered .", "Notice that in case n is very large , one would need to consider , for the lattice in Figure 18 , all 83 million paths .", "The transducer contains 106 states and 359 transitions .", "Determinization applies to this lattice .", "The resulting transducer W2 Figure 19 is sparser .", "Recall that it is equivalent to W1 , realizing exactly the same function mapping strings to weights .", "For a given sentence s recognized by WI , there are many different paths with different total weights .", "W2 contains a path labeled with s and with a total weight equal to the minimum of the weights of the paths of W1 .", "Let us insist on the fact that no pruning , heuristic , or approximation has been used here .", "The lattice W2 only contains 18 paths .", "Obviously , the search stage in speech recognition is greatly simplified when applied to W2 rather than Wi .", "W2 admits 38 states and 51 transitions .", "The transducer W2 can still be minimized .", "The minimization algorithm described in the previous section leads to the transducer W3 shown in Figure 20 .", "It contains 25 states and 33 transitions and of course the same number of paths as W2 , 18 .", "The effect of minimization appears to be less important .", "This is because , in this case , determinization includes a large part of the minimization by reducing the size of the first lattice .", "This can be explained by the degree of nondeterminism of word lattices such as WI . '", "Many states can be reached by the same set of strings .", "These states are grouped into a single subset during determinization .", "Also , the complexity of determinization is exponential in general , but in the case of the lattices considered in speech recognition , it is not . 16 Since they contain a lot of redundancy , the resulting lattice is smaller than the initial one .", "In fact , the time complexity of determinization can be expressed in terms of the initial and resulting lattices , W1 and W2 , by 0 1E I log II 1W1I1W21 2 , where I Wi I and 114 , 721 denote the sizes of W1 and W2 .", "Clearly if we restrict determinization to the cases where 1W2 I I W1 its complexity is polynomial in terms of the size of the initial transducer I Wi I .", "This also applies to the space complexity of the algorithm .", "In practice , the algorithm appears to be very efficient .", "As an example , it took about 0 . 02s on a Silicon Graphics Indy 100 MHZ Processor , 64 Mb RAM to determinize the transducer of Figure 18 . 1' Determinization makes the use of lattices much faster .", "Since at any state there exists at most one transition labeled with the word considered , finding the weight associated with a sentence does not depend on the size of the lattice .", "The time and space complexity of such an operation is simply linear in the size of the sentence .", "When dealing with large tasks , most speech recognition systems use a rescoring method Figure 21 .", "This consists of first using a simple acoustic and grammar model to produce a word lattice , and then to reevaluate this word lattice with a more sophisticated model .", "The size of the word lattice is then a critical parameter in the time and space efficiency of the system .", "The determinization and minimization algorithms we presented allow the size of such word lattices to be considerably reduced , as seen in the examples .", "We experimented with both determinization and minimization algorithms in the ATIS task .", "Table 1 illustrates these results .", "It shows these algorithms to be very effective in reducing the redundancy of speech networks in this task .", "The reduction is also illustrated by an example in the ATIS task .", "The number of paths of the word lattice before determinization was larger than that of the largest integer representable with 32 bit machines .", "We also experimented with the minimization algorithm by applying it to several word lattices obtained in the 60 , 000 word ARPA North American Business News task NAB .", "These lattices were already determinized .", "Table 2 shows the average reduction factors we obtained when using the minimization algorithms with several subsequential lattices obtained for utterances of the NAB task .", "The reduction factors help to measure the gain of minimization alone , since the lattices are already subsequential .", "The numbers in example 2 , an example of reduction we obtained , correspond to a typical case .", "An important characteristic of the determinization algorithm is that it can be used on the fly .", "Indeed , the determinization algorithm is such that given a subset representing a state of the resulting transducer , the definition of the transitions leaving that state depends only on that state or , equivalently , on the states of that subset , and on the transducer to determinize .", "In particular , the definition and construction of these transitions do not depend directly on the previous subsets constructed .", "We have produced an implementation of the determinization that allows one both to completely expand the result or to expand it on demand .", "Arcs leaving a state of Mohri Transducers in Language and Speech the determinized transducer are expanded only if necessary This characteristic of the implementation is important .", "It can then be used , for instance , at any step in an onthe fly cascade of composition of transducers in speech recognition to expand only the necessary part of a lattice or transducer Pereira and Riley 1996 ; Mohri , Pereira , and Riley 1996 .", "One of the essential implications of the implementation is that it contributes to saving space during the search stage .", "It is also very useful in speeding up the n best decoder in speech recognition . '", "The determinization and minimization algorithms for string to weight transducers seem to have other applications in speech processing .", "Many new experiments can be done using these algorithms at different stages of speech recognition , which might lead to the reshaping of some of the methods used in this field and create a renewed interest in the theory of automata and transducers .", "We have briefly presented the theoretical bases , algorithmic tools , and practical use of a set of devices that seem to fit the complexity of language and provide efficiency in space and time .", "From the theoretical point of view , the understanding of these objects is crucial .", "It helps to describe the possibilities they offer and to guide algorithmic choices .", "Many new theoretical issues arise when more precision is sought .", "The notion of determinization can be generalized to that of E determinization for instance Salomaa and Soittola 1978 , chapter 3 , exercise requiring more general algorithms .", "It can also be extended to local determinization determinization at only those states of a transducer that admit a predefined property , such as that of having a large number of outgoing transitions .", "An important advantage of local determinization is that it can be applied to any transducer without restriction .", "Furthermore , local determinization also admits an on the fly implementation .", "New characterizations of rational functions shed new light on some aspects of the theory of finite state transducers Reutenauer and Schiitzenberger 1995 .", "We have also offered a generalization of the operations we use based on the notions of semiring and power series , which help to simplify problems and algorithms used in various cases .", "In particular , the string semirirtg that we introduced makes it conceptually easier to describe many algorithms and properties .", "Subsequential transducers admit very efficient algorithms .", "The determinization and minimization algorithms in the case of string to weight transducers presented here complete a large series of algorithms that have been shown to give remarkable results in natural language processing .", "Sequential machines lead to useful algorithms in many other areas of computational linguistics .", "In particular , subsequential power series allow for efficient results in indexation of natural language texts Crochemore 1986 ; Mohri 1996b .", "We briefly illustrated the application of these algorithms to speech recognition .", "More precision in acoustic modeling , finer language models , large lexicon grammars , and a larger vocabulary will lead , in the near future , to networks of much larger sizes in speech recognition .", "The determinization and minimization algorithms might help to limit the size of these networks while maintaining their time efficiency .", "These algorithms can also be used in text to speech synthesis .", "In fact , the same operations of composition of transducers Sproat 1995 and perhaps more important size issues can be found in this field .", "Subsequential power series S nonbisubsequential .", "The determinization algorithm for power series can also be used to minimize transducers in many cases .", "Let us first consider the case of automata .", "Brzozowski 1962 showed that determinization can be used to minimize automata .", "This nice result has also been proved more recently in elegant papers by Bauer 1988 and Urbanek 1989 .", "These authors refine the method to obtain better complexities . 19 Theorem 16 Brzozowski 1962 Let A be a nondeterministic automaton .", "Then the automaton A' Q' , i' , F' , E , 6' obtained by reversing A , applying determinization , rereversing the obtained automaton and determiruizing it is the minimal deterministic automaton equivalent to A .", "We generalize this theorem to the case of string to weight transducers .", "We say that a rational power series S is bisubsequential when S is subsequential and the power series SR EwEE .", "5 , wR w is also subsequential . '", "Not all subsequential transducers are bisubsequential .", "Figure 22 shows a transducer representing a power series S that is not bisubsequential .", "S is such that The transducer of Figure 22 is subsequential so S is subsequential .", "But the reverse SR is not , because it does not have bounded variation .", "Indeed , since We have Vn E AT , l SR , a quot ; b SR , an c I n 1 Mohri Transducers in Language and Speech A characterization similar to that of string to string transducers Choffrut 1978 is possible for bisubsequential power series defined on the tropical semiring .", "In particular , the theorem of the previous sections shows that S is bisubsequential if S and SR have bounded variation .", "We similarly define bideterminizable transducers as the transducers T defined on the tropical semiring admitting two applications of determinization , as follows In this definition , we assume that the reverse operation is performed simply by reversing the direction of the transitions and exchanging initial and final states .", "Given this definition , we can present the extension of the theorem of Brzozowski 1962 to bideterminizable transducers . 21 Theorem 17 Let T be a bideterminizable transducer defined on the tropical semiring .", "Then the transducer det klet TR r obtained by reversing T , applying determinization , rereversing the obtained transducer and determinizing it is a minimal subsequential transducer equivalent to T . Proof We denote by The double reverse and determinization algorithms clearly do not change the function that T realizes .", "So T' is a subsequential transducer equivalent to T . We only need to prove that T' is minimal .", "This is equivalent to showing that T quot ; is minimal , since T' and T quot ; have the same number of states .", "Ti is the result of a determinization , hence it is a trim subsequential transducer .", "We show that T' det n is minimal if Ti is a trim subsequential transducer .", "Notice that the theorem does not require that T be subsequential .", "Let Si and 52 be two states of T quot ; equivalent in the sense of automata .", "We prove that Si Sz , namely that no two distinct states of T quot ; can be merged .", "This will prove that T quot ; is minimal .", "Since pushing only affects the output labels , T' and T quot ; have the same set of states Q' Q quot ; .", "Hence Si and S2 are also states of T' .", "The states of T' can be viewed as weighted subsets whose state elements belong to T , , because T' is obtained by determinization of T . Let q , c E Qi X R . be a pair of the subset Si .", "Since Ti is trim there exists w E E such that Si ii , w q , so Si Si , w E F' .", "Since Si and S2 are equivalent , we also have 21 The theorem also holds in the case of string to string bideterminizable transducers .", "We give the proof in the more complex case of string to weight transducers .", "Minimal transducer 04 obtained by reversing 03 and applying determinization .", "6' S2 , 7 . 1 E F' .", "Since T1 is subsequential , there exists only one state of Tf admitting a path labeled with w to ii ; that state is q .", "Thus , q E S2 .", "Therefore any state q member of a pair of Si is also member of a pair of S2 .", "By symmetry the reverse is also true .", "Thus exactly the same states are members of the pairs of Si and S2 .", "There exists k 0 such that We prove that weights are also the same in Si and 52 .", "Let Hy , 0 j k , be the set of strings labeling the paths from i3 to qi in T1 . cffi ii , w is the weight output corresponding to a string w E R . Consider the accumulated weights c11 , 1 i 2 , 0 j k , in determinization of T . Each cl ; for instance corresponds to the weight not yet output in the paths reaching Si .", "It needs to be added to the weights of any path from qj E S1 to a final state in rev Ti .", "In other terms , the determinization algorithm will assign the weight c w Ai to a path labeled with wR reaching a final state of T' from Si .", "T quot ; is obtained by pushing from T' .", "Therefore the weight of such We noticed in the proof of the determinization theorem that the minimum weight of the pairs of any subset is 0 .", "Therefore Vj E O . k , C11 C21 and S2 S1 .", "This ends the proof of the theorem .", "0 Figures 23 25 illustrate the minimization of string to weight transducers using the determinization algorithm .", "The transducer 02 of Figure 23 is obtained from that of Figure 15 , IA , by reversing it .", "The application of determinization to 02 results in 03 Figure 24 .", "Notice that since 01 is subsequential , according to the theorem the transducer 03 is minimal too .", "03 is then reversed and determinized Figure 25 .", "The resulting transducer 04 is minimal and equivalent to 01 .", "Comparing the transducer 04 to the transducer of Figure 17 , Si , we note that both are minimal and realize the same function .", "Si provides output weights as soon as possible ; it can be obtained from 04 by pushing .", "I thank Michael Riley , and also CL reviewers , for their comments on earlier versions of this paper , Fernando Pereira and Michael Riley for discussions , Andrej Ljolje for providing the word lattices cited herein , Phil Terscaphen for useful advice , and Dominique Perrin for his help in finding references relating to the minimization of automata by determinization ."], "summary_lines": ["Finite-State Transducers In Language And Speech Processing\n", "Finite-state machines have been used in various domains of natural language processing.\n", "We consider here the use of a type of transducer that supports very efficient programs: sequential transducers.\n", "We recall classical theorems and give new ones characterizing sequential string-tostring transducers.\n", "Transducers that output weights also play an important role in language and speech processing.\n", "We give a specific study of string-to-weight transducers, including algorithms for determinizing and minimizing these transducers very efficiently, and characterizations of the transducers admitting determinization and the corresponding algorithms.\n", "Some applications of these algorithms in speech recognition are described and illustrated.\n", "Application of cascades of weighted string transducers (WSTs) has been well-studied in this work.\n"]}
{"article_lines": ["Multi Engine Machine Translation Guided By Explicit Word Matching", "We describe a new approach for synthetically combining the output of several different Machine Translation MT engines operating on the same input .", "The goal is to produce a synthetic combination that surpasses all of the original systems in translation quality .", "Our approach uses the individual MT engines as black boxes and does not require any explicit cooperation from the original MT systems .", "A decoding algorithm uses explicit word matches , in conjunction with confidence estimates for the various engines and a trigram language model in order to score and rank a collection of sentence hypotheses that are synthetic combinations of words from the various original engines .", "The highest scoring sentence hypothesis is selected as the final output of our system .", "Experiments , using several Arabicto English systems of similar quality , show a substantial improvement in the quality of the translation output .", "A variety of different paradigms for machine translation MT have been developed over the years , ranging from statistical systems that learn mappings between words and phrases in the source language and their corresponding translations in the target language , to Interlingua based systems that perform deep semantic analysis .", "Each approach and system has different advantages and disadvantages .", "While statistical systems provide broad coverage with little manpower , the quality of the corpus based systems rarely reaches the quality of knowledge based systems .", "With such a wide range of approaches to machine translation , it would be beneficial to have an effective framework for combining these systems into an MT system that carries many of the advantages of the individual systems and suffers from few of their disadvantages .", "Attempts at combining the output of different systems have proved useful in other areas of language technologies , such as the ROVER approach for speech recognition Fiscus 1997 .", "Several approaches to multi engine machine translation systems have been proposed over the past decade .", "The Pangloss system and work by several other researchers attempted to combine lattices from many different MT systems Frederking et Nirenburg 1994 , Frederking et al 1997 ; Tidhar K\u00fcssner 2000 ; Lavie , Probst et al . 2004 .", "These systems suffer from requiring cooperation from all the systems to produce compatible lattices as well as the hard research problem of standardizing confidence scores that come from the individual engines .", "In 2001 , Bangalore et al used string alignments between the different translations to train a finite state machine to produce a consensus translation .", "The alignment algorithm described in that work , which only allows insertions , deletions and substitutions , does not accurately capture long range phrase movement .", "In this paper , we propose a new way of combining the translations of multiple MT systems based on a more versatile word alignment algorithm .", "A decoding algorithm then uses these alignments , in conjunction with confidence estimates for the various engines and a trigram language model , in order to score and rank a collection of sentence hypotheses that are synthetic combinations of words from the various original engines .", "The highest scoring sentence hypothesis is selected as the final output of our system .", "We experimentally tested the new approach by combining translations obtained from combining three Arabic to English translation systems .", "Translation quality is scored using the METEOR MT evaluation metric Lavie , Sagae et al 2004 .", "Our experiments demonstrate that our new MEMT system achieves a substantial improvement over all of the original systems , and also outperforms an oracle capable of selecting the best of the original systems on a sentence by sentence basis .", "The remainder of this paper is organized as follows .", "In section 2 we describe the algorithm for generating multi engine synthetic translations .", "Section 3 describes the experimental setup used to evaluate our approach , and section 4 presents the results of the evaluation .", "Our conclusions and directions for future work are presented in section 5 .", "Our Multi Engine Machine Translation MEMT system operates on the single top best translation output produced by each of several MT systems operating on a common input sentence .", "MEMT first aligns the words of the different translation systems using a word alignment matcher .", "Then , using the alignments provided by the matcher , the system generates a set of synthetic sentence hypothesis translations .", "Each hypothesis translation is assigned a score based on the alignment information , the confidence of the individual systems , and a language model .", "The hypothesis translation with the best score is selected as the final output of the MEMT combination .", "The task of the matcher is to produce a wordto word alignment between the words of two given input strings .", "Identical words that appear in both input sentences are potential matches .", "Since the same word may appear multiple times in the sentence , there are multiple ways to produce an alignment between the two input strings .", "The goal is to find the alignment that represents the best correspondence between the strings .", "This alignment is defined as the alignment that has the smallest number of crossing edges .", "The matcher can also consider morphological variants of the same word as potential matches .", "To simultaneously align more than two sentences , the matcher simply produces alignments for all pair wise combinations of the set of sentences .", "In the context of its use within our MEMT approach , the word alignment matcher provides three main benefits .", "First , it explicitly identifies translated words that appear in multiple MT translations , allowing the MEMT algorithm to reinforce words that are common among the systems .", "Second , the alignment information allows the algorithm to ensure that aligned words are not included in a synthetic combination more than once .", "Third , by allowing long range matches , the synthetic combination generation algorithm can consider different plausible orderings of the matched words , based on their location in the original translations .", "After the matcher has word aligned the original system translations , the decoder goes to work .", "The hypothesis generator produces synthetic combinations of words and phrases from the original translations that satisfy a set of adequacy constraints .", "The generation algorithm is an iterative process and produces these translation hypotheses incrementally .", "In each iteration , the set of existing partial hypotheses is extended by incorporating an additional word from one of the original translations .", "For each partial hypothesis , a data structure keeps track of the words from the original translations which are accounted for by this partial hypothesis .", "One underlying constraint observed by the generator is that the original translations are considered in principle to be word synchronous in the sense that selecting a word from one original translation normally implies marking a corresponding word in each of the other original translations as used .", "The way this is determined is explained below .", "Two partial hypotheses that have the same partial translation , but have a different set of words that have been accounted for are considered different .", "A hypothesis is considered complete if the next word chosen to extend the hypothesis is the explicit end of sentence marker from one of the original translation strings .", "At the start of hypothesis generation , there is a single hypothesis , which has the empty string as its partial translation and where none of the words in any of the original translations are marked as used .", "In each iteration , the decoder extends a hypothesis by choosing the next unused word from one of the original translations .", "When the decoder chooses to extend a hypothesis by selecting word w from original system A , the decoder marks w as used .", "The decoder then proceeds to identify and mark as used a word in each of the other original systems .", "If w is aligned to words in any of the other original translation systems , then the words that are aligned with w are also marked as used .", "For each system that does not have a word that aligns with w , the decoder establishes an artificial alignment between w and a word in this system .", "The intuition here is that this artificial alignment corresponds to a different translation of the same source language word that corresponds to w . The choice of an artificial alignment cannot violate constraints that are imposed by alignments that were found by the matcher .", "If no artificial alignment can be established , then no word from this system will be marked as used .", "The decoder repeats this process for each of the original translations .", "Since the order in which the systems are processed matters , the decoder produces a separate hypothesis for each order .", "Each iteration expands the previous set of partial hypotheses , resulting in a large space of complete synthetic hypotheses .", "Since this space can grow exponentially , pruning based on scoring of the partial hypotheses is applied when necessary .", "A major component in the scoring of hypothesis translations is a confidence score that is assigned to each of the original translations , which reflects the translation adequacy of the system that produced it .", "We associate a confidence score with each word in a synthetic translation based on the confidence of the system from which it originated .", "If the word was contributed by several different original translations , we sum the confidences of the contributing systems .", "This word confidence score is combined multiplicatively with a score assigned to the word by a trigram language model .", "The score assigned to a complete hypothesis is its geometric average word score .", "This removes the inherent bias for shorter hypotheses that is present in multiplicative cumulative scores .", "The basic algorithm works well as long the original translations are reasonably word synchronous .", "This rarely occurs , so several additional constraints are applied during hypothesis generation .", "First , the decoder discards unused words in original systems that linger around too long .", "Second , the decoder limits how far ahead it looks for an artificial alignment , to prevent incorrect long range artificial alignments .", "Finally , the decoder does not allow an artificial match between words that do not share the same part of speech .", "We combined outputs of three Arabic to English machine translation systems on the 2003 TIDES Arabic test set .", "The systems were AppTek s rule based system , CMU s EBMT system , and Systran s web based translation system .", "We compare the results of MEMT to the individual online machine translation systems .", "We also compare the performance of MEMT to the score of an oracle system that chooses the best scoring of the individual systems for each sentence .", "Note that this oracle is not a realistic system , since a real system cannot determine at runtime which of the original systems is best on a sentence by sentence basis .", "One goal of the evaluation was to see how rich the space of synthetic translations produced by our hypothesis generator is .", "To this end , we also compare the output selected by our current MEMT system to an oracle system that chooses the best synthetic translation that was generated by the decoder for each sentence .", "This too is not a realistic system , but it allows us to see how well our hypothesis scoring currently performs .", "This also provides a way of estimating a performance ceiling of the MEMT approach , since our MEMT can only produce words that are provided by the original systems Hogan and Frederking 1998 .", "Due to the computational complexity of running the oracle system , several practical restrictions were imposed .", "First , the oracle system only had access to the top 1000 translation hypotheses produced by MEMT for each sentence .", "While this does not guarantee finding the best translation that the decoder can produce , this method provides a good approximation .", "We also ran the oracle experiment only on the first 140 sentences of the test sets due to time constraints .", "All the system performances are measured using the METEOR evaluation metric Lavie , Sagae et al . , 2004 .", "METEOR was chosen since , unlike the more commonly used BLEU metric Papineni et al . , 2002 , it provides reasonably reliable scores for individual sentences .", "This property is essential in order to run our oracle experiments .", "METEOR produces scores in the range of 0 , 1 , based on a combination of unigram precision , unigram recall and an explicit penalty related to the average length of matched segments between the evaluated translation and its reference .", "On the 2003 TIDES data , the three original systems had similar METEOR scores .", "Table 1 shows the scores of the three systems , with their names obscured to protect their privacy .", "Also shown are the score of MEMT s output and the score of the oracle system that chooses the best original translation on a sentence by sentence basis .", "The score of the MEMT system is significantly better than any of the original systems , and the sentence oracle .", "On the first 140 sentences , the oracle system that selects the best hypothesis translation generated by the MEMT generator has a METEOR score of 0 . 5883 .", "This indicates that the scoring algorithm used to select the final MEMT output can be significantly further improved .", "Our MEMT algorithm shows consistent improvement in the quality of the translation compared any of the original systems .", "It scores better than an oracle that chooses the best original translation on a sentence by sentence basis .", "Furthermore , our MEMT algorithm produces hypotheses that are of yet even better quality , but our current scoring algorithm is not yet able to effectively select the best hypothesis .", "The focus of our future work will thus be on identifying features that support improved hypothesis scoring .", "This research work was partly supported by a grant from the US Department of Defense .", "The word alignment matcher was developed by Satanjeev Banerjee .", "We wish to thank Robert Frederking , Ralf Brown and Jaime Carbonell for their valuable input and suggestions ."], "summary_lines": ["Multi-Engine Machine Translation Guided By Explicit Word Matching\n", "We describe a new approach for synthetically combining the output of several different Machine Translation (MT) engines operating on the same input.\n", "The goal is to produce a synthetic combination that surpasses all of the original systems in translation quality.\n", "Our approach uses the individual MT engines as \"black boxes\" and does not require any explicit cooperation from the original MT systems.\n", "A decoding algorithm uses explicit word matches, in conjunction with confidence estimates for the various engines and a trigram language model in order to score and rank a collection of sentence hypotheses that are synthetic combinations of words from the various original engines.\n", "The highest scoring sentence hypothesis is selected as the final output of our system.\n", "Experiments, using several Arabic-to-English systems of similar quality, show a substantial improvement in the quality of the translation output.\n", "We propose a heuristic-based matching algorithm which allows non monotonic alignments to align the words between the hypotheses.\n"]}
{"article_lines": ["Learning To Recognize Features Of Valid Textual Entailments", "This paper advocates a new architecture for textual inference in which finding a good alignment is separated from evaluating entailment .", "Current approaches to semantic inference in question answering and textual entailment have approximated the entailment problem as that of computing the best alignment of the hypothesis to the text , using a locally decomposable matching score .", "We argue that there are significant weaknesses in this approach , including flawed assumptions of monotonicity and locality .", "Instead we propose a pipelined approach where alignment is followed by a classification step , in which we extract features representing high level characteristics of the entailment problem , and pass the resulting feature vector to a statistical classifier trained on development data .", "We report results on data from the 2005 Pascal RTE Challenge which surpass previously reported results for alignment based systems .", "During the last five years there has been a surge in work which aims to provide robust textual inference in arbitrary domains about which the system has no expertise .", "The best known such work has occurred within the field of question answering Pasca and Harabagiu , 2001 ; Moldovan et al . , 2003 ; more recently , such work has continued with greater focus in addressing the PASCAL Recognizing Textual Entailment RTE Challenge Dagan et al . , 2005 and within the U . S . Government AQUAINT program .", "Substantive progress on this task is key to many text and natural language applications .", "If one could tell that Protestors chanted slogans opposing a free trade agreement was a match for people demonstrating against free trade , then one could offer a form of semantic search not available with current keywordbased search .", "Even greater benefits would flow to richer and more semantically complex NLP tasks .", "Because full , accurate , open domain natural language understanding lies far beyond current capabilities , nearly all efforts in this area have sought to extract the maximum mileage from quite limited semantic representations .", "Some have used simple measures of semantic overlap , but the more interesting work has largely converged on a graphalignment approach , operating on semantic graphs derived from syntactic dependency parses , and using a locally decomposable alignment score as a proxy for strength of entailment .", "Below , we argue that even approaches relying on weighted abduction may be seen in this light .", "In this paper , we highlight the fundamental semantic limitations of this type of approach , and advocate a multi stage architecture that addresses these limitations .", "The three key limitations are an assumption of monotonicity , an assumption of locality , and a confounding of alignment and evaluation of entailment .", "We focus on the PASCAL RTE data , examples from which are shown in table 1 .", "This data set contains pairs consisting of a short text followed by a one sentence hypothesis .", "The goal is to say whether the hypothesis follows from the text and general background knowledge , according to the intuitions of an intelligent human reader .", "That is , the standard is not whether the hypothesis is logically entailed , but whether it can reasonably be inferred .", "In this section we try to give a unifying overview to current work on robust textual inference , to present fundamental limitations of current methods , and then to outline our approach to resolving them .", "Nearly all current textual inference systems use a single stage matching proof process , and differ mainly in the sophistication of the matching stage .", "The simplest approach is to base the entailment prediction on the degree of semantic overlap between the text and hypothesis using models based on bags of words , bags of n grams , TF IDF scores , or something similar Jijkoun and de Rijke , 2005 .", "Such models have serious limitations semantic overlap is typically a symmetric relation , whereas entailment is clearly not , and , because overlap models do not account for syntactic or semantic structure , they are easily fooled by examples like ID 2081 .", "A more structured approach is to formulate the entailment prediction as a graph matching problem Haghighi et al . , 2005 ; de Salvo Braz et al . , 2005 .", "In this formulation , sentences are represented as normalized syntactic dependency graphs like the one shown in figure 1 and entailment is approximated with an alignment between the graph representing the hypothesis and a portion of the corresponding graph s representing the text .", "Each possible alignment of the graphs has an associated score , and the score of the best alignment is used as an approximation to the strength of the entailment a betteraligned hypothesis is assumed to be more likely to be entailed .", "To enable incremental search , alignment scores are usually factored as a combination of local terms , corresponding to the nodes and edges of the two graphs .", "Unfortunately , even with factored scores the problem of finding the best alignment of two graphs is NP complete , so exact computation is intractable .", "Authors have proposed a variety of approximate search techniques .", "Haghighi et al . 2005 divide the search into two steps in the first step they consider node scores only , which relaxes the problem to a weighted bipartite graph matching that can be solved in polynomial time , and in the second step they add the edges scores and hillclimb the alignment via an approximate local search .", "A third approach , exemplified by Moldovan et al . 2003 and Raina et al .", "2005 , is to translate dependency parses into neo Davidsonian style quasilogical forms , and to perform weighted abductive theorem proving in the tradition of Hobbs et al . , 1988 .", "Unless supplemented with a knowledge base , this approach is actually isomorphic to the graph matching approach .", "For example , the graph in figure 1 might generate the quasi LF rose e1 , nsubj e1 , x1 , sales x1 , nn x1 , x2 , Mitsubishi x2 , dobj e1 , x3 , percent x3 , num x3 , x4 , 46 x4 .", "There is a term corresponding to each node and arc , and the resolution steps at the core of weighted abduction theorem proving consider matching an individual node of the hypothesis e . g . rose e1 with something from the text e . g . fell e1 , just as in the graph matching approach .", "The two models become distinct when there is a good supply of additional linguistic and world knowledge axioms as in Moldovan et al . 2003 but not Raina et al .", "Then the theorem prover may generate intermediate forms in the proof , but , nevertheless , individual terms are resolved locally without reference to global context .", "Finally , a few efforts Akhmatova , 2005 ; Fowler et al . , 2005 ; Bos and Markert , 2005 have tried to translate sentences into formulas of first order logic , in order to test logical entailment with a theorem prover .", "While in principle this approach does not suffer from the limitations we describe below , in practice it has not borne much fruit .", "Because few problem sentences can be accurately translated to logical form , and because logical entailment is a strict standard , recall tends to be poor .", "The simple graph matching formulation of the problem belies three important issues .", "First , the above systems assume a form of upward monotonicity if a good match is found with a part of the text , other material in the text is assumed not to affect the validity of the match .", "But many situations lack this upward monotone character .", "Consider variants on ID 98 .", "Suppose the hypothesis were Arafat targeted for assassination .", "This would allow a perfect graph match or zero cost weighted abductive proof , because the hypothesis is a subgraph of the text .", "However , this would be incorrect because it ignores the modal operator could .", "Information that changes the validity of a proof can also exist outside a matching clause .", "Consider the alternate text Sharon denies Arafat is targeted for assassination . 1 The second issue is the assumption of locality .", "Locality is needed to allow practical search , but many entailment decisions rely on global features of the alignment , and thus do not naturally factor by nodes and edges .", "To take just one example , dropping a restrictive modifier preserves entailment in a positive context , but not in a negative one .", "For example , Dogs barked loudly entails Dogs barked , but No dogs barked loudly does not entail No dogs barked .", "These more global phenomena cannot be modeled with a factored alignment score .", "The last issue arising in the graph matching approaches is the inherent confounding of alignment and entailment determination .", "The way to show that one graph element does not follow from another is to make the cost of aligning them high .", "However , since we are embedded in a search for the lowest cost alignment , this will just cause the system to choose an alternate alignment rather than recognizing a non entailment .", "In ID 152 , we would like the hypothesis to align with the first part of the text , to be able to prove that civilians are not members of law enforcement agencies and conclude that the hypothesis does not follow from the text .", "But a graphmatching system will to try to get non entailment by making the matching cost between civilians and members of law enforcement agencies be very high .", "However , the likely result of that is that the final part of the hypothesis will align with were civilians at the end of the text , assuming that we allow an alignment with loose arc correspondence . 2 Under this candidate alignment , the lexical alignments are perfect , and the only imperfect alignment is the subject arc of were is mismatched in the two .", "A robust inference guesser will still likely conclude that there is entailment .", "We propose that all three problems can be resolved in a two stage architecture , where the alignment phase is followed by a separate phase of entailment determination .", "Although developed independently , the same division between alignment and classification has also been proposed by Marsi and Krahmer 2005 , whose textual system is developed and evaluated on parallel translations into Dutch .", "Their classification phase features an output space of five semantic relations , and performs well at distinguishing entailing sentence pairs .", "Finding aligned content can be done by any search procedure .", "Compared to previous work , we emphasize structural alignment , and seek to ignore issues like polarity and quantity , which can be left to a subsequent entailment decision .", "For example , the scoring function is designed to encourage antonym matches , and ignore the negation of verb predicates .", "The ideas clearly generalize to evaluating several alignments , but we have so far worked with just the one best alignment .", "Given a good alignment , the determination of entailment reduces to a simple classification decision .", "The classifier is built over features designed to recognize patterns of valid and invalid inference .", "Weights for the features can be hand set or chosen to minimize a relevant loss function on training data using standard techniques from machine learning .", "Because we already have a complete alignment , the classifier s decision can be conditioned on arbitrary global features of the aligned graphs , and it can detect failures of monotonicity .", "Our system has three stages linguistic analysis , alignment , and entailment determination .", "Our goal in this stage is to compute linguistic representations of the text and hypothesis that contain as much information as possible about their semantic content .", "We use typed dependency graphs , which contain a node for each word and labeled edges representing the grammatical relations between words .", "Figure 1 gives the typed dependency graph for ID 971 .", "This representation contains much of the information about words and relations between them , and is relatively easy to compute from a syntactic parse .", "However many semantic phenomena are not represented properly ; particularly egregious is the inability to represent quantification and modality .", "We parse input sentences to phrase structure trees using the Stanford parser Klein and Manning , 2003 , a statistical syntactic parser trained on the Penn TreeBank .", "To ensure correct parsing , we preprocess the sentences to collapse named entities into new dedicated tokens .", "Named entities are identified by a CRF based NER system , similar to that described in McCallum and Li , 2003 .", "After parsing , contiguous collocations which appear in WordNet Fellbaum , 1998 are identified and grouped .", "We convert the phrase structure trees to typed dependency graphs using a set of deterministic handcoded rules de Marneffe et al . , 2006 .", "In these rules , heads of constituents are first identified using a modified version of the Collins head rules that favor semantic heads such as lexical verbs rather than auxiliaries , and dependents of heads are typed using tregex patterns Levy and Andrew , 2006 , an extension of the tgrep pattern language .", "The nodes in the final graph are then annotated with their associated word , part of speech given by the parser , lemma given by a finite state transducer described by Minnen et al . 2001 and named entity tag .", "The purpose of the second phase is to find a good partial alignment between the typed dependency graphs representing the hypothesis and the text .", "An alignment consists of a mapping from each node word in the hypothesis graph to a single node in the text graph , or to null . 3 Figure 1 gives the alignment for ID 971 .", "The space of alignments is large there are O m 1 ' possible alignments for a hypothesis graph with n nodes and a text graph with m nodes .", "We define a measure of alignment quality , and a procedure for identifying high scoring alignments .", "We choose a locally decomposable scoring function , such that the score of an alignment is the sum of the local node and edge alignment scores .", "Unfortunately , there is no polynomial time algorithm for finding the exact best alignment .", "Instead we use an incremental beam search , combined with a node ordering heuristic , to do approximate global search in the space of possible alignments .", "We have experimented with several alternative search techniques , and found that the solution quality is not very sensitive to the specific search procedure used .", "Our scoring measure is designed to favor alignments which align semantically similar subgraphs , irrespective of polarity .", "For this reason , nodes receive high alignment scores when the words they represent are semantically similar .", "Synonyms and antonyms receive the highest score , and unrelated words receive the lowest .", "Our hand crafted scoring metric takes into account the word , the lemma , and the part of speech , and searches for word relatedness using a range of external resources , including WordNet , precomputed latent semantic analysis matrices , and special purpose gazettes .", "Alignment scores also incorporate local edge scores , which are based on the shape of the paths between nodes in the text graph which correspond to adjacent nodes in the hypothesis graph .", "Preserved edges receive the highest score , and longer paths receive lower scores .", "In the final stage of processing , we make a decision about whether or not the hypothesis is entailed by the text , conditioned on the typed dependency graphs , as well as the best alignment between them .", "Because we have a data set of examples that are labeled for entailment , we can use techniques from supervised machine learning to learn a classifier .", "We adopt the standard approach of defining a featural representation of the problem and then learning a linear decision boundary in the feature space .", "We focus here on the learning methodology ; the next section covers the definition of the set of features .", "Defined in this way , one can apply any statistical learning algorithm to this classification task , such as support vector machines , logistic regression , or naive Bayes .", "We used a logistic regression classifier with a Gaussian prior parameter for regularization .", "We also compare our learning results with those achieved by hand setting the weight parameters for the classifier , effectively incorporating strong prior human knowledge into the choice of weights .", "An advantage to the use of statistical classifiers is that they can be configured to output a probability distribution over possible answers rather than just the most likely answer .", "This allows us to get confidence estimates for computing a confidence weighted score see section 5 .", "A major concern in applying machine learning techniques to this classification problem is the relatively small size of the training set , which can lead to overfitting problems .", "We address this by keeping the feature dimensionality small , and using high regularization penalties in training .", "In the entailment determination phase , the entailment problem is reduced to a representation as a vector of 28 features , over which the statistical classifier described above operates .", "These features try to capture salient patterns of entailment and non entailment , with particular attention to contexts which reverse or block monotonicity , such as negations and quantifiers .", "This section describes the most important groups of features .", "Polarity features .", "These features capture the presence or absence of linguistic markers of negative polarity contexts in both the text and the hypothesis , such as simple negation not , downward monotone quantifiers no , few , restricting prepositions without , except and superlatives tallest .", "Adjunct features .", "These indicate the dropping or adding of syntactic adjuncts when moving from the text to the hypothesis .", "For the common case of restrictive adjuncts , dropping an adjunct preserves truth Dogs barked loudly Dogs barked , while adding an adjunct does not Dogs barked K Dogs barked today .", "However , in negative polarity contexts such as No dogs barked , this heuristic is reversed adjuncts can safely be added , but not dropped .", "For example , in ID 59 , the hypothesis aligns well with the text , but the addition of in Iraq indicates non entailment .", "We identify the root nodes of the problem the root node of the hypothesis graph and the corresponding aligned node in the text graph .", "Using dependency information , we identify whether adjuncts have been added or dropped .", "We then determine the polarity negative context , positive context or restrictor of a universal quantifier of the two root nodes to generate features accordingly .", "Antonymy features .", "Entailment problems might involve antonymy , as in ID 971 .", "We check whether an aligned pairs of text hypothesis words appear to be antonymous by consulting a pre computed list of about 40 , 000 antonymous and other contrasting pairs derived from WordNet .", "For each antonymous pair , we generate one of three boolean features , indicating whether i the words appear in contexts of matching polarity , ii only the text word appears in a negative polarity context , or iii only the hypothesis word does .", "Modality features .", "Modality features capture simple patterns of modal reasoning , as in ID 98 , which illustrates the heuristic that possibility does not entail actuality .", "According to the occurrence or not of predefined modality markers , such as must or maybe , we map the text and the hypothesis to one of six modalities possible , not possible , actual , not actual , necessary , and not necessary .", "The text hypothesis modality pair is then mapped into one of the following entailment judgments yes , weak yes , don t know , weak no , or no .", "For example not possible not actual ?", "yes possible necessary ?", "weak no Factivity features .", "The context in which a verb phrase is embedded may carry semantic presuppositions giving rise to non entailments such as The gangster tried to escape 6 The gangster escaped .", "This pattern of entailment , like others , can be reversed by negative polarity markers The gangster managed to escape The gangster escaped while The gangster didn t manage to escape 6 The gangster escaped .", "To capture these phenomena , we compiled small lists of factive and non factive verbs , clustered according to the kinds of entailments they create .", "We then determine to which class the parent of the text aligned with the hypothesis root belongs to .", "If the parent is not in the list , we only check whether the embedding text is an affirmative context or a negative one .", "Quantifier features .", "These features are designed to capture entailment relations among simple sentences involving quantification , such as Every company must report A company must report or The company , or IBM .", "No attempt is made to handle multiple quantifiers or scope ambiguities .", "Each quantifier found in an aligned pair of text hypothesis words is mapped into one of five quantifier categories no , some , many , most , and all .", "The no category is set apart , while an ordering over the other four categories is defined .", "The some category also includes definite and indefinite determiners and small cardinal numbers .", "A crude attempt is made to handle negation by interchanging no and all in the presence of negation .", "Features are generated given the categories of both hypothesis and text .", "Number , date , and time features .", "These are designed to recognize mis matches between numbers , dates , and times , as in IDs 1806 and 231 .", "We do some normalization e . g . of date representations and have a limited ability to do fuzzy matching .", "In ID 1806 , the mismatched years are correctly identified .", "Unfortunately , in ID 231 the significance of over is not grasped and a mismatch is reported .", "Alignment features .", "Our feature representation includes three real valued features intended to represent the quality of the alignment score is the raw score returned from the alignment phase , while goodscore and badscore try to capture whether the alignment score is good or bad by computing the sigmoid function of the distance between the alignment score and hard coded good and bad reference values .", "We present results based on the First PASCAL RTE Challenge , which used a development set containing 567 pairs and a test set containing 800 pairs .", "The data sets are balanced to contain equal numbers of yes and no answers .", "The RTE Challenge recommended two evaluation metrics raw accuracy and confidence weighted score CWS .", "The CWS is computed as follows for each positive integer k up to the size of the test set , we compute accuracy over the k most confident predictions .", "The CWS is then the average , over k , of these partial accuracies .", "Like raw accuracy , it lies in the interval 0 , 1 , but it will exceed raw accuracy to the degree that predictions are well calibrated .", "Several characteristics of the RTE problems should be emphasized .", "Examples are derived from a broad variety of sources , including newswire ; therefore systems must be domain independent .", "The inferences required are , from a human perspective , fairly superficial no long chains of reasoning are involved .", "However , there are trick questions expressly designed to foil simplistic techniques .", "The definition of entailment is informal and approximate whether a competent speaker with basic knowledge of the world would typically infer the hypothesis from the text .", "Entailments will certainly depend on linguistic knowledge , and may also depend on world knowledge ; however , the scope of required world knowledge is left unspecified . 4 Despite the informality of the problem definition , human judges exhibit very good agreement on the RTE task , with agreement rate of 91 96 Dagan et al . , 2005 .", "In principle , then , the upper bound for machine performance is quite high .", "In practice , however , the RTE task is exceedingly difficult for computers .", "Participants in the first PASCAL RTE workshop reported accuracy from 49 to 59 , and CWS from 50 . 0 to 69 . 0 Dagan et al . , 2005 .", "Table 2 shows results for a range of systems and testing conditions .", "We report accuracy and CWS on each RTE data set .", "The baseline for all experiments is random guessing , which always attains 50 accuracy .", "We show comparable results from recent systems based on lexical similarity Jijkoun and de Rijke , 2005 , graph alignment Haghighi et al . , 2005 , weighted abduction Raina et al . , 2005 , and a mixed system including theorem proving Bos and Markert , 2005 .", "We then show results for our system under several different training regimes .", "The row labeled alignment only describes experiments in which all features except the alignment score are turned off .", "We predict entailment just in case the alignment score exceeds a threshold which is optimized on development data .", "Hand tuning describes experiments in which all features are on , but no training occurs ; rather , weights are set by hand , according to human intuition .", "Finally , learning describes experiments in which all features are on , and feature weights are trained on the development data .", "The 4Each RTE problem is also tagged as belonging to one of seven tasks .", "Previous work Raina et al . , 2005 has shown that conditioning on task can significantly improve accuracy .", "In this work , however , we ignore the task variable , and none of the results shown in table 2 reflect optimization by task . figures reported for development data performance therefore reflect overfitting ; while such results are not a fair measure of overall performance , they can help us assess the adequacy of our feature set if our features have failed to capture relevant aspects of the problem , we should expect poor performance even when overfitting .", "It is therefore encouraging to see CWS above 70 .", "Finally , the figures reported for test data performance are the fairest basis for comparison .", "These are significantly better than our results for alignment only Fisher s exact test , p 0 . 05 , indicating that we gain real value from our features .", "However , the gain over comparable results from other teams is not significant at the p 0 . 05 level .", "A curious observation is that the results for handtuned weights are as good or better than results for learned weights .", "A possible explanation runs as follows .", "Most of the features represent high level patterns which arise only occasionally .", "Because the training data contains only a few hundred examples , many features are active in just a handful of instances ; their learned weights are therefore quite noisy .", "Indeed , a feature which is expected to favor entailment may even wind up with a negative weight the modal feature weak yes is an example .", "As shown in table 3 , the learned weight for this feature was strongly negative but this resulted from a single training example in which the feature was active but the hypothesis was not entailed .", "In such cases , we shouldn t expect good generalization to test data , and human intuition about the value of specific features may be more reliable .", "Table 3 shows the values learned for selected feature weights .", "As expected , the features added adjunct in all context , modal yes , and text is factive were all found to be strong indicators of entailment , while date insert , date modifier insert , widening from text to hyp all indicate lack of entailment .", "Interestingly , text has neg marker and text hyp diffpolarity were also found to disfavor entailment ; while this outcome is sensible , it was not anticipated or designed .", "The best current approaches to the problem of textual inference work by aligning semantic graphs , using a locally decomposable alignment score as a proxy for strength of entailment .", "We have argued that such models suffer from three crucial limitations an assumption of monotonicity , an assumption of locality , and a confounding of alignment and entailment determination .", "We have described a system which extends alignment based systems while attempting to address these limitations .", "After finding the best alignment between text and hypothesis , we extract highlevel semantic features of the entailment problem , and input these features to a statistical classifier to make an entailment decision .", "Using this multi stage architecture , we report results on the PASCAL RTE data which surpass previously reported results for alignment based systems .", "We see the present work as a first step in a promising direction .", "Much work remains in improving the entailment features , many of which may be seen as rough approximations to a formal monotonicity calculus .", "In future , we aim to combine more precise modeling of monotonicity effects with better modeling of paraphrase equivalence .", "We thank Anna Rafferty , Josh Ainslie , and particularly Roger Grosse for contributions to the ideas and system reported here .", "This work was supported in part by the Advanced Research and Development Activity ARDA s Advanced Question Answering for Intelligence AQUAINT Program ."], "summary_lines": ["Learning To Recognize Features Of Valid Textual Entailments\n", "This paper advocates a new architecture for textual inference in which finding a good alignment is separated from evaluating entailment.\n", "Current approaches to semantic inference in question answering and textual entailment have approximated the entailment problem as that of computing the best alignment of the hypothesis to the text, using a locally decomposable matching score.\n", "We argue that there are significant weaknesses in this approach, including flawed assumptions of monotonicity and locality.\n", "Instead we propose a pipelined approach where alignment is followed by a classification step, in which we extract features representing high-level characteristics of the entailment problem, and pass the resulting feature vector to a statistical classifier trained on development data.\n", "We report results on data from the 2005 Pascal RTE Challenge which surpass previously reported results for alignment-based systems.\n", "We emphasize that there is more to inferential validity than close lexical or structural correspondence: negations, models, non-factive and implicative verbs, and other linguistic constructs can affect validity in ways hard to capture in alignment.\n"]}
{"article_lines": ["Coping With Syntactic Ambiguity Or How To Put The Block In The Box On The Table", "we construct a table so that the entry in the tells the parser how to parse i occurrences of 9 .", "An Example Suppose for example that we were given the following grammar 40a S NP VP ADJS 40b S V NP PP ADJS ADJS 40c VP 0 .", "V NP PP ADJS 40d PP P NP 40e NP NI NP PP ADJS adj ADJS I In this example we will assume no lexical ambiguity V , P , inspection , we notice that NP are Catalan grammars and that ADJS is a Step grammar .", "PP E i 0 NP N ADJS With these observations , the parser can process PPs , and by counting the number of occurrencof terminal symbols and looking up numbers in the appropriate tables .", "We now substitute 41a c into 40c .", "42 VP V NP 1 PP ADJS V N E N ' E E and simplify the convolution of the two Catalan functions VP V N E adj' so that the parser can also find VPs by just counting coccurrences of terminal symbols .", "Now we simplify so that can also be parsed by just counting occurrences of terminal symbols . translate 40a b into the equation 44 S NP VP ADJS V NP 1 PP ADJS ADJS and then expand VP using 42 45 S NP V NP 1 PP ADJS ADJS V NP 1 PP ADJS ADJS and factor S NP 1 V NP 1 PP That can be simplified considerably because NP 1 PP N E E N E and 48 E adj' E adj' i so that S N E 1 N E Cat .", "14 i has the following Jump Jump 1 , i 1 50 The entire example grammar has now been compiled into a form that is easier for parsing .", "This formula says that sentences are all of the form 51 S N P N V N P N adj which could be recognized by the following finite state machine 52 c Journal of Computational Linguistics , Volume 8 , Number 3 4 , July December 1982 Kenneth Church and Ramesh Patil Coping with Syntactic Ambiguity Furthermore , the number of parse trees for a given input sentence can be found by multiplying three numbers a the Catalan of the number of P N's before the verb , b the Catalan of one more than the number of P N's after the verb , and c the ramp of the number of adj's .", "For example , the sentence 53 The man on the hill saw the boy with a telescope yesterday in the morning .", "Cat 3 6 parses .", "That is , there is one way to parse quot ; the man on the hill , quot ; two ways to parse quot ; saw the boy with a telescope quot ; quot ; telescope quot ; is either a complement of quot ; see quot ; as in 54a c or is attached to quot ; boy quot ; as in 54d f , and three ways to parse the adjuncts they could both attach to the S 54a , d , or they could both attach to the VP 54b , e , or they could split 54c , f .", "54a The man on the hill saw the boy with a telescope yesterday in the morning .", "54b The man on the hill saw the boy with a telescope yesterday in the morning .", "54c The man on the hill saw the boy with a telescope yesterday in the morning .", "54d The man on the hill saw the boy with a telescope yesterday in the morning .", "54e The man on the hill saw the boy with a telescope yesterday in the morning .", "54f The man on the hill saw the boy with a telescope yesterday in the morning .", "All and only these possibilities are permitted by the grammar .", "Conclusion We began our discussion with the observation that certain grammars are quot ; every way ambiguous quot ; and suggested that this observation could lead to improved parsing performance .", "Catalan grammars were then introduced to remedy the situation so that the processor can delay attachment decisions until it discovers some more useful constraints .", "Until such time , the processor can do little more than note that the input sentence is quot ; every way ambiguous . quot ; We suggested that a table lookup scheme might be an effective method to implement such a processor .", "We then introduced rules for combining primitive grammars , such as Catalan grammars , into composite grammars .", "This linear systems view quot ; bundles up quot ; all the parse trees into a single concise description capable of telling us everything we might want to know about the parses including how much it might cost to ask a particular question .", "This abstract view of ambiguity enables us to ask questions in the most convenient order , and to delay asking until it is clear that the pay off will exceed the cost .", "This abstraction was strongly influenced by the notion of binding .", "We have presented combination rules in three different representation systems power series , ATNs , and context free grammars , each of which contributed its own insights .", "Power series are convenient for defining the algebraic operations , ATNs are most suited for discussing implementation issues , and context free grammars enable the shortest derivations .", "Perhaps the following quotation best summarizes our motivation for alternating among these three representation systems thing or idea seems meaningful only when we have different ways to represent it different perspectives and different associations .", "Then you can turn it around in your mind , so to speak ; however , it seems at the moment you can see it another way ; you never come to a full stop .", "Minsky 1981 , p . 19 In each of these representation schemes , we have introduced five primitive grammars Catalan , Unit Step , 1 , and 0 , and terminals ; and four composition rules addition , subtraction , multiplication , and division .", "We have seen that it is often possible to employ these analytic tools in order to re organize compile the grammar into a form more suitable for processing efficiently .", "We have identified certain where the ambiguity is combinatoric , and have sketched a few modifications to the grammar that enable processing to proceed in a more efficient manner .", "In particular , we have observed it to be important for the grammar to avoid referencing quantities that are not easily determined , such as the dividing point between a noun phrase and a prepositional phrase as in 55 Put the block in the box on the table in the kitchen . . . We have seen that the desired re organization can be achieved by taking advantage of the fact that the autoconvolution of a Catalan series produces another Caseries .", "This reduced processing time from to almost linear time .", "Similar analyses have been discussed for a number of lexically and structurally ambiguous constructions , culminating with the example in section 9 , where we transformed a grammar into a form that could be parsed by a single left to right pass over the terminal elements .", "Currently , these grammar reformulations have to be performed by hand .", "It ought to be possible to automate this process so that the reformulations could be performed by a grammar compiler .", "We leave this project open for future research .", "Acknowledgments We would like to thank Jon Allen , Sarah Ferguson , Lowell Hawkinson , Kris Halvorsen , Bill Long , Mitch Marcus , Rohit Parikh , and Peter Szolovits for their very useful comments on earlier drafts .", "We would Journal of Computational Linguistics , Volume 8 , Number 3 4 , July December 1982 especially like to thank Bill Martin for initiating the project .", "Sentences are far more ambiguous than one might have thought .", "There may be hundreds , perhaps thousands , of syntactic parse trees for certain very natural sentences of English .", "This fact has been a major problem confronting natural language processing , especially when a large percentage of the syntactic parse trees are enumerated during semantic pragmatic processing .", "In this paper we propose some methods for dealing with syntactic ambiguity in ways that exploit certain regularities among alternative parse trees .", "These regularities will be expressed as linear combinations of ATN networks , and also as sums and products of formal power series .", "We believe that such encoding of ambiguity will enhance processing , whether syntactic and semantic constraints are processed separately in sequence or interleaved together .", "Most parsers find the set of parse trees by starting with the empty set and adding to it each time they find a new possibility .", "We make the observation that in certain situations it would be much more efficient to work in the other direction , starting from the universal set i . e , the set of all binary trees and ruling trees out when the parser decides that they cannot be parses .", "Ruling out is easier when the set of parse trees is closer to the universal set and ruling in is easier when the set of parse trees is closer to the empty set .", "Rulingout is particularly suited for quot ; every way ambiguous quot ; constructions such as prepositional phrases that have just as many parse trees as there are binary trees over the terminal elements .", "Since every tree is a parse , the parser doesn't have to rule any of them out .", "In some sense , this is a formalization of an idea that has been in the literature for some time .", "That is , it has been noticed for a long time that these sorts of very ambiguous constructions are very difficult for most parsing algorithms , but apparently not for people .", "This observation has led some researchers to hypothesize additional parsing mechanisms , such as pseudo attachment Church 1980 , pp .", "65 71 2 and permanent predictable ambiguity Sager 1973 , so that the parser could quot ; attach all ways quot ; in a single step .", "However , these mechanisms have always lacked a precise interpretation ; we will present a much more formal way of coping with quot ; every way ambiguous quot ; grammars , defined in terms of Catalan numbers Knuth 1975 , pp .", "Sentences are far more ambiguous than one might have thought .", "Our experience with the EQSP parser Martin , Church , and Patil 1981 indicates that there may be hundreds , perhaps thousands , of syntactic parse trees for certain very natural sentences of English .", "For example , consider the following sentence with two prepositional phrases 2 The idea of pseudo attachment was first proposed by Marcus private communication , though Marcus does not accept the formulation in Church 1980 .", "Copyright 1982 by the Association for Computational Linguistics .", "Permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the Journal reference and this copyright notice are included on the first page .", "To copy otherwise , or to republish , requires a fee and or specific permission .", "These syntactic ambiguities grow quot ; combinatorially quot ; with the number of prepositional phrases .", "For example , when a third PP is added to the sentence above , there are five interpretations When a fourth PP is added , there are fourteen trees , and so on .", "This sort of combinatoric ambiguity has been a major problem confronting natural language processing .", "In this paper we propose some methods for dealing with syntactic ambiguity in ways that take advantage of regularities among the alternative parse trees .", "In particular , we observe that enumerating the parse trees as above fails to capture the important generalization that prepositional phrases are quot ; every way ambiguous , quot ; or more precisely , the set of parse trees over i PPs is the same as the set of binary trees that can be constructed over i terminal elements .", "Notice , for example , that there are two possible binary trees over three elements , corresponding to 2a and 2b , respectively , and that there are five binary trees over four elements corresponding to 3a 3c , respectively .", "PPs , adjuncts , conjuncts , noun noun modification , stack relative clauses , and other quot ; every way ambiguous quot ; constructions will be treated as primitive objects .", "They can be combined in various ways to produce composite constructions , such as lexical ambiguity , which may also be very ambiguous but not necessarily quot ; every way ambiguous . quot ; Lexical ambiguity , for example , will be analyzed as the sum of its senses , or in flow graph terminology Oppenheim and Schafer 1975 as a parallel connection of its senses .", "Structural ambiguity , on the other hand , will be analyzed as the product of its components , or in flow graph terminology as a series connection .", "This section will make the linear systems analogy more precise by relating context free grammars to formal power series polynominals .", "Formal power series are a well known device in the formal language literature e . g . , Salomaa 1973 for developing the algebraic properties of context free grammars .", "We introduce them here to establish a formal basis for our upcoming discussion of processing issues .", "The power series for grammar 5a is 5b .", "Each term consists of a sentence generated by the grammar and an ambiguity coefficient3 which counts how many ways the sentence can be generated .", "For example , the sentence quot ; John quot ; has one parse tree and so on .", "The reader can verify for himself that quot ; John and John and John and John and John quot ; has fourteen trees .", "Note that the power series encapsulates the ambiguity response of the system grammar to all possible input sentences .", "In this way , the power series is analogous to the impulse response in electrical engineering , which encapsulates the response of the system circuit to all possible input frequencies .", "Ambiguity coefficients bear a strong resemblance to frequency coefficients in Fourier analysis .", "All of these transformed representation systems e . g . , power series , impulse response , and Fourier series provide a complete description of the system with no loss of information4 and no heuristic approximations , for example , search strategies Kaplan 1972 .", "Transforms are often very useful because they provide a different point of view .", "Certain observations are more easily seen in the transform space than in the original space , and vice versa .", "This paper will discuss several ways to generate the power series .", "Initially let us consider successive approximation .", "Of all the techniques to be presented here , successive approximations most closely resembles the approach taken by most current chart parsers including EQSP Martin , Church , and Patil 1981 .", "The alternative approaches take advantage of certain regularities in the power series in order to produce the same results more efficiently .", "Successive approximation works as follows .", "First we translate grammar 5a into the equation where quot ; quot ; connects two ways of generating an NP and quot ; . quot ; concatenates two parts of an NP .", "In some sense , we want to quot ; solve quot ; this equation for NP .", "This can be accomplished by refining successive approximations .", "An initial approximation NP0 is formed by taking NP to be the empty language , Then we form the next approximation by substituting the previous approximation into equation 7 , and simplifying according to the usual rules of algebra e . g . , assuming distributivity , associativity , 5 identity element , and zero element .", "4 This needs a qualification .", "It is true that the power series provides a complete description of the ambiguity response to any input sentence .", "However , the power series representation may be losing some information that would be useful for parsing .", "In particular , there might be some cases where it is impossible to recover the parse trees exactly , as we will see , though this may not be too serious a problem for many practical applications .", "That is , it is often possible to recover most if not all of the structure , which may be adequate for many applications .", "5 The careful reader may correctly object to this assumption .", "We include it here for expository convenience , as it greatly simplifies the derivations though it should be noted that many of the results could be derived without the assumption .", "Furthermore , this assumption is valid for counting ambiguity .", "That is , IA BI ICI IAI I8 CI , where A , B , and C are sets of trees and Eventually , we have NP expressed as an infinitely long polynominal 5b above .", "This expression can be simplified by introducing a notation for exponentiation .", "Let x' be an abbreviation for multiplying x x . . . x , i times .", "Note that parentheses are interpreted differently in algebraic equations than in context free rules .", "In context free rules , parentheses denote optionality , where in equations they denote precedence relations among algebraic operations .", "Ambiguity coefficients take on an important practical significance when we can model them directly without resorting to successive approximation as above .", "This can result in substantial time and space savings in certain special cases where there are much more efficient ways to compute the coefficients than successive approximation chart parsing .", "Equation 9 is such a special case ; the coefficients follow a well known combinatoric series called the Catalan Numbers Knuth 1975 , pp .", "388 389 , 531 533 . 6 This section will describe Catalan numbers and their relation to parsing .", "The first few Catalan numbers are 1 , 1 , 2 , 5 , 14 , 42 , 132 , 469 , 1430 , 4862 .", "They are generated by the closed form expression 7 This formula can be explained in terms of parenthesized expressions , which are equivalent to trees .", "Cat , is the number of ways to parenthesize a formula of length n . There are two conditions on parenthesization a there must be the same number of open and close parentheses , and b they must be properly nested so that an open parenthesis precedes its matching close parenthesis .", "The first term counts the number of 6 This fact was first pointed out to us by V . Pratt .", "We suspect that it is a generally well known result in the formal language community , though its origin is unclear . where a ! is equal to the product of all integers between 1 and a . Binomial coefficients are very common in combinatorics where they are interpreted as the number of ways to pick b objects out of a set of a objects .", "American Journal of Computational Linguistics , Volume 8 , Number 3 4 , July December 1982 141 Kenneth Church and Ramesh Path Coping with Syntactic Ambiguity sequences of 2n parentheses , such that there are the same number of opens and closes .", "The second term subtracts cases violating condition b .", "This explanation is elaborated in Knuth 1975 , p . 531 .", "It is very useful to know that the ambiguity coefficients are Catalan numbers because this observation enables us to replace equation 9 with 11 , where Cat i denotes the ith Catalan number .", "All summations range from 0 to 00 unless noted otherwise .", "The ith Catalan number is the number of binary trees that can be constructed over i phrases .", "This theoretical model correctly predicts our practical experience with EQSP .", "EQSP found exactly the Catalan number of parse trees for each sentence in the following sequence .", "14 It was the number of products of products of products of products .", "These predictions continue to hold with as many as nine prepositional phrases 4862 parse trees .", "We could improve EQSP's performance on PPs if we could find a more efficient way to compute Catalan numbers than chart parsing , the method currently employed by EQSP .", "Let us propose two alternatives table lookup and evaluating expression 10 directly .", "Both are very efficient over practical ranges of n , say no more than 20 phrases or so . 8 In both cases , the ambiguity of a sentence in grammar 5a can be determined by counting the number of occurrences of quot ; and John quot ; and then retrieving the Catalan of that number .", "These approaches both take linear time over practical ranges of n , 9 whereas chart parsing requires cubic time to parse sentences in these grammars , a significant improvement .", "So far we have shown how to compute in linear time the number of ambiguous interpretations of a sentence in an quot ; every way ambiguous quot ; grammar .", "However , we are really interested in finding parse trees , not just the number of ambiguous interpretations .", "We could extend the table lookup algorithm to find trees rather than ambiguity coefficients , by modifying the table to store trees instead of numbers .", "For parsing purposes , Cati can be thought of as a pointer to the ith entry of the table .", "So , for a sentence in grammar 5a , for example , the machine could count the number of occurrences of quot ; and John quot ; and then retrieve the table entry for that number . index trees John and John and John The table would be more general if it did not specify the lexical items at the leaves .", "Let us replace the table above with index trees and assume the machine can bind the x's to the appropriate lexical items .", "There is a real problem with this table lookup machine .", "The parse trees may not be exactly correct because the power series computation assumed that multiplication was associative , which is an appropriate assumption for computing ambiguity , but inappropriate for constructing trees .", "For example , we observed that prepositional phrases and conjunction are both quot ; every way ambiguous quot ; grammars because their ambiguity coefficients are Catalan numbers .", "However , it is not the case that they generate exactly the same parse trees .", "Nevertheless we present the table lookup pseudoparser here because it seems to be a speculative new approach with considerable promise .", "It is often more efficient than a real parser , and the trees that it finds may be just as useful as the correct one for many practical purposes .", "For example , many speech recognition projects employ a parser to filter out syntactically inappropriate hypotheses .", "However , a full parser is not really necessary for this task ; a recognizer such as this table lookup pseudo parser may be perfectly adequate for this task .", "Furthermore , it is often possible to recover the correct trees from the output of the pseudo parser .", "In particular , the difference between prepositional phrases and conjunction could be accounted for by modifying the interpretation of the PP category label , so that the trees would be interpreted correctly even though they are not exactly correct .", "8 The table lookup scheme ought to have a way to handle the theoretical possibility that there are an unlimited number of prepositional phrases .", "The table lookup routine will employ a more traditional parsing algorithm e . g . , Earley's algorithm when the number of phrases in the input sentence is not stored in the table .", "The table lookup approach works for primitive grammars .", "The next two sections show how to decompose composite grammars into series and parallel combinations of primitive grammars .", "Parallel decomposition can be very useful for dealing with lexical ambiguity , as in where quot ; total quot ; can be taken as a noun or as a verb , as in 14a The accountant brought the daily sales to total with products near profits organized according to the new law . noun 14b The daily sales were ready for the accountant to total with products near profits organized according to the new law . verb The analysis of these sentences makes use of the additivity property of linear systems .", "That is , each case , 14a and 14b , is treated separately , and then the results are added together .", "Assuming quot ; total quot ; is a noun , there are three prepositional phrases contributing Cat3 bracketings , and assuming it is a verb , there are two prepositional phrases for Cat2 ambiguities .", "Combining the two cases produces Cat3 Cat2 5 2 7 parses .", "Adding another prepositional phrase yields Cat4 Cat3 14 5 19 parses .", "EOSP behaved as predicted in both cases .", "This behavior is generalized by the following power series This observation can be incorporated into the table lookup pseudo parser outlined above .", "Recall that Cat , is interpreted as the ith index in a table containing all binary trees dominating i leaves .", "Similarly , Cati Cati I will be interpreted as an instruction to quot ; append quot ; the ith entry and i 1 th entry of the table . 10 Let us consider a system where syntactic processing strictly precedes semantic and pragmatic processing .", "In such a system , how could we incorporate semantic and pragmatic heuristics once we have already parsed the input sentence and found that it was the sum of two Catalans ?", "The parser can simply subtract the inappropriate interpretations .", "If the oracle says that quot ; total quot ; is a verb , then 16a would be subtracted from the combined sum , and if the oracle says that quot ; total quot ; is a noun , then 16b would be subtracted .", "On the other hand , our analysis is also useful in a system that interleaves syntactic processing with semantic and pragmatic processing .", "Suppose that we had a semantic routine that could disambiguate quot ; total , quot ; but only at a very high cost in execution time .", "We need a way to estimate the usefulness of executing the semantic routine so that we don't spend the time if it is not likely to pay off .", "The analysis above provides a very simple way to estimate the benefit of disambiguating quot ; total . quot ; If it turns out to be a verb , then 16a trees have been ruled out , and if it turns out to be a noun , then 16b trees have been ruled out .", "We prefer our declarative algebraic approach over procedural heuristic search strategies e . g . , Kaplan 1972 because we do not have to specify the order of evaluation .", "We can delay the binding of decisions until the most opportune moment .", "Suppose we have a non terminal S that is a series combination of two other non terminals , NP and VP .", "By inspection , the power series of S is This result is easily verified when there is an unmistakable dividing point between the subject and the predicate .", "For example , the verb quot ; is quot ; separates the PPs in the subject from those in the predicate in 19a , but not in 19b .", "In 19a , the total number of parse trees is the product of the number of ways of parsing the subject times the number of ways of parsing the predicate .", "Both the subject and the predicate produce a Catalan number of parses , and hence the result is the product of two Catalan numbers , which was verified by EQSP Martin , Church , and Patil 1981 , p . 53 .", "This result can be formalized in terms of the power series 10 This can be implemented efficiently , given an appropriate representation of sets of trees .", "Kenneth Church and Ramesh Patil Coping with Syntactic Ambiguity The power series says that the ambiguity of a particular sentence is the product of Cati and Cat , where i is the number of PPs before quot ; is quot ; and j is the number after quot ; is . quot ; This could be incorporated in the table lookup parser as an instruction to quot ; multiply quot ; the ith entry in the table by the jth entry .", "Multiplication is a cross product operation ; L x R generates the set of binary trees whose left sub tree l is from L and whose right sub tree r is from R . 22 L x R 1 , r 11 cL r R This is a formal definition .", "For practical purposes , it may be more useful for the parser to output the list in the factored form which is much more concise than a list of trees .", "It is possible , for example , that semantic processing can take advantage of factoring , capturing a semantic generalization that holds across all subjects or all predicates .", "Imagine , for example , that there is a semantic agreement constraint between predicates and arguments .", "For example , subjects and predicates might have to agree on the feature human .", "Suppose that we were given sentences where this constraint was violated by all ambiguous interpretations of the sentence .", "In this case , it would be more efficient to employ a feature vector scheme Dostert and Thompson 1971 which propagates the features in factored form .", "That is , it computes a feature vector for the union of all possible subjects , and a vector for the union of all possible VPs , and then compares intersects these vectors to check if there are any interpretations that meet the constraint .", "A system such as this , which keeps the parses in factored form , is much more efficient than one that multiplies them out .", "Even if semantics cannot take advantage of the factoring , there is no harm in keeping the representation in factored form , because it is straightforward to expand 23 into a list of trees though it may be somewhat slow .", "This example is relatively simple because quot ; is quot ; helps the parser determine the value of i and j .", "Now let us return to example 19b where quot ; is quot ; does not separate the two strings of PPs .", "Again , we determine the power series by multiplying the two subcases However , this form is not so useful for parsing because the parser cannot easily determine i and j , the number of prepositional phrases in the subject and the number in the predicate .", "It appears the parser will have to compute the product of two Catalans for each way of picking i and j , which is somewhat expensive . 11 Fortunately , the Catalan function has some special properties so that it is possible algebraically to remove the references to i and j .", "In the next section we show how this expression can be reformulated in terms of n , the total number of PPs .", "Some readers may have noticed that expression 24 is in convolution form .", "We will make use of this in the reformulation .", "Notice that the Catalan series is a fixed point under auto convolution except for a shift ; that is , multiplying a Catalan power series i . e . , 1 x 2x2 5x3 14x4 Catix1 . . . with itself produces another polynomial with Catalan coefficients . 12 The multiplication is worked out for the first few terms .", "This property can be summarized as 25 E Cat , xi E Cat x'1i xi E Catn X where n equals i j .", "Intuitively , this equation says that if we have two quot ; every way ambiguous quot ; Catalan constructions , and we combine them in every possible way convolution , the result is an quot ; every way ambiguous quot ; Catalan construction .", "With this observation , equation 24 reduces to Hence the number of parses in the auxiliary inverted case is the Catalan of one more than in the noninverted cases .", "As predicted , EQSP found the following inverted sentences to be more ambiguous than their non inverted counterparts previously discussed on page 142 by one Catalan number .", "11 Earley's algorithm and most other context free parsing algorithms actually work this way .", "12 The proof immediately follows from the z transform of the Catalan series Knuth 1975 , p . 388 zB z B z 1 . of products .", "14 It was the number of products of products of products of products .", "How could this result be incorporated into the table lookup pseudo parser ?", "Recall that the pseudo parser implements Catalan grammars by returning an index into the Catalan table .", "For example , if there were i PPs , the parser would return CAT TABLE i .", "We now extend the indexing scheme so that the parser implements a series connection of two Catalan grammars by returning one higher index than it would for a simple Catalan grammar .", "That is , if there were n PPs , the parser would return CAT TABLE 4 n 1 .", "Series connections of Catalan grammars are very common in every day natural language , as illustrated by the following two sentences , which have received considerable attention in the literature because the parser cannot separate the direct object from the prepositional complement .", "Both examples have a Catalan number of ambiguities because the auto convolution of a Catalan series yields another Catalan series . 13 This result can improve parsing performance because it suggests ways to reorganize compile the grammar so that there will be fewer references to quantities that are not readily available .", "This re organization will reap benefits that chart parsers e . g . , Earley's algorithm do not currently achieve because the re organization is taking advantage of a number of combinatoric regularities , especially convolution , that are not easily encoded into a chart .", "Section 9 presents an example of the reorganization .", "13 There is a difference between these two sentences because quot ; put quot ; subcategorizes for two objects unlike quot ; see . quot ; Suppose we analyze quot ; see quot ; as lexically ambiguous between two senses , one that selects for exactly two objects like quot ; put quot ; and one that selects for exactly one object as in quot ; I saw it . quot ; The first sense contributes the same number of parses as quot ; put quot ; and the second sense contributes an additional Catalan factor .", "Perhaps it is worthwhile to reformulate chart parsing in our terms in order to show which of the above results can be captured by such an approach and which cannot .", "Traditionally , chart parsers maintain a chart or matrix M , whose entries M1 contain the set of category labels that span from position i to position j in the input sentence .", "This is accomplished by finding a position k between i and j such that there is a phrase from i to k that can combine with another phrase from k to j .", "An implementation of the inner loop looks something like Essentially , then , a chart parser is maintaining the invariant where addition and multiplication of matrix elements is related to parallel and series combination .", "Thus chart parsers are able to process very ambiguous sentences in polynomial time , as opposed to exponential or Catalan time .", "However , the examples above illustrate cases where chart parsers are not as efficient as they might be .", "In particular , chart parsers implement convolution the quot ; long way , quot ; by picking each possible dividing point k , and parsing from i to k and from k to j ; they do not reduce the convolution of two Catalans as we did above .", "Similarly , chart parsers do not make use of the quot ; every way ambiguous quot ; generalization ; given a Catalan grammar , chart parsers will eventually enumerate all possible values of i , j , and k . Thus far , most of our derivations have been justified in terms of successive approximation .", "It is also possible to derive some interesting and well known results directly from the grammar itself .", "Suppose , for the sake of discussion , that we choose to analyze adjuncts with a right branching grammar . 14 By convention , terminal symbols appear in lower case .", "First we translate the grammar into an equation in the usual way .", "That is , ADJS is modeled as a parallel combination of two subgrammars , adj ADJS and A .", "A , the empty string , is modeled as 1 because it is the 14 A similar analysis of adjuncts is adopted in Kaplan and Bresnan 1981 .", "This analysis can also be defended on performance grounds as an efficiency approximation .", "This approximation is in the spirit of pseudo attachment Church 1980 .", "We can simplify 31b so the right hand side is expressed in terminal symbols alone , with no references to non terminals .", "This is very useful for processing because it is much easier for the parser to determine the presence or absence of terminals than of nonterminals .", "That is , it is easier for the parser to determine , for example , whether a word is an adj , than it is to decide whether a substring is an ADJS phrase .", "The simplification moves all references to ADJS to the left hand side , by subtracting from both sides , Grammars like ADJS will sometimes be referred to as a step , by analogy to a unit step function in electrical engineering .", "Computing the Power Series from the ATN This section will re derive the power series for the unit step grammar directly from the ATN representation by treating the networks as flow graphs Oppenheim 1975 .", "The graph transformations presented here are directly analogous to the algebraic simplifications employed in the previous section .", "First we translate the grammar into an ATN in the usual way Woods 1970 .", "This graph can be simplified by performing a compiler optimization call tail recursion Church and Kaplan 1981 and references therein .", "This transformation replaces the final push arc with a jump Jump Tail recursion corresponds directly to the algebraic operations of moving the ADJS term to the left hand side , factoring out the ADJS , and dividing from both sides .", "Then we remove the top jump arc by series reduction .", "This step corresponds to multiplying by 1 since a jump arc is the ATN representation for the identity element under series combination . where the zero th term corresponds to zero iterations around the loop , the first term corresponds to a single iteration , the second term to two iterations , and so on .", "Recall that 36 is equivalent to 37 1 1 adj With this observation , it is possible to open the loop 38 ADJS 01 1 adj Jump Pop After one final series reduction , the ATN is equivalent to expression 31e above .", "Intuitively , an ATN loop or step grammar is a division operator .", "We now have composition operators for parallel composition addition , series composition multiplication , and loops division .", "An ATN loop can be implemented in terms of the table lookup scheme discussed above .", "First we reformulate the loop as an infinite sum Then we construct a table so that the ith entry in the table tells the parser how to parse i occurrences of adj .", "Suppose for example that we were given the following grammar In this example we will assume no lexical ambiguity among N , V , P , and adj .", "By inspection , we notice that NP and PP are Catalan grammars and that ADJS is a Step grammar .", "With these observations , the parser can process PPs , NPs , and ADJSs by counting the number of occurrences of terminal symbols and looking up those numbers in the appropriate tables .", "We now substitute 41a c into 40c .", "42 VP V NP 1 PP ADJS V N E Cati P N ' E Cati P E adji and simplify the convolution of the two Catalan functions 43 VP V N E Cati i P N i E adj' so that the parser can also find VPs by just counting coccurrences of terminal symbols .", "Now we simplify 40a b so that S phrases can also be parsed by just counting occurrences of terminal symbols .", "First , translate 40a b into the equation Furthermore , the number of parse trees for a given input sentence can be found by multiplying three numbers a the Catalan of the number of P N's before the verb , b the Catalan of one more than the number of P N's after the verb , and c the ramp of the number of adj's .", "For example , the sentence 53 The man on the hill saw the boy with a telescope yesterday in the morning . has Cat Cat2 3 6 parses .", "That is , there is one way to parse quot ; the man on the hill , quot ; two ways to parse quot ; saw the boy with a telescope quot ; quot ; telescope quot ; is either a complement of quot ; see quot ; as in 54a c or is attached to quot ; boy quot ; as in 54d f , and three ways to parse the adjuncts they could both attach to the S 54a , d , or they could both attach to the VP 54b , e , or they could split 54c , f .", "All and only these possibilities are permitted by the grammar .", "We began our discussion with the observation that certain grammars are quot ; every way ambiguous quot ; and suggested that this observation could lead to improved parsing performance .", "Catalan grammars were then introduced to remedy the situation so that the processor can delay attachment decisions until it discovers some more useful constraints .", "Until such time , the processor can do little more than note that the input sentence is quot ; every way ambiguous . quot ; We suggested that a table lookup scheme might be an effective method to implement such a processor .", "We then introduced rules for combining primitive grammars , such as Catalan grammars , into composite grammars .", "This linear systems view quot ; bundles up quot ; all the parse trees into a single concise description capable of telling us everything we might want to know about the parses including how much it might cost to ask a particular question .", "This abstract view of ambiguity enables us to ask questions in the most convenient order , and to delay asking until it is clear that the pay off will exceed the cost .", "This abstraction was very strongly influenced by the notion of delayed binding .", "We have presented combination rules in three different representation systems power series , ATNs , and context free grammars , each of which contributed its own insights .", "Power series are convenient for defining the algebraic operations , ATNs are most suited for discussing implementation issues , and context free grammars enable the shortest derivations .", "Perhaps the following quotation best summarizes our motivation for alternating among these three representation systems A thing or idea seems meaningful only when we have several different ways to represent it different perspectives and different associations .", "Then you can turn it around in your mind , so to speak ; however , it seems at the moment you can see it another way ; you never come to a full stop .", "Minsky 1981 , p . 19 In each of these representation schemes , we have introduced five primitive grammars Catalan , Unit Step , 1 , and 0 , and terminals ; and four composition rules addition , subtraction , multiplication , and division .", "We have seen that it is often possible to employ these analytic tools in order to re organize compile the grammar into a form more suitable for processing efficiently .", "We have identified certain situations where the ambiguity is combinatoric , and have sketched a few modifications to the grammar that enable processing to proceed in a more efficient manner .", "In particular , we have observed it to be important for the grammar to avoid referencing quantities that are not easily determined , such as the dividing point between a noun phrase and a prepositional phrase as in 55 Put the block in the box on the table in the kitchen . . . We have seen that the desired re organization can be achieved by taking advantage of the fact that the autoconvolution of a Catalan series produces another Catalan series .", "This reduced processing time from 0 n3 to almost linear time .", "Similar analyses have been discussed for a number of lexically and structurally ambiguous constructions , culminating with the example in section 9 , where we transformed a grammar into a form that could be parsed by a single left to right pass over the terminal elements .", "Currently , these grammar reformulations have to be performed by hand .", "It ought to be possible to automate this process so that the reformulations could be performed by a grammar compiler .", "We leave this project open for future research .", "We would like to thank Jon Allen , Sarah Ferguson , Lowell Hawkinson , Kris Halvorsen , Bill Long , Mitch Marcus , Rohit Parikh , and Peter Szolovits for their very useful comments on earlier drafts .", "We would especially like to thank Bill Martin for initiating the project ."], "summary_lines": ["Coping With Syntactic Ambiguity Or How To Put The Block In The Box On The Table\n", "Sentences are far more ambiguous than one might have thought.\n", "There may be hundreds, perhaps thousands, of syntactic parse trees for certain very natural sentences of English.\n", "This fact has been a major problem confronting natural language processing, especially when a large percentage of the syntactic parse trees are enumerated during semantic/pragmatic processing.\n", "In this paper we propose some methods for dealing with syntactic ambiguity in ways that exploit certain regularities among alternative parse trees.\n", "These regularities will be expressed as linear combinations of ATN networks, and also as sums and products of formal power series.\n", "We believe that such encoding of ambiguity will enhance processing, whether syntactic and semantic constraints are processed separately in sequence or interleaved together.\n", "The number of possible binary-branching parses of a sentence is defined by the Catalan number, an exponential combinatoric function.\n"]}
{"article_lines": ["ONE SENSE PER COLLOCATION David Yarowsky Department of Computer and In format ion Science Univers i ty of Pennsy lvania Philadelphia , PA 19104 yarowsky unagi . c is . upenn . edu ABSTRACT Previous work Gale , Church and Yarowsky , 1992 showed that with high probability a polysemous word has one sense per discourse .", "In this paper we show that for certain definitions of collocation , a polysemous word exhibits essentially only one sense per collocation .", "We test his empirical hypothesis for several definitions of sense and collocation , and discover that it holds with 90 99 accuracy for binary ambiguities .", "We utilize this property in a disambiguation algorithm that achieves precision of 92 using combined models of very local context .", "INTRODUCTION The use of collocations to resolve lexical ambiguities i cer tainly not a new idea .", "The first approaches to sense dis ambiguation , such as Kelly and Stone 1975 , were based on simple hand built decision tables consisting almost ex clusively of questions about observed word associations in specific positions .", "Later work from the AI community relied heavily upon selectional restrictions for verbs , although pri marily in terms of features exhibited by their arguments uch as DRINKABLE rather than in terms of individual words or word classes .", "More recent work Brown et al .", "1991 Hearst 1991 has utilized a set of discrete local questions such as word to the right in the development of statistical decision procedures .", "However , astrong trend in recent years is to treat a reasonably wide context window as an unordered bag of in dependent evidence points .", "This technique from information retrieval has been used in neural networks , Bayesian discrim inators , and dictionary definition matching .", "In a comparative paper in this volume Leacock et al .", "1993 , all three methods under investigation used words in wide context as a pool of evidence independent of relative position .", "It is perhaps not a coincidence that this work has focused almost exclusively on nouns , as will be shown in Section 6 . 2 .", "In this study we will return again to extremely local sources of evidence , and show that models of discrete syntactic relationships have considerable advantages .", "This research was supported by an NDSEG Fellowship and by DARPA grant N00014 90 J 1863 .", "The author is also affiliated with the Linguistics Research Department ofAT T Bell Laboratories , and greatly appreciates the use of its resources in support of this work .", "He would also like to thank Eric Bfill , Bill Gale , Libby Levison , Mitch Marcus and Philip Resnik for their valuable feedback .", "DEF IN IT IONS OF SENSE The traditional definition of word sense is One of several meanings assigned to the same orthographic string .", "As meanings can always be partitioned into multiple refinements , senses are typically organized in a tree such as one finds in a dictionary .", "In the extreme case , one could continue making refinements until a word has a slightly different sense every time it is used .", "If so , the title of this paper is a tautology .", "However , the studies in this paper are focused on the sense distinctions atthe top of the tree .", "A good working definition of the distinctions considered are those meanings which are not typically translated to the same word in a foreign language .", "Therefore , one natural type of sense distinction to consider are those words in English which indeed have multiple trans lations in a language such as French .", "As is now standard in the field , we use the Canadian Hansards , a parallel bilingual corpus , to provide sense tags in the form of French transla tions .", "Unfortunately , the Hansards are highly skewed in their sense distributions , and it is difficult to find words for which there are adequate numbers of a second sense .", "More diverse large bilingual corpora re not yet readily available .", "We also use data sets which have been hand tagged bynative English speakers .", "To make the selection of sense distinc tions more objective , we use words such as bass where the sense distinctions fish and musical instrument correspond to pronunciation differences b es and beIs .", "Such data is often problematic , as the tagging is potentially subjective and error filled , and sufficient quantities are difficult o obtain .", "As a solution to the data shortages for the above methods , Gale , Church and Yarowsky 1992b proposed the use of pseudo words , artificial sense ambiguities created by tak ing two English words with the same part of speech such as guerilla and reptile , and replacing each instance of both in a corpus with a new polysemous word guerrilla reptile .", "As it is entirely possible that the concepts guerrilla nd reptile are represented by the same orthographic string in some foreign language , choosing between these two meanings based on context is a problem a word sense disambiguation algorithm could easily face .", "Pseudo words are very useful for devel oping and testing disambiguation methods because of their nearly unlimited availability and the known , fully reliable 266 ground truth they provide when grading performance .", "Finally , we consider sense disambiguation for mediums other than clean English text .", "For example , we look at word pairs such as terse tense and cookie rookie which may be plausi bly confused in optical character recognition OCR .", "Homo phones , such as aid aide , and censor sensor , are ideal can didates for such a study because large data sets with known ground truth are available in written text , yet they are true ambiguities which must be resolved routinely in oral commu nication .", "We discover that the central claims of this paper hold for all of these potential definitions of sense .", "This corroborating evidence makes us much more confident in our results than if they were derived solely from a relatively small hand tagged data set .", "DEF IN IT IONS OF COLLOCATION Collocation means the co occurrence of two words in some defined relationship .", "We look at several such relationships , in cluding direct adjacency and first word to the left or right hav ing a certain part of speech .", "We also consider certain direct syntactic relationships , uch as verb object , subject verb , and adjective noun pairs .", "It appears that content words nouns , verbs , adjectives , and adverbs behave quite differently from function words other parts of speech ; we make use of this distinction in several definitions of collocation .", "We will attempt to quantify the validity of the one sense per collocation hypothesis for these different collocation types .", "EXPERIMENTS In the experiments , we ask two central , related questions For each definition of sense and collocation , ?", "What is the mean entropy of the distribution Pr Sense Collocation ?", "What is the performance of a disambiguation algorithm which uses only that collocation type as evidence ?", "We examine several permutations for each , and are interested in how the results of these questions differ when applied to polysemous nouns , verbs , and adjectives .", "To limit the already very large number of parameters consid ered , we study only binary sense distinctions .", "In all cases the senses being compared have the same part of speech .", "The selection between different possible parts of speech as been heavily studied and is not replicated here .", "Sample Collection All samples were extracted from a 380 million word cor pus collection consisting of newswire text AP Newswire and ?", "Hand Tagged homographs bass , axes , chi , bow , colon , lead , IV , sake , tear , . . . ?", "French Translation Distinctions sentence , duty , drug , language , position , paper , single . . . . ?", "Homophones aid aide , cellar seller , censor sensor , cue queue , pedal petal . . . . ?", "OCR Ambiguities terse tense , gum gym , deaf dear , cookie rookie , beverage leverage . . . . ?", "Pseudo Words covered waved , kissed slapped , abused escorted , cute compatible . . . . Table 1 A sample of the words used in the experiments Wall Street Journal , scientific abstracts from NSF and the Department ofEnergy , the Canadian Hansards parliamentary debate records , Groliers Encyclopedia , a medical encyclo pedia , over 100 Harper Row books , and several smaller corpora including the Brown Corpus , and ATIS and TIMIT sentences . 1 The homophone pairs used were randomly selected from a list of words having the same pronunciation orwhich differed in only one phoneme .", "The OCR and pseudo word pairs were randomly selected from corpus wordlists , with the former restricted to pairs which could plausibly be confused in a noisy FAX , typically words differing in only one character .", "Due to the difficulty of obtaining new data , the hand tagged and French translation examples were borrowed from those used in our previous tudies in sense disambiguation .", "Measuring Entropies When computing the entropy of Pr Sense Collocation , we enumerate all collocations of a given type observed for the word or word pair being disambiguated .", "Table 2 shows the example of the homophone ambiguity aid aide for the collo cation type content word to the left .", "We list all words 2 ap pearing in such a collocation with either of these two senses of the homograph , and calculate the raw distributional count for each .", "Note that the vast majority of the entries in Table 2 have zero as one of the frequency counts .", "It is not acceptable , however , t Training and test samples were not only extracted from different articles or discourses but also from entirely different blocks of the corpus .", "This was done to minimize long range discourse ffects such as one finds in the AP or Hansards .", "2Note the entries in this table are lemmas uninflected root forms , rather than raw words .", "By treating the verbal inflections squander , squanders , squandering , and squandered asthe same word , one can improve statistics and coverage at a slight cost of lost subtlety .", "Although we will refer to words in collocation throughout this paper for simplicity , this should always be interpreted as lemmas in collocation .", "267 Frequency as Frequency as Collocation Aid Aide foreign federal western provide zovert appose future imilar presidential hief longtime aids infected deepy disaffected Lndispensable ractical ; quander 718 297 146 88 26 13 9 6 0 0 0 0 0 0 2 2 1 1 0 0 0 0 0 0 0 63 40 26 2 1 1 1 0 0 Table 2 A typical collocational distribution for the homo phone ambiguity aid aide .", "to treat these as having zero probability and hence a zero entropy for the distribution .", "It is quite possible , especially for the lower frequency distributions , that we would see a contrary example in a larger sample .", "By cross validation , we discover for the aid aide xample that for collocations with an observed 1 0 distribution , we would actually expect he minor sense to occur 6 of the time in an independent sample , on average .", "Thus a fairer distribution would be . 94 . 06 , giving a cross validated ntropy of . 33 bits rather than 0 bits .", "For a more unbalanced observed istribution , such as 10 0 , the probability of seeing the minor sense decreases to 2 , giving a cross validated ntropy of H . 98 , . 02 . 14 bits .", "Repeating this process and taking the weighted mean yields the entropy of the full distribution , in this case . 09 bits for the aid aide ambiguity .", "For each type of collocation , we also compute how well an observed probability distribution predicts the correct classifi cation for novel examples .", "In general , this is a more useful measure for most of the comparison purposes we will address .", "Not only does it reflect he underlying entropy of the distribu tion , but it also has the practical advantage of showing how a working system would perform given this data .", "ALGORITHM The sense disambiguation algorithm used is quite straightfor ward .", "When based on a single collocation type , such as the object of the verb or word immediately to the left , the pro cedure is very simple .", "One identifies if this collocation type exists for the novel context and if the specific words found are listed in the table of probability distributions as computed above .", "If so , we return the sense which was most frequent for that collocation in the training data .", "If not , we return the sense which is most frequent overall .", "When we consider more than one collocation type and com bine evidence , the process is more complicated .", "The algo rithm used is based on decision lists Rivest , 1987 , and was discussed in Sproat , Hirschberg , and Yarowsky 1992 .", "The goal is to base the decision on the single best piece of evi dence available .", "Cross validated probabilities are computed as in Section 4 . 2 , and the different ypes of evidence are sorted by the absolute value of the log of these probabil ?", "P r Sense l Co l loca ion i ratios .", "Abs Log prls , n , Conocauo , , When a novel lty context is encountered , one steps through the decision list until the evidence at that point in the list such as word to eft presidential matches the current context under con sideration .", "The sense with the greatest listed probability is returned , and this cross validated probability represents he confidence in the answer .", "This approach is well suited for the combination of multi ple evidence types which are clearly not independent such as those found in this study as probabilities are never com bined .", "Therefore this method offers advantages over Bayesian classifier techniques which assume independence of the fea tures used .", "It also offers advantages over decision tree based techniques because the training pools are not split at each question .", "The interesting problems are how one should re estimate probabilities conditional on questions asked earlier in the list , or how one should prune lower evidence which is categorically subsumed by higher evidence or is entirely conditional on higher evidence .", "1989 have dis cussed some of these issues at length , and there is not space to consider them here .", "For simplicity , in this experiment no secondary smoothing or pruning is done .", "This does not ap pear to be problematic when small numbers of independent evidence types are used , but performance should increase if this extra step is taken .", "RESULTS AND DISCUSSION 6 . 1 .", "One Sense Per Co l locat ion For the collocations tudied , it appears that the hypothesis of one sense per collocation holds with high probability for binary ambiguities .", "The experimental results in the precision column of Table 3 quantify the validity of this claim .", "Accu racy varies from 90 to 99 for different types of collocation and part of speech , with a mean of 95 .", "The significance of these differences will be discussed in Section 6 . 2 .", "These precision values have several interpretations .", "First , they reflect the underlying probability distributions of sense 268 Collocation Part Ent Prec Rec No No Type of Sp .", "Coil Data Content ALL . 18 . 97 . 29 . 57 . 14 word to Noun . 98 . 25 . 66 . 09 immediate Verb . 95 . 14 . 71 . 15 right A Adj . 97 . 51 . 27 . 22 Content ALL . 24 . 96 . 26 . 58 . 16 word to Noun . 99 . 33 . 56 . 11 immediate Verb . 91 . 23 . 47 . 30 left B Adj . 96 . 15 . 75 . 10 First ALL . 33 . 94 . 51 . 09 . 40 Content Noun . 94 . 49 . 13 . 38 Word to Verb . 91 . 44 . 05 . 51 Right Adj . 96 . 58 . 04 . 38 First ALL . 40 . 92 . 50 . 06 . 44 Content Noun . 96 . 58 . 06 . 36 Word to Verb . 87 . 37 . 05 . 58 Left Adj . 90 . 45 . 06 . 49 Subject Noun . 33 . 94 . 13 . 87 . 06 Verb Pairs Verb . 43 . 91 . 28 . 33 . 38 Verb Noun . 46 . 90 . 07 . 81 . 07 Object Pairs Verb . 29 . 95 . 36 . 32 . 32 Adj Noun Adj . 14 . 98 . 54 . 20 . 26 A BAbove ALL . 97 . 47 I . 31 I . 21 I All Above ALL . 92 . 98 . 00 . 02 Table 3 IncludestheentropyofthePr SennelGollocation distribution for several types of collocation , and the performance achieved when basing sense disambiguation solely on that evidence .", "Results are itemized by the part of speech of the ambiguous word not of the collocate .", "Precision Prec .", "indicates percent correct and Recall Rec .", "refers to the percentage of samples for which an answer is returned .", "Precision is measured on this subset .", "No collocation No Coil indicates the failure to provide an answer because no collocation of that type was present in the test context , and No Data indicates the failure to return an answer because no data for the observed collocation was present in the model .", "See Section 7 . 3 for a discussion of the All Above result .", "The results tated above are based on the average of the different types of sense considered , and have a mean prior probability of . 69 and a mean sample size of 3944 . conditional on collocation .", "For example , for the collocation type content word to the right , t evalue of . 97 indicates that on average , given a Specific collocation we will expect o see the same sense 97 of the time .", "This mean distribution is also reflected in the entropy column .", "However , these numbers have much more practical interpre tations .", "If we actually build a disambiguation procedure using exclusively the content word to the right as information , such a system performs with 97 precision on new data where a content word appears to the right and for which there is in formation in the model . 3 This is considerably higher than the 3The correlation between these numbers is not a coincidence .", "Because the probability distributions are based oncross validated t sts on indepen dent data and weighted by collocation frequency , if on average we find that Per formance Us ing Ev idence a t D i f fe rent D is tances 8 t Verbs Adjectives 2o , o 8o go , no Distance Figure 1 Comparison of the performance ofnouns , verbs and adjectives based strictly on a 5 word window centered at the distance shown on the horizontal axis .", "performance of 69 one would expect simply by chance due to the unbalanced prior probability of the two senses .", "It should be noted that such precision is achieved at only partial recall .", "The three rightmost columns of Table 3 give the breakdown of the recall .", "On average , the model content word to right could only be applied in 29 of the test samples .", "In 57 of the cases , no content word appeared to the right , so this collocational model did not hold .", "In 14 of the cases , a content word did appear to the right , but no instances of that word appeared in the training data , so the model had no information on which to base a decision .", "There are several solutions to both these deficiencies , and they are discussed in Section 7 .", "Part of Speech Differences It is interesting to note the difference in behavior between different parts of speech .", "Verbs , for example , derive more disambiguating information from their objects . 95 than from their subjects . 90 .", "Adjectives derive almost all of their disambiguatinginformation fr m the nouns they modify . 98 .", "Nouns are best disambiguated bydirectly adjacent adjectives or nouns , with the content word to the left indicating a single sense with 99 precision .", "Verbs appear to be less useful for noun sense disambiguation , although they are relatively better indicators when the noun is their object rather than their subject .", "97 of samples of a given collocation exhibit he same sense , this is the expected precision of a disambiguafion algorithm which assumes one sense per collocation , when applied to new samples of these collocations .", "269 Figure shows that nouns , verbs and adjectives also differ in their ability to be disambiguated by wider context .", "1993 previously showed that nouns can be disambiguated based strictly on distant context , and that useful information was present up to 10 , 000 words away .", "We replicated an exper iment in which performance was calculated for disambigua tions based strictly on 5 word windows centered at various distances shown on the horizontal axis .", "Gales observation was tested only on nouns ; our experiment also shows that reasonably accurate decisions may be made for nouns using exclusively remote context .", "Our results in this case are based on test sets with equal numbers of the two senses .", "Hence chance performance is at 50 .", "However , when tested on verbs and adjectives , precision drops off with a much steeper slope as the distance from the ambiguous word increases .", "This would indicate that approaches giving equal weight o all po sitions in a broad window of context may be less well suited for handling verbs and adjectives .", "Models which give greater weight o immediate context would seem more appropriate in these circumstances .", "A similar experiment was applied to function words , and the dropoff beyond strictly immediate context was precipitous , converging at near chance performance for distances greater than 5 .", "However , function words did appear to have pre dictive power of roughly 5 greater than chance in directly adjacent positions .", "The effect was greatest for verbs , where the function word to the right typically a preposition or par ticle served to disambiguate at a precision of 13 above chance .", "This would indicate that methods which exclude function words from models to minimize noise should con sider their inclusion , but only for restricted local positions .", "Comparison of Sense Definitions Results for the 5 different definitions of sense ambiguity stud ied here are similar .", "However they tend to fluctuate relative to each other across experiments , and there appears to be no consistent ordering of the mean entropy of the different types of sense distributions .", "Because of the very large num ber of permutations considered , it is not possible to give a full breakdown of the differences , and such a breakdown does not appear to be terribly informative .", "The important observa tion , however , is that the basic conclusions drawn from this paper hold for each of the sense definitions considered , and hence corroborate and strengthen the conclusions which can be drawn from any one .", "Performance Given Little Evidence One of the most striking conclusions to emerge from this study is that for the local collocations considered , decisions based on a single data point are highly reliable .", "Normally one would consider a 1 0 sense distribution i a 3944 sample training set to be noise , with performance based on this information ot Low Counts a re Re l iab le ; , o o , ; o o ; o , go . , Training Frequency f Figure 2 Percentage correct for disambiguations based solely on a single content word to the rightcollocation seen ft imes in the training data without counter examples .", "likely to much exceed the 69 prior probability expected by chance .", "But this is not what we observe .", "For example , when tested on the word to the right collocation , disambiguations based solely on a single data point exceed 92 accuracy , and performance on 2 0 and 3 0 distributions climb rapidly from there , and reach nearly perfect accuracy for training samples as small as 15 0 , as shown in Figure 2 .", "In contrast , acollocation 30 words away which also exhibits a 1 0 sense distribution has a predictive value of only 3 greater than chance .", "This difference in the reliability of low frequency data from local and wide context will have implications for algorithm design .", "APPL ICAT IONS 7 . 1 .", "Training Set Creation and Verification This last observation has relevance for new data set creation and correction .", "Collocations with an ambiguous content word which have frequency greater than 10 15 and which do not belong exclusively to one sense should be flagged for human reinspection , as they are most likely in error .", "One can speed the sense tagging process by computing the most frequent col locates , and for each one assigning all examples to the same sense .", "For the data in Table 2 , this will apparently fail for the foreignAid Aide example in 1 out of 719 instances till 99 . 9 correct .", "However , in this example the models classification was actually correct ; the given usage was a misspelling in the 1992 AP Newswire Bush accelerated foreign aide and weapons ales to Iraq . .", "It is quite likely that if were in deed a foreign assistant being discussed , this example would also have another collocation with the verb , for example , 270 which would indicate the correct sense .", "Such inconsisten cies should also be flagged for human supervision .", "Working from the most to least frequent collocates in this manner , one can use previously tagged collocates to automatically suggest the classification of other words appearing in different collo cation types for those tagged examples .", "The one sense per discourse constraint can be used to refine this process further .", "We are working on a similar use of these two constraints for unsupervised sense clustering .", "Algor i thm Design Our results also have implications for algorithm design .", "For the large number of current approaches which treat wide con text as an unordered bag of words , it may be beneficial to model certain local collocations separately .", "We have shown that reliability of collocational evidence differs considerably between local and distant context , especially for verbs and adjectives .", "If one one is interested inproviding aprobability with an answer , modeling local collocations separately will improve the probability estimates and reduce cross entropy .", "Another eason for modeling local collocations separately is that his will allow the reliable inclusion of evidence with very low frequency counts .", "Evidence with observed frequency dis tributions of 1 0 typically constitute on the order of 50 of all available vidence types , yet in a wide context window this low frequency evidence is effectively noise , with predic tive power little better than chance .", "However , in very local collocations , ingle data points carry considerable informa tion , and when used alone can achieve precision in excess of 92 .", "Their inclusion should improve system recall , with a much reduced danger of overmodeling the data .", "Bui lding a Full Disambiguation System Finally , one may ask to what extent can local collocational evidence alone support apractical sense disambiguation algo rithm .", "As shown in Table 3 , our models of single collocation types achieve high precision , but individually their applica bility is limited .", "However , if we combine these models as described in Section 5 , and use an additional function word collocation model when no other evidence is available , we achieve full coverage at a precision of 92 .", "This result is comparable to those previously reported in the literature us ing wider context of up to 50 words away 5 , 6 , 7 , 12 .", "Due to the large number of variables involved , we shall not at tempt o compare these directly .", "Our results are encouraging , however , and and we plan to conduct amore formal compari son of the bag of words approaches relative to our separate modeling of local collocation types .", "We will also consider ad ditional collocation types covering awider range of syntactic relationships .", "In addition , we hope to incorporate class based techniques , uch as the modeling of verb argument selectional preferences Resnik , 1992 , as a mechanism for achieving im proved performance on unfamiliar collocations .", "CONCLUSION This paper has examined some of the basic distributional prop erties of lexical ambiguity in the English language .", "Our ex periments have shown that for several definitions of sense and collocation , an ambiguous word has only one sense in a given collocation with a probability of 90 99 .", "We showed how this claim is influenced by part of speech , distance , and sample frequency .", "We discussed the implications of these results for data set creation and algorithm design , identifying potential weaknesses in the common bag of words approach to disambiguation .", "Finally , we showed that models of local collocation can be combined in a disambiguation algorithm that achieves overall precision of 92 .", "Bahl , L . , P . Brown , P . de Souza , R . Mercer , A Tree Based Sta tistical Language Model for Natural Language Speech Recog nition , in IEEE Transactions on Acoustics , Speech , and Signal Processing , 37 , 1989 .", "Brown , Peter , Stephen Della Pietra , Vincent Della Pietra , and Robert Mercer , Word Sense Disambiguation using Statisti cal Methods , Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics , 1991 , pp 264 270 .", "Gale , W . , K . Church , and D . Yarowsky , One Sense Per Dis course , Proceedings of the 4th DARPA Speech and Natural Language Workshop , 1992 .", "Gale , W . , K . Church , and D . Yarowsky , On Evaluation of Word Sense Disambiguation Systems , in Proceedings , 30th Annual Meeting of the Association for Computational Linguis tics , 1992b .", "Gale , W . , K . Church , and D . Yarowsky , A Method for Disam biguating Word Senses in a Large Corpus , in Computers and the Humanities , 1993 .", "Hearst , Marti , Noun Homograph Disambiguation Using Local Context in Large Text Corpora , in Using Corpora , University of Waterloo , Waterloo , Ontario , 1991 .", "Leacock , Claudia , Geoffrey Towell and Ellen Voorhees Corpus Based Statistical Sense Resolution , inProceedings , ARPA Human Language Technology Workshop , 1993 .", "Kelly , Edward , and Phillip Stone , Computer Recognition of English Word Senses , North Holland , Amsterdam , 1975 .", "Resnik , Philip , A Class based Approach to Lexical Discov ery , in Proceedings of 3Oth Annual Meeting of the Association for Computational Linguistics , 1992 .", "Rivest , R . L . , Learning Decision Lists , in Machine Learning , 2 , 1987 , pp 229 246 .", "Sproat , R . , J . Hirschberg and D . Yarowsky A Corpus based Synthesizer , in Proceedings , International Conference on Spoken Language Processing , Banff , Alberta .", "Yarowsky , David Word Sense Disambiguation Using Statisti cal Models of Rogets Categories Trained on Large Corpora , in Proceedings , COLING 92 , Nantes , France , 1992 ."], "summary_lines": ["One Sense Per Collocation\n", "Previous work [Gale, Church and Yarowsky, 1992] showed that with high probability a polysemous word has one sense per discourse.\n", "In this paper we show that for certain definitions of collocation, a polysemous word exhibits essentially only one sense per collocation.\n", "We test this empirical hypothesis for several definitions of sense and collocation, and discover that it holds with 90-99% accuracy for binary ambiguities.\n", "We utilize this property in a disambiguation algorithm that achieves precision of 92% using combined models of very local context.\n", "We define collocation as a co-occurrence of two words in a defined relation.\n", "In order to analyze and compare the behavior of several kinds of collocations, we use a measure of entropy as well as the results obtained when tagging heldout data with the collocations organized as decision lists.\n", "We find that the objects of verbs play a more dominant role than their subjects in WSD and nouns acquire more stable disambiguating information from their noun or adjective modifiers.\n"]}
{"article_lines": ["Three Generative Lexicalized Models For Statistical Parsing", "In this paper we first propose a new statistical parsing model , which is a generative model of lexicalised context free grammar .", "We then extend the model to include a probabilistic treatment of both subcategorisation and wh movement .", "Results on Wall Street Journal text show that the parser performs at 88 . 1 87 . 5 constituent precision recall , an average improvement of 2 . 3 over Collins 96 .", "Generative models of syntax have been central in linguistics since they were introduced in Chomsky 57 .", "Each sentence tree pair S , T in a language has an associated top down derivation consisting of a sequence of rule applications of a grammar .", "These models can be extended to be statistical by defining probability distributions at points of non determinism in the derivations , thereby assigning a probability 'P S , T to each S , T pair .", "Probabilistic context free grammar Booth and Thompson 73 was an early example of a statistical grammar .", "A PCFG can be lexicalised by associating a headword with each non terminal in a parse tree ; thus far , Magerman 95 ; Jelinek et al . 94 and Collins 96 , which both make heavy use of lexical information , have reported the best statistical parsing performance on Wall Street Journal text .", "Neither of these models is generative , instead they both estimate 'P T 1 S directly .", "This paper proposes three new parsing models .", "Model 1 is essentially a generative version of the model described in Collins 96 .", "In Model 2 , we extend the parser to make the complement adjunct distinction by adding probabilities over subcategorisation frames for head words .", "In Model 3 we give a probabilistic treatment of wh movement , which is derived from the analysis given in Generalized Phrase Structure Grammar Gazdar et al . 95 .", "The work makes two advances over previous models First , Model 1 performs significantly better than Collins 96 , and Models 2 and 3 give further improvements our final results are 88 . 1 87 . 5 constituent precision recall , an average improvement of 2 . 3 over Collins 96 .", "Second , the parsers in Collins 96 and NIagerman 95 ; Jelinek et al . 94 produce trees without information about whmovement or subcategorisation .", "Most NLP applications will need this information to extract predicateargument structure from parse trees .", "In the remainder of this paper we describe the 3 models in section 2 , discuss practical issues in section 3 , give results in section 4 , and give conclusions in section 5 .", "In general , a statistical parsing model defines the conditional probability , P T S , for each candidate parse tree T for a sentence S . The parser itself is an algorithm which searches for the tree , Tb t , that maximises 'P T 1 S .", "A generative model uses the observation that maximising P T , S is equivalent to maximising P T I S 1 to a top down derivation of the tree .", "In a PCFG , for a tree derived by n applications of context free re write rules LH Si RHS , 1 i n , The re write rules are either internal to the tree , where LHS is a non terminal and RHS is a string of one or more non terminals ; or lexical , where LHS is a part of speech tag and RHS is a word .", "A PCFG can be lexicalised2 by associating a word w and a part of speech POS tag t with each nonterminal X in the tree .", "Thus we write a nonterminal as X x , where x w , t , and X is a constituent label .", "Each rule now has the form3 H is the head child of the phrase , which inherits the head word h from its parent P . L1 . . . L7 , and are left and right modifiers of H . Either n or m may be zero , and n m 0 for unary rules .", "Figure 1 shows a tree which will be used as an example throughout this paper .", "The addition of lexical heads leads to an enormous number of potential rules , making direct estimation of P RHS I LHS infeasible because of sparse data problems .", "We decompose the generation of the RHS of a rule such as 3 , given the LHS , into three steps first generating the head , then making the independence assumptions that the left and right modifiers are generated by separate 0th order markov processes 4 . .", "For example , the probability of the rule S bought NP week NP Marks VP bought would be estimated as but in general the probabilities could be conditioned on any of the preceding modifiers .", "In fact , if the derivation order is fixed to be depth first that is , each modifier recursively generates the sub tree below it before the next modifier is generated then the model can also condition on any structure below the preceding modifiers .", "For the moment we exploit this by making the approximations where distancei and distance , . are functions of the surface string from the head word to the edge of the constituent see figure 2 .", "The distance measure is the same as in Collins 96 , a vector with the following 3 elements 1 is the string of zero length ?", "Allowing the model to learn a preference for rightbranching structures ; 2 does the string contain a verb ?", "Allowing the model to learn a preference for modification of the most recent verb .", "3 Does the string contain 0 , 1 , 2 or 2 commas ?", "where a comma is anything tagged as quot ; , quot ; or probability P R3 r3 I P , H , h , distance , 2 .", "The distance is a function of the surface string from the word after h to the last word of R2 , inclusive .", "In principle the model could condition on any structure dominated by H , R1 or R2 . distinction and subcategorisation The tree in figure 1 is an example of the importance of the complement adjunct distinction .", "It would be useful to identify quot ; Marks quot ; as a subject , and quot ; Last week quot ; as an adjunct temporal modifier , but this distinction is not made in the tree , as both NPs are in the same position' sisters to a VP under an S node .", "From here on we will identify complements by attaching a quot ; C quot ; suffix to non terminals figure 3 gives an example tree .", "A post processing stage could add this detail to the parser output , but we give two reasons for making the distinction while parsing First , identifying complements is complex enough to warrant a probabilistic treatment .", "Lexical information is needed for example , knowledge that quot ; week quot ; is likely to be a temporal modifier .", "Knowledge about subcategorisation preferences for example that a verb takes exactly one subject is also required .", "These problems are not restricted to NPs , compare quot ; The spokeswoman said SBAR that the asbestos was dangerous quot ; vs . quot ; Bonds beat short term investments SBAR because the market is down quot ; , where an SBAR headed by quot ; that quot ; is a complement , but an SBAR headed by quot ; because quot ; is an adjunct .", "The second reason for making the complement adjunct distinction while parsing is that it may help parsing accuracy .", "The assumption that complements are generated independently of each other often leads to incorrect parses see figure 4 for further explanation .", "Adjuncts in the Penn Treebank We add the quot ; C quot ; suffix to all non terminals in training data which satisfy the following conditions In addition , the first child following the head of a prepositional phrase is marked as a complement .", "The model could be retrained on training data with the enhanced set of non terminals , and it might learn the lexical properties which distinguish complements and adjuncts quot ; Marks quot ; vs week quot ; , or quot ; that quot ; vs . quot ; because quot ; .", "However , it would still suffer from the bad independence assumptions illustrated in figure 4 .", "To solve these kinds of problems , the generative process is extended to include a probabilistic choice of left and right subcategorisation frames other leads to errors .", "In 1 the probability of generating both quot ; Dreyfus quot ; and quot ; fund quot ; as subjects , P NP C Dreyf us I S , VP , was P NP C fund I S , VP , was is unreasonably high .", "2 is similar P NP C bill , VP C funding I VP , VB , was P NP C bill I VP , VB , was 'P VP C funding I VP , VB , was is a bad independence assumption .", "Prc RC I P , H , h .", "Each subcat frame is a multiset6 specifying the complements which the head requires in its left or right modifiers . spectively .", "Thus the subcat requirements are added to the conditioning context .", "As complements are generated they are removed from the appropriate subcat multiset .", "Most importantly , the probability of generating the STOP symbol will be 0 when the subcat frame is non empty , and the probability of generating a complement will be 0 when it is not in the subcat frame ; thus all and only the required complements will be generated .", "The probability of the phrase S bought NP week NP C Marks VP bought is now Here the head initially decides to take a single NP C subject to its left , and no complements to its right .", "NP C Marks is immediately generated as the required subject , and NP C is removed from LC , leaving it empty when the next modifier , NP week is generated .", "The incorrect structures in figure 4 should now have low probability because Pic NP C , NP C I S , VP , bought and P NP C , VP C I VP , VB , was are small .", "Another obstacle to extracting predicate argument structure from parse trees is wh movement .", "This section describes a probabilistic treatment of extraction from relative clauses .", "Noun phrases are most often extracted from subject position , object position , or from within PPs It might be possible to write rule based patterns which identify traces in a parse tree .", "However , we argue again that this task is best integrated into the parser the task is complex enough to warrant a probabilistic treatment , and integration may help parsing accuracy .", "A couple of complexities are that modification by an SBAR does not always involve extraction e . g . , quot ; the fact SBAR that besoboru is played with a ball and a bat quot ; , and it is not uncommon for extraction to occur through several constituents , e . g . , quot ; The changes SBAR that he said the government was prepared to make TRACE quot ; .", "The second reason for an integrated treatment of traces is to improve the parameterisation of the model .", "In particular , the subcategorisation probabilities are smeared by extraction .", "In examples 1 , 2 and 3 above 'bought' is a transitive verb , but without knowledge of traces example 2 in training data will contribute to the probability of 'bought' being an intransitive verb .", "Formalisms similar to GPSG Gazdar et al . 95 handle NP extraction by adding a gap feature to each non terminal in the tree , and propagating gaps through the tree until they are finally discharged as a trace complement see figure 5 .", "In extraction cases the Penn treebank annotation co indexes a TRACE with the WHNP head of the SBAR , so it is straightforward to add this information to trees in training data .", "Given that the LHS of the rule has a gap , there are 3 ways that the gap can be passed down to the RHS Head The gap is passed to the head of the phrase , as in rule 3 in figure 5 .", "Left , Right The gap is passed on recursively to one of the left or right modifiers of the head , or is discharged as a trace argument to the left right of the head .", "In rule 2 it is passed on to a right modifier , the S complement .", "In rule 4 a trace is generated to the right of the head VB .", "We specify a parameter PG G I P , h , H where G is either Head , Left or Right .", "The generative process is extended to choose between these cases after generating the head of the phrase .", "The rest of the phrase is then generated in different ways depending on how the gap is propagated In the Head case the left and right modifiers are generated as normal .", "In the Left , Right cases a gap requirement is added to either the left or right SUBCAT variable .", "This requirement is fulfilled and removed from the subcat list when a trace or a modifier non terminal which has the gap feature is generated .", "For example , Rule 2 , SBAR that gap WHNP that S C bought gap , has probability In rule 2 Right is chosen , so the gap requirement is added to RC .", "Generation of S C bought gap ulfills both the S C and gap requirements in RC .", "In rule 4 Right is chosen again .", "Note that generation of trace satisfies both the NP C and gap subcat requirements .", "Table 1 shows the various levels of back off for each type of parameter in the model .", "Note that we decompose P L L , 1ws , Its I P , H , w , t , A , LC where wi and t , are the word and POS tag generated with non terminal Ls , A is the distance measure into the product 'PLI Li itz I P , H , w , t , A , LC x PL , 2 hoi I Ls , its , P , H , w , t , A , LC , and then smooth these two probabilities separately Jason Eisner , p . c . .", "In each case' the final estimate is where e1 , e2 and e3 are maximum likelihood estimates with the context at levels 1 , 2 and 3 in the table , and Ai , A2 and A3 are smoothing parameters where 0 Ai 1 .", "All words occurring less than 5 times in training data , and words in test data which have never been seen in training , are replaced with the quot ; UNKNOWN quot ; token .", "This allows the model to robustly handle the statistics for rare or new words .", "Part of speech tags are generated along with the words in this model .", "When parsing , the POS tags allowed for each word are limited to those which have been seen in training data for that word .", "For unknown words , the output from the tagger described in Ratnaparkhi 96 is used as the single possible tag for that word .", "A CKY style dynamic programming chart parser is used to find the maximum probability tree for each sentence see figure 6 .", "The parser was trained on sections 02 21 of the Wall Street Journal portion of the Penn Treebank Marcus et al . 93 approximately 40 , 000 sentences , and tested on section 23 2 , 416 sentences .", "We use the PARSEVAL measures Black et al . 91 to compare performance number of correct constituents in proposed parse number of constituents in proposed parse number of correct constituents in proposed parse number of constituents in treebank parse Crossing Brackets number of constituents which violate constituent boundaries with a constituent in the treebank parse .", "For a constituent to be 'correct' it must span the same set of words ignoring punctuation , i . e . all tokens tagged as commas , colons or quotes and have the same label' as a constituent in the treebank parse .", "Table 2 shows the results for Models 1 , 2 and 3 .", "The precision recall of the traces found by Model 3 was 93 . 3 90 . 1 out of 436 cases in section 23 of the treebank , where three criteria must be met for a trace to be quot ; correct quot ; 1 it must be an argument to the correct head word ; 2 it must be in the correct position in relation to that head word preceding or following ; 3 it must be dominated by the correct non terminal label .", "For example , in figure 5 the trace is an argument to bought , which it follows , and it is dominated by a VP .", "Of the 436 cases , 342 were string vacuous extraction from subject position , recovered with 97 . 1 98 . 2 precision recall ; and 94 were longer distance cases , recovered with 76 60 . 6 precision recall 9 .", "Model 1 is similar in structure to Collins 96 the major differences being that the quot ; score quot ; for each bigram dependency is Ps Li , 1 , IH , P , h , distances 8 Magerman 95 collapses ADVP and PRT to the same label , for comparison we also removed this distinction when calculating scores .", "9We exclude infinitival relative clauses from these figures , for example quot ; I called a plumber TRACE to fix the sink quot ; where 'plumber' is co indexed with the trace subject of the infinitival .", "The algorithm scored 41 18 precision recall on the 60 cases in section 23 but infinitival relatives are extremely difficult even for human annotators to distinguish from purpose clauses in this case , the infinitival could be a purpose clause modifying 'called' Ann Taylor , p . c . rather than Ps Li , P , H I I , h , distances , and that there are the additional probabilities of generating the head and the STOP symbols for each constituent .", "However , Model 1 has some advantages which may account for the improved performance .", "The model in Collins 96 is deficient , that is for most sentences S , ET P T I S 1 , because probability mass is lost to dependency structures which violate the hard constraint that no links may cross .", "For reasons we do not have space to describe here , Model 1 has advantages in its treatment of unary rules and the distance measure .", "The generative model can condition on any structure that has been previously generated we exploit this in models 2 and 3 whereas Collins 96 is restricted to conditioning on features of the surface string alone .", "Charniak 95 also uses a lexicalised generative model .", "In our notation , he decomposes P RHSi I LHS as P R , , . . . R1HL1 . . L , I P , h x Penn treebank annotation style leads to a very large number of context free rules , so that directly estimating 'P R7 , . . . R1HL1 . . Lin I P , h may lead to sparse data problems , or problems with coverage a rule which has never been seen in training may be required for a test data sentence .", "The complement adjunct distinction and traces increase the number of rules , compounding this problem .", "Eisner 96 proposes 3 dependency models , and gives results that show that a generative model similar to Model 1 performs best of the three .", "However , a pure dependency model omits non terminal information , which is important .", "For example , quot ; hope quot ; is likely to generate a VP TO modifier e . g . , I hope VP to sleep whereas quot ; require quot ; is likely to generate an S TO modifier e . g . , I require S Jim to sleep , but omitting non terminals conflates these two cases , giving high probability to incorrect structures such as quot ; I hope Jim to sleep quot ; or quot ; I require to sleep quot ; .", "Alshawi 96 extends a generative dependency model to include an additional state variable which is equivalent to having non terminals his suggestions may be close to our models 1 and 2 , but he does not fully specify the details of his model , and doesn't give results for parsing accuracy .", "Miller et al . 96 describe a model where the RHS of a rule is generated by a Markov process , although the process is not head centered .", "They increase the set of non terminals by adding semantic labels rather than by adding lexical head words .", "Magerman 95 ; Jelinek et al . 94 describe a history based approach which uses decision trees to estimate P TIS .", "Our models use much less sophisticated n gram estimation methods , and might well benefit from methods such as decision tree estimation which could condition on richer history than just surface distance .", "There has recently been interest in using dependency based parsing models in speech recognition , for example Stolcke 96 .", "It is interesting to note that Models 1 , 2 or 3 could be used as language models .", "The probability for any sentence can be estimated as P S ET P T , s , or making a Viterbi approximation for efficiency reasons as P S P Tbest , S .", "We intend to perform experiments to compare the perplexity of the various models , and a structurally similar 'pure' PCFG1 .", "This paper has proposed a generative , lexicalised , probabilistic parsing model .", "We have shown that linguistically fundamental ideas , namely subcategorisation and wh movement , can be given a statistical interpretation .", "This improves parsing performance , and , more importantly , adds useful information to the parser's output .", "I would like to thank Mitch Marcus , Jason Eisner , Dan Melamed and Adwait Ratnaparkhi for many useful discussions , and comments on earlier versions of this paper .", "This work has also benefited greatly from suggestions and advice from Scott Miller ."], "summary_lines": ["Three Generative Lexicalized Models For Statistical Parsing\n", "In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar.\n", "We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement.\n", "Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).\n", "We provide a 29-million word parsed corpus from the Wall Street Journal.\n"]}
{"article_lines": ["Explo i t ing a Probabi l ist ic Hierarchical Mode l for Generat ion Srinivas Bangalore and Owen Rambow AT T Labs Research 180 Park Avenue F lorham Park , NJ 07932 sr in ? , rambow research , a r t .", "com Abst ract Previous stochastic approaches to generation do not include a tree based representation of syntax .", "While this may be adequate or even advantageous for some applications , other ap plications profit from using as much syntactic knowledge as is available , leaving to a stochas tic model only those issues that are not deter mined by the grammar .", "We present initial re suits showing that a tree based model derived from a tree annotated corpus improves on a tree model derived from an unannotated corpus , and that a tree based stochastic model with a hand crafted grammar outpertbrms both .", "1 I n t roduct ion For many apt lications in natural anguage gen eration NLG , the range of linguistic expres sions that must be generated is quite restricted , and a grammar tbr generation can be fltlly spec ified by hand .", "Moreover , in ma W cases it ; is very important not to deviate from certain linguis tic standards in generation , in which case hand crafted grammars give excellent control .", "How ever , in other applications tbr NLG the variety of the output is much bigger , and the demands on the quality of the output somewhat less strin gent .", "A typical example is NLG in the con text of interlingua or transthr based machine translation .", "Another reason for reb xing the quality of the output may be that not enough time is available to develop a flfll grammar tbr a new target language in NLG .", "In all these cases , stochastic empiricist methods pro vide an alternative to hand crafted rational ist approaches to NLG .", "To our knowledge , the first to use stochastic techniques in NLG were Langkilde and Knight 1998a and 1998b .", "In this paper , we present FERGUS Flexible Em piricist Rationalist Generation Using Syntax .", "FErtGUS follows Langkilde and Knights seminal work in using an n gram language model , but ; we augment it with a tree based stochastic model and a traditional tree based syntactic grammar .", "More recent work on aspects of stochastic gen eration include Langkilde and Knight , 2000 , Malouf , 1999 and Ratnaparkhi , 2000 .", "Betbre we describe in more detail how we use stochastic models in NLG , we recall the basic tasks in NLG Rainbow and Korelsky , 1992 ; Re iter , 1994 .", "During text p lanning , content and structure of the target text ; are determined to achieve the overall communicative goal .", "Dur ing sentence planning , linguistic means in particular , lexical and syntactic means are de termined to convey smaller pieces of meaning .", "l uring real izat ion , the specification chosen in sentence planning is transtbrmed into a surface string , by line rizing and intlecting words in the sentence and typically , adding function words .", "As in the work by Langkilde and Knight , our work ignores the text planning stage , but it ; does address the sentence , planning and the realiza tion stages .", "The structure of the paper is as tbllows .", "In Section 2 , we present he underlying rammat ical tbrmalism , lexicalized tree adjoining gram mar LTAG .", "In Section 3 , we describe the ar chitecture of the system , and some of the mod ules .", "In Section 4 we discuss three experiments .", "In Section 5 we colnpare our work to that of Langkilde and Knight 1998a .", "We conclude with a summary of on going work .", "2 Modeling Syntax In order to model syntax , we use an existing wide coverage grammar of English , the XTAG grammar developed at the University of Peru sylvania XTAG Gronp , 1999 .", "XTAG is a tree adjoining grammar TAG Joshi , 1987a .", "T rees used in der ivat ion , 7 , P A , i P , 7 N Aux 1 N N l NI l A N I I I I I I . JI I I there was n cost estimate iof the second phase 1 3 71 Y2 z2 74 71 75 z 1 Other supertags for the loxemcs found in the training corpus IIOI1C Z4 Z 1 I z 1 z4 z 5 z 2 7 2 z 5 5 more 11 more 4 more I more IlOIle z 3 z 2 z 1 3 2 5 IIIOrC 2 Ill We Figure 1 An excerl t from the XqAG gr um lm t derive Th , r .", "wa . s u , o to . st stim , , tc . fi r the . second phase .", "; dotted lines show t ossit le a ljun ti ms that were not made a TAG , the elementary structures are hrase structure trees which are comt osed using two ot er tions , sut stitui , ion w fich al i ; n ts one tree 1 ; the fl ontier of another mtd a tjnlmtio t which ins ; rts one tree into the mi l ll , of im o ; her .", "In gral hi al re i rese l l ta l ; i l l , l o tes I ; which substitul ; ion can take 1 lac are uarked with dow arrows .", "In linguisI ; ic uses f TAG , we asso ial ; e one lexical item its anchor with each tree , and he or typically more trees with each lexical ire n ; its a result we obtain a lexi calized TAG or LTAG .", "Since ea h lexi al item is associated with a whole tree rather than just a phrase stru ture ule , tbr exa nl le , we cm st e i y t oth the t re licate argument struc ture of the lexeme t y includillg nodes at which its arguments must sut stitute and morl h syntactic onstraints uch as sut je t verb agree men within the sl ; rucl ; ure associated with the l exeme .", "This property is retbrred to as TAGs cztcndcd domain of locality .", "N l ; e that in an LTAG , I ; here is no distinction betw en lexicon nnd grammar .", "A Smnl le grammar is shown in F igure 1 .", "We depart fl om XTAG in our treatment of ; rees tbr adjuncts such as adverl s , an t in stead tbllow McDonMd and Pusteiovsky 1985 .", "While in XTAG the elementary tree for an ad . iuncl ; conl ; ains 1 hrase sl ; ru i ; ure ; hat atta hes l , he adjmmt to ll tes in another tree with the stag anchored by 71 et 72 N 7 A ux 7 . t Pro , l 0 T 1 , A lj adjoins to direction NP N S , VP NP , VP S N right right right h ft right right Figure 2 Adjmmtion table tbr graamnar frag lUe l l t sl ecitie 1 label say , VP from the specified di rection say , fronl the left , in our systenl the trees for adjuncts imply express their active va lency , trot 11o1 ; how they connect to the lexical item they modi y .", "This ilflormal ; ion is kept in the adjunct on table which is associated with the .", "grammar ; an excerpt is shown in Figure 2 .", "Trees t ; hat can adjoin to other trees and have entries in the adjunct on table ; re called gamma trees , the other trees which can only t e substituted into other trees are alpha trees .", "Note that we can refer to a tree by a combi nation of its name , called its supertag , and its anchor .", "N r example , q is the supertag of an all ha tree anchored 1 y a noun that projects up to NP , wMle 72 is ; lie superi ; ag of it gamma tree anchored by a noun that only t rojects 1 ; N we 43 assume adjectives are adjoined at N , and , as the adjunction table shows , can right adjoin to an N . So that es t imate is a particular tree in our LTAG grammar .", "Another tree that a su pertag can be associated with is t , which rep resents the predicative use of a noun . 1 Not all nouns are associated with all nominal supertags the expletive there is only an cq .", "When we derive a sentence using an LTAG , we combine elementary trees flom the grmnmar using adjunction and substitution .", "For extort pie , to derive the sentence There was no cost estimate for the second phase from the gram mar in Figure 1 , we substitute the tree tbr there into the tree tbr estimate .", "We then adjoin in the trees tbr the auxiliary was , the determiner no , and the modit ing noun cost .", "Note that these adjunctions occur at different nodes at VP , NP and N , respectively .", "We then adjoin in the preposition , into which we substitute ph , ase , into which we adjoin the and second .", "Note that all adjunctions are by gamma trees , and all sub stitution by alpha trees .", "If we want to represent this derivation graphi cally , we can do so in a derivation tree , which we obtain as follows whelmver we adjoin or sub stitute a tree t into a tree t2 , we add a new daughter labeled t to the node labeled tg .", "As explained above , the name of each tree used is the lexeme along with the supertag .", "We omit the address at which substitution or adjunction takes place .", "The derivation tree t br our deriva tion is shown in Figure 3 .", "As can be seen , this structure is a dependency tree and resembles a representation of lexical argument structure .", "aoshi 1987b claims that TAGs properties make it particularly suited as a syntactic rep resentation tbr generation .", "Specifically , its ex tended domain of locality is useflfl in genera tion tbr localizing syntactic properties includ ing word order as well as agreement and other morphological processes , and lexicalization is useful tbr providing an interfime from seman tics the deriw tion tree represent the sentences predicate argument structure .", "Indeed , LTAG has been used extensively in generation , start ing with McDonald and Pustejovsky , 1985 .", "1Sentences such as Peter is a doctor can be analyzed with with be as the head , as is more usual , or with doctor as the head , as is done in XTAG 1 eeause the be really behaves like an auxiliary , not like a flfll verb .", "estimate there was no cost for 74 c 1 7 3 71 72 phase c the second 71 75 Figure 3 Derivation tree tbr LTAG deriw tion of There was no cost estimate for the second phase 3 System Overv iew FERGUS is composed of three modules the 2 ? ee Chooser , the Unraveler , and the Linear Prece dence LP Chooser .", "The input to the system is a dependency tree as shown in Figme 4 .", "Note that the nodes are labeled only with lexemes , not with supertags .", "2 The Tree Chooser then uses a stochastic tree model to choose TAG trees fbr the nodes in the input structure .", "This step can be seen as analogous to supertag ging Bangalore and Joshi , 1999 , except that now supertags i . e . , names of trees must be fbund tbr words in a tree rather than tbr words in a linear sequence .", "The Unraveler then uses the XTAG grammar to produce a lattice of all possible linearizations that arc compatible with the supertagged tree and the XTAG .", "The LP Chooser then chooses the most likely traversal of this lattice , given a language model .", "We dis cuss the three components in more detail .", "The Tree Chooser draws on a tree model , which is a representation of XTAG derivation tbr 1 , 000 , 000 words of the Wall Street Journal .", "a The IYee Chooser makes the simplifying as 2In the system that we used in the experiments de scribed in Section 4 , all words including flmction words need to be present in tt , e inlmt representation , flflly in flected .", "This is of course unrealistic for applications .", "In this paper , we only aim to show that the use of a ee Model improves performance of a stochastic generator .", "See Section 6 for further discussion .", "3This was constructed from the Penn lS ee Bank us ing some heuristics , since the Pemt IYee Bank does not contain hill head dependent infornlation ; as a result of the use of heuristics , the Tree Model is not flflly correct .", "44 estimate there was no cost for phase the second Figure 4 Inlmt to FEII . GUS Smnl tions that the hoice of n tree .", "tbr t node dei ends only on its daughter nodes , thus allow ing or a tot lown dynamic l rogrmnlning algo ril ; hln .", "St ccifically , a node q in the intml ; si ; ru ture is assigned t sui e , rt ; g s so th ; tt the 1 rol t ilil ; y of fin ling the treelet ; m ose t of 1 with superta X , ; rod dl of its l ; mght ; rs as foun t in I ; he ini ut sl ; rucl ; ure is m ; rximiz ; d , and such l ; ha , t . is Oml a , tit le with q s mother tll l her sut e , rtag . sin .", "Here , omt atible l nemis ; hat ; the tree ret resclfl ; ed by . can 1 e adjoined or substii ; uted into the tree ret resented by , , m or ling to the XTAG gra , nmmr .", "For our exmn t le senl ; en ; , the , ini ui ; 1 ; o the sysl , e , m is the t ; ree shown in Figure d , and the oul ; 1 ul ; fiom l ; he . l ee hooser is the , tree .", "; ts shown in , igure .", "No ; c that while a le , riw tion tree in TAG fully Sl iiies a derivation and thus t smTth , e , s mte . n e , the oul ; lmt fl om the l ee Chooser loes not ; .", "There are two reasons .", "? irstly , as exi laine . d at ; the end of Section 2 , fin us trees orrespond ing to adjuncts are underspe .", "itied with rest ect to the adjunct ion site aat t or I ; h ; a ljmwl ; ion direction from left ; or flOln right in the tree of the mother node , or they nmy 1 e m orde . re l with respc t o other ad . iun ts tbr ex nni l ; , the fmnous adjective ordering t roblem .", "Secondly , Sul ert ; ags nl y h ve been hose . n incorre l ; ly or not at ; ill .", "The Unr ; veler takes ; s input the senti specitied derivation tree , Figure 3 ml l 1 ro duces a word lattice .", "Each node , in the deriw tion tree consisl ; s of t lexi al item m d a su pertag .", "The linear order f the dmlghte . rs with rest cot to l ; he he ; td 1 osil ; ion of ; t sut ertng is st ecilied in the XrlAG grmnmar .", "This informa tion is onsulted to order the laughter nodes I TAG I eliwtlion Tree wilht , ut , SIIpeltags llCC ? h ? scl J Tree h I One siagle sealli specified IAG l cdvalion lees I halll , llill W nd l . altice Shillg Figure 5 Ar hii ; e ture of FERGUS with rcsl e t to the head at each le . vel of the terival ; ion tree .", "in cases where daughter node C ll I ntta hed at more thin1 lie t lace in the head SUl ertag as is the ; se in our exmnt le for was and for , n disjunction of M1 these , positions are .", "assigned to the dmlghter node .", "A botton up algorithm the . n constructs lattice that ell ; odes the strings rei re . sented 1 y ; 1 level of th !", "derivation tr x . .", "The latti e at the .", "root of the teriwttion tr w . is the result o171 ; m Um ; tveler .", "Fhe resulting l ttti ; for the . Xaml h . s ml ; e . nce is shown in Figure 6 .", "OUtlmt from the .", "Unra . veha en codes all t ossible word sequences l erniitted 1 y the derivation strueialre .", "word sequen es in the order of their likeli hoo l 1 y composing the lattice with a finite state machine rel rese . nting trigrmn bmgu Ge 1no tel .", "This mo M has 1 ee . n onstructed froln 1 , 000 , 0000 words of W dl Stre , et Journal orpus .", "We 1 i k the 1 est path through the lattice , re sulting from the comt osition using the Viterl i algorithm , ; m t this to I ranking word sequence is the outt ut of the LP Chooser .", "4 Experiments and Results In order l o show ; ll tl ; Lhe llSO , of t tl ce lIlode trod a , grmmnar doe . s indeed hell pe , rformmme , we pe . rforme , d three experiments 45 Q Figure 6 Word lattice tbr example sentence after Tree Chooser and Unraveler using the supertag based model ?", "For the baseline experiment , we impose a random tree structure ibr each sentence of the cortms and build a Tree Model whose parameters consist of whether a lexeme l t precedes or tbllows her mother lexeme lm .", "We call this the Baseline Left Right LR Model .", "This model generates There was est imate for phase the second no cost .", "for our example input .", "In the second experiment , we derive the parmneters tbr the LR model flom an an notated corpus , in particular , the XTAG derivation tree cortms .", "This model gener ates Th , crc no est imate Jor the second phase was cost .", "tbr our example input .", "In the third experiment , as described in Section 3 , we employ the supertag based tree model whose parameters consist of whether a lexeme l d with supertag Sd is zt dependent of Im with supertag sin .", "Fm thermore we use the supertag in brmation provided by the XTAG grammar to or der the dependents .", "This model generates Thcrc was no cost est imate for the second phase .", "tbr our example input , which is in deed the sentence ibund in the WSJ .", "As in the case of machine translation , evalu ation in generation is a complex issue .", "We use two metrics suggested in the MT literature A1 shawl et al . , 1 . 998 based on string edit ; distance t etween the outtmt of the generation system and the reference corpus string front the WSJ .", "These metrics , simple accuracy and generation accuracy , allow us to evaluate without human intervention , automatically and objectively .", "4 Simple accuracy is the mnnber of insertion I , deletion D and substitutions S errors between the target language strings in the test corpus and the strings produced by the genera tion model .", "The metric is summarized in Equa tion 1 .", "R is the number of tokens in the target string .", "This metric is similar to the string dis tance metric used for measuring speech recog nition accuracy .", "I D . q S implcAccuracy 1 1 R 4 7c do not address the issue of whether these metrics can be used for comparative valuation of other genera tion systems .", "46 Tree Model Simt le Go , ner rtion Ac ura y Accuracy Average time per scnten ; Baseline LR Model 41 . 2 56 . 2 186ms l ?", "; cbank derived LI .", "Model 52 . 9 66 . 8 129ms Sut ertag bascd Model 58 . !", "72 . 4 517ms Tabl ; 1 Performance results front the thre , tree models .", "Unlike sl eech recognition , the task of gener ation involves reordering of tokens .", "The simple accuracy metric , however , penalizes a mist lacc . d token twice , as a deletion from its c . xpo , ct . d posi tion and insertion at at different l osition .", "Wc llSO second metric , Generation A ura y , shown in Eqm tion 2 , which treats hilt ion of token tt OIIC location in 1 ; 11 ; string md th ; insertion of the same tok m t anoth a location in tim string as one single mov an mt ; trot M .", "This is in addition to the rem fining insertions 1 t and deletions Dl .", "Gen rationAcc , , racy 1 54 I I 1 t , q 2 The siml lc , a cura y , g merntion a ur ; my a , n l tim av n ag time , ti r goamration of ; a , h l ; cst ; s , u t m c for tim tino , , Xl crinmnts ; r tabul m , xl in d le 1 .", "The test set consist xl of 1 O0 r m tonfly hoscn WS . I s mt m ; with ml w n age l ngt ; h of 16 words .", "As can be seen , tim sut crtng 1 ased mo M rot roves over the LR model derived from mmotated ata md both models improv ; over the baseline LR mod l . Sul ertngs incorl or te richer infbrmation st oh as argunmnt mid a tjunci disl ; in tion , and nmnbcr and types of argunmnts .", "YVe cxt ; t to iml rove the performance of the supcrtag bas ; d model by taking these features into a ount .", "In ongoing work , we h vc developed tree based metrics in addition to the string l ased presented here , in order to ewfluate sto hastic gener tion models .", "We h vc also attempted to correlate these quantitative metrics with human tualitativ judgcnl mts .", "Ado , tail d dis ussion of these experiments and results is t r , s mto , d in Bangalore , ; al . , 2000 .", "5 Compar i son w i th Langk i lde 8z Kn ight Langkildc and Knight 1998a use a hand rafted grmmmu that maps semantic represen tations to sequences of words with lino , arization constraints .", "A COml lex semantic st , ructur , is trnnsl ted to L lattice , , mid a bigrmn langunge mode , 1 t ; hell hoost , s lltOllg ; lo , l ossiblo , surface , strings moo led in the l ttice .", "The system of Langkildc 8 Knight , Nitrogen , is similar to FERGUS in that generation is di vided into two phases , the first of which results in a lattice flom which a surNcc si ; ring is chosen during the , s ; cond t has ; using a language model in our case a trigram model , in Nitrogens case a .", "1 igr ml 1no M .", "Ih w , ver , ; t1 ; first t hases nr , quit , ditf ; r mt .", "In FEI . GUS , we sI ; m i ; with a lex i d pr , dit at ; argulnent st ; ru ; l ; ur while in Ni trogen , a more s0 , mantic intmt is used .", "FEII . GUS ould , asily ; augm ; nt ; d with a t r ; t ro cssor l ; h d ; maps a so , m ; mti rc , t ro , s mtal ; ion t ; o ore syn ta ti inl ut ; this is not the focus of our r sc uch .", "Iowev , r , ther are two more imt orl , mfl ; differ m es .", "First , ; t1 ; h m t crafl ; ed grmmnar in Ni trogen maps dir ; tly from semantics to a linear r l r ; sentation , skipping tho , nr or ; s mt rcI rc sentation usually f vore t br the , rod r , s mtn ; ion of syntax .", "There is no stochastic tree model , since , the , re , tr , no trees .", "In FEI GUS , in tied hoices arc , ma tc stochastically t ascd on tim tree rcl rcscntation in the I ? ce Chooser .", "This allows us to capture stochastically certain long tisl ; ance cfli , ts which n grmns camlot , such as sct ration of p ; nts of a collocations such as peT form an ope ution through interl osing ad juncts John peT formed a long , . somewhat e dious , and quite frustrating opcration on hi , s border collie .", "Second , tim hand rafl ; cd gram ln ; tr llSCd in FEll .", "IUS was crafted indel endcntly flom the n ; xl for gent , rat ; ion and is a imrcly l ; larative rcl rcs mtation of English syntax .", "As 47 such , we can use it to handle morphological ef fects such as agreement , which cannot in gen eral be clone by an n gram model and which are , at ; the same time , descriptively straightforward and which are handled by all non stochastic generation modules .", "6 Conclus ion and Out look We have presented empirical evidence that us ing a tree model in addition to a language model can improve stochastic NLG .", "FERGUS aS presented in this paper is not ready to be used as a module in applications .", "Specifically , we will add a morphological compo nent , a component that handles flmction words auxiliaries , determiners , and a component that handles imnctuation .", "In all three cases , we will provide both knowledge based and stochas tic components , with the aim of comparing their behaviors , and using one type as a back up tbr the other type .", "Finally , we will explore FI ; R OUS when applied to a language tbr which a much more limited XTAG grammar is available for example , specit ying only the basic sentence word order as , sw , SVO , and speci ying subject verb agreement .", "In the long run , we intend FEI OUS to become a flexible system which will use hand crafted knowledge as much as possible and stochastic models as much as necessary .", "References Hiyan Alshawi , Srinivas Bangalore , and Shona Douglas .", "Automatic acquisition of hi erarchical transduction models tbr machine tr anslation .", "In Proceedings of the 36th Annual Meeting Association for Computational Lin guistics , Montreal , Canada .", "Srinivas Bangalore and Aravind Joshi .", "Supertagging An approach to ahnost pars ing .", "Computational Linguistics , 25 2 .", "Sriniw s Bangalore , Owen Rainbow , and Steve Whittaker .", "Ewfluation Metrics for Generation .", "In Proceedings of International Cor ferenee on Natural Language Generation , Mitzpe Ramon .", "Aravind K . Joshi .", "An introduction to Tree Adjoining Grammars .", "In A . Manaster Ramer , editor , Mathematics of Language , pages 87 115 .", "John Benjamins , Amsterdam .", "Aravind K . Joshi .", "Tlm relevance of tree adjoining grammar to generation .", "In Gerard Kempeu , editor , Natural Language Generation New Results in Artificial In teUigence , Psychology and Linguistics , pages 233 252 .", "Kluwer Academic Publishers , Dor drecht Boston Lancaster .", "Irene Langkilde and Kevin Knight .", "Gen eration that exploits corpus based statistical knowledge .", "In 36th Meeting of the Associa tion . for Computational Linguistics and 17th International Cor crcnce on Computational Linguistics COLING A CL98 , pages 704 710 , Montrdal , Canada .", "Irene Langkilde and Kevin Knight .", "The practical value of n grams in genera tion .", "In Proceedings of the Ninth Interna tional Natural Language Generation Work shop INLG98 , Niagara on the Lake , On tario .", "Irene Langkilde and Kevin Knight .", "Forest based statistical sentence generation .", "In Proceedings of First North American A CL , Seattle , USA , May .", "Two methods tbr 1 re dieting the order of prenonfinal t djectives in english .", "In Pwceedings of CLINg9 .", "David D . McDonMd and James D . Pusteiovsky .", "gs as a grammatical formalism tbr generation .", "In 23rd Meeting of the Associa tion for Computational Linguistics A CL 85 , pages 94 103 , Chicago , IL .", "Owen l ambow and Tany Korelsky .", "Ap plied text generation .", "In Third Conference on Applied Natural Language Processing , pages 40 47 , ento , Italy .", "Adwait t . atllaparkhi .", "Trainable methods for surface natural language generation .", "In Proceedings of First North American ACL , Seattle , USA , May .", "Has a consensus NL gen eration architecture appeared , and is it psy cholinguistically plausible ?", "In Proceedings of the 7th International Workshop on Natural Language Generation , pages 163 170 , Maine .", "The XTAG Group .", "A lexicalized ee Adjoining Grammar for English .", "Technical Report ht tp w rw .", "upenn , edu xtag tech repor t tech repor t . htral , The Insti tute for Research in Cognitive Science , Uni versity of Pennsylvania ."], "summary_lines": ["Exploiting A Probabilistic Hierarchical Model For Generation\n", "Previous stochastic approaches to generation do not include a tree-based representation of syntax.\n", "While this may be adequate or even advantageous for some applications, other applications profit from using as much syntactic knowledge as is available, leaving to a stochastic model only those issues that are not determined by the grammar.\n", "We present initial results showing that a tree-based model derived from a tree-annotated corpus improves on a tree model derived from an unannotated corpus, and that a tree-based stochastic model with a hand-crafted grammar outperforms both.\n", "Our system, FERGUS takes dependency structures as inputs, and produced XTAG derivations by a stochastic tree model automatically acquired from an annotated corpus.\n", "The Fergus system employs a statistical tree model to select probable trees and a word n-gram model to rank the string candidates generated from the best trees.\n"]}
{"article_lines": ["Classifying The Semantic Relations In Noun Compounds Via A Domain Specific Lexical Hierarchy", "We are developing corpus based techniques for identifying semantic relations at an intermediate level of description more specific than those used in case frames , but more general than those used in traditional knowledge representation systems .", "In this paper we describe a classification algorithm for identifying relationships between two word noun compounds .", "We find that a very simple approach using a machine learning algorithm and a domain specific lexical hierarchy successfully generalizes from training instances , performing better on previously unseen words than a baseline consisting of training on the words themselves .", "We are exploring empirical methods of determining semantic relationships between constituents in natural language .", "Our current project focuses on biomedical text , both because it poses interesting challenges , and because it should be possible to make inferences about propositions that hold between scientific concepts within biomedical texts Swanson and Smalheiser , 1994 .", "One of the important challenges of biomedical text , along with most other technical text , is the proliferation of noun compounds .", "A typical article title is shown below ; it consists a cascade of four noun phrases linked by prepositions Open labeled long term study of the efficacy , safety , and tolerability of subcutaneous sumatriptan in acute migraine treatment .", "The real concern in analyzing such a title is in determining the relationships that hold between different concepts , rather than on finding the appropriate attachments which is especially difficult given the lack of a verb .", "And before we tackle the prepositional phrase attachment problem , we must find a way to analyze the meanings of the noun compounds .", "Our goal is to extract propositional information from text , and as a step towards this goal , we classify constituents according to which semantic relationships hold between them .", "For example , we want to characterize the treatment for disease relationship between the words of migraine treatment versus the method of treatment relationship between the words of aerosol treatment .", "These relations are intended to be combined to produce larger propositions that can then be used in a variety of interpretation paradigms , such as abductive reasoning Hobbs et al . , 1993 or inductive logic programming Ng and Zelle , 1997 .", "Note that because we are concerned with the semantic relations that hold between the concepts , as opposed to the more standard , syntax driven computational goal of determining left versus right association , this has the fortuitous effect of changing the problem into one of classification , amenable to standard machine learning classification techniques .", "We have found that we can use such algorithms to classify relationships between two word noun compounds with a surprising degree of accuracy .", "A one out of eighteen classification using a neural net achieves accuracies as high as 62 .", "By taking advantage of lexical ontologies , we achieve strong results on noun compounds for which neither word is present in the training set .", "Thus , we think this is a promising approach for a variety of semantic labeling tasks .", "The reminder of this paper is organized as follows Section 2 describes related work , Section 3 describes the semantic relations and how they were chosen , and Section 4 describes the data collection and ontologies .", "In Section 5 we describe the method for automatically assigning semantic relations to noun compounds , and report the results of experiments using this method .", "Section 6 concludes the paper and discusses future work .", "Several approaches have been proposed for empirical noun compound interpretation .", "Lauer and Dras 1994 point out that there are three components to the problem identification of the compound from within the text , syntactic analysis of the compound left versus right association , and the interpretation of the underlying semantics .", "Several researchers have tackled the syntactic analysis Lauer , 1995 ; Pustejovsky et al . , 1993 ; Liberman and Sproat , 1992 , usually using a variation of the idea of finding the subconstituents elsewhere in the corpus and using those to predict how the larger compounds are structured .", "We are interested in the third task , interpretation of the underlying semantics .", "Most related work relies on hand written rules of one kind or another .", "Finin 1980 examines the problem of noun compound interpretation in detail , and constructs a complex set of rules .", "Vanderwende 1994 uses a sophisticated system to extract semantic information automatically from an on line dictionary , and then manipulates a set of hand written rules with handassigned weights to create an interpretation .", "Rindflesch et al . 2000 use hand coded rule based systems to extract the factual assertions from biomedical text .", "Lapata 2000 classifies nominalizations according to whether the modifier is the subject or the object of the underlying verb expressed by the head noun . 1 In the related sub area of information extraction Cardie , 1997 ; Riloff , 1996 , the main goal is to find every instance of particular entities or events of interest .", "These systems use empirical techniques to learn which terms signal entities of interest , in order to fill in pre defined templates .", "Our goals are more general than those of information extraction , and so this work should be helpful for that task .", "However , our approach will not solve issues surrounding previously unseen proper nouns , which are often important for information extraction tasks .", "There have been several efforts to incorporate lexical hierarchies into statistical processing , primarily for the problem of prepositional phrase PP attachment .", "The current standard formulation is given a verb followed by a noun and a prepositional phrase , represented by the tuple v , n1 , p , n2 , determine which of v or n1 the PP consisting of p and n2 attaches to , or is most closely associated with .", "Because the data is sparse , empirical methods that train on word occurrences alone Hindle and Rooth , 1993 have been supplanted by algorithms that generalize one or both of the nouns according to classmembership measures Resnik , 1993 ; Resnik and Hearst , 1993 ; Brill and Resnik , 1994 ; Li and Abe , 1998 , but the statistics are computed for the particular preposition and verb .", "It is not clear how to use the results of such analysis after they are found ; the semantics of the relationship between the terms must still be determined .", "In our framework we would cast this problem as finding the relationship R p , n2 that best characterizes the preposition and the NP that follows it , and then seeing if the categorization algorithm determines their exists any relationship R' n1 , R p , n2 or R' v , R p , n2 .", "The algorithms used in the related work reflect the fact that they condition probabilities on a particular verb and noun .", "Resnik 1993 ; 1995 use classes in Wordnet Fellbaum , 1998 and a measure of conceptual association to generalize over the nouns .", "Brill and Resnik 1994 use Brill s transformation based algorithm along with simple counts within a lexical hierarchy in order to generalize over individual words .", "Li and Abe 1998 use a minimum description length based algorithm to find an optimal tree cut over WordNet for each classification problem , finding improvements over both lexical association Hindle and Rooth , 1993 and conceptual association , and equaling the transformation based results .", "Our approach differs from these in that we are using machine learning techniques to determine which level of the lexical hierarchy is appropriate for generalizing across nouns .", "In this work we aim for a representation that is intermediate in generality between standard case roles such as Agent , Patient , Topic , Instrument , and the specificity required for information extraction .", "We have created a set of relations that are sufficiently general to cover a significant number of noun compounds , but that can be domain specific enough to be useful in analysis .", "We want to support relationships between entities that are shown to be important in cognitive linguistics , in particular we intend to support the kinds of inferences that arise from Talmy s force dynamics Talmy , 1985 .", "It has been shown that relations of this kind can be combined in order to determine the directionality of a sentence e . g . , whether or not a politician is in favor of , or opposed to , a proposal Hearst , 1990 .", "In the medical domain this translates to , for example , mapping a sentence into a representation showing that a chemical removes an entity that is blocking the passage of a fluid through a channel .", "The problem remains of determining what the appropriate kinds of relations are .", "In theoretical linguistics , there are contradictory views regarding the semantic properties of noun compounds NCs .", "Levi 1978 argues that there exists a small set of semantic relationships that NCs may imply .", "Downing 1977 argues that the semantics of NCs cannot be exhausted by any finite listing of relationships .", "Between these two extremes lies Warren s 1978 taxonomy of six major semantic relations organized into a hierarchical structure .", "We have identified the 38 relations shown in Table 1 .", "We tried to produce relations that correspond to the linguistic theories such as those of Levi and Warren , but in many cases these are inappropriate .", "Levi s classes are too general for our purposes ; for example , she collapses the location and time relationships into one single class In and therefore field mouse and autumnal rain belong to the same class .", "Warren s classification schema is much more detailed , and there is some overlap between the top levels of Warren s hierarchy and our set of relations .", "For example , our Cause 2 1 for flu virus corresponds to her Causer Result of hay fever , and our Person Afflicted migraine patient can be thought as Warren s Belonging Possessor of gunman .", "Warren differentiates some classes also on the basis of the semantics of the constituents , so that , for example , the Time relationship is divided up into Time Animate Entity of weekend guests and Time Inanimate Entity of Sunday paper .", "Our classification is based on the kind of relationships that hold between the constituent nouns rather than on the semantics of the head nouns .", "For the automatic classification task , we used only the 18 relations indicated in bold in Table 1 for which an adequate number of examples were found in the current collection .", "Many NCs were ambiguous , in that they could be described by more than one semantic relationship .", "In these cases , we simply multi labeled them for example , cell growth is both Activity and Change , tumor regression is Ending reduction and Change and bladder dysfunction is Location and Defect .", "Our approach handles this kind of multi labeled classification .", "Two relation types are especially problematic .", "Some compounds are non compositional or lexicalized , such as vitamin k and e2 protein ; others defy classification because the nouns are subtypes of one another .", "This group includes migraine headache , guinea pig , and hbv carrier .", "We placed all these NCs in a catch all category .", "We also included a wrong category containing word pairs that were incorrectly labeled as NCs . 2 The relations were found by iterative refinement based on looking at 2245 extracted compounds described in the next section and finding commonalities among them .", "Labeling was done by the authors of this paper and a biology student ; the NCs were classified out of context .", "We expect to continue development and refinement of these relationship types , based on what ends up clearly being use2The percentage of the word pairs extracted that were not true NCs was about 6 ; some examples are treat migraine , ten patient , headache more .", "We do not know , however , how many NCs we missed .", "The errors occurred when the wrong label was assigned by the tagger see Section 4 . ful downstream in the analysis .", "The end goal is to combine these relationships in NCs with more that two constituent nouns , like in the example intranasal migraine treatment of Section 1 .", "To create a collection of noun compounds , we performed searches from MedLine , which contains references and abstracts from 4300 biomedical journals .", "We used several query terms , intended to span across different subfields .", "We retained only the titles and the abstracts of the retrieved documents .", "On these titles and abstracts we ran a part of speech tagger Cutting et al . , 1991 and a program that extracts only sequences of units tagged as nouns .", "We extracted NCs with up to 6 constituents , but for this paper we consider only NCs with 2 constituents .", "The Unified Medical Language System UMLS is a biomedical lexical resource produced and maintained by the National Library of Medicine Humphreys et al . , 1998 .", "We use the MetaThesaurus component to map lexical items into unique concept IDs CUIs . 3 The UMLS also has a mapping from these CUIs into the MeSH lexical hierarchy Lowe and Barnett , 1994 ; we mapped the CUIs into MeSH terms .", "There are about 19 , 000 unique main terms in MeSH , as well as additional modifiers .", "There are 15 main subhierarchies trees in MeSH , each corresponding to a major branch of medical ontology .", "For example , tree A corresponds to Anatomy , tree B to Organisms , and so on .", "The longer the name of the MeSH term , the longer the path from the root and the more precise the description .", "For example migraine is C10 . 228 . 140 . 546 . 800 . 525 , that is , C a disease , C10 Nervous System Diseases , C10 . 228 Central Nervous System Diseases and so on .", "We use the MeSH hierarchy for generalization across classes of nouns ; we use it instead of the other resources in the UMLS primarily because of MeSH s hierarchical structure .", "For these experiments , we considered only those noun compounds for which both nouns can be mapped into MeSH terms , resulting in a total of 2245 NCs .", "Because we have defined noun compound relation determination as a classification problem , we can make use of standard classification algorithms .", "In particular , we used neural networks to classify across all relations simultaneously . shown in boldface are those used in the experiments reported on here .", "Relation ID numbers are shown in parentheses by the relation names .", "The second column shows the number of labeled examples for each class ; the last row shows a class consisting of compounds that exhibit more than one relation .", "The notation 1 2 and 2 1 indicates the directionality of the relations .", "For example , Cause 1 2 indicates that the first noun causes the second , and Cause 2 1 indicates the converse .", "We ran the experiments creating models that used different levels of the MeSH hierarchy .", "For example , for the NC flu vaccination , flu maps to the MeSH term D4 . 808 . 54 . 79 . 429 . 154 . 349 and vaccination to G3 . 770 . 670 . 310 . 890 .", "Flu vaccination for Model 4 would be represented by a vector consisting of the concatenation of the two descriptors showing only the first four levels D4 . 808 . 54 . 79 G3 . 770 . 670 . 310 see Table 2 .", "When a word maps to a general MeSH term like treatment , Y11 zeros are appended to the end of the descriptor to stand in place of the missing values so , for example , treatment in Model 3 is Y 11 0 , and in Model 4 is Y 11 0 0 , etc . .", "The numbers in the MeSH descriptors are categorical values ; we represented them with indicator variables .", "That is , for each variable we calculated the number of possible categories c and then represented an observation of the variable as a sequence of c binary variables in which one binary variable was one and the remaining c 1 binary variables were zero .", "We also used a representation in which the words themselves were used as categorical input variables we call this representation lexical .", "For this collection of NCs there were 1184 unique nouns and therefore the feature vector for each noun had 1184 components .", "In Table 3 we report the length of the feature vectors for one noun for each model .", "The entire NC was described by concatenating the feature vectors for the two nouns in sequence .", "The NCs represented in this fashion were used as input to a neural network .", "We used a feed forward network trained with conjugate gradient descent . number corresponds to the level of the MeSH hierarchy used for classification .", "Lexical NN is Neural Network on Lexical and Lexical Log Reg is Logistic Regression on NN .", "Acc1 refers to how often the correct relation is the top scoring relation , Acc2 refers to how often the correct relation is one of the top two according to the neural net , and so on .", "Guessing would yield a result of 0 . 077 .", "The network had one hidden layer , in which a hyperbolic tangent function was used , and an output layer representing the 18 relations .", "A logistic sigmoid function was used in the output layer to map the outputs into the interval 0 , 1 .", "The number of units of the output layer was the number of relations 18 and therefore fixed .", "The network was trained for several choices of numbers of hidden units ; we chose the best performing networks based on training set error for each of the models .", "We subsequently tested these networks on held out testing data .", "We compared the results with a baseline in which logistic regression was used on the lexical features .", "Given the indicator variable representation of these features , this logistic regression essentially forms a table of log odds for each lexical item .", "We also compared to a method in which the lexical indicator variables were used as input to a neural network .", "This approach is of interest to see to what extent , if any , the MeSH based features affect performance .", "Note also that this lexical neural network approach is feasible in this setting because the number of unique words is limited 1184 such an approach would not scale to larger problems .", "In Table 4 and in Figure 1 we report the results from these experiments .", "Neural network using lexical features only yields 62 accuracy on average across all 18 relations .", "A neural net trained on Model 6 using the MeSH terms to represent the nouns yields an accuracy of 61 on average across all 18 relations .", "Note that reasonable performance is also obtained for Model 2 , which is a much more general representation .", "Table 4 shows that both methods achieve up to 78 accuracy at including the correct relation among the top three hypothesized .", "Multi class classification is a difficult problem Vapnik , 1998 .", "In this problem , a baseline in which Testing set performance on the best models for each MeSH level Levels of the MeSH Hierarchy the algorithm guesses yields about 5 accuracy .", "We see that our method is a significant improvement over the tabular logistic regression based approach , which yields an accuracy of only 31 percent .", "Additionally , despite the significant reduction in raw information content as compared to the lexical representation , the MeSH based neural network performs as well as the lexical based neural network .", "And we again stress that the lexical based neural network is not a viable option for larger domains .", "Figure 2 shows the results for each relation .", "MeSH based generalization does better on some relations for example 14 and 15 and Lexical on others 7 , 22 .", "It turns out that the test set for relationship 7 Produces on a genetic level is dominated by NCs containing the words alleles and mrna and that all the NCs in the training set containing these words are assigned relation label 7 .", "A similar situation is seen for relation 22 , Time 2 1 .", "In the test set examples the second noun is either recurrence , season or time .", "In the training set , these nouns appear only in NCs that have been labeled as belonging to relation 22 .", "On the other hand , if we look at relations 14 and 15 , we find a wider range of words , and in some cases the words in the test set are not present in the training set .", "In relationship 14 Purpose , for example , vaccine appears 6 times in the test set e . g . , varicella vaccine .", "In the training set , NCs with vaccine in it have also been classified as Instrument antigen vaccine , polysaccharide vaccine , as Object vaccine development , as Subtype of opv vaccine and as Wrong vaccines using .", "Other words in the test set for 14 are varicella which is present in the trainig set only in varicella serology labeled as Attribute of clinical study , drainage which is in the training set only as Location gallbladder drainage and tract drainage and Activity bile drainage .", "Other test set words such as immunisation and carcinogen do not appear in the training set at all .", "In other words , it seems that the MeSHk based categorization does better when generalization is required .", "Additionally , this data set is dense in the sense that very few testing words are not present in the training data .", "This is of course an unrealistic situation and we wanted to test the robustness of the method in a more realistic setting .", "The results reported in Table 4 and in Figure 1 were obtained splitting the data into 50 training and 50 testing for each relation and we had a total of 855 training points and 805 test points .", "Of these , only 75 examples in the testing set consisted of NCs in which both words were not present in the training set .", "We decided to test the robustness of the MeSHbased model versus the lexical model in the case of unseen words ; we are also interested in seeing the relative importance of the first versus the second noun .", "Therefore , we split the data into 5 training 73 data points and 95 testing 1587 data points and partitioned the testing set into 4 subsets as follows the numbers in parentheses are the numbers of points for each case Table 5 and Figures 3 and 4 present the accuracies for these test set partitions .", "Figure 3 shows that the MeSH based models are more robust than the lexical when the number of unseen words is high and when the size of training set is very small .", "In this more realistic situation , the MeSH models are able to generalize over previously unseen words .", "For unseen words , lexical reduces to guessing . 4 Figure 4 shows the accuracy for the MeSH basedmodel for the the four cases of Table 5 .", "It is interesting to note that the accuracy for Case 1 first noun not present in the training set is much higher than the accuracy for Case 2 second noun not present in the training set .", "This seems to indicate that the second noun is more important for the classification that the first one .", "We have presented a simple approach to corpusbased assignment of semantic relations for noun compounds .", "The main idea is to define a set of relations that can hold between the terms and use standard machine learning techniques and a lexical hierarchy to generalize from training instances to new examples .", "The initial results are quite promising .", "In this task of multi class classification with 18 classes we achieved an accuracy of about 60 .", "These results can be compared with Vanderwende Note that for unseen words , the baseline lexical based logistic regression approach , which essentially builds a tabular representation of the log odds for each class , also reduces to random guessing .", "Testing set performances for different partitions on the test set Levels of the MeSH Hierarchy els accuracies for the entire test set and for case 4 and the dashed lines represent the corresponding lexical accuracies .", "The accuracies are smaller than the previous case of Table 4 because the training set is much smaller , but the point of interest is the difference in the performance of MeSH vs . lexical in this more difficult setting .", "Note that lexical for case 4 reduces to random guessing .", "Testing set performances for different partitions on the test set for the MeSH based model Levels of the MeSH Hierarchy 1994 who reports an accuracy of 52 with 13 classes and Lapata 2000 whose algorithm achieves about 80 accuracy for a much simpler binary classification .", "We have shown that a class based representation performes as well as a lexical based model despite the reduction of raw information content and despite a somewhat errorful mapping from terms to concepts .", "We have also shown that representing the nouns of the compound by a very general representation Model 2 achieves a reasonable performance of aout 52 accuracy on average .", "This is particularly important in the case of larger collections with a much bigger number of unique words for which the lexical based model is not a viable option .", "Our results seem to indicate that we do not lose much in terms of accuracy using the more compact MeSH representation .", "We have also shown how MeSH besed models out perform a lexical based approach when the number of training points is small and when the test set consists of words unseen in the training data .", "This indicates that the MeSH models can generalize successfully over unseen words .", "Our approach handles mixed class relations naturally .", "For the mixed class Defect in Location , the algorithm achieved an accuracy around 95 for both Defect and Location simultaneously .", "Our results also indicate that the second noun the head is more important in determining the relationships than the first one .", "In future we plan to train the algorithm to allow different levels for each noun in the compound .", "We also plan to compare the results to the tree cut algorithm reported in Li and Abe , 1998 , which allows different levels to be identified for different subtrees .", "We also plan to tackle the problem of noun compounds containing more than two terms .", "We would like to thank Nu Lai for help with the classification of the noun compound relations .", "This work was supported in part by NSF award number IIS 9817353 ."], "summary_lines": ["Classifying The Semantic Relations In Noun Compounds Via A Domain-Specific Lexical Hierarchy\n", "We are developing corpus-based techniques for identifying semantic relations at an intermediate level of description (more specific than those used in case frames, but more general than those used in traditional knowledge representation systems).\n", "In this paper we describe a classification algorithm for identifying relationships between two-word noun compounds.\n", "We find that a very simple approach using a machine learning algorithm and a domain-specific lexical hierarchy successfully generalizes from training instances, performing better on previously unseen words than a baseline consisting of training on the words themselves.\n", "We classify noun compounds from the domain of medicine using 13 classes that describe the semantic relation between the head noun and the modifier in a given noun compound.\n", "We use a discriminative classifier to assing 18 relations for noun compounds from biomedical text and achieve 60% accuracy.\n"]}
{"article_lines": ["GATE A Framework And Graphical Development Environment For Robust NLP Tools And Applications", "peg , . pyscropyrssexa pa . . . Inn bee Esrey , we6onee nocnemere seressartba 43csrassartsmeuroto ssoprbonourseLproronparserrna flaw sror spe6yer , r1 Transducerloacleo a erkirlE . FOL .", "I PD . 021 fP21111 enwialitneI r Figure 2 Unicode text in Gate2 witnessed on the software scene with the emergence of Unicode as a universal standard for representing textual data .", "GATE supports multilingual data processing using Unicode as its default text encoding .", "It also provides a means of entering text in various languages , using virtual keyboards where the language is not supported by the underlying operating platform .", "Note that although Java represents characters as Unicode , it doesn't support input in many of the languages covered by Unicode .", "Currently 28 languages are supported , and more are planned for future releases .", "Because GATE is an open architecture , new virtual keyboards can be defined by the users and added to the system as needed .", "For displaying the text , GATE relies on the rendering facilities offered by the Java implementation for the platform it runs on .", "Figure 2 gives an example of text in various languages displayed by GATE .", "The ability to handle Unicode data , along with the separation between data and implementation , allows LE systems based on GATE to be ported to new languages with no additional overhead apart from the development of the resources needed for the specific language .", "These facilities have been developed as part of the EMILLE project McEnery et al . , 2000 , which focuses on the construction a 63 million word electronic corpus of South Asian languages .", "3 Applications One of GATE's strengths is that it is flexible and robust enough to enable the development of a wide range of applications within its framework .", "In this section , we describe briefly some of the NLP applications we have developed using the GATE architecture .", "3 . 1 MUSE The MUSE system Maynard et al . , 2001 is a multi purpose Named Entity recognition system which is capable of processing texts from widely different domains and genres , thereby aiming to reduce the need for costly and time consuming adaptation of existing resources to new applications and domains .", "The system aims to identify the parameters relevant to the creation of a name recognition system across different types of variability such as changes in domain , genre and media .", "For example , less formal texts may not follow standard capitalisation , punctuation and spelling formats , which can be a problem for many generic NE systems .", "Current evaluations with this system average around 93 precision and 95 recall across a variety of text types .", "3 . 2 ACE The MUSE system has also been adapted to take part in the current ACE Automatic Content Extraction program run by NIST .", "This requires systems to perform recognition and tracking tasks of named , nominal and pronominal entities and their mentions across three types of clean news text newswire , broadcast news and newspaper and two types of degraded news text OCR output and ASR output .", "3 . 3 MUMIS The MUMIS MUltiMedia Indexing and Searching environment system uses Information Extraction components developed within GATE to produce formal annotations about essential events in football video programme material .", "This IE system comprises versions of the tokenisation , sentence detection , POS tagging , and semantic tagging modules developed as part of GATE's standard resources , but also includes morphological analysis , full syntactic parsing and discourse interpretation modules , thereby enabling the production of annotations over text encoding structural , lexical , syntactic and semantic information .", "The semantic tagging module currently achieves around 91 precision and 76 recall , a significant improvement on a baseline named entity recognition system evaluated against it .", "4 Processing Resources Provided with GATE is a set of reusable processing resources for common NLP tasks .", "None of them are definitive , and the user can replace and or extend them as necessary .", "These are packaged together to form ANNIE , A Nearly New IE system , but can also be used individually or coupled together with new modules in order to create new applications .", "For example , many other NLP tasks might require a sentence splitter and POS tagger , but would not necessarily require resources more specific to IE tasks such as a named entity transducer .", "The system is in use for a variety of IE and other tasks , sometimes in combination with other sets of application specific modules .", "ANNIE consists of the following main processing resources tokeniser , sentence splitter , POS tagger , gazetteer , finite state transducer based on GATE's built in regular expressions over annotations language Cunningham et al . , 2002 , orthomatcher and coreference resolver .", "The resources communicate via GATE's annotation API , which is a directed graph of arcs bearing arbitrary feature value data , and nodes rooting this data into document content in this case text . text into simple tokens , such as numbers , punctuation , symbols , and words of different types e . g . with an initial capital , all upper case , etc . .", "The aim is to limit the work of the tokeniser to maximise efficiency , and enable greater flexibility by placing the burden of analysis on the grammars .", "This means that the tokeniser does not need to be modified for different applications or text types . splitter a cascade of finitestate transducers which segments the text into sentences .", "This module is required for the tagger .", "Both the splitter and tagger are domainand application independent . a modified version of the Brill tagger , which produces a part of speech tag as an annotation on each word or symbol .", "Neither the splitter nor the tagger are a mandatory part of the NE system , but the annotations they produce can be used by the grammar described below , in order to increase its power and coverage . of lists such as cities , organisations , days of the week , etc .", "It not only consists of entities , but also of names of useful as typical company designators e . g .", "'Ltd , titles , etc .", "The gazetteer lists are compiled into finite state machines , which can match text tokens . tagger of handcrafted rules written in the JAPE Java Annotations Pattern Engine language Cunningham et al . , 2002 , which describe patterns to match and annotations to be created as a result .", "JAPE is a version of CPSL Common Pattern Specification Language Appelt , 1996 , which provides finite state transduction over annotations based on regular expressions .", "A JAPE grammar consists of a set of phases , each of which consists of a set of pattern action rules , and which run sequentially .", "Patterns can be specified by describing a specific text string , or annotations previously created by modules such as the tokeniser , gazetteer , or document format analysis .", "Rule prioritisation if activated prevents multiple assignment of annotations to the same text string . another optional module for the IE system .", "Its primary objective is to perform co reference , or entity tracking , by recognising relations between entities .", "It also has a secondary role in improving named entity recognition by assigning annotations to previously unclassified names , based on relations with existing entities . identity relations between entities in the text .", "For more details see Dimitrov , 2002 .", "4 . 1 Implementation The implementation of the processing resources is centred on robustness , usability and the clear distinction between declarative data representations and finite state algorithms The behaviour of all the processors is completely controlled by external resources such as grammars or rule sets , which makes them easily modifiable by users who do not need to be familiar with programming languages .", "The fact that all processing resources use finite state transducer technology makes them quite performant in terms of execution times .", "Our initial experiments show that the full named entity recognition system is capable of processing around 2 . 5KB s on a PITT 450 with 256 MB RAM independently of the size of the input file ; the processing requirement is linear in relation to the text size .", "Scalability was tested by running the ANNIE modules over a randomly chosen part of the British National Corpus 10 of all documents , which contained documents of up to 17MB in size .", "5 Language Resource Creation Since many NLP algorithms require annotated corpora for training , GATE's development environment provides easy to use and extendable facilities for text annotation .", "In order to test their usability in practice , we used these facilities to build corpora of named entity annotated texts for the MUSE , ACE , and MUMIS applications .", "The annotation can be done manually by the user or semi automatically by running some processing resources over the corpus and then correcting adding new annotations manually .", "Depending on the information that needs to be annotated , some ANNIE modules can be used or adapted to bootstrap the corpus annotation task .", "For example , users from the humanities created a gazetteer list with 18th century place names in London , which when supplied to the ANNIE gazetteer , allows the automatic annotation of place information in a large collection of 18th century court reports from the Old Bailey in London .", "Since manual annotation is a difficult and error prone task , GATE tries to make it simple to use and yet keep it flexible .", "To add a new annotation , one selects the text with the mouse e . g . , quot ; Mr . Clever quot ; and then clicks on the desired annotation type e . g . , Person , which is shown in the list of types on the right handside of the document viewer see Figure 1 .", "If however the desired annotation type does not already appear there or the user wants to associate more detailed information with the annotation not just its type , then an annotation editing dialogue can be used .", "6 Evaluation A vital part of any language engineering application is the evaluation of its performance , and a development environment for this purpose would not be complete without some mechanisms for its measurement in a large number of test cases .", "GATE contains two such mechanisms an evaluation tool AnnotationDiff which enables automated performance measurement and visualisation of the results , and a benchmarking tool , which enables the tracking of a system's progress and regression testing .", "6 . 1 The AnnotationDiff Tool Gate's AnnotationDiff tool enables two sets of annotations on a document to be compared , in order to either compare a system annotated text with a reference hand annotated text , or to compare the output of two different versions of the system or two different systems .", "For each annotation type , figures are generated for precision , recall , F measure and false positives .", "The AnnotationDiff viewer displays the two sets of annotations , marked with different colours similar to 'visual diff' implementations such as in the MKS Toolkit or TkDiff .", "Annotations in the key set have two possible colours depending on their state white for annotations which have a compatible or partially compatible annotation in the response set , and orange for annotations which are missing in the response set .", "Annotations in the response set have three possible colours green if they are compatible with the key annotation , blue if they Figure 3 Fragment of results from benchmark tool are partially compatible , and red if they are spurious .", "In the viewer , two annotations will be positioned on the same row if they are co extensive , and on different rows if not .", "6 . 2 Benchmarking tool GATE's benchmarking tool differs from the AnnotationDiff in that it enables evaluation to be carried out over a whole corpus rather than a single document .", "It also enables tracking of the system's performance over time .", "The tool requires a clean version of a corpus with no annotations and an annotated reference corpus .", "First of all , the tool is run in generation mode to produce a set of texts annotated by the system .", "These texts are stored for future use .", "The tool can then be run in three ways 1 .", "Comparing the annotated set with the reference set ; 2 .", "Comparing the annotated set with the set produced by a more recent version of the system resources the latest set ; 3 .", "Comparing the latest set with the reference set .", "In each case , performance statistics will be provided for each text in the set , and overall statistics for the entire set , in comparison with the reference set .", "In case 2 , information is also provided about whether the figures have increased or decreased in comparison with the annotated set .", "The annotated set can be updated at any time by rerunning the tool in generation mode with the latest version of the system resources .", "Furthermore , the system can be run in verbose mode , where for each figure below a certain threshold set by the user , the non coextensive annotations and their corresponding text will be displayed .", "The output of the tool is written to an HTML file in tabular form , as shown in Figure 3 .", "Current evaluations for the MUSE NE system are producing average figures of 90 95 Precision and Recall on a selection of different text types spoken transcriptions , emails etc . .", "The default ANNIE system produces figures of between 80 90 Precision and Recall on news texts .", "This figure is lower than for the MUSE system , because the resources have not been tuned to a specific text type or application , but are intended to be adapted as necessary .", "Work on resolution of anaphora is currently averaging 63 Precision and 45 Recall , although this work is still very much in progress , and we expect these figures to improve in the near future .", "7 Related Work GATE draws from a large pool of previous work on infrastructures , architectures and development environments for representing and processing language resources , corpora , and annotations .", "Due to space limitations here we will discuss only a small subset .", "For a detailed review and its use for deriving the desiderata for this architecture see Cunningham , 2000 .", "Work on standard ways to deal with XML data is relevant here , such as the LT XML work at Edinburgh Thompson and McKelvie , 1997 , as is work on managing collections of documents and their formats , e . g .", "Brugman et al . , 1998 ; Grishman , 1997 ; Zajac , 1998 .", "We have also drawn from work on representing information about text and speech , e . g .", "Brugman et al . , 1998 ; Mikheev and Finch , 1997 ; Zajac , 1998 ; Young et al . , 1999 , as well as annotation standards , such as the ATLAS project an architecture for linguistic annotation at LDC Bird et kirlactunerkterripNerl ABC19980430 . 1830 . 0858 . sgm Annotation tope .", ", GPE RecallIncreaseon Atonaltmarked iron 06371426571426571la10 type Organization 1 . 0 increaseon hurnan markedto 1 0 09444444444444444 Mug 07 , limEll . ncreaseon tom0345to 07 , 14153ING ANNOTATIONSIt To automatetetteABC Ir . NNOTATKMISinTo embroil , bath PARTIALLYCORRECT Pl4071 quot ; ATIC 15nhe automateIDA , PratotationType Annotation type Person Precision increase on human marked from 08947368421052632 lc 09444444444444444 03444444444444444 al . , 2000 .", "Our approach is also related to work on user interfaces to architectural facilities such as development environments , e . g .", "Brugman et al . , 1998 and to work on comparing different versions of information , e . g .", "Sparck Jones and Galliers , 1996 ; Paggio , 1998 .", "This work is particularly novel in that it addresses the complete range of issues in NLP application development in a flexible and extensible way , rather than focusing just on some particular aspect of the development process .", "In addition , it promotes robustness , re usability , and scalability as important principles that help with the construction of practical NLP systems .", "8 Conclusions In this paper we have described an infrastructure for language engineering software which aims to assist the develeopment of robust tools and resources for NLP .", "One future direction is the integration of processing resources which learn in the background while the user is annotating corpora in GATE's visual environment .", "Currently , statistical models can be integrated but need to be trained separately .", "We are also extending the system to handle language generation modules , in order to enable the construction of applications which require language production in addition to analysis , e . g . intelligent report generation from IE data .", "ferent components , and ensures that the component interactions satisfy the system requirements .", "As a framework , it provides a reusable design for an LE software system and a set of prefabricated software building blocks that language engineers can use , extend and customise for their specific needs .", "As a development environment , it helps its users to minimise the time they spend building new LE systems or modifying existing ones , by aiding overall development and providing a debugging mechanism for new modules .", "Because GATE has a componentbased model , this allows for easy coupling and decoupling of the processors , thereby facilitating comparison of alternative configurations of the system or different implementations of the same module e . g . , different parsers .", "The availability of tools for easy visualisation of data at each point during the development process aids immediate interpretation of the results .", "The GATE framework comprises a core library analogous to a bus backplane and a set of reusable LE modules .", "The framework implements the architecture and provides amongst other things facilities for processing and visualising resources , including representation , import and export of data .", "The reusable modules provided with the backplane are able to perform basic language processing tasks such as POS tagging and semantic tagging .", "This eliminates the need for users to keep recreating the same kind of resources , and provides a good starting point for new applications .", "The modules are described in more detail in Section 4 .", "Applications developed within GATE can be deployed outside its Graphical User Interface GUI , using programmatic access via the GATE API see http gat e . ac . uk .", "In addition , the reusable modules , the document and annotation model , and the visualisation components can all be used independently of the development environment .", "GATE components may be implemented by a variety of programming languages and databases , but in each case they are represented to the system as a Java class .", "This class may simply call the underlying program or provide an access layer to a database ; alternatively it may implement the whole component .", "In the rest of this section , we show how the GATE infrastructure takes care of the resource discovery , loading , and execution , and briefly discuss data storage and visualisation .", "The title expresses succinctly the distinction made in GATE between data , algorithms , and ways of visualising them .", "In other words , GATE components are one of three types These resources can be local to the user's machine or remote available via HTTP , and all can be extended by users without modification to GATE itself .", "One of the main advantages of separating the algorithms from the data they require is that the two can be developed independently by language engineers with different types of expertise , e . g . programmers and linguists .", "Similarly , separating data from its visualisation allows users to develop alternative visual resources , while still using a language resource provided by GATE .", "Collectively , all resources are known as CREOLE a Collection of REusable Objects for Language Engineering , and are declared in a repository XML file , which describes their name , implementing class , parameters , icons , etc .", "This repository is used by the framework to discover and load available resources .", "A parameters tag describes the parameters which each resource needs when created or executed .", "Parameters can be optional , e . g . if a document list is provided when the corpus is constructed , it will be populated automatically with these documents .", "When an application is developed within GATE's graphical environment , the user chooses which processing resources go into it e . g . tokeniser , POS tagger , in what order they will be executed , and on which data e . g . document or corpus .", "The execution parameters of each resource are also set there , e . g . a loaded document is given as a parameter to each PR .", "When the application is run , the modules will be executed in the specified order on the given document .", "The results can be viewed in the document viewer editor see Figure 1 .", "GATE supports a variety of formats including XML , RTF , HTML , SGML , email and plain text .", "In all cases , when a document is created opened in GATE , the format is analysed and converted into a single unified model of annotation .", "The annotation format is a modified form of the TIPSTER format Grishman , 1997 which has been made largely compatible with the Atlas format Bird et al . , 2000 , and uses the now standard mechanism of 'stand off markup' Thompson and McKelvie , 1997 .", "The annotations associated with each document are a structure central to GATE , because they encode the language data read and produced by each processing module .", "The GATE framework also provides persistent storage of language resources .", "It currently offers three storage mechanisms one uses relational databases e . g .", "Oracle and the other two are file based , using Java serialisation or an XML based internal format .", "GATE documents can also be exported back to their original format e . g .", "SGML XML for the British National Corpus BNC and the user can choose whether some additional annotations e . g . named entity information are added to it or not .", "To summarise , the existence of a unified data structure ensures a smooth communication between components , while the provision of import and export capabilities makes communication with the outside world simple .", "In recent years , the emphasis on multilinguality has grown , and important advances have been witnessed on the software scene with the emergence of Unicode as a universal standard for representing textual data .", "GATE supports multilingual data processing using Unicode as its default text encoding .", "It also provides a means of entering text in various languages , using virtual keyboards where the language is not supported by the underlying operating platform .", "Note that although Java represents characters as Unicode , it doesn't support input in many of the languages covered by Unicode .", "Currently 28 languages are supported , and more are planned for future releases .", "Because GATE is an open architecture , new virtual keyboards can be defined by the users and added to the system as needed .", "For displaying the text , GATE relies on the rendering facilities offered by the Java implementation for the platform it runs on .", "Figure 2 gives an example of text in various languages displayed by GATE .", "The ability to handle Unicode data , along with the separation between data and implementation , allows LE systems based on GATE to be ported to new languages with no additional overhead apart from the development of the resources needed for the specific language .", "These facilities have been developed as part of the EMILLE project McEnery et al . , 2000 , which focuses on the construction a 63 million word electronic corpus of South Asian languages .", "One of GATE's strengths is that it is flexible and robust enough to enable the development of a wide range of applications within its framework .", "In this section , we describe briefly some of the NLP applications we have developed using the GATE architecture .", "The MUSE system Maynard et al . , 2001 is a multi purpose Named Entity recognition system which is capable of processing texts from widely different domains and genres , thereby aiming to reduce the need for costly and time consuming adaptation of existing resources to new applications and domains .", "The system aims to identify the parameters relevant to the creation of a name recognition system across different types of variability such as changes in domain , genre and media .", "For example , less formal texts may not follow standard capitalisation , punctuation and spelling formats , which can be a problem for many generic NE systems .", "Current evaluations with this system average around 93 precision and 95 recall across a variety of text types .", "The MUSE system has also been adapted to take part in the current ACE Automatic Content Extraction program run by NIST .", "This requires systems to perform recognition and tracking tasks of named , nominal and pronominal entities and their mentions across three types of clean news text newswire , broadcast news and newspaper and two types of degraded news text OCR output and ASR output .", "The MUMIS MUltiMedia Indexing and Searching environment system uses Information Extraction components developed within GATE to produce formal annotations about essential events in football video programme material .", "This IE system comprises versions of the tokenisation , sentence detection , POS tagging , and semantic tagging modules developed as part of GATE's standard resources , but also includes morphological analysis , full syntactic parsing and discourse interpretation modules , thereby enabling the production of annotations over text encoding structural , lexical , syntactic and semantic information .", "The semantic tagging module currently achieves around 91 precision and 76 recall , a significant improvement on a baseline named entity recognition system evaluated against it .", "Provided with GATE is a set of reusable processing resources for common NLP tasks .", "None of them are definitive , and the user can replace and or extend them as necessary .", "These are packaged together to form ANNIE , A NearlyNew IE system , but can also be used individually or coupled together with new modules in order to create new applications .", "For example , many other NLP tasks might require a sentence splitter and POS tagger , but would not necessarily require resources more specific to IE tasks such as a named entity transducer .", "The system is in use for a variety of IE and other tasks , sometimes in combination with other sets of application specific modules .", "ANNIE consists of the following main processing resources tokeniser , sentence splitter , POS tagger , gazetteer , finite state transducer based on GATE's built in regular expressions over annotations language Cunningham et al . , 2002 , orthomatcher and coreference resolver .", "The resources communicate via GATE's annotation API , which is a directed graph of arcs bearing arbitrary feature value data , and nodes rooting this data into document content in this case text .", "The tokeniser splits text into simple tokens , such as numbers , punctuation , symbols , and words of different types e . g . with an initial capital , all upper case , etc . .", "The aim is to limit the work of the tokeniser to maximise efficiency , and enable greater flexibility by placing the burden of analysis on the grammars .", "This means that the tokeniser does not need to be modified for different applications or text types .", "The sentence splitter is a cascade of finitestate transducers which segments the text into sentences .", "This module is required for the tagger .", "Both the splitter and tagger are domainand application independent .", "The tagger is a modified version of the Brill tagger , which produces a part of speech tag as an annotation on each word or symbol .", "Neither the splitter nor the tagger are a mandatory part of the NE system , but the annotations they produce can be used by the grammar described below , in order to increase its power and coverage .", "The gazetteer consists of lists such as cities , organisations , days of the week , etc .", "It not only consists of entities , but also of names of useful indicators , such as typical company designators e . g .", "'Ltd , titles , etc .", "The gazetteer lists are compiled into finite state machines , which can match text tokens .", "The semantic tagger consists of handcrafted rules written in the JAPE Java Annotations Pattern Engine language Cunningham et al . , 2002 , which describe patterns to match and annotations to be created as a result .", "JAPE is a version of CPSL Common Pattern Specification Language Appelt , 1996 , which provides finite state transduction over annotations based on regular expressions .", "A JAPE grammar consists of a set of phases , each of which consists of a set of pattern action rules , and which run sequentially .", "Patterns can be specified by describing a specific text string , or annotations previously created by modules such as the tokeniser , gazetteer , or document format analysis .", "Rule prioritisation if activated prevents multiple assignment of annotations to the same text string .", "The orthomatcher is another optional module for the IE system .", "Its primary objective is to perform co reference , or entity tracking , by recognising relations between entities .", "It also has a secondary role in improving named entity recognition by assigning annotations to previously unclassified names , based on relations with existing entities .", "The coreferencer finds identity relations between entities in the text .", "For more details see Dimitrov , 2002 .", "The implementation of the processing resources is centred on robustness , usability and the clear distinction between declarative data representations and finite state algorithms The behaviour of all the processors is completely controlled by external resources such as grammars or rule sets , which makes them easily modifiable by users who do not need to be familiar with programming languages .", "The fact that all processing resources use finite state transducer technology makes them quite performant in terms of execution times .", "Our initial experiments show that the full named entity recognition system is capable of processing around 2 . 5KB s on a PITT 450 with 256 MB RAM independently of the size of the input file ; the processing requirement is linear in relation to the text size .", "Scalability was tested by running the ANNIE modules over a randomly chosen part of the British National Corpus 10 of all documents , which contained documents of up to 17MB in size .", "Since many NLP algorithms require annotated corpora for training , GATE's development environment provides easy to use and extendable facilities for text annotation .", "In order to test their usability in practice , we used these facilities to build corpora of named entity annotated texts for the MUSE , ACE , and MUMIS applications .", "The annotation can be done manually by the user or semi automatically by running some processing resources over the corpus and then correcting adding new annotations manually .", "Depending on the information that needs to be annotated , some ANNIE modules can be used or adapted to bootstrap the corpus annotation task .", "For example , users from the humanities created a gazetteer list with 18th century place names in London , which when supplied to the ANNIE gazetteer , allows the automatic annotation of place information in a large collection of 18th century court reports from the Old Bailey in London .", "Since manual annotation is a difficult and error prone task , GATE tries to make it simple to use and yet keep it flexible .", "To add a new annotation , one selects the text with the mouse e . g . , quot ; Mr . Clever quot ; and then clicks on the desired annotation type e . g . , Person , which is shown in the list of types on the right handside of the document viewer see Figure 1 .", "If however the desired annotation type does not already appear there or the user wants to associate more detailed information with the annotation not just its type , then an annotation editing dialogue can be used .", "A vital part of any language engineering application is the evaluation of its performance , and a development environment for this purpose would not be complete without some mechanisms for its measurement in a large number of test cases .", "GATE contains two such mechanisms an evaluation tool AnnotationDiff which enables automated performance measurement and visualisation of the results , and a benchmarking tool , which enables the tracking of a system's progress and regression testing .", "Gate's AnnotationDiff tool enables two sets of annotations on a document to be compared , in order to either compare a system annotated text with a reference hand annotated text , or to compare the output of two different versions of the system or two different systems .", "For each annotation type , figures are generated for precision , recall , F measure and false positives .", "The AnnotationDiff viewer displays the two sets of annotations , marked with different colours similar to 'visual diff' implementations such as in the MKS Toolkit or TkDiff .", "Annotations in the key set have two possible colours depending on their state white for annotations which have a compatible or partially compatible annotation in the response set , and orange for annotations which are missing in the response set .", "Annotations in the response set have three possible colours green if they are compatible with the key annotation , blue if they are partially compatible , and red if they are spurious .", "In the viewer , two annotations will be positioned on the same row if they are co extensive , and on different rows if not .", "GATE's benchmarking tool differs from the AnnotationDiff in that it enables evaluation to be carried out over a whole corpus rather than a single document .", "It also enables tracking of the system's performance over time .", "The tool requires a clean version of a corpus with no annotations and an annotated reference corpus .", "First of all , the tool is run in generation mode to produce a set of texts annotated by the system .", "These texts are stored for future use .", "The tool can then be run in three ways In each case , performance statistics will be provided for each text in the set , and overall statistics for the entire set , in comparison with the reference set .", "In case 2 , information is also provided about whether the figures have increased or decreased in comparison with the annotated set .", "The annotated set can be updated at any time by rerunning the tool in generation mode with the latest version of the system resources .", "Furthermore , the system can be run in verbose mode , where for each figure below a certain threshold set by the user , the non coextensive annotations and their corresponding text will be displayed .", "The output of the tool is written to an HTML file in tabular form , as shown in Figure 3 .", "Current evaluations for the MUSE NE system are producing average figures of 90 95 Precision and Recall on a selection of different text types spoken transcriptions , emails etc . .", "The default ANNIE system produces figures of between 80 90 Precision and Recall on news texts .", "This figure is lower than for the MUSE system , because the resources have not been tuned to a specific text type or application , but are intended to be adapted as necessary .", "Work on resolution of anaphora is currently averaging 63 Precision and 45 Recall , although this work is still very much in progress , and we expect these figures to improve in the near future .", "GATE draws from a large pool of previous work on infrastructures , architectures and development environments for representing and processing language resources , corpora , and annotations .", "Due to space limitations here we will discuss only a small subset .", "For a detailed review and its use for deriving the desiderata for this architecture see Cunningham , 2000 .", "Work on standard ways to deal with XML data is relevant here , such as the LT XML work at Edinburgh Thompson and McKelvie , 1997 , as is work on managing collections of documents and their formats , e . g .", "Brugman et al . , 1998 ; Grishman , 1997 ; Zajac , 1998 .", "We have also drawn from work on representing information about text and speech , e . g .", "Brugman et al . , 1998 ; Mikheev and Finch , 1997 ; Zajac , 1998 ; Young et al . , 1999 , as well as annotation standards , such as the ATLAS project an architecture for linguistic annotation at LDC Bird et al . , 2000 .", "Our approach is also related to work on user interfaces to architectural facilities such as development environments , e . g .", "Brugman et al . , 1998 and to work on comparing different versions of information , e . g .", "Sparck Jones and Galliers , 1996 ; Paggio , 1998 .", "This work is particularly novel in that it addresses the complete range of issues in NLP application development in a flexible and extensible way , rather than focusing just on some particular aspect of the development process .", "In addition , it promotes robustness , re usability , and scalability as important principles that help with the construction of practical NLP systems .", "In this paper we have described an infrastructure for language engineering software which aims to assist the develeopment of robust tools and resources for NLP .", "One future direction is the integration of processing resources which learn in the background while the user is annotating corpora in GATE's visual environment .", "Currently , statistical models can be integrated but need to be trained separately .", "We are also extending the system to handle language generation modules , in order to enable the construction of applications which require language production in addition to analysis , e . g . intelligent report generation from IE data ."], "summary_lines": ["GATE: A Framework And Graphical Development Environment For Robust NLP Tools And Applications\n", "In this paper we present GATE, a framework and graphical development environment which enables users to develop and deploy language engineering components and resources in a robust fashion.\n", "The GATE architecture has enabled us not only to develop a number of successful applications for various language processing tasks (such as Information Extraction), but also to build and annotate corpora and carry out evaluations on the applications generated.\n", "The framework can be used to develop applications and resources in multiple languages, based on its thorough Unicode support.\n", "We include the ANNIE IE system in the standard GATE distribution for text tokenization, sentence splitting and part-of-speech tagging.\n", "We propose mechanisms to help heterogeneous linguistic modules to communicate through a common XML interface.\n"]}
{"article_lines": ["Characterising Measures Of Lexical Distributional Similarity", "This work investigates the variation in a word ? s dis tributionally nearest neighbours with respect to the similarity measure used .", "We identify one type ofvariation as being the relative frequency of the neighbour words with respect to the frequency of the target word .", "We then demonstrate a three way connec tion between relative frequency of similar words , aconcept of distributional gnerality and the seman tic relation of hyponymy .", "Finally , we consider theimpact that this has on one application of distributional similarity methods judging the composition ality of collocations .", "Over recent years , many Natural Language Pro cessing NLP techniques have been developedthat might benefit from knowledge of distribu tionally similar words , i . e . , words that occur in similar contexts .", "For example , the sparse dataproblem can make it difficult to construct language models which predict combinations of lex ical events .", "Similarity based smoothing Brown et al , 1992 ; Dagan et al , 1999 is an intuitivelyappealing approach to this problem where prob abilities of unseen co occurrences are estimatedfrom probabilities of seen co occurrences of dis tributionally similar events . Other potential applications apply the hy pothesised relationship Harris , 1968 betweendistributional similarity and semantic similar ity ; i . e . , similarity in the meaning of words can be predicted from their distributional similarity . One advantage of automatically generated the sauruses Grefenstette , 1994 ; Lin , 1998 ; Curranand Moens , 2002 over large scale manually cre ated thesauruses such as WordNet Fellbaum , 1998 is that they might be tailored to a partic ular genre or domain . However , due to the lack of a tight defini tion for the concept of distributional similarity and the broad range of potential applications , alarge number of measures of distributional similarity have been proposed or adopted see Section 2 .", "Previous work on the evaluation of dis tributional similarity methods tends to either compare sets of distributionally similar words to a manually created semantic resource Lin , 1998 ; Curran and Moens , 2002 or be orientedtowards a particular task such as language mod elling Dagan et al , 1999 ; Lee , 1999 .", "The first approach is not ideal since it assumes that the goal of distributional similarity methods is topredict semantic similarity and that the semantic resource used is a valid gold standard .", "Further , the second approach is clearly advanta geous when one wishes to apply distributional similarity methods in a particular application area .", "However , it is not at all obvious that oneuniversally best measure exists for all applica tions Weeds and Weir , 2003 .", "Thus , applying adistributional similarity technique to a new ap plication necessitates evaluating a large number of distributional similarity measures in addition to evaluating the new model or algorithm .", "We propose a shift in focus from attemptingto discover the overall best distributional sim ilarity measure to analysing the statistical and linguistic properties of sets of distributionally similar words returned by different measures .", "This will make it possible to predict in advanceof any experimental evaluation which distributional similarity measures might be most appro priate for a particular application .", "Further , we explore a problem faced by the automatic thesaurus generation community , which is that distributional similarity methodsdo not seem to offer any obvious way to distinguish between the semantic relations of syn onymy , antonymy and hyponymy .", "Previous work on this problem Caraballo , 1999 ; Lin et al . , 2003 involves identifying specific phrasal patterns within text e . g . , ? Xs and other Ys ?", "is used as evidence that X is a hyponym of Y . Our work explores the connection between relativefrequency , distributional generality and seman tic generality with promising results .", "The rest of this paper is organised as follows . In Section 2 , we present ten distributional simi larity measures that have been proposed for use in NLP .", "In Section 3 , we analyse the variation in neighbour sets returned by these measures .", "In Section 4 , we take one fundamental statisticalproperty word frequency and analyse correla tion between this and the nearest neighbour setsgenerated .", "In Section 5 , we relate relative fre quency to a concept of distributional generalityand the semantic relation of hyponymy .", "In Sec tion 6 , we consider the effects that this has on a potential application of distributional similarity techniques , which is judging compositionality of collocations .", "In this section , we introduce some basic con cepts and then discuss the ten distributional similarity measures used in this study .", "The co occurrence types of a target word are the contexts , c , in which it occurs and these have associated frequencies which may be used to form probability estimates .", "In our work , theco occurrence types are always grammatical de pendency relations .", "For example , in Sections 3 to 5 , similarity between nouns is derived fromtheir co occurrences with verbs in the direct object position .", "In Section 6 , similarity between verbs is derived from their subjects and objects .", "The k nearest neighbours of a target word w are the k words for which similarity with w is greatest .", "Our use of the term similarity measure encompasses measures which should strictly bereferred to as distance , divergence or dissimilar ity measures .", "An increase in distance correlates with a decrease in similarity .", "However , eithertype of measure can be used to find the k near est neighbours of a target word . Table 1 lists ten distributional similarity mea sures .", "The cosine measure Salton and McGill , 1983 returns the cosine of the angle between two vectors .", "The Jensen Shannon JS divergence measure Rao , 1983 and the ? skew divergence measure Lee , 1999 are based on the Kullback Leibler KL divergence measure .", "The KL divergence , or relative entropy , D p q , between two prob ability distribution functions p and q is defined Cover and Thomas , 1991 as the ? inefficiency of assuming that the distribution is q when the true distribution is p ? D p q ? c p log p q . However , D p q ? if there are any con texts c for which p c 0 and q c 0 .", "Thus , this measure cannot be used directly on maxi mum likelihood estimate MLE probabilities . One possible solution is to use the JS diver gence measure , which measures the cost of usingthe average distribution in place of each individual distribution .", "Another is the ? skew diver gence measure , which uses the p distribution tosmooth the q distribution .", "The value of the pa rameter ? controls the extent to which the KL divergence is approximated .", "We use ? 0 . 99 since this provides a close approximation to the KL divergence and has been shown to provide good results in previous research Lee , 2001 .", "The confusion probability Sugawara et al , 1985 is an estimate of the probability that one word can be substituted for another .", "Words w1 and w2 are completely confusable if we are equally as likely to see w2 in a given context as we are to see w1 in that context .", "Jaccard ? s coefficient Salton and McGill , 1983 calculates the proportion of features be longing to either word that are shared by both words .", "In the simplest case , the features of a word are defined as the contexts in which it has been seen to occur .", "simja mi is a variant Lin , 1998 in which the features of a word are thosecontexts for which the pointwise mutual infor mation MI between the word and the context is positive , where MI can be calculated usingI c , w log P c w P c . The related Dice Coeffi cient Frakes and Baeza Yates , 1992 is omitted here since it has been shown van Rijsbergen , 1979 that Dice and Jaccard ? s Coefficients are monotonic in each other .", "Lin ? s Measure Lin , 1998 is based on his information theoretic similarity theorem , whichstates , ? the similarity between A and B is measured by the ratio between the amount of in formation needed to state the commonality of A and B and the information needed to fully describe what A and B are . ?", "The final three measures are settings in the additive MI based Co occurrence Retrieval Model AMCRM Weeds and Weir , 2003 ; Weeds , 2003 .", "We can measure the precisionand the recall of a potential neighbour ? s re trieval of the co occurrences of the target word , where the sets of required and retrieved co occurrences F w1 and F w2 respectively are those co occurrences for which MI is positive .", "Neighbours with both high precision and high recall retrieval can be obtained by computing Measure Function cosine simcm w2 , w1 ? c P c w1 . P c w2 ? ?", "c P c w1 2 ? c P c w2 2 Jens . Shan .", "distjs w2 , w1 12 D p p q2 D q p q2 where p P c w1 and q P c w2 ? skew dist ? w2 , w1 D p ? . q 1 ?", "? . p where p P c w1 and q P c w2 conf .", "prob .", "simcp w2 w1 ? c P w1 c . P w2 c . P c P w1 Jaccard ? s simja w2 , w1 F w1 ? F w2 F w1 ? F w2 where F w c P c v 0 Jacc . MI simja mi w2 , W1 F w1 ? F w2 F w1 ? F w2 where F w c I c , w 0 Lin ? s simlin w2 , w1 ? F w1 ? F w2 I c , w1 I c , w2 ? F w1 I c , w1 ? F w2 I c , w2 where F w c I c , w 0 precision simP w2 , w1 ? F w1 ? F w2 I c , w2 ? F w2 I c , w2 where F w c I c , w 0 recall simR w2 , w1 ? F w1 ? F w2 I c , w1 ? F w1 I c , w1 where F w c I c , w 0 harm .", "mean simhm w2 , w1 2 . simP w2 , w1 . simR w2 , w1 simP w2 , w1 simR w2 , w1 where F w c I c , w 0 Table 1 Ten distributional similarity measures their harmonic mean or F score .", "We have described a number of ways of calculating distributional similarity .", "We now con sider whether there is substantial variation ina word ? s distributionally nearest neighbours ac cording to the chosen measure .", "We do this by calculating the overlap between neighbour setsfor 2000 nouns generated using different mea sures from direct object data extracted from the British National Corpus BNC .", "3 . 1 Experimental set up .", "The data from which sets of nearest neighbours are derived is direct object data for 2000 nouns extracted from the BNC using a robust accurate statistical parser RASP Briscoe and Carroll , 2002 .", "For reasons of computational efficiency , we limit ourselves to 2000 nouns and directobject relation data .", "Given the goal of comparing neighbour sets generated by different mea sures , we would not expect these restrictions to affect our findings .", "The complete set of 2000 nouns WScomp is the union of two sets WShigh and WSlow for which nouns were selected on the basis of frequency WShigh contains the 1000 most frequently occurring nouns frequency 500 , and WSlow contains the nouns ranked 3001 4000 frequency ? 100 .", "By excludingmid frequency nouns , we obtain a clear sepa ration between high and low frequency nouns . The complete data set consists of 1 , 596 , 798 cooccurrence tokens distributed over 331 , 079 co occurrence types .", "From this data , we computedthe similarity between every pair of nouns according to each distributional similarity mea sure .", "We then generated ranked sets of nearest neighbours of size k 200 and where a word is excluded from being a neighbour of itself for each word and each measure . For a given word , we compute the overlap between neighbour sets using a comparison tech nique adapted from Lin 1998 .", "Given a word w , each word w ?", "in WScomp is assigned a rankscore of k ? rank if it is one of the k near est neighbours of w using measure m and zero otherwise .", "If NS w , m is the vector of such scores for word w and measure m , then theoverlap , C NS w , m1 , NS w , m2 , of two neigh bour sets is the cosine between the two vectors C NS w , m1 , NS w , m2 ? w ? rm1 w ? , w ?", "rm2 w ? , w ? k i 1 i2 The overlap score indicates the extent to which sets share members and the extent to whichthey are in the same order .", "To achieve an over lap score of 1 , the sets must contain exactly the same items in exactly the same order .", "An overlap score of 0 is obtained if the sets do not contain any common items .", "If two sets share roughly half their items and these shared items are dispersed throughout the sets in a roughlysimilar order , we would expect the overlap be tween sets to be around 0 . 5 .", "cm js ? cp ja ja mi lin cm 1 . 0 0 . 0 0 . 69 0 . 12 0 . 53 0 . 15 0 . 33 0 . 09 0 . 26 0 . 12 0 . 28 0 . 15 0 . 32 0 . 15 js 0 . 69 0 . 12 1 . 0 0 . 0 0 . 81 0 . 10 0 . 46 0 . 31 0 . 48 0 . 18 0 . 49 0 . 20 0 . 55 0 . 16 ? 0 . 53 0 . 15 0 . 81 0 . 10 1 . 0 0 . 0 0 . 61 0 . 08 0 . 4 0 . 27 0 . 39 0 . 25 0 . 48 0 . 19 cp 0 . 33 0 . 09 0 . 46 0 . 31 0 . 61 0 . 08 1 . 0 0 . 0 0 . 24 0 . 24 0 . 20 0 . 18 0 . 29 0 . 15 ja 0 . 26 0 . 12 0 . 48 0 . 18 0 . 4 0 . 27 0 . 24 0 . 24 1 . 0 0 . 0 0 . 81 0 . 08 0 . 69 0 . 09 ja mi 0 . 28 0 . 15 0 . 49 0 . 20 0 . 39 0 . 25 0 . 20 0 . 18 0 . 81 0 . 08 1 . 0 0 . 0 0 . 81 0 . 10 lin 0 . 32 0 . 15 0 . 55 0 . 16 0 . 48 0 . 19 0 . 29 0 . 15 0 . 69 0 . 09 0 . 81 0 . 10 1 . 0 0 . 0 Table 2 Cross comparison of first seven similarity measures in terms of mean overlap of neighbour sets and corresponding standard deviations .", "P R hm cm 0 . 18 0 . 10 0 . 31 0 . 13 0 . 30 0 . 14 js 0 . 19 0 . 12 0 . 55 0 . 18 0 . 51 0 . 18 ? 0 . 08 0 . 08 0 . 74 0 . 14 0 . 41 0 . 23 cp 0 . 03 0 . 04 0 . 57 0 . 10 0 . 25 0 . 18 ja 0 . 36 0 . 30 0 . 38 0 . 30 0 . 74 0 . 14 ja mi 0 . 42 0 . 30 0 . 40 0 . 31 0 . 86 0 . 07 lin 0 . 46 0 . 25 0 . 52 0 . 22 0 . 95 0 . 039 Table 3 Mean overlap scores for seven simi larity measures with precision , recall and the harmonic mean in the AMCRM .", "3 . 2 Results .", "Table 2 shows the mean overlap score between every pair of the first seven measures in Table 1 calculated over WScomp .", "Table 3 shows the mean overlap score between each of these measures and precision , recall and the harmonic mean inthe AMCRM .", "In both tables , standard devia tions are given in brackets and boldface denotes the highest levels of overlap for each measure .", "For compactness , each measure is denoted by its subscript from Table 1 .", "Although overlap between most pairs of measures is greater than expected if sets of 200 neighbours were generated randomly from WScomp in this case , average overlap would be 0 . 08 and only the overlap between the pairs ? , P and cp , P is not significantly greaterthan this at the 1 level , there are substantial differences between the neighbour sets gen erated by different measures .", "For example , for many pairs , neighbour sets do not appear to have even half their members in common .", "We have seen that there is a large variation inneighbours selected by different similarity mea sures .", "In this section , we analyse how neighboursets vary with respect to one fundamental statis tical property ? word frequency .", "To do this , we measure the bias in neighbour sets towards high frequency nouns and consider how this varies depending on whether the target noun is itself a high frequency noun or low frequency noun .", "4 . 1 Measuring bias .", "If a measure is biased towards selecting high frequency words as neighbours , then we would ex pect that neighbour sets for this measure wouldbe made up mainly of words from WShigh .", "Fur ther , the more biased the measure is , the more highly ranked these high frequency words will tend to be .", "In other words , there will be highoverlap between neighbour sets generated con sidering all 2000 nouns as potential neighbours and neighbour sets generated considering just the nouns in WShigh as potential neighbours .", "In the extreme case , where all of a noun ? s k nearestneighbours are high frequency nouns , the over lap with the high frequency noun neighbour set will be 1 and the overlap with the low frequency noun neighbour set will be 0 .", "The inverse is , ofcourse , true if a measure is biased towards se lecting low frequency words as neighbours .", "If NSwordset is the vector of neighbours and associated rank scores for a given word , w , andsimilarity measure , m , and generated considering just the words in wordset as potential neigh bours , then the overlap between two neighboursets can be computed using a cosine as be fore .", "If Chigh C NScomp , NShigh and Clow C NScomp , NSlow , then we compute the bias towards high frequency neighbours for word w us ing measure m as biashighm w Chigh Chigh Clow The value of this normalised score lies in the range 0 , 1 where 1 indicates a neighbour set completely made up of high frequency words , 0 indicates a neighbour set completely made up oflow frequency words and 0 . 5 indicates a neighbour set with no biases towards high or low fre quency words .", "This score is more informative than simply calculating the proportion of high high freq .", "low freq .", "target nouns target nouns cm 0 . 90 0 . 87 js 0 . 94 0 . 70 ? 0 . 98 0 . 90 cp 1 . 00 0 . 99 ja 0 . 99 0 . 21 ja mi 0 . 95 0 . 14 lin 0 . 85 0 . 38 P 0 . 12 0 . 04 R 0 . 99 0 . 98 hm 0 . 92 0 . 28 Table 4 Mean value of biashigh according to measure and frequency of target noun .", "and low frequency words in each neighbour set because it weights the importance of neighbours by their rank in the set .", "Thus , a large numberof high frequency words in the positions clos est to the target word is considered more biased than a large number of high frequency words distributed throughout the neighbour set .", "4 . 2 Results .", "Table 4 shows the mean value of the biashigh score for every measure calculated over the set of high frequency nouns and over the set of low frequency nouns .", "The standard deviations not shown all lie in the range 0 , 0 . 2 .", "Any deviation from 0 . 5 of greater than 0 . 0234 is significant at the 1 level .", "For all measures and both sets of target nouns , there appear to be strong tendencies toselect neighbours of particular frequencies .", "Further , there appears to be three classes of mea sures those that select high frequency nouns as neighbours regardless of the frequency of thetarget noun cm , js , ? , cp andR ; those that select low frequency nouns as neighbours regard less of the frequency of the target noun P ; and those that select nouns of a similar frequency to the target noun ja , ja mi , lin and hm . This can also be considered in terms of distri butional generality .", "By definition , recall preferswords that have occurred in more of the con texts that the target noun has , regardless of whether it occurs in other contexts as well i . e . , it prefers distributionally more general words .", "The probability of this being the case increasesas the frequency of the potential neighbour increases and so , recall tends to select high fre quency words .", "In contrast , precision prefers words that have occurred in very few contextsthat the target word has not i . e . , it prefers distributionally more specific words .", "The prob ability of this being the case increases as the frequency of the potential neighbour decreases and so , precision tends to select low frequencywords .", "The harmonic mean of precision and re call prefers words that have both high precision and high recall .", "The probability of this beingthe case is highest when the words are of sim ilar frequency and so , the harmonic mean will tend to select words of a similar frequency .", "In this section , we consider the observed fre quency effects from a semantic perspective . The concept of distributional generality in troduced in the previous section has parallels with the linguistic relation of hyponymy , where a hypernym is a semantically more general term and a hyponym is a semantically more specificterm .", "For example , animal is an indirect1 hypernym of dog and conversely dog is an indi rect hyponym of animal .", "Although one can obviously think of counter examples , we would generally expect that the more specific term dog can only be used in contexts where animal can be used and that the more general term animal might be used in all of the contexts where dogis used and possibly others .", "Thus , we might ex pect that distributional generality is correlated with semantic generality ? a word has high recall low precision retrieval of its hyponyms ? co occurrences and high precision low recall re trieval of its hypernyms ?", "co occurrences .", "Thus , if n1 and n2 are related and P n2 , n1 R n2 , n1 , we might expect that n2 is a hy ponym of n1 and vice versa .", "However , having discussed a connection between frequency and distributional generality , we might also expect to find that the frequency of the hypernymic term is greater than that of the hyponymicterm .", "In order to test these hypotheses , we ex tracted all of the possible hyponym hypernym pairs 20 , 415 pairs in total from our list of 2000 nouns using WordNet 1 . 6 .", "We then calculatedthe proportion for which the direction of the hy ponymy relation could be accurately predicted by the relative values of precision and recall andthe proportion for which the direction of the hy ponymy relation could be accurately predictedby relative frequency .", "We found that the direc tion of the hyponymy relation is correlated in the predicted direction with the precision recall 1There may be other concepts in the hypernym chain between dog and animal e . g . carnivore and mammal . values in 71 of cases and correlated in the pre dicted direction with relative frequency in 70 of cases .", "This supports the idea of a three waylinking between distributional generality , rela tive frequency and semantic generality .", "We now consider the impact that this has on a potential application of distributional similarity methods .", "In its most general sense , a collocation is a habitual or lexicalised word combination .", "How ever , some collocations such as strong tea arecompositional , i . e . , their meaning can be determined from their constituents , whereas oth ers such as hot dog are not .", "Both types areimportant in language generation since a sys tem must choose between alternatives but onlynon compositional ones are of interest in language understanding since only these colloca tions need to be listed in the dictionary .", "Baldwin et al 2003 explore empiricalmodels of compositionality for noun noun com pounds and verb particle constructions .", "Based on the observation Haspelmath , 2002 that compositional collocations tend to be hyponyms of their head constituent , they propose a model which considers the semantic similarity between a collocation and its constituent words . McCarthy et al 2003 also investigate sev eral tests for compositionality including one simplexscore based on the observation that compositional collocations tend to be similar inmeaning to their constituent parts .", "They ex tract co occurrence data for 111 phrasal verbs e . g . rip off and their simplex constituents e . g . rip from the BNC using RASP and cal culate the value of simlin between each phrasal verb and its simplex constituent .", "The test simplexscore is used to rank the phrasal verbs according to their similarity with their simplexconstituent .", "This ranking is correlated with hu man judgements of the compositionality of the phrasal verbs using Spearman ? s rank correlationcoefficient .", "The value obtained 0 . 0525 is dis appointing since it is not statistically significant the probability of this value under the null hy pothesis of ? no correlation ?", "is 0 . 3 . 2 However , Haspelmath 2002 notes that a compositional collocation is not just similar to one of its constituents ? it can be considered tobe a hyponym of its head constituent .", "For ex ample , ? strong tea ?", "is a type of ? tea ?", "and ? to2Other tests for compositionality investigated by Mc Carthy et al 2003 do much better .", "Measure rs P rs under H0 simlin 0 . 0525 0 . 2946 precision 0 . 160 0 . 0475 recall 0 . 219 0 . 0110 harmonic mean 0 . 011 0 . 4562 Table 5 Correlation with compositionality for different similarity measures rip up ?", "is a way of ? ripping ? .", "Thus , we hypothesised that a distributional measure which tends to select more generalterms as neighbours of the phrasal verb e . g . re call would do better than measures that tend to select more specific terms e . g . precision or measures that tend to select terms of a similar specificity e . g simlin or the harmonic mean of precision and recall .", "Table 5 shows the results of using different similarity measures with the simplexscore test and data of McCarthy et al 2003 .", "We now see significant correlation between compositionality judgements and distributional similarity of thephrasal verb and its head constituent .", "The cor relation using the recall measure is significant at the 5 level ; thus we can conclude that if the simplex verb has high recall retrieval of the phrasal verb ? s co occurrences , then the phrasal is likely to be compositional .", "The correlation score using the precision measure is negative since we would not expect the simplex verb to be a hyponym of the phrasal verb and thus , ifthe simplex verb does have high precision re trieval of the phrasal verb ? s co occurrences , it is less likely to be compositional .", "Finally , we obtained a very similar result 0 . 217 by ranking phrasals according to their inverse relative frequency with their simplex constituent i . e . , freq simplex freq phrasal .", "Thus , it would seem that the three way connection betweendistributional generality , hyponymy and rela tive frequency exists for verbs as well as nouns .", "We have presented an analysis of a set of dis tributional similarity measures .", "We have seen that there is a large amount of variation in the neighbours selected by different measures andtherefore the choice of measure in a given appli cation is likely to be important .", "We also identified one of the major axes ofvariation in neighbour sets as being the fre quency of the neighbours selected relative to the frequency of the target word .", "There are threemajor classes of distributional similarity mea sures which can be characterised as 1 higher frequency selecting or high recall measures ; 2 lower frequency selecting or high precision mea sures ; and 3 similar frequency selecting or high precision and recall measures .", "A word tends to have high recall similarity with its hyponyms and high precision similarity with its hypernyms .", "Further , in the majority ofcases , it tends to be more frequent than its hy ponyms and less frequent than its hypernyms . Thus , there would seem to a three way corre lation between word frequency , distributional generality and semantic generality . We have considered the impact of these observations on a technique which uses a distributional similarity measure to determine compositionality of collocations .", "We saw that in this ap plication we achieve significantly better resultsusing a measure that tends to select higher frequency words as neighbours rather than a mea sure that tends to select neighbours of a similar frequency to the target word .", "There are a variety of ways in which this workmight be extended .", "First , we could use the ob servations about distributional generality andrelative frequency to aid the process of organising distributionally similar words into hierar chies .", "Second , we could consider the impact of frequency characteristics in other applications . Third , for the general application of distribu tional similarity measures , it would be usefulto find other characteristics by which distribu tional similarity measures might be classified .", "AcknowledgementsThis work was funded by a UK EPSRC stu dentship to the first author , UK EPSRC project GR S26408 01 NatHab and UK EPSRC project GR N36494 01 RASP .", "We would liketo thank Adam Kilgarriff and Bill Keller for use ful discussions ."], "summary_lines": ["Characterising Measures Of Lexical Distributional Similarity\n", "This work investigates the variation in a word's distributionally nearest neighbours with respect to the similarity measure used.\n", "We identify one type of variation as being the relative frequency of the neighbour words with respect to the frequency of the target word.\n", "We then demonstrate a three-way connection between relative frequency of similar words, a concept of distributional gnerality and the semantic relation of hyponymy.\n", "Finally, we consider the impact that this has on one application of distributional similarity methods (judging the compositionality of collocations).\n", "Abstracting from results for concrete test sets, we try to identify statistical and linguistic properties on that the performance of similarity metrics generally depends.\n", "We also found that frequency played a large role in determining the direction of entailment, with the more general term often occurring more frequently.\n", "We analyzed the variation in a word's distribution ally nearest neighbours with respect to a variety of similarity measures.\n", "We attempted to refine the distributional similarity goal to predict whether one term is a generalization/specification of the other.\n"]}
{"article_lines": ["Edit Detection And Parsing For Transcribed Speech", "We present a simple architecture for parsing transcribed speech in which an edited word detector first removes such words from the sentence string , and then a standard statistical parser trained on transcribed speech parses the remaining words .", "The edit detector achieves a misclassification rate on edited words of 2 . 2 . which marks everything as not edited , has an error rate of 5 . 9 .", "To evaluate our parsing results we introduce a new evaluation metric , the purpose of which is to make evaluation of a parse tree relatively indifferent the exact tree position of By this metric the parser achieves 85 . 3 precision and 86 . 5 recall .", "While significant effort has been expended on the parsing of written text , parsing speech has received relatively little attention .", "The comparative neglect of speech or transcribed speech is understandable , since parsing transcribed speech presents several problems absent in regular text um s and ah s or more formally , filled pauses , frequent use of parentheticals e . g . , you know , ungrammatical constructions , and speech repairs e . g . , Why didn t he , why didn t she stay home ? .", "In this paper we present and evaluate a simple two pass architecture for handling the problems of parsing transcribed speech .", "The first pass tries to identify which of the words in the string are edited why didn t he , in the above example .", "These words are removed from the string given to the second pass , an already existing statistical parser trained on a transcribed speech This research was supported in part by NSF grant LIS SBR 9720368 and by NSF ITR grant 20100203 . corpus .", "In particular , all of the research in this paper was performed on the parsed Switchboard corpus as provided by the Linguistic Data Consortium .", "This architecture is based upon a fundamental assumption that the semantic and pragmatic content of an utterance is based solely on the unedited words in the word sequence .", "This assumption is not completely true .", "For example , Core and Schubert 8 point to counterexamples such as have the engine take the oranges to Elmira , um , I mean , take them to Corning where the antecedent of them is found in the EDITED words .", "However , we believe that the assumption is so close to true that the number of errors introduced by this assumption is small compared to the total number of errors made by the system .", "In order to evaluate the parser s output we compare it with the gold standard parse trees .", "For this purpose a very simple third pass is added to the architecture the hypothesized edited words are inserted into the parser output see Section 3 for details .", "To the degree that our fundamental assumption holds , a real application would ignore this last step .", "This architecture has several things to recommend it .", "First , it allows us to treat the editing problem as a pre process , keeping the parser unchanged .", "Second , the major clues in detecting edited words in transcribed speech seem to be relatively shallow phenomena , such as repeated word and part of speech sequences .", "The kind of information that a parser would add , e . g . , the node dominating the EDITED node , seems much less critical .", "Note that of the major problems associated with transcribed speech , we choose to deal with only one of them , speech repairs , in a special fashion .", "Our reasoning here is based upon what one might and might not expect from a secondpass statistical parser .", "For example , ungrammaticality in some sense is relative , so if the training corpus contains the same kind of ungrammatical examples as the testing corpus , one would not expect ungrammaticality itself to be a show stopper .", "Furthermore , the best statistical parsers 3 , 5 do not use grammatical rules , but rather define probability distributions over all possible rules .", "Similarly , parentheticals and filled pauses exist in the newspaper text these parsers currently handle , albeit at a much lower rate .", "Thus there is no particular reason to expect these constructions to have a major impact . 1 This leaves speech repairs as the one major phenomenon not present in written text that might pose a major problem for our parser .", "It is for that reason that we have chosen to handle it separately .", "The organization of this paper follows the architecture just described .", "Section 2 describes the first pass .", "We present therein a boosting model for learning to detect edited nodes Sections 2 . 1 2 . 2 and an evaluation of the model as a stand alone edit detector Section 2 . 3 .", "Section 3 describes the parser .", "Since the parser is that already reported in 3 , this section simply describes the parsing metrics used Section 3 . 1 , the details of the experimental setup Section 3 . 2 , and the results Section 3 . 3 .", "The Switchboard corpus annotates disfluencies such as restarts and repairs using the terminology of Shriberg 15 .", "The disfluencies include repetitions and substitutions , italicized in 1a and 1b respectively .", "Restarts and repairs are indicated by disfluency tags , and in the disfluency POS tagged Switchboard corpus , and by EDITED nodes in the tree tagged corpus .", "This section describes a procedure for automatically identifying words corrected by a restart or repair , i . e . , words that 1Indeed , 17 suggests that filled pauses tend to indicate clause boundaries , and thus may be a help in parsing . are dominated by an EDITED node in the treetagged corpus .", "This method treats the problem of identifying EDITED nodes as a word token classification problem , where each word token is classified as either edited or not .", "The classifier applies to words only ; punctuation inherits the classification of the preceding word .", "A linear classifier trained by a greedy boosting algorithm 16 is used to predict whether a word token is edited .", "Our boosting classifier is directly based on the greedy boosting algorithm described by Collins 7 .", "This paper contains important implementation details that are not repeated here .", "We chose Collins algorithm because it offers good performance and scales to hundreds of thousands of possible feature combinations .", "This section describes the kinds of linear classifiers that the boosting algorithm infers .", "Abstractly , we regard each word token as an event characterized by a finite tuple of random variables Y , X1 , . . . , Xm .", "Y is the the conditioned variable and ranges over 1 , 1 , with Y 1 indicating that the word is not edited .", "X1 , . . . , Xm are the conditioning variables ; each Xj ranges over a finite set Xj .", "For example , X1 is the orthographic form of the word and X1 is the set of all words observed in the training section of the corpus .", "Our classifiers use m 18 conditioning variables .", "The following subsection describes the conditioning variables in more detail ; they include variables indicating the POS tag of the preceding word , the tag of the following word , whether or not the word token appears in a rough copy as explained below , etc .", "The goal of the classifier is to predict the value of Y given values for X1 , . . . , Xm .", "The classifier makes its predictions based on the occurence of combinations of conditioning variable value pairs called features .", "A feature F is a set of variable value pairs Xj , xj , with xj E Xj .", "Our classifier is defined in terms of a finite number n of features F1 , . . . , Fn , where n 106 in our classifiers . 2 Each feature Fi defines an associated random boolean variable where X x takes the value 1 if X x and 0 otherwise .", "That is , Fi 1 iff Xj xj for all Xj , xj E Fi .", "Our classifier estimates a feature weight \u03b1i for each feature Fi , that is used to define the prediction variable Z The prediction made by the classifier is sign Z Z Z , i . e . , 1 or 1 depending on the sign of Z .", "Intuitively , our goal is to adjust the vector of feature weights \u03b1 \u03b11 , . . . , \u03b1n to minimize the expected misclassiication rate E sign Z Y .", "This function is difficult to minimize , so our boosting classifier minimizes the expected Boost loss E exp Y Z .", "As Singer and Schapire 16 point out , the misclassification rate is bounded above by the Boost loss , so a low value for the Boost loss implies a low misclassification rate . b Our classifier estimates the Boost loss as Et exp Y Z , where Et is the expectation on the empirical training corpus distribution .", "The feature weights are adjusted iteratively ; one weight is changed per iteration .", "The feature whose weight is to be changed is selected greedily to minimize the Boost loss using the algorithm described in 7 .", "Training continues for 25 , 000 iterations .", "After each iteration the misclassification rate on the development corpus bEd sign Z Y is estimated , where bEd is the expectation on empirical development corpus distribution .", "While each iteration lowers the Boost loss on the training corpus , a graph of the misclassification rate on the development corpus versus iteration number is a noisy U shaped curve , rising at later iterations due to overlearning .", "The value of \u03b1 returned word token in our training data .", "We developed a method for quickly identifying such extensionally equivalent feature pairs based on hashing XORed random bitmaps , and deleted all but one of each set of extensionally equivalent features we kept a feature with the smallest number of conditioning variables . by the estimator is the one that minimizes the misclassficiation rate on the development corpus ; typically the minimum is obtained after about 12 , 000 iterations , and the feature weight vector \u03b1 contains around 8000 nonzero feature weights since some weights are adjusted more than once . 3 This subsection describes the conditioning variables used in the EDITED classifier .", "Many of the variables are defined in terms of what we call a rough copy .", "Intuitively , a rough copy identifies repeated sequences of words that might be restarts or repairs .", "Punctuation is ignored for the purposes of defining a rough copy , although conditioning variables indicate whether the rough copy includes punctuation .", "A rough copy in a tagged string of words is a substring of the form \u03b11Q y\u03b12 , where The set of free inal words includes all partial words i . e . , ending in a hyphen and a small set of conjunctions , adverbs and miscellanea , such as and , or , actually , so , etc .", "The set of interregnum strings consists of a small set of expressions such as uh , you know , I guess , I mean , etc .", "We search for rough copies in each sentence starting from left to right , searching for longer copies first .", "After we find a rough copy , we restart searching for additional rough copies following the free final string of the previous copy .", "We say that a word token is in a rough copy iff it appears in either the source or the free final . 4 2 is an example of a rough copy . ish the work Table 1 lists the conditioning variables used in our classifier .", "In that table , subscript integers refer to the relative position of word tokens relative to the current word ; e . g .", "T1 is the POS tag of the following word .", "The subscript f refers to the tag of the first word of the free final match .", "If a variable is not defined for a particular word it is given the special value NULL ; e . g . , if a word is not in a rough copy then variables such as Nm , Nu , Ni , Nl , Nr and Tf all take the value NULL .", "Flags are booleanvalued variables , while numeric valued variables are bounded to a value between 0 and 4 as well as NULL , if appropriate .", "The three variables Ct , Cw and Ti are intended to help the classifier capture very short restarts or repairs that may not involve a rough copy .", "The flags Ct and Ci indicate whether the orthographic form and or tag of the next word ignoring punctuation are the same as those of the current word .", "Ti has a non NULL value only if the current word is followed by an interregnum string ; in that case Ti is the POS tag of the word following that interregnum .", "As described above , the classifier s features are sets of variable value pairs .", "Given a tuple of variables , we generate a feature for each tuple of values that the variable tuple assumes in the training data .", "In order to keep the feature set managable , the tuples of variables we consider are restricted in various ways .", "The most important of these are constraints of the form if Xj is included among feature s variables , then so is Xk .", "For example , we require that if a feature contains Pi 1 then it also contains Pi for i 0 , and we impose a similiar constraint on POS tags .", "For the purposes of this research the Switchboard corpus , as distributed by the Linguistic Data Consortium , was divided into four sections and the word immediately following the interregnum also appears in a different rough copy , then we say that the interregnum word token appears in a rough copy .", "This permits us to approximate the Switchboard annotation convention of annotating interregna as EDITED if they appear in iterated edits .", "or subcorpora .", "The training subcorpus consists of all files in the directories 2 and 3 of the parsed merged Switchboard corpus .", "Directory 4 is split into three approximately equal size sections .", "Note that the files are not consecutively numbered .", "The first of these files sw4004 . mrg to sw4153 . mrg is the testing corpus .", "All edit detection and parsing results reported herein are from this subcorpus .", "The files sw4154 . mrg to sw4483 . mrg are reserved for future use .", "The files sw4519 . mrg to sw4936 . mrg are the development corpus .", "In the complete corpus three parse trees were sufficiently ill formed in that our tree reader failed to read them .", "These trees received trivial modifications to allow them to be read , e . g . , adding the missing extra set of parentheses around the complete tree .", "We trained our classifier on the parsed data files in the training and development sections , and evaluated the classifer on the test section .", "Section 3 evaluates the parser s output in conjunction with this classifier ; this section focuses on the classifier s performance at the individual word token level .", "In our complete application , the classifier uses a bitag tagger to assign each word a POS tag .", "Like all such taggers , our tagger has a nonnegligible error rate , and these tagging could conceivably affect the performance of the classifier .", "To determine if this is the case , we report classifier performance when trained both on Gold Tags the tags assigned by the human annotators of the Switchboard corpus and on Machine Tags the tags assigned by our bitag tagger .", "We compare these results to a baseline null classifier , which never identifies a word as EDITED .", "Our basic measure of performance is the word misclassification rate see Section 2 . 1 .", "However , we also report precision and recall scores for EDITED words alone .", "All words are assigned one of the two possible labels , EDITED or not .", "However , in our evaluation we report the accuracy of only words other than punctuation and filled pauses .", "Our logic here is much the same as that in the statistical parsing community which ignores the location of punctuation for purposes of evaluation 3 , 5 , 6 on the grounds that its placement is entirely conventional .", "The same can be said for filled pauses in the switchboard corpus .", "Our results are given in Table 2 .", "They show that our classifier makes only approximately 1 3 of the misclassification errors made by the null classifier 0 . 022 vs . 0 . 059 , and that using the POS tags produced by the bitag tagger does not have much effect on the classifier s performance e . g . , EDITED recall decreases from 0 . 678 to 0 . 668 .", "We now turn to the second pass of our two pass architecture , using an off the shelf statistical parser to parse the transcribed speech after having removed the words identified as edited by the first pass .", "We first define the evaluation metric we use and then describe the results of our experiments .", "In this section we describe the metric we use to grade the parser output .", "As a first desideratum we want a metric that is a logical extension of that used to grade previous statistical parsing work .", "We have taken as our starting point what we call the relaxed labeled precision recall metric from previous research e . g .", "This metric is characterized as follows .", "For a particular test corpus let N be the total number of nonterminal and non preterminal constituents in the gold standard parses .", "Let M be the number of such constituents returned by the parser , and let C be the number of these that are correct as defined below .", "Then precision C M and recall C N .", "A constituent c is correct if there exists a constituent d in the gold standard such that In 2 and 3 above we introduce an equivalence relation r between string positions .", "We define r to be the smallest equivalence relation satisfying a r b for all pairs of string positions a and b separated solely by punctuation symbols .", "The parsing literature uses r rather than because it is felt that two constituents should be considered equal if they disagree only in the placement of , say , a comma or any other sequence of punctuation , where one constituent includes the punctuation and the other excludes it .", "Our new metric , relaxed edited labeled precision recall is identical to relaxed labeled precision recall except for two modifications .", "First , in the gold standard all non terminal subconstituents of an EDITED node are removed and the terminal constituents are made immediate children of a single EDITED node .", "Furthermore , two or more EDITED nodes with no separating non edited material between them are merged into a single EDITED node .", "We call this version a simplified gold standard parse . All precision recall measurements are taken with respected to the simplified gold standard .", "Second , we replace r with a new equivalence relation e which we define as the smallest equivalence relation containing r and satisfying begin c e end c for each EDITED node c in the gold standard parse . 6 We give a concrete example in Figure 1 .", "The first row indicates string position as usual in parsing work , position indicators are between words .", "The second row gives the words of the sentence .", "Words that are edited out have an E above them .", "The third row indicates the equivalence relation by labeling each string position with the smallest such position with which it is equivalent .", "There are two basic ideas behind this definition .", "First , we do not care where the EDITED nodes appear in the tree structure produced by the parser .", "Second , we are not interested in the fine structure of EDITED sections of the string , just the fact that they are EDITED .", "That we do care which words are EDITED comes into our figure of merit in two ways .", "First , noncontiguous EDITED nodes remain , even though their substructure does not , and thus they are counted in the precision and recall numbers .", "Secondly and probably more importantly , failure to decide on the correct positions of edited nodes can cause collateral damage to neighboring constituents by causing them to start or stop in the wrong place .", "This is particularly relevant because according to our definition , while the positions at the beginning and ending of an edit node are equivalent , the interior positions are not unless related by the punctuation rule . than the simplified gold standard .", "We rejected this because the e relation would then itself be dependent on the parser s output , a state of affairs that might allow complicated schemes to improve the parser s performance as measured by the metric .", "See Figure 1 .", "The parser described in 3 was trained on the Switchboard training corpus as specified in section 2 . 1 .", "The input to the training algorithm was the gold standard parses minus all EDITED nodes and their children .", "We tested on the Switchboard testing subcorpus again as specified in Section 2 . 1 .", "All parsing results reported herein are from all sentences of length less than or equal to 100 words and punctuation .", "When parsing the test corpus we carried out the following operations We ran the parser in three experimental situations , each using a different edit detector in step 2 .", "In the first of the experiments labeled Gold Edits the edit detector was simply the simplified gold standard itself .", "This was to see how well the parser would do it if had perfect information about the edit locations .", "In the second experiment labeled Gold Tags , the edit detector was the one described in Section 2 trained and tested on the part ofspeech tags as specified in the gold standard trees .", "Note that the parser was not given the gold standard part of speech tags .", "We were interested in contrasting the results of this experiment with that of the third experiment to gauge what improvement one could expect from using a more sophisticated tagger as input to the edit detector .", "In the third experiment Machine Tags we used the edit detector based upon the machine generated tags .", "The results of the experiments are given in Table 3 .", "The last line in the figure indicates the performance of this parser when trained and tested on Wall Street Journal text 3 .", "It is the Machine Tags results that we consider the true capability of the detector parser combination 85 . 3 precision and 86 . 5 recall .", "The general trends of Table 3 are much as one might expect .", "Parsing the Switchboard data is much easier given the correct positions of the EDITED nodes than without this information .", "The difference between the Gold tags and the Machine tags parses is small , as would be expected from the relatively small difference in the performance of the edit detector reported in Section 2 .", "This suggests that putting significant effort into a tagger for use by the edit detector is unlikely to produce much improvement .", "Also , as one might expect , parsing conversational speech is harder than Wall Street Journal text , even given the gold standard EDITED nodes .", "Probably the only aspect of the above numbers likely to raise any comment in the parsing community is the degree to which precision numbers are lower than recall .", "With the exception of the single pair reported in 3 and repeated above , no precision values in the recent statistical parsing literature 2 , 3 , 4 , 5 , 14 have ever been lower than recall values .", "Even this one exception is by only 0 . 1 and not statistically significant .", "We attribute the dominance of recall over precision primarily to the influence of edit detector mistakes .", "First , note that when given the gold standard edits the difference is quite small 0 . 3 .", "When using the edit detector edits the difference increases to 1 . 2 .", "Our best guess is that because the edit detector has high precision , and lower recall , many more words are left in the sentence to be parsed .", "Thus one finds more nonterminal constituents in the machine parses than in the gold parses and the precision is lower than the recall .", "While there is a significant body of work on finding edit positions 1 , 9 , 10 , 13 , 17 , 18 , it is difficult to make meaningful comparisons between the various research efforts as they differ in a the corpora used for training and testing , b the information available to the edit detector , and c the evaluation metrics used .", "For example , 13 uses a subsection of the ATIS corpus , takes as input the actual speech signal and thus has access to silence duration but not to words , and uses as its evaluation metric the percentage of time the program identifies the start of the interregnum see Section 2 . 2 .", "On the other hand , 9 , 10 use an internally developed corpus of sentences , work from a transcript enhanced with information from the speech signal and thus use words , but do use a metric that seems to be similar to ours .", "Undoubtedly the work closest to ours is that of Stolcke et al . 18 , which also uses the transcribed Switchboard corpus .", "However , they use information on pause length , etc . , that goes beyond the transcript .", "They categorize the transitions between words into more categories than we do .", "At first glance there might be a mapping between their six categories and our two , with three of theirs corresponding to EDITED words and three to not edited .", "If one accepts this mapping they achieve an error rate of 2 . 6 , down from their NULL rate of 4 . 5 , as contrasted with our error rate of 2 . 2 down from our NULL rate of 5 . 9 .", "The difference in NULL rates , however , raises some doubts that the numbers are truly measuring the same thing .", "There is also a small body of work on parsing disfluent sentences 8 , 11 .", "Hindle s early work 11 does not give a formal evaluation of the parser s accuracy .", "The recent work of Schubert and Core 8 does give such an evaluation , but on a different corpus from Rochester Trains project .", "Also , their parser is not statistical and returns parses on only 62 of the strings , and 32 of the strings that constitute sentences .", "Our statistical parser naturally parses all of our corpus .", "Thus it does not seem possible to make a meaningful comparison between the two systems .", "We have presented a simple architecture for parsing transcribed speech in which an edited word detector is first used to remove such words from the sentence string , and then a statistical parser trained on edited speech with the edited nodes removed is used to parse the text .", "The edit detector reduces the misclassification rate on edited words from the null model marking everything as not edited rate of 5 . 9 to 2 . 2 .", "To evaluate our parsing results we have introduced a new evaluation metric , relaxed edited labeled precision recall .", "The purpose of this metric is to make evaluation of a parse tree relatively indifferent to the exact tree position of EDITED nodes , in much the same way that the previous metric , relaxed labeled precision recall , make it indifferent to the attachment of punctuation .", "By this metric the parser achieved 85 . 3 precision and 86 . 5 recall .", "There is , of course , great room for improvement , both in stand alone edit detectors , and their combination with parsers .", "Also of interest are models that compute the joint probabilities of the edit detection and parsing decisions that is , do both in a single integrated statistical process ."], "summary_lines": ["Edit Detection And Parsing For Transcribed Speech\n", "We present a simple architecture for parsing transcribed speech in which an edited-word detector first removes such words from the sentence string, and then a standard statistical parser trained on transcribed speech parses the remaining words.\n", "The edit detector achieves a misclassification rate on edited words of 2.2%.\n", "(The NULL-model, which marks everything as not edited, has an error rate of 5.9%.)\n", "To evaluate our parsing results we introduce a new evaluation metric, the purpose of which is to make evaluation of a parse tree relatively indifferent to the exact tree position of EDITED nodes.\n", "By this metric the parser achieves 85.3% precision and 86.5% recall.\n", "Our work in statistically parsing conversational speech has examined the performance of a parser that removes edit regions in an earlier step.\n"]}
{"article_lines": ["V Measure A Conditional Entropy Based External Cluster Evaluation Measure", "We present V measure , an external entropybased cluster evaluation measure .", "V measure provides an elegant solution tomany problems that affect previously defined cluster evaluation measures includ ing 1 dependence on clustering algorithm or data set , 2 the ? problem of matching ? , where the clustering of only a portion of datapoints are evaluated and 3 accurate evaluation and combination of two desirable aspects of clustering , homogeneity and completeness .", "We compare V measure to a num ber of popular cluster evaluation measuresand demonstrate that it satisfies several desirable properties of clustering solutions , us ing simulated clustering results .", "Finally , we use V measure to evaluate two clustering tasks document clustering and pitch accent type clustering .", "Clustering techniques have been used successfully for many natural language processing tasks , such as document clustering Willett , 1988 ; Zamir and Etzioni , 1998 ; Cutting et al , 1992 ; Vempala and Wang , 2005 , word sense disambiguation Shin and Choi , 2004 , semantic role labeling Baldewein et al . , 2004 , pitch accent type disambiguation Levow , 2006 .", "They are particularly appealing for tasks in which there is an abundance of language data available , but manual annotation of this data is very resource intensive .", "Unsupervised clustering can eliminate the need for full manual annotation of the data into desired classes , but often at the cost of making evaluation of success more difficult .", "External evaluation measures for clustering can be applied when class labels for each data point in some evaluation set can be determined a priori .", "The clustering task is then to assign these data points toany number of clusters such that each cluster con tains all and only those data points that are membersof the same class Given the ground truth class la bels , it is trivial to determine whether this perfect clustering has been achieved .", "However , evaluating how far from perfect an incorrect clustering solution is a more difficult task Oakes , 1998 and proposed approaches often lack rigor Meila , 2007 .", "In this paper , we describe a new entropy based external cluster evaluation measure , V MEASURE1 , designed to address the problem of quantifying such imperfection .", "Like all external measures , V measurecompares a target clustering ? e . g . , a manually an notated representative subset of the available data ? against an automatically generated clustering to de termine now similar the two are .", "We introduce twocomplementary concepts , completeness and homo geneity , to capture desirable properties in clustering tasks .", "In Section 2 , we describe V measure and how itis calculated in terms of homogeneity and complete ness .", "We describe several popular external cluster evaluation measures and draw some comparisons to V measure in Section 3 .", "In Section 4 , we discusshow some desirable properties for clustering are satisfied by V measure vs . other measures .", "In Sec tion 5 , we present two applications of V measure , ondocument clustering and on pitch accent type clus tering .", "V measure is an entropy based measure which explicitly measures how successfully the criteria of homogeneity and completeness have been satisfied .", "Vmeasure is computed as the harmonic mean of dis tinct homogeneity and completeness scores , just as1The ? V ? stands for ? validity ? , a common term used to de scribe the goodness of a clustering solution .", "410 precision and recall are commonly combined into F measure Van Rijsbergen , 1979 .", "As F measure scores can be weighted , V measure can be weightedto favor the contributions of homogeneity or com pleteness . For the purposes of the following discussion , as sume a data set comprising N data points , and two partitions of these a set of classes , C ci i 1 , . . .", ", n and a set of clusters , K ki 1 , . . .", ", m . Let A be the contingency table produced by the clus tering algorithm representing the clustering solution , such that A aij where aij is the number of data points that are members of class ci and elements of cluster kj . To discuss cluster evaluation measures we introduce two criteria for a clustering solution homogeneity and completeness .", "A clustering result sat isfies homogeneity if all of its clusters contain only data points which are members of a single class .", "A clustering result satisfies completeness if all the data points that are members of a given class are elementsof the same cluster .", "The homogenity and completeness of a clustering solution run roughly in opposition Increasing the homogeneity of a clustering so lution often results in decreasing its completeness .", "Consider , two degenerate clustering solutions .", "In one , assigning every datapoint into a single cluster , guarantees perfect completeness ? all of the datapoints that are members of the same class are triv ially elements of the same cluster .", "However , this cluster is as unhomogeneous as possible , since allclasses are included in this single cluster .", "In another solution , assigning each data point to a dis tinct cluster guarantees perfect homogeneity ? each cluster trivially contains only members of a singleclass .", "However , in terms of completeness , this so lution scores very poorly , unless indeed each classcontains only a single member .", "We define the dis tance from a perfect clustering is measured as theweighted harmonic mean of measures of homogene ity and completeness .", "Homogeneity In order to satisfy our homogeneity criteria , a clustering must assign only those datapoints that are members of a single class to a single cluster .", "That is , the class distribution within each cluster should beskewed to a single class , that is , zero entropy .", "We de termine how close a given clustering is to this ideal by examining the conditional entropy of the class distribution given the proposed clustering .", "In the perfectly homogeneous case , this value , H C K , is 0 .", "However , in an imperfect situation , the size of this value , in bits , is dependent on the size of thedataset and the distribution of class sizes .", "There fore , instead of taking the raw conditional entropy , we normalize this value by the maximum reduction in entropy the clustering information could provide , specifically , H C .", "Note that H C K is maximal and equals H C when the clustering provides no new information ? the class distribution within each cluster is equal to the overall class distribiution .", "H C K is 0 when each cluster contains only members of a single class , a perfectly homogenous clustering .", "In the degen erate case where H C 0 , when there is only a single class , we define homogeneity to be 1 .", "For a perfectly homogenous solution , this normalization , H C K H C , equals 0 .", "Thus , to adhere to the conventionof 1 being desirable and 0 undesirable , we define ho mogeneity as h 1 if H C , K 0 1 ?", "H C K H C else 1 where H C K ? K ? k 1 C ? c 1 ack N log ack ? C c 1 ack H C ? C ? c 1 ? K k 1 ack n log ? K k 1 ack n Completeness Completeness is symmetrical to homogeneity .", "Inorder to satisfy the completeness criteria , a clustering must assign all of those datapoints that are members of a single class to a single cluster .", "To eval uate completeness , we examine the distribution of cluster assignments within each class .", "In a perfectlycomplete clustering solution , each of these distribu tions will be completely skewed to a single cluster . We can evaluate this degree of skew by calculat ing the conditional entropy of the proposed clusterdistribution given the class of the component dat apoints , H K C .", "In the perfectly complete case , H K C 0 .", "However , in the worst case scenario , 411each class is represented by every cluster with a dis tribution equal to the distribution of cluster sizes , H K C is maximal and equals H K .", "Finally , in the degenerate case where H K 0 , when there is a single cluster , we define completeness to be 1 .", "Therefore , symmetric to the calculation above , we define completeness as c 1 if H K , C 0 1 ? H K C H K else 2 where H K C ? C ? c 1 K ? k 1 ack N log ack ? K k 1 ack H K ? K ? k 1 ? C c 1 ack n log ? C c 1 ack n Based upon these calculations of homogeneity and completeness , we then calculate a clustering solution ? s V measure by computing the weighted harmonic mean of homogeneity and completeness , V ? 1 ? ? h ? c ? ? h c . Similarly to the familiar F measure , if ? is greater than 1 completeness is weighted more strongly in the calculation , if ? is less than 1 , homogeneity is weighted more strongly .", "Notice that the computations of homogeneity , completeness and V measure are completely inde pendent of the number of classes , the number ofclusters , the size of the data set and the clustering al gorithm used .", "Thus these measures can be applied toand compared across any clustering solution , regard less of the number of data points n invariance , thenumber of classes or the number of clusters .", "More over , by calculating homogeneity and completenessseparately , a more precise evaluation of the perfor mance of the clustering can be obtained .", "Clustering algorithms divide an input data set into a number of partitions , or clusters .", "For tasks wheresome target partition can be defined for testing purposes , we define a ? clustering solution ?", "as a map ping from each data point to its cluster assignments in both the target and hypothesized clustering .", "In the context of this discussion , we will refer to the target partitions , or clusters , as CLASSES , referring only to hypothesized clusters as CLUSTERS . Two commonly used external measures for as sessing clustering success are Purity and Entropy Zhao and Karypis , 2001 , defined as , Purity ? kr 1 1n maxi nir Entropy ? kr 1 nrn ?", "1log q ? q i 1 nir nr log nir nr where q is the number of classes , k the number of clusters , nr is the size of cluster r , and nir is the number of data points in class i clustered in cluster r . Both these approaches represent plausable ways to evaluate the homogeneity of a clustering solution . However , our completeness criterion is not measured at all .", "That is , they do not address the question of whether all members of a given class are in cluded in a single cluster .", "Therefore the Purity and Entropy measures are likely to improve increased Purity , decreased Entropy monotonically withthe number of clusters in the result , up to a degen erate maximum where there are as many clusters as data points .", "However , clustering solutions rated high by either measure may still be far from ideal . Another frequently used external clustering evaluation measure is commonly refered to as ? cluster ing accuracy ? .", "The calculation of this accuracy isinspired by the information retrieval metric of F Measure Van Rijsbergen , 1979 .", "The formula for this clustering F measure as described in Fung et al . , 2003 is shown in Figure 3 .", "Let N be the number of data points , C the set of classes , K the set of clusters and nij be the number of members of class ci ? C that are elements of cluster kj ? K . F C , K X ci ? C ci N maxkj ? K F ci , kj 3 F ci , kj 2 ? R ci , kj ? P ci , kj R ci , kj P ci , kj R ci , kj nij ci P ci , kj nij kj Figure 1 Calculation of clustering F measure This measure has a significant advantage over Purity and Entropy , in that it does measure boththe homogeneity and the completeness of a cluster ing solution .", "Recall is calculated as the portion of items from class i that are present in cluster j , thus measuring how complete cluster j is with respect toclass i . Similarly , Precision is calculated as the por 412 Solution A Solution B F Measure 0 . 5 F Measure 0 . 5 V Measure 0 . 14 V Measure 0 . 39 Solution C Solution D F Measure 0 . 6 F Measure 0 . 6 V Measure 0 . 30 V Measure 0 . 41 Figure 2 Examples of the Problem of Matchingtion of cluster j that is a member of class i , thus mea suring how homogenous cluster j is with respect to class i . Like some other external cluster evaluation tech niques misclassification index MI Zeng et al , 2002 , H Meila and Heckerman , 2001 , L Larsenand Aone , 1999 , D van Dongen , 2000 , micro averaged precision and recall Dhillon et al , 2003 , F measure relies on a post processing step in which each cluster is assigned to a class .", "These techniques share certain problems .", "First , they calculate the goodness not only of the given clustering solution , but also of the cluster class matching .", "Therefore , in order for the goodness of two clustering solutions to be compared using one these measures , an identicalpost processing algorithm must be used .", "This problem can be trivially addressed by fixing the classcluster matching function and including it in the def inition of the measure as in H . However , a secondand more critical problem is the ? problem of matching ?", "Meila , 2007 .", "In calculating the similarity between a hypothesized clustering and a ? true ?", "cluster ing , these measures only consider the contributions from those clusters that are matched to a target class . This is a major problem , as two significantly differ ent clusterings can result in identical scores .", "In figure 2 , we present some illustrative examples of the problem of matching .", "For the purposes of thisdiscussion we will be using F Measure as the mea sure to describe the problem of matching , however , these problems affect any measure which requires a mapping from clusters to classes for evaluation . In the figures , the shaded regions represent CLUS TERS , the shapes represent CLASSES .", "In a perfect clustering , each shaded region would contain all and only the same shapes .", "The problem of matchingcan manifest itself either by not evaluating the en tire membership of a cluster , or by not evaluating every cluster .", "The former situation is presented in the figures A and B in figure 2 .", "The F Measure ofboth of these clustering solutions in 0 . 6 .", "The preci sion and recall for each class is 35 . That is , for each class , the best or ? matched ?", "cluster contains 3 of 5 elements of the class Recall and 3 of 5 elements of the cluster are members of the class Precision .", "The make up of the clusters beyond the majority class is not evaluated by F Measure .", "Solution B is a better clustering solution than solution A , in terms of both homogeneity crudely , ? each cluster contains fewer2 classes ?", "and completeness ? each class is containedin fewer clusters ? .", "Indeed , the V Measure of so lution B 0 . 387 is greater than that of solution A 0 . 135 .", "Solutions C and D represent a case in which not every cluster is considered in the evaluation of F Measure .", "In this example , the F Measure of both solutions is 0 . 5 the harmonic mean of 35 and 37 .", "The small ? unmatched ?", "clusters are not measured at allin the calculation of F Measure .", "Solution D is a bet ter clustering than solution C ? there are no incorrect clusterings of different classes in the small clusters .", "V Measure reflects this , solution C has a V measure of 0 . 30 while the V measure of solution D is 0 . 41 .", "A second class of clustering evaluation techniquesis based on a combinatorial approach which examines the number of pairs of data points that are clustered similarly in the target and hypothesized clus tering .", "That is , each pair of points can either be 1 clustered together in both clusterings N11 , 2 clustered separately in both clusterings N00 , 3 clustered together in the hypothesized but not the tar get clustering N01 or 4 clustered together in the target but not in the hypothesized clustering N10 .", "Based on these 4 values , a number of measures have been proposed , including Rand Index Rand , 1971 , 2Homogeneity is not measured by V measure as a count of the number of classes contained by a cluster but ? fewer ?", "is an acceptable way to conceptualize this criterion for the purposes of these examples .", "413 Adjusted Rand Index Hubert and Arabie , 1985 , ? statistic Hubert and Schultz , 1976 , Jaccard Mil ligan et al , 1983 , Fowlkes Mallows Fowlkes andMallows , 1983 and Mirkin Mirkin , 1996 .", "We il lustrate this class of measures with the calculation of Rand Index .", "Rand C , K N11 N00n n ? 1 2 Rand Index can be interpreted as the probability that a pair of points is clustered similarly together or separately in C and K . Meila 2007 describes a number of poten tial problems of this class of measures posed by Fowlkes and Mallows , 1983 and Wallace , 1983 .", "The most basic is that these measures tend not to vary over the interval of 0 , 1 .", "Transformations likethose applied by the adjusted Rand Index and a mi nor adjustment to the Mirkin measure see Section4 can address this problem .", "However , pair matching measures also suffer from distributional problems .", "The baseline for Fowlkes Mallows varies sig nificantly between 0 . 6 and 0 when the ratio of datapoints to clusters is greater than 3 ? thus including nearly all real world clustering problems .", "Similarly , the Adjusted Rand Index , as demonstrated using Monte Carlo simulations in Fowlkes and Mal lows , 1983 , varies from 0 . 5 to 0 . 95 .", "This variance in the measure ? s baseline prompts Meila to ask if the assumption of linearity following normalization can be maintained .", "If the behavior of the measure is so unstable before normalization can users reasonably expect stable behavior following normalization ?", "A final class of cluster evaluation measures arebased on information theory .", "These measures analyze the distribution of class and cluster member ship in order to determine how successful a givenclustering solution is or how different two parti tions of a data set are .", "We have already examined one member of this class of measures , Entropy .", "From a coding theory perspective , Entropy is theweighted average of the code lengths of each cluster .", "Our V measure is a member of this class of clustering measures .", "One significant advantage that in formation theoretic evaluation measures have is that they provide an elegant solution to the ? problem of matching ? .", "By examining the relative sizes of the classes and clusters being evaluated , these measures all evaluate the entire membership of each cluster ? not just a ? matched ?", "portion .", "Dom ? s Q0 measure Dom , 2001 uses conditional entropy , H C K to calculate the goodness of a clustering solution .", "That is , given the hypothesized partition , what is the number of bits necessary to represent the true clustering ?", "However , this term ? like the Purity andEntropy measures ? only evaluates the homogene ity of a solution .", "To measure the completeness of the hypothesized clustering , Dom includes a model cost term calculated using a coding theory argument .", "The overall clustering quality measure presented is the sum of the costs of representing the data H C K and the model .", "The motivation for this approachis an appeal to parsimony Given identical condi tional entropies , H C K , the clustering solution with the fewest clusters should be preferred .", "Dom also presents a normalized version of this term , Q2 , which has a range of 0 , 1 with greater scores being representing more preferred clusterings .", "Q0 C , K H C K 1 n K ? k 1 log h k C ? 1 C ? 1 where C is the target partition , K is the hypothe sized partition and h k is the size of cluster k . Q2 C , K 1 n ? C c 1 log h c C ? 1 C ? 1 Q0 C , K We believe that V measure provides two significantadvantages over Q0 that make it a more useful diag nostic tool .", "First , Q0 does not explicitly calculate the degree of completeness of the clustering solution .", "The cost term captures some of this information , since a partition with fewer clusters is likely to be more complete than a clustering solution with more clusters .", "However , Q0 does not explicitly address the interaction between the conditional entropy and the cost of representing the model .", "While this is an application of the minimum description length MDL principle Rissanen , 1978 ; Rissanen , 1989 , it does not provide an intuitive manner for assessingour two competing criteria of homogeneity and com pleteness .", "That is , at what point does an increase inconditional entropy homogeneity justify a reduc tion in the number of clusters completeness .", "Another information based clustering measure is variation of information V I Meila , 2007 , V I C , K H C K H K C .", "V I is presented 414 as a distance measure for comparing partitions or clusterings of the same data .", "It therefore does notdistinguish between hypothesized and target cluster ings .", "V I has a number of useful properties .", "First , it satisfies the metric axioms .", "This quality allowsusers to intuitively understand how V I values combine and relate to one another .", "Secondly , it is ? con vexly additive ? .", "That is to say , if a cluster is split , the distance from the new cluster to the original is the distance induced by the split times the size of the cluster .", "This property guarantees that all changes to the metric are ? local ? the impact of splitting ormerging clusters is limited to only those clusters in volved , and its size is relative to the size of these clusters .", "Third , VI is n invariant the number of data points in the cluster do not affect the value of the measure .", "V I depends on the relative sizes of the partitions of C and K , not on the number of points in these partitions .", "However , V I is bounded by themaximum number of clusters in C or K , k ? .", "With out manual modification however , k ?", "n , where each cluster contains only a single data point .", "Thus , while technically n invariant , the possible values of V I are heavily dependent on the number of datapoints being clustered .", "Thus , it is difficult to compare V I values across data sets and clustering algorithms without fixing k ? , as V I will vary over differ ent ranges .", "It is a trivial modification to modify V I such that it varies over 0 , 1 .", "Normalizing , V I by log n or 1 2 log k ?", "guarantee this range .", "However , Meila 2007 raises two potential problems with thismodification .", "The normalization should not be applied if data sets of different sizes are to be com pared ? it negates the n invariance of the measure . Additionally , if two authors apply the latter normal ization and do not use the same value for k ? , their results will not be comparable .", "While V I has a number of very useful distance properties when analyzing a single data set across a number of settings , it has limited utility as a general purpose clustering evaluation metric for use across disparate clusterings of disparate data sets .", "Our homogeneity h and completeness c terms both range over 0 , 1 and are completely n invariant andk ? invariant .", "Furthermore , measuring each as a ra tio of bit lengths has greater intuitive appeal than a more opportunistic normalization .", "V measure has another advantage as a clusteringevaluation measure over V I and Q0 .", "By evaluating homogeneity and completeness in a symmetrical , complementary manner , the calculation of V measure makes their relationship clearly observable . Separate analyses of homogeneity and completeness are not possible with any other cluster evalu ation measure .", "Moreover , by using the harmonic mean to combine homogeneity and completeness , V measure is unique in that it can also prioritize one criterion over another , depending on the clustering task and goals .", "Dom 2001 describes a parametric technique for generating example clustering solutions .", "He then proceeds to define five ? desirable properties ?", "that clustering accuracy measures should display , basedon the parameters used to generate the clustering solution .", "To compare V measure more directly to alter native clustering measures , we evaluate V measure and other measures against these and two additional desirable properties . The parameters used in generating a clustering so lution are as follows .", "C The number of classes ? K The number of clusters ? Knoise Number of ? noise ?", "clusters ; Knoise K ? Cnoise Number of ? noise ?", "classes ; Cnoise C ? ?", "Error probability ; ? ? 1 ? 2 ? 3 .", "? ? 1 The error mass within ? useful ?", "class cluster pairs ? ? 2 The error mass within noise clusters ? ? 3 The error mass within noise classes The construction of a clustering solution begins with a matching of ? useful ?", "clusters to ? useful ?", "classes3 .", "There are Ku K ? Knoise ? useful ?", "clusters and Cu C ? Cnoise ? useful ?", "classes .", "The claim is useful classes and clusters are matched to each other and matched pairs contain more data points than unmatched pairs .", "Probability mass of1 ? ?", "is evenly distributed across each match .", "Er ror mass of ? 1 is evenly distributed across each pair 3The operation of this matching is omitted in the interest of space .", "Interested readers should see Dom , 2001 .", "415 of non matching useful class cluster pairs .", "Noise clusters are those that contain data points equally from each cluster .", "Error mass of ? 2 is distributed across every ? noise ? cluster ? useful ? class pair .", "We extend the parameterization technique described in Dom , 2001 in with Cnoise and ? 3 .", "Noise classes are those that contain data points equally from each cluster .", "Error mass of ? 3 is distributed across every ? useful ? cluster ? noise ? class pair .", "An example so lution , along with its generating parameters is given in Figure 3 .", "C1 C2 C3 Cnoise1 K1 12 12 2 3 K2 2 2 12 3 Knoise1 4 4 4 0 Figure 3 Sample parametric clustering solution with n 60 , K 3 , Knoise 1 , C 3 , Cnoise 1 , ? 1 . 1 , ? 2 . 2 , ? 3 . 1 The desirable properties proposed by Dom aregiven as P1 P5 in Table 1 .", "We include two additional properties P6 , P7 relating the examined mea sure value to the number of ? noise ?", "classes and ? 3 .", "P1 For Ku C and ? Ku ?", "C ? Ku , ? M ? Ku 0 P2 For Ku ? C , ? M ? Ku 0 P3 ? M ? Knoise 0 , if ? 2 0 P4 ? M ? ? 1 ? 0 , with equality only if Ku 1 P5 ? M ? ? 2 ? 0 , with equality only if Knoise 0 P6 ? M ? Cnoise 0 , if ? 3 0 P7 ? M ? ? 3 ? 0 , with equality only if Cnoise 0 Table 1 Desirable Properties of a cluster evaluation measure MTo evaluate how different clustering measures satisfy each of these properties , we systematically var ied each parameter , keeping C 5 fixed .", "Ku 10 values 2 , 3 , .", ", 11 ? Knoise 7 values 0 , 1 , .", ", 6 ? Cnoise 7 values 0 , 1 , .", ", 6 ? ? 1 4 values 0 , 0 . 033 , 0 . 066 , 0 . 1 ? ? 2 4 values 0 , 0 . 066 , 0 . 133 , 0 . 2 ? ? 3 4 values 0 , 0 . 066 , 0 . 133 , 0 . 2 We evaluated the behavior of V Measure , Rand , Mirkin , Fowlkes Mallows , Gamma , Jaccard , VI , Q0 , F Measure against the desirable properties P1 P74 .", "Based on the described systematic modificationof each parameter , only V measure , VI and Q0 empirically satisfy all of P1 P7 in all experimental con ditions .", "Full results reporting how frequently each evaluated measure satisfied the properties based on these experiments can be found in table 2 . All evaluated measures satisfy P4 and P7 .", "However , Rand , Mirkin , Fowlkes Mallows , Gamma , Jac card and F Measure all fail to satisfy P3 and P6 inat least one experimental configuration .", "This indi cates that the number of ? noise ?", "classes or clusterscan be increased without reducing any of these mea sures .", "This implies a computational obliviousness topotentially significant aspects of an evaluated clus tering solution .", "In this section , we present two clustering experiments .", "We describe a document clustering experiment and evaluate its results using V measure , high lighting the interaction between homogeneity and completeness .", "Second , we present a pitch accent type clustering experiment .", "We present results fromboth of these experiments in order to show how V measure can be used to drawn comparisons across data sets .", "5 . 1 Document Clustering .", "Clustering techniques have been used widely to sort documents into topic clusters .", "We reproduce such an experiment here to demonstrate the usefulnessof V measure .", "Using a subset of the TDT 4 cor pus Strassel and Glenn , 2003 1884 English newswire and broadcast news documents manually la beled with one of 12 topics , we ran clustering experiments using k means clustering McQueen , 1967 and evaluated the results using V Measure , VI and Q0 ? those measures that satisfied the desirable properties defined in section 4 .", "The top ics and relative distributions are as follows Acts 4The inequalities in the desirable properties are inverted inthe evaluation of VI , Q0 and Mirkin as they are defined as dis tance , as opposed to similarity , measures .", "416 Property Rand Mirkin Fowlkes ? Jaccard F measure Q0 VI V Measure P1 0 . 18 0 . 22 1 . 0 1 . 0 1 . 0 1 . 0 1 . 0 1 . 0 1 . 0 P2 1 . 0 1 . 0 0 . 76 1 . 0 0 . 89 0 . 98 1 . 0 1 . 0 1 . 0 P3 0 . 0 0 . 0 0 . 30 0 . 19 0 . 21 0 . 0 1 . 0 1 . 0 1 . 0 P4 1 . 0 1 . 0 1 . 0 1 . 0 1 . 0 1 . 0 1 . 0 1 . 0 1 . 0 P5 0 . 50 0 . 57 1 . 0 1 . 0 1 . 0 1 . 0 1 . 0 1 . 0 1 . 0 P6 0 . 20 0 . 20 0 . 41 0 . 26 0 . 52 0 . 87 1 . 0 1 . 0 1 . 0 P7 1 . 0 1 . 0 1 . 0 1 . 0 1 . 0 1 . 0 1 . 0 1 . 0 1 . 0 Table 2 Rates of satisfaction of desirable propertiesof Violence War 22 . 3 , Elections 14 . 4 , Diplomatic Meetings 12 . 9 , Accidents 8 . 75 , Natural Disasters 7 . 4 , Human Interest 6 . 7 , Scan dals 6 . 5 , Legal Cases 6 . 4 , Miscellaneous 5 . 3 , Sports 4 . 7 , New Laws 3 . 2 , Science and Discovery 1 . 4 . We employed stemmed Porter , 1980 , tf idf weighted term vectors extracted for each document as the clustering space for these experiments , which yielded a very high dimension space .", "To reduce this dimensionality , we performed a simple feature selection procedure including in the feature vector only those terms that represented the highest tf idf value for at least one data point .", "This resulted in a feature vector containing 484 tf idf values for each document .", "Results from k means clustering are are shown in Figure 4 .", "0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 1 10 100 1000 3 3 . 5 4 4 . 5 5 5 . 5V m ea su re a nd Q 2 va lue s VI v al ue s number of clusters V Measure VI Q2 Figure 4 Results of document clustering measured by V Measure , VI and Q2 The first observation that can be drawn from these results is the degree to which VI is dependent on the number of clusters k .", "This dependency severelylimits the usefulness of VI it is inappropriate in selecting an appropriate parameter for k or for evaluating the distance between clustering solutions gen erated using different values of k . V measure and Q2 demonstrate similar behavior in evaluating these experimental results .", "They both reach a maximal value with 35 clusters , however , Q2shows a greater descent as the number of clusters in creases .", "We will discuss this quality in greater detail in section 5 . 2 .", "5 . 2 Pitch Accent Clustering .", "Pitch accent is how speakers of many languages make a word intonational prominent .", "In mostpitch accent languages , words can also be accented in different ways to convey different meanings Hirschberg , 2002 .", "In the ToBI labeling con ventions for Standard American English Silvermanet al , 1992 , for example , there are five different ac cent types H , L , H ! H , L H , L H .", "We extracted a number of acoustic features from accented words within the read portion of the Boston Directions Corpus BDC Nakatani et al , 1995 andexamined how well clustering in these acoustic dimensions correlates to manually annotated pitch ac cent types .", "We obtained a very skewed distribution , with a majority of H pitch accents . 5 We there fore included only a randomly selected 10 sample of H accents , providing a more even distribution of pitch accent types for clustering H 54 . 4 , L 32 . 1 , L H 26 . 5 , L H 2 . 8 , H ! H 2 . 1 . We extracted ten acoustic features from each ac cented word to serve as the clustering space for this experiment .", "Using Praat ? s Boersma , 2001 Get Pitch ac . . . function , we calculated the mean F0and ? F0 , as well as z score speaker normalized ver sions of the same .", "We included in the feature vector the relative location of the maximum pitch value inthe word as well as the distance between this max5Pitch accents containing a high tone may also be downstepped , or spoken in a compressed pitch range .", "Here we col lapsed all DOWNSTEPPED instances of each pitch accent with the corresponding non downstepped instances .", "417 imum and the point of maximum intensity .", "Finally , we calculated the raw and speaker normalized slope from the start of the word to the maximum pitch , and from the maximum pitch to the end of the word .", "Using this feature vector , we performed k meansclustering and evaluate how successfully these di mensions represent differences between pitch accenttypes .", "The resulting V measure , VI and Q0 calcula tions are shown in Figure 5 .", "0 0 . 05 0 . 1 0 . 15 0 . 2 1 10 100 1000 2 3 4 5 6 7 8V m ea su re a nd Q 2 va lue s VI v al ue s number of clusters VI V measure Q2Figure 5 Results of pitch accent clustering mea sured by V Measure , VI and Q0 In evaluating the results from these experiments , Q2 and V measure reveal considerably different behaviors .", "Q2 shows a maximum at k 10 , and de scends at k increases .", "This is an artifact of the MDLprinciple .", "Q2 makes the claim that a clustering so lution based on fewer clusters is preferable to one using more clusters , and that the balance between the number of clusters and the conditional entropy , H C K , should be measured in terms of codinglength .", "With V measure , we present a different argu ment .", "We contend that the a high value of k does notinherently reduce the goodness of a clustering solu tion .", "Using these results as an example , we find that at approximately 30 clusters an increase of clusters translates to an increase in V Measure .", "This is due to an increased homogeneity H C K H C and a relatively stable completeness H K C H K .", "That is , inclusion of more clusters leads to clusters with a more skewedwithin cluster distribution and a equivalent distribu tion of cluster memberships within classes .", "This is intuitively preferable ? one criterion is improved , the other is not reduced ? despite requiring additionalclusters .", "This is an instance in which the MDL principle limits the usefulness of Q2 .", "We again see sec tion 5 . 1 observe the close dependency of VI and k . Moreover , in considering figures 5 and 4 , simulta neously , we see considerably higher values achieved by the document clustering experiments .", "Given the na ? ? ve approaches taken in these experiments , this is expected ? and even desired ? given the previous work on these tasks document clustering has been notably more successfully applied than pitch accent clustering .", "These examples allow us to observe how transparently V measure can be used to compare the behavior across distinct data sets .", "We have presented a new external cluster evaluation measure , V measure , and compared it with existing clustering evaluation measures .", "V measure is basedupon two criteria for clustering usefulness , homogeneity and completeness , which capture a clustering solution ? s success in including all and only data points from a given class in a given cluster .", "We havealso demonstrated V measure ? s usefulness in com paring clustering success across different domainsby evaluating document and pitch accent cluster ing solutions .", "We believe that V measure addressessome of the problems that affect other cluster measures .", "1 It evaluates a clustering solution indepen dent of the clustering algorithm , size of the data set , number of classes and number of clusters .", "2 It does not require its user to map each cluster to a class . Therefore , it only evaluates the quality of the clustering , not a post hoc class cluster mapping .", "3 It eval uates the clustering of every data point , avoiding the ? problem of matching ? .", "4 By evaluating the criteria of both homogeneity and completeness , V measure is more comprehensive than those that evaluate onlyone .", "5 Moreover , by evaluating these criteria separately and explicitly , V measure can serve as an el egant diagnositic tool providing greater insight into clustering behavior .", "Acknowledgments The authors thank Kapil Thadani , Martin Jansche and Sasha Blair Goldensohn and for their feedback .", "This work was funded in part by the DARPA GALE program under a subcontract to SRI International ."], "summary_lines": ["V-Measure: A Conditional Entropy-Based External Cluster Evaluation Measure\n", "We present V-measure, an external entropy-based cluster evaluation measure.\n", "V-measure provides an elegant solution to many problems that affect previously defined cluster evaluation measures including 1) dependence on clustering algorithm or data set, 2) the problem of matching, where the clustering of only a portion of data points are evaluated and 3) accurate evaluation and combination of two desirable aspects of clustering, homogeneity and completeness.\n", "We compare V-measure to a number of popular cluster evaluation measures and demonstrate that it satisfies several desirable properties of clustering solutions, using simulated clustering results.\n", "Finally, we use V-measure to evaluate two clustering tasks: document clustering and pitch accent type clustering.\n", "F score is not suitable for comparing results with different cluster numbers.\n", "The V-measure (VM) is an information theoretic metric that reports the harmonic mean of homogeneity (each cluster should contain only instances of a single class) and completeness (all instances of a class should be members of the same cluster).\n", "A significant limitation of F-Score is that it does not evaluate the make up of clusters beyond the majority class.\n"]}
{"article_lines": ["Joshua An Open Source Toolkit for Parsing Based Machine Translation", "describe an open source toolkit for statistical machine translation .", "Joshua implements all of the algorithms required for synchronous context grammars SCFGs chart parsing , gram language model integration , beamcube pruning , and extraction .", "The toolkit also implements suffix array grammar extraction and minimum error rate training .", "It uses parallel and distributed computing techniques for scalability .", "We demonstrate that the toolkit achieves state of the art translation performance on the WMT09 French English translation task .", "Large scale parsing based statistical machine translation e . g . , Chiang 2007 , Quirk et al . 2005 , Galley et al .", "2006 , and Liu et al . 2006 has made remarkable progress in the last few years .", "However , most of the systems mentioned above employ tailor made , dedicated software that is not open source .", "This results in a high barrier to entry for other researchers , and makes experiments difficult to duplicate and compare .", "In this paper , we describe Joshua , a general purpose open source toolkit for parsing based machine translation , serving the same role as Moses Koehn et al . , 2007 does for regular phrase based machine translation .", "Our toolkit is written in Java and implements all the essential algorithms described in Chiang 2007 chart parsing , n gram language model integration , beam and cube pruning , and k best extraction .", "The toolkit also implements suffix array grammar extraction Lopez , 2007 and minimum error rate training Och , 2003 .", "Additionally , parallel and distributed computing techniques are exploited to make it scalable Li and Khudanpur , 2008b .", "We have also made great effort to ensure that our toolkit is easy to use and to extend .", "The toolkit has been used to translate roughly a million sentences in a parallel corpus for largescale discriminative training experiments Li and Khudanpur , 2008a .", "We hope the release of the toolkit will greatly contribute the progress of the syntax based machine translation research . '", "When designing our toolkit , we applied general principles of software engineering to achieve three major goals Extensibility , end to end coherence , and scalability .", "Extensibility The Joshua code is organized into separate packages for each major aspect of functionality .", "In this way it is clear which files contribute to a given functionality and researchers can focus on a single package without worrying about the rest of the system .", "Moreover , to minimize the problems of unintended interactions and unseen dependencies , which is common hinderance to extensibility in large projects , all extensible components are defined by Java interfaces .", "Where there is a clear point of departure for research , a basic implementation of each interface is provided as an abstract class to minimize the work necessary for new extensions .", "End to end Cohesion There are many components to a machine translation pipeline .", "One of the great difficulties with current MT pipelines is that these diverse components are often designed by separate groups and have different file format and interaction requirements .", "This leads to a large investment in scripts to convert formats and connect the different components , and often leads to untenable and non portable projects as well as hindering repeatability of experiments .", "To combat these issues , the Joshua toolkit integrates most critical components of the machine translation pipeline .", "Moreover , each component can be treated as a stand alone tool and does not rely on the rest of the toolkit we provide .", "Scalability Our third design goal was to ensure that the decoder is scalable to large models and data sets .", "The parsing and pruning algorithms are carefully implemented with dynamic programming strategies , and efficient data structures are used to minimize overhead .", "Other techniques contributing to scalability includes suffix array grammar extraction , parallel and distributed decoding , and bloom filter language models .", "Below we give a short description about the main functions implemented in our Joshua toolkit .", "Rather than inducing a grammar from the full parallel training data , we made use of a method proposed by Kishore Papineni personal communication to select the subset of the training data consisting of sentences useful for inducing a grammar to translate a particular test set .", "This method works as follows for the development and test sets that will be translated , every n gram up to length 10 is gathered into a map W and associated with an initial count of zero .", "Proceeding in order through the training data , for each sentence pair whose source to target length ratio is within one standard deviation of the average , if any n gram found in the source sentence is also found in W with a count of less than k , the sentence is selected .", "When a sentence is selected , the count of every n gram in W that is found in the source sentence is incremented by the number of its occurrences in the source sentence .", "For our submission , we used k 20 , which resulted in 1 . 5 million out of 23 million sentence pairs being selected for use as training data .", "There were 30 , 037 , 600 English words and 30 , 083 , 927 French words in the subsampled training corpus .", "Hierarchical phrase based translation requires a translation grammar extracted from a parallel corpus , where grammar rules include associated feature values .", "In real translation tasks , the grammars extracted from large training corpora are often far too large to fit into available memory .", "In such tasks , feature calculation is also very expensive in terms of time required ; huge sets of extracted rules must be sorted in two directions for relative frequency calculation of such features as the translation probability p f e and reverse translation probability p e f Koehn et al . , 2003 .", "Since the extraction steps must be re run if any change is made to the input training data , the time required can be a major hindrance to researchers , especially those investigating the effects of tokenization or word segmentation .", "To alleviate these issues , we extract only a subset of all available rules .", "Specifically , we follow Callison Burch et al . 2005 ; Lopez 2007 and use a source language suffix array to extract only those rules which will actually be used in translating a particular set of test sentences .", "This results in a vastly smaller rule set than techniques which extract all rules from the training set .", "The current code requires suffix array rule extraction to be run as a pre processing step to extract the rules needed to translate a particular test set .", "However , we are currently extending the decoder to directly access the suffix array .", "This will allow the decoder at runtime to efficiently extract exactly those rules needed to translate a particular sentence , without the need for a rule extraction pre processing step .", "Grammar formalism Our decoder assumes a probabilistic synchronous context free grammar SCFG .", "Currently , it only handles SCFGs of the kind extracted by Heiro Chiang , 2007 , but is easily extensible to more general SCFGs e . g . , Galley et al . , 2006 and closely related formalisms like synchronous tree substitution grammars Eisner , 2003 .", "Chart parsing Given a source sentence to decode , the decoder generates a one best or k best translations using a CKY algorithm .", "Specifically , the decoding algorithm maintains a chart , which contains an array of cells .", "Each cell in turn maintains a list of proven items .", "The parsing process starts with the axioms , and proceeds by applying the inference rules repeatedly to prove new items until proving a goal item .", "Whenever the parser proves a new item , it adds the item to the appropriate chart cell .", "The item also maintains backpointers to antecedent items , which are used for k best extraction .", "Pruning Severe pruning is needed in order to make the decoding computationally feasible for SCFGs with large target language vocabularies .", "In our decoder , we incorporate two pruning techniques beam and cube pruning Chiang , 2007 .", "Hypergraphs and k best extraction For each source language sentence , the chart parsing algorithm produces a hypergraph , which represents an exponential set of likely derivation hypotheses .", "Using the k best extraction algorithm Huang and Chiang , 2005 , we extract the k most likely derivations from the hypergraph .", "Parallel and distributed decoding We also implement parallel decoding and a distributed language model by exploiting multi core and multi processor architectures and distributed computing techniques .", "More details on these two features are provided by Li and Khudanpur 2008b .", "In addition to the distributed LM mentioned above , we implement three local n gram language models .", "Specifically , we first provide a straightforward implementation of the n gram scoring function in Java .", "This Java implementation is able to read the standard ARPA backoff n gram models , and thus the decoder can be used independently from the SRILM toolkit . 3 We also provide a native code bridge that allows the decoder to use the SRILM toolkit to read and score n grams .", "This native implementation is more scalable than the basic Java LM implementation .", "We have also implemented a Bloom Filter LM in Joshua , following Talbot and Osborne 2007 .", "Johsua s MERT module optimizes parameter weights so as to maximize performance on a development set as measuered by an automatic evaluation metric , such as Bleu .", "The optimization consists of a series of line optimizations along the dimensions corresponding to the parameters .", "The search across a dimension uses the efficient method of Och 2003 .", "Each iteration of our MERT implementation consists of multiple weight updates , each reflecting a greedy selection of the dimension giving the most gain .", "Each iteration also optimizes several random intermediate initial points in addition to the one surviving from the previous iteration , as an approximation to performing multiple random restarts .", "More details on the MERT method and the implementation can be found in Zaidan 2009 . 4", "We assembled a very large French English training corpus Callison Burch , 2009 by conducting a web crawl that targted bilingual web sites from the Canadian government , the European Union , and various international organizations like the Amnesty International and the Olympic Committee .", "The crawl gathered approximately 40 million files , consisting of over 1TB of data .", "We converted pdf , doc , html , asp , php , etc . files into text , and preserved the directory structure of the web crawl .", "We wrote set of simple heuristics to transform French URLs onto English URLs , and considered matching documents to be translations of each other .", "This yielded 2 million French documents paired with their English equivalents .", "We split the sentences and paragraphs in these documents , performed sentence aligned them using software that IBM Model 1 probabilities into account Moore , 2002 .", "We filtered and de duplcated the resulting parallel corpus .", "After discarding 630 thousand sentence pairs which had more than 100 words , our final corpus had 21 . 9 million sentence pairs with 587 , 867 , 024 English words and 714 , 137 , 609 French words .", "We distributed the corpus to the other WMT09 participants to use in addition to the Europarl v4 French English parallel corpus Koehn , 2005 , which consists of approximately 1 . 4 million sentence pairs with 39 million English words and 44 million French words .", "Our translation model was trained on these corpora using the subsampling descried in Section 2 . 1 .", "For language model training , we used the monolingual news and blog data that was assembled by the University of Edinburgh and distributed as part of WMT09 .", "This data consisted of 21 . 2 million English sentences with half a billion words .", "We used SRILM to train a 5 gram language model using a vocabulary containing the 500 , 000 most frequent words in this corpus .", "Note that we did not use the English side of the parallel corpus as language model training data .", "To tune the system parameters we used News Test Set from WMT08 Callison Burch et al . , 2008 , which consists of 2 , 051 sentence pairs with 43 thousand English words and 46 thousand French words .", "This is in domain data that was gathered from the same news sources as the WMT09 test set .", "The translation scores for four different systems are reported in Table 1 . 5 Baseline In this system , we use the GIZA toolkit Och and Ney , 2003 , a suffix array architecture Lopez , 2007 , the SRILM toolkit Stolcke , 2002 , and minimum error rate training Och , 2003 to obtain word alignments , a translation model , language models , and the optimal weights for combining these models , respectively .", "Minimum Bayes Risk Rescoring In this system , we re ranked the n best output of our baseline system using Minimum Bayes Risk Kumar and Byrne , 2004 .", "We re score the top 300 translations to minimize expected loss under the Bleu metric .", "Deterministic Annealing In this system , instead of using the regular MERT Och , 2003 whose training objective is to minimize the onebest error , we use the deterministic annealing training procedure described in Smith and Eisner 2006 , whose objective is to minimize the expected error together with the entropy regularization technique .", "Variational Decoding Statistical models in machine translation exhibit spurious ambiguity .", "That is , the probability of an output string is split among many distinct derivations e . g . , trees or segmentations .", "In principle , the goodness of a string is measured by the total probability of its many derivations .", "However , finding the best string e . g . , during decoding is then computationally intractable .", "Therefore , most systems use a simple Viterbi approximation that measures the goodness of a string using only its most probable derivation .", "Instead , we develop a variational approximation , which considers all the derivations but still allows tractable decoding .", "More details will be provided in Li et al . 2009b .", "In this system , we have used both deterministic annealing for training and variational decoding for decoding .", "We have described a scalable toolkit for parsingbased machine translation .", "It is written in Java and implements all the essential algorithms described in Chiang 2007 and Li and Khudanpur 2008b chart parsing , n gram language model integration , beam and cube pruning , and k best extraction .", "The toolkit also implements suffixarray grammar extraction Callison Burch et al . , 2005 ; Lopez , 2007 and minimum error rate training Och , 2003 .", "Additionally , parallel and distributed computing techniques are exploited to make it scalable .", "The decoder achieves state of the art translation performance .", "This research was supported in part by the Defense Advanced Research Projects Agency s GALE program under Contract No .", "HR0011 06 2 0001 and the National Science Foundation under grants No .", "0713448 and 0840112 .", "The views and findings are the authors alone ."], "summary_lines": ["Joshua: An Open Source Toolkit for Parsing-Based Machine Translation\n", "We describe Joshua, an open source toolkit for statistical machine translation.\n", "Joshua implements all of the algorithms required for synchronous context free grammars (SCFGs): chart-parsing, n-gram language model integration, beam- and cube-pruning, and k-best extraction.\n", "The toolkit also implements suffix-array grammar extraction and minimum error rate training.\n", "It uses parallel and distributed computing techniques for scalability.\n", "We demonstrate that the toolkit achieves state of the art translation performance on the WMT09 French-English translation task.\n", "We develop the syntax-based MT system Joshua, which implements dynamic programming algorithms for second-order expectation semi rings (Li and Eisner, 2009) to efficiently compute the gradients needed for optimization.\n"]}
{"article_lines": ["Web Scale Distributional Similarity and Entity Set Expansion", "Computing the pairwise semantic similarity between all words on the Web is a computationally challenging task .", "Parallelization and optimizations are necessary .", "We propose a highly scalable implementation based on distributional similarity , implemented in the MapReduce framework and deployed over a 200 billion word crawl of the Web .", "The pairwise similarity between 500 million terms is computed in 50 hours using 200 quad core nodes .", "We apply the learned similarity matrix to the task of automatic set expansion and present a large empirical study to quantify the effect on expansion performance of corpus size , corpus quality , seed composition and seed size .", "We make public an experimental testbed for set expansion analysis that includes a large collection of diverse entity sets extracted from Wikipedia .", "Computing the semantic similarity between terms has many applications in NLP including word classification Turney and Littman 2003 , word sense disambiguation Yuret and Yatbaz 2009 , contextspelling correction Jones and Martin 1997 , fact extraction Pa\u015fca et al . 2006 , semantic role labeling Erk 2007 , and applications in IR such as query expansion Cao et al .", "2008 and textual advertising Chang et al . 2009 .", "For commercial engines such as Yahoo ! and Google , creating lists of named entities found on the Web is critical for query analysis , document categorization , and ad matching .", "Computing term similarity is typically done by comparing cooccurrence vectors between all pairs of terms Sarmento et al . 2007 .", "Scaling this task to the Web requires parallelization and optimizations .", "In this paper , we propose a large scale term similarity algorithm , based on distributional similarity , implemented in the MapReduce framework and deployed over a 200 billion word crawl of the Web .", "The resulting similarity matrix between 500 million terms is applied to the task of expanding lists of named entities automatic set expansion .", "We provide a detailed empirical analysis of the discovered named entities and quantify the effect on expansion accuracy of corpus size , corpus quality , seed composition , and seed set size .", "Below we review relevant work in optimizing similarity computations and automatic set expansion .", "The distributional hypothesis Harris 1954 , which links the meaning of words to their contexts , has inspired many algorithms for computing term similarities Lund and Burgess 1996 ; Lin 1998 ; Lee 1999 ; Erk and Pad\u00f3 2008 ; Agirre et al . 2009 .", "Brute force similarity computation compares all the contexts for each pair of terms , with complexity O n2m where n is the number of terms and m is the number of possible contexts .", "More efficient strategies are of three kinds Smoothing Techniques such as Latent Semantic Analysis reduce the context space by applying truncated Singular Value Decomposition SVD Deerwester et al . 1990 .", "Computing the matrix decomposition however does not scale well to web size term context matrices .", "Other currently unscalable smoothing techniques include Probabilistic Latent Semantic Analysis Hofmann 1999 , Iterative Scaling Ando 2000 , and Latent Dirichlet Allocation Blei et al . 2003 .", "Randomized Algorithms Randomized techniques for approximating various similarity measures have been successfully applied to term similarity Ravichandran et al . 2005 ; Gorman and Curran 2006 .", "Common techniques include Random Indexing based on Sparse Distributed Memory Kanerva 1993 and Locality Sensitive Hashing Broder 1997 .", "Bayardo et al . 2007 present a sparse matrix optimization strategy capable of efficiently computing the similarity between terms which s similarity exceeds a given threshold .", "Rychl\u00fd and Kilgarriff 2007 , Elsayed et al . 2008 and Agirre et al .", "2009 use reverse indexing and the MapReduce framework to distribute the similarity computations across several machines .", "Our proposed approach combines these two strategies and efficiently computes the exact similarity cosine , Jaccard , Dice , and Overlap between all pairs .", "Building entity lexicons is a task of great interest for which structured , semi structured and unstructured data have all been explored GoogleSets ; Sarmento et al . 2007 ; Wang and Cohen 2007 ; Bunescu and Mooney 2004 ; Etzioni et al .", "2005 ; Pa\u015fca et al . 2006 .", "Our own work focuses on set expansion from unstructured Web text .", "Apart from the choice of a data source , state of the art entity extraction methods differ in their use of numerous , few or no labeled examples , the open or targeted nature of the extraction as well as the types of features employed .", "Supervised approaches McCallum and Li 2003 , Bunescu and Mooney 2004 rely on large sets of labeled examples , perform targeted extraction and employ a variety of sentence and corpus level features .", "While very precise , these methods are typically used for coarse grained entity classes People , Organizations , Companies for which large training data sets are available .", "Unsupervised approaches rely on no labeled data and use either bootstrapped class specific extraction patterns Etzioni et al . 2005 to find new elements of a given class for targeted extraction or corpusbased term similarity Pantel and Lin 2002 to find term clusters in an open extraction framework .", "Finally , semi supervised methods have shown great promise for identifying and labeling entities Riloff and Shepherd 1997 ; Riloff and Jones 1999 ; Banko et al . 2007 ; Downey et al .", "2007 ; Pa\u015fca et al . 2006 ; Pa\u015fca 2007a ; Pa\u015fca 2007b ; Pa\u015fca and Durme 2008 .", "Starting with a set of seed entities , semisupervised extraction methods use either classspecific patterns to populate an entity class or distributional similarity to find terms similar to the seed set Pa\u015fca s work also examines the advantages of combining these approaches .", "Semisupervised methods including ours are useful for extending finer grain entity classes , for which large unlabeled data sets are available .", "Previous work has examined the effect of using large , sometimes Web size corpora , on system performance in the case of familiar NLP tasks .", "Banko and Brill 2001 show that Web scale data helps with confusion set disambiguation while Lapata and Keller 2005 find that the Web is a good source of n gram counts for unsupervised models .", "Atterer and Schutze 2006 examine the influence of corpus size on combining a supervised approach with an unsupervised one for relative clause and PP attachment .", "Etzioni et al . 2005 and Pantel et al .", "2004 show the advantages of using large quantities of generic Web text over smaller corpora for extracting relations and named entities .", "Overall , corpus size and quality are both found to be important for extraction .", "Our paper adds to this body of work by focusing on the task of similarity based set expansion and providing a large empirical study quantify the relative corpus effects .", "Previous extraction systems report on the size and quality of the training data or , if semi supervised , the size and quality of entity or pattern seed sets .", "Narrowing the focus to closely related work , Pa\u015fca 2007a ; 2007b and Pa\u015fca and Durme 2008 show the impact of varying the number of instances representative of a given class and the size of the attribute seed set on the precision of class attribute extraction .", "An example observation is that good quality class attributes can still be extracted using 20 or even 10 instances to represent an entity class .", "Among others , Etzioni et al . 2005 shows that a small pattern set can help bootstrap useful entity seed sets and reports on the impact of seed set noise on final performance .", "Unlike previous work , empirically quantifying the influence of seed set size and quality on extraction performance of random entity types is a key objective of this paper .", "Term semantic models normally invoke the distributional hypothesis Harris 1985 , which links the meaning of terms to their contexts .", "Models are built by recording the surrounding contexts for each term in a large collection of unstructured text and storing them in a term context matrix .", "Methods differ in their definition of a context e . g . , text window or syntactic relations , or by a means to weigh contexts e . g . , frequency , tf idf , pointwise mutual information , or ultimately in measuring the similarity between two context vectors e . g . , using Euclidean distance , Cosine , Dice .", "In this paper , we adopt the following methodology for computing term similarity .", "Our various web crawls , described in Section 6 . 1 , are POStagged using Brill s tagger 1995 and chunked using a variant of the Abney chunker Abney 1991 .", "Terms are NP chunks with some modifiers removed ; their contexts i . e . , features are defined as their rightmost and leftmost stemmed chunks .", "We weigh each context f using pointwise mutual information Church and Hanks 1989 .", "Let PMI w denote a pointwise mutual information vector , constructed for each term as follows PMI w pmiw1 , pmiw2 , . . . , pmiwm , where pmiwf is the pointwise mutual information between term w and feature f where cwf is the frequency of feature f occurring for term w , n is the number of unique terms and N is the total number of features for all terms .", "Term similarities are computed by comparing these pmi context vectors using measures such as cosine , Jaccard , and Dice .", "Computing the similarity between terms on a large Web crawl is a non trivial problem , with a worst case cubic running time O n2m where n is the number of terms and m is the dimensionality of the feature space .", "Section 2 . 1 introduces several optimization techniques ; below we propose an algorithm for large scale term similarity computation which calculates exact scores for all pairs of terms , generalizes to several different metrics , and is scalable to a large crawl of the Web .", "Our optimization strategy follows a generalized sparse matrix multiplication approach Sarawagi and Kirpal 2004 , which is based on the wellknown observation that a scalar product of two vectors depends only on the coordinates for which both vectors have non zero values .", "Further , we observe that most commonly used similarity scores for feature vectors x and y , such as cosine and Dice , can be decomposed into three values one depending only on features of x , another depending only on features of y , and the third depending on the features shared both by x and y .", "More formally , commonly used similarity scores F x , y can be expressed as and f3 for some common similarity functions .", "For each of these scores , f2 f3 .", "In our work , we compute all of these scores , but report our results using only the cosine function .", "Let A and B be two matrices of PMI feature vectors .", "Our task is to compute the similarity between all vectors in A and all vectors in B .", "In computing the similarity between all pairs of terms , A B .", "Figure 1 outlines our algorithm for computing the similarity between all elements of A and B .", "Efficient computation of the similarity matrix can be achieved by leveraging the fact that is determined solely by the features shared by and i . e . , 0 for any x an d that most of the feature vectors are very sparse i . e . , most possible contexts never occur for a given term .", "In this case , calculating f1 x , y is only required when both feature vectors have a shared non zero feature , significantly reducing the cost of computation .", "Determining which vectors share a non zero feature can easily be achieved by first building an inverted index for the features .", "The computational cost of this algorithm is 2 Ni , where Ni is the number of vectors that have a non zero ith coordinate .", "Its worst case time complexity is O ncv where n is the number of terms to be compared , c is the maximum number of non zero coordinates of any vector , and v is the number of vectors that have a nonzero ith coordinate where i is the coordinate which is non zero for the most vectors .", "In other words , the algorithm is efficient only when the density of the coordinates is low .", "On our datasets , we observed near linear running time in the corpus size .", "Bayardo et al . 2007 described a strategy that potentially reduces the cost even further by omitting the coordinates with the highest number of non zero value .", "However , their algorithm gives a significant advantage only when we are interested in finding solely the similarity between highly similar terms .", "In our experiments , we compute the exact similarity between all pairs of terms .", "The pseudo code in Figure 1 assumes that A can fit into memory , which for large A may be impossible .", "Also , as each element of B is processed independently , running parallel processes for nonintersecting subsets of B makes the processing faster .", "In this section , we outline our MapReduce implementation of Figure 1 deployed using Hadoop1 , the open source software package implementing the MapReduce framework and distributed file system .", "Hadoop has been shown to scale to several thousands of machines , allowing users to write simple map and reduce code , and to seamlessly manage the sophisticated parallel execution of the code .", "A good primer on MapReduce programming is in Dean and Ghemawat 2008 .", "Our implementation employs the MapReduce model by using the Map step to start M N Map tasks in parallel , each caching 1 Mth part of A as an inverted index and streaming 1 Nth part of B through it .", "The actual inputs are read by the tasks Input Two matrices A and B of feature vectors .", "Build an inverted index for A optimiza tion for data sparseness for k in non zero features of A i if k not in AA AA k empty set append vector id , feature value pairs to the set of non zero values for feature k directly from HDFS Hadoop Distributed File System .", "Each part of A is processed N times , and each part of B is processed M times .", "M is determined by the amount of memory dedicated for the inverted index , and N should be determined by trading off the fact that as N increases , more parallelism can be obtained at the increased cost of building the same inverse index N times .", "The similarity algorithm from Figure 1 is run in each task of the Map step of a MapReduce job .", "The Reduce step is used to group the output by bi .", "Creating lists of named entities is a critical problem at commercial engines such as Yahoo ! and Google .", "The types of entities to be expanded are often not known a priori , leaving supervised classifiers undesirable .", "Additionally , list creators typically need the ability to expand sets of varying granularity .", "Semi supervised approaches are predominantly adopted since they allow targeted expansions while requiring only small sets of seed entities .", "State of the art techniques first compute term term similarities for all available terms and then select candidates for set expansion from amongst the terms most similar to the seeds Sarmento et al . 2007 .", "Formally , we define our expansion task as Task Definition Given a set of seed entities S s1 , s2 , . . . , sk of a class C s1 , s2 , . . . , sk , . . . , , sn and an unlabeled textual corpus T , find all members of the class C . For example , consider the class of Bottled Water Brands .", "Given the set of seeds S Volvic , San Pellegrino , Gerolsteiner Brunnen , Bling H2O , our task is to find all other members of this class , such as Agua Vida , Apenta , Culligan , Dasani , Ethos Water , Iceland Pure Spring Water , Imsdal , . . . Our goal is not to propose a new set expansion algorithm , but instead to test the effect of using our Web scale term similarity matrix enabled by the algorithm proposed in Section 3 on a state of theart distributional set expansion algorithm , namely Sarmento et al . 2007 .", "We consider S as a set of prototypical examples of the underlying entity set .", "A representation for the meaning of S is computed by building a feature vector consisting of a weighted average of the features of its seed elements s1 , s2 , . . . , sk , a centroid .", "For example , given the seed elements Volvic , San Pellegrino , Gerolsteiner Brunnen , Bling H2O , the resulting centroid consists of details of the feature extraction protocol are in Section 6 . 1 brand , mineral water , monitor , lake , water , take over , . . . Centroids are represented in the same space as terms allowing us to compute the similarity between centroids and all terms in our corpus .", "A scored and ranked set for expansion is ultimately generated by sorting all terms according to their similarity to the seed set centroid , and applying a cutoff on either the similarity score or on the total number of retrieved terms .", "In our reported experiments , we expanded over 22 , 000 seed sets using our Web similarity model from Section 3 .", "In this section , we describe our methodology for evaluating Web scale set expansion .", "Estimating the quality of a set expansion algorithm requires a random sample from the universe of all entity sets that may ever be expanded , where a set represents some concept such as Stage Actors .", "An approximation of this universe can be extracted from the List of pages in Wikipedia2 .", "Upon inspection of a random sample of the List of pages , we found that several lists were compositions or joins of concepts , for example List of World War II aces from Denmark and List of people who claimed to be God .", "We addressed this issue by constructing a quasi random sample as follows .", "We randomly sorted the list of every noun occurring in Wikipedia2 .", "Then , for each noun we verified whether or not it existed in a Wikipedia list , and if so we extracted this list .", "If a noun belonged to multiple lists , the authors chose the list that seemed most appropriate .", "Although this does not generate a perfect random sample , diversity is ensured by the random selection of nouns and relevancy is ensured by the author adjudication .", "The final gold standard consists of 50 sets , including classical pianists , Spanish provinces , Texas counties , male tennis players , first ladies , cocktails , bottled water brands , and Archbishops of Canterbury .", "For each set , we then manually scraped every instance from Wikipedia keeping track also of the listed variants names .", "The gold standard is available for download at http www . patrickpantel . com cgi bin Web Tools getfile . pl ? type data id ssegold wikipedia . 20071218 . goldsets . tgz The 50 sets consist on average of 208 instances with a minimum of 11 and a maximum of 1 , 116 for a total of 10 , 377 instances .", "In order to analyze the corpus and seed effects on performance , we created 30 copies of each of the 50 sets and randomly sorted each copy .", "Then , for each of the 1500 copies , we created a trial for each of the following 23 seed sizes 1 , 2 , 5 , 10 , 20 , 30 , 40 , . . . , 200 .", "Each trial of seed size s was created by taking the first s entries in each of the 1500 random copies .", "For sets that contained fewer than 200 items , we only generated trials for seed sizes smaller than the set size .", "The resulting trial dataset consists of 20 , 220 trials3 .", "Set expansion systems consist of an expansion algorithm such as the one described in Section 4 . 1 as well as a corpus such as Wikipedia , a news corpus , or a web crawl .", "For a given system , each of the 20 , 220 trials described in the previous section are expanded .", "In our work , we limited the total number of system expansions , per trial , to 1000 .", "Before judgment of an expanded set , we first collapse each instance that is a variant of another determined using the variants in our gold standard into one single instance keeping the highest system score 4 .", "Then , each expanded instance is judged as correct or incorrect automatically against the gold standard described in Section 5 . 1 .", "Our experiments in Section 6 consist of precision vs . recall or precision vs . rank curves , where a precision is defined as the percentage of correct instances in the expansion of a seed set ; and b recall is defined as the percentage of non seed gold standard instances retrieved by the system .", "Since the gold standard sets vary significantly in size , we also provide the R precision metric to normalize for set size For the above metrics , 95 confidence bounds are computed using the randomly generated samples described in Section 5 . 2 .", "Our goal is to study the performance gains on set expansion using our Web scale term similarity algorithm from Section 3 .", "We present a large empirical study quantifying the importance of corpus and seeds on expansion accuracy .", "We extracted statistics to build our model from Section 3 using four different corpora , outlined in Table 2 .", "The Wikipedia corpus consists of a snapshot of the English articles in December 20085 .", "The Web100 corpus consists of an extraction from a large crawl of the Web , from Yahoo ! , of over 600 million English webpages .", "For each crawled document , we removed paragraphs containing fewer than 50 tokens as a rough approximation of the narrative part of a webpage and then removed all duplicate sentences .", "The resulting corpus consists of over 200 billion words .", "The Web020 corpus is a random sample of 1 5th of the sentences in Web100 whereas Web004 is a random sample of 1 25th of Web100 .", "For each corpus , we tagged and chunked each sentence as described in Section 3 .", "We then computed the similarity between all noun phrase chunks using the model of Section 3 . 1 .", "Our proposed optimization for term similarity computation produces exact scores unlike randomized techniques for all pairs of terms on a large Web crawl .", "For our largest corpus , Web100 , we computed the pairwise similarity between over 500 million words in 50 hours using 200 four core machines .", "Web004 is of similar scale to the largest reported randomized technique Ravichandran et al . 2005 .", "On this scale , we compute the exact similarity matrix in a little over two hours whereas Ravichandran et al . 2005 compute an approximation in 570 hours .", "On average they only find 73 5 To avoid biasing our Wikipedia corpus with the test sets , Wikipedia List of pages were omitted from our statistics as were any page linked to gold standard list members from List of pages . of the top 1000 similar terms of a random term whereas we find all of them .", "For set expansion , experiments have been run on corpora as large as Web004 and Wikipedia Sarmento et al . 2007 , a corpora 300 times smaller than our Web crawl .", "Below , we compare the expansion accuracy of Sarmento et al . 2007 on Wikipedia and our Web crawls .", "Figure 2 illustrates the precision and recall tradeoff for our four corpora , with 95 confidence intervals computed over all 20 , 220 trials described in Section 4 . 2 .", "Table 3 lists the resulting Rprecision along with the system precisions at ranks 25 , 50 , and 100 see Figure 2 for detailed precision analysis .", "Why are the precision scores so low ?", "Compared with previous work that manually select entity types for expansion , such as countries and companies , our work is the first to evaluate over a large set of randomly selected entity types .", "On just the countries class , our R Precision was 0 . 816 using Web100 .", "The following sections analyze the effects of various expansion variables corpus size , corpus quality , seed size , and seed quality .", "Not surprisingly , corpus size and quality have a significant impact on expansion performance .", "Figure 2 and Table 3 quantify this expectation .", "On our Web crawl corpora , we observe that the full 200 billion token crawl Web100 has an average Rprecision 13 higher than 1 5th of the crawl Web020 and 53 higher than 1 25th of the crawl .", "Figure 2 also illustrates that throughout the full precision recall curve , Web100 significantly outperforms Web020 , which in turn significantly outperforms Web004 .", "The higher text quality Wikipedia corpus , which consists of roughly 60 times fewer tokens than Web020 , performs nearly as well as Web020 see Figure 2 .", "We omitted statistics from Wikipedia List of pages in order to not bias our evaluation to the test set described in Section 5 . 1 .", "Inspection of the precision vs . rank graph omitted for lack of space revealed that from rank 1 thru 550 , Wikipedia had the same precision as Web020 .", "From rank 550 to 1000 , however , Wikipedia s precision dropped off significantly compared with Web020 , accounting for the fact that the Web corpus contains a higher recall of gold standard instances .", "The R precision reported in Table 3 shows that this precision drop off results in a significantly lower R precision for Wikipedia compared with Web020 .", "Intuitively , some seeds are better than others .", "We study the impact of seed selection effect by inspecting the system performance for several randomly selected seed sets of fixed size and we find that seed set composition greatly affects performance .", "Figure 3 illustrates the precision vs . recall tradeoff on our best performing corpus Web100 for 30 random seed sets of size 10 for each of our 50 gold standard sets i . e . , 1500 trials were tested .", "Each of the trials performed better than the average system performance the double lined curve lowest in Figure 3 .", "Distinguishing between the various data series is not important , however important to notice is the very large gap between the precision recall curves of the best and worst performing random seed sets .", "On average , the best performing seed sets had 42 higher precision and 39 higher recall than the worst performing seed set .", "Similar curves were observed for inspected seed sets of size 5 , 20 , 30 , and 40 .", "Although outside of the scope of this paper , we are currently investigating ways to automatically detect which seed elements are better than others in order to reduce the impact of seed selection effect .", "Here we aim to confirm , with a large empirical study , the anecdotal claims in Pa\u015fca and Durme 2008 that few seeds are necessary .", "We found that a very small seed sets of size 1 or 2 are not sufficient for representing the intended entity set ; b 520 seeds yield on average best performance ; and c surprisingly , increasing the seed set size beyond 20 or 30 on average does not find any new correct instances .", "We inspected the effect of seed size on Rprecision over the four corpora .", "Each seed size curve is computed by averaging the system performance over the 30 random trials of all 50 sets .", "For each corpus , R precision increased sharply from seed size 1 to 10 and the curve flattened out for seed sizes larger than 20 figure omitted for lack of space .", "Error analysis on the Web100 corpus shows that once our model has seen 10 20 seeds , the distributional similarity model seems to have enough statistics to discover as many new correct instances as it could ever find .", "Some entities could never be found by the distributional similarity model since they either do not occur or infrequently occur in the corpus or they occur in contexts that vary a great deal from other set elements .", "Figure 4 illustrates this behavior by plotting for each seed set size the rate of increase in discovery of new correct instances i . e . , not found in smaller seed set sizes .", "We see that most gold standard instances are discovered with the first 5 10 seeds .", "After the 30th seed is introduced , no new correct instances are found .", "An important finding is that the error rate does not increase with increased seed set size see Figure 5 .", "This study shows that only few seeds 10 20 yield best performance and that adding more seeds beyond this does not on average affect performance in a positive or negative way .", "We proposed a highly scalable term similarity algorithm , implemented in the MapReduce framework , and deployed over a 200 billion word crawl of the Web .", "The pairwise similarity between 500 million terms was computed in 50 hours using 200 quad core nodes .", "We evaluated the impact of the large similarity matrix on a set expansion task and found that the Web similarity matrix gave a large performance boost over a state of the art expansion algorithm using Wikipedia .", "Finally , we release to the community a testbed for experimentally analyzing automatic set expansion , which includes a large collection of nearly random entity sets extracted from Wikipedia and over 22 , 000 randomly sampled seed expansion trials ."], "summary_lines": ["Web-Scale Distributional Similarity and Entity Set Expansion\n", "Computing the pairwise semantic similarity between all words on the Web is a computationally challenging task.\n", "Parallelization and optimizations are necessary.\n", "We propose a highly scalable implementation based on distributional similarity, implemented in the MapReduce framework and deployed over a 200 billion word crawl of the Web.\n", "The pairwise similarity between 500 million terms is computed in 50 hours using 200 quadcore nodes.\n", "We apply the learned similarity matrix to the task of automatic set expansion and present a large empirical study to quantify the effect on expansion performance of corpus size, corpus quality, seed composition and seed size.\n", "We make public an experimental testbed for set expansion analysis that includes a large collection of diverse entity sets extracted from Wikipedia.\n", "Our DASH stores the case for each phrase in Wikipedia.\n", "We find that 10 to 20 seeds are a sufficient starting set in a distributional similarity model to discover as many new correct instances as may ever be found.\n", "Given the seeds set S, a seeds centroid vector is produced using the surrounding word contexts of all occurrences of all the seeds in the corpus.\n"]}
{"article_lines": ["Deterministic Dependency Parsing Of English Text", "This paper presents a deterministic dependency parser based on memory based learning , which parses English text in linear time .", "When trainedand evaluated on the Wall Street Journal sec tion of the Penn Treebank , the parser achieves a maximum attachment score of 87 . 1 .", "Unlikemost previous systems , the parser produces la beled dependency graphs , using as arc labels a combination of bracket labels and grammaticalrole labels taken from the Penn Treebank II annotation scheme .", "The best overall accuracy ob tained for identifying both the correct head and the correct arc label is 86 . 0 , when restricted to grammatical role labels 7 labels , and 84 . 4 for the maximum set 50 labels .", "There has been a steadily increasing interest in syntactic parsing based on dependency analysis in re cent years .", "One important reason seems to be thatdependency parsing offers a good compromise be tween the conflicting demands of analysis depth , on the one hand , and robustness and efficiency , on the other .", "Thus , whereas a complete dependency structure provides a fully disambiguated analysisof a sentence , this analysis is typically less complex than in frameworks based on constituent analysis and can therefore often be computed determin istically with reasonable accuracy .", "Deterministicmethods for dependency parsing have now been ap plied to a variety of languages , including Japanese Kudo and Matsumoto , 2000 , English Yamada and Matsumoto , 2003 , Turkish Oflazer , 2003 , and Swedish Nivre et al , 2004 .", "For English , the interest in dependency parsing has been weaker than for other languages .", "To some extent , this can probably be explained by the strong tradition of constituent analysis in Anglo American linguistics , but this trend has been reinforced by the fact that the major treebank of American English , the Penn Treebank Marcus et al , 1993 , is anno tated primarily with constituent analysis .", "On the other hand , the best available parsers trained on thePenn Treebank , those of Collins 1997 and Charniak 2000 , use statistical models for disambigua tion that make crucial use of dependency relations .", "Moreover , the deterministic dependency parser of Yamada and Matsumoto 2003 , when trained on the Penn Treebank , gives a dependency accuracy that is almost as good as that of Collins 1997 and Charniak 2000 .", "The parser described in this paper is similar to that of Yamada and Matsumoto 2003 in that it uses a deterministic parsing algorithm in combination with a classifier induced from a treebank .", "However , there are also important differences between the twoapproaches .", "First of all , whereas Yamada and Matsumoto employs a strict bottom up algorithm es sentially shift reduce parsing with multiple passes over the input , the present parser uses the algorithmproposed in Nivre 2003 , which combines bottom up and top down processing in a single pass in order to achieve incrementality .", "This also means that the time complexity of the algorithm used here is linearin the size of the input , while the algorithm of Ya mada and Matsumoto is quadratic in the worst case .", "Another difference is that Yamada and Matsumoto use support vector machines Vapnik , 1995 , whilewe instead rely on memory based learning Daele mans , 1999 .", "Most importantly , however , the parser presented in this paper constructs labeled dependency graphs , i . e . dependency graphs where arcs are labeled with dependency types .", "As far as we know , this makesit different from all previous systems for dependency parsing applied to the Penn Treebank Eis ner , 1996 ; Yamada and Matsumoto , 2003 , althoughthere are systems that extract labeled grammatical relations based on shallow parsing , e . g . Buchholz 2002 .", "The fact that we are working with labeled dependency graphs is also one of the motivations for choosing memory based learning over sup port vector machines , since we require a multi class classifier .", "Even though it is possible to use SVMfor multi class classification , this can get cumber some when the number of classes is large .", "For the The ? DEP finger pointing ? NP SBJ has already ? ADVP begun ? VP . ? DEP Figure 1 Dependency graph for English sentenceunlabeled dependency parser of Yamada and Matsumoto 2003 the classification problem only in volves three classes .", "The parsing methodology investigated here haspreviously been applied to Swedish , where promis ing results were obtained with a relatively smalltreebank approximately 5000 sentences for train ing , resulting in an attachment score of 84 . 7 and a labeled accuracy of 80 . 6 Nivre et al , 2004 . 1 However , since there are no comparable resultsavailable for Swedish , it is difficult to assess the significance of these findings , which is one of the reasons why we want to apply the method to a bench mark corpus such as the the Penn Treebank , even though the annotation in this corpus is not ideal for labeled dependency parsing . The paper is structured as follows .", "Section 2 describes the parsing algorithm , while section 3 ex plains how memory based learning is used to guidethe parser .", "Experimental results are reported in sec tion 4 , and conclusions are stated in section 5 .", "In dependency parsing the goal of the parsing pro cess is to construct a labeled dependency graph of the kind depicted in Figure 1 .", "In formal terms , we define dependency graphs as follows 1 .", "Let R r1 , . . .", ", rm be the set of permissible .", "dependency types arc labels .", "A dependency graph for a string of words W w1 ?", "? wn is a labeled directed graph D W , A , where a W is the set of nodes , i . e . word tokens in the input string , b A is a set of labeled arcs wi , r , wj wi , wj ? W , r ? R , c for every wj ? W , there is at most one arc wi , r , wj ? A . 1The attachment score only considers whether a word is as signed the correct head ; the labeled accuracy score in additionrequires that it is assigned the correct dependency type ; cf .", "sec tion 4 .", "acyclic , projective and connected .", "For a more detailed discussion of dependency graphs and well formedness conditions , the reader is referred to Nivre 2003 . The parsing algorithm used here was first de fined for unlabeled dependency parsing in Nivre 2003 and subsequently extended to labeled graphsin Nivre et al 2004 .", "Parser configurations are rep resented by triples ? S , I , A ? , where S is the stack represented as a list , I is the list of remaining input tokens , and A is the current arc relation for the dependency graph .", "Since in a dependencygraph the set of nodes is given by the input tokens , only the arcs need to be represented explicitly .", "Given an input string W , the parser is initial ized to ? nil , W , ? ? 2 and terminates when it reaches a configuration ? S , nil , A ?", "for any list S and set ofarcs A .", "The input string W is accepted if the de pendency graph D W , A given at termination is well formed ; otherwise W is rejected .", "Given an arbitrary configuration of the parser , there are four possible transitions to the next configuration where t is the token on top of the stack , n is the next input token , w is any word , and r , r ?", "R 1 .", "Left Arc In a configuration ? t S , n I , A ? , if .", "there is no arc w , r , t ? A , extend A with n , r ? , t and pop the stack , giving the configu ration ? S , n I , A ? n , r ? , t ? .", "Right Arc In a configuration ? t S , n I , A ? , if .", "there is no arc w , r , n ? A , extend A with t , r ? , n and push n onto the stack , giving the configuration ? n t S , I , A ? t , r ? , n ? .", "is an arc w , r , t ? A , pop the stack , giving the configuration ? S , I , A ? .", "n onto the stack , giving the configuration ? n S , I , A ? .", "2We use nil to denote the empty list and a A to denote a list with head a and tail A . TH . POS ? T . DEP . . .", "TL . POS ? TL . DEP . . .", "T . POS T . LEX ? TR . DEP . . .", "TR . POS . . .", "NL . POS ? NL . DEP . . .", "N . POS N . LEX L1 . POS L2 . POS L3 . POS T Top of the stack N Next input token TL Leftmost dependent of T TR Rightmost dependent of T NL Leftmost dependent of N Li Next plus i input token X . LEX Word form of X X . POS Part of speech of X X . DEP Dependency type of X Figure 2 Parser state featuresAfter initialization , the parser is guaranteed to ter minate after at most 2n transitions , given an input string of length n Nivre , 2003 .", "Moreover , the parser always constructs a dependency graph that isacyclic and projective .", "This means that the depen dency graph given at termination is well formed if and only if it is connected Nivre , 2003 .", "Otherwise , it is a set of connected components , each of which is a well formed dependency graph for a substring of the original input . The transition system defined above is nondeterministic in itself , since several transitions can often be applied in a given configuration .", "To con struct deterministic parsers based on this system , we use classifiers trained on treebank data in or der to predict the next transition and dependency type given the current configuration of the parser .", "In this way , our approach can be seen as a form ofhistory based parsing Black et al , 1992 ; Mager man , 1995 .", "In the experiments reported here , we use memory based learning to train our classifiers .", "3 Memory Based Learning .", "Memory based learning and problem solving is based on two fundamental principles learning is thesimple storage of experiences in memory , and solv ing a new problem is achieved by reusing solutionsfrom similar previously solved problems Daele mans , 1999 .", "It is inspired by the nearest neighborapproach in statistical pattern recognition and arti ficial intelligence Fix and Hodges , 1952 , as well as the analogical modeling approach in linguistics Skousen , 1989 ; Skousen , 1992 .", "In machine learning terms , it can be characterized as a lazy learning method , since it defers processing of input un til needed and processes input by combining stored data Aha , 1997 .", "Memory based learning has been successfully applied to a number of problems in natural languageprocessing , such as grapheme to phoneme conver sion , part of speech tagging , prepositional phraseattachment , and base noun phrase chunking Daele mans et al , 2002 .", "Previous work on memory based learning for deterministic parsing includes Veenstra and Daelemans 2000 and Nivre et al 2004 .", "For the experiments reported in this paper , we have used the software package TiMBL TilburgMemory Based Learner , which provides a vari ety of metrics , algorithms , and extra functions on top of the classical k nearest neighbor classification kernel , such as value distance metrics and distance weighted class voting Daelemans et al , 2003 . The function we want to approximate is a map ping f from configurations to parser actions , where each action consists of a transition and except for Shift and Reduce a dependency type f Config ? LA , RA , RE , SH ?", "R ? nil Here Config is the set of all configurations and R is the set of dependency types .", "In order to make theproblem tractable , we approximate f with a func tion f ?", "whose domain is a finite space of parser states , which are abstractions over configurations .", "For this purpose we define a number of features that can be used to define different models of parser state .", "Figure 2 illustrates the features that are used to define parser states in the present study .", "The two central elements in any configuration are the token on top of the stack T and the next input token N , the tokens which may be connected by a de pendency arc in the next configuration .", "For these tokens , we consider both the word form T . LEX , N . LEX and the part of speech T . POS , N . POS , as assigned by an automatic part of speech tagger ina preprocessing phase .", "Next , we consider a selection of dependencies that may be present in the cur rent arc relation , namely those linking T to its head TH and its leftmost and rightmost dependent TL , TR , and that linking N to its leftmost dependent NL , 3 considering both the dependency type arclabel and the part of speech of the head or depen dent .", "Finally , we use a lookahead of three tokens , considering only their parts of speech .", "We have experimented with two different statemodels , one that incorporates all the features depicted in Figure 2 Model 1 , and one that ex cludes the parts of speech of TH , TL , TR , NL Model 2 .", "Models similar to model 2 have been found towork well for datasets with a rich annotation of de pendency types , such as the Swedish dependency treebank derived from Einarsson 1976 , where the extra part of speech features are largely redundant Nivre et al , 2004 .", "Model 1 can be expected towork better for datasets with less informative dependency annotation , such as dependency trees ex tracted from the Penn Treebank , where the extra part of speech features may compensate for the lack of information in arc labels .", "The learning algorithm used is the IB1 algorithm Aha et al , 1991 with k 5 , i . e . classification basedon 5 nearest neighbors . 4 Distances are measured us ing the modified value difference metric MVDM Stanfill and Waltz , 1986 ; Cost and Salzberg , 1993 for instances with a frequency of at least 3 andthe simple overlap metric otherwise , and classifica tion is based on distance weighted class voting with inverse distance weighting Dudani , 1976 .", "Thesesettings are the result of extensive experiments partially reported in Nivre et al 2004 .", "For more infor mation about the different parameters and settings , see Daelemans et al 2003 .", "4 Experiments .", "The data set used for experimental evaluation is the standard data set from the Wall Street Journal section of the Penn Treebank , with sections 2 ? 21 3Given the parsing algorithm , N can never have a head or a right dependent in the current configuration . 4In TiMBL , the value of k in fact refers to k nearest dis tances rather than k nearest neighbors , which means that , evenwith k 1 , the nearest neighbor set can contain several instances that are equally distant to the test instance .", "This is dif ferent from the original IB1 algorithm , as described in Aha et al .", "used for training and section 23 for testing Collins , 1999 ; Charniak , 2000 .", "The data has been converted to dependency trees using head rules Magerman , 1995 ; Collins , 1996 .", "We are grateful to Ya mada and Matsumoto for letting us use their rule set , which is a slight modification of the rules used byCollins 1999 .", "This permits us to make exact com parisons with the parser of Yamada and Matsumoto 2003 , but also the parsers of Collins 1997 and Charniak 2000 , which are evaluated on the same data set in Yamada and Matsumoto 2003 . One problem that we had to face is that the standard conversion of phrase structure trees to de pendency trees gives unlabeled dependency trees , whereas our parser requires labeled trees .", "Since the annotation scheme of the Penn Treebank does notinclude dependency types , there is no straightfor ward way to derive such labels .", "We have therefore experimented with two different sets of labels , none of which corresponds to dependency types in a strict sense .", "The first set consists of the function tags forgrammatical roles according to the Penn II annota tion guidelines Bies et al , 1995 ; we call this set G . The second set consists of the ordinary bracket la bels S , NP , VP , etc . , combined with function tags for grammatical roles , giving composite labels such as NP SBJ ; we call this set B . We assign labels to arcs by letting each non root word that heads aphrase P in the original phrase structure have its in coming edge labeled with the label of P modulo the set of labels used .", "In both sets , we also includea default label DEP for arcs that would not other wise get a label .", "This gives a total of 7 labels in the G set and 50 labels in the B set .", "Figure 1 shows a converted dependency tree using the B labels ; in the corresponding tree with G labels NP SBJ would be replaced by SBJ , ADVP and VP by DEP .", "We use the following metrics for evaluation 1 .", "Unlabeled attachment score UAS The pro .", "portion of words that are assigned the correct head or no head if the word is a root Eisner , 1996 ; Collins et al , 1999 .", "Labeled attachment score LAS The pro .", "portion of words that are assigned the correct head and dependency type or no head if the word is a root Nivre et al , 2004 .", "Dependency accuracy DA The proportion .", "of non root words that are assigned the correct head Yamada and Matsumoto , 2003 .", "Root accuracy RA The proportion of root .", "words that are analyzed as such Yamada and Matsumoto , 2003 .", "sentences whose unlabeled dependency structure is completely correct Yamada and Mat sumoto , 2003 .", "All metrics except CM are calculated as meanscores per word , and punctuation tokens are con sistently excluded . Table 1 shows the attachment score , both unla beled and labeled , for the two different state models with the two different label sets .", "First of all , we see that Model 1 gives better accuracy than Model 2 with the smaller label set G , which confirms our expectations that the added part of speech featuresare helpful when the dependency labels are less informative .", "Conversely , we see that Model 2 outper forms Model 1 with the larger label set B , which is consistent with the hypothesis that part of speech features become redundant as dependency labels get more informative .", "It is interesting to note that this effect holds even in the case where the dependencylabels are mostly derived from phrase structure cate gories .", "We can also see that the unlabeled attachment score improves , for both models , when the set of dependency labels is extended .", "On the other hand , the labeled attachment score drops , but it must beremembered that these scores are not really comparable , since the number of classes in the classifi cation problem increases from 7 to 50 as we move from the G set to the B set .", "Therefore , we have also included the labeled attachment score restricted to the G set for the parser using the B set BG , and wesee then that the attachment score improves , espe cially for Model 2 .", "All differences are significant beyond the . 01 level ; McNemar ? s test .", "Table 2 shows the dependency accuracy , root accuracy and complete match scores for our best parser Model 2 with label set B in comparison with Collins 1997 Model 3 , Charniak 2000 , and Yamada and Matsumoto 2003 . 5 It is clear that , with respect to unlabeled accuracy , our parser does not quite reach state of the art performance , evenif we limit the competition to deterministic meth ods such as that of Yamada and Matsumoto 2003 .", "We believe that there are mainly three reasons for this .", "First of all , the part of speech tagger used for preprocessing in our experiments has a loweraccuracy than the one used by Yamada and Mat sumoto 2003 96 . 1 vs . 97 . 1 .", "Although this is not a very interesting explanation , it undoubtedly accounts for part of the difference .", "Secondly , since 5The information in the first three rows is taken directly from Yamada and Matsumoto 2003 . our parser makes crucial use of dependency type in formation in predicting the next action of the parser , it is very likely that it suffers from the lack of realdependency labels in the converted treebank .", "Indi rect support for this assumption can be gained fromprevious experiments with Swedish data , where al most the same accuracy 85 unlabeled attachment score has been achieved with a treebank whichis much smaller but which contains proper depen dency annotation Nivre et al , 2004 .", "A third important factor is the relatively low rootaccuracy of our parser , which may reflect a weak ness in the one pass parsing strategy with respect tothe global structure of complex sentences .", "It is note worthy that our parser has lower root accuracy than dependency accuracy , whereas the inverse holds for all the other parsers .", "The problem becomes even more visible when we consider the dependency and root accuracy for sentences of different lengths , as shown in Table 3 .", "Here we see that for really short sentences up to 10 words root accuracy is indeedhigher than dependency accuracy , but while depen dency accuracy degrades gracefully with sentence length , the root accuracy drops more drastically which also very clearly affects the complete match score .", "This may be taken to suggest that some kind of preprocessing in the form of clausing may help to improve overall accuracy . Turning finally to the assessment of labeled de pendency accuracy , we are not aware of any strictlycomparable results for the given data set , but Buch holz 2002 reports a labeled accuracy of 72 . 6 for the assignment of grammatical relations using a cascade of memory based processors .", "This can be compared with a labeled attachment score of 84 . 4 for Model 2 with our B set , which is of about the same size as the set used by Buchholz , although the labels are not the same .", "In another study , Blaheta and Charniak 2000 report an F measure of 98 . 9 for the assignment of Penn Treebank grammatical role labels our G set to phrases that were correctly parsed by the parser described in Charniak 2000 .", "If null labels corresponding to our DEP labels areexcluded , the F score drops to 95 . 7 .", "The corre sponding F measures for our best parser Model 2 , BG are 99 . 0 and 94 . 7 .", "For the larger B set , our best parser achieves an F measure of 96 . 9 DEP labels included , which can be compared with 97 . 0 for a similar but larger set of labels inCollins 1999 . 6 Although none of the previous re sults on labeling accuracy is strictly comparable to ours , it nevertheless seems fair to conclude that the 6This F measure is based on the recall and precision figures reported in Figure 7 . 15 in Collins 1999 .", "Model 1 Model 2 G B BG G B BG UAS 86 . 4 86 . 7 85 . 8 87 . 1 LAS 85 . 3 84 . 0 85 . 5 84 . 6 84 . 4 86 . 0 Table 1 Parsing accuracy Attachment score BG evaluation of B restricted to G labels DA RA CM Charniak 92 . 1 95 . 2 45 . 2 Collins 91 . 5 95 . 2 43 . 3 Yamada Matsumoto 90 . 3 91 . 6 38 . 4 Nivre Scholz 87 . 3 84 . 3 30 . 4 Table 2 Comparison with related work Yamada and Matsumoto , 2003 labeling accuracy of the present parser is close to the state of the art , even if its capacity to derive correct structures is not .", "5 Conclusion .", "This paper has explored the application of a data driven dependency parser to English text , using data from the Penn Treebank .", "The parser is deterministic and uses a linear time parsing algorithm , guided bymemory based classifiers , to construct labeled de pendency structures incrementally in one pass over the input .", "Given the difficulty of extracting labeled dependencies from a phrase structure treebank with limited functional annotation , the accuracy attainedis fairly respectable .", "And although the structural ac curacy falls short of the best available parsers , the labeling accuracy appears to be competitive . The most important weakness is the limited ac curacy in identifying the root node of a sentence , especially for longer sentences .", "We conjecture that an improvement in this area could lead to a boost in overall performance .", "Another important issue to investigate further is the influence of different kinds of arc labels , and in particular labels that are based on a proper dependency grammar .", "In thefuture , we therefore want to perform more experi ments with genuine dependency treebanks like the Prague Dependency Treebank Hajic , 1998 and the Danish Dependency Treebank Kromann , 2003 .", "We also want to apply dependency based evaluation schemes such as the ones proposed by Lin 1998 and Carroll et al 1998 .", "Acknowledgements The work presented in this paper has been supportedby a grant from the Swedish Research Council 621 2002 4207 .", "The memory based classifiers used in the experiments have been constructed using theTilburg Memory Based Learner TiMBL Daelemans et al , 2003 .", "The conversion of the Penn Tree bank to dependency trees has been performed using head rules kindly provided by Hiroyasu Yamada and Yuji Matsumoto ."], "summary_lines": ["Deterministic Dependency Parsing Of English Text\n", "This paper presents a deterministic dependency parser based on memory-based learning, which parses English text in linear time.\n", "When trained and evaluated on the Wall Street Journal section of the Penn Treebank, the parser achieves a maximum attachment score of 87.1%.\n", "Unlike most previous systems, the parser produces labeled dependency graphs, using as arc labels a combination of bracket labels and grammatical role labels taken from the Penn Treebank II annotation scheme.\n", "The best overall accuracy obtained for identifying both the correct head and the correct arc label is 86.0%, when restricted to grammatical role labels (7 labels), and 84.4% for the maximum set (50 labels).\n", "We propose a variant of the model of Yamada and Matsumoto that reduces the complexity, from the worst case quadratic to linear.\n", "Our deterministic shift/reduce classifier-based dependency parsing approach offers state-of-the-art accuracy with high efficiency due to a greedy search strategy.\n"]}
{"article_lines": ["TextRank Bringing Order Into Texts", "In this paper , we introduce TextRank a graph based ranking model for text processing , and show how this model can be successfully used in natural language applications .", "In particular , we propose two innovative unsupervised methods for keyword and sentence extraction , and show that the results obtained compare favorably with previously published results on established benchmarks .", "Graph based ranking algorithms like Kleinberg s HITS algorithm Kleinberg , 1999 or Google s PageRank Brin and Page , 1998 have been successfully used in citation analysis , social networks , and the analysis of the link structure of the World Wide Web .", "Arguably , these algorithms can be singled out as key elements of the paradigm shift triggered in the field of Web search technology , by providing a Web page ranking mechanism that relies on the collective knowledge of Web architects rather than individual content analysis of Web pages .", "In short , a graph based ranking algorithm is a way of deciding on the importance of a vertex within a graph , by taking into account global information recursively computed from the entire graph , rather than relying only on local vertex specific information .", "Applying a similar line of thinking to lexical or semantic graphs extracted from natural language documents , results in a graph based ranking model that can be applied to a variety of natural language processing applications , where knowledge drawn from an entire text is used in making local ranking selection decisions .", "Such text oriented ranking methods can be applied to tasks ranging from automated extraction of keyphrases , to extractive summarization and word sense disambiguation Mihalcea et al . , 2004 .", "In this paper , we introduce the TextRank graphbased ranking model for graphs extracted from natural language texts .", "We investigate and evaluate the application of TextRank to two language processing tasks consisting of unsupervised keyword and sentence extraction , and show that the results obtained with TextRank are competitive with state of the art systems developed in these areas .", "Graph based ranking algorithms are essentially a way of deciding the importance of a vertex within a graph , based on global information recursively drawn from the entire graph .", "The basic idea implemented by a graph based ranking model is that of voting or recommendation .", "When one vertex links to another one , it is basically casting a vote for that other vertex .", "The higher the number of votes that are cast for a vertex , the higher the importance of the vertex .", "Moreover , the importance of the vertex casting the vote determines how important the vote itself is , and this information is also taken into account by the ranking model .", "Hence , the score associated with a vertex is determined based on the votes that are cast for it , and the score of the vertices casting these votes .", "Formally , let be a directed graph with the set of vertices and set of edges , where is a subset of .", "For a given vertex , let be the set of vertices that point to it predecessors , and let be the set of vertices that vertex points to successors .", "The score of a vertex is defined as follows Brin and Page , 1998 where is a damping factor that can be set between 0 and 1 , which has the role of integrating into the model the probability of jumping from a given vertex to another random vertex in the graph .", "In the context of Web surfing , this graph based ranking algorithm implements the random surfer model , where a user clicks on links at random with a probability , and jumps to a completely new page with probability .", "The factor is usually set to 0 . 85 Brin and Page , 1998 , and this is the value we are also using in our implementation .", "Starting from arbitrary values assigned to each node in the graph , the computation iterates until convergence below a given threshold is achieved 1 .", "After running the algorithm , a score is associated with each vertex , which represents the importance of the vertex within the graph .", "Notice that the final values obtained after TextRank runs to completion are not affected by the choice of the initial value , only the number of iterations to convergence may be different .", "It is important to notice that although the TextRank applications described in this paper rely on an algorithm derived from Google s PageRank Brin and Page , 1998 , other graph based ranking algorithms such as e . g .", "HITS Kleinberg , 1999 or Positional Function Herings et al . , 2001 can be easily integrated into the TextRank model Mihalcea , 2004 .", "Although traditionally applied on directed graphs , a recursive graph based ranking algorithm can be also applied to undirected graphs , in which case the outdegree of a vertex is equal to the in degree of the vertex .", "For loosely connected graphs , with the number of edges proportional with the number of vertices , undirected graphs tend to have more gradual convergence curves .", "Figure 1 plots the convergence curves for a randomly generated graph with 250 vertices and 250 edges , for a convergence threshold of 0 . 0001 .", "As the connectivity of the graph increases i . e . larger number of edges , convergence is usually achieved after fewer iterations , and the convergence curves for directed and undirected graphs practically overlap .", "In the context of Web surfing , it is unusual for a page to include multiple or partial links to another page , and hence the original PageRank definition for graph based ranking is assuming unweighted graphs .", "However , in our model the graphs are build from natural language texts , and may include multiple or partial links between the units vertices that are extracted from text .", "It may be therefore useful to indicate and incorporate into the model the strength of the connection between two vertices and as a weight added to the corresponding edge that connects the two vertices .", "Consequently , we introduce a new formula for graph based ranking that takes into account edge weights when computing the score associated with a vertex in the graph .", "Notice that a similar formula can be defined to integrate vertex weights .", "Figure 1 plots the convergence curves for the same sample graph from section 2 . 1 , with random weights in the interval 0 10 added to the edges .", "While the final vertex scores and therefore rankings differ significantly as compared to their unweighted alternatives , the number of iterations to convergence and the shape of the convergence curves is almost identical for weighted and unweighted graphs .", "To enable the application of graph based ranking algorithms to natural language texts , we have to build a graph that represents the text , and interconnects words or other text entities with meaningful relations .", "Depending on the application at hand , text units of various sizes and characteristics can be added as vertices in the graph , e . g . words , collocations , entire sentences , or others .", "Similarly , it is the application that dictates the type of relations that are used to draw connections between any two such vertices , e . g . lexical or semantic relations , contextual overlap , etc .", "Regardless of the type and characteristics of the elements added to the graph , the application of graphbased ranking algorithms to natural language texts consists of the following main steps In the following , we investigate and evaluate the application of TextRank to two natural language processing tasks involving ranking of text units 1 A keyword extraction task , consisting of the selection of keyphrases representative for a given text ; and 2 A sentence extraction task , consisting of the identification of the most important sentences in a text , which can be used to build extractive summaries .", "The task of a keyword extraction application is to automatically identify in a text a set of terms that best describe the document .", "Such keywords may constitute useful entries for building an automatic index for a document collection , can be used to classify a text , or may serve as a concise summary for a given document .", "Moreover , a system for automatic identification of important terms in a text can be used for the problem of terminology extraction , and construction of domain specific dictionaries .", "The simplest possible approach is perhaps to use a frequency criterion to select the important keywords in a document .", "However , this method was generally found to lead to poor results , and consequently other methods were explored .", "The state ofthe art in this area is currently represented by supervised learning methods , where a system is trained to recognize keywords in a text , based on lexical and syntactic features .", "This approach was first suggested in Turney , 1999 , where parametrized heuristic rules are combined with a genetic algorithm into a system for keyphrase extraction GenEx that automatically identifies keywords in a document .", "A different learning algorithm was used in Frank et al . , 1999 , where a Naive Bayes learning scheme is applied on the document collection , with improved results observed on the same data set as used in Turney , 1999 .", "Neither Turney nor Frank report on the recall of their systems , but only on precision a 29 . 0 precision is achieved with GenEx Turney , 1999 for five keyphrases extracted per document , and 18 . 3 precision achieved with Kea Frank et al . , 1999 for fifteen keyphrases per document .", "More recently , Hulth , 2003 applies a supervised learning system to keyword extraction from abstracts , using a combination of lexical and syntactic features , proved to improve significantly over previously published results .", "As Hulth suggests , keyword extraction from abstracts is more widely applicable than from full texts , since many documents on the Internet are not available as full texts , but only as abstracts .", "In her work , Hulth experiments with the approach proposed in Turney , 1999 , and a new approach that integrates part of speech information into the learning process , and shows that the accuracy of the system is almost doubled by adding linguistic knowledge to the term representation .", "In this section , we report on our experiments in keyword extraction using TextRank , and show that the graph based ranking model outperforms the best published results in this problem .", "Similar to Hulth , 2003 , we are evaluating our algorithm on keyword extraction from abstracts , mainly for the purpose of allowing for a direct comparison with the results she reports with her keyphrase extraction system .", "Notice that the size of the text is not a limitation imposed by our system , and similar results are expected with TextRank applied on full texts .", "The expected end result for this application is a set of words or phrases that are representative for a given natural language text .", "The units to be ranked are therefore sequences of one or more lexical units extracted from text , and these represent the vertices that are added to the text graph .", "Any relation that can be defined between two lexical units is a potentially useful connection edge that can be added between two such vertices .", "We are using a co occurrence relation , controlled by the distance between word occurrences two vertices are connected if their corresponding lexical units co occur within a window of maximum words , where can be set anywhere from 2 to 10 words .", "Co occurrence links express relations between syntactic elements , and similar to the semantic links found useful for the task of word sense disambiguation Mihalcea et al . , 2004 , they represent cohesion indicators for a given text .", "The vertices added to the graph can be restricted with syntactic filters , which select only lexical units of a certain part of speech .", "One can for instance consider only nouns and verbs for addition to the graph , and consequently draw potential edges based only on relations that can be established between nouns and verbs .", "We experimented with various syntactic filters , including all open class words , nouns and verbs only , etc . , with best results observed for nouns and adjectives only , as detailed in section 3 . 2 .", "The TextRank keyword extraction algorithm is fully unsupervised , and proceeds as follows .", "First , Compatibility of systems of linear constraints over the set of natural numbers .", "Criteria of compatibility of a system of linear Diophantine equations , strict inequations , and nonstrict inequations are considered .", "Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given .", "These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types . the text is tokenized , and annotated with part of speech tags a preprocessing step required to enable the application of syntactic filters .", "To avoid excessive growth of the graph size by adding all possible combinations of sequences consisting of more than one lexical unit ngrams , we consider only single words as candidates for addition to the graph , with multi word keywords being eventually reconstructed in the post processing phase .", "Next , all lexical units that pass the syntactic filter are added to the graph , and an edge is added between those lexical units that co occur within a window of words .", "After the graph is constructed undirected unweighted graph , the score associated with each vertex is set to an initial value of 1 , and the ranking algorithm described in section 2 is run on the graph for several iterations until it converges usually for 20 30 iterations , at a threshold of 0 . 0001 .", "Once a final score is obtained for each vertex in the graph , vertices are sorted in reversed order of their score , and the top vertices in the ranking are retained for post processing .", "While may be set to any fixed value , usually ranging from 5 to 20 keywords e . g .", "Turney , 1999 limits the number of keywords extracted with his GenEx system to five , we are using a more flexible approach , which decides the number of keywords based on the size of the text .", "For the data used in our experiments , which consists of relatively short abstracts , is set to a third of the number of vertices in the graph .", "During post processing , all lexical units selected as potential keywords by the TextRank algorithm are marked in the text , and sequences of adjacent keywords are collapsed into a multi word keyword .", "For instance , in the text Matlab code for plotting ambiguity functions , if both Matlab and code are selected as potential keywords by TextRank , since they are adjacent , they are collapsed into one single keyword Matlab code .", "Figure 2 shows a sample graph built for an abstract from our test collection .", "While the size of the abstracts ranges from 50 to 350 words , with an average size of 120 words , we have deliberately selected a very small abstract for the purpose of illustration .", "For this example , the lexical units found to have higher importance by the TextRank algorithm are with the TextRank score indicated in parenthesis numbers 1 . 46 , inequations 1 . 45 , linear 1 . 29 , diophantine 1 . 28 , upper 0 . 99 , bounds 0 . 99 , strict 0 . 77 .", "Notice that this ranking is different than the one rendered by simple word frequencies .", "For the same text , a frequency approach provides the following top ranked lexical units systems 4 , types 3 , solutions 3 , minimal 3 , linear 2 , inequations 2 , algorithms 2 .", "All other lexical units have a frequency of 1 , and therefore cannot be ranked , but only listed .", "The data set used in the experiments is a collection of 500 abstracts from the Inspec database , and the corresponding manually assigned keywords .", "This is the same test data set as used in the keyword extraction experiments reported in Hulth , 2003 .", "The Inspec abstracts are from journal papers from Computer Science and Information Technology .", "Each abstract comes with two sets of keywords assigned by professional indexers controlled keywords , restricted to a given thesaurus , and uncontrolled keywords , freely assigned by the indexers .", "We follow the evaluation approach from Hulth , 2003 , and use the uncontrolled set of keywords .", "In her experiments , Hulth is using a total of 2000 abstracts , divided into 1000 for training , 500 for development , and 500 for test2 .", "Since our approach is completely unsupervised , no training development data is required , and we are only using the test docu2Many thanks to Anette Hulth for allowing us to run our algorithm on the data set used in her keyword extraction experiments , and for making available the training test development data split . ments for evaluation purposes .", "The results are evaluated using precision , recall , and F measure .", "Notice that the maximum recall that can be achieved on this collection is less than 100 , since indexers were not limited to keyword extraction as our system is but they were also allowed to perform keyword generation , which eventually results in keywords that do not explicitly appear in the text .", "For comparison purposes , we are using the results of the state of the art keyword extraction system reported in Hulth , 2003 .", "Shortly , her system consists of a supervised learning scheme that attempts to learn how to best extract keywords from a document , by looking at a set of four features that are determined for each candidate keyword 1 within document frequency , 2 collection frequency , 3 relative position of the first occurrence , 4 sequence of part of speech tags .", "These features are extracted from both training and test data for all candidate keywords , where a candidate keyword can be Ngrams unigrams , bigrams , or trigrams extracted from the abstracts , NP chunks noun phrases , patterns a set of part of speech patterns detected from the keywords attached to the training abstracts .", "The learning system is a rule induction system with bagging .", "Our system consists of the TextRank approach described in Section 3 . 1 , with a co occurrence windowsize set to two , three , five , or ten words .", "Table 1 lists the results obtained with TextRank , and the best results reported in Hulth , 2003 .", "For each method , the table lists the total number of keywords assigned , the mean number of keywords per abstract , the total number of correct keywords , as evaluated against the set of keywords assigned by professional indexers , and the mean number of correct keywords .", "The table also lists precision , recall , and F measure .", "Discussion .", "TextRank achieves the highest precision and F measure across all systems , although the recall is not as high as in supervised methods possibly due the limitation imposed by our approach on the number of keywords selected , which is not made in the supervised systema .", "A larger window does not seem to help on the contrary , the larger the window , the lower the precision , probably explained by the fact that a relation between words that are further apart is not strong enough to define a connection in the text graph .", "Experiments were performed with various syntactic filters , including all open class words , nouns and adjectives , and nouns only , and the best performance was achieved with the filter that selects nouns and adjectives only .", "We have also experimented with a setting where no part of speech information was added to the text , and all words except a predefined list of stopwords were added to the graph .", "The results with this setting were significantly lower than the systems that consider part of speech information , which corroborates with previous observations that linguistic information helps the process of keyword extraction Hulth , 2003 .", "Experiments were also performed with directed graphs , where a direction was set following the natural flow of the text , e . g . one candidate keyword recommends and therefore has a directed arc to the candidate keyword that follows in the text , keeping the restraint imposed by the co occurrence relation .", "We have also tried the reversed direction , where a lexical unit points to a previous token in the text .", "Table 1 includes the results obtained with directed graphs for a co occurrence window of 2 .", "Regardless of the direction chosen for the arcs , results obtained with directed graphs are worse than results obtained with undirected graphs , which suggests that despite a natural flow in running text , there is no natural direction that can be established between cooccurring words .", "Overall , our TextRank system leads to an Fmeasure higher than any of the previously proposed systems .", "Notice that TextRank is completely unsupervised , and unlike other supervised systems , it relies exclusively on information drawn from the text itself , which makes it easily portable to other text collections , domains , and languages .", "The other TextRank application that we investigate consists of sentence extraction for automatic summarization .", "In a way , the problem of sentence extraction can be regarded as similar to keyword extraction , since both applications aim at identifying sequences that are more representative for the given text .", "In keyword extraction , the candidate text units consist of words or phrases , whereas in sentence extraction , we deal with entire sentences .", "TextRank turns out to be well suited for this type of applications , since it allows for a ranking over text units that is recursively computed based on information drawn from the entire text .", "To apply TextRank , we first need to build a graph associated with the text , where the graph vertices are representative for the units to be ranked .", "For the task of sentence extraction , the goal is to rank entire sentences , and therefore a vertex is added to the graph for each sentence in the text .", "The co occurrence relation used for keyword extraction cannot be applied here , since the text units in consideration are significantly larger than one or few words , and co occurrence is not a meaningful relation for such large contexts .", "Instead , we are defining a different relation , which determines a connection between two sentences if there is a similarity relation between them , where similarity is measured as a function of their content overlap .", "Such a relation between two sentences can be seen as a process of recommendation a sentence that addresses certain concepts in a text , gives the reader a recommendation to refer to other sentences in the text that address the same concepts , and therefore a link can be drawn between any two such sentences that share common content .", "The overlap of two sentences can be determined simply as the number of common tokens between the lexical representations of the two sentences , or it can be run through syntactic filters , which only count words of a certain syntactic category , e . g . all open class words , nouns and verbs , etc .", "Moreover , to avoid promoting long sentences , we are using a normalization factor , and divide the content overlap TextRank extractive summary Hurricane Gilbert swept toward the Dominican Republic Sunday , and the Civil De fense alerted its heavily populated south coast to prepare for high winds , heavy rains and high seas .", "The National Hurricane Center in Miami reported its position at 2 a . m . Sunday at latitude 16 . 1 north , longitude 67 . 5 west , about 140 miles south of Ponce , Puerto Rico , and 200 miles southeast of Santo Domingo .", "The National Weather Service in San Juan , Puerto Rico , said Gilbert was moving westward at 15 mph with a quot ; broad area of cloudiness and heavy weather quot ; rotating around the center of the storm .", "Strong winds associated with Gilbert brought coastal flooding , strong southeast winds and up to 12 feet to Puerto Rico s south coast .", "Manual abstract I Hurricane Gilbert is moving toward the Dominican Republic , where the residents of the south coast , especially the Barahona Province , have been alerted to prepare for heavy rains , and high wind and seas .", "Tropical storm Gilbert formed in the eastern Carribean and became a hurricane on Saturday night .", "By 2 a . m . Sunday it was about 200 miles southeast of Santo Domingo and moving westward at 15 mph with winds of 75 mph .", "Flooding is expected in Puerto Rico and in the Virgin Islands .", "The second hurricane of the season , Florence , is now over the southern United States and down graded to a tropical storm .", "Manual abstract II Tropical storm Gilbert in the eastern Carribean strenghtened into a hurricane Saturday night .", "The National Hurricane Center in Miami reported its position at 2 a . m . Sunday to be about 140 miles south of Puerto Rico and 200 miles southeast of Santo Domingo .", "It is moving westward at 15 mph with a broad area of cloudiness and heavy weather with sustained winds of 75 mph gusting to 92 mph .", "The Dominican Republic s Civil Defense alerted that country s heavily populated south coast and the National Weather Service in San Juan , Puerto Rico issued a flood watch for Puerto Rico and the Virgin Islands until at least 6 p . m . Sunday . of two sentences with the length of each sentence .", "Formally , given two sentences and , with a sentence being represented by the set of words that appear in the sentence , the similarity of and is defined as Other sentence similarity measures , such as string kernels , cosine similarity , longest common subsequence , etc . are also possible , and we are currently evaluating their impact on the summarization performance .", "The resulting graph is highly connected , with a weight associated with each edge , indicating the strength of the connections established between various sentence pairs in the text .", "The text is therefore represented as a weighted graph , and consequently we are using the weighted graph based ranking formula introduced in Section 2 . 2 .", "After the ranking algorithm is run on the graph , sentences are sorted in reversed order of their score , and the top ranked sentences are selected for inclusion in the summary .", "Figure 3 shows a text sample , and the associated weighted graph constructed for this text .", "The figure also shows sample weights attached to the edges connected to vertex 94 , and the final TextRank score computed for each sentence .", "The sentences with the highest rank are selected for inclusion in the abstract .", "For this sample article , the sentences with id s 9 , 15 , 16 , 18 are extracted , resulting in a summary of about 100 words , which according to automatic evaluation measures , is ranked the second among summaries produced by 15 other systems see Section 4 . 2 for evaluation methodology .", "We evaluate the TextRank sentence extraction algorithm on a single document summarization task , using 567 news articles provided during the Document Understanding Evaluations 2002 DUC , 2002 .", "For each article , TextRank generates an 100 words summary the task undertaken by other systems participating in this single document summarization task .", "For evaluation , we are using the ROUGE evaluation toolkit , which is a method based on Ngram statistics , found to be highly correlated with human evaluations Lin and Hovy , 2003 .", "Two manually produced reference summaries are provided , and used in the evaluation process5 .", "Fifteen different systems participated in this task , and we compare the performance of TextRank with the top five performing systems , as well as with the baseline proposed by the DUC evaluators consisting of a 100 word summary constructed by taking the first sentences in each article .", "Table 2 shows the results obtained on this data set of 567 news articles , including the results for TextRank shown in bold , the baseline , and the results of the top five performing systems in the DUC 2002 single document summarization task DUC , 2002 .", "TextRank , top 5 out of 15 DUC 2002 systems , and baseline .", "Evaluation takes into account a all words ; b stemmed words ; c stemmed words , and no stopwords .", "Discussion .", "TextRank succeeds in identifying the most important sentences in a text based on information exclusively drawn from the text itself .", "Unlike other supervised systems , which attempt to learn what makes a good summary by training on collections of summaries built for other articles , TextRank is fully unsupervised , and relies only on the given text to derive an extractive summary , which represents a summarization model closer to what humans are doing when producing an abstract for a given document .", "Notice that TextRank goes beyond the sentence connectivity in a text .", "For instance , sentence 15 in the example provided in Figure 3 would not be identified as important based on the number of connections it has with other vertices in the graph , but it is identified as important by TextRank and by humans see the reference summaries displayed in the same figure .", "Another important aspect of TextRank is that it gives a ranking over all sentences in a text which means that it can be easily adapted to extracting very short summaries headlines consisting of one The evaluation is done using the Ngram 1 , 1 setting of ROUGE , which was found to have the highest correlation with human judgments , at a confidence level of 95 .", "Only the first 100 words in each summary are considered . sentence , or longer more explicative summaries , consisting of more than 100 words .", "We are also investigating combinations of keyphrase and sentence extraction techniques as a method for building short long summaries .", "Finally , another advantage of TextRank over previously proposed methods for building extractive summaries is the fact that it does not require training corpora , which makes it easily adaptable to other languages or domains .", "Intuitively , TextRank works well because it does not only rely on the local context of a text unit vertex , but rather it takes into account information recursively drawn from the entire text graph .", "Through the graphs it builds on texts , TextRank identifies connections between various entities in a text , and implements the concept of recommendation .", "A text unit recommends other related text units , and the strength of the recommendation is recursively computed based on the importance of the units making the recommendation .", "For instance , in the keyphrase extraction application , co occurring words recommend each other as important , and it is the common context that enables the identification of connections between words in text .", "In the process of identifying important sentences in a text , a sentence recommends another sentence that addresses similar concepts as being useful for the overall understanding of the text .", "The sentences that are highly recommended by other sentences in the text are likely to be more informative for the given text , and will be therefore given a higher score .", "An analogy can be also drawn with PageRank s random surfer model , where a user surfs the Web by following links from any given Web page .", "In the context of text modeling , TextRank implements what we refer to as text surfing , which relates to the concept of text cohesion Halliday and Hasan , 1976 from a certain concept in a text , we are likely to follow links to connected concepts that is , concepts that have a relation with the current concept be that a lexical or semantic relation .", "This also relates to the knitting phenomenon Hobbs , 1974 facts associated with words are shared in different parts of the discourse , and such relationships serve to knit the discourse together .", "Through its iterative mechanism , TextRank goes beyond simple graph connectivity , and it is able to score text units based also on the importance of other text units they link to .", "The text units selected by TextRank for a given application are the ones most recommended by related text units in the text , with preference given to the recommendations made by most influential ones , i . e . the ones that are in turn highly recommended by other related units .", "The underlying hypothesis is that in a cohesive text fragment , related text units tend to form a Web of connections that approximates the model humans build about a given context in the process of discourse understanding .", "In this paper , we introduced TextRank a graphbased ranking model for text processing , and show how it can be successfully used for natural language applications .", "In particular , we proposed and evaluated two innovative unsupervised approaches for keyword and sentence extraction , and showed that the accuracy achieved by TextRank in these applications is competitive with that of previously proposed state of the art algorithms .", "An important aspect of TextRank is that it does not require deep linguistic knowledge , nor domain or language specific annotated corpora , which makes it highly portable to other domains , genres , or languages ."], "summary_lines": ["TextRank: Bringing Order Into Texts\n", "In this paper, we introduce TextRank - a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications.\n", "In particular, we propose two innovative unsupervised methods for keyword and sentence extraction, and show that the results obtained compare favorably with previously published results on established benchmarks.\n", "we propose TextRank, which is one of the most well-known graph based approaches to key phrase extraction.\n", "we propose the TextRank model to rank key words based on the co-occurrence links between words.\n"]}
{"article_lines": ["An All Subtrees Approach To Unsupervised Parsing", "We investigate generalizations of the allsubtrees quot ; DOP quot ; approach to unsupervised parsing .", "Unsupervised DOP models assign all possible binary trees to a set of sentences and next use a large random subset of all subtrees from these binary trees to compute the most probable parse trees .", "We will test both a relative frequency estimator for unsupervised DOP and a maximum likelihood estimator which is known to be statistically consistent .", "We report state ofthe art results on English WSJ , German NEGRA and Chinese CTB data .", "To the best of our knowledge this is the first paper which tests a maximum likelihood estimator for DOP on the Wall Street Journal , leading the surprising result that unsupervised parsing model beats a widely used supervised model a treebank PCFG .", "The problem of bootstrapping syntactic structure from unlabeled data has regained considerable interest .", "While supervised parsers suffer from shortage of hand annotated data , unsupervised parsers operate with unlabeled raw data of which unlimited quantities are available .", "During the last few years there has been steady progress in the field .", "Where van Zaanen 2000 achieved 39 . 2 unlabeled f score on ATIS word strings , Clark 2001 reports 42 . 0 on the same data , and Klein and Manning 2002 obtain 51 . 2 f score on ATIS part of speech strings using a constituent context model called CCM .", "On Penn Wall Street Journal po s strings 10 WSJ10 , Klein and Manning 2002 report 71 . 1 unlabeled f score with CCM .", "And the hybrid approach of Klein and Manning 2004 , which combines constituency and dependency models , yields 77 . 6 f score .", "Bod 2006 shows that a further improvement on the WSJ10 can be achieved by an unsupervised generalization of the all subtrees approach known as Data Oriented Parsing DOP .", "This unsupervised DOP model , coined U DOP , first assigns all possible unlabeled binary trees to a set of sentences and next uses all subtrees from a large subset of these trees to compute the most probable parse trees .", "Bod 2006 reports that U DOP not only outperforms previous unsupervised parsers but that its performance is as good as a binarized supervised parser i . e . a treebank PCFG on the WSJ .", "A possible drawback of U DOP , however , is the statistical inconsistency of its estimator Johnson 2002 which is inherited from the DOP1 model Bod 1998 .", "That is , even with unlimited training data , U DOP's estimator is not guaranteed to converge to the correct weight distribution .", "Johnson 2002 76 argues in favor of a maximum likelihood estimator for DOP which is statistically consistent .", "As it happens , in Bod 2000 we already developed such a DOP model , termed ML DOP , which reestimates the subtree probabilities by a maximum likelihood procedure based on Expectation Maximization .", "Although crossvalidation is needed to avoid overlearning , ML DOP outperforms DOP1 on the OVIS corpus Bod 2000 .", "This raises the question whether we can create an unsupervised DOP model which is also statistically consistent .", "In this paper we will show that an unsupervised version of ML DOP can be constructed along the lines of U DOP .", "We will start out by summarizing DOP , U DOP and ML DOP , and next create a new unsupervised model called UML DOP .", "We report that UML DOP not only obtains higher parse accuracy than U DOP on three different domains , but that it also achieves this with fewer subtrees than U DOP .", "To the best of our knowledge , this paper presents the first unsupervised parser that outperforms a widely used supervised parser on the WSJ , i . e . a treebank PCFG .", "We will raise the question whether the end of supervised parsing is in sight .", "The key idea of DOP is this given an annotated corpus , use all subtrees , regardless of size , to parse new sentences .", "The DOP1 model in Bod 1998 computes the probabilities of parse trees and sentences from the relative frequencies of the subtrees .", "Although it is now known that DOP1's relative frequency estimator is statistically inconsistent Johnson 2002 , the model yields excellent empirical results and has been used in state of the art systems .", "Let's illustrate DOP1 with a simple example .", "Assume a corpus consisting of only two trees , as given in figure 1 .", "DOP1 computes the probability of a subtree t as the probability of selecting t among all corpus subtrees that can be substituted on the same node as t . This probability is computed as the number of occurrences of t in the corpus , t , divided by the total number of occurrences of all subtrees t' with the same root label as t . 1 Let r t return the root label of t . Then we may write New sentences may be derived by combining fragments , i . e . subtrees , from this corpus , by means of a node substitution operation indicated as o . Node substitution identifies the leftmost nonterminal frontier node of one subtree with the root node of a second subtree i . e . , the second subtree is substituted on the leftmost nonterminal frontier node of the first subtree .", "Thus a new sentence such as Mary likes Susan can be derived by The probability of a derivation t1o . . . otn is computed by the product of the probabilities of its subtrees ti As we have seen , there may be several distinct derivations that generate the same parse tree .", "The probability of a parse tree T is the sum of the assigning all possible binary trees to this string , where each root node is labeled S and each internal node is labeled X .", "Thus NNS VBD JJ NNS has a total of five binary trees shown in figure 4 where for readability we add words as well .", "Thus DOP1 considers counts of subtrees of a wide range of sizes everything from counts of singlelevel rules to entire trees is taken into account to compute the most probable parse tree of a sentence .", "A disadvantage of the approach may be that an extremely large number of subtrees and derivations must be considered .", "Fortunately there exists a compact isomorphic PCFG reduction of DOP1 whose size is linear rather than exponential in the size of the training set Goodman 2003 .", "Moreover , Collins and Duffy 2002 show how a tree kernel can be applied to DOP1's all subtrees representation .", "The currently most successful version of DOP1 uses a PCFG reduction of the model with an n best parsing algorithm Bod 2003 .", "U DOP extends DOP1 to unsupervised parsing Bod 2006 .", "Its key idea is to assign all unlabeled binary trees to a set of sentences and to next use in principle all subtrees from these binary trees to parse new sentences .", "U DOP thus proposes one of the richest possible models in bootstrapping trees .", "Previous models like Klein and Manning's 2002 , 2005 CCM model limit the dependencies to quot ; contiguous subsequences of a sentence quot ; .", "This means that CCM neglects dependencies that are non contiguous such as between more and than in quot ; BA carried more people than cargo quot ; .", "Instead , UDOP's all subtrees approach captures both contiguous and non contiguous lexical dependencies .", "As with most other unsupervised parsing models , U DOP induces trees for p o s strings rather than for word strings .", "The extension to word strings is straightforward as there exist highly accurate unsupervised part of speech taggers e . g .", "Sch\u00fctze 1995 which can be directly combined with unsupervised parsers .", "To give an illustration of U DOP , consider the WSJ p o s string NNS VBD JJ NNS which may correspond for instance to the sentence Investors suffered heavy losses .", "U DOP starts by While we can efficiently represent the set of all binary trees of a string by means of a chart , we need to unpack the chart if we want to extract subtrees from this set of binary trees .", "And since the total number of binary trees for the small WSJ10 is almost 12 million , it is doubtful whether we can apply the unrestricted U DOP model to such a corpus .", "U DOP therefore randomly samples a large subset from the total number of parse trees from the chart see Bod 2006 and next converts the subtrees from these parse trees into a PCFG reduction Goodman 2003 .", "Since the computation of the most probable parse tree is NP complete Sima'an 1996 , U DOP estimates the most probable tree from the 100 most probable derivations using Viterbi n best parsing .", "We could also have used the more efficient k best hypergraph parsing technique by Huang and Chiang 2005 , but we have not yet incorporated this into our implementation .", "To give an example of the dependencies that U DOP can take into account , consider the following subtrees in figure 5 from the trees in probabilities of its distinct derivations .", "Let tid be the i th subtree in the derivation d that produces tree T , then the probability of T is given by figure 4 where we again add words for readability .", "These subtrees show that U DOP takes into account both contiguous and non contiguous substrings .", "Of course , if we only had the sentence Investors suffered heavy losses in our corpus , there would be no difference in probability between the five parse trees in figure 4 .", "However , if we also have a different sentence where JJ NNS heavy losses appears in a different context , e . g . in Heavy losses were reported , its covering subtree gets a relatively higher frequency and the parse tree where heavy losses occurs as a constituent gets a higher total probability .", "ML DOP Bod 2000 extends DOP with a maximum likelihood reestimation technique based on the expectation maximization EM algorithm Dempster et al . 1977 which is known to be statistically consistent Shao 1999 .", "ML DOP reestimates DOP's subtree probabilities in an iterative way until the changes become negligible .", "The following exposition of ML DOP is heavily based on previous work by Bod 2000 and Magerman 1993 .", "It is important to realize that there is an implicit assumption in DOP that all possible derivations of a parse tree contribute equally to the total probability of the parse tree .", "This is equivalent to saying that there is a hidden component to the model , and that DOP can be trained using an EM algorithm to determine the maximum likelihood estimate for the training data .", "The EM algorithm for this ML DOP model is related to the Inside Outside algorithm for context free grammars , but the reestimation formula is complicated by the presence of subtrees of depth greater than 1 .", "To derive the reestimation formula , it is useful to consider the state space of all possible derivations of a tree .", "The derivations of a parse tree T can be viewed as a state trellis , where each state contains a partially constructed tree in the course of a leftmost derivation of T . st denotes a state containing the tree t which is a subtree of T . The state trellis is defined as follows .", "The initial state , s0 , is a tree with depth zero , consisting of simply a root node labeled with S . The final state , sT , is the given parse tree T . A state st is connected forward to all states stf such that tf t t' , for some t' .", "Here the appropriate t' is defined to be tf t . A state st is connected backward to all states stb such that t tb t' , for some t' .", "Again , t' is defined to be t tb .", "The construction of the state lattice and assignment of transition probabilities according to the ML DOP model is called the forward pass .", "The probability of a given state , P s , is referred to as \u03b1 s .", "The forward probability of a state st is computed recursively The backward probability of a state , referred to as \u03b2 s , is calculated according to the following recursive formula where the backward probability of the goal state is set equal to the forward probability of the goal state , \u03b2 sT \u03b1 sT .", "The update formula for the count of a subtree t is where r t is the root label of t The updated probability distribution , P' t r t , is defined to be In practice , ML DOP starts out by assigning the same relative frequencies to the subtrees as DOP1 , which are next reestimated by the formulas above .", "We may in principle start out with any initial parameters , including random initializations , but since ML estimation is known to be very sensitive to the initialization of the parameters , it is convenient to start with parameters that are known to perform well .", "To avoid overtraining , ML DOP uses the subtrees from one half of the training set to be trained on the other half , and vice versa .", "This crosstraining is important since otherwise UML DOP would assign the training set trees their empirical frequencies and assign zero weight to all other subtrees cf .", "Prescher et al . 2004 .", "The updated probabilities are iteratively reestimated until the decrease in cross entropy becomes negligible .", "Unfortunately , no compact PCFG reduction of MLDOP is known .", "As a consequence , parsing with ML DOP is very costly and the model has hitherto never been tested on corpora larger than OVIS Bonnema et al . 1997 .", "Yet , we will show that by clever pruning we can extend our experiments not only to the WSJ , but also to the German NEGRA and the Chinese CTB .", "Zollmann and Sima'an 2005 propose a different consistent estimator for DOP , which we cannot go into here .", "Analogous to U DOP , UML DOP is an unsupervised generalization of ML DOP it first assigns all unlabeled binary trees to a set of sentences and next extracts a large random set of subtrees from this tree set .", "It then reestimates the initial probabilities of these subtrees by ML DOP on the sentences from a held out part of the tree set .", "The training is carried out by dividing the tree set into two equal parts , and reestimating the subtrees from one part on the other .", "As initial probabilities we use the subtrees' relative frequencies as described in section 2 smoothed by Good Turing see Bod 1998 , though it would also be interesting to see how the model works with other initial parameters , in particular with the usage frequencies proposed by Zuidema 2006 .", "As with U DOP , the total number of subtrees that can be extracted from the binary tree set is too large to be fully taken into account .", "Together with the high computational cost of reestimation we propose even more drastic pruning than we did in Bod 2006 for U DOP .", "That is , while for sentences _ 7 words we use all binary trees , for each sentence _ 8 words we randomly sample a fixed number of 128 trees which effectively favors more frequent trees .", "The resulting set of trees is referred to as the binary tree set .", "Next , we randomly extract for each subtree depth a fixed number of subtrees , where the depth of subtree is the longest path from root to any leaf .", "This has roughly the same effect as the correction factor used in Bod 2003 , 2006 .", "That is , for each particular depth we sample subtrees by first randomly selecting a node in a random tree from the binary tree set after which we select random expansions from that node until a subtree of the particular depth is obtained .", "For our experiments in section 6 , we repeated this procedure 200 , 000 times for each depth .", "The resulting subtrees are then given to MLDOP's reestimation procedure .", "Finally , the reestimated subtrees are used to compute the most probable parse trees for all sentences using Viterbi n best , as described in section 3 , where the most probable parse is estimated from the 100 most probable derivations .", "A potential criticism of U ML DOP is that since we use DOP1's relative frequencies as initial parameters , ML DOP may only find a local maximum nearest to DOP1's estimator .", "But this is of course a criticism against any iterative ML approach it is not guaranteed that the global maximum is found cf .", "Manning and Sch\u00fctze 1999 401 .", "Nevertheless we will see that our reestimation procedure leads to significantly better accuracy compared to U DOP the latter would be equal to UML DOP under 0 iterations .", "Moreover , in contrast to U DOP , UML DOP can be theoretically motivated it maximizes the likelihood of the data using the statistically consistent EM algorithm .", "To compare UML DOP to U DOP , we started out with the WSJ10 corpus , which contains 7422 sentences _ 10 words after removing empty elements and punctuation .", "We used the same evaluation metrics for unlabeled precision UP and unlabeled recall UR as defined in Klein 2005 2122 .", "Klein's definitions differ slightly from the standard PARSEVAL metrics multiplicity of brackets is ignored , brackets of span one are ignored and the bracket labels are ignored .", "The two metrics of UP and UR are combined by the unlabeled f score F1 which is defined as the harmonic mean of UP and UR F1 2 UP UR UP UR .", "For the WSJ10 , we obtained a binary tree set of 5 . 68 105 trees , by extracting the binary trees as described in section 5 .", "From this binary tree set we sampled 200 , 000 subtrees for each subtreedepth .", "This resulted in a total set of roughly 1 . 7 106 subtrees that were reestimated by our maximum likelihood procedure .", "The decrease in cross entropy became negligible after 14 iterations for both halfs of WSJ10 .", "After computing the most probable parse trees , UML DOP achieved an f score of 82 . 9 which is a 20 . 5 error reduction compared to U DOP's f score of 78 . 5 on the same data Bod 2006 .", "We next tested UML DOP on two additional domains which were also used in Klein and Manning 2004 and Bod 2006 the German NEGRA10 Skut et al . 1997 and the Chinese CTB10 Xue et al .", "2002 both containing 2200 sentences _ 10 words after removing punctuation .", "Table 1 shows the results of UML DOP compared to U DOP , the CCM model by Klein and Manning 2002 , the DMV dependency learning model by Klein and Manning 2004 as well as their combined model DMV CCM .", "Table 1 shows that UML DOP scores better than U DOP and Klein and Manning's models in all cases .", "It thus pays off to not only use subtrees rather than substrings as in CCM but to also reestimate the subtrees' probabilities by a maximum likelihood procedure rather than using their smoothed relative frequencies as in U DOP .", "Note that UML DOP achieves these improved results with fewer subtrees than U DOP , due to UML DOP's more drastic pruning of subtrees .", "It is also noteworthy that UMLDOP , like U DOP , does not employ a separate class for non constituents , so called distituents , while CCM and CCM DMV do .", "Interestingly , the top 10 most frequently learned constituents by UMLDOP were exactly the same as by U DOP see the relevant table in Bod 2006 .", "We were also interested in testing UML DOP on longer sentences .", "We therefore included all WSJ sentences up to 40 words after removing empty elements and punctuation WSJ40 and again sampled 200 , 000 subtrees for each depth , using the same method as before .", "Furthermore , we compared UML DOP against a supervised binarized PCFG , i . e . a treebank PCFG whose simple relative frequency estimator corresponds to maximum likelihood Chi and Geman 1998 , and which we shall refer to as quot ; ML PCFG quot ; .", "To this end , we used a random 90 10 division of WSJ40 into a training set and a test set .", "The ML PCFG had thus access to the Penn WSJ trees in the training set , while UML DOP had to bootstrap all structure from the flat strings from the training set to next parse the 10 test set clearly a much more challenging task .", "Table 2 gives the results in terms of f scores .", "The table shows that UML DOP scores better than U DOP , also for WSJ40 .", "Our results on WSJ10 are somewhat lower than in table 1 due to the use of a smaller training set of 90 of the data .", "But the most surprising result is that UML DOP's fscore is higher than the supervised binarized treebank PCFG ML PCFG for both WSJ10 and WSJ40 .", "In order to check whether this difference is statistically significant , we additionally tested on 10 different 90 10 divisions of the WSJ40 which were the same splits as in Bod 2006 .", "For these splits , UML DOP achieved an average f score of 66 . 9 , while ML PCFG obtained an average f score of 64 . 7 .", "The difference in accuracy between UMLDOP and ML PCFG was statistically significant according to paired t testing p 0 . 05 .", "To the best of our knowledge this means that we have shown for the first time that an unsupervised parsing model UML DOP outperforms a widely used supervised parsing model a treebank PCFG on the WSJ40 .", "We should keep in mind that 1 a treebank PCFG is not state of the art its performance is mediocre compared to e . g .", "Bod 2003 or McClosky et al . 2006 , and 2 that our treebank PCFG is binarized as in Klein and Manning 2005 to make results comparable .", "To be sure , the unbinarized version of the treebank PCFG obtains 89 . 0 average f score on WSJ10 and 72 . 3 average f score on WSJ40 .", "Remember that the Penn Treebank annotations are often exceedingly flat , and many branches have arity larger than two .", "It would be interesting to see how UML DOP performs if we also accept ternary and wider branches though the total number of possible trees that can be assigned to strings would then further explode .", "UML DOP's performance still remains behind that of supervised binarized DOP parsers , such as DOP1 , which achieved 81 . 9 average fscore on the 10 WSJ40 splits , and ML DOP , which performed slightly better with 82 . 1 average fscore .", "And if DOP1 and ML DOP are not binarized , their average f scores are respectively 90 . 1 and 90 . 5 on WSJ40 .", "However , DOP1 and ML DOP heavily depend on annotated data whereas UML DOP only needs unannotated data .", "It would thus be interesting to see how close UML DOP can get to ML DOP's performance if we enlarge the amount of training data .", "7 Conclusion Is the end of supervised parsing in sight ?", "Now that we have outperformed a well known supervised parser by an unsupervised one , we may raise the question as to whether the end of supervised NLP comes in sight .", "All supervised parsers are reaching an asymptote and further improvement does not seem to come from more hand annotated data but by adding unsupervised or semi unsupervised techniques cf .", "McClosky et al . 2006 .", "Thus if we modify our question as does the exclusively supervised approach to parsing come to an end , we believe that the answer is certainly yes .", "Yet we should neither rule out the possibility that entirely unsupervised methods will in fact surpass semi supervised methods .", "The main problem is how to quantitatively compare these different parsers , as any evaluation on handannotated data like the Penn treebank will unreasonably favor semi supervised parsers .", "There is thus is a quest for designing an annotationindependent evaluation scheme .", "Since parsers are becoming increasingly important in applications like syntax based machine translation and structural language models for speech recognition , one way to go would be to compare these different parsing methods by isolating their contribution in improving a concrete NLP system , rather than by testing them against gold standard annotations which are inherently theory dependent .", "The initially disappointing results of inducing trees entirely from raw text was not so much due to the difficulty of the bootstrapping problem per se , but to 1 the poverty of the initial models and 2 the difficulty of finding theoryindependent evaluation criteria .", "The time has come to fully reappraise unsupervised parsing models which should be trained on massive amounts of data , and be evaluated in a concrete application .", "There is a final question as to how far the DOP approach to unsupervised parsing can be stretched .", "In principle we can assign all possible syntactic categories , semantic roles , argument structures etc . to a set of given sentences and let the statistics decide which assignments are most useful in parsing new sentences .", "Whether such a massively maximalist approach is feasible can only be answered by empirical investigation in due time .", "Thanks to Willem Zuidema , David Tugwell and especially to three anonymous reviewers whose unanymous suggestions on DOP and EM considerably improved the original paper .", "A substantial part of this research was carried out in the context of the NWO Exact project quot ; Unsupervised Stochastic Grammar Induction from Unlabeled Data quot ; , project number 612 . 066 . 405 ."], "summary_lines": ["An All-Subtrees Approach To Unsupervised Parsing\n", "We investigate generalizations of the all-subtrees \"DOP\" approach to unsupervised parsing.\n", "Unsupervised DOP models assign all possible binary trees to a set of sentences and next use (a large random subset of) all subtrees from these binary trees to compute the most probable parse trees.\n", "We will test both a relative frequency estimator for unsupervised DOP and a maximum likelihood estimator which is known to be statistically consistent.\n", "We report state-of-the-art results on English (WSJ), German (NEGRA) and Chinese (CTB) data.\n", "To the best of our knowledge this is the first paper which tests a maximum likelihood estimator for DOP on the Wall Street Journal, leading to the surprising result that an unsupervised parsing model beats a widely used supervised model (a treebank PCFG).\n", "We find that an unbinarized treebank grammar achieves an average 72.3% f-score on WSJ sentences ? 40 words, while the binarized version achieves only 64.6% f-score.\n"]}
{"article_lines": ["Characterizing Structural Descriptions Produced By Various Grammatical Formalisms", "We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate .", "In considering the relationship between formalisms , we show that it is useful to abstract away from the details of the formalism , and examine the nature of their derivation process as reflected by properties their trees . find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context Free Grammars On the basis of this observation , we describe a class of formalisms which we call Linear Context Free Rewriting Systems , and show they are recognizable in polynomial time and generate only semilinear languages .", "Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical formalism .", "Little attention , however , has been paid to the structural descriptions that these formalisms can assign to strings , i . e . their strong generative capacity .", "This aspect of the formalism is both linguistically and computationally important .", "For example , Gazdar 1985 discusses the applicability of Indexed Grammars IG's to Natural Language in terms of the structural descriptions assigned ; and Berwick 1984 discusses the strong generative capacity of Lexical Functional Grammar LFG and Government and Bindings grammars GB .", "The work of Thatcher 1973 and Rounds 1969 define formal systems that generate tree sets that are related to CFG's and IG's .", "We consider properties of the tree sets generated by CFG's , Tree Adjoining Grammars TAG's , Head Grammars HG's , Categorial Grammars CG's , and IG's .", "We examine both the complexity of the paths of trees in the tree sets , and the kinds of dependencies that the formalisms can impose between paths .", "These two properties of the tree sets are not only linguistically relevant , but also have computational importance .", "By considering derivation trees , and thus abstracting away from the details of the composition operation and the structures being manipulated , we are able to state the similarities and differences between the 'This work was partially supported by NSF grants MCS42 19116 CER , MCS82 07294 and DCR 84 10413 , ARO grant DAA 29 84 9 0027 , and DARPA grant N00014 85 K0018 .", "We are very grateful to Tony Kroc . h , Michael Pails , Sunil Shende , and Mark Steedman for valuable discussions . formalisms .", "It is striking that from this point of view many formalisms can be grouped together as having identically structured derivation tree sets .", "This suggests that by generalizing the notion of context freeness in CFG's , we can define a class of grammatical formalisms that manipulate more complex structures .", "In this paper , we outline how such family of formalisms can be defined , and show that like CFG's , each member possesses a number of desirable linguistic and computational properties in particular , the constant growth property and polynomial recognizability .", "From Thatcher's 1973 work , it is obvious that the complexity of the set of paths from root to frontier of trees in a local set the tree set of a CFG is regular' .", "We define the path set of a tree 1 as the set of strings that label a path from the root to frontier of 7 .", "The path set of a tree set is the union of the path sets of trees in that tree set .", "It can be easily shown from Thatcher's result that the path set of every local set is a regular set .", "As a result , CFG's can not provide the structural descriptions in which there are nested dependencies between symbols labelling a path .", "For example , CFG's cannot produce trees of the form shown in Figure 1 in which there are nested dependencies between S and NP nodes appearing on the spine of the tree .", "Gazdar 1985 argues this is the appropriate analysis of unbounded dependencies in the hypothetical Scandinavian language Norwedish .", "He also argues that paired English complementizers may also require structural descriptions whose path sets have nested dependencies .", "Head Grammars HG's , introduced by Pollard 1984 , is a formalism that manipulates headed strings i . e . , strings , one of whose symbols is distinguished as the head .", "Not only is concatenation of these strings possible , but head wrapping can be used to split a string and wrap it around another string .", "The productions of HG's are very similar to those of CFG's except that the operation used must be made explicit .", "Thus , the tree sets generated by HG's are similar to those of CFG's , with each node annotated by the operation concatenation or wrapping used to combine the headed strings derived by the daughters of Tree Adjoining Grammars , a tree rewriting formalism , was introduced by Joshi , Levy and Takahashi 1975 and Joshi 1983 85 .", "A TAG consists of a finite set of elementary trees that are either initial trees or auxiliary trees .", "Trees are composed using an operation called adjoining , which is defined as follows .", "Let n be some node labeled X in a tree y see Figure 3 .", "Let 71 be a tree with root and foot labeled by X .", "When 7' is adjoined at ? I in the tree 7 we obtain a tree v quot ; .", "The subtree under , ; is excised from 7 , the tree 7' is inserted in its place and the excised subtree is inserted below the foot of y' .", "It can be shown that the path set of the tree set generated by a TAG G is a context free language .", "TAG's can be used to give the structural descriptions discussed by Gazdar 1985 for the unbounded nested dependencies in Norwedish , for cross serial dependencies in Dutch subordinate clauses , and for the nestings of paired English complementizers .", "From the definition of TAG's , it follows that the choice of adjunction is not dependent on the history of the derivation .", "Like CFG's , the choice is predetermined by a finite number of rules encapsulated in the grammar .", "Thus , the derivation trees for TAG's have the same structure as local sets .", "As with HG's derivation structures are annotated ; in the case of TAG's , by the trees used for adjunction and addresses of nodes of the elementary tree where adjunctions occurred .", "We can define derivation trees inductively on the length of the derivation of a tree 1 .", "If 7 is an elementary tree , the derivation tree consists of a single node labeled 7 .", "Suppose y results from the adjunction of 71 , , y , at the k distinct tree addresses 141 , , nk in some elementary tree 7' , respectively .", "The tree denoting this derivation of 7 is rooted with a node labeled 7' having k subtrees for the derivations of 71 , . . . , 7a .", "The edge from the root to the subtree for the derivation of 7i is labeled by the address ni .", "To show that the derivation tree set of a TAG is a local set , nodes are labeled by pairs consisting of the name of an elementary tree and the address at which it was adjoined , instead of labelling edges with addresses .", "The following rule corresponds to the above derivation , where 71 , , 7k are derived from the auxiliary trees , , fik , respectively . for all addresses n in some elementary tree at which 7' can be adjoined .", "If 7' is an initial tree we do not include an address on the left hand side .", "There has been recent interest in the application of Indexed Grammars IG's to natural languages .", "Gazdar 1985 considers a number of linguistic analyses which IG's but not CFG's can make , for example , the Norwedish example shown in Figure 1 .", "The work of Rounds 1969 shows that the path sets of trees derived by IG's like those of TAG's are context free languages .", "Trees derived by IG's exhibit a property that is not exhibited by the trees sets derived by TAG's or CFG's .", "Informally , two or more paths can be dependent on each other for example , they could be required to be of equal length as in the trees in Figure 4 . generates such a tree set .", "We focus on this difference between the tree sets of CFG's and IG's , and formalize the notion of dependence between paths in a tree set in Section 3 .", "An IG can be viewed as a CFG in which each nonterminal is associated with a stack .", "Each production can push or pop symbols on the stack as can be seen in the following productions that generate tree of the form shown in Figure 4b .", "Gazdar 1985 argues that sharing of stacks can be used to give analyses for coordination .", "Analogous to the sharing of stacks in IC's , Lexical Functional Grammar's LFG's use the unification of unbounded hierarchical structures .", "Unification is used in LFG's to produce structures having two dependent spines of unbounded length as in Figure 5 .", "Bresnan , Kaplan , Peters , and Zaenen 1982 argue that these structures are needed to describe crossed serial dependencies in Dutch subordinate clauses .", "Gazdar 1985 considers a restriction of IG's in which no more than one nonterminal on the right hand side of a production can inherit the stack from the left hand side .", "Unbounded dependencies between branches are not possible in such a system .", "TAG's can be shown to be equivalent to this restricted system .", "Thus , TAG's can not give analyses in which dependencies between arbitrarily large branches exist .", "Steedman 1986 considers Categorial Grammars in which both the operations of function application and composition may be used , and in which function can specify whether they take their arguments from their right or left .", "While the generative power of CG's is greater that of CFG's , it appears to be highly constrained .", "Hence , their relationship to formalisms such as HG's and TAG's is of interest .", "On the one hand , the definition of composition in Steedman 1985 , which technically permits composition of functions with unbounded number of arguments , generates tree sets with dependent paths such as those shown in Figure 6 .", "This kind of dependency arises from the use of the composition operation to compose two arbitrarily large categories .", "This allows an unbounded amount of information about two separate paths e . g . an encoding of their length to be combined and used to influence the later derivation .", "A consequence of the ability to generate tree sets with this property is that CC's under this definition can generate the following language which can not be generated by either TAG's or HG's .", "0n0'i'i0'2 quot ; bin242bn I n 711 n2 On the other hand , no linguistic use is made of this general form of composition and Steedman personal communication and Steedman 1986 argues that a more limited definition of composition is more natural .", "With this restriction the resulting tree sets will have independent paths .", "The equivalence of CC's with this restriction to TAG's and HG's is , however , still an open problem .", "An extension of the TAG system was introduced by Joshi et al . 1975 and later redefined by Joshi 1987 in which the adjunction operation is defined on sets of elementary trees rather than single trees .", "A multicomponent Tree Adjoining Grammar MCTAG consists of a finite set of finite elementary tree sets .", "We must adjoin all trees in an auxiliary tree set together as a single step in the derivation .", "The adjunction operation with respect to tree sets multicomponent adjunction is defined as follows .", "Each member of a set of trees can be adjoined into distinct nodes of trees in a single elementary tree set , i . e , derivations always involve the adjunction of a derived auxiliary tree set into an elementary tree set .", "Lilo CFG's , TAG's , and HG's the derivation tree set of a MCTAG will be a local set .", "The derivation trees of a MCTAG are similar to those of a TAG .", "Instead of the names of elementary trees of a TAG , the nodes are labeled by a sequence of names of trees in an elementary tree set .", "Since trees in a tree set are adjoined together , the addressing scheme uses a sequence of pairings of the address and name of the elementary tree adjoined at that address .", "The following context free production captures the derivation step of the grammar shown in Figure 7 , in which the trees in the auxiliary tree set are adjoined into themselves at the root node address c .", "fii , Q2 , Pa , 01 , i32 , 03 , e , 02 , e Oa , en The path complexity of the tee set generated by a MCTAG is not necessarily context free .", "Like the string languages of MCTAG's , the complexity of the path set increases as the cardinality of the elementary tee sets increases , though both the string languages and path sets will always be semilinear .", "MCTAG's are able to generate tee sets having dependent paths .", "For example , the MCTAG shown in Figure 7 generates trees of the form shown in Figure 4b .", "The number of paths that can be dependent is bounded by the grammar in fact the maximum cardinality of a tree set determines this bound .", "Hence , trees shown in Figure 8 can not be generated by any MCTAG but can be generated by an IG because the number of pairs of dependent paths grows with n . Since the derivation tees of TAG's , MCTAG's , and HG's are local sets , the choice of the structure used at each point in a derivation in these systems does not depend on the context at that point within the derivation .", "Thus , as in CFG's , at any point in the derivation , the set of structures that can be applied is determined only by a finite set of rules encapsulated by the grammar .", "We characterize a class of formalisms that have this property in Section 4 .", "We loosely describe the class of all such systems as Linear Context Free Rewriting Formalisms .", "As is described in Section 4 , the property of having a derivation tree set that is a local set appears to be useful in showing important properties of the languages generated by the formalisms .", "The semilinearity of Tree Adjoining Languages TAL's , MCTAL's , and Head Languages HL's can be proved using this property , with suitable restrictions on the composition operations .", "Roughly speaking , we say that a tee set contains trees with dependent paths if there are two paths p . , vim . , and g . , in each 7 E r such that v . , is some , possibly empty , shared initial subpath ; v . , and wi are not bounded in length ; and there is some quot ; dependence quot ; such as equal length between the set of all v . , and w . , for each 7 Er .", "A tree set may be said to have dependencies between paths if some quot ; appropriate quot ; subset can be shown to have dependent paths as defined above .", "We attempt to formalize this notion in terms of the tee pumping lemma which can be used to show that a tee set does not have dependent paths .", "Thatcher 1973 describes a tee pumping lemma for recognizable sets related to the string pumping lemma for regular sets .", "The tee in Figure 9a can be denoted by t1 i223 where tee substitution is used instead of concatenation .", "The tee pumping lemma states that if there is tree , t 22 t2t3 , generated by a CFG G , whose height is more than a predetermined bound k , then all trees of the form ti tP3 for each i 0 will also generated by G as shown in Figure 9b .", "The string pumping lemma for CFG's uvwxy theorem can be seen as a corollary of this lemma . from this pumping lemma a single path can be pumped independently .", "For example , let us consider a tree set containing trees of the form shown in Figure 4a .", "The tree t2 must be on one of the two branches .", "Pumping t2 will change only one branch and leave the other branch unaffected .", "Hence , the resulting trees will no longer have two branches of equal size .", "We can give a tree pumping lemma for TAG's by adapting the uvwxy theorem for CFL's since the tree sets of TAG's have independent and context free paths .", "This pumping lemma states that if there is tree , t t2t3t4t5 , generated by a TAG G , such that its height is more than a predetermined bound k , then all trees of the form ti it tstt ts for each i 0 will also generated by G . Similarly , for tree sets with independent paths and more complex path sets , tree pumping lemmas can be given .", "We adapt the string pumping lemma for the class of languages corresponding to the complexity of the path set .", "A geometrical progression of language families defined by Weir 1987 involves tree sets with increasingly complex path sets .", "The independence of paths in the tree sets of the k tI grammatical formalism in this hierarchy can be shown by means of tree pumping lemma of the form t1ti3t .", ". t The path set of tree sets at level k 1 have the complexity of the string language of level k . The independence of paths in a tree set appears to be an important property .", "A formalism generating tree sets with complex path sets can still generate only semilinear languages if its tree sets have independent paths , and semilinear path sets .", "For example , the formalisms in the hierarchy described above generate semilinear languages although their path sets become increasingly more complex as one moves up the hierarchy .", "From the point of view of recognition , independent paths in the derivation structures suggests that a top down parser for example can work on each branch independently , which may lead to efficient parsing using an algorithm based on the Divide and Conquer technique .", "From the discussion so far it is clear that a number of formalisms involve some type of context free rewriting they have derivation trees that are local sets .", "Our goal is to define a class of formal systems , and show that any member of this class will possess certain attractive properties .", "In the remainder of the paper , we outline how a class of Linear Context Free Rewriting Systems LCFRS's may be defined and sketch how semilinearity and polynomial recognition of these systems follows .", "In defining LCFRS's , we hope to generalize the definition of CFG's to formalisms manipulating any structure , e . g . strings , trees , or graphs .", "To be a member of LCFRS a formalism must satisfy two restrictions .", "First , any grammar must involve a finite number of elementary structures , composed using a finite number of composition operations .", "These operations , as we see below , are restricted to be size preserving as in the case of concatenation in CFG which implies that they will be linear and non erasing .", "A second restriction on the formalisms is that choices during the derivation are independent of the context in the derivation .", "As will be obvious later , their derivation tree sets will be local sets as are those of CFG's .", "Each derivation of a grammar can be represented by a generalized context free derivation tree .", "These derivation trees show how the composition operations were used to derive the final structures from elementary structures .", "Nodes are annotated by the name of the composition operation used at that step in the derivation .", "As in the case of the derivation trees of CFG's , nodes are labeled by a member of some finite set of symbols perhaps only implicit in the grammar as in TAG's used to denote derived structures .", "Frontier nodes are annotated by zero arty functions corresponding to elementary structures .", "Each treelet an internal node with all its children represents the use of a rule that is encapsulated by the grammar The grammar encapsulates either explicitly or implicitly a finite number of rules that can be written as follows n 0 In the case of CFG's , for each production In the case of TAG's , a derivation step in which the derived trees RI , , On are adjoined into fi at rhe addresses in . would involve the use of the following rule2 .", "The composition operations in the case of CFG's are parameterized by the productions .", "In TAG's the elementary tree and addresses where adjunction takes place are used to instantiate the operation .", "To show that the derivation trees of any grammar in LCFRS is a local set , we can rewrite the annotated derivation trees such that every node is labelled by a pair to include the composition operations .", "These systems are similar to those described by Pollard 1984 as Generalized Context Free Grammars GCFG's .", "Unlike GCFG's , however , the composition operations of LCFRS's are restricted to be linear do not duplicate unboundedly large structures and nonerasing do not erase unbounded structures , a restriction made in most modern transformational grammars .", "These two restrictions impose the constraint that the result of composing any two structures should be a structure whose quot ; size quot ; is the sum of its constituents plus some constant For example , the operation 4 , discussed in the case of CFG's in Section 4 . 1 adds the constant equal to the sum of the length of the strings VI , un r Since we are considering formalisms with arbitrary structures it is difficult to precisely specify all of the restrictions on the composition operations that we believe would appropriately generalize the concatenation operation for the particular structures used by the formalism .", "In considering recognition of LCFRS's , we make further assumption concerning the contribution of each structure to the input string , and how the composition operations combine structures in this respect .", "We can show that languages generated by LCFRS's are semilinear as long as the composition operation does not remove any terminal symbols from its arguments .", "Semilinearity and the closely related constant growth property a consequence of semilinearity have been discussed in the context of grammars for natural languages by Joshi 1983 85 and Berwick and Weinberg 1984 .", "Roughly speaking , a language , L , has the property of semilinearity if the number of occurrences of each symbol in any string is a linear combination of the occurrences of these symbols in some fixed finite set of strings .", "Thus , the length of any string in L is a linear combination of the length of strings in some fixed finite subset of L , and thus L is said to have the constant growth property .", "Although this property is not structural , it depends on the structural property that sentences can be built from a finite set of clauses of bounded structure as noted by Joshi 1983 85 .", "The property of semilinearity is concerned only with the occurrence of symbols in strings and not their order .", "Thus , any language that is letter equivalent to a semilinear language is also semilinear .", "Two strings are letter equivalent if they contain equal number of occurrences of each terminal symbol , and two languages are letter equivalent if every string in one language is letter equivalent to a string in the other language and vice versa .", "Since every CFL is known to be semilinear Parikh , 1966 , in order to show semilinearity of some language , we need only show the existence of a letter equivalent CFL Our definition of LCFRS's insists that the composition operations are linear and nonerasing .", "Hence , the terminal symbols appearing in the structures that are composed are not lost though a constant number of new symbols may be introduced .", "If 0 A gives the number of occurrences of each terminal in the structure named by A , then , given the constraints imposed on the formalism , for each rule A . fp Ai , , An we have the equality where c is some constant .", "We can obtain a letter equivalent CFL defined by a CFG in which the for each rule as above , we have the production A A1 Anup where tk up cp .", "Thus , the language generated by a grammar of a LCFRS is semilinear .", "We now turn our attention to the recognition of string languages generated by these formalisms LCFRL's .", "As suggested at the end of Section 3 , the restrictions that have been specified in the definition of LCFRS's suggest that they can be efficiently recognized .", "In this section for the purposes of showing that polynomial time recognition is possible , we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input .", "Since each composition operation is linear and nonerasing , a bounded sequences of substrings associated with the resulting structure is obtained by combining the substrings in each of its arguments using only the concatenation operation , including each substring exactly once .", "CFG's , TAG's , MCTAG's and HG's are all members of this class since they satisfy these restrictions .", "Giving a recognition algorithm for LCFRL's involves describing the substrings of the input that are spanned by the structures derived by the LCFRS's and how the composition operation combines these substrings .", "For example , in TAG's a derived auxiliary tree spans two substrings to the left and right of the foot node , and the adjunction operation inserts another substring spanned by the subtree under the node where adjunction takes place between them see Figure 3 .", "We can represent any derived tree of a TAG by the two substrings that appear in its frontier , and then define how the adjunction operation concatenates the substrings .", "Similarly , for all the LCFRS's , discussed in Section 2 , we can define the relationship between a structure and the sequence of substrings it spans , and the effect of the composition operations on sequences of substrings .", "A derived structure will be mapped onto a sequence zi of substrings not necessarily contiguous in the input , and the composition operations will be mapped onto functions that can defined as follows3 . f zi , , zni , m . , , Yn3 Z1 , , Zn3 where each z , is the concatenation of strings from z , 's and yk's .", "The linear and nonerasing assumptions about the operations discussed in Section 4 . 1 require that each z , and yk is used exactly once to define the strings zi , , z1 , 3 .", "Some of the operations will be constant functions , corresponding to elementary structures , and will be written as f zi , where each z , is a constant , the string of terminal symbols al an , , , .", "This representation of structures by substrings and the composition operation by its effect on substrings is related to the work of Rounds 1985 .", "Although embedding this version of LCFRS's in the framework of ILFP developed by Rounds 1985 is straightforward , our motivation was to capture properties shared by a family of grammatical systems and generalize them defining a class of related formalisms .", "This class of formalisms have the properties that their derivation trees are local sets , and manipulate objects , using a finite number of composition operations that use a finite number of symbols .", "With the additional assumptions , inspired by Rounds 1985 , we can show that members of this class can be recognized in polynomial time .", "We use Alternating Turing Machines Chandra , Kozen , and Stockmeyer , 1981 to show that polynomial time recognition is possible for the languages discussed in Section 4 . 3 .", "An ATM has two types of states , existential and universal .", "In an existential state an ATM behaves like a nondeterministic TM , accepting if one of the applicable moves leads to acceptance ; in an universal state the ATM accepts if all the applicable moves lead to acceptance .", "An ATM may be thought of as spawning independent processes for each applicable move .", "A k tape ATM , M , has a read only input tape and k read write work tapes .", "A step of an ATM consists of reading a symbol from each tape and optionally moving each head to the left or right one tape cell .", "A configuration of M consists of a state of the finite control , the nonblank contents of the input tape and k work tapes , and the position of each head .", "The space of a configuration is the sum of the lengths of the nonblank tape contents of the k work tapes .", "M works in space S n if for every string that M accepts no configuration exceeds space S n .", "It has been shown in Chandra et al . , 1981 that if M works in space log n then there is a deterministic TM which accepts the same language in polynomial time .", "In the next section , we show how an ATM can accept the strings generated by a grammar in a LCFRS formalism in logspace , and hence show that each family can be recognized in polynomial time .", "We define an ATM , M , recognizing a language generated by a grammar , G , having the properties discussed in Section 43 .", "It can be seen that M performs a top down recognition of the input al . . . nin logspace .", "The rewrite rules and the definition of the composition operations may be stored in the finite state control since G uses a finite number of them .", "Suppose M has to determine whether the k substrings , . . . , ak can be derived from some symbol A .", "Since each zi is a contiguous substring of the input say ai , , and no two substrings overlap , we can represent zi by the pair of integers i2 , i2 .", "We assume that M is in an existential state qA , with integers i1 and i2 representing zi in the 2i 1 th and 22th work tape , for 1 i k . For each rule p A fp B , C such that fp is mapped onto the function fp defined by the following rule . jp xi , . . , rnt , 1ii , Yn3 Zi , , Zk M breaks xi , zk into substrings xi , , xn , and yi , . . . , y quot ; conforming to the definition of fp .", "M spawns as many processes as there are ways of breaking up ri , . . , zt , and rules with A on their left hand side .", "Each spawned process must check if xi , , xn , and , yn , can be derived from B and C , respectively .", "To do this , the x's and y's are stored in the next 2ni 2n2 tapes , and M goes to a universal state .", "Two processes are spawned requiring B to derive z , . . , and C to derive yi , , y , .", "Thus , for example , one successor process will be have M to be in the existential state qa with the indices encoding xi , , xn , in the first 2n i tapes .", "For rules p A fpo such that fp is constant function , giving an elementary structure , fp is defined such that fp Si . . . xi where each z is a constant string .", "M must enter a universal state and check that each of the k constant substrings are in the appropriate place as determined by the contents of the first 2k work tapes on the input tape .", "In addition to the tapes required to store the indices , M requires one work tape for splitting the substrings .", "Thus , the ATM has no more than 6km quot ; 1 work tapes , where km quot ; is the maximum number of substrings spanned by a derived structure .", "Since the work tapes store integers which can be written in binary that never exceed the size of the input , no configuration has space exceeding 0 log n .", "Thus , M works in logspace and recognition can be done on a deterministic TM in polynomial tape .", "We have studied the structural descriptions tree sets that can be assigned by various grammatical systems , and classified these formalisms on the basis of two features path complexity ; and path independence .", "We contrasted formalisms such as CFG's , HG's , TAG's and MCTAG's , with formalisms such as IG's and unificational systems such as LFG's and FUG's .", "We address the question of whether or not a formalism can generate only structural descriptions with independent paths .", "This property reflects an important aspect of the underlying linguistic theory associated with the formalism .", "In a grammar which generates independent paths the derivations of sibling constituents can not share an unbounded amount of information .", "The importance of this property becomes clear in contrasting theories underlying GPSG Gazdar , Klein , Pulluna , and Sag , 1985 , and GB as described by Berwick , 1984 with those underlying LFG and FUG .", "It is interesting to note , however , that the ability to produce a bounded number of dependent paths where two dependent paths can share an unbounded amount of information does not require machinery as powerful as that used in LFG , FUG and IG's .", "As illustrated by MCTAG's , it is possible for a formalism to give tree sets with bounded dependent paths while still sharing the constrained rewriting properties of CFG's , HG's , and TAG's .", "In order to observe the similarity between these constrained systems , it is crucial to abstract away from the details of the structures and operations used by the system .", "The similarities become apparent when they are studied at the level of derivation structures derivation nee sets of CFG's , HG's , TAG's , and MCTAG's are all local sets .", "Independence of paths at this level reflects context freeness of rewriting and suggests why they can be recognized efficiently .", "As suggested in Section 4 . 3 . 2 , a derivation with independent paths can be divided into subcomputations with limited sharing of information .", "We outlined the definition of a family of constrained grammatical formalisms , called Linear Context Free Rewriting Systems .", "This family represents an attempt to generalize the properties shared by CFG's , HG's , TAG's , and MCTAG's .", "Like HG's , TAG's , and MCTAG's , members of LCFRS can manipulate structures more complex than terminal strings and use composition operations that are more complex that concatenation .", "We place certain restrictions on the composition operations of LCFRS's , restrictions that are shared by the composition operations of the constrained grammatical systems that we have considered .", "The operations must be linear and nonerasing , i . e . , they can not duplicate or erase structure from their arguments .", "Notice that even though IG's and LFG's involve CFG like productions , they are linguistically fundamentally different from CFG's because the composition operations need not be linear .", "By sharing stacks in IG's or by using nonlinear equations over f structures in FUG's and LFG's , structures with unbounded dependencies between paths can be generated .", "LCFRS's share several properties possessed by the class of mildly context sensitive formalisms discussed by Joshi 1983 85 .", "The results described in this paper suggest a characterization of mild context sensitivity in terms of generalized context freeness .", "Having defined LCFRS's , in Section 4 . 2 we established the semilinearity and hence constant growth property of the languages generated .", "In considering the recognition of these languages , we were forced to be more specific regarding the relationship between the structures derived by these formalisms and the substrings they span .", "We insisted that each structure dominates a bounded number of not necessarily adjacent substrings .", "The composition operations are mapped onto operations that use concatenation to define the substrings spanned by the resulting structures .", "We showed that any system defined in this way can be recognized in polynomial time .", "Members of LCFRS whose operations have this property can be translated into the ILFP notation Rounds , 1985 .", "However , in order to capture the properties of various grammatical systems under consideration , our notation is more restrictive that ILFP , which was designed as a general logical notation to characterize the complete class of languages that are recognizable in polynomial time .", "It is known that CFG's , HG's , and TAG's can be recognized in polynomial time since polynomial time algorithms exist in for each of these formalisms .", "A corollary of the result of Section 4 . 3 is that polynomial time recognition of MCTAG's is possible .", "As discussed in Section 3 , independent paths in tree sets , rather than the path complexity , may be crucial in characterizing semilinearity and polynomial time recognition .", "We would like to relax somewhat the constraint on the path complexity of formalisms in LCFRS .", "Formalisms such as the restricted indexed grammars Gazdar , 1985 and members of the hierarchy of grammatical systems given by Weir 1987 have independent paths , but more complex path sets .", "Since these path sets are semilinear , the property of independent paths in their tree sets is sufficient to cause semilinearity of the languages generated by them .", "In addition , the restricted version of CG's discussed in Section 6 generates tree sets with independent paths and we hope that it can be included in a more general definition of LCFRS's containing formalisms whose tree sets have path sets that are themselves LCFRL's as in the case of the restricted indexed grammars , and the hierarchy defined by Weir .", "LCFRS's have only been loosely defined in this paper ; we have yet to provide a complete set of formal properties associated with members of this class .", "In this paper , our goal has been to use the notion of LCFRS's to classify grammatical systems on the basis of their strong generative capacity .", "In considering this aspect of a formalism , we hope to better understand the relationship between the structural descriptions generated by the grammars of a formalism , and the properties of semilinearity and polynomial recognizability ."], "summary_lines": ["Characterizing Structural Descriptions Produced By Various Grammatical Formalisms\n", "We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate.\n", "In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties of their derivation trees.\n", "We find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammar.\n", "On the basis of this observation, we describe a class of formalisms which we call Linear Context-Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.\n", "We introduce Linear context-free rewriting system (LCFRS), which is a mildly context-sensitive formalism that allows the derivation of tuples of strings, i.e., discontinuous phrases.\n"]}
{"article_lines": ["Lexical Semantic Techniques For Corpus Analysis", "In this paper we outline a research program for computational linguistics , making extensive use of text corpora .", "We demonstrate how a semantic framework for lexical knowledge can suggest richer relationships among words in text beyond that of simple co occurrence .", "The work suggests how linguistic phenomena such as metonymy and polysemy might be exploitable for semantic tagging of lexical items .", "Unlike with purely statistical collocational analyses , the framework of a semantic theory allows the automatic construction of predictions about deeper semantic among words appearing in systems . illustrate the approach for the acquisition of lexical information for several classes of nominals , and how such techniques can fine tune the lexical structures acquired from an initial seeding of a machine readable dictionary .", "In addition to conventional lexical semantic relations , we show how information concerning lexical presuppositions and preference relations can also be acquired from corpora , when analyzed with the appropriate semantic tools .", "Finally , we discuss the potential that corpus studies have for enriching the data set for theoretical linguistic research , as well as helping to confirm or disconfirm linguistic hypotheses .", "In this paper we outline a research program for computational linguistics , making extensive use of text corpora .", "We demonstrate how a semantic framework for lexical knowledge can suggest richer relationships among words in text beyond that of simple co occurrence .", "The work suggests how linguistic phenomena such as metonymy and polysemy might be exploitable for semantic tagging of lexical items .", "Unlike with purely statistical collocational analyses , the framework of a semantic theory allows the automatic construction of predictions about deeper semantic relationships among words appearing in collocational systems .", "We illustrate the approach for the acquisition of lexical information for several classes of nominals , and how such techniques can fine tune the lexical structures acquired from an initial seeding of a machine readable dictionary .", "In addition to conventional lexical semantic relations , we show how information concerning lexical presuppositions and preference relations can also be acquired from corpora , when analyzed with the appropriate semantic tools .", "Finally , we discuss the potential that corpus studies have for enriching the data set for theoretical linguistic research , as well as helping to confirm or disconfirm linguistic hypotheses .", "The proliferation of on line textual information poses an interesting challenge to linguistic researchers for several reasons .", "First , it provides the linguist with sentence and word usage information that has been difficult to collect and consequently largely ignored by linguists .", "Second , it has intensified the search for efficient automated indexing and retrieval techniques .", "Full text indexing , in which all the content words in a document are used as keywords , is one of the most promising of recent automated approaches , yet its mediocre precision and recall characteristics indicate that there is much room for improvement Croft 1989 .", "The use of domain knowledge can enhance the effectiveness of a full text system by providing related terms that can be used to broaden , narrow , or refocus a query at retrieval time Debili , Fluhr , and Radasua 1988 ; Anick et al . 1989 .", "Likewise , domain knowledge may be applied at indexing time to do word sense disambiguation Krovetz and Croft 1989 or content analysis Jacobs 1991 .", "Unfortunately , for many domains , such knowledge , even in the form of a thesaurus , is either not available or is incomplete with respect to the vocabulary of the texts indexed .", "In this paper we examine how linguistic phenomena such as metonymy and polysemy might be exploited for the semantic tagging of lexical items .", "Unlike purely statistical collocational analyses , employing a semantic theory allows for the automatic construction of deeper semantic relationships among words appearing in collocational systems .", "We illustrate the approach for the acquisition of lexical information for several classes of nominals , and how such techniques can fine tune the lexical structures acquired from an initial seeding of a machine readable dictionary .", "In addition to conventional lexical semantic relations , we show how information concerning lexical presuppositions and preference relations Wilks 1978 can also be acquired from corpora , when analyzed with the appropriate semantic tools .", "Finally , we discuss the potential that corpus studies have for enriching the data set for theoretical linguistic research , as well as helping to confirm or disconfirm linguistic hypotheses .", "The aim of our research is to discover what kinds of knowledge can be reliably acquired through the use of these methods , exploiting , as they do , general linguistic knowledge rather than domain knowledge .", "In this respect , our program is similar to Zernik's 1989 work on extracting verb semantics from corpora using lexical categories .", "Our research , however , differs in two respects first , we employ a more expressive lexical semantics ; second , our focus is on all major categories in the language , and not just verbs .", "This is important since for full text information retrieval , information about nominals is paramount , as most queries tend to be expressed as conjunctions of nouns .", "From a theoretical perspective , we believe that the contribution of the lexical semantics of nominals to the overall structure of the lexicon has been somewhat neglected , relative to that of verbs .", "While Zernik 1989 presents ambiguity and metonymy as a potential obstacle to effective corpus analysis , we believe that the existence of motivated metonymic structures actually provides valuable clues for semantic analysis of nouns in a corpus .", "We will assume , for this paper , the general framework of a generative lexicon as outlined in Pustejovsky 1991 .", "In particular , we make use of the principles of type coercion and qualia structure .", "This model of semantic knowledge associated with words is based on a system of generative devices that is able to recursively define new word senses for lexical items in the language .", "These devices and the associated dictionary make up a generative lexicon , where semantic information is distributed throughout the lexicon to all categories .", "The general framework assumes four basic levels of semantic description argument structure , qualia structure , lexical inheritance structure , and event structure .", "Connecting these different levels is a set of generative devices that provide for the compositional interpretation of words in context .", "The most important of these devices is a semantic transformation called type coercion analogous to coercion in programming languages which captures the semantic relatedness between syntactically distinct expressions .", "As an operation on types within a A calculus , type coercion can be seen as transforming a monomorphic language into one with polymorphic types cf .", "Cardelli and Wegner 1985 .", "Argument , event , and qualia types must conform to the well formedness conditions defined by the type system defined by the lexical inheritance structure when undergoing operations of semantic composition . '", "One component of this approach , the qualia structure , specifies the different aspects of a word's meaning through the use of subtyping .", "These include the subtypes CONSTITUTIVE , FORMAL , TELIC , and AGENTIVE .", "To illustrate how these are used , the qualia structure for book is given below . 2 This structured representation allows one to use the same lexical entry in different contexts , where the word refers to different qualia of the noun's denotation .", "For example , the sentences in 1 3 below refer to different aspects or qualia of the general meaning of book . 3 Example 1 This book weighs four ounces .", "Example 2 John finished a book .", "This is an interesting book .", "Example 1 makes reference to the formal role , while 3 refers to the constitutive role .", "Example 2 , however , can refer to either the telic or the agentive aspects given above .", "The utility of such knowledge for information retrieval is readily apparent .", "This theory claims that noun meanings should make reference to related concepts and the relations into which they enter .", "The qualia structure , thus , can be viewed as a kind of generic template for structuring this knowledge .", "Such information about how nouns relate to other lexical items and their concepts might prove to be much more useful in full text information retrieval than what has come from standard statistical techniques .", "To illustrate how such semantic structuring might be useful , consider the general class of artifact nouns .", "A generative view of the lexicon predicts that by classifying an element into a particular category , we can generate many aspects of its semantic structure , and hence , its syntactic behavior .", "For example , the representation above for book refers to several word senses , all of which are logically related by the semantic template for an artifactual object .", "That is , it contains information , it has a material extension , it serves some function , and it is created by some particular act or event .", "In the qualia structures given below , we adopt the convention that a , 0 denotes conjunction of formulas within the feature structure , while a ; 0 will denote disjunction .", "Such an analysis allows us to minimally structure objects according to these four qualia .", "As an example of how objects cluster according to these dimensions , we will briefly consider three object types 1 containers of information , e . g . , book , tape , record ; 2 instruments , e . g . , gun , hammer , paintbrush ; and 3 figure ground objects , e . g . , door , room , fireplace .", "Because of how their qualia structures differ , these classes appear in vastly different grammatical contexts .", "As with containers in general , information containers permit metonymic extensions between the container and the material contained within it .", "Collocations such as those in Examples 4 through 7 indicate that this metonymy is grammaticalized through specific and systematic head PP constructions . read the information on the tape Instruments , on the other hand , display classic agent instrument causative alternations , such as those in Examples 8 through 11 cf .", "Fillmore 1968 ; Lakoff 1968 , 1970 .", ". . . smash the vase with the hammer The hammer smashed the vase .", ". . . kill him with a gun The gun killed him .", "Finally , figure ground nominals Pustejovsky and Anick 1988 permit perspective shifts such as those in Examples 12 through 15 .", "These are nouns that refer to physical objects as well as the specific enclosure or aperture associated with it .", "John painted the door .", "John walked through the door .", "John is scrubbing the fireplace .", "The smoke filled the fireplace .", "That is , paint and scrub are actions on physical objects while walk through and fi are processes in spaces .", "These collocational patterns , we argue , are systematically predictable from the lexical semantics of the noun , and we term such sets of collocated structures lexical conceptual paradigms LCPs . 4 To make this point clearer , let us consider a specific example of an LCP from the computer science domain , namely for the noun tape .", "Because of the particular metonymy observed for a noun like tape , we will classify it as belonging to the container containee LCP .", "This general class is represented as follows , where P and Q are predicate variables 5 The LCP is a generic qualia structure that captures not only the semantic relationship between arguments types of a relation , but also , through corpus tuning , the collocation relations that realize these roles .", "The telic function of a container , for example , is the relation hold , but this underspecifies which spatial prepositions would adequately satisfy this semantic relation e . g . in , on , inside , etc . .", "In this view , a noun such as tape would have the following qualia structure This states that a tape is an quot ; information container quot ; that is also a two dimensional physical object , where the information is written onto the object . '", "With such nouns , a logical metonymy exists as the result of type coercion , when the logical argument of a semantic type , which is selected by a function of some sort , denotes the semantic type itself .", "Thus , in this example , the type selected for by a verb such as read refers to the quot ; information quot ; argument for tape , while a verb such as carry would select for the quot ; physical object quot ; argument .", "They are , however , logically related , since the noun itself denotes a relation .", "The representation above simply states that any semantics for tape must logically make reference to the object itself formal , what it can contain const , what purpose it serves telic , and how it arises agentive .", "This provides us with a semantic representation that can capture the multiple perspectives a single lexical item may assume in different contexts .", "Yet , the qualia for a lexical item such as tape are not isolated values for that one word , but are integrated into a global knowledge base indicating how these senses relate to other lexical items and their senses .", "This is the contribution of inheritance and the hierarchical structuring of knowledge cf .", "Evans and Gazdar 1990 ; Copestake and Briscoe 1992 ; Russell et al . 1992 .", "In Pustejovsky 1991 it is suggested that there are two types of relational structures for lexical knowledge ; a fixed inheritance similar to that of an is a hierarchy cf .", "Touretzky 1986 ; and a dynamic structure that operates generatively from the qualia structure of a lexical item to create a relational structure for ad hoc categories . '", "Reviewing briefly , the basic idea is that semantics allows for the dynamic creation of arbitrary concepts through the application of certain transformations to lexical meanings .", "Thus for every predicate , Q , we can generate its opposition , Q .", "Similarly , these two predicates can be related temporally to generate the transition events defining this opposition .", "These operations include but may not be limited to negation ; , temporal precedence ; , temporal succession ; , temporal equivalence ; and act , an operator adding agency to an argument .", "We will call the concept space generated by these operations the Projective Conclusion Space of a specific quale for a lexical item .", "To return to the example of tape above , the predicates read and copy are related to the telic value by just such an operation , while predicates such as mount and dismount i . e . unmount are related to the formal role .", "Following the previous discussion , with mounted as the predicate Q , successive applications of the negation and temporal precedence operators derives the transition verbs mount and dismount . '", "We return to a discussion of this in Section 3 , and to how this space relates to statistically significant collocations in text .", "It is our view that the approach outlined above for representing lexical knowledge can be put to use in the service of information retrieval tasks .", "In this respect , our proposal can be compared to attempts at object classification in information science .", "One approach , known as faceted classification Vickery 1975 proceeds roughly as follows collect all terms lying within a field ; then group the terms into facets by assigning them to categories .", "Typical examples of this are state , property , reaction , and device .", "However , each subject area is likely to have its own sets of categories , which makes it difficult to re use a set of facet classifications . 9 Even if the relational information provided by the qualia structure and inheritance would improve performance in information retrieval tasks , one problem still remains , namely that it would be very time consuming to hand code such structures for all nouns in a domain .", "Since it is our belief that such representations are generic structures across all domains , it is our long term goal to develop methods for automatically extracting these relations and values from on line corpora .", "In the sections that follow , we describe several experiments indicating that the qualia structures do , in fact , correlate with well behaved collocational patterns , thereby allowing us to perform structure matching operations over corpora to find these relations .", "In this section we discuss briefly how a lexical semantic theory can help in extracting information from machine readable dictionaries MRDs .", "We describe research on conversion of a machine tractable dictionary Wilks et al . 1993 into a usable lexical knowledge base Boguraev 1991 .", "Although the results here are preliminary , it is important to mention the process of converting an MRD into a lexical knowledge base , so that the process of corpus tuning is put into the proper perspective .", "The initial seeding of lexical structures is being done independently both from the Oxford Advanced Learners Dictionary OALD and from lexical entries in the Longman Dictionary of Contemporary English Procter , Ilson , and Ayto 1978 .", "These are then automatically adapted to the format of generative lexical structures .", "It is these lexical structures that are then statistically tuned against the corpus , following the methods outlined in Anick and Pustejovsky 1990 and Pustejovsky 1992 .", "Previous work by Amsler 1980 , Calzolari 1984 , Chodorow , Byrd , and Heidorn 1985 , Byrd et al . 1987 , Markowitz , Ahlswede , and Evens 1986 , and Nakamura and Nagao 1988 showed that taxonomic information and certain semantic relations can be extracted from MRDs using fairly simple techniques .", "Later work by Veronis and Ide 1991 , Klavans , Chodorow , and Wacholder 1990 , and Wilks et al . 1992 provides us with a number of techniques for transfering information from MRDs to a representation language such as that described in the previous section .", "Our goal is to automate , to the extent possible , the initial construction of these structures .", "Extensive research has been done on the kind of information needed by natural language programs and on the representation of that information Wang , Vandendorpe , and Evens 1985 ; Ahlswede and Evens 1988 .", "Following Boguraev et al . 1989 and Wilks et al . of 1989 , we believe that much of what is needed for NLP lexicons can be found either explicitly or implicitly in a dictionary , and empirical evidence suggests that this information gives rise to a sufficiently rich lexical representation for use in extracting information from texts .", "Techniques for identifying explicit information in machine readable dictionaries have been developed by many researchers Boguraev et al . 1989 ; Slator 1988 ; Slator and Wilks 1987 ; Guthrie et al .", "1990 and are well understood .", "Many properties of a word sense or the semantic relationships between word senses are available in MRDs , but this information can only be identified computationally through some analysis of the definition text of an entry Atkins 1991 .", "Some research has already been done in this area .", "Alshawi 1987 , Boguraev et al . 1989 , Vossen , Meijs , and den Broeder 1989 , and the work described in Wilks et al .", "1992 have made explicit some kinds of implicit information found in MRDs .", "Here we propose to refine and merge some of the previous techniques to make explicit the implicit information specified by a theory of generative lexicons .", "Given what we described above for the lexical structures for nominals , we can identify these semantic relations in the OALD and LDOCE by pattern matching on the parse trees of definitions .", "To illustrate what specific information can be derived by automatic seeding from machine readable dictionaries , consider the following examples . 1 For example , the LDOCE definition for book is quot ; a collection of sheets of paper fastened together as a thing to be read , or to be written in quot ; while the OALD provides a somewhat different definition quot ; number of sheet of papers , either printed or blank , fastened together in a cover . quot ; Note that both definitions are close to , but not identical to the information structure suggested in the previous section , using a qualia structure for nominals .", "LDOCE suggests write in rather than write as the value for the telic role , while the OALD suggests nothing for this role .", "Furthermore , although the physical contents of a book as quot ; a collection of sheets of paper quot ; is mentioned , nowhere is information made reference to in the definition .", "When the dictionary fails to provide the value for a semantic role , the information must be either hand entered or the lexical structure must be tuned against a large corpus , in the hope of extracting such features automatically .", "We turn to this issue in the next two sections .", "Although the two dictionaries differ in substantial respects , it is remarkable how systematic the definition structures are for extracting semantic information , if there is a clear idea how this information should be structured .", "For example , from the following OALD definition for cigarette , cigarette n roll of shredded tobacco enclosed in thin paper for smoking . the initial lexical structure below is generated .", "Parsing the LDOCE entry for the same noun results in a different lexical structure cigarette n finely cut shredded tobacco rolled in a narrow tube of thin paper for smoking . gls cigarette , syn type n , One obvious problem with the above representation is that there is no information indicating how the word being defined binds to the relations in the qualia .", "Currently , subsequent routines providing for argument binding analyze the relational structure for particular aspects of noun meaning , giving us a lexical structure fairly close to what we need for representation and retrieval purposes , although the result is in no way ideal or uniform over all nominal forms .", "cf .", "Cowie , Guthrie , and Pustejovsky 1992 for details of this operation on LDOCE .", "quot ; In a related set of experiments performed while constructing a large lexical database for data extraction purposes , we seeded a lexicon with 6000 verbs from LDOCE .", "This process and the corpus tuning for both argument typing and subcategorization acquisition are described in Cowie , Guthrie , and Pustejovsky 1992 and Pustejovsky et al . 1992 .", "In summary , based on a theory of lexical semantics , we have discussed how an MRD can be useful as a corpus for automatically seeding lexical structures .", "Rather than addressing the specific problems inherent in converting MRDs into useful lexicons , we have emphasized how it provides us , in a sense , with a generic vocabulary from which to begin lexical acquisition over corpora .", "In the next section , we will address the problem of taking these initial , and often very incomplete lexical structures , and enriching them with information acquired from corpus analysis .", "As mentioned in the previous section , the power of a generative lexicon is that it takes much of the burden of semantic interpretation off of the verbal system by supplying a much richer semantics for nouns and adjectives .", "This makes the lexical structures ideal as an initial representation for knowledge acquisition and subsequent information retrieval tasks .", "A machine readable dictionary provides the raw material from which to construct computationally useful representations of the generic vocabulary contained within it .", "The lexical structures discussed in the previous section are one example of how such information can be exploited .", "Many sublanguages , however , are poorly represented in on line dictionaries , if represented at all .", "Vocabularies geared to specialized domains will be necessary for many applications , such as text categorization and information retrieval .", "The second area of our research program that we discuss is aimed at developing techniques for building sublanguage lexicons via syntactic and statistical corpus analysis coupled with analytic techniques based on the tenets of generative lexicon theory .", "To understand fully the experiments described in the next two sections , we will refer to several semantic notions introduced in previous sections .", "These include type coercion , where a lexical item requires a specific type specification for its argument , and 11 As one reviewer correctly pointed out , more than simple argument binding is involved here .", "For example , the model must know that paper can enclose shredded tobacco , but not the reverse .", "Such information , typically part of commonsense knowledge , is well outside the domain of lexical semantics , as envisioned here .", "One approach to this problem , consistent with our methodology , is to examine the corpus and the collocations that result from training on specific qualia relations .", "Further work will hopefully clarify the nature of this problem , and whether it is best treated lexically or not . the argument is able to change type accordingly this explains the behavior of logical metonymy and the syntactic variation seen in complements to verbs and nominals ; and cospecification , a semantic tagging of what collocational patterns the lexical item may enter into .", "Metonymy , in this view , can be seen as a case of the quot ; licensed violation quot ; of selectional restrictions .", "For example , while the verb announce selects for a human subject , sentences like The Dow Corporation announced third quarter losses are not only an acceptable paraphrase of the selectionally correct form Mr . Dow Jr . announced third quarter losses for Dow Corp , but they are the preferred form in the corpora being examined .", "This is an example of subject type coercion , where the semantics for Dow Corp as a company must specify that there is a human typically associated with such official pronouncements see Section 5 . 12 For one set of experiments , we used a corpus of approximately 3 , 000 articles written by Digital Equipment Corporation's Customer Support Specialists for an on line computer troubleshooting library .", "The articles , each one to two page long descriptions of a problem and its solution , comprise about 1 million words .", "Our analysis proceeds in two phases .", "In the first phase , we pre process the corpus to build a database of phrasal relationships .", "This consists briefly of the following steps indicators .", "Any words that are ambiguous with respect to category are disambiguated according to a set of several dozen ordered disambiguation heuristics , which choose a category based on the categories of the words immediately preceding and following the ambiguous term . transitions , to indicate likely phrase boundaries .", "No attempt is made to construct a full parse tree or resolve prepositional phrase attachment , conjunction scoping , etc .", "A concordance is constructed , identifying , for each word appearing in the corpus , the set of sentences , phrases , and phrase locations in which the word appears .", "12 Within the current framework , a distinction is made between logical metonymy , where the metonymic extension or relation is transparent from the lexical semantics of the coerced phrase , and conventional metonymy , where the relation may not be directly calculated from information provided grammatically .", "For example , in the sentence quot ; The Boston office called today , quot ; it is not clear from logical metonymy what relation Boston bears to office other than location ; i . e . , it is not obvious that it is a branch office .", "This is well beyond lexical semantics cf .", "Lakoff 1987 and Martin 1990 .", "The database of partially parsed sentences provides the raw material for a number of sublanguage analyses .", "This begins the second phase of analysis querying and thesaurus browsing .", "We construct bracketed noun compounds from our database of partial parses in a two step process .", "The first simply searches the corpus for recurring contiguous sequences of nouns .", "Then , to bracket each compound that includes more than two nouns , we test whether possible subcomponents of the phrase exist on their own as complete noun compounds elsewhere in the corpus .", "Sample bracketed compounds derived from the computer troubleshooting database include syst em management utility , TK50 tape drive , database management system .", "Generation of taxonomic relationships on the basis of collocational information .", "Technical sublanguages often express subclass relationships in noun compounds of the form instance name class name , as in quot ; Unix operating system quot ; and quot ; C language . quot ; Unfortunately , noun compounds are also employed to express numerous other relationships , as in quot ; Unix kernel quot ; and quot ; C debugger . quot ; We have found , however , that collocational evidence can be employed to suggest which noun compounds reflect taxonomic relationships , using a strategy similar to that employed by Hindle 1990 for detecting synonyms .", "Given a term T , we extract from the phrase database those nouns N , that appear as the head of any phrase in which T is the immediately preceding term .", "These nouns represent candidate classes of which T may be a member .", "We then generate the set of verbs that take T as direct object and calculate the mutual information value for each verb T collocation cf .", "Hindle 1990 .", "We do the same for each noun N . Under the assumption that instance and class nouns are likely to co occur with the same verbs , we compute a similarity score between T and each noun N by summing the product of the mutual information values for those verbs occurring with both nouns .", "Verbs with negative mutual information values are left out of the calculation .", "The noun with the highest similarity score is often the class of which T is an instance , as illustrated by the sample results in Figure 1 .", "For each word displayed in Figure 1 , its quot ; class quot ; is the head noun with the highest similarity score .", "Other head nouns occurring with the word as modifier are listed as well .", "As with all the automated procedures described here , this algorithm yields useful , but imperfect results .", "The class chosen for quot ; VMS , quot ; for example , is incorrect , and may reflect the fact that in a DEC troubleshooting database , authors see no need to further specify VMS as quot ; VMS operating system . quot ; A more interesting observation is that , among the collocations associated with the terms , there are often several that might qualify as classes of which the term is an instance , e . g . , DECWindows could also be classified as quot ; software quot ; ; TK50 might also qualify as quot ; tape . quot ; From a generative lexicon perspective , these alternative classifications reflect multiple inheritance through the noun's qualia .", "That is , quot ; cartridge quot ; is further specifying the formal role of tape for TK50 .", "DECWindows is functionally an quot ; environment , quot ; its telic role , while quot ; software quot ; characterizes its formal quale .", "Extraction of information relating to noun's qualia .", "Under certain circumstances , it may be possible to elicit information about a noun's qualia from automated procedures on a corpus .", "In this line of research , we hayed employed the notion of quot ; lexical conceptual paradigm quot ; described above .", "An LCP relates a set of syntactic behaviors to the lexical semantic structures of the participating lexical items .", "For example , the set of expressions involving the word quot ; tape quot ; in the context of its use as a secondary storage device suggests that it fits the container artifact schema of the qualia structure , with quot ; information quot ; and quot ; file quot ; as its containees As mentioned in Section 1 , containers tend to appear as objects of the prepositions to , from , in , and on as well as in direct object position , in which case they are typically serving metonymically for the containee .", "Thus , the container LCP relates the set of generalized syntactic patterns V , Ni to , from , on Nk vi N This LCP includes a nominal alternation between the container and containee in the object position of verbs .", "For tape , this alternation is manifested for verbs that predicate the telic role of data storage but not the formal role of physical object , which refers to the object as a whole regardless of its contents We have explored the use of heuristics to distinguish those predicates that relate to the Telic quale of the noun .", "Consider the word tape , which occurs as the direct object in 107 sentences in our corpus .", "It appears with a total of 34 different verbs .", "By applying the mutual information metric MI to the verb object pairs , we can sort the verbs accordingly , giving us the table of verbs most highly associated with tape , shown in Figure 2 .", "While the mutual information statistic does a good job of identifying verbs that semantically relate to the word tape , it provides no information about how the verbs relate to the noun's qualia structure .", "That is , verbs such as unload , position , and mount are selecting for the formal quale of tape , a physical object that can be physically manipulated with respect to a tape drive .", "Read , write , and copy , on the other hand , relate to the telic role , the function of a tape as a medium for storing information .", "Our hypothesis was that the nominal alternation can help to distinguish the two sets of verbs .", "We reasoned that , if the alternation is based on the container containee metonymy , then it will be those verbs that apply to the telic role of the direct object that participate in the alternation .", "We tested this hypothesis as follows .", "We generated a candidate set of containees for tape by identifying all the nouns that appeared in the corpus to the left of the adjunct on tape .", "Intersection and set difference for three container nouns .", "Then we took the set of verbs that had one of these containee nouns as a direct object and compared this set to the set of verbs that had the container noun tape as a direct object in the corpus .", "According to our hypothesis , verbs applying to the telic role should appear in the intersection of these two sets as a result of the alternation , while those applying to the formal role will appear in the set difference verbs with containers as direct object verbs with containees as direct object .", "The difference operation should serve to remove any verbs that co occur with containee objects .", "Figure 3 shows the results of intersection and set difference for three container nouns tape , disk , and directory .", "The results indicate that the container LCP is able to differentiate nouns with respect to their telic and formal qualia , for the nouns tape and disk but not for directory .", "The poor discrimination in the latter case can be attributed to the fact that a directory is a recursive container .", "A directory contains files , and a directory is itself a file .", "Therefore , verbs that apply to the formal role of directory are likely to apply to the formal role of objects contained in directories such as other directories .", "This can be seen as a shortcoming of the container LCP for the task at hand , but may be a useful way of diagnosing when containers contain objects functionally similar to themselves .", "The result of this corpus acquisition procedure is a kind of minimal faceted analysis for the noun tape , as illustrated below , showing only the qualia that are relevant to the discussion . '", "13 Because the technique was sensitive to grammatical position of the object NP , the argument can be bound to the appropriate variable in the relation expressed in the qualia .", "It should be pointed out that these qualia values do not carry event place variables , since such discrimination was beyond the scope of this experiment .", "What is interesting about the qualia values is how close they are to the concepts in the projective conclusion space of tape , as mentioned in Section 1 .", "To illustrate this procedure on another semantic category , consider the term mouse in its computer artifact sense .", "In our corpus , it appears in the object position of the verb use in a quot ; use NP to quot ; construction , as well as the object of the preposition with following a transitive verb and its object These constructions are symptomatic of its role as an instrument ; and the VP complement of to as well as the VP dominating the with PP identify the telic predicates for the noun .", "Other verbs , for which mouse appears as a direct object are currently defaulted into the formal role , resulting in an entry for mouse as follows The above experiments have met with limited success , enough to warrant continuing our application of lexical semantic theory to knowledge acquisition from corpora , but not enough to remove the human from the loop .", "As they currently exist , the algorithms described here can be used as tools to help the knowledge engineer extract useful information from on line textual sources , and in some applications e . g . , a quot ; related terms quot ; thesaurus for full text information retrieval may provide a useful way to heuristically organize sublanguage terminology when human resources are unavailable .", "The purpose of the research described in this section is to experiment with the automatic acquisition of semantic tags for words in a sublanguage , tags well beyond that available from the seeding of MRDs .", "The identification of semantic tags is the result of type coercion on known syntactic forms , to induce a semantic feature , such as event or object .", "A pervasive example of type coercion is seen in the complements of aspectual verbs such as begin and finish , and verbs such as enjoy .", "That is , in sentences such as quot ; John began the book , quot ; the normal complement expected is an action or event of some sort , most often expressed by a gerundive or infinitival phrase quot ; John began reading the book , quot ; quot ; John began to read the book . quot ; In Pustejovsky 1991 it was argued that in such cases , the verb need not have multiple subcategorizations , but only one deep semantic type , in this case , an event .", "Thus , the verb coerces its complement e . g .", "quot ; the book quot ; into an event related to that object .", "Such information can be represented by means of a representational schema called qualia structure , which , among other things , specifies the relations associated with objects .", "Counts for objects of begin V .", "In related work being carried out with Mats Rooth of the University of Stuttgart , we are exploring what the range of coercion types is , and what environments they may appear in , as discovered in corpora .", "Some of our initial data suggest that the hypothesis of deep semantic selection may in fact be correct , as well as indicating what the nature of the coercion rules may be .", "Using techniques described in Church and Hindle 1990 , Church and Hanks 1990 , and Hindle and Rooth 1991 , Figure 4 shows some examples of the most frequent V 0 pairs from the AP corpus .", "Corpus studies confirm similar results for quot ; weakly intensional contexts quot ; such as the complement of coercive verbs such as veto .", "These are interesting because regardless of the noun type appearing as complement , it is embedded within a semantic interpretation of quot ; the proposal to , quot ; thereby clothing the complement within an intensional context .", "The examples in Figure 5 with the verb veto indicate two things first , that such coercions are regular and pervasive in corpora ; second , that almost anything can be vetoed , but that the most frequently occurring objects are closest to the type selected by the verb .", "What these data show is that the highest count complement types match the type required by the verb ; namely , that one vetoes a bill or proposal to do something , not the thing itself .", "These nouns can therefore be used with some predictive certainty for inducing the semantic type in coercive environments such as quot ; veto the expedition . quot ; This work is still preliminary , however , and requires further examination Pustejovsky and Rooth unpublished .", "In this section , we present another experiment indicating the feasibility of inducing semantic tags for lexical items from corpora . '", "Imagine being able to take the V 0 pairs Counts for objects of veto V . such as those given in Section 4 . 1 , and then applying semantic tags to the verbs that are appropriate to the role they play for that object i . e . , induction of the qualia roles for that noun .", "This is similar to the experiment reported on in Section 3 .", "Here we apply a similar technique to a much larger corpus , in order to induce the agentive role for nouns ; that is , the semantic predicate associated with bringing about the object .", "In this example we look at the behavior of noun phrases and the prepositional phrases that follow them .", "In particular , we look at the co occurrence of nominals with between , with , and to .", "Table 1 shows results of the conflating noun plus preposition patterns .", "The percentage shown indicates the ratio of the particular collocation to the key word .", "Mutual information MI statistics for the two words in collocation are also shown .", "What these results indicate is that induction of semantic type from conflating syntactic patterns is possible .", "Based on the semantic types for these prepositions , the syntactic evidence suggests that there is an equivalence class where each preposition makes reference to a symmetric relation between the arguments in the following two patterns We then take these results and , for those nouns where the association ratios for N with and N between are similar , we pair them with the set of verbs governing these quot ; NP PP quot ; combinations in corpus , effectively partitioning the original V 0 set into agentive predicates and agentive predicates .", "These are semantic n grams rather than direct interpretations of the prepositions .", "What these expressions in effect indicate is the range of semantic environments they will appear in .", "That is , in sentences like those in Example 16 , the force of the relational nouns agreement and talks is that they are unsaturated for the predicate bringing about this relation .", "In 17 , on the other hand , the NPs headed by agreement and talks are saturated in this respect .", "If our hypothesis is correct , we expect that verbs governing nominals collocated with a with phrase will be mostly those predicates referring to the agentive quale of the nominal .", "This is because the with phrase is unsaturated as a predicate , and acts to identify the agent of the verb as its argument cf .", "Nilsen 1973 .", "This is confirmed by our data , shown in Figure 6 .", "Conversely , verbs governing nominals collocating with a between phrase will not refer to the agentive since the phrase is saturated already .", "Indeed , the only verb occurring in this position with any frequency is the copula be , namely with the following counts 12 be V venture O .", "Thus , weak semantic types can be induced on the basis of syntactic behavior .", "There is a growing literature on corpus based acquisition and tuning Smadja 1991a ; Zernik and Jacobs 1991 ; Brent 1991 ; as well as Grishman and Sterling 1992 .", "We share with these researchers a general dependence on well behaved collocational patterns and distributional structures .", "Probably the main distinguishing feature of our approach is its reliance on a fairly well studied semantic framework to aid and guide the semantic induction process itself , whether it involves selectional restrictions or semantic types .", "In the previous section we presented algorithms for extracting collocational information from corpora , in order to supplement and fine tune the lexical structures seeded by a machine readable dictionary .", "In this section we demonstrate that , in addition to conventional lexical semantic relations , it is also possible to acquire information concerning lexical presuppositions and preferences from corpora , when analyzed with the appropriate semantic tools .", "In particular , we will discuss a phenomenon we call discourse polarity , and how corpus based experiments provide clues toward the representation of this phenomenon , as well as information on preference relations .", "As we have seen , providing a representational system for lexical semantic relations is a nontrivial task .", "Representing presuppositional information , however , is even more daunting .", "Nevertheless , there are some systematic semantic generalizations associated with such subtle lexical inferences .", "To illustrate this , consider the following examples taken from the Wall Street Journal Corpus , involving the verb insist .", "But the BNL sources yesterday insisted that the head office was aware of only a small portion of the credits to Iraq made by Atlanta .", "Mr . Smale , who ordinarily insists on a test market before a national roll out , told the team to go ahead although he said he was skeptical that Pringle's could survive , Mr . Tucker says .", "The Cantonese insist that their fish be quot ; fresh , quot ; though one whiff of Hong Kong harbor and the visitor may yearn for something shipped from distant seas .", "Example 25 Money isn't the issue , Mr . Bush insists .", "From analyzing these and similar data , a pattern emerges concerning the use of verbs like insist in discourse ; namely , the co occurrence with discourse markers denoting negative affect , such as although and but , as well as literal negatives , e . g . , no and not .", "This is reminiscent of the behavior of negative polarity items such as any more and at all .", "Such lexical items occur only in the context of negatives within a certain structural configuration . '", "In a similar way , verbs such as insist seem to require an overt or implicit negation within the immediate discourse context , rather than within the clause .", "For this reason , we will call such verbs discourse polarity items .", "For our purposes , the significance of such data is twofold first , experiments on corpora can test and confirm linguistic intuitions concerning a subtle semantic judgment ; second , if such knowledge is in fact so systematic , then it must be at least partially represented in the lexical semantics of the verb .", "To test whether the intuitions supported by the above data could be confirmed in corpora , Bergler 1991 derived the statistical co occurrence of insist with discourse polarity markers in the 7 million word corpus of Wall Street Journal articles .", "She derived the statistics reported in Figure 7 .", "Let us assume , on the basis of this preliminary date presented in Bergler 1992 that these verbs in fact do behave as discourse polarity items .", "The question then", "insist 586 occurrences throughout the corpus insist on 109 these have been cleaned by hand and are actually occurrences of the idiom insist on rather than accidental co occurrences . insist but 117 occurrences of both insist and but in the same sentence insist negation 186 includes not and n't insist Sr subjunctive 159 includes would , could , should , and be Negative markers with insist in WSJC immediately arises as to how we represent this type of knowledge .", "Using the language of the qualia structure discussed above , we can make explicit reference to the polarity behavior , in the following informal but intuitive representation for the verb insist ! '", "This entry states that in the REPORTING VERB sense of the word , insist is a relation between an individual and a statement that is the negation of a proposition , p , presupposed in the context of the utterance .", "As argued in Pustejovsky 1991 and Miller and Fellbaum 1991 , such simple oppositional predicates form a central part of our lexicalization of concepts .", "Semantically motivated collocations such as these extracted from large corpora can provide presuppositional information for words that would otherwise be missing from the lexical semantics of an entry .", "While full automatic extraction of semantic collocations is not yet feasible , some recent research in related areas is promising .", "Hindle 1990 reports interesting results of this kind based on literal collocations , where he parses the corpus Hindle 1983 into predicate argument structures and applies a mutual information measure Fano 1961 ; Magerman and Marcus 1990 to weigh the association between the predicate and each of its arguments .", "For example , as a list of the most frequent objects for the verb drink in his corpus , Hindle found beer , tea , Pepsi , and champagne .", "Based on the distributional hypothesis that the degree of shared contexts is a similarity measure for words , he develops a similarity metric for nouns based on their substitutability in certain verb contexts .", "Hindle thus finds sets of semantically similar nouns based on syntactic co occurrence data .", "The sets he extracts are promising ; for example , the ten most similar nouns to treaty in his corpus are agreement , plan , constitution , contract , proposal , accord , amendment , rule , law , and legislation .", "This work is very close in spirit to our own investigation here ; the emphasis on syntactic co occurrence enables Hindle to extract his similarity lists automatically ; they are therefore easy to compile for different corpora , different sublanguages , etc .", "Here we are attempting to use these techniques together with a model of lexical meaning , to capture deeper lexical semantic collocations ; e . g . , the generalization that the list of objects occurring for the word drink contains only liquids .", "In the final part of this section , we turn to how the analysis of corpora can provide lexical semantic preferences for verb selection .", "As discussed above , there is a growing body of research on deriving collocations from corpora cf .", "Church and Hanks 1990 ; Klavans , Chodorow , and Wacholder 1990 ; Wilks et al . 1993 ; Smadja 1991a , 1991b ; Calzolari and Bindi 1990 .", "Here we employ the tools of semantic analysis from Section 1 to examine the behavior of metonymy with reporting verbs .", "We will show , on the basis of corpus analysis , how verbs display marked differences in the ability to license metonymic operations over their arguments .", "Such information , we argue , is part of the preference semantics for a sublanguage , as automatically derived from corpus .", "Metonymy can be seen as a case of quot ; licensed violation quot ; of selectional restrictions .", "For example , while the verb announce selects for a human subject , sentences like The Phantasie Corporation announced third quarter losses are not only an acceptable paraphrase of the selectionally correct form Mr . Phantasie Jr . announced third quarter losses for Phantasie Corp , but they are the preferred form in the Wall Street Journal .", "This is an example of subject type coercion , as discussed in Section 1 .", "For example , the qualia structure for a noun such as corporation might be represented as below The metonymic extension in this example is straightforward a spokesman , executive , or otherwise legitimate representative quot ; speaking for quot ; a company or institution can be metonymically replaced by that company or institution . '", "We find that this type of metonymic extension for the subject is natural and indeed very frequent with reporting verbs Bergler 1991 , such as announce , report , release , and claim , while it is in general not possible with other verbs selecting human subjects , e . g . , the verbs of contemplation such as contemplate , consider , and think .", "However , there are subtle differences in the occurrence of such metonymies for the different members of the same semantic verb class that arise from corpus analysis .", "A reporting verb is an utterance verb that is used to relate the words of a source .", "In a careful study of seven reporting verbs on a 250 , 000 word corpus of Time magazine articles from 1963 , we found that the preference for different metonymic extensions varies considerably within this field Bergler 1991 .", "Figure 8 shows the findings for the words insist , deny , admit , claim , announce , said , and told for two metonymic extensions , namely where a group stands for an individual Analysts said .", ". and where a company or other institution stands for the individual IBM announced . . . . 19 The difference in patterns of metonymic behavior is quite striking semantically similar verbs seem to pattern similarly over all three categories ; admit , insist , and deny show a closer resemblance to each other than to any of the others , while said and Preference for metonymies for said in a 160 , 000 word fragment of the Wall Street Journal corpus . told form a category by themselves .", "There may be a purely semantic explanation why said and told seem not to prefer the metonymic use in subject position ; e . g . , perhaps these verbs relate more closely to the act of uttering , or perhaps they are too informal , stylistically .", "Evidence from other corpora , however , suggests that such information is accurately characterized as lexical preference .", "An initial experiment on a subset of the Wall Street Journal Corpus , for example , shows that said has a quite different metonymic distribution there , reported in Figure 9 .", "In this corpus we discovered that subject selection for an individual person appeared in only 50 of the sentences , while a company institution appeared in 34 of the cases .", "This difference could either be attributed to a difference in style between Time magazine and the Wall Street Journal or perhaps to a difference in general usage between 1963 and 1989 .", "The statistics presented here can of course not determine the reason for the difference , but rather help establish the lexical semantic preferences that exist in a certain corpus and sublanguage .", "An important question related to the extraction of preference information is what the corpus should be .", "Recent effort has been spent constructing balanced corpora , containing text from different styles and sources , such as novels , newspaper texts , scientific journal articles , etc .", "The assumption is of course that given a representative mix of samples of language use , we can extract the general properties and usage of words .", "But if we gain access to sophisticated automatic corpus analysis tools such as those discussed above , and indeed if we have specialized algorithms for sublanguage extraction , then homogeneous corpora might provide better data .", "The few examples of lexical preference mentioned in this section might not tell us anything conclusive for the definitive usage of a word such as said , if there even exists such a notion .", "Nevertheless the statistics provide an important tool for text analysis within the corpus from which they are derived .", "Because we can systematically capture the violation of selectional restrictions as semantically predicted , there is no need for a text analysis system to perform extensive commonsense inferencing .", "Thus , such presupposition and preference statistics are vital to efficient processing of real text .", "In this paper we have presented a particularly directed program of research for how text corpora can contribute to linguistics and computational linguistics .", "We first presented a representation language for lexical knowledge , the generative lexicon , and demonstrated how it facilitates the structuring of lexical relations among words , looking in particular at the problems of metonymy and polysemy .", "Such a framework for lexical knowledge suggests that there are richer relationships among words in text beyond that of simple co occurrence that can be extracted automatically .", "The work suggests how linguistic phenomena such as metonymy and polysemy might be exploited for knowledge acquisition for lexical items .", "Unlike purely statistical collocational analyses , the framework of a semantic theory allows the automatic construction of predictions about deeper semantic relationships among words appearing in collocational systems .", "We illustrated the approach for the acquisition of lexical information for several classes of nominals , and how such techniques can fine tune the lexical structures acquired from an initial seeding of a machine readable dictionary .", "In addition to conventional lexical semantic relations , we then showed how information concerning lexical presuppositions and preference relations can also be acquired from corpora , when analyzed with the appropriate semantic tools .", "In conclusion , we feel that the application of computational resources to the analysis of text corpora has and will continue to have a profound effect on the direction of linguistic and computational linguistic research .", "Unlike previous attempts at corpus research , the current focus is supported and guided by theoretical tools , and not merely statistical techniques .", "We should furthermore welcome the ability to expand the data set used for the confirmation of linguistic hypotheses .", "At the same time , we must remember that statistical results themselves reveal nothing , and require careful and systematic interpretation by the investigator to become linguistic data .", "This research was supported by DARPA contract MDA904 91 C 9328 .", "We would like to thank Scott Waterman for his assistance in preparing the statistics .", "We would also like to thank Mats Rooth , Scott Waterman , and four anonymous reviewers for useful comments and discussion ."], "summary_lines": ["Lexical Semantic Techniques For Corpus Analysis\n", "In this paper we outline a research program for computational linguistics, making extensive use of text corpora.\n", "We demonstrate how a semantic framework for lexical knowledge can suggest richer relationships among words in text beyond that of simple co-occurrence.\n", "The work suggests how linguistic phenomena such as metonymy and polysemy might be exploitable for semantic tagging of lexical items.\n", "Unlike with purely statistical collocational analyses, the framework of a semantic theory allows the automatic construction of predictions about deeper semantic relationships among words appearing in collocational systems.\n", "We illustrate the approach for the acquisition of lexical information for several classes of nominals, and how such techniques can fine-tune the lexical structures acquired from an initial seeding of a machine-readable dictionary.\n", "In addition to conventional lexical semantic relations, we show how information concerning lexical presuppositions and preference relations can also be acquired from corpora, when analyzed with the appropriate semantic tools.\n", "Finally, we discuss the potential that corpus studies have for enriching the data set for theoretical linguistic research, as well as helping to confirm or disconfirm linguistic hypotheses.\n", "we present an interesting framework for the acquisition of semantic relations from corpora not only relying on statistics, but guided by theoretical lexicon principles.\n", "We show how statistical techniques, such as mutual information measures can contribute to automatically acquire lexical information regarding the link between a noun and a predicate.\n", "We use generalized syntactic patterns for extracting qualia structures from a partially parsed corpus.\n"]}
{"article_lines": ["Inducing Probabilistic CCG Grammars from Logical Form with Higher Order Unification", "This paper addresses the problem of learning to map sentences to logical form , given training data consisting of natural language sentences paired with logical representations of their meaning .", "Previous approaches have been designed for particular natural languages or specific meaning representations ; here we present a more general method .", "The approach induces a probabilistic CCG grammar that represents the meaning of individual words and defines how these meanings can be combined to analyze complete sentences .", "We use higher order unification to define a hypothesis space containing all grammars consistent with the training data , and develop an online learning algorithm that efficiently searches this space while simultaneously estimating the parameters of a log linear parsing model .", "Experiments demonstrate high accuracy on benchmark data sets in four languages with two different meaning representations .", "A key aim in natural language processing is to learn a mapping from natural language sentences to formal representations of their meaning .", "Recent work has addressed this problem by learning semantic parsers given sentences paired with logical meaning representations Thompson Mooney , 2002 ; Kate et al . , 2005 ; Kate Mooney , 2006 ; Wong Mooney , 2006 , 2007 ; Zettlemoyer Collins , 2005 , 2007 ; Lu et al . , 2008 .", "For example , the training data might consist of English sentences paired with lambda calculus meaning representations Given pairs like this , the goal is to learn to map new , unseen , sentences to their corresponding meaning .", "Previous approaches to this problem have been tailored to specific natural languages , specific meaning representations , or both .", "Here , we develop an approach that can learn to map any natural language to a wide variety of logical representations of linguistic meaning .", "In addition to data like the above , this approach can also learn from examples such as Sentence hangi eyaletin texas ye siniri vardir Meaning answer state borders tex where the sentence is in Turkish and the meaning representation is a variable free logical expression of the type that has been used in recent work Kate et al . , 2005 ; Kate Mooney , 2006 ; Wong Mooney , 2006 ; Lu et al . , 2008 .", "The reason for generalizing to multiple languages is obvious .", "The need to learn over multiple representations arises from the fact that there is no standard representation for logical form for natural language .", "Instead , existing representations are ad hoc , tailored to the application of interest .", "For example , the variable free representation above was designed for building natural language interfaces to databases .", "Our approach works by inducing a combinatory categorial grammar CCG Steedman , 1996 , 2000 .", "A CCG grammar consists of a language specific lexicon , whose entries pair individual words and phrases with both syntactic and semantic information , and a universal set of combinatory rules that project that lexicon onto the sentences and meanings of the language via syntactic derivations .", "The learning process starts by postulating , for each sentence in the training data , a single multi word lexical item pairing that sentence with its complete logical form .", "These entries are iteratively refined with a restricted higher order unification procedure Huet , 1975 that defines all possible ways to subdivide them , consistent with the requirement that each training sentence can still be parsed to yield its labeled meaning .", "For the data sets we consider , the space of possible grammars is too large to explicitly enumerate .", "The induced grammar is also typically highly ambiguous , producing a large number of possible analyses for each sentence .", "Our approach discriminates between analyses using a log linear CCG parsing model , similar to those used in previous work Clark Curran , 2003 , 2007 , but differing in that the syntactic parses are treated as a hidden variable during training , following the approach of Zettlemoyer Collins 2005 , 2007 .", "We present an algorithm that incrementally learns the parameters of this model while simultaneously exploring the space of possible grammars .", "The model is used to guide the process of grammar refinement during training as well as providing a metric for selecting the best analysis for each new sentence .", "We evaluate the approach on benchmark datasets from a natural language interface to a database of US Geography Zelle Mooney , 1996 .", "We show that accurate models can be learned for multiple languages with both the variable free and lambdacalculus meaning representations introduced above .", "We also compare performance to previous methods Kate Mooney , 2006 ; Wong Mooney , 2006 , 2007 ; Zettlemoyer Collins , 2005 , 2007 ; Lu et al . , 2008 , which are designed with either language or representation specific constraints that limit generalization , as discussed in more detail in Section 6 .", "Despite being the only approach that is general enough to run on all of the data sets , our algorithm achieves similar performance to the others , even outperforming them in several cases .", "The goal of our algorithm is to find a function f x z that maps sentences x to logical expressions z .", "We learn this function by inducing a probabilistic CCG PCCG grammar from a training set xZ , zz i 1 . . . n containing example sentence , logical form pairs such as New York borders Vermont , next to ny , vt .", "The induced grammar consists of two components which the algorithm must learn tion over the possible parses y , conditioned on the sentence x .", "We will present the approach in two parts .", "The lexical induction process Section 4 uses a restricted form of higher order unification along with the CCG combinatory rules to propose new entries for A .", "The complete learning algorithm Section 5 integrates this lexical induction with a parameter estimation scheme that learns 0 .", "Before presenting the details , we first review necessary background .", "This section provides an introduction to the ways in which we will use lambda calculus and higher order unification to construct meaning representations .", "It also reviews the CCG grammar formalism and probabilistic extensions to it , including existing parsing and parameter estimation techniques .", "We assume that sentence meanings are represented as logical expressions , which we will construct from the meaning of individual words by using the operations defined in the lambda calculus .", "We use a version of the typed lambda calculus cf .", "Carpenter 1997 , in which the basic types include e , for entities ; t , for truth values ; and i for numbers .", "There are also function types of the form e , t that are assigned to lambda expressions , such as Ax . state x , which take entities and return truth values .", "We represent the meaning of words and phrases using lambda calculus expressions that can contain constants , quantifiers , logical connectors , and lambda abstractions .", "The advantage of using the lambda calculus lies in its generality .", "The meanings of individual words and phrases can be arbitrary lambda expressions , while the final meaning for a sentence can take different forms .", "It can be a full lambdacalculus expression , a variable free expression such as answer state borders tex , or any other logical expression that can be built from the primitive meanings via function application and composition .", "The higher order unification problem Huet , 1975 involves finding a substitution for the free variables in a pair of lambda calculus expressions that , when applied , makes the expressions equal each other .", "This problem is notoriously complex ; in the unrestricted form Huet , 1973 , it is undecidable .", "In this paper , we will guide the grammar induction process using a restricted version of higherorder unification that is tractable .", "For a given expression h , we will need to find expressions for f and g such that either h f g or h Ax . f g x .", "This limited form of the unification problem will allow us to define the ways to split h into subparts that can be recombined with CCG parsing operations , which we will define in the next section , to reconstruct h . CCG Steedman , 2000 is a linguistic formalism that tightly couples syntax and semantics , and can be used to model a wide range of language phenomena .", "For present purposes a CCG grammar includes a lexicon A with entries like the following where each lexical item w X h has words w , a syntactic category X , and a logical form h expressed as a lambda calculus expression .", "For the first example , these are New York , NP , and ny .", "CCG syntactic categories may be atomic such as S , NP or complex such as S NP NP .", "CCG combines categories using a set of combinatory rules .", "For example , the forward and These rules apply to build syntactic and semantic derivations under the control of the word order information encoded in the slash directions of the lexical entries .", "For example , given the lexicon above , the sentence New York borders Vermont can be parsed to produce where each step in the parse is labeled with the combinatory rule or that was used .", "CCG also includes combinatory rules of forward B and backward B composition These rules provide for a relaxed notion of constituency which will be useful during learning as we reason about possible refinements of the grammar .", "We also allow vertical slashes in CCG categories , which act as wild cards .", "For example , with this extension the forward application combinator could be used to combine the category S S NP with any of S NP , S NP , or SNP .", "Figure 1 shows two parses where the composition combinators and vertical slashes are used .", "These parses closely resemble the types of analyses that will be possible under the grammars we learn in the experiments described in Section 8 .", "Given a CCG lexicon A , there will , in general , be many possible parses for each sentence .", "We select the most likely alternative using a log linear model , which consists of a feature vector 0 and a parameter vector 0 .", "The joint probability of a logical form z constructed with a parse y , given a sentence x is Section 7 defines the features used in the experiments , which include , for example , lexical features that indicate when specific lexical items in A are used in the parse y .", "For parsing and parameter estimation , we use standard algorithms Clark Curran , 2007 , as described below .", "The parsing , or inference , problem is to find the most likely logical form z given a sentence x , assuming the parameters 0 and lexicon A are known where the probability of the logical form is found by summing over all parses that produce it In this approach the distribution over parse trees y is modeled as a hidden variable .", "The sum over parses in Eq .", "3 can be calculated efficiently using the inside outside algorithm with a CKY style parsing algorithm .", "To estimate the parameters themselves , we use stochastic gradient updates LeCun et al . , 1998 .", "Given a set of n sentence meaning pairs xi , zi i 1 . . . n , we update the parameters 0 iteratively , for each example i , by following the local gradient of the conditional log likelihood objective Oi log P zi xi ; 0 , A .", "The local gradient of the individual parameter 0j associated with feature Oj and training instance xi , zi is given by As with Eq .", "3 , all of the expectations in Eq .", "4 are calculated through the use of the inside outside algorithm on a pruned parse chart .", "In the experiments , each chart cell was pruned to the top 200 entries .", "Before presenting a complete learning algorithm , we first describe how to use higher order unification to define a procedure for splitting CCG lexical entries .", "This splitting process is used to expand the lexicon during learning .", "We seed the lexical induction with a multi word lexical item xi S zi for each training example xi , zi , consisting of the entire sentence xi and its associated meaning representation zi .", "For example , one initial lexical item might be Although these initial , sentential lexical items can parse the training data , they will not generalize well to unseen data .", "To learn effectively , we will need to split overly specific entries of this type into pairs of new , smaller , entries that generalize better .", "For example , one possible split of the lexical entry given in 5 would be the pair New York borders S NP Ax . next to ny , x , Vermont NP vt where we broke the original logical expression into two new ones Ax . next to ny , x and vt , and paired them with syntactic categories that allow the new lexical entries to be recombined to produce the original analysis .", "The next three subsections define the set of possible splits for any given lexical item .", "The process is driven by solving a higher order unification problem that defines all of the ways of splitting the logical expression into two parts , as described in Section 4 . 1 .", "Section 4 . 2 describes how to construct syntactic categories that are consistent with the two new fragments of logical form and which will allow the new lexical items to recombine .", "Finally , Section 4 . 3 defines the full set of lexical entry pairs that can be created by splitting a lexical entry .", "As we will see , this splitting process is overly prolific for any single language and will yield many lexical items that do not generalize well .", "For example , there is nothing in our original lexical entry above that provides evidence that the split should pair Vermont with the constant vt and not Ax . next to ny , x .", "Section 5 describes how we estimate the parameters of a probabilistic parsing model and how this parsing model can be used to guide the selection of items to add to the lexicon .", "The set of possible splits for a logical expression h is defined as the solution to a pair of higherorder unification problems .", "We find pairs of logical expressions f , g such that either f g h or Ax . f g x h . Solving these problems creates new expressions f and g that can be recombined according to the CCG combinators , as defined in Section 3 . 2 , to produce h . In the unrestricted case , there can be infinitely many solution pairs f , g for a given expression h . For example , when h tex and f Ax . tex , the expression g can be anything .", "Although it would be simple enough to forbid vacuous variables in f and g , the number of solutions would still be exponential in the size of h . For example , when h contains a conjunction , such as h Ax . city x n major x n in x , tex , any subset of the expressions in the conjunction can be assigned to f or g .", "To limit the number of possible splits , we enforce the following restrictions on the possible higherorder solutions that will be used during learning Together , these three restrictions guarantee that the number of splits is , in the worst case , an Ndegree polynomial of the number of constants in h . The constraints were designed to increase the efficiency of the splitting algorithm without impacting performance on the development data .", "We define the set of possible splits for a category X h with syntax X and logical form h by enumerating the solution pairs f , g to the higher order unification problems defined above and creating syntactic categories for the resulting expressions .", "For example , given X h S NP Ax . in x , tex , f AyAx . in x , y , and g tex , we would produce the following two pairs of new categories which were constructed by first choosing the syntactic category for g , in this case NP , and then enumerating the possible directions for the new slash in the category containing f . We consider each of these two steps in more detail below .", "The new syntactic category for g is determined based on its type , T g .", "For example , T tex e and T Ax . state x e , t .", "Then , the function QT takes an input type T and returns the syntactic category of T as follows The basic types e and t are assigned syntactic categories NP and S , and all functional types are assigned categories recursively .", "For example Q e , t S NP and Q e , e , t S NP NP .", "This definition of CCG categories is unconventional in that it never assigns atomic categories to functional types .", "For example , there is no distinct syntactic category N for nouns which have semantic type he , ti .", "Instead , the more complex category S NP is used .", "Now , we are ready to define the set of all category splits .", "For a category A X h we can define which is a union of sets , each of which includes splits for a single CCG operator .", "For example , FA X h is the set of category pairs where each pair can be combined with the forward application combinator , described in Section 3 . 2 , to reconstruct X h . The remaining three sets are defined similarly , and are associated with the backward application and forward and backward composition operators , respectively where the composition sets FC and BC only accept input categories with the appropriate outermost slash direction , for example FC X Y h .", "We can now define the lexical splits that will be used during learning .", "For lexical entry w0 n A , with word sequence w0 n hw0 , . . . , wni and CCG category A , define the set SL of splits to be where we enumerate all ways of splitting the words sequence w0 n and aligning the subsequences with categories in SC A , as defined in the last section .", "The previous section described how a splitting procedure can be used to break apart overly specific lexical items into smaller ones that may generalize better to unseen data .", "The space of possible lexical items supported by this splitting procedure is too large to explicitly enumerate .", "Instead , we learn the parameters of a PCCG , which is used both to guide the splitting process , and also to select the best parse , given a learned lexicon .", "Figure 2 presents the unification based learning algorithm , UBL .", "This algorithm steps through the data incrementally and performs two steps for each training example .", "First , new lexical items are induced for the training instance by splitting and merging nodes in the best correct parse , given the current parameters .", "Next , the parameters of the PCCG are updated by making a stochastic gradient update on the marginal likelihood , given the updated lexicon .", "Inputs and Initialization The algorithm takes as input the training set of n sentence , logical form pairs xi , zi i 1 . . . n along with an NP list , ANP , of proper noun lexical items such as Texas NP tex .", "The lexicon , A , is initialized with a single lexical item xi S zi for each of the training pairs along with the contents of the NP list .", "It is possible to run the algorithm without the initial NP list ; we include it to allow direct comparisons with previous approaches , which also included NP lists .", "Features and initial feature weights are described in Section 7 .", "Step 1 Updating the Lexicon In the lexical update step the algorithm first computes the best correct parse tree y for the current training example and then uses y as input to the procedure NEW LEX , which determines which if any new lexical items to add to A . NEW LEX begins by enumerating all pairs C , wi j , for i j , where C is a category occurring at a node in y and wi j are the two or more words it spans .", "For example , in the left parse in Figure 1 , there would be four pairs one with the category C NP NP Ax . border x and the phrase wi j ye siniri vardir , and one for each non leaf node in the tree .", "For each pair C , wi j , NEW LEX considers introducing a new lexical item wi j C , which allows for the possibility of a parse where the subtree rooted at C is replaced with this new entry .", "If C is a leaf node , this item will already exist .", "NEW LEX also considers adding each pair of new lexical items that is obtained by splitting wi j C as described in Section 4 , thereby considering many different ways of reanalyzing the node .", "This process creates a set of possible new lexicons , where each lexicon expands A in a different way by adding the items from either a single split or a single merge of a node in y .", "For each potential new lexicon A' , NEW LEX computes the probability p y xi , zi ; B' , A' of the original parse y under A' and parameters B' that are the same as B but have weights for the new lexical items , as described in Section 7 .", "It also finds the best new parse y' arg maxy p y xi , zi ; B' , A' . 1 Finally , NEW LEX selects the A' with the largest difference in log probability between y' and y , and returns the new entries in A' .", "If y is the best parse for every A' , NEW LEX returns the empty set ; the lexicon will not change .", "Step 2 Parameter Updates For each training example we update the parameters B using the stochastic gradient updates given by Eq .", "Discussion The alternation between refining the lexicon and updating the parameters drives the learning process .", "The initial model assigns a conditional likelihood of one to each training example there is a single lexical item for each sentence xi , and it contains the labeled logical form zi .", "Although the splitting step often decreases the probability of the data , the new entries it produces are less specific and should generalize better .", "Since we initially assign positive weights to the parameters for new lexical items , the overall approach prefers splitting ; trees with many lexical items will initially be much more likely .", "However , if the learned lexical items are used in too many incorrect parses , the stochastic gradient updates will down weight them to the point where the lexical induction step can merge or re split nodes in the trees that contain them .", "This allows the approach to correct the lexicon and , hopefully , improve future performance .", "Previous work has focused on a variety of different meaning representations .", "Several approaches have been designed for the variable free logical representations shown in examples throughout this paper .", "For example , Kate Mooney 2006 present a method KRISP that extends an existing SVM learning algorithm to recover logical representations .", "The 1This computation can be performed efficiently by incrementally updating the parse chart used to find y .", "Inputs Training set xi , zi i 1 . . . n where each example is a sentence xi paired with a logical form zi .", "Set of NP lexical items ANP .", "Number of iterations T . Learning rate parameter \u03b10 and cooling rate parameter c . Definitions The function NEW LEX y takes a parse y and returns a set of new lexical items found by splitting and merging categories in y , as described in Section 5 .", "The distributions p y x , z ; B , A and p y , z x ; B , A are defined by the log linear model , as described in Section 3 . 3 .", "Initialization WASP system Wong Mooney , 2006 uses statistical machine translation techniques to learn synchronous context free grammars containing both words and logic .", "Lu et al . 2008 Lu08 developed a generative model that builds a single hybrid tree of words , syntax and meaning representation .", "These algorithms are all language independent but representation specific .", "Other algorithms have been designed to recover lambda calculus representations .", "For example , Wong Mooney 2007 developed a variant of WASP A WASP specifically designed for this alternate representation .", "Zettlemoyer Collins 2005 , 2007 developed CCG grammar induction techniques where lexical items are proposed according to a set of hand engineered lexical templates .", "Our approach eliminates this need for manual effort .", "Another line of work has focused on recovering meaning representations that are not based on logic .", "Examples include an early statistical method for learning to fill slot value representations Miller et al . , 1996 and a more recent approach for recovering semantic parse trees Ge Mooney , 2006 .", "Exploring the extent to which these representations are compatible with the logic based learning approach we developed is an important area for future work .", "Finally , there is work on using categorial grammars to solve other , related learning problems .", "For example , Buszkowski Penn 1990 describe a unification based approach for grammar discovery from bracketed natural language sentences and Villavicencio 2002 developed an approach for modeling child language acquisition .", "Additionally , Bos et al . 2004 consider the challenging problem of constructing broad coverage semantic representations with CCG , but do not learn the lexicon .", "Features We use two types of features in our model .", "First , we include a set of lexical features For each lexical item L E A , we include a feature OL that fires when L is used .", "Second , we include semantic features that are functions of the output logical expression z .", "Each time a predicate p in z takes an argument a with type T a in position i it triggers two binary indicator features O p , a , i for the predicate argument relation ; and O p , T a , i for the predicate argument type relation .", "Initialization The weights for the semantic features are initialized to zero .", "The weights for the lexical features are initialized according to coocurrance statistics estimated with the Giza Och Ney , 2003 implementation of IBM Model 1 .", "We compute translation scores for word , constant pairs that cooccur in examples in the training data .", "The initial weight for each OL is set to ten times the average score over the word , constant pairs in L , except for the weights of seed lexical entries in ANP which are set to 10 equivalent to the highest possible coocurrence score .", "We used the learning rate \u03b10 1 . 0 and cooling rate c 10 5 in all training scenarios , and ran the algorithm for T 20 iterations .", "These values were selected with cross validation on the Geo880 development set , described below .", "Data and Evaluation We evaluate our system on the GeoQuery datasets , which contain naturallanguage queries of a geographical database paired with logical representations of each query s meaning .", "The full Geo880 dataset contains 880 Englishsentence , logical form pairs , which we split into a development set of 600 pairs and a test set of 280 pairs , following Zettlemoyer Collins 2005 .", "The Geo250 dataset is a subset of Geo880 containing 250 sentences that have been translated into Turkish , Spanish and Japanese as well as the original English .", "Due to the small size of this dataset we use 10 fold cross validation for evaluation .", "We use the same folds as Wong Mooney 2006 , 2007 and Lu et al . 2008 , allowing a direct comparison .", "The GeoQuery data is annotated with both lambda calculus and variable free meaning representations , which we have seen examples of throughout the paper .", "We report results for both representations , using the standard measures of Recall percentage of test sentences assigned correct logical forms , Precision percentage of logical forms returned that are correct and F1 the harmonic mean of Precision and Recall .", "Two Pass Parsing To investigate the trade off between precision and recall , we report results with a two pass parsing strategy .", "When the parser fails to return an analysis for a test sentence due to novel words or usage , we reparse the sentence and allow the parser to skip words , with a fixed cost .", "Skipping words can potentially increase recall , if the ignored word is an unknown function word that does not contribute semantic content .", "Tables 1 , 2 , and 3 present the results for all of the experiments .", "In aggregate , they demonstrate that our algorithm , UBL , learns accurate models across languages and for both meaning representations .", "This is a new result ; no previous system is as general .", "We also see the expected tradeoff between precision and recall that comes from the two pass parsing approach , which is labeled UBL s . With the ability to skip words , UBL s achieves the highest recall of all reported systems for all evaluation conditions .", "However , UBL achieves much higher precision and better overall F1 scores , which are generally comparable to the best performing systems .", "The comparison to the CCG induction techniques of ZC05 and ZC07 Table 3 is particularly striking .", "These approaches used language specific templates to propose new lexical items and also required as input a set of hand engineered lexical entries to model phenomena such as quantification and determiners .", "However , the use of higher order unification allows UBL to achieve comparable performance while automatically inducing these types of entries .", "For a more qualitative evaluation , Table 4 shows a selection of lexical items learned with high weights for the lambda calculus meaning representations .", "Nouns such as state or estado are consistently learned across languages with the category S NP , which stands in for the more conventional N . The algorithm also learns language specific constructions such as the Japanese case markers no and wa , which are treated as modifiers that do not add semantic content .", "Language specific word order is also encoded , using the slash directions of the CCG categories .", "For example , what and que take their arguments to the right in the wh initial English and Spanish .", "However , the Turkish wh word nelerdir and the Japanese question marker nan desu ka are sentence final , and therefore take their arguments to the left .", "Learning regularities of this type allows UBL to generalize well to unseen data .", "There is less variation and complexity in the learned lexical items for the variable free representation .", "The fact that the meaning representation is deeply nested influences the form of the induced grammar .", "For example , recall that the sentence what states border texas would be paired with the meaning answer state borders tex .", "For this representation , lexical items such as can be used to construct the desired output .", "In practice , UBL often learns entries with only a single slash , like those above , varying only in the direction , as required for the language .", "Even the more complex items , such as those for quantifiers , are consistently simpler than those induced from the lambda calculus meaning representations .", "For example , one of the most complex entries learned in the experiments for English is the smallest NP NP NP NP AfAx . smallest one f x .", "There are also differences in the aggregate statistics of the learned lexicons .", "For example , the average length of a learned lexical item for the lambdacalculus , variable free meaning representations is 1 . 21 , 1 . 08 for Turkish , 1 . 34 , 1 . 19 for English , 1 . 43 , 1 . 25 for Spanish and 1 . 63 , 1 . 42 for Japanese .", "For both meaning representations the model learns significantly more multiword lexical items for the somewhat analytic Japanese than the agglutinative Turkish .", "There are also variations in the average number of learned lexical items in the best parses during the final pass of training 192 for Japanese , 206 for Spanish , 188 for English and 295 for Turkish .", "As compared to the other languages , the morpologically rich Turkish requires significantly more lexical variation to explain the data .", "Finally , there are a number of cases where the UBL algorithm could be improved in future work .", "In cases where there are multiple allowable word orders , the UBL algorithm must learn individual entries for each possibility .", "For example , the following two categories are often learned with high weight for the Japanese word chiisai and are treated as distinct entries in the lexicon .", "Similarly , the approach presented here does not model morphology , and must repeatedly learn the correct categories for the Turkish words nehri , nehir , nehirler , and nehirlerin , all of which correspond to the logical form Ax . river x .", "This paper has presented a method for inducing probabilistic CCGs from sentences paired with logical forms .", "The approach uses higher order unification to define the space of possible grammars in a language and representation independent manner , paired with an algorithm that learns a probabilistic parsing model .", "We evaluated the approach on four languages with two meaning representations each , achieving high accuracy across all scenarios .", "For future work , we are interested in exploring the generality of the approach while extending it to new understanding problems .", "One potential limitation is in the constraints we introduced to ensure the tractability of the higher order unification procedure .", "These restrictions will not allow the approach to induce lexical items that would be used with , among other things , many of the type raised combinators commonly employed in CCG grammars .", "We are also interested in developing similar grammar induction techniques for context dependent understanding problems , such as the one considered by Zettlemoyer Collins 2009 .", "Such an approach would complement ideas for using high order unification to model a wider range of language phenomena , such as VP ellipsis Dalrymple et al . , 1991 .", "We thank the reviewers for useful feedback .", "This work was supported by the EU under IST Cognitive Systems grant IP FP6 2004 IST 4 27657 Paco Plus and ERC Advanced Fellowship 249520 GRAMPLUS to Steedman .", "Kwiatkowski was supported by an EPRSC studentship .", "Zettlemoyer was supported by a US NSF International Research Fellowship ."], "summary_lines": ["Inducing Probabilistic CCG Grammars from Logical Form with Higher-Order Unification\n", "This paper addresses the problem of learning to map sentences to logical form, given training data consisting of natural language sentences paired with logical representations of their meaning.\n", "Previous approaches have been designed for particular natural languages or specific meaning representations; here we present a more general method.\n", "The approach induces a probabilistic CCG grammar that represents the meaning of individual words and defines how these meanings can be combined to analyze complete sentences.\n", "We use higher-order unification to define a hypothesis space containing all grammars consistent with the training data, and develop an online learning algorithm that efficiently searches this space while simultaneously estimating the parameters of a log-linear parsing model.\n", "Experiments demonstrate high accuracy on benchmark data sets in four languages with two different meaning representations.\n", "We present an approach for language-independent learning that replaces the hand-specified templates with a higher-order-unification-based lexical induction method.\n", "We initialise lexical weights in their learning algorithm using corpus-wide alignment statistics across words and meaning elements.\n"]}
{"article_lines": ["Word Association Norms Mutual Information And Lexicography", "rs Sunday , calling for greater economic reforms to mtniasion asserted that quot ; the Postal Service could Then , she said , the family hopes to e out of work steelworker .", "quot ; because that doesn't quot ; We suspend reality when we say we'll scientists has won the first round in an effort to about three children in a mining town who plot to GM executives say the shutdowns will rtment as receiver , instructed officials to try to The package , which is to newly enhanced image as the moderate who moved to million offer from chairman Victor Posner to help after telling a delivery room doctor not to try to h birthday Tuesday , cheered by those who fought to at he had formed an alliance with Moslem rebels to quot ; Basically we could We worked for a year to their expensive mirrors , just like in wartime , to ard of many who risked their own lives in order to We must increase the amount Americans save China from poverty . save enormous sums of money in contracting out individual c save enough for a down payment on a home . save jobs , that costs jobs .", "quot ; save money by spending 10 , 000 in wages for a public works save one of Egypt's great treasures , the decaying tomb of R save the quot ; pit ponies quot ; doomed to be slaughtered . save the automaker 500 million a year in operating costs a save the company rather than liquidate it and then declared save the country nearly 2 billion , also includes a program save the country . save the financially troubled company , but said Posner stil save the infant by inserting a tube in its throat to help i save the majestic Beaux Arts architectural masterpiece . save the nation from communism . save the operating costs of the Pershings and pound launch save the site at enormous expense to us , quot ; said Leveillee . save them from drunken Yankee brawlers , quot ; Tess said . save those who were passengers .", "quot ; save .", "quot ; Figure 2 Some AP 1987 Concordance lines to 'save . . . from , ' roughly sorted into categories save X from Y 65 concordance lines 1 save PERSON from Y 23 concordance lines 1 . 1 save PERSON from BAD 19 concordance lines Robert DeNiro to quot ; We wanted to Murphy was sacrificed to quot ; God sent this man to Pope John Paul II to quot ; save Indian tribes PERSON from genocide DESTRUCT BAD at the hands of save hirn PERSON from undue trouble BADI and loss BAD of money , quot ; save more powerful Democrats PERSON from harm BAD . save my five diikiren PERSON from being burned to death DESTRUCT BAD and save us PERSON1 from sin BAD .", "quot ; 1 . 2 save PERSON from BAD LOC ATION 4 concordance lines rescuers who helped save the toddler PERSON from an abandoned well LOC will be feted with a parade while attempting to save two drowning boys PERSON from a turbulent BAD creek LOC in Ohio LOC 2 . save INST ITUTION from ECON BAD 27 concordance lines member states to help should be sought quot ; to law was necessary to operation quot ; to were not needed to his efforts to save the BEC INST from possible bankruptcy ECONPAD this year . save the company CORP DIST from bankruptcy ECON BAD . save the country NATIONINSTD from dismter BAD1 save the nation NATION INST J from Conununism BADNPOLITICAL save the system from bankruptcy ECONHBAD . save the world INST from the likes of Lothar and the Spider Woman 3 . save ANIMAL from DESTRUCT ION 5 concordance lines give them the money to save the dogs ANIMAL from being destroyed DESTRUC11 , program intended to save the giant birds ANIMAL from extinction DESTRUCT , UNCLASSIFIED 10 concordance lines walnut and ash trees to save them from the axes and saws of a logging company . after the attack to save the ship from a terrible BAD fire , Navy reports concluded Thursday .", "It is common practice in linguistics to classify words not only on the basis of their meanings but also on the basis of their co occurrence with other words .", "Running through the whole Firthian tradition , for example , is the theme that quot ; You shall know a word by the company it keeps quot ; Firth , 1957 .", "quot ; On the one hand , bank co occurs with words and expression such u money , notes , loan , account , investment , clerk . official , manager , robbery , vaults , working in a , Its actions , First National . of England . and so forth .", "On the other hand . we find bank co oorurring with river swim , boat , east and of course Wen and South , which have acquired special meanings of their own , on top of the , and of the Rhine . quot ; Hanks 1987 , p . 127 The search for increasingly delicate word classes is not new .", "In lexicography , for example , it goes back at least to the quot ; verb patterns quot ; described in Hornby's Advanced Learner's Dictionary first edition 1948 .", "What is new is that facilities for the computational storage and analysis of large bodies of natural language have developed significantly in recent years , so that it is now becoming possible to test and apply informal assertions of this kind in a more rigorous way , and to see what company our words do keep .", "The proposed statistical description has a large number of potentially important applications , including a constraining the language model both for speech recognition and optical character recognition OCR , b providing disambiguation cues for parsing highly ambiguous syntactic structures such as noun compounds , conjunctions , and prepositional phrases , c retrieving texts from large databases e . g . , newspapers , patents , d enhancing the productivity of computational linguists in compiling lexicons of lexico syntactic facts , and e enhancing the productivity of lexicographers in identifying normal and conventional usage .", "Consider the optical character recognizer OCR application .", "Suppose that we have an OCR device such as Kahan , Pavlidis , Baird 1987 , and it has assigned about equal probability to having recognized quot ; farm quot ; and quot ; form , quot ; where the context is either 1 quot ; federal credit quot ; or 2 quot ; some of . quot ; The proposed association measure can make use of the fact that quot ; farm quot ; is much more likely in the first context and quot ; form quot ; is much more likely in the second to resolve the ambiguity .", "Note that alternative disambiguation methods based on syntactic constraints such as part of speech are unlikely to help in this case since both quot ; form quot ; and quot ; farm quot ; are commonly used as nouns .", "Word association norms are well known to be an important factor in psycholinguistic research , especially in the area of lexical retrieval .", "Generally speaking , subjects respond quicker than normal to the word quot ; nurse quot ; if it follows a highly associated word such as quot ; doctor . quot ; quot ; Some results and implications are summarized from reaction time . experiments in which subjects either a classified successive strings of letters as words and nonwords , or b pronounced the strings .", "Both types of response to words e . g . , BUTTER were consistently faster when preceded by associated words e . g . , BREAD rather than unassociated words e . g .", "NURSE . quot ; Meyer , Schvaneveldt and Ruddy 1975 , p . 98 Much of this psycholinguistic research is based on empirical estimates of word association norms such as Palermo and Jenkins 1964 , perhaps the most influential study of its kind , though extremely small and somewhat dated .", "This study measured 200 words by asking a few thousand subjects to write down a word after each of the 200 words to be measured .", "Results are reported in tabular form , indicating which words were written down , and by how many subjects , factored by grade level and sex .", "The word quot ; doctor , quot ; for example , is reported on pp .", "98 100 , to be most often associated with quot ; nurse , quot ; followed by quot ; sick , quot ; quot ; health , quot ; quot ; medicine , quot ; quot ; hospital , quot ; quot ; man , quot ; quot ; sickness , quot ; quot ; lawyer , quot ; and about 70 more words .", "We propose an alternative measure , the association ratio , for measuring word association norms , based on the information theoretic concept of mutual information .", "The proposed measure is more objective and less costly than the subjective method employed in Palermo and Jenkins 1964 .", "The association ratio can be scaled up to provide robust estimates of word association norms for a large portion of the language .", "Using the association ratio measure , the five most associated words are in order quot ; dentists , quot ; quot ; nurses , quot ; quot ; treating , quot ; quot ; treat , quot ; and quot ; hospitals . quot ; What is quot ; mutual information quot ; ?", "According to Fano 1961 , p . 28 , if two points words , x and y , have probabilities P x and P y , then their mutual information , x , y , is defined to be Informally , mutual information compares the probability of observing x and y together the joint probability with the probabilities of observing x and y independently chance .", "If there is a genuine association between x and y , then the joint probability P x , y will be much larger than chance P x P y , and consequently 1 x , y 0 .", "If there is no interesting relationship between x and y , then P x , y P x P y , and thus , 1 x , y 2 0 .", "If x and y are in complementary distribution , then P x , y will be much less than P x P y , forcing 1 x , y 0 .", "In our application , word probabilities , P x and P y , are estimated by counting the number of observations of x and y in a corpus , f x and f y , and normalizing by N , the size of the corpus .", "Our examples use a number of different corpora with different sizes 15 million words for the 1987 AP corpus , 36 million words for the 1988 AP corpus , and 8 . 6 million tokens for the tagged corpus .", "Joint probabilities , P x , y , are estimated by counting the number of times that x is followed by y in a window of w words , f x , y , and normalizing by N . The window size parameter allows us to look at different scales .", "Smaller window sizes will identify fixed expressions idioms and other relations that hold over short ranges ; larger window sizes will highlight semantic concepts and other relationships that hold over larger scales .", "For the remainder of this paper , the window size , w , will be set to 5 words as a compromise ; this setting is large enough to show some of the constraints between verbs and arguments , but not so large that it would wash out constraints that make use of strict adjacency . 1 Since the association ratio becomes unstable when the counts are very small , we will not discuss word pairs with f x , y S 5 .", "An improvement would make use of t scores , and throw out pairs that were not significant .", "Unfortunately , this requires an estimate of the variance of f x , y , which goes beyond the scope of this paper .", "For the remainder of this paper , we will adopt the simple but arbitrary threshold , and ignore pairs with small counts .", "Technically , the association ratio is different from mutual information in two respects .", "First , joint probabilities are supposed to be symmetric P x , y P y , x , and thus , mutual information is also symmetric 1 x , y 1 y , x .", "However , the association ratio is not symmetric , since f x , y encodes linear precedence .", "Recall that f x , y denotes the number of times that word x appears before y in the window of w words , not the number of times the two words appear in either order .", "Although we could fix this problem by redefining f x , y to be symmetric by averaging the matrix with its transpose , we have decided not to do so , since order information appears to be very interesting .", "Notice the asymmetry in the pairs below computed from 36 million words of 1988 AP text , illustrating a wide variety of biases ranging from sexism to syntax .", "Some Un interesting Associations with quot ; Doctor quot ; Asymmetry in 1988 AP Corpus N 36 million f x , y f y , x doctors nurses 81 10 man woman 209 42 doctors lawyers 25 16 bread butter 14 0 save life 106 8 save money 155 8 save from 144 16 supposed to 982 21 Secondly , one might expect f x , y _f x and f x , y f Y , but the way we have been counting , this needn't be the case if x and y happen to appear several times in the window .", "For example , given the sentence , quot ; Library workers were prohibited from saving books from this heap of ruins , quot ; which appeared in an AP story on April 1 , 1988 , f prohibited 1 and f prohibited , front 2 .", "This problem can be fixed by dividing f x , y by w 1 which has the consequence of subtracting log2 W I 2 from our association ratio scores .", "This adjustment has the additional benefit of assuring that f x , y f x f y N . When 1 x , y is large , the association ratio produces very credible results not unlike those reported in Palermo and Jenkins 1964 , as illustrated in the table below .", "In contrast , when 1 x , y ' 0 , the pairs less interesting .", "As a very rough rule of thumb , we have observed that pairs with 1 x , y 3 tend to be interesting , and pairs with smaller f x , y are generally not .", "One can make this statement precise by calibrating the measure with subjective measures .", "Alternatively , one could make estimates of the variance and then make statements about confidence levels , e . g . , with 95 confidence , P x , y P x P y .", "Some Interesting Associations with quot ; Doctor quot ; In the 1987 AP Corpus N 15 million If 1 x , y 0 , we would predict that x and y are in complementary distribution .", "However , we are rarely able to observe 1 x , y 0 because our corpora are too small and our measurement techniques are too crude .", "Suppose , for example , that both x and y appear about 10 times per million words of text .", "Then , P x P y 10 5 and chance is P x P x 10i0 .", "Thus , to say that x , y is much less than 0 , we need to say that P x , y is much less than 10 1 , a statement that is hard to make with much confidence given the size of presently available corpora .", "In fact , we cannot easily observe a probability less than 1 N 10 7 , and therefore , it is hard to know if I x , y is much less than chance or not , unless chance is very large .", "In fact , the pair a , doctors above , appears significantly less often than chance .", "But to justify this statement , we need to compensate for the window size which shifts the score downward by 2 . 0 , e . g . from 0 . 96 down to 1 . 04 and we need to estimate the standard deviation , using a method such as Good 1953 .", "Although the psycholinguistic literature documents the significance of noun noun word associations such as doctor nurse in considerable detail , relatively little is said about associations among verbs , function words , adjectives , and other non nouns .", "In addition to identifying semantic relations of the doctor nurse variety , we believe the association ratio can also be used to search for interesting lexico syntactic relationships between verbs and typical arguments adjuncts .", "The proposed association ratio can be viewed as a formalization of Sinclair's argument quot ; How common are the phrasal verbs with set ?", "Set is particularly rich in making combinations with words like about . in . up , out . on , off , and these words are themselves very common .", "How likely is set off to occur ?", "Both are frequent words ; set occurs approximately 250 times in a million words and off occurs approximately 556 times in a million words . . . The question we are asking can be roughly rephrased as follows how likely is off to occur immediately after set ?", "This is 0 . 00025 x0 . 00055 P s P y , which gives us the tiny figure of 0 . 0000001375 . . .", "The assumption behind this calculation is that the words are distributed at random in a text at chance , in our terminology .", "It is obvious to a linguist that this is not so , and a rough measure of how much set and off attract each other is to compare the probability with what actually happens . . . Set off occurs nearly 70 times in the 7 . 3 million word corpus P x , y . 0701 7 . 3 106 P x P y .", "That is enough to show its main patterning and it suggests that in currently held corpora there will be found sufficient evidence for the descliption of a substantial collection of phrases . . . Sinclair 1987 e . pp .", "151 152 It happens that set . . . off was found 177 times in the 1987 AP Corpus of approximately 15 million words , about the same number of occurrences per million as Sinclair found in his mainly British corpus .", "Quantitatively , 1 set , o ff 5 . 9982 , indicating that the probability of set . . . off is almost 64 times greater than chance .", "This association is relatively strong ; the other particles that Sinclair mentions have association ratios of about 1 . 4 , in 2 . 9 , up 6 . 9 , out 4 . 5 , on 3 . 3 in the 1987 AP Corpus .", "As Sinclair suggests , the approach is well suited for identifying phrasal verbs .", "However , phrasal verbs involving the preposition to raise an interesting problem because of the possible confusion with the infinitive marker to .", "We have found that if we first tag every word in the corpus with a part of speech using a method such as Church 1988 , and then measure associations between tagged words , we can identify interesting contrasts between verbs associated with a following preposition to in and verbs associated with a following infinitive marker to to .", "Part of speech notation is borrowed from Francis and Kucera 1982 ; in preposition ; to infinitive marker ; vb bare verb ; vbg verb ing ; vbd verb ed ; vbz verb s ; vbn verb en .", "The association ratio identifies quite a number of verbs associated in an interesting way with to ; restricting our attention to pairs with a score of 3 . 0 or more , there are 768 verbs associated with the preposition to in and 551 verbs with the infinitive marker to to .", "The ten verbs found to be most associated before to in are Thus , we see there is considerable leverage to be gained by preprocessing the corpus and manipulating the inventory of tokens .", "For measuring syntactic constraints , it may be useful to include some part of speech information and to exclude much of the internal structure of noun phrases .", "For other purposes , it may be helpful to tag items and or phrases with semantic labels such as person , place , time , body part , bad , etc .", "Hindle personal communication has found it helpful to preprocess the input with the Fidditch parser Hindle 1983a , b in order to identify associations between verbs and arguments , and postulate semantic classes for nouns on this basis .", "Large machine readable corpora are only just now becoming available to lexicographers .", "Up to now , lexicographers have been reliant either on citations collected by human readers , which introduced an element of selectivity and so inevitably distortion rare words and uses were collected but common uses of common words were not , or on small corpora of only a million words or so , which are reliably informative for only the most common uses of the few most frequent words of English .", "A million word corpus such as the Brown Corpus is reliable , roughly , for only some uses of only some of the forms of around 4000 dictionary entries .", "But standard dictionaries typically contain twenty times this number of entries .", "The computational tools available for studying machine readable corpora are at present still rather primitive .", "There are concordancing programs see Figure 1 at the end of this paper , which are basically KWIC key word in context Aho , Kernighan , and Weinberger 1988 , p . 122 indexes with additional features such as the ability to extend the context , sort leftwards as well as rightwards , and so on .", "There is very little interactive software .", "In a typical situation in the lexicography of the 1980s , a lexicographer is given the concordances for a word , marks up the printout with colored pens in order to identify the salient senses , and then writes syntactic descriptions and definitions .", "Although this technology is a great improvement on using human readers to collect boxes of citation index cards the method Murray used in constructing the Oxford English Dictionary a century ago , it works well if there are no more than a few dozen concordance lines for a word , and only two or three main sense divisions .", "In analyzing a complex word such as quot ; take quot ; , quot ; save quot ; , or quot ; from quot ; , the lexicographer is trying to pick out significant patterns and subtle distinctions that are buried in literally thousands of concordance lines pages and pages of computer printout .", "The unaided human mind simply cannot discover all the significant patterns , let alone group them and rank in order of importance .", "The AP 1987 concordance to quot ; save quot ; is many pages long ; there are 666 lines for the base form alone , and many more for the inflected forms quot ; saved , quot ; quot ; saves , quot ; quot ; saving , quot ; and quot ; savings . quot ; In the discussion that follows , we shall , for the sake of simplicity , not analyze the inflected forms and we shall only look at the patterns to the right of quot ; save quot ; .", "It is hard to know what is important in such a concordance and what is not .", "For example , although it is easy to see from the concordance selection in Figure 1 that the word quot ; to quot ; often comes before quot ; save quot ; and the word quot ; the quot ; often comes after quot ; save , quot ; it is hard to say from examination of a concordance alone whether either or both of these co occurrences have any significance .", "Two examples will be illustrate how the association ratio measure helps make the analysis both quicker and more accurate .", "The association ratios above show that association norms apply to function words as well as content words .", "For example , one of the words significantly associated with quot ; save quot ; is quot ; from quot ; .", "Many dictionaries , for example Merriam Webster's Ninth , make no explicit mention of quot ; from quot ; in the entry for quot ; save quot ; , although British learners' dictionaries do make specific mention of quot ; from quot ; in connection with quot ; save quot ; .", "These learners' dictionaries pay more attention to language structure and collocation than do American collegiate dictionaries , and lexicographers trained in the British tradition are often fairly skilled at spotting these generalizations .", "However , teasing out such facts , and distinguishing true intuitions from false intuitions takes a lot of time and hard work , and there is a high probability of inconsistencies and omissions .", "Which other verbs typically associate with quot ; from , quot ; and where does quot ; save quot ; rank in such a list ?", "The association ratio identified 1530 words that are associated with quot ; from quot ; ; 911 of them were tagged as verbs .", "The first 100 verbs are quot ; Save . . . from quot ; is a good example for illustrating the advantages of the association ratio .", "Save is ranked 319th in this list , indicating that the association is modest , strong enough to be important 21 times more likely than chance , but not so strong that it would pop out at us in a concordance , or that it would be one of the first things to come to mind .", "If the dictionary is going to list quot ; save . . . from , quot ; listing all of the more important associations as well .", "Of the 27 bare verbs tagged vb , in the list above , all but 7 are listed in the Cobuild dictionary as occurring with quot ; from quot ; .", "However , this dictionary does not note that vary , ferry , strip , divert , forbid , and reap occur with quot ; from . quot ; If the Cobuild lexicographers had had access to the proposed measure , they could possibly have obtained better coverage at less cost .", "Having established the relative importance of quot ; save . . . from quot ; , and having noted that the two words are rarely adjacent , we would now like to speed up the labor intensive task of categorizing the concordance lines .", "Ideally , we would like to develop a set of semi automatic tools that would help a lexicographer produce something like Figure 2 , which provides an annotated summary of the 65 concordance lines for quot ; save . . . from . quot ; 2 The quot ; save . . . from quot ; pattern occurs in about 10 of the 666 concordance lines for quot ; save . quot ; Traditionally , semantic categories have been only vaguely recognized , and to date little effort has been devoted to a systematic classification of a large corpus .", "Lexicographers have tended to use concordances impressionistically ; semantic theorist , AI ers , and others have concentrated on a few interesting examples , e . g . , quot ; bachelor , quot ; and have not given much thought to how the results might be scaled up .", "With this concern in mind , it seems reasonable to ask how well these 65 lines for quot ; save . . . from quot ; fit in with all other uses of quot ; save quot ; ?", "A laborious concordance analysis was undertaken to answer this question .", "When it was nearing completion , we noticed that the tags that we were inventing to capture the generalizations could in most cases have been suggested by looking at the lexical items listed in the association ratio table for quot ; save quot ; .", "For example , we had failed to notice the significance of time adverbials in our analysis of quot ; save , quot ; and no 2 .", "The last unclassified line , quot ; . . . save shoppers anywhere from SSD . . . quot ; raises interesting problems .", "Syntactic quot ; chunking quot ; shows that , in spite of its co occurrence of quot ; from quot ; with quot ; save quot ; , this line does not belong here .", "An intriguing exercise , given the lookup table we are trying to construct , is how to guard against false inferences such as that since quot ; shoppers quot ; is tagged PERSON .", "quot ; SO to 500 quot ; must here count an either BAD or a LOCATION .", "Accidental coincidences of this kind do not have a significant effect on the measure , however , although they do serve u a reminder of the probabilistic nature of the findings . dictionary records this .", "Yet it should be clear from the association ratio table above that quot ; annually quot ; and quot ; month quot ; 3 are commonly found with quot ; save quot ; .", "More detailed inspection shows that the time adverbials correlate interestingly with just one group of quot ; save quot ; objects , namely those tagged MONEY .", "The AP wire is full of discussions of quot ; saving 1 . 2 billion per month quot ; ; computational lexicography should measure and record such patterns if they are general , even when traditional dictionaries do not .", "As another example illustrating how the association ratio tables would have helped us analyze the quot ; save quot ; concordance lines , we found ourselves contemplating the semantic tag ENV IRONMENT in order to analyze lines such as the trend to save the forests ENV it's our turn to save the lake ENV , joined a fight to save their forests ENV , can we get busy to save the planet ENV ?", "If we had looked at the association ratio tables before labeling the 65 lines for quot ; save . . . from , quot ; we might have noticed the very large value for quot ; save . . . forests , quot ; suggesting that there may be an important pattern here .", "In fact , this pattern probably subsumes most of the occurrences of the quot ; save ANIMAL quot ; pattern noticed in Figure 2 .", "Thus , tables do not provide semantic tags , but they provide a powerful set of suggestions to the lexicographer for what needs to be accounted for in choosing a set of semantic tags .", "It may be that everything said here about quot ; save quot ; and other words is true only of 1987 American journalese .", "Intuitively , however , many of the patterns discovered seem to be good candidates for conventions of general English .", "A future step would be to examine other more balanced corpora and test how well the patterns hold up .", "We began this paper with the psycholinguistic notion of word association norm , and extended that concept toward the information theoretic definition of mutual information .", "This provided a precise statistical calculation that could be applied to a very 3 .", "The word quot ; time quot ; itself also occurs significantly in the table , but on closer examination it is clear that this use of quot ; time quot ; e . g . , quot ; to save time quot ; counts as something like a commodity or resource , not as part of a time adjunct .", "Such are the pitfalls of lexicography obvious when they are pointed out . large corpus of text in order to produce a table of associations for tens of thousands of words .", "We were then able to show that the table encoded a number of very interesting patterns ranging from doctor . . . nurse to save . . . from .", "We finally concluded by showing how the patterns in the association ratio table might help a lexicographer organize a concordance .", "In point of fact , we actually developed these results in basically the reverse order .", "Concordance analysis is still extremely labor intensive , and prone to errors of omission .", "The ways that concordances are sorted don't adequately support current lexicographic practice .", "Despite the fact that a concordance is indexed by a single word , often lexicographers actually use a second word such as quot ; from quot ; or an equally common semantic concept such as a time adverbial to decide how to categorize concordance lines .", "In other words , they use two words to triangulate in on a word sense .", "This triangulation approach clusters concordance lines together into word senses based primarily on usage distributional evidence , as opposed to intuitive notions of meaning .", "Thus , the question of what is a word sense can be addressed with syntactic methods symbol pushing , and need not address semantics interpretation , even though the inventory of tags may appear to have semantic values .", "The triangulation approach requires quot ; art . quot ; How does the lexicographer decide which potential cut points are quot ; interesting quot ; and which are merely due to chance ?", "The proposed association ratio score provides a practical and objective measure which is often a fairly good approximation to the quot ; art . quot ; Since the proposed measure is objective , it can be applied in a systematic way over a large body of material , steadily improving consistency and productivity .", "But on the other hand , the objective score can be misleading .", "The score takes only distributional evidence into account .", "For example , the measure favors quot ; set . . . for quot ; over quot ; set . . . down quot ; ; it doesn't know that the former is less interesting because its semantics are compositional .", "In addition , the measure is extremely superficial ; it cannot cluster words into appropriate syntactic classes without an explicit preprocess such as Church's parts program 'or Hindle's parser .", "Neither of these preprocesses , though , can help highlight the quot ; natural quot ; similarity between nouns such as quot ; picture quot ; and quot ; photograph . quot ; Although one might imagine a preprocess that would help in this particular case , there will probably always be a class of generalizations that are obvious to an intelligent lexicographer , but lie hopelessly beyond the objectivity of a computer .", "Despite these problems , the association ratio could be an important tool to aid the lexicographer , rather like an index to the concordances .", "It can help us decide what to look for ; it provides a quick summary of what company our words do keep .", "rs Sunday , calling for greater economic reforms to mtniasion asserted that quot ; the Postal Service could Then , she said , the family hopes to e out of work steelworker .", "quot ; because that doesn't quot ; We suspend reality when we say we'll scientists has won the first round in an effort to about three children in a mining town who plot to GM executives say the shutdowns will rtment as receiver , instructed officials to try to The package , which is to newly enhanced image as the moderate who moved to million offer from chairman Victor Posner to help after telling a delivery room doctor not to try to h birthday Tuesday , cheered by those who fought to at he had formed an alliance with Moslem rebels to quot ; Basically we could We worked for a year to their expensive mirrors , just like in wartime , to ard of many who risked their own lives in order to We must increase the amount Americans save China from poverty . save enormous sums of money in contracting out individual c save enough for a down payment on a home . save jobs , that costs jobs .", "quot ; save money by spending 10 , 000 in wages for a public works save one of Egypt's great treasures , the decaying tomb of R save the quot ; pit ponies quot ; doomed to be slaughtered . save the automaker 500 million a year in operating costs a save the company rather than liquidate it and then declared save the country nearly 2 billion , also includes a program save the country . save the financially troubled company , but said Posner stil save the infant by inserting a tube in its throat to help i save the majestic Beaux Arts architectural masterpiece . save the nation from communism . save the operating costs of the Pershings and pound launch save the site at enormous expense to us , quot ; said Leveillee . save them from drunken Yankee brawlers , quot ; Tess said . save those who were passengers .", "quot ; save .", "quot ;", "Robert DeNiro to quot ; We wanted to Murphy was sacrificed to quot ; God sent this man to Pope John Paul II to quot ; save Indian tribes PERSON from genocide DESTRUCT BAD at the hands of save hirn PERSON from undue trouble BADI and loss BAD of money , quot ; save more powerful Democrats PERSON from harm BAD . save my five diikiren PERSON from being burned to death DESTRUCT BAD and save us PERSON1 from sin BAD .", "quot ; rescuers who helped save the toddler PERSON from an abandoned well LOC will be feted with a parade while attempting to save two drowning boys PERSON from a turbulent BAD creek LOC in Ohio LOC", "member states to help should be sought quot ; to law was necessary to operation quot ; to were not needed to his efforts to save the BEC INST from possible bankruptcy ECONPAD this year . save the company CORP DIST from bankruptcy ECON BAD . save the country NATIONINSTD from dismter BAD1 save the nation NATION INST J from Conununism BADNPOLITICAL save the system from bankruptcy ECONHBAD . save the world INST from the likes of Lothar and the Spider Woman", "give them the money to save the dogs ANIMAL from being destroyed DESTRUC11 , program intended to save the giant birds ANIMAL from extinction DESTRUCT ,"], "summary_lines": ["Word Association Norms Mutual Information And Lexicography\n", "The term word association is used in a very particular sense in the psycholinguistic literature.\n", "(Generally speaking, subjects respond quicker than normal to the word \"nurse\" if it follows a highly associated word such as \"doctor\").\n", "We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word) to lexico-syntactic co-occurrence constraints between verbs and prepositions (content word/function word).\n", "This paper will propose a new objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora.\n", "(The standard method of obtaining word association norms, testing a few thousand subjects on a few hundred words, is both costly and unreliable).\n", "The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, making it possible to estimate norms for tens of thousands of words.\n"]}
{"article_lines": ["Reranking And Self Training For Parser Adaptation", "Statistical parsers trained and tested on the Wall Street Journal treebank have shown vast improvements over the last 10 years .", "Much of this improvement , however , is based upon an ever increasing number of features to be trained on typithe data .", "This has led to concern that such parsers may be too finely tuned to this corpus at the expense of portability to other genres .", "Such worries have merit .", "The standard Charniak parser checks in at a labeled precisionof 89 . 7 on the Penn set , but only 82 . 9 on the test set from the Brown treebank corpus .", "This paper should allay these fears .", "In particular , we show that the reranking parser described in Charniak and Johnson 2005 improves performance of the parser on Brown to 85 . 2 .", "Furthermore , use of the self training techniques described in Mc Closky et al . , 2006 raise this to 87 . 8 an error reduction of 28 again without any use of labeled Brown data .", "This is remarkable since training the parser and reranker on labeled Brown data achieves only 88 . 4 .", "Modern statistical parsers require treebanks to train their parameters , but their performance declines when one parses genres more distant from the training data s domain .", "Furthermore , the treebanks required to train said parsers are expensive and difficult to produce .", "Naturally , one of the goals of statistical parsing is to produce a broad coverage parser which is relatively insensitive to textual domain .", "But the lack of corpora has led to a situation where much of the current work on parsing is performed on a single domain using training data from that domain the Wall Street Journal WSJ section of the Penn Treebank Marcus et al . , 1993 .", "Given the aforementioned costs , it is unlikely that many significant treebanks will be created for new genres .", "Thus , parser adaptation attempts to leverage existing labeled data from one domain and create a parser capable of parsing a different domain .", "Unfortunately , the state of the art in parser portability i . e . using a parser trained on one domain to parse a different domain is not good .", "The Charniak parser has a labeled precision recall f measure of 89 . 7 on WSJ but a lowly 82 . 9 on the test set from the Brown corpus treebank .", "Furthermore , the treebanked Brown data is mostly general non fiction and much closer to WSJ than , e . g . , medical corpora would be .", "Thus , most work on parser adaptation resorts to using some labeled in domain data to fortify the larger quantity of outof domain data .", "In this paper , we present some encouraging results on parser adaptation without any in domain data .", "Though we also present results with indomain data as a reference point .", "In particular we note the effects of two comparatively recent techniques for parser improvement .", "The first of these , parse reranking Collins , 2000 ; Charniak and Johnson , 2005 starts with a standard generative parser , but uses it to generate the n best parses rather than a single parse .", "Then a reranking phase uses more detailed features , features which would mostly be impossible to incorporate in the initial phase , to reorder the list and pick a possibly different best parse .", "At first blush one might think that gathering even more fine grained features from a WSJ treebank would not help adaptation .", "However , we find that reranking improves the parsers performance from 82 . 9 to 85 . 2 .", "The second technique is self training parsing unlabeled data and adding it to the training corpus .", "Recent work , McClosky et al . , 2006 , has shown that adding many millions of words of machine parsed and reranked LA Times articles does , in fact , improve performance of the parser on the closely related WSJ data .", "Here we show that it also helps the father afield Brown data .", "Adding it improves performance yet again , this time from 85 . 2 to 87 . 8 , for a net error reduction of 28 .", "It is interesting to compare this to our results for a completely Brown trained system i . e . one in which the first phase parser is trained on just Brown training data , and the second phase reranker is trained on Brown 50 best lists .", "This system performs at a 88 . 4 level only slightly higher than that achieved by our system with only WSJ data .", "Work in parser adaptation is premised on the assumption that one wants a single parser that can handle a wide variety of domains .", "While this is the goal of the majority of parsing researchers , it is not quite universal .", "Sekine 1997 observes that for parsing a specific domain , data from that domain is most beneficial , followed by data from the same class , data from a different class , and data from a different domain .", "He also notes that different domains have very different structures by looking at frequent grammar productions .", "For these reasons he takes the position that we should , instead , simply create treebanks for a large number of domains .", "While this is a coherent position , it is far from the majority view .", "There are many different approaches to parser adaptation .", "Steedman et al . 2003 apply cotraining to parser adaptation and find that cotraining can work across domains .", "The need to parse biomedical literature inspires Clegg and Shepherd , 2005 ; Lease and Charniak , 2005 .", "Clegg and Shepherd 2005 provide an extensive side by side performance analysis of several modern statistical parsers when faced with such data .", "They find that techniques which combine differBrown test corpora using different WSJ and Brown training sets .", "Gildea evaluates on sentences of length 40 , Bacchiani on all sentences . ent parsers such as voting schemes and parse selection can improve performance on biomedical data .", "Lease and Charniak 2005 use the Charniak parser for biomedical data and find that the use of out of domain trees and in domain vocabulary information can considerably improve performance .", "However , the work which is most directly comparable to ours is that of Ratnaparkhi , 1999 ; Hwa , 1999 ; Gildea , 2001 ; Bacchiani et al . , 2006 .", "All of these papers look at what happens to modern WSJ trained statistical parsers Ratnaparkhi s , Collins , Gildea s and Roark s , respectively as training data varies in size or usefulness because we are testing on something other than WSJ .", "We concentrate particularly on the work of Gildea , 2001 ; Bacchiani et al . , 2006 as they provide results which are directly comparable to those presented in this paper .", "Looking at Table 1 , the first line shows us the standard training and testing on WSJ both parsers perform in the 86 87 range .", "The next line shows what happens when parsing Brown using a WSJ trained parser .", "As with the Charniak parser , both parsers take an approximately 6 hit .", "It is at this point that our work deviates from these two papers .", "Lacking alternatives , both Gildea , 2001 and Bacchiani et al . , 2006 give up on adapting a pure WSJ trained system , instead looking at the issue of how much of an improvement one gets over a pure Brown system by adding WSJ data as seen in the last two lines of Table 1 .", "Both systems use a model merging Bacchiani et al . , 2006 approach .", "The different corpora are , in effect , concatenated together .", "However , Bacchiani et al . , 2006 achieve a larger gain by weighting the in domain Brown data more heavily than the out of domain WSJ data .", "One can imagine , for instance , five copies of the Brown data concatenated with just one copy of WSJ data .", "We primarily use three corpora in this paper .", "Selftraining requires labeled and unlabeled data .", "We assume that these sets of data must be in similar domains e . g . news articles though the effectiveness of self training across domains is currently an open question .", "Thus , we have labeled WSJ and unlabeled NANC out of domain data and labeled in domain data BROWN .", "Unfortunately , lacking a corresponding corpus to NANC for BROWN , we cannot perform the opposite scenario and adapt BROWN to WSJ .", "The BROWN corpus Francis and Ku\u02c7cera , 1979 consists of many different genres of text , intended to approximate a balanced corpus .", "While the full corpus consists of fiction and nonfiction domains , the sections that have been annotated in Treebank II bracketing are primarily those containing fiction .", "Examples of the sections annotated include science fiction , humor , romance , mystery , adventure , and popular lore . We use the same divisions as Bacchiani et al . 2006 , who base their divisions on Gildea 2001 .", "Each division of the corpus consists of sentences from all available genres .", "The training division consists of approximately 80 of the data , while held out development and testing divisions each make up 10 of the data .", "The treebanked sections contain approximately 25 , 000 sentences 458 , 000 words .", "Our out of domain data is the Wall Street Journal WSJ portion of the Penn Treebank Marcus et al . , 1993 which consists of about 40 , 000 sentences one million words annotated with syntactic information .", "We use the standard divisions Sections 2 through 21 are used for training , section 24 for held out development , and section 23 for final testing .", "In addition to labeled news data , we make use of a large quantity of unlabeled news data .", "The unlabeled data is the North American News Corpus , NANC Graff , 1995 , which is approximately 24 million unlabeled sentences from various news sources .", "NANC contains no syntactic information and sentence boundaries are induced by a simple discriminative model .", "We also perform some basic cleanups on NANC to ease parsing .", "NANC contains news articles from various news sources including the Wall Street Journal , though for this paper , we only use articles from the LA Times portion .", "To use the data from NANC , we use self training McClosky et al . , 2006 .", "First , we take a WSJ trained reranking parser i . e . both the parser and reranker are built from WSJ training data and parse the sentences from NANC with the 50 best Charniak and Johnson , 2005 parser .", "Next , the 50 best parses are reordered by the reranker .", "Finally , the 1 best parses after reranking are combined with the WSJ training set to retrain the firststage parser . 1 McClosky et al . 2006 find that the self trained models help considerably when parsing WSJ .", "We use the Charniak and Johnson 2005 reranking parser in our experiments .", "Unless mentioned otherwise , we use the WSJ trained reranker as opposed to a BROWN trained reranker .", "To evaluate , we report bracketing f scores . 2 Parser f scores reported are for sentences up to 100 words long , while reranking parser f scores are over all sentences .", "For simplicity and ease of comparison , most of our evaluations are performed on the development section of BROWN .", "Our first experiment examines the performance of the self trained parsers .", "While the parsers are created entirely from labeled WSJ data and unlabeled NANC data , they perform extremely well on BROWN development Table 2 .", "The trends are the same as in McClosky et al . , 2006 Adding NANC data improves parsing performance on BROWN development considerably , improving the f score from 83 . 9 to 86 . 4 .", "As more NANC data is added , the f score appears to approach an asymptote .", "The NANC data appears to help reduce data sparsity and fill in some of the gaps in the WSJ model .", "Additionally , the reranker provides further benefit and adds an absolute 1 2 to the fscore .", "The improvements appear to be orthogonal , as our best performance is reached when we use the reranker and add 2 , 500k self trained sentences from NANC . training data on parsing performance . f scores for the parser with and without the WSJ reranker are shown when evaluating on BROWN development .", "For this experiment , we use the WSJ trained reranker .", "The results are even more surprising when we compare against a parser3 trained on the labeled training section of the BROWN corpus , with parameters tuned against its held out section .", "Despite seeing no in domain data , the WSJ based parser is able to match the BROWN based parser .", "For the remainder of this paper , we will refer to the model trained on WSJ 2 , 500k sentences of NANC as our best WSJ NANC model .", "We also note that this best parser is different from the best parser for parsing WSJ , which was trained on WSJ with a relative weight4 of 5 and 1 , 750k sentences from NANC .", "For parsing BROWN , the difference between these two parsers is not large , though .", "Increasing the relative weight of WSJ sentences versus NANC sentences when testing on BROWN development does not appear to have a significant effect .", "While McClosky et al . , 2006 showed that this technique was effective when testing on WSJ , the true distribution was closer to WSJ so it made sense to emphasize it .", "Up to this point , we have only considered the situation where we have no in domain data .", "We now explore different ways of making use of labeled and unlabeled in domain data .", "Bacchiani et al . 2006 applies self training to parser adaptation to utilize unlabeled in domain data .", "The authors find that it helps quite a bit when adapting from BROWN to WSJ .", "They use a parser trained from the BROWN train set to parse WSJ and add the parsed WSJ sentences to their training set .", "We perform a similar experiment , using our WSJtrained reranking parser to parse BROWN train and testing on BROWN development .", "We achieved a boost from 84 . 8 to 85 . 6 when we added the parsed BROWN sentences to our training .", "Adding in 1 , 000k sentences from NANC as well , we saw a further increase to 86 . 3 .", "However , the technique does not seem as effective in our case .", "While the self trained BROWN data helps the parser , it adversely affects the performance of the reranking parser .", "When self trained BROWN data is added to WSJ training , the reranking parser s performance drops from 86 . 6 to 86 . 1 .", "We see a similar degradation as NANC data is added to the training set as well .", "We are not yet able to explain this unusual behavior .", "We now turn to the scenario where we have some labeled in domain data .", "The most obvious way to incorporate labeled in domain data is to combine it with the labeled out of domain data .", "We have already seen the results Gildea , 2001 and Bacchiani et al . , 2006 achieve in Table 1 .", "We explore various combinations of BROWN , WSJ , and NANC corpora .", "Because we are mainly interested in exploring techniques with self trained models rather than optimizing performance , we only consider weighting each corpus with a relative weight of one for this paper .", "The models generated are tuned on section 24 from WSJ .", "The results are summarized in Table 3 .", "While both WSJ and BROWN models benefit from a small amount of NANC data , adding more than 250k NANC sentences to the BROWN or combined models causes their performance to drop .", "This is not surprising , though , since adding too much NANC overwhelms the more accurate BROWN or WSJ counts .", "By weighting the counts from each corpus appropriately , this problem can be avoided .", "Another way to incorporate labeled data is to tune the parser back off parameters on it .", "Bacchiani et al . 2006 report that tuning on held out BROWN data gives a large improvement over tuning on WSJ data .", "The improvement is mostly but not entirely in precision .", "We do not see the same improvement Figure 1 but this is likely due to differences in the parsers .", "However , we do see a similar improvement for parsing accuracy once NANC data has been added .", "The reranking parser generally sees an improvement , but it does not appear to be significant .", "We have shown that the WSJ trained reranker is actually quite portable to the BROWN fiction domain .", "This is surprising given the large number of features over a million in the case of the WSJ reranker tuned to adjust for errors made in the 50best lists by the first stage parser .", "It would seem the corrections memorized by the reranker are not as domain specific as we might expect .", "As further evidence , we present the results of applying the WSJ model to the Switchboard corpus a domain much less similar to WSJ than BROWN .", "In Table 4 , we see that while the parser s performance is low , self training and reranking provide orthogonal benefits .", "The improvements represent a 12 error reduction with no additional in domain data .", "Naturally , in domain data and speech specific handling e . g . disfluency modeling would probably help dramatically as well .", "Finally , to compare against a model fully trained on BROWN data , we created a BROWN reranker .", "We parsed the BROWN training set with 20 fold cross validation , selected features that occurred 5 times or more in the training set , and fed the 50 best lists from the parser to a numerical optimizer to estimate feature weights .", "The resulting reranker model had approximately 700 , 000 features , which is about half as many as the WSJ trained reranker .", "This may be due to the smaller size of the BROWN training set or because the feature schemas for the reranker were developed on WSJ data .", "As seen in Table 5 , the BROWN reranker is not a significant improvement over the WSJ reranker for parsing BROWN data .", "We perform several types of analysis to measure some of the differences and similarities between the BROWN trained and WSJ trained reranking parsers .", "While the two parsers agree on a large number of parse brackets Section 5 . 2 , there are categorical differences between them as seen in Section 5 . 3 .", "Table 6 shows the f scores of an oracle reranker i . e . one which would always choose the parse with the highest f score in the n best list .", "While the WSJ parser has relatively low f scores , adding NANC data results in a parser with comparable oracle scores as the parser trained from BROWN training .", "Thus , the WSJ NANC model has better oracle rates than the WSJ model McClosky et al . , 2006 for both the WSJ and BROWN domains .", "In this section , we compare the output of the WSJ NANC trained and BROWN trained reranking parsers .", "We use evalb to calculate how similar the two sets of output are on a bracket level .", "Table 7 shows various statistics .", "The two parsers achieved an 88 . 0 f score between them .", "Additionally , the two parsers agreed on all brackets almost half the time .", "The part of speech tagging agreement is fairly high as well .", "Considering they were created from different corpora , this seems like a high level of agreement .", "We conducted randomization tests for the significance of the difference in corpus f score , based on the randomization version of the paired sample ttest described by Cohen 1995 .", "The null hypothesis is that the two parsers being compared are in fact behaving identically , so permuting or swapping the parse trees produced by the parsers for of NANC sentences added under four test conditions .", "BROWN tuned indicates that BROWN training data was used to tune the parameters since the normal held out section was being used for testing .", "For WSJ tuned , we tuned the parameters from section 24 of WSJ .", "Tuning on BROWN helps the parser , but not for the reranking parser . ment .", "The reranking parser used the WSJ trained reranker model .", "The BROWN parsing model is naturally better than the WSJ model for this task , but combining the two training corpora results in a better model as in Gildea 2001 .", "Adding small amounts of NANC further improves the models . test .", "The WSJ NANC parser with the WSJ reranker comes close to the BROWN trained reranking parser .", "The BROWN reranker provides only a small improvement over its WSJ counterpart , which is not statistically significant . parser with the WSJ reranker and the BROWN parser with the BROWN reranker .", "Complete match is how often the two reranking parsers returned the exact same parse . the same test sentence should not affect the corpus f scores .", "By estimating the proportion of permutations that result in an absolute difference in corpus f scores at least as great as that observed in the actual output , we obtain a distributionfree estimate of significance that is robust against parser and evaluator failures .", "The results of this test are shown in Table 8 .", "The table shows that the BROWN reranker is not significantly different from the WSJ reranker .", "In order to better understand the difference between the reranking parser trained on Brown and the WSJ NANC WSJ reranking parser a reranking parser with the first stage trained on WSJ NANC and the second stage trained on WSJ on Brown data , we constructed a logistic regression model of the difference between the two parsers fscores on the development data using the R statistical package5 .", "Of the 2 , 078 sentences in the development data , 29 sentences were discarded because evalb failed to evaluate at least one of the parses . 6 A Wilcoxon signed rank test on the remaining 2 , 049 paired sentence level f scores was significant at p 0 . 0003 .", "Of these 2 , 049 sentences , there were 983 parse pairs with the same sentence level f score .", "Of the 1 , 066 sentences for which the parsers produced parses with different f scores , there were 580 sentences for which the BROWN BROWN parser produced a parse with a higher sentence level f score and 486 sentences for which the WSJ NANC WSJ parser produce a parse with a higher f score .", "We constructed a generalized linear model with a binomial link with BROWN BROWN f score WSJ NANC WSJ f score as the predicted variable , and sentence length , the number of prepositions IN , the number of conjunctions CC and Brown f score WSJ NANC WSJ f score identified by model selection .", "The feature IN is the number prepositions in the sentence , while ID identifies the Brown subcorpus that the sentence comes from .", "Stars indicate significance level . subcorpus ID as explanatory variables .", "Model selection using the step procedure discarded all but the IN and Brown ID explanatory variables .", "The final estimated model is shown in Table 9 .", "It shows that the WSJ NANC WSJ parser becomes more likely to have a higher f score than the BROWN BROWN parser as the number of prepositions in the sentence increases , and that the BROWN BROWN parser is more likely to have a higher f score on Brown sections K , N , P , G and L these are the general fiction , adventure and western fiction , romance and love story , letters and memories , and mystery sections of the Brown corpus , respectively .", "The three sections of BROWN not in this list are F , M , and R popular lore , science fiction , and humor .", "We have demonstrated that rerankers and selftrained models can work well across domains .", "Models self trained on WSJ appear to be better parsing models in general , the benefits of which are not limited to the WSJ domain .", "The WSJtrained reranker using out of domain LA Times parses produced by the WSJ trained reranker achieves a labeled precision recall f measure of 87 . 8 on Brown data , nearly equal to the performance one achieves by using a purely Brown trained parser reranker .", "The 87 . 8 f score on Brown represents a 24 error reduction on the corpus .", "Of course , as corpora differences go , Brown is relatively close to WSJ .", "While we also find that our the difference in parentheses as estimated by a randomization test with 106 samples .", "x y indicates that the first stage parser was trained on data set x and the second stage reranker was trained on data set y .", "best WSJ parser reranker improves performance on the Switchboard corpus , it starts from a much lower base 74 . 0 , and achieves a much less significant improvement 3 absolute , 11 error reduction .", "Bridging these larger gaps is still for the future .", "One intriguing idea is what we call self trained bridging corpora . We have not yet experimented with medical text but we expect that the best WSJ NANC parser will not perform very well .", "However , suppose one does self training on a biology textbook instead of the LA Times .", "One might hope that such a text will split the difference between more normal newspaper articles and the specialized medical text .", "Thus , a selftrained parser based upon such text might do much better than our standard best . This is , of course , highly speculative .", "This work was supported by NSF grants LIS9720368 , and IIS0095940 , and DARPA GALE contract HR0011 06 20001 .", "We would like to thank the BLLIP team for their comments ."], "summary_lines": ["Reranking And Self-Training For Parser Adaptation\n", "Statistical parsers trained and tested on the Penn Wall Street Journal (WSJ) treebank have shown vast improvements over the last 10 years.\n", "Much of this improvement, however, is based upon an ever-increasing number of features to be trained on (typically) the WSJ treebank data.\n", "This has led to concern that such parsers may be too finely tuned to this corpus at the expense of portability to other genres.\n", "Such worries have merit.\n", "The standard \"Charniak parser\" checks in at a labeled precision-recall f-measure of 89.7% on the Penn WSJ test set, but only 82.9% on the test set from the Brown treebank corpus.\n", "This paper should allay these fears.\n", "In particular, we show that the reranking parser described in Charniak and Johnson (2005) improves performance of the parser on Brown to 85.2%.\n", "Furthermore, use of the self-training techniques described in (McClosky et al., 2006) raise this to 87.8% (an error reduction of 28%) again without any use of labeled Brown data.\n", "We successfully applied self-training to parsing by exploiting available unlabeled data, and obtained remarkable results when the same technique was applied to parser adaptation.\n"]}
{"article_lines": ["Vector based Models of Semantic Composition", "This paper proposes a framework for representing the meaning of phrases and sentences in vector space .", "Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions .", "Under this framework , we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task .", "Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments .", "Vector based models of word meaning Lund and Burgess , 1996 ; Landauer and Dumais , 1997 have become increasingly popular in natural language processing NLP and cognitive science .", "The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar Harris , 1968 .", "A variety of NLP tasks have made good use of vector based models .", "Examples include automatic thesaurus extraction Grefenstette , 1994 , word sense discrimination Sch utze , 1998 and disambiguation McCarthy et al . , 2004 , collocation extraction Schone and Jurafsky , 2001 , text segmentation Choi et al . , 2001 , and notably information retrieval Salton et al . , 1975 .", "In cognitive science vector based models have been successful in simulating semantic priming Lund and Burgess , 1996 ; Landauer and Dumais , 1997 and text comprehension Landauer and Dumais , 1997 ; Foltz et al . , 1998 .", "Moreover , the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments McDonald , 2000 and word association norms Denhire and Lemaire , 2004 .", "Despite their widespread use , vector based models are typically directed at representing words in isolation and methods for constructing representations for phrases or sentences have received little attention in the literature .", "In fact , the commonest method for combining the vectors is to average them .", "Vector averaging is unfortunately insensitive to word order , and more generally syntactic structure , giving the same representation to any constructions that happen to share the same vocabulary .", "This is illustrated in the example below taken from Landauer et al . 1997 .", "Sentences 1 a and 1 b contain exactly the same set of words but their meaning is entirely different .", "1 a .", "It was not the sales manager who hit the bottle that day , but the office worker with the serious drinking problem . b .", "That day the office manager , who was drinking , hit the problem sales worker with a bottle , but it was not serious .", "While vector addition has been effective in some applications such as essay grading Landauer and Dumais , 1997 and coherence assessment Foltz et al . , 1998 , there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing Neville et al . , 1991 ; West and Stanovich , 1986 and modulate cognitive behavior in sentence priming Till et al . , 1988 and inference tasks Heit and Rubinstein , 1994 .", "Computational models of semantics which use symbolic logic representations Montague , 1974 can account naturally for the meaning of phrases or sentences .", "Central in these models is the notion of compositionality the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them .", "Here , semantic analysis is guided by syntactic structure , and therefore sentences 1 a and 1 b receive distinct representations .", "The downside of this approach is that differences in meaning are qualitative rather than quantitative , and degrees of similarity cannot be expressed easily .", "In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations .", "We present a general framework for vector based composition which allows us to consider different classes of models .", "Specifically , we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment .", "Our results show that the multiplicative models are superior and correlate significantly with behavioral data .", "The problem of vector composition has received some attention in the connectionist literature , particularly in response to criticisms of the ability of connectionist representations to handle complex structures Fodor and Pylyshyn , 1988 .", "While neural networks can readily represent single distinct objects , in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects .", "For the hierarchical structure of natural language this binding problem becomes particularly acute .", "For example , simplistic approaches to handling sentences such as John loves Mary and Mary loves John typically fail to make valid representations in one of two ways .", "Either there is a failure to distinguish between these two structures , because the network fails to keep track of the fact that John is subject in one and object in the other , or there is a failure to recognize that both structures involve the same participants , because John as a subject has a distinct representation from John as an object .", "In contrast , symbolic representations can naturally handle the binding of constituents to their roles , in a systematic manner that avoids both these problems .", "Smolensky 1990 proposed the use of tensor products as a means of binding one vector to another .", "The tensor product u v is a matrix whose components are all the possible products uivj of the components of vectors u and v . A major difficulty with tensor products is their dimensionality which is higher than the dimensionality of the original vectors precisely , the tensor product has dimensionality m x n .", "To overcome this problem , other techniques have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components .", "Holographic reduced representations Plate , 1991 are one implementation of this idea where the tensor product is projected back onto the space of its components .", "The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors .", "The compression is achieved by summing along the transdiagonal elements of the tensor product .", "Noisy versions of the original vectors can be recovered by means of circular correlation which is the approximate inverse of circular convolution .", "The success of circular correlation crucially depends on the components of the n dimensional vectors u and v being randomly distributed with mean 0 and variance 1n .", "This poses problems for modeling linguistic data which is typically represented by vectors with non random structure .", "Vector addition is by far the most common method for representing the meaning of linguistic sequences .", "For example , assuming that individual words are represented by vectors , we can compute the meaning of a sentence by taking their mean Foltz et al . , 1998 ; Landauer and Dumais , 1997 .", "Vector addition does not increase the dimensionality of the resulting vector .", "However , since it is order independent , it cannot capture meaning differences that are modulated by differences in syntactic structure .", "Kintsch 2001 proposes a variation on the vector addition theme in an attempt to model how the meaning of a predicate e . g . , run varies depending on the arguments it operates upon e . g , the horse ran vs . the color ran .", "The idea is to add not only the vectors representing the predicate and its argument but also the neighbors associated with both of them .", "The neighbors , Kintsch argues , can strengthen features of the predicate that are appropriate for the argument of the predication . animal stable village gallop jokey horse 0 6 2 10 4 run 1 8 4 4 0 Unfortunately , comparisons across vector composition models have been few and far between in the literature .", "The merits of different approaches are illustrated with a few hand picked examples and parameter values and large scale evaluations are uniformly absent see Frank et al . 2007 for a criticism of Kintsch s 2001 evaluation standards .", "Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations , multiplication and addition and their combination .", "Under this framework , we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology .", "We formulate semantic composition as a function of two vectors , u and v . We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature . 1 We briefly note here that a word s vector typically represents its co occurrence with neighboring words .", "The construction of the semantic space depends on the definition of linguistic context e . g . , neighbouring words can be documents or collocations , the number of components used e . g . , the k most frequent words in a corpus , and their values e . g . , as raw co occurrence frequencies or ratios of probabilities .", "A hypothetical semantic space is illustrated in Figure 1 .", "Here , the space has only five dimensions , and the matrix cells denote the co occurrence of the target words horse and run with the context words animal , stable , and so on .", "Let p denote the composition of two vectors u and v , representing a pair of constituents which stand in some syntactic relation R . Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition .", "We define a general class of models for this process of composition as The expression above allows us to derive models for which p is constructed in a distinct space from u and v , as is the case for tensor products .", "It also allows us to derive models in which composition makes use of background knowledge K and models in which composition has a dependence , via the argument R , on syntax .", "To derive specific models from this general framework requires the identification of appropriate constraints to narrow the space of functions being considered .", "One particularly useful constraint is to hold R fixed by focusing on a single well defined linguistic structure , for example the verb subject relation .", "Another simplification concerns K which can be ignored so as to explore what can be achieved in the absence of additional knowledge .", "This reduces the class of models to However , this still leaves the particular form of the function f unspecified .", "Now , if we assume that p lies in the same space as u and v , avoiding the issues of dimensionality associated with tensor products , and that f is a linear function , for simplicity , of the cartesian product of u and v , then we generate a class of additive models where A and B are matrices which determine the contributions made by u and v to the product p . In contrast , if we assume that f is a linear function of the tensor product of u and v , then we obtain multiplicative models where C is a tensor of rank 3 , which projects the tensor product of u and v onto the space of p . Further constraints can be introduced to reduce the free parameters in these models .", "So , if we assume that only the ith components of u and v contribute to the ith component of p , that these components are not dependent on i , and that the function is symmetric with regard to the interchange of u and v , we obtain a simpler instantiation of an additive model Analogously , under the same assumptions , we obtain the following simpler multiplicative model only the ith components of u and v contribute to the ith component of p . Another class of models can be derived by relaxing this constraint .", "To give a concrete example , circular convolution is an instance of the general multiplicative model which breaks this constraint by allowing uj to contribute to pi For example , according to 5 , the addition of the two vectors representing horse and run in Figure 1 would yield horse run 1 14 6 14 4 .", "Whereas their product , as given by 6 , is horse run 0 48 8 40 0 .", "Although the composition model in 5 is commonly used in the literature , from a linguistic perspective , the model in 6 is more appealing .", "Simply adding the vectors u and v lumps their contents together rather than allowing the content of one vector to pick out the relevant content of the other .", "Instead , it could be argued that the contribution of the ith component of u should be scaled according to its relevance to v , and vice versa .", "In effect , this is what model 6 achieves .", "As a result of the assumption of symmetry , both these models are bag of words models and word order insensitive .", "Relaxing the assumption of symmetry in the case of the simple additive model produces a model which weighs the contribution of the two components differently This allows additive models to become more syntax aware , since semantically important constituents can participate more actively in the composition .", "As an example if we set \u03b1 to 0 . 4 and \u03b2 to 0 . 6 , then horse 0 2 . 4 0 . 8 4 1 . 6 and run 0 . 6 4 . 8 2 . 4 2 . 4 0 , and their sum horse run 0 . 6 5 . 6 3 . 2 6 . 4 1 . 6 .", "An extreme form of this differential in the contribution of constituents is where one of the vectors , say u , contributes nothing at all to the combination Admittedly the model in 8 is impoverished and rather simplistic , however it can serve as a simple baseline against which to compare more sophisticated models .", "The models considered so far assume that components do not interfere with each other , i . e . , that It is also possible to re introduce the dependence on K into the model of vector composition .", "For additive models , a natural way to achieve this is to include further vectors into the summation .", "These vectors are not arbitrary and ideally they must exhibit some relation to the words of the construction under consideration .", "When modeling predicate argument structures , Kintsch 2001 proposes including one or more distributional neighbors , n , of the predicate Note that considerable latitude is allowed in selecting the appropriate neighbors .", "Kintsch 2001 considers only the m most similar neighbors to the predicate , from which he subsequently selects k , those most similar to its argument .", "So , if in the composition of horse with run , the chosen neighbor is ride , ride 2 15 7 9 1 , then this produces the representation horse run ride 3 29 13 23 5 .", "In contrast to the simple additive model , this extended model is sensitive to syntactic structure , since n is chosen from among the neighbors of the predicate , distinguishing it from the argument .", "Although we have presented multiplicative and additive models separately , there is nothing inherent in our formulation that disallows their combination .", "The proposal is not merely notational .", "One potential drawback of multiplicative models is the effect of components with value zero .", "Since the product of zero with any number is itself zero , the presence of zeroes in either of the vectors leads to information being essentially thrown away .", "Combining the multiplicative model with an additive model , which does not suffer from this problem , could mitigate this problem pi \u03b1ui \u03b2vi \u03b3uivi 11 where \u03b1 , \u03b2 , and \u03b3 are weighting constants .", "We evaluated the models presented in Section 3 on a sentence similarity task initially proposed by Kintsch 2001 .", "In his study , Kintsch builds a model of how a verb s meaning is modified in the context of its subject .", "He argues that the subjects of ran in The color ran and The horse ran select different senses of ran .", "This change in the verb s sense is equated to a shift in its position in semantic space .", "To quantify this shift , Kintsch proposes measuring similarity relative to other verbs acting as landmarks , for example gallop and dissolve .", "The idea here is that an appropriate composition model when applied to horse and ran will yield a vector closer to the landmark gallop than dissolve .", "Conversely , when color is combined with ran , the resulting vector will be closer to dissolve than gallop .", "Focusing on a single compositional structure , namely intransitive verbs and their subjects , is a good point of departure for studying vector combination .", "Any adequate model of composition must be able to represent argument verb meaning .", "Moreover by using a minimal structure we factor out inessential degrees of freedom and are able to assess the merits of different models on an equal footing .", "Unfortunately , Kintsch 2001 demonstrates how his own composition algorithm works intuitively on a few hand selected examples but does not provide a comprehensive test set .", "In order to establish an independent measure of sentence similarity , we assembled a set of experimental materials and elicited similarity ratings from human subjects .", "In the following we describe our data collection procedure and give details on how our composition models were constructed and evaluated .", "Materials and Design Our materials consisted of sentences with an an intransitive verb and its subject .", "We first compiled a list of intransitive verbs from CELEX2 .", "All occurrences of these verbs with a subject noun were next extracted from a RASP parsed Briscoe and Carroll , 2002 version of the British National Corpus BNC .", "Verbs and nouns that were attested less than fifty times in the BNC were removed as they would result in unreliable vectors .", "Each reference subject verb tuple e . g . , horse ran was paired with two landmarks , each a synonym of the verb .", "The landmarks were chosen so as to represent distinct verb senses , one compatible with the reference e . g . , horse galloped and one incompatible e . g . , horse dissolved .", "Landmarks were taken from WordNet Fellbaum , 1998 .", "Specifically , they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath 1997 measure . 3 Our initial set of candidate materials consisted of 20 verbs , each paired with 10 nouns , and 2 landmarks 400 pairs of sentences in total .", "These were further pretested to allow the selection of a subset of items showing clear variations in sense as we wanted to have a balanced set of similar and dissimilar sentences .", "In the pretest , subjects saw a reference sentence containing a subject verb tuple and its landmarks and were asked to choose which landmark was most similar to the reference or neither .", "Our items were converted into simple sentences all in past tense by adding articles where appropriate .", "The stimuli were administered to four separate groups ; each group saw one set of 100 sentences .", "The pretest was completed by 53 participants .", "For each reference verb , the subjects responses were entered into a contingency table , whose rows corresponded to nouns and columns to each possible answer i . e . , one of the two landmarks .", "Each cell recorded the number of times our subjects selected the landmark as compatible with the noun or not .", "We used Fisher s exact test to determine which verbs and nouns showed the greatest variation in landmark preference and items with p values greater than 0 . 001 were discarded .", "This yielded a reduced set of experimental items 120 in total consisting of 15 reference verbs , each with 4 nouns , and 2 landmarks .", "Procedure and Subjects Participants first saw a set of instructions that explained the sentence similarity task and provided several examples .", "Then the experimental items were presented ; each contained two sentences , one with the reference verb and one with its landmark .", "Examples of our items are given in Table 1 .", "Here , burn is a high similarity landmark High for the reference The fire glowed , whereas beam is a low similarity landmark Low .", "The opposite is the case for the reference The face glowed .", "Sentence pairs were presented serially in random order .", "Participants were asked to rate how similar the two sentences were on a scale of one to seven .", "The study was conducted remotely over the Internet using Webexp4 , a software package designed for conducting psycholinguistic studies over the web .", "49 unpaid volunteers completed the experiment , all native speakers of English .", "Analysis of Similarity Ratings The reliability of the collected judgments is important for our evaluation experiments ; we therefore performed several tests to validate the quality of the ratings .", "First , we examined whether participants gave high ratings to high similarity sentence pairs and low ratings to low similarity ones .", "Figure 2 presents a box and whisker plot of the distribution of the ratings .", "As we can see sentences with high similarity landmarks are perceived as more similar to the reference sentence .", "A Wilcoxon rank sum test confirmed that the difference is statistically significant p 0 . 01 .", "We also measured how well humans agree in their ratings .", "We employed leave one out resampling Weiss and Kulikowski , 1991 , by correlating the data obtained from each participant with the ratings obtained from all other participants .", "We used Spearman s \u03c1 , a non parametric correlation coefficient , to avoid making any assumptions about the distribution of the similarity ratings .", "The average inter subject agreement5 was \u03c1 0 . 40 .", "We believe that this level of agreement is satisfactory given that naive subjects are asked to provide judgments on fine grained semantic distinctions see Table 1 .", "More evidence that this is not an easy task comes from Figure 2 where we observe some overlap in the ratings for High and Low similarity items .", "Model Parameters Irrespectively of their form , all composition models discussed here are based on a semantic space for representing the meanings of individual words .", "The semantic space we used in our experiments was built on a lemmatised version of the BNC .", "Following previous work Bullinaria and Levy , 2007 , we optimized its parameters on a word based semantic similarity task .", "The task involves examining the degree of linear relationship between the human judgments for two individual words and vector based similarity values .", "We experimented with a variety of dimensions ranging from 50 to 500 , 000 , vector component definitions e . g . , pointwise mutual information or log likelihood ratio and similarity measures e . g . , cosine or confusion probability .", "We used WordSim353 , a benchmark dataset Finkelstein et al . , 2002 , consisting of relatedness judgments on a scale of 0 to 10 for 353 word pairs .", "We obtained best results with a model using a context window of five words on either side of the target word , the cosine measure , and 2 , 000 vector components .", "The latter were the most common context words excluding a stop list of function words .", "These components were set to the ratio of the probability of the context word given the target word to the probability of the context word overall .", "This configuration gave high correlations with the WordSim353 similarity judgments using the cosine measure .", "In addition , Bullinaria and Levy 2007 found that these parameters perform well on a number of other tasks such as the synonymy task from the Test ofEnglish as a Foreign Language TOEFL .", "Our composition models have no additional parameters beyond the semantic space just described , with three exceptions .", "First , the additive model in 7 weighs differentially the contribution of the two constituents .", "In our case , these are the subject noun and the intransitive verb .", "To this end , we optimized the weights on a small held out set .", "Specifically , we considered eleven models , varying in their weightings , in steps of 10 , from 100 noun through 50 of both verb and noun to 100 verb .", "For the best performing model the weight for the verb was 80 and for the noun 20 .", "Secondly , we optimized the weightings in the combined model 11 with a similar grid search over its three parameters .", "This yielded a weighted sum consisting of 95 verb , 0 noun and 5 of their multiplicative combination .", "Finally , Kintsch s 2001 additive model has two extra parameters .", "The m neighbors most similar to the predicate , and the k of m neighbors closest to its argument .", "In our experiments we selected parameters that Kintsch reports as optimal .", "Specifically , m was set to 20 and m to 1 .", "Evaluation Methodology We evaluated the proposed composition models in two ways .", "First , we used the models to estimate the cosine similarity between the reference sentence and its landmarks .", "We expect better models to yield a pattern of similarity scores like those observed in the human ratings see Figure 2 .", "A more scrupulous evaluation requires directly correlating all the individual participants similarity judgments with those of the models . 6 We used Spearman s p for our correlation analyses .", "Again , better models should correlate better with the experimental data .", "We assume that the inter subject agreement can serve as an upper bound for comparing the fit of our models against the human judgments .", "Our experiments assessed the performance of seven composition models .", "These included three additive models , i . e . , simple addition equation 5 , Add , weighted addition equation 7 , WeightAdd , and Kintsch s 2001 model equation 10 , Kintsch , a multiplicative model equation 6 , Multiply , and also a model which combines multiplication with addition equation 11 , Combined .", "As a baseline we simply estimated the similarity between the reference verb and its landmarks without taking the subject noun into account equation 8 , NonComp .", "Table 2 shows the average model ratings for High and Low similarity items .", "For comparison , we also show the human ratings for these items UpperBound .", "Here , we are interested in relative differences , since the two types of ratings correspond to different scales .", "Model similarities have been estimated using cosine which ranges from 0 to 1 , whereas our subjects rated the sentences on a scale from 1 to 7 .", "The simple additive model fails to distinguish between High and Low Similarity items .", "We observe a similar pattern for the non compositional baseline model , the weighted additive model and Kintsch 2001 .", "The multiplicative and combined models yield means closer to the human ratings .", "The difference between High and Low similarity values estimated by these models are statistically significant p 0 . 01 using the Wilcoxon rank sum test .", "Figure 3 shows the distribution of estimated similarities under the multiplicative model .", "The results of our correlation analysis are also given in Table 2 .", "As can be seen , all models are significantly correlated with the human ratings .", "In order to establish which ones fit our data better , we examined whether the correlation coefficients achieved differ significantly using a t test Cohen and Cohen , 1983 .", "The lowest correlation p 0 . 04 is observed for the simple additive model which is not significantly different from the non compositional baseline model .", "The weighted additive model p 0 . 09 is not significantly different from the baseline either or Kintsch 2001 p 0 . 09 .", "Given that the basis of Kintsch s model is the summation of the verb , a neighbor close to the verb and the noun , it is not surprising that it produces results similar to a summation which weights the verb more heavily than the noun .", "The multiplicative model yields a better fit with the experimental data , \u03c1 0 . 17 .", "The combined model is best overall with \u03c1 0 . 19 .", "However , the difference between the two models is not statistically significant .", "Also note that in contrast to the combined model , the multiplicative model does not have any free parameters and hence does not require optimization for this particular task .", "In this paper we presented a general framework for vector based semantic composition .", "We formulated composition as a function of two vectors and introduced several models based on addition and multiplication .", "Despite the popularity of additive models , our experimental results showed the superiority of models utilizing multiplicative combinations , at least for the sentence similarity task attempted here .", "We conjecture that the additive models are not sensitive to the fine grained meaning distinctions involved in our materials .", "Previous applications of vector addition to document indexing Deerwester et al . , 1990 or essay grading Landauer et al . , 1997 were more concerned with modeling the gist of a document rather than the meaning of its sentences .", "Importantly , additive models capture composition by considering all vector components representing the meaning of the verb and its subject , whereas multiplicative models consider a subset , namely non zero components .", "The resulting vector is sparser but expresses more succinctly the meaning of the predicate argument structure , and thus allows semantic similarity to be modelled more accurately .", "Further research is needed to gain a deeper understanding of vector composition , both in terms of modeling a wider range of structures e . g . , adjectivenoun , noun noun and also in terms of exploring the space of models more fully .", "We anticipate that more substantial correlations can be achieved by implementing more sophisticated models from within the framework outlined here .", "In particular , the general class of multiplicative models see equation 4 appears to be a fruitful area to explore .", "Future directions include constraining the number of free parameters in linguistically plausible ways and scaling to larger datasets .", "The applications of the framework discussed here are many and varied both for cognitive science and NLP .", "We intend to assess the potential of our composition models on context sensitive semantic priming Till et al . , 1988 and inductive inference Heit and Rubinstein , 1994 .", "NLP tasks that could benefit from composition models include paraphrase identification and context dependent language modeling Coccaro and Jurafsky , 1998 ."], "summary_lines": ["Vector-based Models of Semantic Composition\n", "This paper proposes a framework for representing the meaning of phrases and sentences in vector space.\n", "Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions.\n", "Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.\n", "Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.\n", "We propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression.\n"]}
{"article_lines": ["The Third PASCAL Recognizing Textual Entailment Challenge", "We would like to thank the people and organizations that made these sources available for the challenge .", "In addition , we thank Idan Szpektor and Roy Bar Haim from Bar Ilan University for their assistance and advice , and Valentina Bruseghini CELCT for managing the RTE 3 We would also like to acknowledge the people and organizations involved in creating and annotating the data Pamela Forner , Errol Hayman , Cameron Fordyce from CELCT and Courtenay Hendricks , Adam Savel and Annika Hamalainen This work was supported in part by the IST Programme of the European Community , under the Network of IST 2002 506778 .", "We wish to thank the managers of the PASCAL challenges program , Michele Sebag and Florence d Alche Buc , for their efforts and support , which made this challenge possible .", "We also thank David Askey , who helped manage the RTE 3 website .", "The goal of the RTE challenges has been to create a benchmark task dedicated to textual entailment recognizing that the meaning of one text is entailed , i . e . can be inferred , by another1 .", "In the recent years , this task has raised great interest since applied semantic inference concerns many practical Natural Language Processing NLP applications , such as Question Answering QA , Information Extraction IE , Summarization , Machine Translation and Paraphrasing , and certain types of queries in Information Retrieval IR .", "More specifically , the RTE challenges have aimed to focus research and evaluation on this common underlying semantic inference task and separate it from other problems that different NLP applications need to handle .", "For example , in addition to textual entailment , QA systems need to handle issues such as answer retrieval and question type recognition .", "By separating out the general problem of textual entailment from these task specific problems , progress on semantic inference for many application areas can be promoted .", "Hopefully , research on textual entailment will finally lead to the development of entailment engines , which can be used as a standard module in many applications similar to the role of part of speech taggers and syntactic parsers in current NLP applications .", "In the following sections , a detailed description of RTE 3 is presented .", "After a quick review of the previous challenges 1 . 2 , section 2 describes the preparation of the dataset .", "In section 3 the evaluation process and the results are presented , together with an analysis of the performance of the participating systems .", "The first RTE challenge2 aimed to provide the NLP community with a new benchmark to test progress in recognizing textual entailment , and to compare the achievements of different groups .", "This goal proved to be of great interest , and the community's response encouraged the gradual expansion of the scope of the original task .", "The Second RTE challenge3 built on the success of the first , with 23 groups from around the world as compared to 17 for the first challenge submitting the results of their systems .", "Representatives of participating groups presented their work at the PASCAL Challenges Workshop in April 2006 in Venice , Italy .", "The event was successful and the number of participants and their contributions to the discussion demonstrated that Textual Entailment is a quickly growing field of NLP research .", "In addition , the workshops spawned an impressive number of publications in major conferences , with more work in progress .", "Another encouraging sign of the growing interest in the RTE challenge was represented by the increase in the number of downloads of the challenge datasets , with about 150 registered downloads for the RTE 2 development set .", "RTE 3 followed the same basic structure of the previous campaigns , in order to facilitate the participation of newcomers and to allow quot ; veterans quot ; to assess the improvements of their systems in a comparable test exercise .", "Nevertheless , some innovations were introduced , on the one hand to make the challenge more stimulating and , on the other , to encourage collaboration between system developers .", "In particular , a limited number of longer texts , i . e . up to a paragraph in length , were incorporated in order to move toward more comprehensive scenarios , which incorporate the need for discourse analysis .", "However , the majority of examples remained similar to those in the previous challenges , providing pairs with relatively short texts .", "Another innovation was represented by a resource pool4 , where contributors had the possibility to share the resources they used .", "In fact , one of the key conclusions at the second RTE Challenge Workshop was that entailment modeling requires vast knowledge resources that correspond to different types of entailment reasoning .", "Moreover , entailment systems also utilize general NLP tools such as POS taggers , parsers and named entity recognizers , sometimes posing specialized requirements to such tools .", "In response to these demands , the RTE Resource Pool was built , which may serve as a portal and forum for publicizing and tracking resources , and reporting on their use .", "In addition , an optional pilot task , called quot ; Extending the Evaluation of Inferences from Texts quot ; was set up by the US National Institute of Standards and Technology NIST , in order to explore two other sub tasks closely related to textual entailment differentiating unknown entailments from identified contradictions and providing justifications for system decisions .", "In the first sub task , the idea was to drive systems to make more precise informational distinctions , taking a three way decision between quot ; YES quot ; , quot ; NO quot ; and quot ; UNKNOWN , so that a hypothesis being unknown on the basis of a text would be distinguished from a hypothesis being shown false contradicted by a text .", "As for the other subtask , the goal for providing justifications for decisions was to explore how eventual users of tools incorporating entailment can be made to understand how decisions were reached by a system , as users are unlikely to trust a system that gives no explanation for its decisions .", "The pilot task exploited the existing RTE 3 Challenge infrastructure and evaluation process by using the same test set , while utilizing human assessments for the new sub tasks . jobs , but opponents say it is vicious and endangers the species , also threatened by global warming IR The Italian parliament may approve a draft law allow Italian royal fam NO ing descendants of the exiled royal family to return ily returns home . home .", "The family was banished after the Second World War because of the King's collusion with the fascist regime , but moves were introduced this year to allow their return .", "QA Aeschylus is often called the father of Greek tragedy ; quot ; The Persians quot ; YES he wrote the earliest complete plays which survive from was written by ancient Greece .", "He is known to have written more than Aeschylus .", "90 plays , though only seven survive .", "The most famous of these are the trilogy known as Orestia .", "Also wellknown are The Persians and Prometheus Bound .", "SUM A Pentagon committee and the congressionally char Bush will meet NO tered Iraq Study Group have been preparing reports for the presidents of Bush , and Iran has asked the presidents of Iraq and Iraq and Syria in Syria to meet in Tehran .", "Tehran .", "The textual entailment recognition task required the participating systems to decide , given two text snippets t and h , whether t entails h . Textual entailment is defined as a directional relation between two text fragments , called text t , the entailing text , and hypothesis h , the entailed text , so that a human being , with common understanding of language and common background knowledge , can infer that h is most likely true on the basis of the content of t . As in the previous challenges , the RTE 3 dataset consisted of 1600 text hypothesis pairs , equally divided into a development set and a test set .", "While the length of the hypotheses h was the same as in the past datasets , a certain number of texts t were longer than in previous datasets , up to a paragraph .", "The longer texts were marked as L , after being selected automatically when exceeding 270 bytes .", "In the test set they were about 17 of the total .", "As in RTE 2 , four applications namely IE , IR , QA and SUM were considered as settings or contexts for the pairs generation see 2 . 2 for a detailed description .", "200 pairs were selected for each application in each dataset .", "Although the datasets were supposed to be perfectly balanced , the number of negative examples were slightly higher in both development and test sets 51 . 50 and 51 . 25 respectively ; this was unintentional .", "Positive entailment examples , where t entailed h , were annotated YES ; the negative ones , where entailment did not hold , NO .", "Each pair was annotated with its related task IE IR QA SUM and entailment judgment YES NO , obviously released only in the development set .", "Table 1 shows some examples taken from the development set .", "The examples in the dataset were based mostly on outputs both correct and incorrect of Webbased systems .", "In order to avoid copyright problems , input data was limited to either what had already been publicly released by official competitions or else was drawn from freely available sources such as WikiNews and Wikipedia .", "In choosing the pairs , the following judgment criteria and guidelines were considered As entailment is a directional relation , the hypothesis must be entailed by the given text , but the text need not be entailed by the hypothesis .", "The hypothesis must be fully entailed by the text .", "Judgment must be NO if the hypothesis includes parts that cannot be inferred from the text .", "Cases in which inference is very probable but not completely certain were judged as YES .", "Common world knowledge was assumed , e . g . the capital of a country is situated in that country , the prime minister of a state is also a citizen of that state , and so on .", "As in RTE 2 , human annotators generated t h pairs within 4 application settings .", "The IE task was inspired by the Information Extraction and Relation Extraction application , where texts and structured templates were replaced by t h pairs .", "As in the 2006 campaign , the pairs were generated using four different approaches The common aim of all these processes was to simulate the need of IE systems to recognize that the given text indeed entails the semantic relation that is expected to hold between the candidate template slot fillers .", "In the IR Information Retrieval application setting , the hypotheses were propositional IR queries , which specify some statement , e . g .", "robots are used to find avalanche victims .", "The hypotheses were adapted and simplified from standard IR evaluation datasets TREC and CLEF .", "Texts t that did or did not entail the hypotheses were selected from documents retrieved by different search engines e . g .", "Google , Yahoo and MSN for each hypothesis .", "In this application setting it was assumed that relevant documents from an IR perspective should entail the given propositional hypothesis .", "For the QA Question Answering task , annotators used questions taken from the datasets of official QA competitions , such as TREC QA and QA CLEF datasets , and the corresponding answers extracted from the Web by actual QA systems .", "Then they transformed the question answer pairs into t h pairs as follows An answer term of the expected answer type was picked from the answer passage either a correct or an incorrect one .", "The question was turned into an affirmative sentence plugging in the answer term .", "t h pairs were generate , using the affirmative sentences as hypotheses h s and the original answer passages as texts t s .", "For example , given the question How high is Mount Everest ? and a text t The above mentioned expedition team comprising of 10 members was permitted to climb 8848m . high Mt .", "Everest from Normal Route for the period of 75 days from 15 April , 2007 under the leadership of Mr . Wolf Herbert of Austria , the annotator , extracting the piece of information 8848m . from the text , would turn the question into an the affirmative sentence Mount Everest is 8848m high , generating a positive entailment pair .", "This process simulated the need of a QA system to verify that the retrieved passage text actually entailed the provided answer .", "In the SUM Summarization setting , the entailment pairs were generated using two procedures .", "In the first one , t s and h s were sentences taken from a news document cluster , a collection of news articles that describe the same news item .", "Annotators were given the output of multi document summarization systems including the document clusters and the summary generated for each cluster .", "Then they picked sentence pairs with high lexical overlap , preferably where at least one of the sentences was taken from the summary this sentence usually played the role of t .", "For positive examples , the hypothesis was simplified by removing sentence parts , until it was fully entailed by t . Negative examples were simplified in a similar manner .", "In alternative , pyramids produced for the experimental evaluation mehod in DUC 2005 Passonneau et al . 2005 were exploited .", "In this new evaluation method , humans select subsentential content units SCUs in several manually produced summaries on a subject , and collocate them in a pyramid , which has at the top the SCUs with the higher frequency , i . e . those which are present in most summaries .", "Each SCU is identified by a label , a sentence in natural language which expresses the content .", "Afterwards , the annotators individuate the SCUs present in summaries generated automatically called peers , and link them to the ones present in the pyramid , in order to assign each peer a weight .", "In this way , the SCUs in the automatic summaries linked to the SCUs in the higher tiers of the pyramid are assigned a heavier weight than those at the bottom .", "For the SUM setting , the RTE 3 annotators selected relevant passages from the peers and used them as T s , meanwhile the labels of the corresponding SCUs were used as H s .", "Small adjustments were allowed , whenever the texts were not grammatically acceptable .", "This process simulated the need of a summarization system to identify information redundancy , which should be avoided in the summary .", "Each pair of the dataset was judged by three annotators .", "As in previous challenges , pairs on which the annotators disagreed were filtered out .", "On the test set , the average agreement between each pair of annotators who shared at least 100 examples was 87 . 8 , with an average Kappa level of 0 . 75 , regarded as substantial agreement according to Landis and Koch 1997 .", "19 . 2 of the pairs in the dataset were removed from the test set due to disagreement .", "The disagreement was generally due to the fact that the h was more specific than the t , for example because it contained more information , or made an absolute assertion where t proposed only a personal opinion .", "In addition , 9 . 4 of the remaining pairs were discarded , as they seemed controversial , too difficult , or too similar when compared to other pairs .", "As far as the texts extracted from the web are concerned , spelling and punctuation errors were sometimes fixed by the annotators , but no major change was allowed , so that the language could be grammatically and stylistically imperfect .", "The hypotheses were finally double checked by a native English speaker .", "The evaluation of all runs submitted in RTE 3 was automatic .", "The judgments classifications returned by the system were compared to the Gold Standard compiled by the human assessors .", "The main evaluation measure was accuracy , i . e . the percentage of matching judgments .", "For systems that provided a confidence ranked list of the pairs , in addition to the YES NO judgment , an Average Precision measure was also computed .", "This measure evaluates the ability of systems to rank all the T H pairs in the test set according to their entailment confidence in decreasing order from the most certain entailment to the least certain .", "Average precision is computed as the average of the system's precision values at all points in the ranked list in which recall increases , that is at all points in the ranked list for which the gold standard annotation is YES , or , more formally where n is the number of the pairs in the test set , R is the total number of positive pairs in the test set , E i is 1 if the i th pair is positive and 0 otherwise , and i ranges over the pairs , ordered by their ranking .", "In other words , the more the system was confident that t entails h , the higher was the ranking of the pair .", "A perfect ranking would have placed all the positive pairs for which the entailment holds before all the negative ones , yielding an average precision value of 1 .", "Twenty six teams participated in the third challenge , three more than in previous year .", "Table 2 presents the list of the results of each submitted runs and the components used by the systems .", "Overall , we noticed a move toward deep approaches , with a general consolidation of approaches based on the syntactic structure of Text and Hypothesis .", "There is an evident increase of systems using some form of logical inferences at least seven systems .", "However , these approaches , with few notably exceptions , do not seem to be consolidated enough , as several systems show results not still at the state of art e . g .", "Natural Logic introduced by Chambers et al . .", "For many systems an open issue is the availability and integration of different and complex semantic resourcesA more extensive and fine grained use of specific semantic phenomena is also emerging .", "As an example , Tatu and Moldovan carry on a sophisticated analysis of named entities , in particular Person names , distinguishing first names from last names .", "Some form of relation extraction , either through manually built patterns Chambers et al . or through the use of an information extraction system Hickl and Bensley have been introduced this year , even if still on a small scale i . e . few relations .", "On the other hand , RTE 3 confirmed that both machine learning using lexical syntactic features and transformation based approaches on dependency representations are well consolidated techniques to address textual entailment .", "The extension of transformation based approaches toward probabilistic settings is an interesting direction investigated by some systems e . g .", "Harmeling .", "On the side of light approaches to textual entailment , Malakasiotis and Androutpoulos provide a useful baseline for the task 0 . 61 using only POS tagging and then applying string based measures to estimate the similarity between Text and Hypothesis .", "As far as resources are concerned , lexical databases mostly WordNet and DIRT are still widely used .", "Extended WordNet is also a common resource for instance in Iftene and BalahurDobrescu and the Extended Wordnet Knowledge Base has been successfully used in Tatu and Moldovan .", "Verb oriented resources are also largely present in several systems , including Framenet e . g .", "Burchardt et al . , Verbnet Bobrow et al . and Propbank e . g .", "Adams et al . .", "It seems that the use of the Web as a resource is more limited when compared to the previous RTE workshop .", "However , as in RTE 2 , the use of large semantic resources is still a crucial factor affecting the performance of systems see , for instance , the use of a large corpus of entailment examples in Hickl and Bensley .", "Finally , an interesting aspect is that , stimulated by the percentage of longer texts included this year , a number of participating systems addressed anaphora resolution e . g .", "Delmonte , Bar Haim et al . , Iftene and Balahur Dobrescu .", "The accuracy achieved by the participating systems ranges from 49 to 80 considering the best run of each group , while most of the systems obtained a score in between 59 and 66 .", "One submission , Hickl and Bensley achieved 80 accuracy , scoring 8 higher than the second system Tatu and Moldovan , 72 , and obtaining the best absolute result achieved in the three RTE challenges .", "As far as the per task results are concerned , the trend registered in RTE 2 was confirmed , in that there was a marked difference in the performances obtained in different task settings .", "In fact , the average accuracy achieved in the QA setting 0 . 71 was 20 points higher than that achieved in the IE setting 0 . 52 ; the average accuracy in the IR and Sum settings was 0 . 66 and 0 . 58 respectively .", "In RTE 2 the best results were achieved in SUM , while the lower score was always recorded in IE .", "As already pointed out by Bar Haim 2006 , these differences should be further investigated , as they could lead to a sensible improvement of the performance .", "As for the LONG pairs , which represented a new element of this year s challenge , no substantial difference was noted in the systems performances the average accuracy over the long pairs was 58 . 72 , compared to 61 . 93 over the short ones .", "At its third round , the Recognizing Textual Entailment task has reached a noticeable level of maturity , as the very high interest in the NLP community and the continuously increasing number of participants in the challenges demonstrate .", "The relevance of Textual Entailment Recognition to different applications , such as the AVE5 track at QA at CLEF6 , has also been acknowledged .", "Furthermore , the debates and the numerous publications about the Textual Entailment have contributed to the better understanding the task and its nature .", "To keep a good balance between the consolidated main task and the need for moving forward , longer texts were introduced in the dataset , in order to make the task more challenging , and a pilot task was proposed .", "The Third RTE Challenge have also confirmed that the methodology for the creation of the datasets , developed in the first two campaigns , is robust .", "Overall , the transition of the challenge coordination from Bar Ilan which organized the first two challenges to CELCT was successful , though some problems were encountered , especially in the preparation of the data set .", "The systems which took part in RTE 3 showed that the technology applied to Entailment Recognition has made significant progress , confirmed by the results , which were generally better than last year .", "In particular , visible progress in defining several new principled scenarios for RTE was represented , such as Hickl s commitment based approach , Bar Haim s proof system , Harmeling s probabilistic model , and Standford s use of Natural Logic .", "If , on the one hand , the success that RTE has had so far is very encouraging , on the other , it incites to overcome certain current limitations , and to set realistic and , at the same time , stimulating goals for the future .", "First at all , theoretical refinements both of the task and the models applied to it need to be developed .", "In particular , more efforts are required to improve knowledge acquisition , as little progress has been made on this front so far .", "Also the data set generation and the evaluation methodology need to be refined and extended .", "A major problem in the current setting of the data collection is that the distribution of the examples is arbitrary to a large extent , being determined by manual selection .", "Therefore new evaluation methodologies , which can reflect realistic distributions should be investigated , as well as the possibility of evaluating Textual Entailment Recognition within additional concrete application scenarios , following the spirit of the QA Answer Validation Exercise .", "The following sources were used in the preparation of the data http www1 . cs . columbia . edu ani DUC2005 We would like to thank the people and organizations that made these sources available for the challenge .", "In addition , we thank Idan Szpektor and Roy Bar Haim from Bar Ilan University for their assistance and advice , and Valentina Bruseghini from CELCT for managing the RTE 3 website .", "We would also like to acknowledge the people and organizations involved in creating and annotating the data Pamela Forner , Errol Hayman , Cameron Fordyce from CELCT and Courtenay Hendricks , Adam Savel and Annika Hamalainen This work was supported in part by the IST Programme of the European Community , under the PASCAL Network of Excellence , IST 2002506778 .", "We wish to thank the managers of the PASCAL challenges program , Michele Sebag and Florence d Alche Buc , for their efforts and support , which made this challenge possible .", "We also thank David Askey , who helped manage the RTE 3 website ."], "summary_lines": ["The Third PASCAL Recognizing Textual Entailment Challenge\n", "This paper presents the Third PASCAL Recognising Textual Entailment Challenge (RTE-3), providing an overview of the dataset creating methodology and the submitted systems.\n", "In creating this year\u2019s dataset, a number of longer texts were introduced to make the challenge more oriented to realistic scenarios.\n", "Additionally, a pool of resources was offered so that the participants could share common tools.\n", "A pilot task was also set up, aimed at differentiating unknown entailments from identified contradictions and providing justifications for overall system decisions.\n", "26 participants submitted 44 runs, using different approaches and generally presenting new entailment models and achieving higher scores than in the previous challenges.\n", "The task of recognizing textual entailment is to decide whether the hypothesis sentence can be entailed by the premise sentence (Giampiccolo et al,2007).\n", "Textual Entailment (TE) has become a prominent paradigm for modeling semantic inference, capturing the needs of a broad range of text understanding applications.\n"]}
{"article_lines": ["A Fast And Portable Realizer For Text Generation Systems", "Systems that generate natural language output as part of their interaction with a user have become a major area of research and development .", "Typically , natural language generation is divided into several phases , namely text planning determining output content and structure , sentence planning determining abstract target language resources to express content , such as lexical items and syntactic constructions , and realization producing the final text string Reiter , 1994 .", "While text and sentence planning may sometimes be combined , a realizer is almost always included as a distinct module .", "It is in the realizer that knowledge about the target language resides syntax , morphology , idiosyncratic properties of lexical items .", "Realization is fairly well understood both from a linguistic and from a computational point of view , and therefore most projects that use text generation do not include the realizer in the scope of their research .", "Instead , such projects use an off the shelf realizer , among which PENMAN Bateman , 1996 and SURGE FUF Elhadad and Robin , 1996 are probably the most popular .", "In this technical note and demo we present a new off theshelf realizer , REALPRO .", "REALPRO is derived from previous systems Iordanskaja et al . , 1988 ; Iordanslcaja et al . , 1992 ; Rambow and Korelsky , 1992 , but represents a new design and a completely new implementation .", "REALPRO has the following characteristics , which we believe are unique in this combination We reserve a more detailed comparison with PENMAN and FUF , as well as with AlethGen GL Coch , 1996 which is perhaps the system most similar to REALPRO , since they are based on the same linguistic theory and are both implemented with speed in mind , for a more extensive paper .", "This technical note presents REALPRO , concentrating on its structure , its coverage , its interfaces , and its performance .", "The input to REALPRO is a syntactic dependency structure .", "It is called the Deep Syntactic Structure or quot ; DSyntS quot ; for short , and is inspired in this form by I . Mel'euk's Meaning Text Theory Mel'euk , 1988 .", "This representation has the following salient features Lexemes which are in the lexicon are in uppercase , those that are not are in lowercase .", "For lexemes not in the lexicon it is necessary to specify the word class This illustrates that function words do need not be included in the input DSyntS , and that syntactic issues such as subject verb and noun determiner agreement are handled automatically .", "The tree in Figure 2 yields 3 Note that REALPRO does not perform the task of lexical choice the input to REALPRO must specify all meaning bearing lexemes , including features for free pronominalization .", "Also , REALPRO does not map any sort of semantic labels to syntactic categories .", "These tasks , we assume , are handled by a separate component such as a sentence planner .", "This has the advantage that the sentence planner can be unabashedly domain specific , which is necessary in today's applications , since a broad coverage implementation of a domain independent theory of conceptual representations and their mapping to linguistic representations is still far from being realistic .", "Furthermore , there is no non determinism in REALPRO the input to REALPRO fully determines the output , though the input is a very abstract linguistic representation which is well suited for interfacing with knowledge based applications .", "This means that REALPRO gives the developer control over the output , while taking care of the linguistic details .", "The architecture of REALPRO is based on MeaningText Theory , which posits a sequence of correspondences between different levels of representation .", "In REALPRO , each transformation is handled by a separate module .", "REALPRO is really a realizer shell , which allows for a run time configuration using specially formatted Linguistic Knowledge Bases LKBs which state grammar rules , lexical entries , and feature defaults .", "Each module draws on one or several LKBs .", "The lexicon is an LKB which is used by all components .", "Figure 3 shows the architecture .", "As mentioned in Section 3 , REALPRO is configured by specifying several LKBs .", "The system comes with LKBs for English ; French is currently under development .", "Normally , the user need not change the two grammar LKBs the DSynt and SSynt grammars , unless the grammar of the target sublanguage is not a subset of English or French .", "However , the user may want to extend the lexicon if a lexeme with irregular morphology is not in it yet .", "Recall that not all words in the input representation need be in the lexicon .", "For example , in order to generate saw rather than the default seed for the past tense of to see , the following entry would be added to the lexicon .", "The user may also want to change the defaults .", "For example if in his her application all sentences must be in past tense , the user can set the default tense to be past rather than present as follows The English grammar currently covers a wide range of syntactic phenomena Most of these points are illustrated by the input in Figure 2 .", "Phenomena currently not handled automatically include certain types of quot ; fancy syntax quot ; such as clefts and it clefts though these can be generated by specifying the surface structure in the input , as well as long distance dependencies such as These are books which I think you should buy where which is an argument of buy .", "REALPRO is currently distributed with a socket interface which allows it to be run as a standalone server .", "It has an application programming interface API , available in C and Java , which can be used to integrate REALPRO in applications .", "For training , debugging , and demonstration purposes , REALPRO can also be used in interactive mode to realize sentences from ASCII files containing syntactic specifications .", "The following ASCII based specification corresponds to the DSyntS of sentence 2 In this definition , parentheses are used to specify the scope of dependency while square brackets are used to specify features associated with a lexeme .", "REALPRO can output text formatted as ASCII , HTML , or RTF .", "In addition , REALPRO can also output an ASCII representation of the DGraphS that a user application can format in application specific ways .", "The following table shows the runtime for sentences of different lengths .", "These sentences are all of the form This small girl often claims that that boy often claims that Mary likes red wine , where the middle clause that that boy often claims is iterated for the longer sentences .", "The row labeled quot ; Length quot ; refers to the length of the output string in words .", "Note that the number of output words is equal to the number of nodes in the SSyntS because it is a dependency tree , and furthermore the number of nodes in the SSyntS is greater than or equal to the number of nodes in the DSyntS .", "In our case , the number of nodes in the input DSyntS is equal to the number of words in the output string .", "The row labeled quot ; Sec quot ; represents average execution time over several test runs for the sentence of the given input length , in seconds , on a PC with a 150MHz Pentium processor and 32 Megs of RAM .", "Length 5 10 15 20 30 40 50 Sec . 11 . 17 . 20 . 28 . 44 . 58 . 72 We also tested the system on the syntactically rather varied and complex input of Figure 2 which is made up of 20 words .", "The average runtime for this input is 0 . 31 seconds , which is comparable to the runtime reported above for the 20 word sentence .", "We conclude that the uniformity of the syntactic constructions found in the sentences used in the above test sequence does not influence the results .", "The complexity of the generation algorithm derives primarily from the tree traversals which must be performed twice , when passing from DSyntS to SSyntS , and from SSyntS to the DMorphS .", "Let n be the length of the output string and hence an upper bound on the size of both DSyntS and SSyntS .", "At each node , each rule in the appropriate grammar deep or surface syntactic must be checked against the subtree rooted at that node .", "This tree matching is in the general case exponential in n . However , in fact it is dependent on two variables , the maximal size of grammar rules in the grammar or n , whichever is greater , and the branching factor maximum number of daughter nodes for a node of the input representation .", "Presumably because of deeper facts about language , the grammar rules are quite small .", "The current grammar does not have any rules with more than three nodes .", "This reduces the tree matching algorithm to polynomial in n . Furthermore , while the branching factor of the input tree can in theory be n 1 , in practice it will be much smaller .", "For example , all the input trees used in the tests discussed above have branching factors of no more than 5 .", "We thus obtain de facto linear performance , which is reflected in the numbers given above .", "The system is fully operational , runs on PC as well as on UNIX work stations , and is currently used in an application we have developed Lavoie et al . , 1997 as well as in several on going projects weather report generation , machine translation , project report generation .", "REALPRO is licensed free of charge to qualified academic institutions , and is licensed for a fee to commercial sites .", "The development of REALPRO was partially supported by USAF Rome Laboratory under contracts F3060293 C 0015 , F30602 94 C 0124 , and F30602 92 C 0163 , and by DARPA under contracts F30602 95 2 0005 and F30602 96 C 0220 .", "We are grateful to R . Kittredge , T . Korelsky , D . McCullough , A . Nasr , E . Reiter , and M . White as well as to three anonymous reviewers for helpful comments about earlier drafts of this technical note and or about REALPRO ."], "summary_lines": ["A Fast And Portable Realizer For Text Generation Systems\n", "We release a surface realizer, RealPro, and it is  intended as off-the-shelf plug-in realizer.\n", "Our RealPro surface realizer which produces a surface linguistic utterance.\n"]}
{"article_lines": ["Mildly Non Projective Dependency Structures", "Syntactic parsing requires a fine balance between expressivity and complexity , so that naturally occurring structures can be accurately parsed without compromising efficiency .", "In dependency based parsing , several constraints have been proposed that restrict the class of permissible structures , such as projectivity , planarity , multi planarity , well nestedness , gap degree , and edge degree .", "While projectivity is generally taken to be too restrictive for natural language syntax , it is not clear which of the other proposals strikes the best balance between expressivity and complexity .", "In this paper , we review and compare the different constraints theoretically , and provide an experimental evaluation using data from two treebanks , investigating how large a proportion of the structures found in the treebanks are permitted under different constraints .", "The results indicate that a combination of the well nestedness constraint and a parametric constraint on discontinuity gives a very good fit with the linguistic data .", "Dependency based representations have become increasingly popular in syntactic parsing , especially for languages that exhibit free or flexible word order , such as Czech Collins et al . , 1999 , Bulgarian Marinov and Nivre , 2005 , and Turkish Eryi git and Oflazer , 2006 .", "Many practical implementations of dependency parsing are restricted to projective structures , where the projection of a head word has to form a continuous substring of the sentence .", "While this constraint guarantees good parsing complexity , it is well known that certain syntactic constructions can only be adequately represented by non projective dependency structures , where the projection of a head can be discontinuous .", "This is especially relevant for languages with free or flexible word order .", "However , recent results in non projective dependency parsing , especially using data driven methods , indicate that most non projective structures required for the analysis of natural language are very nearly projective , differing only minimally from the best projective approximation Nivre and Nilsson , 2005 ; Hall and Nov\u00e1k , 2005 ; McDonald and Pereira , 2006 .", "This raises the question of whether it is possible to characterize a class of mildly non projective dependency structures that is rich enough to account for naturally occurring syntactic constructions , yet restricted enough to enable efficient parsing .", "In this paper , we review a number of proposals for classes of dependency structures that lie between strictly projective and completely unrestricted non projective structures .", "These classes have in common that they can be characterized in terms of properties of the dependency structures themselves , rather than in terms of grammar formalisms that generate the structures .", "We compare the proposals from a theoretical point of view , and evaluate a subset of them empirically by testing their representational adequacy with respect to two dependency treebanks the Prague Dependency Treebank PDT Haji\u02c7c et al . , 2001 , and the Danish Dependency Treebank DDT Kromann , 2003 .", "The rest of the paper is structured as follows .", "In section 2 , we provide a formal definition of dependency structures as a special kind of directed graphs , and characterize the notion of projectivity .", "In section 3 , we define and compare five different constraints on mildly non projective dependency structures that can be found in the literature planarity , multiplanarity , well nestedness , gap degree , and edge degree .", "In section 4 , we provide an experimental evaluation of the notions of planarity , well nestedness , gap degree , and edge degree , by investigating how large a proportion of the dependency structures found in PDT and DDT are allowed under the different constraints .", "In section 5 , we present our conclusions and suggestions for further research .", "For the purposes of this paper , a dependency graph is a directed graph on the set of indices corresponding to the tokens of a sentence .", "We write n to refer to the set of positive integers up to and including n . Throughout this paper , we use standard terminology and notation from graph theory to talk about dependency graphs .", "In particular , we refer to the elements of the set V as nodes , and to the elements of the set E as edges .", "We write i j to mean that there is an edge from the node i to the node j i . e . , i , j E E , and i j to mean that the node i dominates the node j , i . e . , that there is a possibly empty path from i to j .", "For a given node i , the set of nodes dominated by i is the yield of i .", "We use the notation 3r i to refer to the projection of i the yield of i , arranged in ascending order .", "Most of the literature on dependency grammar and dependency parsing does not allow arbitrary dependency graphs , but imposes certain structural constraints on them .", "In this paper , we restrict ourselves to dependency graphs that form forests .", "Definition 2 A dependency forest is a dependency graph with two additional properties Figure 1 shows a dependency forest taken from PDT .", "It has two roots node 2 corresponding to the complementizer proto and node 8 corresponding to the final punctuation mark .", "Some authors extend dependency forests by a special root node with position 0 , and add an edge 0 , i for every root node i of the remaining graph McDonald et al . , 2005 .", "This ensures that the extended graph always is a tree .", "Although such a definition can be useful , we do not follow it here , since it obscures the distinction between projectivity and planarity to be discussed in section 3 .", "In contrast to acyclicity and the indegree constraint , both of which impose restrictions on the dependency relation as such , the projectivity constraint concerns the interaction between the dependency relation and the positions of the nodes in the sentence it says that the nodes in a subtree of a dependency graph must form an interval , where an interval with endpoints i and j is the set i , j kEV I i k and k j .", "Definition 3 A dependency graph is projective , if the yields of its nodes are intervals .", "Since projectivity requires each node to dominate a continuous substring of the sentence , it corresponds to a ban on discontinuous constituents in phrase structure representations .", "Projectivity is an interesting constraint on dependency structures both from a theoretical and a practical perspective .", "Dependency grammars that only allow projective structures are closely related to context free grammars Gaifman , 1965 ; Obre bski and Grali nski , 2004 ; among other things , they have the same weak expressivity .", "The projectivity constraint also leads to favourable parsing complexities chart based parsing of projective dependency grammars can be done in cubic time Eisner , 1996 ; hard wiring projectivity into a deterministic dependency parser leads to linear time parsing in the worst case Nivre , 2003 .", "While the restriction to projective analyses has a number of advantages , there is clear evidence that it cannot be maintained for real world data Zeman , 2004 ; Nivre , 2006 .", "For example , the graph in Figure 1 is non projective the yield of the node 1 marked by the dashed rectangles does not form an interval the node 2 is missing .", "In this section , we present several proposals for structural constraints that relax projectivity , and relate them to each other .", "The notion of planarity appears in work on Link Grammar Sleator and Temperley , 1993 , where it is traced back to Mel \u02c7cuk 1988 .", "Informally , a dependency graph is planar , if its edges can be drawn above the sentence without crossing .", "We emphasize the word above , because planarity as it is understood here does not coincide with the standard graph theoretic concept of the same name , where one would be allowed to also use the area below the sentence to disentangle the edges .", "Figure 2a shows a dependency graph that is planar but not projective while there are no crossing edges , the yield of the node 1 the set 11 , 3 does not form an interval .", "Using the notation linked i , j as an abbreviation for the statement there is an edge from i to j , or vice versa , we formalize planarity as follows Definition 4 A dependency graph is planar , if it does not contain nodes a , b , c , d such that linked a , c A linked b , d A a b c d .", "Yli Jyr\u00e4 2003 proposes multiplanarity as a generalization of planarity suitable for modelling dependency analyses , and evaluates it experimentally using data from DDT .", "Definition 5 A dependency graph G V ; E is m planar , if it can be split into m planar graphs such that E E1U UEm .", "The planar graphs Gi are called planes .", "As an example of a dependency forest that is 2planar but not planar , consider the graph depicted in Figure 2b .", "In this graph , the edges 1 , 4 and 3 , 5 are crossing .", "Moving either edge to a separate graph partitions the original graph into two planes .", "Bodirsky et al . 2005 present two structural constraints on dependency graphs that characterize analyses corresponding to derivations in Tree Adjoining Grammar the gap degree restriction and the well nestedness constraint .", "A gap is a discontinuity in the projection of a node in a dependency graph Pl\u00e1tek et al . , 2001 .", "More precisely , let 7ri be the projection of the node i .", "Then a gap is a pair jk , jk 1 of nodes adjacent in 7ri such that Definition 6 The gap degree of a node i in a dependency graph , gd i , is the number of gaps in 7ri .", "As an example , consider the node labelled i in the dependency graphs in Figure 3 .", "In Graph 3a , the projection of i is an interval 2 , 3 , 4 , so i has gap degree 0 .", "In Graph 3b , 7ri 2 , 3 , 6 contains a single gap 3 , 6 , so the gap degree of i is 1 .", "In the rightmost graph , the gap degree of i is 2 , since 7ri 2 , 4 , 6 contains two gaps 2 , 4 and 4 , 6 .", "Definition 7 The gap degree of a dependency graph G , gd G , is the maximum among the gap degrees of its nodes .", "Thus , the gap degree of the graphs in Figure 3 is 0 , 1 and 2 , respectively , since the node i has the maximum gap degree in all three cases .", "The well nestedness constraint restricts the positioning of disjoint subtrees in a dependency forest .", "Two subtrees are called disjoint , if neither of their roots dominates the other .", "Definition 8 Two subtrees T1 , T2 interleave , if there are nodes l1 , r1 E T1 and l2 , r2 E T2 such that l1 l2 r1 r2 .", "A dependency graph is well nested , if no two of its disjoint subtrees interleave .", "Both Graph 3a and Graph 3b are well nested .", "Graph 3c is not well nested .", "To see this , let T1 be the subtree rooted at the node labelled i , and let T2 be the subtree rooted at j .", "These subtrees interleave , as T1 contains the nodes 2 and 4 , and T2 contains the nodes 3 and 5 .", "The notion of edge degree was introduced by Nivre 2006 in order to allow mildly non projective structures while maintaining good parsing efficiency in data driven dependency parsing . 2 Define the span of an edge i , j as the interval S i , j W min i , j , max i , j .", "Definition 9 Let G V I E be a dependency forest , let e i , j be an edge in E , and let Ge be the subgraph of G that is induced by the nodes contained in the span of e . The degree of an edge e 2 E , ed e , is the number of connected components c in Ge such that the root of c is not dominated by the head of e . The edge degree of G , ed G , is the maximum among the degrees of the edges in G . To illustrate the notion of edge degree , we return to Figure 3 .", "Graph 3a has edge degree 0 the only edge that spans more nodes than its head and its dependent is 1 , 5 , but the root of the connected component f2 , 3 , 4g is dominated by 1 .", "Both Graph 3b and 3c have edge degree 1 the edge 3 , 6 in Graph 3b and the edges 2 , 4 , 3 , 5 and 4 , 6 in Graph 3c each span a single connected component that is not dominated by the respective head .", "Apart from proposals for structural constraints relaxing projectivity , there are dependency frameworks that in principle allow unrestricted graphs , but provide mechanisms to control the actually permitted forms of non projectivity in the grammar .", "The non projective dependency grammar of Kahane et al . 1998 is based on an operation on dependency trees called lifting a lift of a tree T is the new tree that is obtained when one replaces one 2We use the term edge degree instead of the original simple term degree from Nivre 2006 to mark the distinction from the notion of gap degree . or more edges i , k in T by edges j , k , where j !", "i .", "The exact conditions under which a certain lifting may take place are specified in the rules of the grammar .", "A dependency tree is acceptable , if it can be lifted to form a projective graph . 3 A similar design is pursued in Topological Dependency Grammar Duchier and Debusmann , 2001 , where a dependency analysis consists of two , mutually constraining graphs the ID graph represents information about immediate dominance , the LP graph models the topological structure of a sentence .", "As a principle of the grammar , the LP graph is required to be a lift of the ID graph ; this lifting can be constrained in the lexicon .", "The structural conditions we have presented here naturally fall into two groups multiplanarity , gap degree and edge degree are parametric constraints with an infinite scale of possible values ; planarity and well nestedness come as binary constraints .", "We discuss these two groups in turn .", "Parametric constraints With respect to the graded constraints , we find that multiplanarity is different from both gap degree and edge degree in that it involves a notion of optimization since every dependency graph is m planar for some sufficiently large m put each edge onto a separate plane , the interesting question in the context of multiplanarity is about the minimal values for m that occur in real world data .", "But then , one not only needs to show that a dependency graph can be decomposed into m planar graphs , but also that this decomposition is the one with the smallest number of planes among all possible decompositions .", "Up to now , no tractable algorithm to find the minimal decomposition has been given , so itis not clear how to evaluate the significance of the concept as such .", "The evaluation presented by Yli Jyr\u00e4 2003 makes use of additional constraints that are sufficient to make the decomposition unique .", "The fundamental difference between gap degree and edge degree is that the gap degree measures the number of discontinuities within a subtree , while the edge degree measures the number of intervening constituents spanned by a single edge .", "This difference is illustrated by the graphs displayed in Figure 4 .", "Graph 4a has gap degree 2 but edge degree 1 the subtree rooted at node 2 marked by the solid edges has two gaps , but each of its edges only spans one connected component not dominated by 2 marked by the squares .", "In contrast , Graph 4b has gap degree 1 but edge degree 2 the subtree rooted at node 2 has one gap , but this gap contains two components not dominated by 2 .", "Nivre 2006 shows experimentally that limiting the permissible edge degree to 1 or 2 can reduce the average parsing time for a deterministic algorithm from quadratic to linear , while omitting less than 1 of the structures found in DDT and PDT .", "It can be expected that constraints on the gap degree would have very similar effects .", "Binary constraints For the two binary constraints , we find that well nestedness subsumes planarity a graph that contains interleaving subtrees cannot be drawn without crossing edges , so every planar graph must also be well nested .", "To see that the converse does not hold , consider Graph 3b , which is well nested , but not planar .", "Since both planarity and well nestedness are proper extensions of projectivity , we get the following hierarchy for sets of dependency graphs projective C planar C well nested C unrestricted The planarity constraint appears like a very natural one at first sight , as it expresses the intuition that crossing edges are bad , but still allows a limited form of non projectivity .", "However , many authors use planarity in conjunction with a special representation of the root node either as an artificial node at the sentence boundary , as we mentioned in section 2 , or as the target of an infinitely long perpendicular edge coming from the outside , as in earlier versions of Word Grammar Hudson , 2003 .", "In these situations , planarity reduces to projectivity , so nothing is gained .", "Even in cases where planarity is used without a special representation of the root node , it remains a peculiar concept .", "When we compare it with the notion of gaps , for example , we find that , in a planar dependency tree , every gap . i ; j must contain the root node r , in the sense that i r j if the gap would only contain non root nodes k , then the two paths from r to k and from i to j would cross .", "This particular property does not seem to be mirrored in any linguistic prediction .", "In contrast to planarity , well nestedness is independent from both gap degree and edge degree in the sense that for every d 0 , there are both wellnested and non well nested dependency graphs with gap degree or edge degree d . All projective dependency graphs d 0 are trivially well nested .", "Well nestedness also brings computational benefits .", "In particular , chart based parsers for grammar formalisms in which derivations obey the well nestedness constraint such as Tree Adjoining Grammar are not hampered by the crossing configurations to which Satta 1992 attributes the fact that the universal recognition problem of Linear Context Free Rewriting Systems is X30 complete .", "In this section , we present an experimental evaluation of planarity , well nestedness , gap degree , and edge degree , by examining how large a proportion of the structures found in two dependency treebanks are allowed under different constraints .", "Assuming that the treebank structures are sampled from naturally occurring structures in natural language , this provides an indirect evaluation of the linguistic adequacy of the different proposals .", "The experiments are based on data from the Prague Dependency Treebank PDT Haji\u02c7c et al . , 2001 and the Danish Dependency Treebank DDT Kromann , 2003 .", "PDT contains 1 . 5M words of newspaper text , annotated in three layers according to the theoretical framework of Functional Generative Description B\u00f6hmov\u00e1 et al . , 2003 .", "Our experiments concern only the analytical layer , and are based on the dedicated training section of the treebank .", "DDT comprises 100k words of text selected from the Danish PAROLE corpus , with annotation property all structures gap degree 0 gap degree 1 gap degree 2 gap degree 3 gap degree 4 edge degree 0 edge degree 1 edge degree 2 edge degree 3 edge degree 4 edge degree 5 edge degree 6 projective planar well nested of primary and secondary dependencies based on Discontinuous Grammar Kromann , 2003 .", "Only primary dependencies are considered in the experiments , which are based on the entire treebank . 4 The results of our experiments are given in Table 1 .", "For the binary constraints planarity , well nestedness , we simply report the number and percentage of structures in each data set that satisfy the constraint .", "For the parametric constraints gap degree , edge degree , we report the number and percentage of structures having degree d d 0 , where degree 0 is equivalent for both gap degree and edge degree to projectivity .", "For DDT , we see that about 15 of all analyses are non projective .", "The minimal degree of non projectivity required to cover all of the data is 2 in the case of gap degree and 4 in the case of edge degree .", "For both measures , the number of structures drops quickly as the degree increases .", "As an example , only 7 or 0 . 17 of the analyses in DDT have gap 4A total number of 17 analyses in DDT were excluded because they either had more than one root node , or violated the indegree constraint .", "Both cases are annotation errors . degree 2 .", "Regarding the binary constraints , we find that planarity accounts for slightly more than the projective structures 86 . 41 of the data is planar , while almost all structures in DDT 99 . 89 meet the well nestedness constraint .", "The difference between the two constraints becomes clearer when we base the figures on the set of non projective structures only out of these , less than 10 are planar , while more than 99 are well nested .", "For PDT , both the number of non projective structures around 23 and the minimal degrees of non projectivity required to cover the full data gap degree 4 and edge degree 6 are higher than in DDT .", "The proportion of planar analyses is smaller than in DDT if we base it on the set of all structures 82 . 16 , but significantly larger when based on the set of non projective structures only 22 . 93 .", "However , this is still very far from the well nestedness constraint , which has almost perfect coverage on both data sets .", "As a general result , our experiments confirm previous studies on non projective dependency parsing Nivre and Nilsson , 2005 ; Hall and Nov\u00e1k , 2005 ; McDonald and Pereira , 2006 The phenomenon of non projectivity cannot be ignored without also ignoring a significant portion of real world data around 15 for DDT , and 23 for PDT .", "At the same time , already a small step beyond projectivity accounts for almost all of the structures occurring in these treebanks .", "More specifically , we find that already an edge degree restriction of d 1 covers 98 . 24 of DDT and 99 . 54 of PDT , while the same restriction on the gap degree scale achieves a coverage of 99 . 84 DDT and 99 . 57 PDT .", "Together with the previous evidence that both measures also have computational advantages , this provides a strong indication for the usefulness of these constraints in the context of non projective dependency parsing .", "When we compare the two graded constraints to each other , we find that the gap degree measure partitions the data into less and larger clusters than the edge degree , which may be an advantage in the context of using the degree constraints as features in a data driven approach towards parsing .", "However , our purely quantitative experiments cannot answer the question , which of the two measures yields the more informative clusters .", "The planarity constraint appears to be of little use as a generalization of projectivity enforcing it excludes more than 75 of the non projective data in PDT , and 90 of the data in DDT .", "The relatively large difference in coverage between the two treebanks may at least partially be explained with their different annotation schemes for sentence final punctuation .", "In DDT , sentence final punctuation marks are annotated as dependents of the main verb of a dependency nexus .", "This , as we have discussed above , places severe restrictions on permitted forms of non projectivity in the remaining sentence , as every discontinuity that includes the main verb must also include the dependent punctuation marks .", "On the other hand , in PDT , a sentencefinal punctuation mark is annotated as a separate root node with no dependents .", "This scheme does not restrict the remaining discontinuities at all .", "In contrast to planarity , the well nestedness constraint appears to constitute a very attractive extension of projectivity .", "For one thing , the almost perfect coverage of well nestedness on DDT and PDT 99 . 89 could by no means be expected on purely combinatorial grounds only 7 of all possible dependency structures for sentences of length 17 the average sentence length in PDT , and only slightly more than 5 of all possible dependency structures for sentences of length 18 the average sentence length in DDT are well nested . 5 Moreover , a cursory inspection of the few problematic cases in DDT indicates that violations of the wellnestedness constraint may , at least in part , be due to properties of the annotation scheme , such as the analysis of punctuation in quotations .", "However , a more detailed analysis of the data from both treebanks is needed before any stronger conclusions can be drawn concerning well nestedness .", "In this paper , we have reviewed a number of proposals for the characterization of mildly non projective dependency structures , motivated by the need to find a better balance between expressivity and complexity than that offered by either strictly projective or unrestricted non projective structures .", "Experimental evaluation based on data from two treebanks shows , that a combination of the wellnestedness constraint and parametric constraints on discontinuity formalized either as gap degree or edge degree gives a very good fit with the empirical linguistic data .", "Important goals for future work are to widen the empirical basis by investigating more languages , and to perform a more detailed analysis of linguistic phenomena that violate certain constraints .", "Another important line of research is the integration of these constraints into parsing algorithms for non projective dependency structures , potentially leading to a better trade off between accuracy and efficiency than that obtained with existing methods .", "Acknowledgements We thank three anonymous reviewers of this paper for their comments .", "The work of Marco Kuhlmann is funded by the Collaborative Research Centre 378 Resource Adaptive Cognitive Processes of the Deutsche Forschungsgemeinschaft .", "The work of Joakim Nivre is partially supported by the Swedish Research Council ."], "summary_lines": ["Mildly Non-Projective Dependency Structures\n", "Syntactic parsing requires a fine balance between expressivity and complexity, so that naturally occurring structures can be accurately parsed without compromising efficiency.\n", "In dependency-based parsing, several constraints have been proposed that restrict the class of permissible structures, such as projectivity, planarity, multi-planarity, well-nestedness, gap degree, and edge degree.\n", "While projectivity is generally taken to be too restrictive for natural language syntax, it is not clear which of the other proposals strikes the best balance between expressivity and complexity.\n", "In this paper, we review and compare the different constraints theoretically, and provide an experimental evaluation using data from two treebanks, investigating how large a proportion of the structures found in the treebanks are permitted under different constraints.\n", "The results indicate that a combination of the well-nestedness constraint and a parametric constraint on discontinuity gives a very good fit with the linguistic data.\n"]}
{"article_lines": ["A Simple Rule Based Part Of Speech Tagger", "Automatic part of speech tagging is an area of natural language processing where statistical techniques have been more successful than rulebased methods .", "In this paper , we present a simple rule based part of speech tagger which automatically acquires its rules and tags with accuracy comparable to stochastic taggers .", "The rule based tagger has many advantages over these taggers , including a vast reduction in stored information required , the perspicuity of a small set of meaningful rules , ease of finding and implementing improvements to the tagger , and better portability from one tag set , corpus genre or language to another .", "Perhaps the biggest contribution of this work is in demonstrating that the stochastic method is not the only viable method for part of speech tagging .", "The fact that a simple rule based tagger that automatically learns its rules can perform so well should offer encouragement for researchers to further explore rule based tagging , searching for a better and more expressive set of rule templates and other variations on the simple but effective theme described below .", "There has been a dramatic increase in the application of probabilistic models to natural language processing over the last few years .", "The appeal of stochastic techniques over traditional rule based techniques comes from the ease with which the necessary statistics can be automatically acquired and the fact that very little handcrafted knowledge need be built into the system .", "In contrast , the rules in rule based systems are usually difficult to construct and are typically not very robust .", "One area in which the statistical approach has done particularly well is automatic part of speech tagging , assigning each word in an input sentence its proper part of speech Church 88 ; Cutting et al . 92 ; DeRose 88 ; Deroualt and Merialdo 86 ; Garside et al .", "87 ; Jelinek 85 ; Kupiec 89 ; Meteer et al . 911 .", "Stochastic taggers have obtained a high degree of accuracy without performing any syntactic analysis on the input .", "These stochastic part of speech taggers make use of a Markov model which captures lexical and contextual information .", "The parameters of the model can be estimated from tagged Church 88 ; DeRose 88 ; Deroualt and Merialdo 86 ; Garside et al . 87 ; Meteer et al .", "91 or untag , ged Cutting et al . 92 ; Jelinek 85 ; Kupiec 89 text .", "Once the parameters of the model are estimated , a sentence can then be automatically tagged by assigning it the tag sequence which is assigned the highest probability by the model .", "Performance is often enhanced with the aid of various higher level pre and postprocessing procedures or by manually tuning the model .", "A number of rule based taggers have been built Klein and Simmons 63 ; Green and Rubin 71 ; Hindle 89 .", "Klein and Simmons 63 and Green and Rubin 71 both have error rates substantially higher than state of the art stochastic taggers .", "Hindle 89 disambiguates words within a deterministic parser .", "We wanted to determine whether a simple rule based tagger without any knowledge of syntax can perform as well as a stochastic tagger , or if part of speech tagging really is a domain to which stochastic techniques are better suited .", "In this paper we describe a rule based tagger which performs as well as taggers based upon probabilistic models .", "The rule based tagger overcomes the limitations common in rule based approaches to language processing it is robust , and the rules are automatically acquired .", "In addition , the tagger has many advantages over stochastic taggers , including a vast reduction in stored information required , the perspicuity of a small set of meaningful rules as opposed to the large tables of statistics needed for stochastic taggers , ease of finding and implementing improvements to the tagger , and better portability from one tag set or corpus genre to another .", "The tagger works by automatically recognizing and remedying its weaknesses , thereby incrementally improving its performance .", "The tagger initially tags by assigning each word its most likely tag , estimated by examining a large tagged corpus , without regard to context .", "In both sentences below , run would be tagged as a verb The run lasted thirty minutes .", "One of the two preceding following words is tagged We run three miles every day .", "The initial tagger has two procedures built in to improve performance ; both make use of no contextual information .", "One procedure is provided with information that words that were not in the training corpus and are capitalized tend to be proper nouns , and attempts to fix tagging mistakes accordingly .", "This information could be acquired automatically see below , but is prespecified in the current implementation .", "In addition , there is a procedure which attempts to tag words not seen in the training corpus by assigning such words the tag most common for words ending in the same three letters .", "For example , blahblahous would be tagged as an adjective , because this is the most common tag for words ending in ous .", "This information is derived automatically from the training corpus .", "This very simple algorithm has an error rate of about 7 . 9 when trained on 90 of the tagged Brown Corpus' Francis and Kueera 82 , and tested on a separate 5 of the corpus . 2 Training consists of compiling a list of the most common tag for each word in the training corpus .", "The tagger then acquires patches to improve its performance .", "Patch templates are of the form The initial tagger was trained on 90 of the corpus the training corpus .", "5 was held back to be used for the patch acquisition procedure the patch corpus and 5 for testing .", "Once the initial tagger is trained , it is used to tag the patch corpus .", "A list of tagging errors is compiled by comparing the output of the tagger to the correct tagging of the patch corpus .", "This list consists of triples taga , tagb , number , indicating the number of times the tagger mistagged a word with taga when it should have been tagged with tagb in the patch corpus .", "Next , for each error triple , it is determined which instantiation of a template from the prespecified set of pdtch templates results in the greatest error reduction .", "Currently , the patch templates are Change tag a to tag b when 8 .", "The previous word is is not capitalized .", "For each error triple taga , tagb , number and patch , we compute the reduction in error which results from applying the patch to remedy the mistagging of a word as taga when it should have been tagged tagb .", "We then compute the number of new errors caused by applying the patch ; that is , the number of times the patch results in a word being tagged as tagb when it should be tagged taga .", "The net improvement is calculated by subtracting the latter value from the former .", "For example , when the initial tagger tags the patch corpus , it mistags 159 words as verbs when they should be nouns .", "If the patch change the tag from verb to noun if one of the two preceding words is tagged as a determiner is applied , it corrects 98 of the 159 errors .", "However , it results in an additional 18 errors from changing tags which really should have been verb to noun .", "This patch results in a net decrease of 80 errors on the patch corpus .", "The patch which results in the greatest improvement to the patch corpus is added to the list of patches .", "The patch is then applied in order to improve the tagging of the patch corpus , and the patch acquisition procedure continues .", "The first ten patches found by the system are listed below3 .", "The first patch states that if a word is tagged TO and the following word is tagged AT , then switch the tag from TO to IN .", "This is because a noun phrase is much more likely to immediately follow a preposition than to immediately follow infinitive TO .", "The second patch states that a tag should be switched from VBN to VBD if the preceding word is capitalized .", "This patch arises from two facts the past verb tag is more likely than the past participle verb tag after a proper noun , and is also the more likely tag for the second word of the sentence . 4 The third patch states that VBD should be changed to VBN if any of the preceding three words are tagged HVD .", "Once the list of patches has been acquired , new text can be tagged as follows .", "First , tag the text using the basic lexical tagger .", "Next , apply each patch in turn to the corpus to decrease the error rate .", "A patch which changes the tagging of a word from a to b only applies if the word has been tagged b somewhere in the training corpus .", "Note that one need not be too careful when constructing the list of patch templates .", "Adding a bad template to the list will not worsen performance .", "If a template is bad , then no rules which are instantiations of that template will appear in the final list of patches learned by the tagger .", "This makes it easy to experiment with extensions to the tagger .", "The tagger was tested on 5 of the Brown Corpus including sections from every genre .", "First , the test corpus was tagged by the simple lexical tagger .", "Next , each of the patches was in turn applied to the corpus .", "Below is a graph showing the improvement in accuracy from applying patches .", "It is significant that with only 71 patches , an error rate of 5 . 1 was obtained' .", "Of the 71 patches , 66 resulted in a reduction in the number of errors in the test corpus , 3 resulted in no net change , and 2 resulted in a higher number of errors .", "Almost all patches which were effective on the training corpus were also effective on the test corpus .", "Unfortunately , it is difficult to compare our results with other published results .", "In Meteer et at .", "91 , an error rate of 3 4 on one domain , Wall Street Journal articles and 5 . 6 on another domain , texts on terrorism in Latin American countries , is quoted .", "However , both the domains and the tag set are different from what we use .", "Church 88 reports an accuracy of quot ; 95 99 correct , depending on the definition of correct quot ; .", "We implemented a version of the algorithm described by Church .", "When trained and tested on the same samples used in our experiment , we found the error rate to be about 4 . 5 .", "DeRose 88 quotes a 4 error rate ; however , the sample used for testing was part of the training corpus .", "Garside et al . 87 reports an accuracy of 96 97 .", "Their probabilistic tagger has been augmented with a handcrafted procedure to pretag problematic quot ; idioms quot ; .", "This procedure , which requires that a list of idioms be laboriously created by hand , contributes 3 toward the accuracy of their tagger , according to DeRose 88 .", "The idiom list would have to be rewritten if one wished to use this tagger for a different tag set or a different corpus .", "It is interesting to note that the information contained in the idiom list can be automatically acquired by the rule based tagger .", "For example , their tagger had difficulty tagging as old as .", "An explicit rule was written to pretag as old as with the proper tags .", "According to the tagging scheme of the Brown Corpus , the first as should be tagged as a qualifier , and the second as a subordinating conjunction .", "In the rule based tagger , the most common tag for as is subordinating conjunction .", "So initially , the second as is tagged correctly and the first as is tagged incorrectly .", "To remedy this , the system acquires the patch if the current word is tagged as a subordinating conjunction , and so is the word two positions ahead , then change the tag of the current word to gualifier . 6 The rule based tagger has automatically learned how to properly tag this quot ; idiom . quot ; Regardless of the precise rankings of the various taggers , we have demonstrated that a simple rule based tagger with very few rules performs on par with stochastic taggers .", "6This was one of the 71 patches acquired by the rule based tagger .", "We have presented a simple part of speech tagger which performs as well as existing stochastic taggers , but has significant advantages over these taggers .", "The tagger is extremely portable .", "Many of the higher level procedures used to improve the performance of stochastic taggers would not readily transfer over to a different tag set or genre , and certainly would not transfer over to a different language .", "Everything except for the proper noun discovery procedure is automatically acquired by the rule based tagger7 , making it much more portable than a stochastic tagger .", "If the tagger were trained on a different corpus , a different set of patches suitable for that corpus would be found automatically .", "Large tables of statistics are not needed for the rulebased tagger .", "In a stochastic tagger , tens of thousands of lines of statistical information are needed to capture contextual information .", "This information is usually a table of trigram statistics , indicating for all tags taga , tag and tag , the probability that tag , follows taga and tagb .", "In the rule based tagger , contextual information is captured in fewer than eighty rules .", "This makes for a much more perspicuous tagger , aiding in better understanding and simplifying further development of the tagger .", "Contextual information is expressed in a much more compact and understandable form .", "As can be seen from comparing error rates , this compact representation of contextual information is just as effective as the information hidden in the large tables of contextual probabilities .", "Perhaps the biggest contribution of this work is in demonstrating that the stochastic method is not the only viable approach for part of speech tagging .", "The fact that the simple rule based tagger can perform so well should offer encouragement for researchers to further explore rule based tagging , searching for a better and more expressive set of patch templates and other variations on this simple but effective theme ."], "summary_lines": ["A Simple Rule-Based Part Of Speech Tagger\n", "Automatic part of speech tagging is an area of natural language processing where statistical techniques have been more successful than rule-based methods.\n", "In this paper, we present a simple rule-based part of speech tagger which automatically acquires its rules and tags with accuracy comparable to stochastic taggers.\n", "The rule-based tagger has many advantages over these taggers, including: a vast reduction in stored information required, the perspicuity of a small set of meaningful rules, ease of finding and implementing improvements to the tagger, and better portability from one tag set, corpus genre or language to another.\n", "Perhaps the biggest contribution of this work is in demonstrating that the stochastic method is not the only viable method for part of speech tagging.\n", "The fact that a simple rule-based tagger that automatically learns its rules can perform so well should offer encouragement for researchers to further explore rule-based tagging, searching for a better and more expressive set of rule templates and other variations on the simple but effective theme described below.\n", "Our rule based POS tagging methods extract rules from training corpus and use these rules to tag new sentence.\n", "We also show that assigning the most common part of speech for each lexical item gives a baseline of 90% accuracy.\n"]}
{"article_lines": ["Creating Speech and Language Data With Amazon rsquo ; s Mechanical Turk", "In this paper we give an introduction to using Amazon ? s Mechanical Turk crowdsourc ing platform for the purpose of collecting data for human language technologies .", "Wesurvey the papers published in the NAACL 2010 Workshop .", "24 researchers participated in the workshop ? s shared task to create data for speech and language applications with 100 .", "This paper gives an overview of the NAACL 2010 Workshop on Creating Speech and Language DataWith Amazon ? s Mechanical Turk .", "A number of recent papers have evaluated the effectiveness of us ing Mechanical Turk to create annotated data for natural language processing applications .", "The lowcost , scalable workforce available through Mechan ical Turk MTurk and other crowdsourcing sites opens new possibilities for annotating speech and text , and has the potential to dramatically changehow we create data for human language technolo gies .", "Open questions include What kind of researchis possible when the cost of creating annotated train ing data is dramatically reduced ?", "What new tasks should we try to solve if we do not limit ourselves to reusing existing training and test sets ?", "Can complex annotation be done by untrained annotators ?", "Howcan we ensure high quality annotations from crowd sourced contributors ? To begin addressing these questions , we orga nized an open ended 100 shared task .", "Researchers were given 100 of credit on Amazon MechanicalTurk to spend on an annotation task of their choosing .", "They were required to write a short paper de scribing their experience , and to distribute the datathat they created .", "They were encouraged to ad dress the following questions How did you conveythe task in terms that were simple enough for non experts to understand ?", "Were non experts as good as experts ?", "What did you do to ensure quality ?", "How quickly did the data get annotated ?", "What is the cost per label ?", "Researchers submitted a 1 page proposalto the workshop organizers that described their in tended experiments and expected outcomes .", "The organizers selected proposals based on merit , and awarded 100 credits that were generously provided by Amazon Mechanical Turk .", "In total , 35 credits were awarded to researchers .", "Shared task participants were given 10 days to run experiments between the distribution of the credit and the initial submission deadline .", "30 papers were submitted to the shared task track , of which 24 were accepted .", "14 papers were submitted to the generaltrack of which 10 were accepted , giving a 77 ac ceptance rate and a total of 34 papers .", "Shared taskparticipants were required to provide the data col lected as part of their experiments .", "All of the shared task data is available on the workshop website .", "Amazon ? s Mechanical Turk1 is an online marketplace for work .", "Amazon ? s tag line for Mechani cal Turk is artificial artificial intelligence , and thename refers to a historical hoax from the 18th cen 1http www . mturk . com 1 1 1 2 2 4 4 8 8 20 20 40 40 1 HIT 1 5 5 10 10 20 20 50 50 100 100 200 200 500 500 1k 1k 5k 5k HITs 1 1 5 5 10 10 20 20 50 50 100 100 200 200 5 17 22 26 19 10 3 100 1 6 9 13 19 17 12 13 5 4 1 100 11 36 22 15 11 4 2 0 100 0 5 10 15 20 25 30 1 1 2 2 4 4 8 8 20 40 Hours spent on Mechanical Turk per week 0 5 10 15 20 1 HIT 5 10 20 50 100 200 500 1k 5k HITs Number of HITs completed per week 0 10 20 30 40 1 5 10 20 50 100 200 Weekly income from Mechanical Turk Figure 1 Time spent , HITs completed , and amount earned from a survey of 1 , 000 Turkers by Ipeirotis 2010 .", "tury where a chess playing automaton appeared tobe able to beat human opponents using a mecha nism , but was , in fact , controlled by a person hidinginside the machine .", "These hint at the the primary focus of the web service , which is to get people to per form tasks that are simple for humans but difficult for computers .", "The basic unit of work on MTurk is even called a Human Intelligence Task HIT .", "Amazon ? s web service provides an easy way to pay people small amounts of money to perform HITs .", "Anyone with an Amazon account can either submit HITs or work on HITs that were submitted by others .", "Workers are referred to as ? Turkers ?", "and people designing the HITs are called ? Requesters . ?", "Requesters set the amount that they will pay for each item that is completed .", "Payments are frequently as low as 0 . 01 .", "Turkers are free to select whichever HITs interest them . , and to disregard HITs that they find uninteresting or which they deem pay too little . Because of its focus on tasks requiring human in telligence , Mechanical Turk is obviously applicable to the field of natural language processing .", "Snow et al 2008 used Mechanical Turk to inexpensively collect labels for several NLP tasks including wordsense disambiguation , word similarity , textual en tailment , and temporal ordering of events .", "Snow et al . had two exciting findings .", "First , they showed that a strong correlation between non expert and expertannotators can be achieved by combining the judgments of multiple non experts , for instance by voting on each label using 10 different Turkers .", "Cor relation and accuracy of labeling could be furtherimproved by weighting each Turker ? s vote by cal ibrating them on a small amount of gold standarddata created by expert annotators .", "Second , they col lected a staggering number of labels for a very small amount of money .", "They collected 21 , 000 labels for just over 25 .", "Turkers put in over 140 hours worth Why do you complete tasks in MTurk ?", "US India To spend free time fruitfully and get cash e . g . , instead of watching TV 70 60 For ? primary ?", "income purposes e . g . , gas , bills , groceries , credit cards 15 27 For ? secondary ?", "income purposes , pocket change for hobbies , gadgets 60 37 To kill time 33 5 The tasks are fun 40 20 Currently unemployed or part time work 30 27 Table 1 Motivations for participating on Mechanical Turk from a survey of 1 , 000 Turkers by Ipeirotis 2010 .", "of human effort to generate the labels .", "The amount of participation is surprisingly high , given the small payment .", "Turker demographics Given the amount of work that can get done for so little , it is natural to ask who would contribute so much work for so little pay , and why ?", "The answers to these questions are often mysterious becauseAmazon does not provide any personal informa tion about Turkers each Turker is identifiable only through a serial number like A23KO2TP7I4KK2 .", "Ipeirotis 2010 elucidates some of the reasons by presenting a demographic analysis of Turkers .", "He built a profile of 1000 Turkers by posting a survey toMTurk and paying 0 . 10 for people to answer questions about their reasons for participating on Me chanical Turk , the amount that they earn each week , and how much time they spend , as well as demo graphic information like country of origin , gender , age , education level , and household income .", "One suspicion that people often have when theyfirst hear about MTurk is that it is some sort of dig ital sweatshop that exploits workers in third world countries .", "However , Ipeirotis reports that nearly half 2 47 of the Turkers who answered his survey were from the United States , with the next largest group 34 coming from India , and the remaining 19 spread between 66 other countries .", "Table 1 gives the survey results for questions relating to why people participate on MechanicalTurk .", "It shows that most US based workers use Me chanical Turk for secondary income purposes to have spending money for hobbies or going out , but that the overwhelming majority of them use it to spend their time more fruitfully i . e . , instead of watching TV .", "The economic downturn mayhave increased participation , with 30 of the US based Turkers reporting that they are unemployedor underemployed .", "The public radio show Mar ketplace recently interviewed unemployed Turkers Rose , 2010 .", "It reports that they earn a little in come , but that they do not earn enough to make a living .", "Figure 1 confirms this , giving a break down of how much time people spend on Mechanical Turk each week , how many HITs they complete , and how much money they earn .", "Most Turkers spend less than 8 hours per week on Mechanical Turk , and earn less than 10 per week through the site .", "Ipeirotis 2010 reports that just over half of Turkers have a college education .", "Despite being reasonably well educated , it is important to keep in mind that Turkers do not have training in specialized subjects like NLP .", "Because the Turkers are non experts , and because the payments are generally so low , quality control is an important consideration when creating data with MTurk . Amazon provides three mechanisms to help en sure quality ? Requesters have the option of rejecting the work of individual Turkers , in which case they are not paid . 2 Turkers can also be blocked from doing future work for a requester .", "2Since the results are downloadable even if they are rejected , this could allow unscrupulous Requesters to abuse Turkers by rejecting all of their work , even if it was done well .", "Turkers have message boards at http www . turkernation . com , where they discuss Requesters .", "They even have a Firefox plu gin called Turkopticon that lets them see ratings of how goodthe Requesters are in terms of communicating with Turkers , be ing generous and fair , and paying promptly .", "Requesters can specify that each HIT should be redundantly completed by several different Turkers .", "This allows higher quality labels to be selected , for instance , by taking the majority label .", "Requesters can require that all workers meeta particular set of qualifications , such as suffi cient accuracy on a small test set or a minimum percentage of previously accepted submissions . Amazon provides two qualifications that a Requester can use by default .", "These are past HIT Approval Rate and Location .", "The location qualifica tion allows the Requester to have HITs done only byresidents of a certain country or to exclude Turk ers from certain regions .", "Additionally , Requesters can design custom Qualification Tests that Turkers must complete before working on a particular HIT .", "These can be created through the MTurk API , and can either be graded manually or automatically .", "An important qualification that isn ? t among Amazon ? s default qualifications is language skills .", "One might design a qualification test to determine a Turker ? s ability to speak Arabic or Farsi before allowing them to do part of speech tagging in those languages , for instance .", "There are several reasons that poor quality data might be generated .", "The task may be too complex orthe instructions might not be clear enough for Turk ers to follow .", "The financial incentives may be too low for Turkers to act conscientiously , and certain HIT designs may allow them to simply randomly click instead of thinking about the task .", "Mason and Watts 2009 present a study of financial incentives on Mechanical Turk and find , counterintuitively , thatincreasing the amount of compensation for a partic ular task does not tend to improve the quality of theresults .", "Anecdotally , we have observed that some times there is an inverse relationship between the amount of payment and the quality of work , because it is more tempting to cheat on high paying HITs ifyou don ? t have the skills to complete them .", "For ex ample , a number of Turkers tried to cheat on an Urdu to English translation HIT by cutting and pastingthe Urdu text into an online machine translation sys tem expressly forbidden in the instructions because we were paying the comparatively high amount of 1 .", "3 3 . 1 Designing HITs for quality control .", "We suggest designing your HITs in a way that will deter cheating or that will make cheating obvious .", "HIT design is part of the art of using MTurk .", "It can ? t be easily quantified , but it has a large impact on the outcome .", "For instance , we reduced cheating on our translation HIT by changing the design so that we displayed images of the Urdu sentences instead of text , which made it impossible to copy and paste into an MT system for anyone who could not type in Arabic script .", "Another suggestion is to include information within the data that you upload to MTurk that will not be displayed to the Turkers , but will be useful to you when reviewing the HITs .", "For example , we include machine translation output along with the source sentences .", "Although this is not displayed to Turkers , when we review the Turkers ?", "translations we compare them to the MT output .", "This allows us to reject translations that are identical to the MT , or which are just random sentences that are unrelated to the original Urdu .", "We also use a javascript3 to gather the IP addresses of the Turkers and do geolocation to look up their location .", "Turkers in Pakistan require less careful scrutiny since they are more likely to be bilingual Urdu speakers than those in Romania , for instance . CrowdFlower4 provides an interface for design ing HITs that includes a phase for the Requester toinput gold standard data with known labels .", "Insert ing items with known labels alongside items which need labels allows a Requester to see which Turkers are correctly replicating the gold standard labels andwhich are not .", "This is an excellent idea .", "If it is possi ble to include positive and negative controls in your HITs , then do so .", "Turkers who fail the controls can be blocked and their labels can be excluded from the final data set .", "CrowdFlower generated HITs even display a score to the Turkers to give them feedback on how well they are doing .", "This provides training for Turkers , and discourages cheating .", "3http wiki . github . com callison burch mechanical_turk_workshop geolocation 4http crowdflower . com 3 . 2 Iterative improvements on MTurk .", "Another class of quality control on Mechanical Turk is through iterative HITs that build on the output of previous HITs .", "This could be used to have Turkersjudge whether the results from a previous HIT con formed to the instructions , and whether it is of high quality .", "Alternately , the second set of Turkers couldbe used to improve the quality of what the first Turkers created .", "For instance , in a translation task , a sec ond set of US based Turkers could edit the English produced by non native speakers .", "CastingWords , 5 a transcription company that usesTurker labor , employs this strategy by having a first pass transcription graded and iteratively improvedin subsequent passes .", "Little et al 2009 even de signed an API specifically for running iterative tasks on MTurk . 6", "Although it is hard to define a set of ? best practices ?", "that applies to all HITs , or even to all NLP HITs , we recommend the following guidelines to Requesters . First and foremost , it is critical to convey instruc tions appropriately for non experts .", "The instructions should be clear and concise .", "To calibrate whetherthe HIT is doable , you should first try the task your self , and then have a friend from outside the field try it .", "This will help to ensure that the instructions are clear , and to calibrate how long each HIT will take which ought to allow you to price the HITs fairly . If possible , you should insert positive and nega tive controls so that you can quickly screen out bad Turkers .", "This is especially important for HITs thatonly require clicking buttons to complete .", "If pos sible , you should include a small amount of gold standard data in each HIT .", "This will allow you todetermine which Turkers are good , but will also al low you weight the Turkers if you are combiningthe judgments of multiple Turkers .", "If you are having Turkers evaluate the output of systems , then ran domize the order that the systems are shown in .", "When publishing papers that use Mechanical Turk as a source of training data or to evaluate the outputof an NLP system , report how you ensured the qual ity of your data .", "You can do this by measuring the 5http castingwords . com 6http groups . csail . mit . edu uid turkit 4inter annotator agreement of the Turkers against ex perts on small amounts of gold standard data , or by stating what controls you used and what criteria youused to block bad Turkers .", "Finally , whenever possi ble you should publish the data that you generate on Mechanical Turk and your analysis scripts and HIT templates alongside your paper so that other people can verify it .", "In the past two years , several papers have published about applying Mechanical Turk to a diverse set ofnatural language processing tasks , including cre ating question answer sentence pairs Kaisser andLowe , 2008 , evaluating machine translation qual ity and crowdsouring translations Callison Burch , 2009 , paraphrasing noun noun compouds for Se mEval Butnariu et al , 2009 , human evaluation oftopic models Chang et al , 2009 , and speech tran scription McGraw et al , 2010 ; Marge et al , 2010a ; Novotney and Callison Burch , 2010a .", "Others haveused MTurk for novel research directions like nonsimulated active learning for NLP tasks such as sen timent classification Hsueh et al , 2009 or doingquixotic things like doing human in the loop min imum error rate training for machine translation Zaidan and Callison Burch , 2009 . Some projects have demonstrated the super scalability of crowdsourced efforts .", "Deng et al 2009 used MTurk to construct ImageNet , an anno tated image database containing 3 . 2 million that arehierarchically categorized using the WordNet ontol ogy Fellbaum , 1998 .", "Because Mechanical Turkallows researchers to experiment with crowdsourc ing by providing small incentives to Turkers , other successful crowdsourcing efforts like Wikipedia or Games with a Purpose von Ahn and Dabbish , 2008 also share something in common with MTurk .", "The workshop included a shared task in which participants were provided with 100 to spend on Me chanical Turk experiments .", "Participants submitted a 1 page proposal in advance describing their intended use of the funds .", "Selected proposals were provided 100 seed money , to which many participants added their own funds .", "As part of their participation , each team submitted a workshop paper describing theirexperiments as well as the data collected and de scribed in the paper .", "Data for the shared papers is available at the workshop website . 7This section describes the variety of data types ex plored and collected in the shared task .", "Of the 24 participating teams , most did not exceed the 100 that they were awarded by a significant amount .", "Therefore , the variety and extent of data described inthis section is the result of a minimal 2 , 400 invest ment .", "This achievement demonstrates the potential for MTurk ? s impact on the creation and curation of speech and language corpora .", "6 . 1 Traditional NLP Tasks .", "An established core set of computational linguistictasks have received considerable attention in the nat ural language processing community .", "These include knowledge extraction , textual entailment and word sense disambiguation .", "Each of these tasks requires a large and carefully curated annotated corpus to train and evaluate statistical models .", "Many of the shared task teams attempted to create new corpora for these tasks at substantially reduced costs using MTurk .", "Parent and Eskenazi 2010 produce new corpora for the task of word sense disambiguation .", "The study used MTurk to create unique word definitions for 50 words , which Turkers then also mapped onto existing definitions .", "Sentences containing these 50words were then assigned to unique definitions ac cording to word sense .", "Madnani and Boyd Graber 2010 measured theconcept of transitivity of verbs in the style of Hop per and Thompson 1980 , a theory that goes beyond simple grammatical transitivity ? whether verbs take objects transitive or not ? to capture the amount of action indicated by a sentence .", "Videos that portrayedverbs were shown to Turkers who described the ac tions shown in the video .", "Additionally , sentencescontaining the verbs were rated for aspect , affirma tion , benefit , harm , kinesis , punctuality , and volition . The authors investigated several approaches for elic iting descriptions of transitivity from Turkers .", "Two teams explored textual entailment tasks .", "Wang and Callison Burch 2010 created data for 7http sites . google . com site amtworkshop2010 5recognizing textual entailment RTE .", "They submit ted 600 text segments and asked Turkers to identifyfacts and counter facts unsupported facts and con tradictions given the provided text .", "The resulting collection includes 790 facts and 203 counter facts . Negri and Mehdad 2010 created a bi lingual entailment corpus using English and Spanish entail ment pairs , where the hypothesis and text come from different languages .", "The authors took a publicly available English RTE data set the PASCAL RTE3 dataset1 and created an English Spanish equivalent by having Turkers translating the hypotheses into Spanish .", "The authors include a timeline of their progress , complete with total cost over the 10 days that they ran the experiments . In the area of natural language generation , Heil man and Smith 2010 explored the potential of MTurk for ranking of computer generated questions about provided texts .", "These questions can be used to test reading comprehension and understanding .", "60 Wikipedia articles were selected , for each of which20 questions were generated .", "Turkers provided 5 ratings for each of the 1 , 200 questions , creating a sig nificant corpus of scored questions .", "Finally , Gordon et al 2010 relied on MTurk to evaluate the quality and accuracy of automatically extracted common sense knowledge factoids fromnews and Wikipedia articles .", "Factoids were pro vided by the KNEXT knowledge extraction system .", "6 . 2 Speech and Vision .", "While MTurk naturally lends itself to text tasks , several teams explored annotation and collection ofspeech and image data .", "We note that one of the pa pers in the main track described tools for collecting such data Lane et al , 2010 .", "Two teams used MTurk to collect text annotations on speech data .", "Marge et al 2010b identified easy and hard sections of meeting speech to transcribe and focused data collection on difficult segments .", "Transcripts were collected on 48 audio clips from4 different speakers , as well as other types of an notations .", "Kunath and Weinberger 2010 collectedratings of accented English speech , in which nonnative speakers were rated as either Arabic , Mandarin or Russian native speakers .", "The authors ob tained multiple annotations for each speech sample , and tracked the native language of each annotator , allowing for an analysis of rating accuracy between native English and non native English annotators .", "Novotney and Callison Burch 2010b used MTurk to elicit new speech samples .", "As part of aneffort to increase the accessibility of public knowl edge , such as Wikipedia , the team prompted Turkers to narrate Wikipedia articles .", "This required Turkers to record audio files and upload them .", "An additionalHIT was used to evaluate the quality of the narra tions .", "A particularly creative data collection approach asked Turkers to create handwriting samples and then to submit images of their writing Tong et al , 2010 .", "Turkers were asked to submit handwrittenshopping lists large vocabulary or weather descrip tions small vocabulary in either Arabic or Spanish .", "Subsequent Turkers provided a transcription and atranslation .", "The team collected 18 images per lan guage , 2 transcripts per image and 1 translation per transcript .", "6 . 3 Sentiment , Polarity and Bias .", "Two papers investigated the topics of sentiment , po larity and bias .", "Mellebeek et al 2010 used severalmethods to obtain polarity scores for Spanish sen tences expressing opinions about automative topics .", "They evaluated three HITs for collecting such data and compared results for quality and expressiveness .", "Yano et al 2010 evaluated the political bias of blogposts .", "Annotators labeled 1000 sentences to deter mine biased phrases in political blogs from the 2008 election season .", "Knowledge of the annotators own biases allowed the authors to study how bias differs on the different ends of the political spectrum .", "6 . 4 Information Retrieval .", "Large scale evaluations requiring significant humanlabor for evaluation have a long history in the in formation retrieval community TREC .", "Grady and Lease 2010 study four factors that influence Turker performance on a document relevance search task .", "The authors present some negative results on how these factors influence data collection .", "For further work on MTurk and information retrieval , readers are encouraged to see the SIGIR 2010 Workshop on Crowdsourcing for Search Evaluation . 8 8http www . ischool . utexas . edu ? cse2010 call . htm 6 6 . 5 Information Extraction .", "Information extraction IE seeks to identify specific types of information in natural languages .", "The IE papers in the shared tasks focused on new domains and genres as well as new relation types . The goal of relation extraction is to identify rela tions between entities or terms in a sentence , such asborn in or religion .", "Gormley et al 2010 automat ically generate potential relation pairs in sentences by finding relation pairs appearing in news articles as given by a knowledge base .", "They ask Turkers ifa sentence supports a relation , does not support a re lation , or whether the relation makes sense .", "They collected close to 2500 annotations for 17 different person relation types . The other IE papers explored new genres and domains .", "Finin et al 2010 obtained named entity an notations person , organization , geopolitical entity for several hundred Twitter messages .", "They conducted experiments using both MTurk and Crowd Flower .", "Yetisgen Yildiz et al 2010 explored medical named entity recognition .", "They selected100 clinical trial announcements from ClinicalTrials . gov . 4 annotators for each of the 100 announce ments identified 3 types of medical entities medical conditions , medications , and laboratory test .", "6 . 6 Machine Translation .", "The most popular shared task topic was MachineTranslation MT .", "MT is a data hungry task that re lies on huge corpora of parallel texts between two languages .", "Performance of MT systems dependson the size of training corpora , so there is a con stant search for new and larger data sets .", "Such data sets are traditionally expensive to produce , requiring skilled translators .", "One of the advantages to MTurk is the diversity of the Turker population , making it an especially attractive source of MT data .", "Shared task papers in MT explored the full range of MT tasks , including alignments , parallel corpus creation , paraphrases and bilingual lexicons .", "Gao and Vogel 2010 create alignments in a 300 sentence Chinese English corpus Chinese aligned to English .", "Both Ambati and Vogel 2010 and Bloodgood and Callison Burch 2010 explore thepotential of MTurk in the creation of MT paral lel corpora for evaluation and training .", "Bloodgoodand Callison Burch replicate the NIST 2009 Urdu English test set of 1792 sentences , paying only 0 . 10 a sentence , a substantially reduced price than the typical annotator cost .", "The result is a data set that isstill effective for comparing MT systems in an eval uation .", "Ambati and Vogel create corpora with 100 sentences and 3 translations per sentence for all the language pairs between English , Spanish , Urdu and Telugu .", "This demonstrates the feasibility of creating cheap corpora for high and low resource languages . Two papers focused on the creation and evaluation of paraphrases .", "Denkowski et al 2010 generated and evaluated 728 paraphrases for Arabic English translation .", "MTurk was used to identify correct and fix incorrect paraphrases .", "Over 1200 high quality paraphrases were created .", "Buzek et al .", "2010 evaluated error driven paraphrases forMT .", "In this setting , paraphrases are used to sim plify potentially difficult to translate segments of text .", "Turkers identified 1780 error regions in 1006 English Chinese sentences .", "Turkers provided 4821 paraphrases for these regions .", "External resources can be an important part of an MT system .", "Irvine and Klementiev 2010 created lexicons for low resource languages .", "They evaluated translation candidates for 100 English words in 32 languages and solicited translations for 10 additional languages .", "Higgins et al 2010 expanded namelists in Arabic by soliciting common Arabic nicknames .", "The 332 collected nicknames were primar ily provided by Turkers in Arab speaking countries 35 , India 46 , and the United States 13 .", "Finally , Zaidan and Ganitkevitch 2010 explored how MTurk could be used to directly improve an MTgrammar .", "Each rule in an Urdu to English translation system was characterized by 12 features .", "Turkers were provided examples for which their feedback was used to rescore grammar productions di rectly .", "This approach shows the potential of fine tuning an MT system with targeted feedback from annotators .", "Looking ahead , we can ? t help but wonder what im pact MTurk and crowdsourcing will have on thespeech and language research community .", "Keeping in mind Niels Bohr ? s famous exhortation ? Pre 7 diction is very difficult , especially if it ? s about the future , ?", "we attempt to draw some conclusions and predict future directions and impact on the field .", "Some have predicted that access to low cost , highly scalable methods for creating language andspeech annotations means the end of work on unsupervised learning .", "Many a researcher has advocated his or her unsupervised learning approach because of annotation costs .", "However , if 100 exam ples for any task are obtainable for less than 100 , why spend the time and effort developing often infe rior unsupervised methods ?", "Such a radical change is highly debatable , in fact , one of this paper ? s authors is a strong advocate of such a position while the other disagrees , perhaps because he himself works on unsupervised methods .", "Certainly , we can agree that the potential exists for a change in focus in a number of ways . In natural language processing , data drives re search .", "The introduction of new large and widelyaccessible data sets creates whole new areas of re search .", "There are many examples of such impact , the most famous of which is the Penn Treebank Marcus .", "et al , 1994 , which has 2910 citations in Google scholar and is the single most cited paper on the ACL anthology network Radev et al , 2009 .", "Other examples include the CoNLL named entity corpus Sang and Meulder 2003 with 348 citationson Google Scholar , the IMDB movie reviews senti ment data Pang et al 2002 with 894 citations and the Amazon sentiment multi domain data Blitzer et al .", "2007 with 109 citations . MTurk means that creating similar data sets is now much cheaper and easier than ever before .", "It is highly likely that new MTurk produced data sets will achieve prominenceand have significant impact .", "Additionally , the cre ation of shared data means more comparison and evaluation against previous work .", "Progress is madewhen it can be demonstrated against previous ap proaches on the same data .", "The reduction of data cost and the rise of independent corpus producers likely means more accessible data .", "More than a new source for cheap data , MTurk isa source for new types of data .", "Several of the pa pers in this workshop collected information about the annotators in addition to their annotations .", "Thiscreates potential for studying how different user demographics understand language and allow for targeting specific demographics in data creation .", "Be yond efficiencies in cost , MTurk provides access to a global user population far more diverse than those provided by more professional annotation settings .", "This will have a significant impact on low resource languages as corpora can be cheaply built for a much wider array of languages .", "As one example , Irvineand Klementiev 2010 collected data for 42 languages without worrying about how to find speakers of such a wide variety of languages .", "Addition ally , the collection of Arabic nicknames requires a diverse and numerous Arabic speaking population Higgins et al , 2010 .", "In addition to extending into new languages , MTurk also allows for the creation of evaluation sets in new genres and domains , which was the focus of two papers in this workshop Fininet al , 2010 ; Yetisgen Yildiz et al , 2010 .", "We ex pect to see new research emphasis on low resource languages and new domains and genres .", "Another factor is the change of data type and itsimpact on machine learning algorithms .", "With pro fessional annotators , great time and care are paid to annotation guidelines and annotator training .", "These are difficult tasks with MTurk , which favors simple intuitive annotations and little training .", "Many papersapplied creative methods of using simpler annota tion tasks to create more complex data sets .", "This process can impact machine learning in a numberof ways .", "Rather than a single gold standard , annotations are now available for many users .", "Learning across multiple annotations may improve sys tems Dredze et al , 2009 .", "Additionally , even withefforts to clean up MTurk annotations , we can ex pect an increase in noisy examples in data .", "This will push for new more robust learning algorithms that are less sensitive to noise .", "If we increase the size of the data ten fold but also increase the noise , can learning still be successful ?", "Another learning area of great interest is active learning , which has long relied on simulated user experiments .", "New workevaluated active learning methods with real users us ing MTurk Baker et al , 2009 ; Ambati et al , 2010 ; Hsueh et al , 2009 ; ? .", "Finally , the composition ofcomplex data set annotations from simple user in puts can transform the method by which we learncomplex outputs .", "Current approaches expect exam ples of labels that exactly match the expectation ofthe system .", "Can we instead provide lower level sim 8 pler user annotations and teach systems how to learn from these to construct complex output ?", "This would open more complex annotation tasks to MTurk .", "A general trend in research is that good ideascome from unexpected places .", "Major transformations in the field have come from creative new ap proaches .", "Consider the Penn Treebank , an ambitious and difficult project of unknown potential .", "Such large changes can be uncommon since they are often associated with high cost , as was the Penn Treebank . However , MTurk greatly reduces these costs , en couraging researchers to try creative new tasks .", "Forexample , in this workshop Tong et al 2010 col lected handwriting samples in multiple languages .", "Their creative data collection may or may not have a significant impact , but it is unlikely that it would have been tried had the cost been very high .", "Finally , while obtaining new data annotationsfrom MTurk is cheap , it is not trivial .", "Workshop par ticipants struggled with how to attract Turkers , howto price HITs , HIT design , instructions , cheating de tection , etc . No doubt that as work progresses , so will a communal knowledge and experience of how to use MTurk .", "There can be great benefit in new toolkits for collecting language data using MTurk , and indeed some of these have already started to emerge Lane et al , 2010 9 .", "AcknowledgementsThanks to Sharon Chiarella of Amazon ? s Mechan ical Turk for providing 100 credits for the shared task , and to CrowdFlower for allowing free use of their tool to workshop participants . Research funding was provided by the NSF under grant IIS 0713448 , by the European Commis sion through the EuroMatrixPlus project , and by the DARPA GALE program under Contract No .", "HR0011 06 2 0001 .", "The views and findings are the authors ?", "alone ."], "summary_lines": ["Creating Speech and Language Data With Amazon&rsquo;s Mechanical Turk\n", "In this paper we give an introduction to using Amazon\u2019s Mechanical Turk crowdsourcing platform for the purpose of collecting data for human language technologies.\n", "We survey the papers published in the NAACL2010 Workshop.\n", "24 researchers participated in the workshop\u2019s shared task to create data for speech and language applications with $100.\n", "We experiment with the use of Amazon Mechanical Turk (AMT) to create and evaluate human language data (Callison-Burch and Dredze, 2010).\n", "We provide an overview of various tasks for which MTurk has been used, and offer a set of best practices for ensuring high-quality data.\n"]}
{"article_lines": ["Selectional Preference And Sense Disambiguation", "absence of is a real problem for corpus based approaches to sense disambiguation , one that is unlikely to be solved soon .", "Selectional preference is traditionally connected with sense ambiguity ; this paper explores how a statistical model of selectional preference , requiring neither manual annotation of selection restrictions nor supervised training , can be used in sense disambiguation .", "It has long been observed that selectional constraints and word sense disambiguation are closely linked .", "Indeed , the exemplar for sense disambiguation in most computational settings e . g . , see Allen's 1995 discussion is Katz and Fodor's 1964 use of Boolean selection restrictions to constrain semantic interpretation .", "For example , although burgundy can be interpreted as either a color or a beverage , only the latter sense is available in the context of Mary drank burgundy , because the verb drink specifies the selection restriction LIQUID for its direct objects .", "Problems with this approach arise , however , as soon as the domain of interest becomes too large or too rich to specify semantic features and selection restrictions accurately by hand .", "This paper concerns the use of selectional constraints for automatic sense disambiguation in such broad coverage settings .", "The approach combines statistical and knowledge based methods , but unlike many recent corpus based approaches to sense disambiguation Yarowsky , 1993 ; Bruce and Wiebe , 1994 ; Miller et al . , 1994 , it takes as its starting point the assumption that senseannotated training text is not available .", "Motivating this assumption is not only the limited availability of such text at present , but skepticism that the situation will change any time soon .", "In marked contrast to annotated training material for partof speech tagging , a there is no coarse level set of sense distinctions widely agreed upon whereas part of speech tag sets tend to differ in the details ; b sense annotation has a comparatively high error rate Miller , personal communication , reports an upper bound for human annotators of around 90 for ambiguous cases , using a non blind evaluation method that may make even this estimate overly optimistic ; and c no fully automatic method provides high enough quality output to support the quot ; annotate automatically , correct manually quot ; methodology used to provide high volume annotation by data providers like the Penn 'Treebank project Marcus et al . , 1993 .", "The treatment of selectional preference used here is that proposed by Resnik 1993a ; 1996 , combining statistical and knowledge based methods .", "The basis of the approach is a probabilistic model capturing the co occurrence behavior of predicates and conceptual classes in the taxonomy .", "The intuition is illustrated in Figure 1 .", "The prior distribution PrR c captures the probability of a class occurring as the argument in predicate argument relation R , regardless of the identity of the predicate .", "For example , given the verb subject relationship , the prior probability for person tends to be significantly higher than the prior probability for insect .", "However , once the identity of the predicate is taken into account , the probabilities can change if the verb is buzz , then the probability for insect can be expected to be higher than its prior , and person will likely be lower .", "In probabilistic terms , it is the difference between this conditional or posterior distribution and the prior distribution that determines selectional preference .", "Information theory provides an appropriate way to quantify the difference between the prior and posterior distributions , in the form of relative entropy Kullback and Leibler , 1951 .", "The model defines the selectional preference strength of a predicate as Intuitively , SR p measures how much information , in bits , predicate p provides about the conceptual class of its argument .", "The better Pr c approximates Pr c1p , the less influence p is having on its argument , and therefore the less strong its selectional preference .", "Given this definition , a natural way to characterize the quot ; semantic fit quot ; of a particular class as the argument to a predicate is by its relative contribution to the overall selectional preference strength .", "In particular , classes that fit very well can be expected to have higher posterior probabilities , compared to their priors , as is the case for insect in Figure 1 .", "Formally , selectional association is defined as This model of selectional preference has turned out to make reasonable predictions about human judgments of argument plausibility obtained by psycholinguistic methods Resnik , 1993a .", "Closely related proposals have been applied in syntactic disambiguation Resnik , 1993b ; Lauer , 1994 and to automatic acquisition of more KatzFodoresque selection restrictions in the form of weighted disjunctions Ribas , 1994 .", "The selectional association has also been used recently to explore apparent cases of syntactic optionality Paola Merlo , personal communication .", "If taxonomic classes were labeled explicitly in a training corpus , estimation of probabilities in the model would be fairly straightforward .", "But since text corpora contain words , not classes , it is necessary to treat each occurrence of a word in an argument position as if it might represent any of the conceptual classes to which it belongs , and assign frequency counts accordingly .", "At present , this is done by distributing the quot ; credit quot ; for an observation uniformly across all the conceptual classes containing an observed argument .", "Formally , given a predicate argument relationship R for example , the verb object relationship , a predicate p , and a conceptual class c , where countR p , w is the number of times word w was observed as the argument of p with respect to R , and classes w is the number of taxonomic classes to which w belongs .", "Given the frequencies , probabilities are currently estimated using maximum likelihood ; the use of word classes is itself a form of smoothing cf .", "Pereira et al . 1993 . 1 This estimation method is similar to that used by Yarowsky 1992 for Roget's thesaurus categories , and works for similar reasons .", "As an example , consider two instances of the verb object relationship in a training corpus , drink coffee and drink wine .", "Coffee has 2 senses in the WordNet 1 . 4 noun taxonomy , and belongs to 13 classes in all , and wine has 2 senses and belongs to a total of 16 classes .", "This means that the observed countverb obj drink , coffee 1 will be distributed by adding A to the joint frequency with drink for each of the 13 classes containing coffee .", "Similarly , the joint frequency with drink will be incremented by for each of the 16 classes containing wine .", "Crucially , although each of the two words is ambiguous , only those taxonomic classes containing both words e . g . , beverage receive credit for both observed instances .", "In general , because different words are ambiguous in different ways , credit tends to accumulate in the taxonomy only in those classes for which there is real evidence of co occurrence ; the rest tends to disperse unsystematically , resulting primarily in noise .", "Thus , despite the absence of class annotation in the training text , it is still possible to arrive at a usable estimate of class based probabilities .", "Table 1 presents a selected sample of Resnik's 1993a comparison with argument plausibility judgments made by human subjects .", "What is most interesting here is the way in which strongly selecting 'Word w is typically the head of a noun phrase , which could lead the model astray for example , toy soldiers behave differently from soldiers McCawley , 1968 .", "In principle , addressing this issue requires that noun phrases be mapped to taxonomic classes based on their compositional interpretation ; however , such complications rarely arise in practice . verbs quot ; choose quot ; the sense of their arguments .", "For example , letter has 3 senses in WordNet , 2 and belongs to 19 classes in all .", "In order to approximate its plausibility as the object of write , the selectional association with write was computed for all 19 classes , and the highest value returned in this case , writing quot ; anything expressed in letters ; reading matter quot ; .", "Since only one sense of letter has this class as an ancestor , this method of determining argument plausibility has , in essence , performed sense disambiguation as a side effect .", "This observation suggests the following simple algorithm for disambiguation by selectional preference .", "Let n be a noun that stands in relationship R to predicate p , and let 81 , sk be its possible senses .", "For i from 1 to k , compute C1 fele is an ancestor of s ai max AR p , c cEc , and assign ai as the score for sense si .", "The simplest way to use the resulting scores , following Miller et al . 1994 , is as follows if n has only one sense , select it ; otherwise select the sense si for which ai is greatest , breaking ties by random choice .", "Task and materials .", "Test and training materials were derived from the Brown corpus of American English , all of which has been parsed and manually verified by the Penn Treebank project Marcus et al . , 1993 and parts of which have been manually sense tagged by the WordNet group Miller et al . , 1993 .", "A parsed , sense tagged corpus was obtained by merging the WordNet sense tagged corpus approximately 200 , 000 words of source text from the Brown corpus , distributed across genres with the corresponding Penn Treebank parses . 3 The rest of the Brown corpus approximately 800 , 000 words of source text remained as a parsed , but not sensetagged , training set .", "The test set for the verb object relationship was constructed by first training a selectional preference model on the training corpus , using the Treebank's tgrep utility to extract verb object pairs from parse trees .", "The 100 verbs that select most strongly for their objects were identified , excluding verbs appearing only once in the training corpus ; test instances of the form verb , object , correct sense were then extracted from the merged test corpus , including all triples where verb was one of the 100 test verbs . 4 Evaluation materials were obtained in the same manner for several other surface syntactic relationships , including verb subject John admires , adjective noun tall building , modifier head river bank , and head modifier river z bank .", "Baseline .", "Following Miller et al . 1994 , disambiguation by random choice was used as a baseline if a noun has one sense , use it ; otherwise select at random among its senses .", "Results .", "Since both the algorithm and the baseline may involve random choices , evaluation involved multiple runs with different random seeds .", "Table 2 summarizes the results , taken over 10 runs , considering only ambiguous test cases .", "All differences between the means for algorithm and baseline were statistically significant .", "Discussion .", "The results of the experiment show that disambiguation using automatically acquired selectional constraints leads to performance significantly better than random choice .", "Not surprisingly , though , the results are far from what one might expect to obtain with supervised training .", "In that respect , the most direct point of comparison is the performance of Miller et al . 's 1994 frequency heuristic always choose the most frequent sense of a word as evaluated using the full sense tagged corpus , including nouns , verbs , adjectives , and adverbs .", "For ambiguous words , they report 58 . 2 correct , as compared to a random baseline of 26 . 8 .", "Crucially , however , the frequency heuristic requires sense tagged training data Miller et al . evaluated via cross validation , and this paper starts from the assumption that such data are unavailable .", "A fairer comparison , therefore , considers al'Excluded were some inapplicable cases , e . g . where object was a proper noun tagged as person . ternative unsupervised algorithms though unfortunately the literature contains more proposed algorithms than quantitative evaluations of those algorithms .", "One experiment where results were reported was conducted by Cowie et at .", "1992 ; their method involved using a stochastic search procedure to maximize the overlap in dictionary definitions LDOCE for alternative senses of words co occurring in a sentence .", "They report an accuracy of 72 for disambiguation to the homograph level , and 47 for disambiguation to the sense level .", "Since the task here involved WordNet sense distinctions , which are rather fine grained , the latter value is more appropriate for comparison .", "Their experiment was more general in that they did not restrict themselves to nouns ; on the other hand , their test set involved disambiguating words taken from full sentences , so the percentage correct may have been improved by the presence of unambiguous words .", "Sussna 1993 has also looked at unsupervised disambiguation of nouns using WordNet .", "Like Cowie et al . , his algorithm optimizes a measure of semantic coherence over an entire sentence , in this case pairwise semantic distance between nouns in the sentence as measured using the noun taxonomy .", "Comparison of results is somewhat difficult , however , for two reasons .", "First , Sussna used an earlier version of WordNet version 1 . 2 having a significantly smaller noun taxonomy 35K nodes vs . 49K nodes .", "Second , and more significant , in creating the test data , Sussna's human sense taggers tagging articles from the Time IR test collection were permitted to tag a noun with as many senses as they felt were quot ; good , quot ; rather than making a forced choice ; Sussna develops a scoring metric based on that fact rather than requiring exact matches to a single best sense .", "This is quite a reasonable move see discussion below , but unfortunately not an option in the present experiment .", "Nonetheless , some comparison is possible , since he reports a quot ; correct , quot ; apparently treating a sense assignment as correct if any of the quot ; good quot ; senses is chosen his experiments have a lower bound chance of about 40 correct , with his algorithm performing at 53 55 , considering only ambiguous cases .", "The best results reported for an unsupervised sense disambiguation method are those of Yarowsky 1992 , who uses evidence from a wider context a window of 100 surrounding words to build up a co occurrence model using classes from Roget's thesaurus .", "He reports accuracy figures in the 72 99 range mean 92 in disambiguating test instances involving twelve quot ; interesting quot ; polysemous words .", "As in the experiments by Cowie et al . , the choice of coarser distinctions presumably accounts in part for the high accuracy .", "By way of comparison , some words in Yarowsky's test set would require choosing among ten senses in WordNet , as compared to a maximum of six using the Roget's thesaurus categories ; the mean level of polysemy for the tested words is a six way distinction in WordNet as compared to a three way distinction in Roget's thesaurus .", "As an aside , a rich taxonomy like WordNet permits a more continuous view of the sense vs . homograph distinction .", "For example , town has three senses in WordNet , corresponding to an administrative district , a geographical area , and a group of people .", "Given town as the object of leave , selectional preference will produce a tie between the first two senses , since both inherit their score from a common ancestor , location .", "In effect , the automatic selection of a class higher in the taxonomy as having the highest score provides the same coarse category that might be provided by a homograph sense distinction in another setting .", "The choice of coarser category varies dynamically with the context as the argument in rural town , the same two senses still tie , but with region a subclass of location as the common ancestor that determines the score .", "In other work , Yarowsky 1993 has shown that local collocational information , including selectional constraints , can be used to great effect in sense disambiguation , though his algorithm requires supervised training .", "The present work can be viewed as an attempt to take advantage of the same kind of information , but in an unsupervised setting .", "Although the definition of selectional preference strength is motivated by the use of relative entropy in information theory , selectional association is not ; the approach would benefit from experimentation with alternative statistical association measures , particularly a comparison with simple mutual information and with the likelihood ratio .", "Combining information about selectional preference could also be helpful , e . g . , where a noun is both the object of a verb and modified by an adjective , though such cases are rarer than one might expect .", "More important is information beyond selectional preference , notably the wider context utilized by Yarowsky 1992 .", "Performance of the method explored here is limited at present , though not surprisingly so when taken in the context of previous attempts at unsupervised disambiguation using finegrained senses .", "One main message to take away from this experiment is the observation that , although selectional preferences are widely viewed as an important factor in disambiguation , their practical broad coverage application appears limited at least when disambiguating nouns because many verbs and modifiers simply do not select strongly enough to make a significant difference .", "They may provide some evidence , but most likely only as a complement to other sources of information such as frequency based priors , topical context , and the like .", "Much of this work was conducted at Sun Microsystems Laboratories in Chelmsford , Massachusetts ."], "summary_lines": ["Selectional Preference And Sense Disambiguation\n", "The absence of training data is a real problem for corpus-based approaches to sense disambiguation, one that is unlikely to be solved soon.\n", "Selectional preference is traditionally connected with sense ambiguity; this paper explores how a statistical model of selectional preference, requiring neither manual annotation of selection restrictions nor supervised training, can be used in sense disambiguation.\n", "We define selectional preference as the amount of information a verb provides about its semantic argument classes.\n", "We present a method to acquire a set of conceptual classes for word senses, employing selectional preferences, based on the idea that certain linguistic predicates constraint the semantic interpretation of underlying words into certain classes.\n", "In determining selectional preferences, we use uniformly distributing observed frequencies for a given word across all its senses.\n"]}
{"article_lines": ["CCG Supertags in Factored Statistical Machine Translation", "Combinatorial Categorial Grammar CCG supertags present phrase based machine translation with an opportunity to access rich syntactic information at a word level . The challenge is incorporating this informa tion into the translation process .", "Factoredtranslation models allow the inclusion of supertags as a factor in the source or target language .", "We show that this results in an im provement in the quality of translation and that the value of syntactic supertags in flat structured phrase based models is largely due to better local reorderings .", "In large scale machine translation evaluations , phrase based models generally outperform syntax based models1 .", "Phrase based models are effectivebecause they capture the lexical dependencies be tween languages .", "However , these models , which are equivalent to finite state machines Kumar and Byrne , 2003 , are unable to model long range word order differences .", "Phrase based models also lack the ability to incorporate the generalisations implicit insyntactic knowledge and they do not respect linguistic phrase boundaries .", "This makes it difficult to im prove reordering in phrase based models .", "Syntax based models can overcome some of theproblems associated with phrase based models because they are able to capture the long range struc tural mappings that occur in translation .", "Recently 1www . nist . gov speech tests mt mt06eval official results . html there have been a few syntax based models that show performance comparable to the phrase basedmodels Chiang , 2005 ; Marcu et al , 2006 .", "How ever , reliably learning powerful rules from parallel data is very difficult and prone to problems with sparsity and noise in the data .", "These models also suffer from a large search space when decoding with an integrated language model , which can lead to search errors Chiang , 2005 . In this paper we investigate the idea of incorporating syntax into phrase based models , thereby lever aging the strengths of both the phrase based models and syntactic structures .", "This is done using CCG supertags , which provide a rich source of syntactic information .", "CCG contains most of the structure ofthe grammar in the lexicon , which makes it possible to introduce CCG supertags as a factor in a factored translation model Koehn et al , 2006 .", "Fac tored models allow words to be vectors of features one factor could be the surface form and other fac tors could contain linguistic information .", "Factored models allow for the easy inclusion of supertags in different ways .", "The first approach is to generate CCG supertags as a factor in the target and then apply an n gram model over them , increasing the probability of more frequently seen sequencesof supertags .", "This is a simple way of including syn tactic information in a phrase based model , and has also been suggested by Hassan et al 2007 .", "For both Arabic English Hassan et al , 2007 and our experiments in Dutch English , n gram models over CCG supertags improve the quality of translation .", "By preferring more likely sequences of supertags , it is conceivable that the output of the decoder is 9 more grammatical .", "However , its not clear exactlyhow syntactic information can benefit a flat structured model the constraints contained within su pertags are not enforced and relationships between supertags are not linear .", "We perform experiments to explore the nature and limits of the contribution of supertags , using different orders of n gram models , reordering models and focussed manual evaluation .", "It seems that the benefit of using n gram supertagsequence models is largely from improving reordering , as much of the gain is eroded by using a lexi calised reordering model .", "This is supported by the manual evaluation which shows a 44 improvement in reordering Dutch English verb final sentences .", "The second and novel way we use supertags is to direct the translation process .", "Supertags on thesource sentence allows the decoder to make decisions based on the structure of the input .", "The subcategorisation of a verb , for instance , might help select the correct translation .", "Using multiple dependencies on factors in the source , we need a strat egy for dealing with sparse data .", "We propose using a logarithmic opinion pool Smith et al , 2005 to combine the more specific models which depend onboth words and supertags with more general mod els which only depends on words .", "This paper is the first to suggest this approach for combining multiple information sources in machine translation . Although the addition of supertags to phrase based translation does show some improvement , their overall impact is limited .", "Sequence models over supertags clearly result in some improvementsin local reordering but syntactic information con tains long distance dependencies which are simply not utilised in phrase based models .", "Inspired by work on factored language models , Koehn et al 2006 extend phrase based models to incorporate multiple levels of linguistic knowledgeas factors .", "Phrase based models are limited to se quences of words as their units with no access to additional linguistic knowledge .", "Factors allow for richer translation models , for example , the gender or tense of a word can be expressed .", "Factors also allow the model to generalise , for example , the lemma of a word could be used to generalise to unseen inflected forms .", "The factored translation model combines features in a log linear fashion Och , 2003 .", "The most likely target sentence t ?", "is calculated using the decision rule in Equation 1 t ?", "argmax t M ?", "m 1 ? mhm s Fs 1 , t Ft 1 1 t ?", "M ? m 1 ? mhm s Fs 1 , t Ft 1 2 where M is the number of features , hm s Fs 1 , t Ft 1 are the feature functions over the factors , and ? are the weights which combine the features which areoptimised using minimum error rate training Venu gopal and Vogel , 2005 .", "Each function depends on a vector sFs1 of source factors and a vector t Ft1 of tar get factors .", "An example of a factored model used in upcoming experiments is t ?", "M ? m 1 ? mhm sw , twc 3 where sw means the model depends on s ource w ords , and twc means the model generates t arget w ords and c cg supertags .", "The model is shown graphically in Figure 1 .", "WordWord CCG SOURCE TARGETFigure 1 .", "Factored translation with source words deter mining target words and CCG supertagsFor our experiments we used the following fea tures the translation probabilities Pr sFs1 t Ft 1 and Pr tFt1 s Fs 1 , the lexical weights Koehn et al , 2003 lex sFs1 t Ft 1 and lex t Ft 1 s Fs 1 , and a phrase penalty e , which allows the model to learn a preference for longer or shorter phrases .", "Added to these features 10 is the word penalty e ? 1 which allows the model to learn a preference for longer or shorter sentences , the distortion model d that prefers monotone word order , and the language model probability Pr t .", "All these features are logged when combined in the log linear model in order to retain the impact of very unlikely translations or sequences .", "One of the strengths of the factored model is it allows for n gram distributions over factors on the target .", "We call these distributions sequence models .", "By analogy with language models , for example , we can construct a bigram sequence model as follows p f1 , f2 , . . .", "fn p f1 n ?", "i 2 p fi f i ? 1 where f is a factor eg .", "CCG supertags and n is the length of the string .", "Sequence models over POS tags or supertags are smaller than language modelsbecause they have restricted lexicons .", "Higher or der , more powerful sequence models can therefore be used .", "Applying multiple factors in the source can lead to sparse data problems .", "One solution is to break down the translation into smaller steps and translate each factor separately like in the following model where source words are translated separately to the source supertags t ?", "M ? m 1 ? mhm sw , tw N ?", "n 1 ? nhn sc , tw However , in many cases multiple dependenciesare desirable .", "For instance translating CCG supertags independently of words could introduce er rors .", "Multiple dependencies require some form of backing off to simpler models in order to cover the cases where , for instance , the word has been seen intraining , but not with that particular supertag .", "Dif ferent backoff paths are possible , and it would beinteresting but prohibitively slow to apply a strat egy similar to generalised parallel backoff Bilmesand Kirchhoff , 2003 which is used in factored language models .", "Backoff in factored language models is made more difficult because there is no obvious backoff path .", "This is compounded for fac tored phrase based translation models where one has to consider backoff in terms of factors and n gramlengths in both source and target languages .", "Fur thermore , the surface form of a word is probably themost valuable factor and so its contribution must al ways be taken into account .", "We therefore did not use backoff and chose to use a log linear combination of features and models instead .", "Our solution is to extract two translation models t ?", "M ? m 1 ? mhm swc , tw N ?", "n 1 ? nhn sw , tw 4 One model consists of more specific features m and would return log probabilities , for example log2Pr tw swc , if the particular word and supertaghad been seen before in training .", "Otherwise it re turns ? C , a negative constant emulating log2 0 .", "The other model consist of more general features n and always returns log probabilities , for example log2Pr tw sw .", "CCGs have syntactically rich lexicons and a small set of combinatory operators which assemble the parse trees .", "Each word in the sentence is assigned a category from the lexicon .", "A category may either be atomic S , NP etc . or complex S S , S NP NP etc . .", "Complex categories have the general form ? ?", "or ? ?", "where ? and ? are themselves cate gories .", "An example of a CCG parse is given Peter eats apples NP S NP NP NP S NP S where the derivation proceeds as follows ? eats ?", "is combined with ? apples ?", "under the operation of forward application .", "? eats ?", "can be thought of as a function that takes a NP to the right and returns a S NP .", "Similarly the phrase ? eats apples ?", "can be thought of as a function which takes a noun phraseNP to the left and returns a sentence S . This opera tion is called backward application . A sentence together with its CCG categories al ready contains most of the information present in a full parse .", "Because these categories are lexicalised , 11they can easily be included into factored phrase based translation .", "CCG supertags are categories that have been provided by a supertagger .", "Supertagswere introduced by Bangalore 1999 as a way of in creasing parsing efficiency by reducing the number of structures assigned to each word .", "Clark 2002 developed a suppertagger for CCG which uses a conditional maximum entropy model to estimate theprobability of words being assigned particular cat egories .", "Here is an example of a sentence that has been supertagged in the training corpus We all agree on that . NP NP NP S dcl NP PP PP NP NP . The verb ? agree ?", "has been assigned a complex su pertag S dcl NP PP which determines the type and direction of its arguments .", "This information can be used to improve the quality of translation .", "The first set of experiments explores the effect of CCG supertags on the target , translating from Dutch into English .", "The last experiment shows the effect of CCG supertags on the source , translating from German into English .", "These language pairs present a considerable reordering challenge .", "For example , Dutch and German have SOVword order in subordi nate clauses .", "This means that the verb often appears at the end of the clause , far from the position of the English verb .", "4 . 1 Experimental Setup .", "The experiments were run using Moses2 , an opensource factored statistical machine translation system .", "The SRILM language modelling toolkit Stolcke , 2002 was used with modified Kneser Ney discounting and interpolation .", "The CCG supertagger Clark , 2002 ; Clark and Curran , 2004 was pro vided with the C C Language Processing Tools3 .", "The supertagger was trained on the CCGBank in English Hockenmaier and Steedman , 2005 and in German Hockenmaier , 2006 .", "The Dutch English parallel training data comesfrom the Europarl corpus Koehn , 2005 and ex cludes the proceedings from the last quarter of 2000 .", "2see http www . statmt . org moses 3see http svn . ask . it . usyd . edu . au trac candc wiki This consists of 855 , 677 sentences with a maximum of 50 words per sentence .", "500 sentences of tuning data and the 2000 sentences of test data are takenfrom the ACLWorkshop on Building and Using Par allel Texts4 .", "The German English experiments use data from the NAACL 2006 Workshop on Statistical Machine Translation5 .", "The data consists of 751 , 088 sentences of training data , 500 sentences of tuning data and3064 sentences of test data .", "The English and Ger man training sets were POS tagged and supertagged before lowercasing .", "The language models and thesequence models were trained on the Europarl train ing data .", "Where not otherwise specified , the POStag and supertag sequence models are 5 gram mod els and the language model is a 3 gram model .", "4 . 2 Sequence Models Over Supertags .", "Our first Dutch English experiment seeks to estab lish what effect sequence models have on machinetranslation .", "We show that supertags improve trans lation quality .", "Together with Shen et al 2006 it is one of the first results to confirm the potential of the factored model .", "Model BLEU sw , tw 23 . 97 sw , twp 24 . 11 sw , twc 24 . 42 sw , twpc 24 . 43 Table 1 .", "The effect of sequence models on Dutch EnglishBLEU score .", "Factors are w ords , p os tags , c cg su pertags on the source s or the target tTable 1 shows that sequence models over CCG su pertags in the target model sw , twc improves over the baseline model sw , tw which has no supertags .", "Supertag sequence models also outperform models which apply POS tag sequence models sw , twp and , interestingly do just as well as models whichapply both POS tag and supertag sequence mod els sw , twps .", "Supertags are more informative than POS tags as they contain the syntactic context of a word .", "These experiments were run with the distortion limit set to 6 .", "This means that at most 6 words in 4see http www . statmt . org wpt05 5see http www . statmt . org wpt06 12 the source sentence can be skipped .", "We tried setting the distortion limit to 15 to see if allowing longer distance reorderings with CCG supertag sequence models could further improve performance , however it resulted in a decrease in performance to a BLEU score of 23 . 84 .", "4 . 3 Manual Analysis .", "The BLEU score improvement in Table 1 does not explain how the supertag sequence models affect the translation process .", "As suggested by Callison Burch et al 2006 we perform a focussed manual analysis of the output to see what changes have occurred .", "From the test set , we randomly selected 100 sentences which required reordering of verbs the Dutch sentences ended with a verb which had to be moved forward in the English translation .", "We record whether or not the verb was correctly translated and whether it was reordered to the correct position in the target sentence .", "Model Translated Reordered sw , tw 81 36 sw , twc 87 52 Table 2 .", "Analysis of correct translation and reordering of verbs for Dutch English translation In Table 2 we can see that the addition of the CCGsupertag sequence model improved both the transla tion of the verbs and their reordering .", "However , theimprovement is much more pronounced for reordering .", "The difference in the reordering results is signif icant at p 0 . 05 using the ? 2 significance test .", "Thisshows that the syntactic information in the CCG su pertags is used by the model to prefer better word order for the target sentence . In Figure 2 we can see two examples of DutchEnglish translations that have improved with the ap plication of CCG supertag sequence models .", "In the first example the verb ? heeft ?", "occurs at the end of the source sentence .", "The baseline model sw , tw does not manage to translate ? heeft ? .", "The model with the CCG supertag sequence model sw , twc translates it correctly as ? has ?", "and reorders it correctly 4 placesto the left .", "The second example also shows the se quence model correctly translating the Dutch verb at the end of the sentence ? nodig ? .", "One can see that it is still not entirely grammatical .", "The improvements in reordering shown here are reorderings over a relatively short distance , two or three positions .", "This is well within the 5 gram orderof the CCG supertag sequence model and we there fore consider this to be local reordering .", "4 . 4 Order of the Sequence Model .", "The CCG supertags describe the syntactic context of the word they are attached to .", "Therefore theyhave an influence that is greater in scope than sur face words or POS tags .", "Increasing the order ofthe CCG supertag sequence model should also increase the ability to perform longer distance reorder ing .", "However , at some point the reliability of the predictions of the sequence models is impaired due to sparse counts .", "Model None 1gram 3gram 5gram 7gram sw , twc 24 . 18 23 . 96 24 . 19 24 . 42 24 . 32 sw , twpc 24 . 34 23 . 86 24 . 09 24 . 43 24 . 14 Table 3 .", "BLUE scores for Dutch English models which apply CCG supertag sequence models of varying orders In Table 3 we can see that the optimal order for the CCG supertag sequence models is 5 .", "4 . 5 Language Model vs . Supertags .", "The language model makes a great contribution to the correct order of the words in the target sentence .", "In this experiment we investigate whether by using astronger language model the contribution of the sequence model will no longer be relevant .", "The relative contribution of the language mode and different sequence models is investigated for different lan guage model n gram lengths .", "Model None 1gram 3gram 5gram 7gram sw , tw 21 . 22 23 . 97 24 . 05 24 . 13 sw , twp 21 . 87 21 . 83 24 . 11 24 . 25 24 . 06 sw , twc 21 . 75 21 . 70 24 . 42 24 . 67 24 . 60 sw , twpc 21 . 99 22 . 07 24 . 43 24 . 48 24 . 42 Table 4 .", "BLEU scores for Dutch English models which use language models of increasing n gram length .", "Column None does not apply any language model .", "Model sw , tw does not apply any sequence models , and model sw , twpc applies both POS tag and supertag sequence models .", "In Table 4 we can see that if no language model is present None , the system benefits slightly from 13 source hij kan toch niet beweren dat hij daar geen exacte informatie over heeft ! reference how can he say he does not have any precise information ? sw , tw he cannot say that he is not an exact information about . sw , twc he cannot say that he has no precise information on this ! source wij moeten hun verwachtingen niet beschamen . meer dan ooit hebben al die landen thans onze bijstand nodig reference we must not disappoint them in their expectations , and now more than ever these countries need our help sw , tw we must not fail to their expectations , more than ever to have all these countries now our assistance necessary sw , twc we must not fail to their expectations , more than ever , those countries now need our assistance Figure 2 .", "Examples where the CCG supertag sequence model improves Dutch English translation having access to all the other sequence models .", "However , the language model contribution is verystrong and in isolation contributes more to transla tion performance than any other sequence model .", "Even with a high order language model , applyingthe CCG supertag sequence model still seems to im prove performance .", "This means that even if we usea more powerful language model , the structural in formation contained in the supertags continues to be beneficial .", "4 . 6 Lexicalised Reordering vs . Supertags .", "In this experiment we investigate using a strongerreordering model to see how it compares to the con tribution that CCG supertag sequence models make .", "Moses implements the lexicalised reordering model described by Tillman 2004 , which learns whetherphrases prefer monotone , inverse or disjoint orienta tions with regard to adjacent phrases .", "We apply this reordering models to the following experiments .", "Model None Lex .", "Reord .", "sw , tw 23 . 97 24 . 72 sw , twc 24 . 42 24 . 78Table 5 .", "Dutch English models with and without a lexi calised reordering model . In Table 5 we can see that lexicalised reordering improves translation performance for both models .", "However , the improvement that was seen us ing CCG supertags without lexicalised reordering , almost disappears when using a stronger reorderingmodel .", "This suggests that CCG supertags ?", "contribution is similar to that of a reordering model .", "The lex icalised reordering model only learns the orientation of a phrase with relation to its adjacent phrase , so its influence is very limited in range .", "If it can replace CCG supertags , it suggests that supertags ?", "influence is also within a local range .", "4 . 7 CCG Supertags on Source .", "Sequence models over supertags improve the performance of phrase based machine translation .", "However , this is a limited way of leveraging the rich syn tactic information available in the CCG categories .", "We explore the potential of letting supertags direct translation by including them as a factor on the source .", "This is similar to syntax directed translation originally proposed for compiling Aho and Ullman , 1969 , and also used in machine translation Quirk et al . , 2005 ; Huang et al , 2006 .", "Information about thesource words ?", "syntactic function and subcategori sation can directly influence the hypotheses beingsearched in decoding .", "These experiments were per formed on the German to English translation task , in contrast to the Dutch to English results given in previous experiments .", "We use a model which combines more specificdependencies on source words and source CCG su pertags , with a more general model which only has dependancies on the source word , see Equation 4 . We explore two different ways of balancing the sta tistical evidence from these multiple sources .", "The first way to combine the general and specific sources of information is by considering features from bothmodels as part of one large log linear model .", "However , by including more and less informative features in one model , we may transfer too much ex planatory power to the more specific features .", "Toovercome this problem , Smith et al 2006 demon strated that using ensembles of separately trainedmodels and combining them in a logarithmic opin ion pool LOP leads to better parameter values .", "This approach was used as the second way in which 14 we combined our models .", "An ensemble of log linearmodels was combined using a multiplicative con stant ? which we train manually using held out data .", "t ? ? M ? m 1 ? mhm swc , tw ?", "N ?", "n 1 ? nhn sw , tw Typically , the two models would need to be normalised before being combined , but here the multi plicative constant fulfils this ro ? le by balancing theirseparate contributions .", "This is the first work suggesting the application of LOPs to decoding in ma chine translation .", "In the future more sophisticated translation models and ensembles of models willneed methods such as LOPs in order to balance sta tistical evidence from multiple sources .", "Model BLEU sw , tw 23 . 30 swc , tw 19 . 73 single 23 . 29 LOP 23 . 46 Table 6 .", "German English CCG supertags are used as a factor on the source .", "The simple models are combined in two ways either as a single log linear model or as a LOP of log linear models Table 6 shows that the simple , general model model sw , tw performs considerably better thanthe simple specific model , where there are multi ple dependencies on both words and CCG supertags model swc , tw .", "This is because there are words in the test sentence that have been seen before but not with the CCG supertag .", "Statistical evidence from multiple sources must be combined .", "The first wayto combine them is to join them in one single log linear model , which is trained over many features . This makes finding good weights difficult as the influence of the general model is greater , and its dif ficult for the more specific model to discover goodweights .", "The second method for combining the in formation is to use the weights from the separately trained simple models and then combine them in a LOP .", "Held out data is used to set the multiplicative constant needed to balance the contribution of the two models .", "We can see that this second approach is more successful and this suggests that it is importantto carefully consider the best ways of combining dif ferent sources of information when using ensembles of models .", "However , the results of this experiment are not very conclusive .", "There is no uncertainty inthe source sentence and the value of modelling it us ing CCG supertags is still to be demonstrated .", "The factored translation model allows for the inclusion of valuable sources of information in many dif ferent ways .", "We have shown that the syntacticallyrich CCG supertags do improve the translation pro cess and we investigate the best way of including them in the factored model .", "Using CCG supertagsover the target shows the most improvement , especially when using targeted manual evaluation .", "How ever , this effect seems to be largely due to improvedlocal reordering .", "Reordering improvements can per haps be more reliably made using better reordering models or larger , more powerful language models .", "A further consideration is that supertags will always be limited to the few languages for which there are treebanks .", "Syntactic information represents embedded structures which are naturally incorporated intogrammar based models .", "The ability of a flat struc tured model to leverage this information seems to be limited .", "CCG supertags ?", "ability to guide translation would be enhanced if the constraints encoded in the tags were to be enforced using combinatory operators .", "We thank Hieu Hoang for assistance with Moses , Ju lia Hockenmaier for access to CCGbank lexicons in German and English , and Stephen Clark and James Curran for providing the supertagger .", "This work was supported in part under the GALE program of theDefense Advanced Research Projects Agency , Con tract No .", "HR0011 06 C 0022 and in part under theEuroMatrix project funded by the European Com mission 6th Framework Programme ."], "summary_lines": ["CCG Supertags in Factored Statistical Machine Translation\n", "Combinatorial Categorial Grammar (CCG) supertags present phrase-based machine translation with an opportunity to access rich syntactic information at a word level.\n", "The challenge is incorporating this information into the translation process.\n", "Factored translation models allow the inclusion of supertags as a factor in the source or target language.\n", "We show that this results in an improvement in the quality of translation and that the value of syntactic supertags in flat structured phrase-based models is largely due to better local reorderings.\n", "we exploit factored phrase-based translation models to associate each word with a supertag, which contains most of the information needed to build a full parse.\n"]}
{"article_lines": ["Machine Translation Divergences A Formal Description and Proposed Solution Bonnie J . Dorr University of Maryland There are many cases in which the natural translation of one language into another esults in a very different form than that of the original .", "The existence of translation divergences i . e . , cross linguistic distinctions makes the straightforward transfer from source structures into target structures impractical .", "Many existing translation systems have mechanisms for handling diver gent structures but do not provide a general procedure that takes advantage of the systematic relation between lexical semantic structure and syntactic structure .", "This paper demonstrates that a systematic solution to the divergence problem can be derived from the formalization of two types of information 1 the linguistically grounded classes upon which lexical semantic divergences are based ; and 2 the techniques by which lexical semantic divergences are resolved .", "This formalization is advantageous in that it facilitates the design and implementation f the system , allows one to make an evaluation of the status of the system , and provides a basis for proving certain important properties about he system .", "Introduction There are many cases in which the natural translation of one language into another results in a very different form than that of the original .", "The existence of translation divergences i . e . , cross linguistic distinctions makes the straightforward transfer from source structures into target structures impractical .", "This paper demonstrates that a sys tematic solution to the divergence problem can be derived from the formalization of two types of information 1 the linguistically grounded classes upon which lexical semantic divergences are based ; and 2 the techniques by which lexical semantic divergences are resolved .", "An important result of this formalization is the provision of a framework for proving that the lexical semantic divergence classification proposed in the current approach covers all source language target language distinctions based on lexical semantic properties .", "Other types of divergences and mismatches are outside of the scope of this paper ; these include distinctions based on purely syntactic informa tion , idiomatic usage , aspectual knowledge , discourse knowledge , domain knowledge , or world knowledge Although other translation approaches have attempted to account for divergences , the main innovation of the current approach is that it provides a formalization of these divergences and the techniques by which they are resolved .", "This is advantageous from a computational point of view in that it facilitates the design and implementation of ?", "Department ofComputer Science , University of Maryland , A . V . Williams Building , College Park , MD 20742 , USA .", "1 The reader is referred to Dorr 1993a for a discussion of how syntactic divergences are handled .", "Aspectual divergences are treated by Dorr 1992a .", "The relatio . n of the current framework to other types of knowledge outside of lexical semantics i discussed by Dorr and Voss 1993b .", "1994 Association for Computational Linguistics Computational Linguistics Volume 20 , Number 4 1 Thematic divergence E I like Mary S Maria me gusta a mi Mary pleases me 2 Promotional divergence E John usually goes home 4 S Juan suele i ra casa John tends to go home 3 Demotional divergence E I like eating G Ich esse gem I eat likingly 4 Structural divergence E John entered the house 4 S Juan entr6 en la casa John entered in the house 5 Conflational divergence E I stabbed John S Yo le di pu aladas a Juan I gave knife wounds to John 6 Categorial divergence E I am hungry G Ich habe Hunger I have hunger 7 Lexical divergence E John broke into the room S Juan forz6 la entrada l cuarto Figure 1 John forced the entry to the room Examples of translation divergences with respect o English , Spanish , and German .", "the system the problem is clearly defined in terms of a small number of divergence categories , and the solution is systematically stated in terms of a uniform translation mapping and a handful of simple lexical semantic parameters .", "In addition , the for malization allows one to make an evaluation of the status of the system .", "For example , given the formal description of the interlingua nd target language root words , one is able to judge whether a particular target language s ntence fully covers the concept that underlies the corresponding source language s ntence .", "Finally , the formalization of the divergence types and the associated solution allows one to prove certain proper ties about the system .", "For example , one might want to determine whether the system is able to handle two or more simultaneous divergences that interact in some way .", "With the mechanism of the current approach , one is able to prove formally that such cases are handled in a uniform fashion .", "This paper will focus on the problem of lexical semantic divergences and will provide support for the view that it is possible to construct a finite cross linguistic classification of divergences and to implement a systematic mapping between the interlingual representation and the surface syntactic structure that accommodates all of the divergences in this classification .", "The types of divergences under consideration are those shown in Figure 1 .", "The first divergence type is thematic in 1 , the theme is realized as the verbal object Mary in English but as the subject Maria of the main verb in Spanish .", "The second divergence type , promotional , is one of two head switching divergence types in 2 , the modifier usually is realized as an adverbial phrase in English but as the main verb soler in Spanish .", "The third divergence type , demotional , is another type of head switching divergence in 3 , the word like is realized as a main verb in English but as an adverbial modifier gern in German .", "2The fourth 2 The distinction between promotional nd demotional divergences is not intuitively obvious at first glance .", "In both 2 and 3 , the translation mapping associates a main verb with an adverbial satellite , or vice versa i . e . , in 2 , the main verb soler is associated with the adverbial satellite usually , and in 3 the main verb like is associated with the adverbial satellite gern .", "The distinction between these two 598 Bonnie J . Dorr Machine Translation Divergences divergence type is structural in 4 , the verbal object is realized as a noun phrase the house in English and as a prepositional phrase en la casa in Spanish .", "The fifth divergence type is conflational .", "Conflation is the incorporation ofnecessary participants or arguments of a given action .", "In 5 , English uses the single word stab for the two Spanish words dar give and pu aladas knife wounds ; this is because the effect of the action i . e . , the knife wounds portion of the lexical token is conflated into the main verb in English .", "The sixth divergence type is categoriah in 6 , the predicate is adjectival hungry in English but nominal Hunger in German .", "Finally , the seventh divergence type is a lexical divergence in 7 , the event is lexically realized as the main verb break in English but as a different verb forzar literally force in Spanish .", "The next section discusses the divergence classification given above , comparing the current divergence categories with those of other researchers .", "Section 3 formally defines the terms used to classify divergences .", "Section 4 uses this terminology to formalize the divergence classification and to define the solution to the divergence problem in the context of detailed examples .", "Finally , Section 5 discusses certain issues of relevance to the divergence problem including the resolution of several recursively interacting divergence types .", "Classification of Machine Translation Divergences The divergence problem in machine translation has received increasingly greater at tention in recent literature see , for example , Barnett et al .", "1991a , 1991b ; Beaven 1992a , 1992b ; Dorr 1990a , 1990b ; Kameyama et al .", "1991 ; Kinoshita , Phillips , and Tsujii 1992 ; Lindop and Tsujii 1991 ; Tsujii and Fujita 1991 ; Whitelock 1992 ; related discussion can also be found in work by Melby 1986 and Nirenburg and Nirenburg 1988 .", "In par ticular , Barnett et al .", "1991a divide distinctions between the source language and the target language into two categories translation divergences , in which the same infor mation is conveyed in the source and target exts , but the structures of the sentences are different as in previous work by Dorr 1990a , 1990b ; and translation mismatches , in which the information that is conveyed is different in the source and target lan guages as described by Kameyama et al .", "3 Although translation mismatches are a major problem for translation systems that must be addressed , they are outside the scope of the model presented here .", "See Barnett et al .", "1991a , 1991b ; Carbonell and Tomita 1987 ; Meyer , Onyshkevych , and Carlson 1990 ; Nirenburg , Raskin , and Tucker 1987 ; Nirenburg and Goodman 1990 ; Nirenburg and Levin 1989 ; Wilks 1973 ; among others , for descriptions of interlingual machine translation approaches that take into account knowledge outside of the domain of lexical semantics .", "Although researchers have only recently begun to classify divergence types sys tematically , the notion of translation divergences i not a new one in the machine translation community .", "For example , a number of researchers working on the Euro tra project have sought o solve divergent source to target translations , although the divergences were named differently and were resolved by construction specific trans fer rules .", "For cogent descriptions of the Eurotra project , see , for example , Arnold and des Tombe 1987 ; Copeland et al .", "1991 ; and Johnson , King , and des Tombe 1985 .", "head switching cases will be made clearer in Section 4 . 3 .", "3 An example of the latter situation is the translation of the English word fish into Spanish the translation is pez if the fish is still in its natural state , but it is pescado if the fish has been caught and is suitable for food .", "It is now widely accepted that , in such a situation , the machine translation system must be able to derive the required information from discourse context and a model of the domain that is being discussed .", "599 Computational Linguistics Volume 20 , Number 4 Figure 2 Formal definition of lexical conceptual structure .", "A comprehensive survey of divergence xamples is presented by Lindop and Tsujii 1991 .", "The term used in this work is complex transfer , but it describes a class of problems inherent in machine translation itself , not just in the transfer or interlingual approaches .", "One of the claims made by Lindop and Tsujii 1991 is that the non Eurotra liter ature rarely goes into great detail when discussing how divergences are handled .", "An additional claim is that combinations of divergences and interaction effects between divergent and nondivergent translations are not described in the literature .", "This paper seeks to change this perceived state of affairs by providing a detailed description of a solution to all of the potentially interacting divergences shown in Figure 1 , not just a subset of them as would typically be found in the description of most translation systems .", "The framework assumed for the current approach makes use of a linguis tically grounded classification of divergence types that can be formally defined and systematically resolved .", "We now turn to a formal description of the terminology used to define the diver gence problem .", "Definitions This section formally defines the lexical semantic representation that serves as the interlingua of the system Definitions 1 3 .", "This representation , which is influenced primarily by Jackendoff 1983 , 1990 , has been described in detail elsewhere see , for example , Dorr 1992b , 1993a and thus will not be the focus of this paper .", "In addition to a formal description of the lexical semantic representation , definitions are provided for syntactic phrases Definition 4 and two translation mappings Definitions 5 and 6 .", "Definition 1 A lexical conceptual structure LCS is a modified version of the representation proposed by Jackendoff 1983 , 1990 that conforms to the following structural form T X X T W Wt , T Zq Ztl T Z , , Ztn T Q , QI T Q , , , Qm This corresponds to the tree like representation shown in Figure 2 , in which 1 X is the logical head ; 2 W is the logical subject ; 3 Z . . . Z are the logical arguments ; and 4 Q . . . Q m are the logical modifiers .", "These four positions are relevant o the mapping 600 Bonnie J . Dorr Machine Translation Divergences oo .", "1 I JOHN TOLo c HAPPILY ?", "Thing path Manner !", "I 1 Thing Location Figure 3 CLCS representation for John went happily to school .", "between the interlingual representation and the surface syntactic representation .", "In addition , T is the logical type Event , State , Path , Position , etc .", "corresponding to the primitive CAUSE , LET , GO , STAY , BE , etc .", "; Primitives are further categorized into fields e . g . , Possessional , Identificational , Temporal , Locational , etc . .", "4 Example 1 The LCS representation f John went happily to school is Event GOLoc Thing JOHN , Path TOLoc Position ATLoc Thing JOHN , Location SCHOOL M .", "HAPPILY This corresponds to the tree like representation shown in Figure 3 , in which 1 the logical head is GOLoc of type Event ; 2 the logical subject is JOHN of type Thing ; 3 the logical argument is TOcoc of type Path ; and 4 the logical modifier is HAPPILY of type Manner .", "Note that the logical argument is itself a LCS that contains a logical argument , SCHOOL of type Location , i . e . , LCSs are recursively defined .", "The LCS representation is used both in the lexicon and in the interlingual repre sentation .", "The former is identified as a root LCS RLCS and the latter is identified as a composed LCS CLCS Definition 2 A RLCS i . e . , a root LCS is an uninstantiated LCS that is associated with a word definition in the lexicon i . e . , a LCS with unfilled variable positions .", "Example 2 The RLCS associated with the word go from Example 1 is Event GOLoc Thing X , Path TOLoc Position ATLoc Thing X , Location Z 4 The validity of the primitives and their compositional properties i not discussed here .", "The LCS has been studied as the basis of a representation for multiple languages see , for example , Hale and Keyser 1986a , 1986b , 1989 ; Hale and Laughren 1983 ; Levin and Rappaport 1986 ; Zubizarreta 1982 , 1987 and is discussed in the context of machine translation by Dorr 1992b .", "601 Computational Linguistics Volume 20 , Number 4 x . . . , l Position I x 7 .", "Thing Location Figure 4 RLCS representation forgo .", "which corresponds to the tree like representation shown in Figure 4 .", "Definition 3 A CLCS i . e . , a composed LCS is an instantiated LCS that is the result of combining two or more RLCSs by means of unification roughly .", "This is the interlingua , or language independent , form that serves as the pivot between the source and target languages .", "Example 3 If we compose the RLCS for go in Figure 4 with the RLCSs for John ThingJOHN , school Location SCHOOL , and happily Manner HAPPILY , we get the CLCS corre sponding to John went happily to school shown in Figure 3 .", "Each content word in the lexicon is associated with a RLCS , whose variable positions may have certain restrictions .", "The CLCS is a structure that results from com bining the lexical items of a source language sentence into a single underlying pivot form by means of LCS composition .", "5 The notion of unification as used in Definition 3 differs from that of the standard unification frameworks see , for example , Shieber et al .", "1989 , 1990 ; Kaplan and Bresnan 1982 ; Kaplan et al .", "1989 ; Kay 1984 ; etc .", "in that it is not directly invertible .", "That is , the generation process operates on the CLCS in a unification like fashion that roughly mirrors the LCS composition process , but it is not a direct inverse of this process .", "The notion of unification used here also differs from others in that it is a more relaxed notion those words that are mapped in a relaxed way are associated with special exical information i . e . , the INT , EXT , PROMOTE , DEMOTE , , , CAT , and CONFLATED parameters , each of which will be formalized shortly .", "A fundamental component of the mapping between the interlingual representation and the surface syntactic representation is the syntactic phrase .", "Definition 4 A syntactic phrase is a maximal projection that conforms to the following structural form 5 This process is described in detail in Dorr 1992b .", "602 Bonnie J . Dorr Machine Translation Divergences Y MAX Q MAXj I . . . Q MAXk Y MAX Q MAXk I . . . Q . MAX m W MAX X MAX X Z MAX1 . . . Z MAX a Q MAX 1 . . . Q MAXi X Q MAXi 1 . . . Q MAXj F igure 5 Formal definition of syntactic phrase .", "Y MAX Q MAXj . . . Q MAXk Y MAX W MAX X MAX X Q MAX1 . . .", "Q MAXi X Q MAXi I . . . Q MAX i Z MAX1 . . .", "Z MAXn Q MAXk 1 . . .", "Q MAXm 6 This corresponds to the tree like representation shown in Figure 5 , in which 1 X is the syntactic head of category V , N , A , P , I , or C ; 2 W MAX is the external argument ; 3 Z MAX1 , . . . , Z MAXn are the internal arguments ; and 4 Q MAX1 .", "Q MAXm are the syntactic adjuncts .", "Example 4 The syntactic phrase corresponding to John went happily to school is C MAX I MAX N MAX John V MAX v went ADV happily P MAX to N MAX school This corresponds to the tree like representation shown in Figure 6 , in which 1 the syntactic head is v went ; 2 the external argument is N MAX John ; 3 the internal argument is P MAX a . . . ; and 4 the syntactic adjunct is ADV happily .", "Note that the internal argument constituent is itself a syntactic phrase that contains an internal argument , N MAX school , i . e . , syntactic phrases are recursively defined .", "In addition to the representations involved in the translation mapping , it is also possible to formalize the mapping itself .", "The current approach is to map between 6 These syntactic structures are based on the X framework of government binding theory see Chomsky 1981 , 1982 , 1986a , 1986b .", "For ease of illustration , the word order used in all formal definitions i head initial spec initial . e . , the setting for English .", "The syntactic operations that determine word order are completely independent from the lexical semantic operations that use these definitions .", "Thus , the formal definitions can be stated in terms of an arbitrary ordering of constituents , without loss of generality , as long as it is understood that the constituent order is independently determined .", "603 Computational Linguistics Volume 20 , Number 4 N MAX A John C MAX I I MAX V MAX V ADV P MAX I I went happily to N MAX school Figure 6 Syntactic phrase representation forJohn went happily to school .", "the LCS representation a d the surface syntactic form by means of two routines that are grounded in linguistic theory a generalized linking routine G ? T4 and a canonical syntactic realization CST4 .", "These routines are defined formally here Definition 5 The ? T4 systematically relates syntactic positions from Definition 1 and lexical semantic positions from Definition 4 as follows 1 .", "Qm Example 5 The correspondence b tween the LCS of Example 1 and the syntactic structure of Example 4 i . e . , for the sentence John went happily to school is 1 X GOLoc 4 X Iv went ; 2 W JOHN 4 W N MAX John ; 3 Z TOcoc 4 Z pp to .", "; and 4 Q HAPPILY Q ADV happily .", "Definition 6 The CST4 systematically relates a lexical semantic type T to a syntactic ategory CAT h , where t is a CLCS constituent related to the syntactic onstituent ? by the ? .", "Example 6 The LCS type Thing corresponds to the syntactic ategory N , which is ultimately projected up to a maximal level i . e . , N MAX .", "The full range of realization possibilities is given in Figure 7 .", "604 Bonnie J . Dorr Machine Translat ion Divergences LCS Type EVENT STATE THING PROPERTY PATH POSITION LOCATION TIME MANNER INTENSIFIER PURPOSE Syntactic Category V V N A P P ADV ADV ADV ADV DILl , Figure 7 CST4 mapp ing between LCS types and syntactic ategories .", "Now that we have fo rmal ly de f ined the representat ions and mapp ings used dur ing t rans la t ion , we wi l l tu rn to a c lass i f i ca t ion of d ivergences that is based on these de f in i t ions .", "The Divergence Problem Formal Classification and Solution In genera l , t rans la t ion d ivergences occur when there is an except ion e i ther to the ? T4 or to the CST4 or to both in one language , but not in the other .", "7 Th is p remise a l lows one to de f ine fo rmal ly a c lass i f i ca t ion of al l l ex ica l semant ic d ivergences that ar i se dur ing t rans la t ion i . e . , d ivergences based on proper t ies assoc ia ted w i th lex ica l 7 Most of the examples in this paper seem to suggest hat a divergence is defined in terms of a language to language phenomenon a divergence occurs when a sentence in language L1 translates into a sentence in L2 in a very different form i . e . , differently shaped parse trees or similarly shaped trees with different basic categories .", "This definition implies that a divergence may arise between two languages L1 and L2 , independent of the way the translation is done i . e . , direct , transfer , or interlingual .", "However , it is also possible to define a divergence from an interlingual point of view , i . e . , with respect to an underlying representation lexical conceptual structure that has been chosen to describe the source and target language sentences .", "From this point of view , a divergent mapping may apply even in cases in which the source and target language pairs do not exhibit any distinctions on the surface e . g . , the translation of the German sentence Hans kuflt Marie gern as the equivalent Dutch sentence Hans kust Marie graag , both of which literally translate to Hans kisses Mary likingly .", "In such cases , there are generally two occurrences of a language to interlingua divergence one from the surface structure and one to the surface structure .", "The terms language to language nd language to interlingua are taken from Dorr and Voss 1993a .", "At first glance , it might seem odd to introduce the notion of a language to interlingua divergence for cases that do not exhibit a language to language divergence .", "However , it is clearly the case that language to language divergences a special case of language to interlingua divergences do exist regardless of the translation approach adopted .", "Thus , we can view divergences more generally as a consequence of the internal mapping between the surface structure and the interlingual representation rather than as an external distinction that shows up on the surface .", "The result is that the interlingua ppears to have been simplified to the extent hat it accommodates constructions in one language without any special information more readily than it accommodates the corresponding construction i another language .", "However , as one reviewer points out , this is not an undesirable consequence , since the development of a suitable representation is where the interlingua builder has a choice and should choose the simplest representation format .", "The appropriate question to ask is whether an approach that addresses the divergence problem from a language to interlingua perspective is an improvement over an approach that addresses the problem strictly from a language to language point of view .", "This paper argues that the language to interlingua approach is the correct one given that the alternative would be to handle language to language divergences by constructing detailed source to target transfer ules for each lexical entry in the source and target language .", "Introducing the notion of language to interlingua divergence allows the translation mapping to be defined in terms of a representation that is general enough to carry over to several different language pairs .", "605 Computational Linguistics Volume 20 , Number 4 CLCS Syntax Y MAX Y x Q W X MAX IN X Z Figure 8 G ? T ?", "mapping between the CLCS and the syntactic structure .", "entries that are not based on purely syntactic information , idiomatic usage , aspectual knowledge , discourse knowledge , domain knowledge , or world knowledge .", "Before we define and resolve each divergence type , we will first make some revi sions to the representations used in Definitions 1 and 4 to simplify the presentation .", "The representation given in Definition 1 is revised so that Z is used to denote a logical argument from the set Z l . . . Z n and Q is used to denote a logical modifier from the set QI . . . Qm .", "The resulting representation is considerably simplified 8 T X X T W Wt , T Z Z , T Q Q Similarly , the representation given in Definition 4 is revised so that W is used to denote the external argument , Z is used to denote an internal argument from the set Z MAX1 . . . Z MAXn , and Q is used to denote a syntactic adjunct from the set Q MAXI . . . Q MAXm .", "The resulting representation has the following simplified form 9 Y MAX Y MAX W IX MAX X Z Q 8 With these simplifications , the G ? T4 can be conceptualized asthe following set of relations 10 Simplified G ? T4 1 .", "Q Q Figure 8 shows the simplified ? T4 in terms of tree like representations .", "9 We are now prepared to define and resolve the translation divergences of Fig ure 1 on the basis of the simplified formalization presented in 8 10 above .", "The 8 For the purposes of this discussion , we will retain the convention that syntactic adjuncts occur on the right at the maximal level .", "Note that this is not always the case the setting of an adjunction parameter described by Dorr 1993b determines the side and level at which a particular adjunct will occur .", "9 For ease of illustration , this diagram omits the type specification .", "There is no loss of generality , since the G ? R mapping does not make use of this specification .", "We will retain this convention throughout the rest of this paper .", "606 Bonnie J . Dorr Machine Translation Divergences a Thematic Divergence CLCS Syntax Y MAX I Y MAX Q I X MAX I X W b Promotional Divergence CLCS Syntax Y MAX i X MAX !", "Q X c Demofional Divergence CLCS Syntax I Y MAX X X MAX I Z d Structural Divergence CLCS Y M 0C Y MAX O X MAX IN X R I Z e Conflational Divergence CLCS Syntax I X MAX I x Q Figure 9 Translation mappings for cases in which G ? default positions are overridden .", "solution to the divergence problem relies solely on three types of information the G ? T4 ; the CST4 ; and a small set of parametric mechanisms .", "The G ? T4 and C T4 are intended to be language independent , whereas the parameters are intended to encode language specific information about lexical items .", "Because the interlingual representa tion preserves relevant lexical semantic relations , these three types of information are all that are required for providing a systematic solution to the divergence types shown in Figure 1 .", "In particular , the solution given here eliminates the need for transfer rules and relies instead on parameterized mappings that are defined and applied uniformly across all languages .", "Seven parameters are used to invoke exceptions to the ? T4 and C T4 functions in the context of translation divergences INT , EXT , PROMOTE , DE MOTE , , , CAT , and CONFLATED .", "We will now present a formal description of each divergence type and its associated parameter .", "4 . 1 Thematic Divergence The first divergence type to be formalized is the one for thematic divergence , i . e . , the repositioning ofarguments with respect to a given head .", "This type of divergence arises 607 Computational Linguistics Volume 20 , Number 4 in cases in which the ? T4 invokes the following sets of relations in place of steps 2 and 3 of 10 11 2 .", "W Z 3 Z l W Figure 9a shows the revised mapping .", "Thematic divergence arises only in cases in which there is a logical subject .", "An example of thematic divergence is the reversal of the subject with an object , as in the thematic divergence xample given earlier in 1 .", "The syntactic structures and corresponding CLCS are shown here 12 C MAX I MAX IN MAX I V MAX V like N MAX Mary State BEIdent Thing II , Position aTIdent Thing I , Thing MARY , Manner LIKINGLY C MAX I MAX IN MAX Maria V MAX IV me gusta l 1 ?", "Here the object Mary has reversed places with the subject I in the Spanish translation .", "The result is that the object Mary turns into the subject Maria , and the subject I turns into the object me .", "This argument reversal is resolved by means of the INT and EXT parameters , which force the ? T4 mapping to be overridden with respect o the positioning of the logical subject and logical argument in Spanish .", "The lexical entries for like and gustar illustrate the difference in the use of these parameters 13 i ii Lexical entry for like State BEIdent Thing W , Position aTIdent Thing Wl , Thing Z , Manner LIKINGLYI Lexical entry for gustar State BEldent Thing INT W , Position aTIdent Thing W , Thing EXT Z , Manner LIKINGLY Because the English entry does not include these parameters , the translation relies on the default argument positionings imposed by the ? T4 .", "By contrast , the INT EXT markers specified in the Spanish entry force the internal and external arguments to swap places in the syntactic structure .", "10 For the purposes of this discussion , the Spanish sentence is given in its uninverted form .", "There are other ways of realizing this sentence .", "In particular , anative speaker of Spanish will frequently invert the subject o post verbal position C MAX I MAX ei V MAX IV MAX V me gusta IN MAX Mafia i .", "However , this does not affect he internal external reversal scheme described here , since inversion is a syntactic operation that takes place independently of the process that handles thematic divergences .", "608 Bonnie J . Dorr Machine Translation Divergences The general solution to thematic divergence is diagrammed as follows 14 RLCS 1 T X , X T W W , r z , Z T Q Q RLCS 2 r x , X r w INT W , T Z EXT Z T Q QI Trans la t ion Y MAX Y MAX W X MAX X Z Q T X X T W W , T Z Z T Q Q Y MAX Y MAX Z X MAX X Wll QI This assumes that there is only one external argument and zero or more internal arguments .", "If the situation arises in which more than one variable is associated with the EXT markers , it is assumed that there is an error in the word definition .", "N Note that the INT and EXT markers how up only in the RLCS .", "The CLCS does not include any such markers , since it is intended to be a language independent r presentation for the source and target language sentences .", "Thematic divergence is one of three types of possible positioning variations that force the G ? T4 to be overridden .", "Two additional positioning variations are promo tional and demotional divergences , which will be defined in the next two sections .", "Whereas thematic divergence involves a repositioning of two satellites relative to a head , promotional and demotional divergences involve a repositioning of the head itself 2 We will see in Section 5 . 1 that these three divergences account for the entire range of repositioning possibilities .", "4 . 2 Promotional Divergence Promotional divergence is characterized by the promotion placement higher up of a logical modifier into a main verb position or vice versa , as shown in Figure 9b .", "In such a situation , the logical modifier is associated with the syntactic head position , and the logical head is then associated with an internal argument position .", "Thus , promotional divergence overrides the G ? T4 , invoking the following sets of relations in place of steps 1 and 4 of 10 15 1 .", "Z 13 4 . Q ? X Figure 9b shows the revised mapping .", "11 The parameters associated with the RLCS are assumed to be correctly specified for the purposes of this formal description .", "However , in practice , there might be errors in the lexical entries , since they are constructed by hand in the current implementation .", "Eventually , the intent is to automate the process of lexical entry construction so that these errors can be avoided .", "12 The notions of demotion and promotion are not the same as the notions of demotion and advancement in the theory of relational grammar see Perlmutter 1983 .", "Dorr 1993b , pp .", "269 274 argues that , although the relational representation might be a convenient tool for illustrating the promotion and demotion operations as used in the current approach , this representation is not an appropriate vehicle for interlingual translation for a number of reasons .", "13 This relation does not mean that X replaces Z if there is a Z , but that X retains the same structural relation with Z i . e . , Z remains an internal argument of X .", "To simplify the current description , Z is not shown in the syntactic structure of Figure 9b .", "609 Computational Linguistics Volume 20 , Number 4 An example of promotional divergence is the case given earlier in 2 .", "The syntactic structures and corresponding CLCS are shown here 16 C MAX I MAX N MAX John V MAX IV usually Iv goes IN MAX home l Event OLoc Thing JOHN , Path TOcoc Position ATcoc Thing JOHN , Location HOUSE , Manner HABITUALLY C MAX I MAX IN MAX Juan V MAX Iv suele V MAX V ir P MAX a casa Here the main verb go is modified by an adverbial adjunct usually , but in Spanish , usually has been placed into a higher position as the main verb soler , and the going home event has been realized as the internal argument of this verb .", "Promotional divergence is resolved by the PROMOTE parameter , which forces the ? T4 mapping to be overridden with respect to the positioning of the logical head and the logical modifier .", "The lexical entries for usually and soler illustrate the difference in the use of this parameter 17 i ii Lexical entry for usually Manner HABITUALLY Lexical entry for soler Manner PROMOTE HABITUALLY Because the English entry does not use this parameter , the translation relies on the de fault argument positionings imposed by the G ? T4 .", "By contrast , he PROMOTE marker specified in the Spanish entry forces the head and adjunct o swap places in the syn tactic structure .", "The general solution to promotional divergence is diagrammed as follows 18 RLCS 1 T Q Q RLCS 2 T Q PROMOTE Q Translation Y MAX Y MAX W X MAX X Z Q T X X T W W , T Z Z T Q Q Y MAX Y MAX W X MAX Q . . . X Z 4 . 3 Demotional Divergence Demotional divergence is characterized by the demotion placement lower down of a logical head into an internal argument position or vice versa , as shown in Figure 9c .", "In such a situation , the logical head is associated with the syntactic adjunct position , and the logical argument is then associated with a syntactic head position .", "Thus , 610 Bonnie J . Dorr Machine Translation Divergences demotional divergence overrides the g ? T4 , invoking the following sets of relations in place of steps 1 and 3 of 10 19 1 .", "Z 4 X Figure 9 c shows the revised mapping .", "An example of demotional divergence is the case given earlier in 3 .", "The syntactic structures and corresponding CLCS are shown here is 20 C MAX I MAX IN MAX I i IV MAX IV l ike C MAX PROi to eat State BEcirc Thing I , Position ATcirc Thing I , Event EAT Thing I , Thing FOOD Manner LIKINGLY C MAX I MAX IN MAX Ich IV MAX IV ADV gern Iv esse 16 Here the main verb like takes the to eat event as an internal argument ; but in German , like has been placed into a lower position as the adjunct gern , and the eat event has been realized as the main verb .", "The distinction between promotional and demotional divergences may not be in tuitively obvious at first glance .", "In both cases , the translation mapping appears to associate a main verb with an adverbial satellite , or vice versa .", "However , the dis tinction between these two head switching cases becomes more apparent when we consider the status of the participating lexical tokens more carefully .", "In the case of soler usually , the main verb soler is , in some sense , the token that triggers the head switching operation its presence forces the adverbial satellite usually to appear in En glish , even if we were to substitute some other event for ir in Spanish e . g . , correr a la tienda , leer un libro , etc . .", "By contrast , in the case of like gern , the triggering element is not the main verb like , since we are able to use like in other contexts that do not require gern e . g . , I like the car Mir gefdllt der Wagen ; instead , the triggering element is the adverbial satellite gern its presence forces the verb like to appear in English even if we were to substitute some other event in place of essen in German e . g . , zum Geschdft laufen , das Buch lesen , etc . .", "We will return to this point in Section 5 . 2 .", "Demotional divergence is resolved by the DEMOTE parameter , which forces the ? T4 mapping to be overridden with respect o the positioning of the logical head and the logical argument .", "The lexical entries for like and gern illustrate the difference in the use of this parameter 17 14 This relation does not mean that X replaces Q if there is a Q , but that X retains the same structural relation with Q i . e . , Q remains a syntactic adjunct of X .", "To simplify the current description , Q is not shown in the syntactic structure of Figure 9c .", "15 The default object being eaten is FOOD , although this argument does not appear on the surface for the current example .", "16 The German syntactic structure is shown here in the uninverted base form .", "In the German surface structure , the verb is moved up into verb second position and the subject is topicalized C MAX IN MAX IchJ Iv esse j I MAX IN MAX t l IV MAX Iv ADV gem Iv t .", "17 Both definitions of like in 21 use the circumstantial field , which means that the Y argument must be an Event e . g . , like ta eat rather than a Thing e . g . , like Mary .", "Thus , the definitions for like and gern are slightly different from the definitions of like given earlier in 13 i . e . , these are additional lexical entries for like .", "611 Computational Linguistics Volume 20 , Number 4 21 i ii Lexical entry for like State BEcirc Thing W , position ATcirc Thing W , Event Z , Manner LIKINGLY Lexical entry for gem State BECirc Thing W , Position ATCirc Thing W , Event DEMOTE Z , Manner LIKINGLY Because the English entry does not use this parameter , the translation relies on the default argument positionings imposed by the ? T4 .", "By contrast , he DEMOTE marker specified in the German entry forces the head and internal argument to swap places in the syntactic structure .", "The general solution to demotional divergence is diagrammed as follows 22 RLCS 1 v x X T W W , T Z Z T Q Q RLCS 2 T X X T W W , T Z DEMOTE Z T Q Q Translation Y MAX Y MAX W IX MAX X Z Q v x X T W W , T Z Z T Q Q l Y MAX Y MAX W IX MAX Z . . . X QI 4 . 4 Structural Divergence Structural divergence differs from the last three divergence types in that it does not alter the positions used in the ? T4 mapping , but it changes the nature of the relation between the different positions i . e . , the 4 correspondence .", "Figure 9d characterizes the alteration that takes place .", "Note that the mapping of Z to the corresponding internal argument position is altered so that it is positioned under the constituent that corresponds to W . An example of structural divergence is the case given earlier in 4 .", "The syntactic structures and corresponding CLCS are shown here 23 C MAX I MAX IN MAX John IV MAX IV entered N MAX the house Event GOLoc Thing JOHN , Path TOcoc Position INcoc Thing JOHN , Location HOUSE C MAX I MAX IN MAX Juan V MAX IV entr6 P MAX en N MAX la casa Here the verbal object is realized as a noun phrase the house in English and as a prepositional phrase en la casa in Spanish .", "Structural divergence is resolved by means of the marker , which forces logical constituents obe realized compositionally atdifferent levels .", "In particular , the serves as a pointer to a RLCS position that must be combined with another RLCS in order to arrive at a portion of a CLCS .", "The lexical entries for enter and entrar illustrate the difference in the use of this parameter 612 Bonnie J . Dorr Machine Translation Divergences 24 i ii Lexical entry for enter Event GOLoc Thing W , Path TOLoc Position INLoc Thing W , Location Z Lexical entry for entrar Event GOLoc Thing W , Path TOLoc Position INcoc Thing W , Location Z Because the English entry contains a marker in the Location Z position , this constituent is realized on the surface as the object i . e . , the house of the main verb .", "By contrast , the ?", "marker is associated with a higher position Path TOcoc . . . in the Spanish entry , thus forcing this constituent to have a more complex realization i . e . , en la casa in the syntactic structure .", "The general solution to structural divergence is diagrammed as follows 25 RLCS 1 T X , X T W W , T R R T Z Z r Q , Q RLCS 2 r x , X r w , W , r R R r z Z T Q Q Translation Y MAX Y MAX r x , X T W Y MAX Y MAX W X MAX X Z Q W , T Z Z T Q Q W X MAX X . . . R Z Q Note that the logical argument R is associated with a marker in the RLCS of the target language , but not in the RLCS of the source language .", "This forces the target language syntactic structure to realize a phrase R that dominates Z ; in contrast , no such dominating phrase occurs in the source language structure .", "4 . 5 Conflational Divergence Conflational divergence is another case in which the correspondence is changed .", "In particular , conflational divergence is characterized by the suppression of a CLCS constituent or the inverse of this process .", "The constituent generally occurs in logical argument or logical modifier position ; thus , the 4 correspondence of either step 3 or step 4 of the ? T4 is changed , depending on which position is conflated .", "Figure 9e characterizes the alteration that takes place .", "Note that the Z position in the CLCS does not have a corresponding realization in the syntax .", "An example of conflational divergence is the case given earlier in 5 .", "The syntactic structures and corresponding LCS are shown here 26 C MAX I MAX IN MAX I V MAX V stabbed N MAX John Event CAUSE Thing I , Event GOposs Thing KNIFE WOUND , Path TOWARDposs Position ATposs Thing KNIFE WOUND , Thing JOHN C MAX I MAX IN MAX Yo V MAX v le di N MAX pufialadas P MAX a Juan 613 Computational Linguistics Volume 20 , Number 4 Here , English uses the single word stab for the two Spanish words dar give and pu aladas knife wounds ; this is because the effect of the action i . e . , the knife wound portion of the lexical token is incorporated into the main verb in English .", "Conflational divergence is resolved by means of the CONFLATED marker , which suppresses the realization of the filler of a particular position .", "The lexical entries for stab and dar illustrate the difference in the use of this parameter is 27 i ii Lexical entry for stab Event CAUSE Thing W , Event GOposs Thing CONFLATED KNIFE WOUND , Path TOWARDposs Position ATposs Thing KNIFE WOUND , Thing Z l Lexical entry for dar Event CAUSE Thing W , Event GOposs Thing Y , Path TOWARDposs Position ATposs Thing Y , Thing ZI Because the English entry contains a CONFLATED marker in the logical position cor responding to KNIFE WOUND , this constituent is suppressed in the syntactic struc ture .", "By contrast , this marker does not appear in the corresponding position in Span ish , thus forcing this constituent to be overtly realized i . e . , puflaladas in the svntactic structure .", "The general solution to conflational divergence is diagrammed as follows 28 RLCS 1 x , X T W W , T Z Z T Q Q RLCS 2 T X X z w W , T Z CONFLATED Z T Q Q Trans lat ion Y MAX Y MAX W X MAX X Z Q T X X T W W , T Z Z r ; Qq Q Y MAX Y MAX W X MAX XI QI Note that the logical argument Z is associated with a CONFLATED marker in the RLCS of the target language , but not in the RLCS of the source language .", "This forces the target language syntactic structure to suppress the realization of this constituent .", "18 Note that the CONFLATED marker appears to be in complementary distribution with the ?", "In fact , one might consider the use of the CONFLATED marker to be unnecessary , since its presence could be implied by the absence of the marker .", "However , the CONFLATED marker plays an important role in the lexical semantic representation it specifies that the constant term e . g . , the KNIFE WOUND of the stab RLCS must obligatorily fill the position and , moreover , that this constant must be a legal LCS primitive of the system .", "In addition , there is an inherent asymmetry between the CONFLATED marker and the marker whereas the former always occurs in a leaf node position , the latter may occur in any position in the RLCS .", "Because the notion of conflation is not meaningful in non leaf node positions , it would be unreasonable to make the assumption that every non leaf position without the marker is conflated .", "The CONFLATED marker is used to identify truly conflated positions , not just those positions without the marker .", "614 Bonnie J . Dorr Machine Translation Divergences 4 . 6 Categorial Divergence Unlike the previous five divergence types , categorial divergence affects the operation of the C , not the ? T4 .", "It is characterized by a situation in which CAT 0 is forced to have a different value than would normally be assigned to T by means of the mapping specified in Figure 7 .", "Thus , categorial divergence is formally described as follows a lexical semantic type T is related to a syntactic category CAT , where CAT CST4 T .", "In such a case , CAT must be specified through lexical parameterization .", "An example of categorial divergence is the case given earlier in 6 .", "The syntactic structures and corresponding CLCS are shown here 29 C MAX I MAX N MAX I V MAX V am A MAX hungry State BEIdent Thing I , Position aTIdent Thing I , Property HUNGRY C MAX I MAX N MAX Ich V MAX IN MAX Hunger v habe 19 Here , the predicate is adjectival hungry in English but nominal Hunger in German .", "Categorial divergence is resolved by means of the CAT parameter .", "The lexical entries for be and haben illustrate the difference in the use of this parameter 2 ?", "30 i ii Lexical entry for be State BEIdent Thing W , Position aTIdent Thing W , Property Y Lexical entry for haben State BEIdent Thing W , Position aTIdent Thing W , Property CAT N Y Because the English entry does not contain a CAT marker in the position correspond ing to Property Y , this constituent is realized in the syntactic structure as C Property Adjective i . e . , hungry .", "By contrast , the CAT N marker specified in the German entry forces the default C T4 category to be overridden and the Property argument is realized as a noun form i . e . , Hunger .", "19 The German structure shown here is the base form of the following surface syntactic tree C MAX IN MAX Ich i C Iv babe j I MAX N MAX tli V MAX N MAX Hunger v t jI I e This form is derived by the syntactic processor after lexical semantic processing is complete see Dorr 1993a , for details .", "20 As might be expected , there are two lexical entries for the verb haben , only one of which is listed here .", "The one not shown here corresponds to the possessional sense of haben , i . e . , the meaning corresponding to the word have in English .", "This entry does not contain a CAT marker .", "615 Computational Linguistics Volume 20 , Number 4 The general solution to categorial divergence is diagrammed as follows 31 RLCS 1 T X , X T W W , IT Z , Z T Q Q RLCS 2 It x , X 7 w W , IT Z , CAT 6 Z Q , Q Translation Y MAX Y MAX W IX MAX X Z Q T X X T W W , T Z Z T Q Q I Y MAX Y MAX W X MAX X a QI where CAT Z MAX .", "4 . 7 Lexical Divergence Lexical divergence arises only in the context of other divergence types .", "21 This is be cause the choice of lexical items in any language relies crucially on the realization and composition properties of those lexical items .", "Because the six preceding divergences potentially alter these properties , lexical divergence is viewed as a side effect of other divergences .", "Thus , the formalization thereof is considered to be some combination of those given above .", "Unlike the first six divergence types , lexical divergence is solved during the pro cess of lexical selection .", "22Thus , there is no specific override marker that is used for this type of divergence .", "For example , in the lexical divergence 7 , a conflational di vergence forces the occurrence of a lexical divergence .", "The syntactic structures and corresponding CLCS for this example are shown here 32 C MAX I MAX N MAX John IV MAX IV broke P MAX into IN MAX the room Event CAUSE Thing JOHN , Event GOLoc Thing JOHN , Path TOLoc Position INcoc Thing JOHN , Location ROOMI I Manner FORCEFULLY C MAX I MAX N MAX Juan IV MAX IV forz6 IN MAX la entrada P MAX al cuarto Because the word particle pair break into subsumes two concepts forceful spatial mo tion and entry to a location , it is crucial that the word forzar literally , force be selected in conjunction with entrada literally , entry for the underlying break into concept .", "21 As noted by a reviewer , this is not strictly true , since there are many cases in which a source language word maps to more than one target language word without he simultaneous occurrence of another divergence type .", "An example of such a case is the English word eat , which maps to essen for humans or fressen for animals .", "Such cases are considered to be outside of the classes of lexical semantic divergences considered here see Footnote 3 .", "However , a simple approach to resolving such cases would be to use featural restrictions during syntactic processing .", "22 The solution to lexical divergence is trivial for transfer machine translation systems , ince transfer entries map source language words directly to their target language equivalents .", "In general , exical selection is not seen as a problem in these systems .", "616 Bonnie J . Dorr Machine Translation Divergences There is also a structural divergence in this example , since the prepositional phrase into the room must be translated into a noun phrase entrada al cuarto .", "This divergence compounds the lexical divergence problem , since it is necessary to choose the target language word a in the absence of a source language counterpart .", "Lexical divergence also shows up in three previously presented examples , 12 , 29 , and 26 , owing to the presence of thematic , categorial , and conflational diver gences , respectively in 12 the word like is chosen for the word gustar literally , to please ; in 29 the word haben literally , to have is chosen for the word be ; and in 26 the word dar literally , to give is chosen for the word stab .", "Discussion This section discusses certain issues of relevance to the formal classification and reso lution of translation divergences .", "In particular , we will discuss 1 the limits imposed on the range of repositioning possibilities ; 2 the justification for distinguishing be tween promotional nd demotional divergences ; 3 the notion of full coverage in the context of lexical selection ; and 4 the resolution of interacting divergence types .", "5 . 1 Limits on Repositioning Divergences In Section 4 . 1 we made the claim that the thematic , promotional , and demotional divergences account for the entire range of repositioning possibilities .", "We will now explore the validity of this claim .", "There are two potential types of syntactic relations that exist between a head and a satellite the first is complementation i . e . , involving the internal argument , and the second is adjunction .", "23 Given these two types of relations , there are only a small number of ways syntactic entities may be repositioned .", "The three CLCS positions that are involved in these relations are X , Z , and Q .", "If we compute the repositionings combinatorically , there are 33 27 configurations i . e . , X , Z , and Q would map into any of three positions .", "However , we can eliminate 15 of these since a CLCS must contain exactly one head , thus leaving only 12 possible configurations .", "One of these corresponds to the default G ? mapping i . e . , the logical head , logical argument , and logical modifier map into canonical positions .", "The remaining 11 configurations can be factored into three cases as follows 1 .", "X 4 X 1 . 1 Q Z ; Z 1 4 Z .", "1 . 2 Z Q ; Q 4 Q .", "1 . 3 Q Z ; Z Q .", "X 2 . 1 X 4 Z ; Z ? ?", "2 . 2 X Z ; Z Q .", "23 We have left out the possibility of an external argument as a participant in the head satellite r lation .", "Of course , the external argument is a satellite with respect to the head , but it turns out that the external argument , which corresponds to the logical subject in the CLCS , has a special status and does not have the same repositioning potential that internal arguments and syntactic adjuncts have .", "In particular , the external argument has the unique property that it never participates as the incorporated argument of a conflational verb .", "Hale and Keyser 1989 provide evidence that this property holds across all languages .", "Thus , we take the external argument to have a special status universally that exempts it from participating in divergences other than thematic divergence .", "617 Computational Linguistics Volume 20 , Number 4 a Case 1 . 1 CLCS X Y MAX Y MAX 71 X MAX IN x Q b Case 1 . 2 CLCS Syntax Y MAX I Y MAX Z I X MAX I x e Case 2 . 3 CLCS Syntax IN Y MAX X I Z , X MAX t ?", "Q z d Case 3 . 2 CLCS Syntax Y MAX X MAX IN Z X Q Figure 10 Illegal translation mappings for natural language .", "2 . 3 X Q ; Z 4 Z .", "Z X 3 . 1 X Z ; Q Z .", "3 . 2 x Z ; Q Q .", "3 . 3 X Q ; Q Z .", "3 . 4 X Q ; Q Q .", "We will discuss in detail how four of these cases , i . e . , the ones characterized in Fig ure 10 , are ruled out .", "Of the remaining , 2 . 1 and 3 . 4 correspond to the definitions of promotional and demotional divergences illustrated in Figures 9b and 9c , respectively ; cases 1 . 3 , 2 . 2 , 2 . 4 , 3 . 1 , and 3 . 3 are ruled out for the same reasons that cases 1 . 1 and 1 . 2 are ruled out as we will see shortly , namely , that an internal argument Z can never be associated with a logical modifier Q and that a syntactic adjunct Q can never be associated with a logical argument Z .", "It cannot be the case that a logical modifier maps to an internal argument position case 1 . 1 because a logical modifier is an optional participant of a particular action , i . e . , it need not be governed by the lexical item that it modifies .", "Internal argument positions are reserved for cases in which a government relation must hold ; thus , logical modifiers must necessarily be mapped into syntactic adjunct positions .", "Similarly , it also cannot be the case that a logical argument maps to a syntactic adjunct position case 1 . 2 .", "A logical argument is a necessary participant of a particular action , and as such , it must be governed by the lexical item that selects it .", "By contrast , adjunct positions are reserved for optional modifying participants that do not need to be governed by the lexical item that they are modifying ; thus , logical arguments must necessarily be mapped into internal argument positions .", "618 Bonnie J . Dorr Machine Translation Divergences Another case that is eliminated is the renaming of a logical head as a syntactic adjunct whose head corresponds to a logical modifier case 2 . 3 .", "The idea is simply that modification is a one way relation .", "If a logical head has a modifier , the head cannot become an adjunct of that modifier because the modifying relation would be reversed i . e . , the logical head would modify the syntactic head rather than the other way around .", "In contrast , a logical head of a CLCS can be mapped to an internal argument position in cases in which a logical modifier is mapped to a syntactic head i . e . , the case of promotional divergence presented earlier , since there is no violation of the one way relation .", "A similar argument is used to eliminate the case in which a logical head is mapped to an internal argument whose head corresponds toa logical argument case 3 . 2 .", "The idea is that heads and arguments participate in a one way relation .", "If a logical head has an argument , he head cannot become an internal argument of that argument because the head argument relation would be reversed i . e . , the logical head would be an argument of the syntactic head rather than the other way around .", "In contrast , he logical head of a CLCS can be mapped to an adjunct i . e . , modifier position in cases in which a logical argument is mapped to a syntactic head i . e . , the case of demotional divergence presented earlier , since these is no violation of the one way relation .", "The argument for the elimination of the last two cases could be viewed as an appeal to a constraint that is analogous to the 0 criterion in syntax .", "Essentially , this constraint states that all arguments and modifiers must be licensed see Chomsky 1986a ; Abney 1989 in order to appear either in the syntactic structure or in the conceptual structure .", "In the context of conceptual structure , a logical modifier may license the realization of a logical head in an internal argument position , but not in an adjunct position , since the modifier elation is already satisfied by virtue of the relation between the head and the modifier .", "Similarly , a logical argument may license the realization of a logical head in a syntactic adjunct position , but not in an internal argument position , since the head argument relation is already satisfied by virtue of the relation between the head and the argument .", "Having eliminated the meaningless possibilities , we are left with the promotional nd demotional cases presented above .", "5 . 2 Promotional versus Demotional Divergences We will now provide justification for the earlier claim that promotional and demo tional divergence should be classified ifferently , even though they exhibit some of the same properties .", "It might be argued that these divergences are essentially the same , since both cases involve an association of a main verb with an adverbial satellite or vice versa .", "In the examples given earlier , the promotional divergence r ferred to a map ping between the adverbial usually and the main verb soler and demotional divergence referred to a mapping between the adverbial gern and the main verb like .", "However , as mentioned in Section 4 . 3 , these are taken to be in distinct classes the difference between these two cases is determined by the triggering element i . e . , promotion is triggered by a main verb such as soler , whereas demotion is triggered by an adverb such as gern .", "Another factor that distinguishes between promotional and demotional diver gences is the fact that verbs such as like and verbs such as soler do not have parallel syntactic distributions , nor do they have analogous logical interpretations .", "The verb like may take a sentential complement that has its own event structure as in I like to eat , or it may take a nominal complement without an event structure as in I like the car .", "In either case , the verb like generally means the same thing i . e . , it describes a state in which an event or a thing is somehow desirable to that person .", "By contrast , the verb soler is a modal verb that contributes an aspectual component of meaning 619 Computational Linguistics Volume 20 , Number 4 that crucially relies on a verbal complement with an event structure ; in a sense , soler is analogous to the modal must in English in that it cannot be used in isolation , but requires the presence of a verbal complement in order for it to be interpretable .", "In such a configuration , the modal soler allows the event to be interpreted as being habitual in nature .", "Given these distinctions , it would not be appropriate to consider the head switch ing mapping to be the same for the soler usually and like gern cases .", "Not only do they have different triggering elements i . e . , the main verb in the former and the adverb in the latter , but they do not have identical syntactic distributions and their logical in terpretations are not analogous .", "Thus , they are taken to be two independent mappings with entirely different syntactic and lexical semantic ramifications .", "The handling of promotional and demotional divergences i a topic that has re ceived recent attention , although it has been labeled differently , depending on how it is manifested .", "An example of such a case is the way construction .", "This phenomenon has been studied by Jackendoff 1990 in his extended version of the original LCS framework 33 Bill belched his way out of the restaurant In such cases , Jackendoff claims that belching is subordinated toa higher predicate like GO , in effect demoting the meaning of the lexical verb to a subordinate accompani ment or means modifier Jackendoff 1990 , p . 214 .", "This characterization is essentially equivalent to that of the soler usually example i . e . , promotional divergence given earlier .", "24 5 . 3 Lexical Selection Full Coverage Constraint Because of the compositional nature of the LCS representation , the current framework automatically imposes a full coverage constraint during the lexical selection process .", "Formally , this constraint is defined as follows 34 Full coverage constraint A RLCS R matches a CLCS C if and only if R fully covers C . where R fully covers C under the following conditions 35 A RLCS R fully covers a CLCS C if and only if a there is no portion of R that does not match C ; 24 Jackendoffs approach to handling the way construction has been criticized by Marantz 1992 for its use of arbitrary exceptions to the usual mappings .", "Marantz takes issue with the characterization f such cases as an idiosyncratic relation between syntax and semantics and proposes , instead , that the conceptual structure looks different from what Jackendoff envisions .", "Whichever of these proposals i correct , neither Jackendoff nor Marantz considers their proposals in the context of interlingual machine translation .", "If the exceptional mappings are indeed arbitrary , then one needs to explain how this affects the handling of different languages .", "Moreover , neither Jackendoff nor Marantz mentions the possibility that the number of exceptional mappings might not be arbitrarily large , but that there might be a fixed number of exceptions , delineated in such a way that only a handful need to be considered at any time for any given language .", "This is why the formalization described in this paper is a valuable resource it provides a means for proving that only certain types of exceptions are allowed and that the number of such exceptions i actually quite small .", "Finally , neither Jackendoff nor Marantz considers keeping Jackendoffs version of the LCS intact and using a single parameterized mapping along the lines proposed in the current framework .", "620 Bonnie J . Dorr Machine Translation Divergences b either R completely matches C i . e . , there is no portion of C that does not match R or R matches all of C except some portion C i . e . , a subcomponent of C that is fully covered by some other RLCS R .", "In cases in which more than one lexical entry matches a current concept , his constraint is used to determine which possibil"], "summary_lines": ["Machine Translation Divergences: A Formal Description And Proposed Solution\n", "There are many cases in which the natural translation of one language into another results in a very different form than that of the original.\n", "The existence of translation divergences (i.e., crosslinguistic distinctions) makes the straightforward transfer from source structures into target structures impractical.\n", "Many existing translation systems have mechanisms for handling divergent structures but do not provide a general procedure that takes advantage of the systematic relation between lexical-semantic structure and syntactic structure.\n", "This paper demonstrates that a systematic solution to the divergence problem can be derived from the formalization of two types of information: (1) the linguistically grounded classes upon which lexical-semantic divergences are based; and (2) the techniques by which lexical-semantic divergences are resolved.\n", "This formalization is advantageous in that it facilitates the design and implementation of the system, allows one to make an evaluation of the status of the system, and provides a basis for proving certain important properties about the system.\n", "We categorize sources of syntactic divergence between languages.\n"]}
{"article_lines": ["Using Syntactic Dependency As Local Context To Resolve Word Sense Ambiguity", "Most previous corpus based algorithms disambiguate a word with a classifier trained from previous usages of the same word .", "Separate classifiers have to be trained for different words .", "We present an algorithm that uses the same knowledge sources to disambiguate different words .", "The algorithm does not require a sense tagged corpus and exploits the fact that two different words are likely to have similar meanings if they occur in identical local contexts .", "Given a word , its context and its possible meanings , the problem of word sense disambiguation WSD is to determine the meaning of the word in that context .", "WSD is useful in many natural language tasks , such as choosing the correct word in machine translation and coreference resolution .", "In several recent proposals Hearst , 1991 ; Bruce and Wiebe , 1994 ; Leacock , Towwell , and Voorhees , 1996 ; Ng and Lee , 1996 ; Yarowsky , 1992 ; Yarowsky , 1994 , statistical and machine learning techniques were used to extract classifiers from hand tagged corpus .", "Yarowsky Yarowsky , 1995 proposed an unsupervised method that used heuristics to obtain seed classifications and expanded the results to the other parts of the corpus , thus avoided the need to hand annotate any examples .", "Most previous corpus based WSD algorithms determine the meanings of polysemous words by exploiting their local contexts .", "A basic intuition that underlies those algorithms is the following 1 Two occurrences of the same word have identical meanings if they have similar local contexts .", "In other words , most previous corpus based WSD algorithms learn to disambiguate a polysemous word from previous usages of the same word .", "This has several undesirable consequences .", "Firstly , a word must occur thousands of times before a good classifier can be learned .", "In Yarowsky's experiment Yarowsky , 1995 , an average of 3936 examples were used to disambiguate between two senses .", "In Ng and Lee's experiment , 192 , 800 occurrences of 191 words were used as training examples .", "There are thousands of polysemous words , e . g . , there are 11 , 562 polysemous nouns in WordNet .", "For every polysemous word to occur thousands of times each , the corpus must contain billions of words .", "Secondly , learning to disambiguate a word from the previous usages of the same word means that whatever was learned for one word is not used on other words , which obviously missed generality in natural languages .", "Thirdly , these algorithms cannot deal with words for which classifiers have not been learned .", "In this paper , we present a WSD algorithm that relies on a different intuition 2 Two different words are likely to have similar meanings if they occur in identical local contexts .", "Consider the sentence 3 The new facility will employ 500 of the existing 600 employees The word quot ; facility quot ; has 5 possible meanings in WordNet 1 . 5 Miller , 1990 a installation , b proficiency technique , c adeptness , d readiness , e toilet bathroom .", "To disambiguate the word , we consider other words that appeared in an identical local context as quot ; facility quot ; in 3 .", "Table 1 is a list of words that have also been used as the subject of quot ; employ quot ; in a 25 million word Wall Street Journal corpus .", "The quot ; freq quot ; column are the number of times these words were used as the subject of quot ; employ quot ; .", "ORG includes all proper names recognized as organizations The logA column are their likelihood ratios Dunning , 1993 .", "The meaning of quot ; facility quot ; in 3 can be determined by choosing one of its 5 senses that is most similar' to the meanings of words in Table 1 .", "This way , a polysemous word is disambiguated with past usages of other words .", "Whether or not it appears in the corpus is irrelevant .", "Our approach offers several advantages The required resources of the algorithm include the following a an untagged text corpus , b a broad coverage parser , c a concept hierarchy , such as the WordNet Miller , 1990 or Roget's Thesaurus , and d a similarity measure between concepts .", "In the next section , we introduce our definition of local contexts and the database of local contexts .", "A description of the disambiguation algorithm is presented in Section 3 .", "Section 4 discusses the evaluation results .", "Psychological experiments show that humans are able to resolve word sense ambiguities given a narrow window of surrounding words Choueka and Lusignan , 1985 .", "Most WSD algorithms take as input ito be defined in Section 3 . 1 a polysemous word and its local context .", "Different systems have different definitions of local contexts .", "In Leacock , Towwell , and Voorhees , 1996 , the local context of a word is an unordered set of words in the sentence containing the word and the preceding sentence .", "In Ng and Lee .", "1996 , a local context of a word consists of an ordered sequence of 6 surrounding part of speech tags , its morphological features , and a set of collocations .", "In our approach , a local context of a word is defined in terms of the syntactic dependencies between the word and other words in the same sentence .", "A dependency relationship Hudson , 1984 ; Mel'euk , 1987 is an asymmetric binary relationship between a word called head or governor , parent , and another word called modifier or dependent , daughter .", "Dependency grammars represent sentence structures as a set of dependency relationships .", "Normally the dependency relationships form a tree that connects all the words in a sentence .", "An example dependency structure is shown in 4 .", "The local context of a word W is a triple that corresponds to a dependency relationship in which W is the head or the modifier type word position where type is the type of the dependency relationship , such as subj subject , adjn adjunct , comp I first complement , etc .", "; word is the word related to W via the dependency relationship ; and posit ion can either be head or mod .", "The position indicates whether word is the head or the modifier in dependency relation .", "Since a word may be involved in several dependency relationships , each occurrence of a word may have multiple local contexts .", "The local contexts of the two nouns quot ; boy quot ; and quot ; dog quot ; in 4 are as follows the dependency relations between nouns and their determiners are ignored boy subj chase head dog adjn brown mod compl chase head Using a broad coverage parser to parse a corpus , we construct a Local Context Database .", "An entry in the database is a pair where lc is a local context and C lc is a set of word frequency likelihood triples .", "Each triple specifies how often word occurred in lc and the likelihood ratio of lc and word .", "The likelihood ratio is obtained by treating word and c as a bigram and computed with the formula in Dunning , 1993 .", "The database entry corresponding to Table 1 is as follows", "The polysemous words in the input text are disambiguated in the following steps Step A . Parse the input text and extract local contexts of each word .", "Let LC . denote the set of local contexts of all occurrences of w in the input text .", "Step B .", "Search the local context database and find words that appeared in an identical local context as w . They are called selectors of w Step C . Select a sense s of w that maximizes the similarity between w and Selectors .", "Step D . The sense s is assigned to all occurrences of w in the input text .", "This implements the quot ; one sense per discourse quot ; heuristic advocated in Gale , Church , and Yarowsky , 1992 .", "Step C . needs further explanation .", "In the next subsection , we define the similarity between two word senses or concepts .", "We then explain how the similarity between a word and its selectors is maximized .", "There have been several proposed measures for similarity between two concepts Lee , Kim , and Lee , 1989 ; Rada et al . , 1989 ; Resnik , 1995b ; Wu and Palmer , 1994 .", "All of those similarity measures are defined directly by a formula .", "We use instead an information theoretic definition of similarity that can be derived from the following assumptions where cornmon A , B is a proposition that states the commonalities between A and B ; s is the amount of information contained in the proposition s . Assumption 2 The differences between A and B is measured by where describe A , B is a proposition that describes what A and B are .", "Assumption 3 The similarity between A and B , sim A , B , is a function of their commonality and differences .", "That is , sim A , B f I common A , B , I describe A , B The domain of f x , y is x , y ix 0 , y 0 , y x .", "Assumption 4 Similarity is independent of the unit used in the information measure .", "According to Information Theory Cover and Thomas , 1991 , s log bP s , where P s is the probability of s and b is the unit .", "When b 2 , s is the number of bits needed to encode s . Since log bx 12412 , Assumption 4 means that the function f must satisfy the following condition Vc 0 , f x , y f cx , Assumption 5 Similarity is additive with respect to commonality .", "If cornman A , B consists of two independent parts , then the sim A , B is the sum of the similarities computed when each part of the commonality is considered .", "In other words f xi x2 , y f xi , y f x2 , y .", "A corollary of Assumption 5 is that Vy , f 0 , y f x 0 , y f x , y 0 , which means that when there is no commonality between A and B , their similarity is 0 , no matter how different they are .", "For example , the similarity between quot ; depth first search quot ; and quot ; leather sofa quot ; is neither higher nor lower than the similarity between quot ; rectangle quot ; and quot ; interest rate quot ; .", "Assumption 6 The similarity between a pair of identical objects is 1 .", "When A and B are identical . knowning their commonalities means knowing what they are , i . e . , I comman A , B I describe A .", "B .", "Therefore , the function f must have the following property Vx , f x , x 1 .", "Assumption 7 The function f x . y is continuous .", "Similarity Theorem The similarity between A and B is measured by the ratio between the amount of information needed to state the commonality of A and B and the information needed to fully describe what A and B are Proof To prove the theorem . we need to show f x , y s . Since f x , y f s . 1 due to Assumption 4 , we only need to show that when LI , is a rational number , f x , y .", "The result can be generalized to all real numbers because f is continuous and for any real number , there are rational numbers that are infinitely close to it .", "Suppose m and n are positive integers .", "due to Assumption 5 .", "Thus . f x , y ; ; f nx , y .", "Substituting fi for x in this equation Q . E . D .", "For example .", "Figure 1 is a fragment of the WordNet .", "The nodes are concepts or synsets as they are called in the WordNet .", "The links represent IS A relationships .", "The number attached to a node C is the probability P C that a randomly selected noun refers to an instance of C . The probabilities are estimated by the frequency of concepts in SemCor Miller et al . , 1994 , a sense tagged subset of the Brown corpus .", "If x is a Hill and y is a Coast , the commonality between x and p is that quot ; x is a GeoForm and y is a GeoForm quot ; .", "The information contained in this statement is 2 x logP GeoF arm .", "The similarity between the concepts Hill and Coast is where p ni co is the probability of that an object belongs to all the maximally specific super classes Cts of both C and C' .", "We now provide the details of Step C in our algorithm .", "The input to this step consists of a polysemous word V110 and its selectors WI , W2 WO .", "The word Wi has ni senses sii , , sin .", "Step C . 1 Construct a similarity matrix 8 .", "The rows and columns represent word senses .", "The matrix is divided into k 1 x k 1 blocks .", "The blocks on the diagonal are all Os .", "The elements in block Sii are the similarity measures between the senses of Wi and the senses of It .", "Similarity measures lower than a threshold 0 are considered to be noise and are ignored .", "In our experiments , 9 0 . 2 was used .", "Step C . 5 Modify the similarity matrix to remove the similarity values between other senses of W . i and senses of other words .", "For all 1 , j , m , such that 1 E 1 , ni , . . and 1 0 Imax and j imax and m E 1 , nil' Let's consider again the word quot ; facility quot ; in 3 .", "It has two local contexts subject of quot ; employ quot ; subj employ head and modifiee of quot ; new quot ; adjn new mod .", "Table 1 lists words that appeared in the first local context .", "Table 2 lists words that appeared in the second local context .", "Only words with top 20 likelihood ratio were used in our experiments .", "The two groups of words are merged and used as the selectors of quot ; facility quot ; .", "The words quot ; facility quot ; has 5 senses in the WordNet .", "Senses 1 and 5 are subclasses of artifact .", "Senses 2 and 3 are kinds of state .", "Sense 4 is a kind of abstraction .", "Many of the selectors in Tables 1 and Table 2 have artifact senses , such as quot ; post quot ; , quot ; product quot ; , quot ; system quot ; , quot ; unit quot ; , quot ; memory device quot ; , quot ; machine quot ; , quot ; plant quot ; , quot ; model quot ; , quot ; program quot ; , etc .", "Therefore , Senses 1 and 5 of quot ; facility quot ; received much more support , 5 . 37 and 2 . 42 respectively , than other senses .", "Sense 1 is selected .", "Consider another example that involves an unknown proper name We treat unknown proper nouns as a polysemous word which could refer to a person , an organization , or a location .", "Since quot ; DreamLand quot ; is the subject of quot ; employed quot ; , its meaning is determined by maximizing the similarity between one of person , organization , locaton and the words in Table 1 .", "Since Table 1 contains many quot ; organization quot ; words , the support for the quot ; organization quot ; sense is much higher than the others .", "We used a subset of the SemCor Miller et al . , 1994 to evaluate our algorithm .", "General purpose lexical resources , such as WordNet , Longman Dictionary of Contemporary English LDOCE , and Roget's Thesaurus , strive to achieve completeness .", "They often make subtle distinctions between word senses .", "As a result , when the WSD task is defined as choosing a sense out of a list of senses in a general purpose lexical resource , even humans may frequently disagree with one another on what the correct sense should be .", "The subtle distinctions between different word senses are often unnecessary .", "Therefore , we relaxed the correctness criterion .", "A selected sense sanswer is correct if it is quot ; similar enough quot ; to the sense tag S key in SemCor .", "We experimented with three interpretations of quot ; similar enough quot ; .", "The strictest interpretation is SiM Sanswer , skey 1 , which is true only when sanswer skey .", "The most relaxed interpretation is SiM Sanewer , S key 0 , which is true if sanswer and skey are the descendents of the same top level concepts in WordNet e . g . , entity , group , location , etc . .", "A compromise between these two is SiM Sanswer , Skey 0 . 27 , where 0 . 27 is the average similarity of 50 , 000 randomly generated pairs w , w' in which w and w' belong to the same Roget's category .", "We use three words quot ; duty quot ; , quot ; interest quot ; and quot ; line quot ; as examples to provide a rough idea about what sim sanswer , skey 0 . 27 means .", "The word quot ; duty quot ; has three senses in WordNet 1 . 5 .", "The similarity between the three senses are all below 0 . 27 , although the similarity between Senses 1 responsibility and 2 assignment , chore is very close 0 . 26 to the threshold .", "The word quot ; interest quot ; has 8 senses .", "Senses 1 sake , benefit and 7 interestingness are merged . 2 Senses 3 fixed charge for borrowing money , 4 a right or legal share of something , and 5 financial interest in something are merged .", "The word quot ; interest quot ; is reduced to a 5 way ambiguous word .", "The other three senses are 2 curiosity , 6 interest group and 8 pastime , hobby .", "The word quot ; line quot ; has 27 senses .", "The similarity threshold 0 . 27 reduces the number of senses to 14 .", "The reduced senses are where each group is a reduced sense and the numbers are original WordNet sense numbers .", "We used a 25 million word Wall Street Journal corpus part of LDC DCI3 CDROM to construct the local context database .", "The text was parsed in 126 hours on a SPARC Ultra 1 140 with 96MB of memory .", "We then extracted from the parse trees 8 , 665 , 362 dependency relationships in which the head or the modifier is a noun .", "We then filtered out lc , word pairs with a likelihood ratio lower than 5 an arbitrary threshold .", "The resulting database contains 354 , 670 local contexts with a total of 1 , 067 , 451 words in them Table 1 is counted as one local context with 20 words in it .", "Since the local context database is constructed from WSJ corpus which are mostly business news , we only used the quot ; press reportage quot ; part of SemCor which consists of 7 files with about 2000 words each .", "Furthermore , we only applied our algorithm to nouns .", "Table 3 shows the results on 2 , 832 polysemous nouns in SemCor .", "This number also includes proper nouns that do not contain simple markers e . g . , Mr . , Inc . to indicate its category .", "Such a proper noun is treated as a 3 way ambiguous word person , organization , or location .", "We also showed as a baseline the performance of the simple strategy of always choosing the first sense of a word in the WordNet .", "Since the WordNet senses are ordered according to their frequency in SemCor , choosing the first sense is roughly the same as choosing the sense with highest prior probability , except that we are not using all the files in SemCor .", "It can be seen from Table 3 that our algorithm performed slightly worse than the baseline when the strictest correctness criterion is used .", "However , when the condition is relaxed , its performance gain is much lager than the baseline .", "This means that when the algorithm makes mistakes , the mistakes tend to be close to the correct answer .", "The Step C in Section 3 . 2 is similar to Resnik's noun group disambiguation Resnik , 1995a , although he did not address the question of the creation of noun groups .", "The earlier work on WSD that is most similar to ours is Li , Szpakowicz , and Matwin , 1995 .", "They proposed a set of heuristic rules that are based on the idea that objects of the same or similar verbs are similar .", "Our algorithm treats all local contexts equally in its decision making .", "However , some local contexts hardly provide any constraint on the meaning of a word .", "For example , the object of quot ; get quot ; can practically be anything .", "This type of contexts should be filtered out or discounted in decision making .", "Our assumption that similar words appear in identical context does not always hold .", "For example , where PER refers to proper names recognized as persons .", "None of these is similar to the quot ; body part quot ; meaning of quot ; heart quot ; .", "In fact , quot ; heart quot ; is the only body part that beats .", "We have presented a new algorithm for word sense disambiguation .", "Unlike most previous corpusbased WSD algorithm where separate classifiers are trained for different words , we use the same local context database and a concept hierarchy as the knowledge sources for disambiguating all words .", "This allows our algorithm to deal with infrequent words or unknown proper nouns .", "Unnecessarily subtle distinction between word senses is a well known problem for evaluating WSD algorithms with general purpose lexical resources .", "Our use of similarity measure to relax the correctness criterion provides a possible solution to this problem ."], "summary_lines": ["Using Syntactic Dependency As Local Context To Resolve Word Sense Ambiguity\n", "Most previous corpus-based algorithms disambiguate a word with a classifier trained from previous usages of the same word.\n", "Separate classifiers have to be trained for different words.\n", "We present an algorithm that uses the same knowledge sources to disambiguate different words.\n", "The algorithm does not require a sense-tagged corpus and exploits the fact that two different words are likely to have similar meanings if they occur in identical local contexts.\n", "We define the similarity between two objects to be the amount of information contained in the commonolity between the objects divided by the amount of information in the descriptions of the objects.\n"]}
{"article_lines": ["Cross lingual Word Clusters for Direct Transfer of Linguistic Structure", "It has been established that incorporating word cluster features derived from large unlabeled corpora can significantly improve prediction of linguistic structure .", "While previous work has focused primarily on English , we extend these results to other languages along two dimensions .", "First , we show that these results hold true for a number of languages across families .", "Second , and more interestingly , we provide an algorithm for inducing cross lingual clusters and we show that features derived from these clusters significantly improve the accuracy of cross lingual structure prediction .", "Specifically , we show that by augmenting direct transfer systems with cross lingual cluster features , the relative error of delexicalized dependency parsers , trained on English treebanks and transferred to foreign languages , can be reduced by up to 13 .", "When applying the same method to direct transfer of named entity recognizers , we observe relative improvements of up to 26 .", "The ability to predict the linguistic structure of sentences or documents is central to the field of natural language processing NLP .", "Structures such as named entity tag sequences Bikel et al . , 1999 or sentiment relations Pang and Lee , 2008 are inherently useful in data mining , information retrieval and other user facing technologies .", "More fundamental structures such as part of speech tag sequences Ratnaparkhi , 1996 or syntactic parse trees Collins , 1997 ; K ubler et al . , 2009 , on the other hand , comprise the core linguistic analysis for many important downstream tasks such as machine translation Chiang , The majority of this work was performed while the author was an intern at Google , New York , NY .", "2005 ; Collins et al . , 2005 .", "Currently , supervised data driven methods dominate the literature on linguistic structure prediction Smith , 2011 .", "Regrettably , the majority of studies on these methods have focused on evaluations specific to English , since it is the language with the most annotated resources .", "Notable exceptions include the CoNLL shared tasks Tjong Kim Sang , 2002 ; Tjong Kim Sang and De Meulder , 2003 ; Buchholz and Marsi , 2006 ; Nivre et al . , 2007 and subsequent studies on this data , as well as a number of focused studies on one or two specific languages , as discussed by Bender 2011 .", "While annotated resources for parsing and several other tasks are available in a number of languages , we cannot expect to have access to labeled resources for all tasks in all languages .", "This fact has given rise to a large body of research on unsupervised Klein and Manning , 2004 , semi supervised Koo et al . , 2008 and transfer Hwa et al . , 2005 systems for prediction of linguistic structure .", "These methods all attempt to benefit from the plethora of unlabeled monolingual and or cross lingual data that has become available in the digital age .", "Unsupervised methods are appealing in that they are often inherently language independent .", "This is borne out by the many recent studies on unsupervised parsing that include evaluations covering a number of languages Cohen and Smith , 2009 ; Gillenwater et al . , 2010 ; Naseem et al . , 2010 ; Spitkovsky et al . , 2011 .", "However , the performance for most languages is still well below that of supervised systems and recent work has established that the performance is also below simple methods of linguistic transfer McDonald et al . , 2011 .", "In this study we focus on semi supervised and linguistic transfer methods for multilingual structure prediction .", "In particular , we pursue two lines of research around the use of word cluster features in discriminative models for structure prediction guages for dependency parsing and 4 languages for named entity recognition NER .", "This is the first study with such a broad view on this subject , in terms of language diversity .", "Cross lingual word cluster features for transferring linguistic structure from English to other languages .", "We develop an algorithm that generates cross lingual word clusters ; that is clusters of words that are consistent across languages .", "This is achieved by means of a probabilistic model over large amounts of monolingual data in two languages , coupled with parallel data through which cross lingual word cluster constraints are enforced .", "We show that by augmenting the delexicalized direct transfer system of McDonald et al . 2011 with cross lingual cluster features , we are able to reduce its error by up to 13 relative .", "Further , we show that by applying the same method to direct transfer NER , we achieve a relative error reduction of 26 .", "In line with much previous work on word clusters for tasks such as dependency parsing and NER , for which local syntactic and semantic constraints are of importance , we induce word clusters by means of a probabilistic class based language model Brown et al . , 1992 ; Clark , 2003 .", "However , rather than the more commonly used model of Brown et al . 1992 , we use the predictive class bigram model introduced by Uszkoreit and Brants 2008 .", "The two models are very similar , but whereas the former takes classto class transitions into account , the latter directly models word to class transitions .", "By ignoring classto class transitions , an approximate maximum likelihood clustering can be found efficiently with the distributed exchange algorithm Uszkoreit and Brants , 2008 .", "This is a useful property , as we later develop an algorithm for inducing cross lingual word clusters that calls this monolingual algorithm as a subroutine .", "More formally , let C V H 1 , . . . , K be a hard clustering function that maps each word type from the vocabulary , V , to one of K cluster identities .", "With the model of Uszkoreit and Brants 2008 , the likelihood of a sequence of word tokens , w wi mi 1 , with wi E V U S , where S is a designated start ofsegment symbol , factors as Compare this to the model of Brown et al . 1992 By incorporating cross lingual cluster features in a m linguistic transfer system , we are for the first time L' w ; C P wi C wi P C wi C wi 1 . combining SSL and cross lingual transfer . i 1", "Word cluster features have been shown to be useful in various tasks in natural language processing , including syntactic dependency parsing Koo et al . , 2008 ; Haffari et al . , 2011 ; Tratz and Hovy , 2011 , syntactic chunking Turian et al . , 2010 , and NER Freitag , 2004 ; Miller et al . , 2004 ; Turian et al . , 2010 ; Faruqui and Pad o , 2010 .", "Intuitively , the reason for the effectiveness of cluster features lie in their ability to aggregate local distributional information from large unlabeled corpora , which aid in conquering data sparsity in supervised training regimes as well as in mitigating cross domain generalization issues .", "While the use of class to class transitions can lead to more compact models , which is often useful for conquering data sparsity , when clustering large data sets we can get reliable statistics directly on the wordto class transitions Uszkoreit and Brants , 2008 .", "In addition to the clustering model that we make use of in this study , a number of additional word clustering and embedding variants have been proposed .", "For example , Turian et al . 2010 assessed the effectiveness of the word embedding techniques of Collobert and Weston 2008 and Mnih and Hinton 2007 along with the word clustering technique of Brown et al .", "1992 for syntactic chunking and NER .", "Recently , Dhillon et al . 2011 proposed a word embedding method based on canonical correlation analysis that provides state of the art results for wordbased SSL for English NER .", "As an alternative to clustering words , Lin and Wu 2009 proposed a phrase clustering approach that obtained the state of the art result for English NER .", "Before moving on to the multilingual setting , we conduct a set of monolingual experiments where we evaluate the use of the monolingual word clusters just described as features for dependency parsing and NER .", "In the parsing experiments , we study the following thirteen languages 1 Danish DA , German DE , Greek EL , English EN , Spanish ES , French FR , Italian IT , Korean KO , Dutch NL , Portugese PT , Russian RU , Swedish SV and Chinese ZH representing the Chinese , Germanic , Hellenic , Romance , Slavic , Altaic and Korean genera .", "In the NER experiments , we study three Germanic languages German DE , English EN and Dutch NL ; and one Romance language Spanish ES .", "Details of the labeled and unlabeled data sets used are given in Appendix A .", "For all experiments we fixed the number of clusters to 256 as this performed well on held out data .", "Furthermore , we only clustered the 1 million most frequent word types in each language for both efficiency and sparsity reasons .", "For languages in which our unlabeled data did not have at least 1 million types , we considered all types .", "All of the parsing experiments reported in this study are based on the transition based dependency parsing paradigm Nivre , 2008 .", "For all languages and settings , we use an arc eager decoding strategy , with a beam of eight hypotheses , and perform ten epochs of the averaged structured perceptron algorithm Zhang and Clark , 2008 .", "We extend the state of the art feature model recently introduced by Zhang and Nivre 2011 by adding an additional word cluster based feature template for each word based template .", "Additionally , we add templates where one or more partof speech feature is replaced with the corresponding cluster feature .", "The resulting set of additional feature templates are shown in Table 1 .", "The expanded feature model includes all of the feature templates defined by Zhang and Nivre 2011 , which we also use as the baseline model , whereas Table 1 only shows our new templates due to space limitations .", "For all NER experiments , we use a sequential firstorder conditional random field CRF with a unit variance Normal prior , trained with L BFGS until c convergence c 0 . 0001 , typically obtained after less than 400 iterations .", "The feature model used for the NER tagger is shown in Table 2 .", "These are similar to the features used by Turian et al . 2010 , with the main difference that we do not use any long range features and that we add templates that conjoin adjacent clusters and adjacent tags as well as templates that conjoin label transitions with tags , clusters and capitalization features .", "The results of the parsing experiments , measured with labeled accuracy score LAS on all sentence lengths , excluding punctuation , are shown in Table 3 .", "The baselines are all comparable to the state of theart .", "On average , the addition of word cluster features yields a 6 relative reduction in error and upwards of 15 for RU .", "All languages improve except FR , which sees neither an increase nor a decrease in LAS .", "We observe an average absolute increase in LAS of approximately 1 , which is inline with previous observations Koo et al . , 2008 .", "It is perhaps not surprising that RU sees a large gain as it is a highly inflected language , making observations of lexical features far more sparse .", "Some languages , e . g . , FR , NL , and ZH see much smaller gains .", "One likely culprit is a divergence between the tokenization schemes used in the treebank and in our unlabeled data , which for Indo European languages is closely related to the Penn Treebank tokenization .", "For example , the NL treebank contains many multi word tokens that are typically broken apart by our automatic tokenizer .", "The NER results , in terms of F1 measure , are listed in Table 4 .", "Introducing word cluster features for NER reduces relative errors on the test set by 21 39 on the development set on average .", "Broken down per language , reductions on the test set vary from substantial for NL 30 and EN 26 , down to more modest for DE 17 and ES 12 .", "The addition of cluster features most markedly improve recognition of the PER category , with an average error reduction on the test set of 44 , while the reductions for ORG 19 , LOC 17 and MISC 10 are more modest , but still significant .", "Although our results are below the best reported results for EN and DE Lin and Wu , 2009 ; Faruqui and Pad o , 2010 , the relative improvements of adding word clusters are inline with previous results on NER for EN Miller et al . , 2004 ; Turian et al . , 2010 , who report error reductions of approximately 25 from adding word cluster features .", "Slightly higher reductions where achieved for DE by Faruqui and Pad o 2010 , who report a reduction of 22 .", "Note that we did not tune hyper parameters of the supervised learning methods and of the clustering method , such as the number of clusters Turian et al . , 2010 ; Faruqui and Pad o , 2010 , and that we did not apply any heuristic for data cleaning such as that used by Turian et al . 2010 .", "All results of the previous section rely on the availability of large quantities of language specific annotations for each task .", "Cross lingual transfer methods and unsupervised methods have recently been shown to hold promise as a way to at least partially sidestep the demand for labeled data .", "Unsupervised methods attempt to infer linguistic structure without using any annotated data Klein and Manning , 2004 or possibly by using a set of linguistically motivated rules Naseem et al . , 2010 or a linguistically informed model structure Berg Kirkpatrick and Klein , 2010 .", "The aim of transfer methods is instead to use knowledge induced from labeled resources in one or more source languages to construct systems for target languages in which no or few such resources are available Hwa et al . , 2005 .", "Currently , the performance of even the most simple direct transfer systems far exceeds that of unsupervised systems Cohen et al . , 2011 ; McDonald et al . , 2011 ; S\u00f8gaard , 2011 .", "Our starting point is the delexicalized direct transfer method proposed by McDonald et al . 2011 based on work by Zeman and Resnik 2008 .", "This method was shown to outperform a number of state of the art unsupervised and transfer based baselines .", "The method is simple ; for a given training set , the learner ignores all lexical identities and only observes features over other characteristics , e . g . , part of speech tags , orthographic features , direction of syntactic attachment , etc .", "The learner builds a model from an annotated source language data set , after which the model is used to directly make target language predictions .", "There are three basic assumptions that drive this approach .", "First , that high level tasks , such as syntactic parsing , can be learned reliably using coarse grained statistics , such as part of speech tags , in place of fine grained statistics such as lexical word identities .", "Second , that the parameters of features over coarsegrained statistics are in some sense language independent , e . g . , a feature that indicates that adjectives modify their closest noun is useful in all languages .", "Third , that these coarse grained statistics are robustly available across languages .", "The approach proposed by McDonald et al . 2011 relies on these three assumptions .", "Specifically , by replacing fine grained language specific part of speech tags with universal part of speech tags , generated with the method described by Das and Petrov 2011 , a universal parser is achieved that can be applied to any language for which universal part of speech tags are available .", "Below , we extend this approach to universal parsing by adding cross lingual word cluster features .", "A cross lingual word clustering is a clustering of words in two languages , in which the clusters correspond to some meaningful cross lingual property .", "For example , prepositions from both languages should be in the same cluster , proper names from both languages in another cluster and so on .", "By adding features defined over these clusters , we can , to some degree , re lexicalize the delexicalized models , while maintaining the universality of the features .", "This approach is outlined in Figure 1 .", "Assuming that we have an algorithm for generating cross lingual word clusters see Section 4 . 2 , we can augment the delexicalized parsing algorithm to use these word cluster features at training and testing time .", "In order to further motivate the proposed approach , consider the accuracy of the supervised English parser .", "A parser with lexical , part of speech and cluster features achieves 90 . 7 LAS see Table 3 .", "If we remove all lexical and cluster features , the same parser achieves 83 . 1 .", "However , if we add back just the cluster features , the accuracy jumps back up to 89 . 5 , which is only 1 . 2 below the full system .", "Thus , if we can accurately learn cross lingual clusters , there is hope of regaining some of the accuracy lost due to the delexicalization process .", "Our first method for inducing cross lingual clusters has two stages .", "First , it clusters a source language S as in the monolingual case , and then projects these clusters to a target language T , using word alignments .", "Given two aligned word sequences set of scored alignments from the source language to the target language , where wTj , wSaj , sj , aj AT S is an alignment from the ajth source word to the jth target word , with score sj , aj \u03b4 . 2 We use the shorthand j AT S to denote those target words wTj that are aligned to some source word wSaj .", "Provided a clustering CS , we assign the target word t VT to the cluster with which it is most often aligned where is the indicator function .", "We refer to the cross lingual clusters induced in this way as PROJECTED CLUSTERS .", "This simple projection approach has two potential drawbacks .", "First , it only provides a clustering of those target language words that occur in the word 2In our case , the alignment score corresponds to the conditional alignment probability p wTj wSa .", "All E alignments are ignored and we use S 0 . 95 throughout . aligned data , which is typically smaller than our monolingual data sets .", "Second , the mapped clustering may not necessarily correspond to an acceptable target language clustering in terms of monolingual likelihood .", "In order to tackle these issues , we propose the following more complex model .", "First , to find clusterings that are good according to both the source and target language , and to make use of more unlabeled data , we model word sequences in each language by the monolingual language model with likelihood function defined by equation 1 .", "Denote these likelihood functions respectively by LS wS ; CS and LT wT ; CT , where we have overloaded notation so that the word sequences denoted by wS and wT include much more plentiful non aligned data when taken as an argument of the monolingual likelihood functions .", "Second , we couple the clusterings defined by these individual models , by introducing additional factors based on word alignments , as proposed by Och 1999 and the symmetric LS T wS ; AS T , CS , CT .", "Note that the simple projection defined by equation 2 correspond to a hard assignment variant of this probabilistic formulation when the source clustering is fixed .", "Combining all four factors results in the joint monolingual and cross lingual objective function The intuition of this approach is that the clusterings CS and CT are forced to jointly explain the source and target data , treating the word alignments as a form of soft constraints .", "We approximately optimize 3 with the alternating procedure in Algorithm 1 , in which we iteratively maximize LS and LT , keeping the other factors fixed .", "In this way we can generate cross lingual clusterings using all the monolingual data while forcing the clusterings to obey the word alignment constraints .", "We refer to the clusters induced with this method as X LINGUAL CLUSTERS .", "In practice we found that each unconstrained monolingual run of the exchange algorithm lines Optimized via the exchange algorithm keeping the cluster of projected words fixed and only clustering additional words not in the projection .", "1 and 3 moves the clustering too far from those that obey the word alignment constraints , which causes the procedure to fail to converge .", "However , we found that fixing the clustering of the words that are assigned clusters in the projection stages lines 2 and 4 and only clustering the remaining words works well in practice .", "Furthermore , we found that iterating the procedure has little effect on performance and set N 1 for all subsequent experiments .", "In our first set of experiments on using cross lingual cluster features , we evaluate direct transfer of our EN parser , trained on Stanford style dependencies De Marneffe et al . , 2006 , to the the ten non EN Indo European languages listed in Section 3 .", "We exclude KO and ZH as initial experiments proved direct transfer a poor technique when transferring parsers between such diverse languages .", "We study the impact of using cross lingual cluster features by comparing the strong delexicalized baseline model of McDonald et al . 2011 , which only has features derived from universal part of speech tags , projected from English with the method of Das and Petrov 2011 , to the same model when adding features derived from cross lingual clusters .", "In both cases the feature models are the same as those used in Section 3 . 1 , except that they are delexicalized by removing all lexical word identity features .", "We evaluate both the PROJECTED CLUSTERS and the X LINGUAL CLUSTERS .", "For these experiments we train the perceptron for only five epochs in order to prevent over fitting , which is an acute problem due to the divergence between the training and testing data sets in this setting .", "Furthermore , in accordance to standard practices we only evaluate unlabeled attachment score UAS due to the fact that each treebank uses a different possibly non overlapping label set .", "In our second set of experiments , we evaluate direct transfer of a NER system trained on EN to DE , ES and NL .", "We use the same feature models as in the monolingual case , with the exception that we use universal part of speech tags for all languages and we remove the capitalization feature when transferring from EN to DE .", "Capitalization is both a prevalent and highly predictive feature of named entities in EN , while in DE , capitalization is even more prevalent , but has very low predictive power .", "Interestingly , while delexicalization has shown to be important for direct transfer of dependency parsers McDonald et al . , 2011 , we noticed in preliminary experiments that it substantially degrades performance for NER .", "We hypothesize that this is because word features are predictive of common proper names and that these are often translated directly across languages , at least in the case of newswire text .", "As for the transfer parser , when training the source NER model , we regularize the model more heavily by setting Q 0 . 1 .", "Appendix A contains the details of the training , testing , unlabeled and parallel aligned data sets .", "Table 5 lists the results of the transfer experiments for dependency parsing .", "The baseline results are comparable to those in McDonald et al . 2011 and thus also significantly outperform the results of recent unsupervised approaches Berg Kirkpatrick and Klein , 2010 ; Naseem et al . , 2010 .", "Importantly , crosslingual cluster features are helpful across the board and give a relative error reduction ranging from 3 for DA to 13 for PT , with an average reduction of 6 , in terms of unlabeled attachment score UAS .", "This shows the utility of cross lingual cluster features for syntactic transfer .", "However , X LINGUAL CLUSTERS provides roughly the same performance as PROJECTED CLUSTERS suggesting that even simple methods of cross lingual clustering are sufficient for direct transfer dependency parsing .", "We would like to stress that these results are likely to be under estimating the parsers actual ability to predict Stanford style dependencies in the target languages .", "This is because the target language annotations that we use for evaluation differ from the Stanford dependency annotation .", "Some of these differences are warranted in that certain target language phenomena are better captured by the native annotation .", "However , differences such as choice of lexical versus functional head are more arbitrary .", "To highlight this point we run two additional experiments .", "First , we had linguists , who were also fluent speakers of German , re annotate the DE test set so that unlabeled arcs are consistent with Stanfordstyle dependencies .", "Using this data , NO CLUSTERS obtains 60 . 0 UAS , PROJECTED CLUSTERS 63 . 6 and X LINGUAL CLUSTERS 64 . 4 .", "When compared to the scores on the original data set 48 . 9 , 50 . 3 and 50 . 7 , respectively we can see that not only is the baseline system doing much better , but that the improvements from cross lingual clustering are much more pronounced .", "Next , we investigated the accuracy of subject and object dependencies , as these are often annotated in similar ways across treebanks , typically modifying the main verb of the sentence .", "The bottom half of Table 5 gives the scores when restricted to such dependencies in the gold data .", "We measure the percentage of modifiers in subject and object dependencies that modify the correct word .", "Indeed , here we see the difference in performance become clearer , with the cross lingual cluster model reducing errors by 14 relative to the non cross lingual model and upwards of 22 relative for IT .", "We now turn to the results of the transfer experiments for NER , listed in Table 6 .", "While the performance of the transfer systems is very poor when no word clusters are used , adding cross lingual word clusters give substantial improvements across all languages .", "The simple PROJECTED CLUSTERS work well , but the X LINGUAL CLUSTERS provide even larger improvements .", "On average the latter reduce errors on the test set by 22 in terms of FI and up to 26 for ES .", "We also measure how well the direct transfer NER systems are able to detect entity boundaries ignoring the entity categories .", "Here , on average , the best clusters provide a 24 relative error reduction on the test set 75 . 8 vs . 68 . 1 FI .", "To our knowledge there are no comparable results on transfer learning of NER systems .", "Based on the results of this first attempt at this scenario , we believe that transfer learning by multilingual word clusters could be developed into a practical way to construct NER systems for resource poor languages .", "In the first part of this study , we showed that word clusters induced from a simple class based language model can be used to significantly improve on stateof the art supervised dependency parsing and NER for a wide range of languages and even across language families .", "Although the improvements vary between languages , the addition of word cluster features never has a negative impact on performance .", "This result has important practical consequences as it allows practitioners to simply plug in word cluster features into their current feature models .", "Given previous work on word clusters for various linguistic structure prediction tasks , these results are not too surprising .", "However , to our knowledge this is the first study to apply the same type of word cluster features across languages and tasks .", "In the second part , we provided two simple methods for inducing cross lingual word clusters .", "The first method works by projecting word clusters , induced from monolingual data , from a source language to a target language directly via word alignments .", "The second method , on the other hand , makes use of monolingual data in both the source and the target language , together with word alignments that act as constraints on the joint clustering .", "We then showed that by using these cross lingual word clusters , we can significantly improve on direct transfer of discriminative models for both parsing and NER .", "As in the monolingual case , both types of cross lingual word cluster features yield improvements across the board , with the more complex method providing a significantly larger improvement for NER .", "Although the performance of transfer systems is still substantially below that of supervised systems , this research provides one step towards bridging this gap .", "Further , we believe that it opens up an avenue for future work on multilingual clustering methods , cross lingual feature projection and domain adaptation for direct transfer of linguistic structure .", "We thank John DeNero for help with creating the word alignments ; Reut Tsarfaty and Joakim Nivre for rewarding discussions on evaluation ; Slav Petrov and Kuzman Ganchev for discussions on cross lingual clustering ; and the anonymous reviewers , along with Joakim Nivre , for valuable comments that helped improve the paper .", "The first author is grateful for the financial support of the Swedish National Graduate School of Language Technology GSLT .", "In the parsing experiments , we use the following data sets .", "For DA , DE , EL , ES , IT , NL , PT and SV , we use the predefined training and evaluation data sets from the CoNLL 2006 2007 data sets Buchholz and Marsi , 2006 ; Nivre et al . , 2007 .", "For EN we use sections 02 21 , 22 , and 23 of the Penn WSJ Treebank Marcus et al . , 1993 for training , development and evaluation .", "For FR we used the French Treebank Abeill e and Barrier , 2004 with splits defined in Candito et al . 2010 .", "For KO we use the Sejong Korean Treebank Han et al . , 2002 randomly splitting the data into 80 training , 10 development and 10 evaluation .", "For RU we use the SynTagRus Treebank Boguslavsky et al . , 2000 ; Apresjan et al . , 2006 randomly splitting the data into 80 training , 10 development and 10 evaluation .", "For ZH we use the Penn Chinese Treebank v6 Xue et al . , 2005 using the proposed data splits from the documentation .", "Both EN and ZH were converted to dependencies using v1 . 6 . 8 of the Stanford Converter De Marneffe et al . , 2006 .", "FR was converted using the procedure defined in Candito et al . 2010 .", "RU and KO are native dependency treebanks .", "For the CoNLL data sets we use the part of speech tags provided with the data .", "For all other data sets , we train a part of speech tagger on the training data in order to tag the development and evaluation data .", "For the NER experiments we use the training , development and evaluation data sets from the CoNLL 2002 2003 shared tasks Tjong Kim Sang , 2002 ; Tjong Kim Sang and De Meulder , 2003 for all four languages DE , EN , ES and NL .", "The data set for each language consists of newswire text annotated with four entity categories Location LOC , Miscellaneous MISC , Organization ORG and Person PER .", "We use the part of speech tags supplied with the data , except for ES where we instead use universal part of speech tags Petrov et al . , 2011 .", "Unlabeled data for training the monolingual cluster models was extracted from one year of newswire articles from multiple sources from a news aggregation website .", "This consists of 0 . 8 billion DA to 121 . 6 billion EN tokens per language .", "All word alignments for the cross lingual clusterings were produced with the dual decomposition aligner described by DeNero and Macherey 2011 using 10 . 5 million DA to 12 . 1 million FR sentences of aligned web data ."], "summary_lines": ["Cross-lingual Word Clusters for Direct Transfer of Linguistic Structure\n", "It has been established that incorporating word cluster features derived from large unlabeled corpora can significantly improve prediction of linguistic structure.\n", "While previous work has focused primarily on English, we extend these results to other languages along two dimensions.\n", "First, we show that these results hold true for a number of languages across families.\n", "Second, and more interestingly, we provide an algorithm for inducing cross-lingual clusters and we show that features derived from these clusters significantly improve the accuracy of cross-lingual structure prediction.\n", "Specifically, we show that by augmenting direct-transfer systems with cross-lingual cluster features, the relative error of delexicalized dependency parsers, trained on English treebanks and transferred to foreign languages, can be reduced by up to 13%.\n", "When applying the same method to direct transfer of named-entity recognizers, we observe relative improvements of up to 26%.\n"]}
{"article_lines": ["Indexing With WordNet Synsets Can Improve Text Retrieval", "tem Experiments in Automatic Document Pro M . Sanderson .", "Word sense disambiguation information retrieval .", "In of 17th International Conference on Research and Development in Information Retrieval .", "A . F .", "Smeaton and A . Quigley .", "Experiments on using semantic distances between words in imcaption retrieval .", "Proceedings of the International Conference on Research and Development in IR .", "A . Smeaton , F . Kelledy , and R . O'Donnell .", "TREC 4 experiments at dublin city university Thresolding posting lists , query expansion with and POS tagging of spanish .", "In Proceedings of TREC 4 .", "M . Voorhees .", "Query relations .", "In of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval .", "Text retrieval deals with the problem of finding all the relevant documents in a text collection for a given user's query .", "A large scale semantic database such as WordNet Miller , 1990 seems to have a great potential for this task .", "There are , at least , two obvious reasons However , the general feeling within the information retrieval community is that dealing explicitly with semantic information does not improve significantly the performance of text retrieval systems .", "This impression is founded on the results of some experiments measuring the role of Word Sense Disambiguation WSD for text retrieval , on one hand , and some attempts to exploit the features of WordNet and other lexical databases , on the other hand .", "In Sanderson , 1994 , word sense ambiguity is shown to produce only minor effects on retrieval accuracy , apparently confirming that query document matching strategies already perform an implicit disambiguation .", "Sanderson also estimates that if explicit WSD is performed with less than 90 accuracy , the results are worse than non disambiguating at all .", "In his experimental setup , ambiguity is introduced artificially in the documents , substituting randomly chosen pairs of words for instance , banana and kalashmkov with artificially ambiguous terms banana kalashnikov .", "While his results are very interesting , it remains unclear , in our opinion , whether they would be corroborated with real occurrences of ambiguous words .", "There is also other minor weakness in Sanderson's experiments .", "When he quot ; disambiguates quot ; a term such as spring bank to get , for instance , bank , he has done only a partial disambiguation , as bank can be used in more than one sense in the text collection .", "Besides disambiguation . many attempts have been done to exploit WordNet for text retrieval purposes .", "Mainly two aspects have been addressed the enrichment of queries with semantically related terms , on one hand , and the comparison of queries and documents via conceptual distance measures , on the other .", "Query expansion with WordNet has shown to be potentially relevant to enhance recall , as it permits matching relevant documents that could not contain any of the query terms Smeaton et al . , 1995 .", "However , it has produced few successful experiments .", "For instance , Voorhees , 1994 manually expanded 50 queries over a TREC 1 collection Harman , 1993 using synonymy and other semantic relations from WordNet 1 . 3 .", "Voorhees found that the expansion was useful with short , incomplete queries , and rather useless for complete topic statements where other expansion techniques worked better .", "For short queries , it remained the problem of selecting the expansions automatically doing it badly could degrade retrieval performance rather than enhancing it .", "In Richardson and Smeaton , 1995 , a combination of rather sophisticated techniques based on WordNet , including automatic disambiguation and measures of semantic relatedness between query document concepts resulted in a drop of effectiveness .", "Unfortunately , the effects of WSD errors could not be discerned from the accuracy of the retrieval strategy .", "However , in Smeaton and Quigley , 1996 , retrieval on a small collection of image captions that is , on very short documents is reasonably improved using measures of conceptual distance between words based on WordNet 1 . 4 .", "Previously , captions and queries had been manually disambiguated against WordNet .", "The reason for such success is that with very short documents e . g . boys playing in the sand the chance of finding the original terms of the query e . g . of children running on a beach are much lower than for average size documents that typically include many phrasings for the same concepts .", "These results are in agreement with Voorhees , 1994 , but it remains the question of whether the conceptual distance matching would scale up to longer documents and queries .", "In addition , the experiments in _ Smeaton and Quigley , 1996 only consider nouns , while WordNet offers the chance to use all open class words nouns , verbs , adjectives and adverbs .", "Our essential retrieval strategy in the experiments reported here is to adapt a classical vector model based system , using WordNet synsets as indexing space instead of word forms .", "This approach combines two benefits for retrieval one , that terms are fully disambiguated this should improve precision ; and two , that equivalent terms can be identified this should improve recall .", "Note that query expansion does not satisfy the first condition , as the terms used to expand are words and , therefore , are in turn ambiguous .", "On the other hand , plain word sense disambiguation does not satisfy the second condition . as equivalent senses of two different words are not matched .", "Thus , indexing by synsets gets maximum matching and minimum spurious matching , seeming a good starting point to study text retrieval with WordNet .", "Given this approach , our goal is to test two main issues which are not clearly answered to our knowledge by the experiments mentioned above WSD .", "This paper reports on our first results answering these questions .", "The next section describes the test collection that we have produced .", "The experiments are described in Section 3 , and the last Section discusses the results obtained .", "The best known publicly available corpus handtagged with WordNet senses is SEMCOR Miller et al . , 1993 , a subset of the Brown Corpus of about 100 documents that occupies about 11 Mb .", "including tags The collection is rather heterogeneous , covering politics , sports , music , cinema , philosophy , excerpts from fiction novels , scientific texts . . . A new , bigger version has been made available recently Landes et al . , 1998 , but we have not still adapted it for our collection .", "We have adapted SEMCOR in order to build a test collection that we call IR SEMCOR in four manual steps ments , with lengths varying between 4 and 50 words and an average of 22 words per summary .", "Each summary is a human explanation of the text contents , not a mere bag of related keywords .", "These summaries serve as queries on the text collection , and then there is exactly one relevant document per query .", "We also generated a list of quot ; stop senses quot ; and a list of quot ; stop synsets quot ; , automatically translating a standard list of stop words for English .", "Such a test collection offers the chance to measure the adequacy of WordNet based approaches to IR independently from the disambiguator being used , but also offers the chance to measure the role of automatic disambiguation by introducing different rates of quot ; disambiguation errors quot ; in the collection .", "The only disadvantage is the small size of the collection , which does not allow fine grained distinctions in the results .", "However , it has proved large enough to give meaningful statistics for the experiments reported here .", "Although designed for our concrete text retrieval testing purposes , the resulting database could also be useful for many other tasks .", "For instance , it could be used to evaluate automatic summarization systems measuring the semantic relation between the manually written and hand tagged summaries of IRSEMCOR and the output of text summarization systems and other related tasks .", "We have performed a number of experiments using a standard vector model based text retrieval system , SmAFrr Salton , 1971 , and three different indexing spaces the original terms in the documents for standard SMART runs , the word senses corresponding to the document terms in other words , a manually disambiguated version of the documents and the WordNet synsets corresponding to the document terms roughly equivalent to concepts occurring in the documents .", "These are all the experiments considered here the file .", "In this case , it is a noun belonging to the noun . communication file .", "With this collection we can see if plain disambiguation is helpful for retrieval , because word senses are distinguished but synonymous word senses are not identified .", "quot ; argument , debatel quot ; a discussion in which reasons are advanced for and against some proposition or proposal ; quot ; the argument over foreign aid goes on and on quot ; This collection represents conceptual indexing , as equivalent word senses are represented with a unique identifier .", "We produced different versions of the synset indexed collection . introducing fixed percentages of erroneous synsets .", "Thus we simulated a word sense disambiguation process with 5 , 10 , 20 , 30 and 60 error rates .", "The errors were introduced randomly in the ambiguous words of each document .", "With this set of experiments we can measure the sensitivity of the retrieval process to disambiguation errors .", "In all cases , we compared at c and nnn standard weighting schemes , and they produced very similar results .", "Thus we only report here on the results for nnn weighting scheme .", "In Figure 1 we compare different indexing approaches indexing by synsets , indexing by words basic SMART and indexing by word senses experiments 1 , 2 and 3 .", "The leftmost point in each curve represents the percentage of documents that were successfully ranked as the most relevant for its summary query .", "The next point represents the documents retrieved as the first or the second most relevant to its summary query , and so on .", "Note that , as there is only one relevant document per query , the leftmost point is the most representative of each curve .", "Therefore , we have included this results separately in Table 1 .", "The results are encouraging documents , a 29 improvement with respect to SMART .", "This is an excellent result , although we should keep in mind that is obtained with manually disambiguated queries and documents .", "Nevertheless , it shows that WordNet can greatly enhance text retrieval the problem resides in achieving accurate automatic Word Sense Disambiguation .", "Indexing by word senses improves performance when considering up to four documents retrieved for each query summary , although it is worse than indexing by synsets .", "This confirms our intuition that synset indexing has advantages over plain word sense disambiguation , because it permits matching semantically similar terms .", "Taking only the first document retrieved for each summary , the disambiguated collection gives a 53 . 2 success against a 48 of the plain SN1ART query , which represents a 11 improvement .", "For recall levels higher than 0 . 85 , however , the disambiguated collection performs slightly worse .", "This may seem surprising , as word sense disambiguation should only increase our knowledge about queries and documents .", "But we should bear in mind that WordNet 1 . 5 is not the perfect database for text retrieval , and indexing by word senses prevents some matchings that can be useful for retrieval .", "For instance , design is used as a noun repeatedly in one of the documents , while its summary uses design as a verb .", "WordNet 1 . 5 does not include cross part of speech semantic relations , so this relation cannot be used with word senses , while term indexing simply and successfully ! does not distinguish them .", "Other problems of WordNet for text retrieval include too much finegrained sense distinctions and lack of domain information ; see Gonzalo et al . , In press for a more detailed discussion on the adequacy of WordNet structure for text retrieval .", "Figure 2 shows the sensitivity of the synset indexing system to degradation of disambiguation accuracy corresponding to the experiments 4 and 5 described above .", "From the plot , it can be seen that differs from Sanderson , 1994 result namely , that it is better not to disambiguate below a 90 accuracy .", "The main difference is that we are using concepts rather than word senses .", "But , in addition , it must be noted that Sanderson's setup used artificially created ambiguous pseudo words such as 'bank spring which are not guaranteed to behave as real ambiguous words .", "Moreover , what he understands as disambiguating is selecting in the example bank or spring which remain to be ambiguous words themselves .", "It is too soon to say if state of the art WSD techniques can perform with less than 30 errors , because each technique is evaluated in fairly different settings .", "Some of the best results on a comparable setting namely , disambiguating against WordNet , evaluating on a subset of the Brown Corpus , and treating the 191 most frequently occurring and ambiguous words of English are reported reported in Ng , 1997 .", "They reach a 58 . 7 accuracy on a Brown Corpus subset and a 75 . 2 on a subset of the Wall Street Journal Corpus .", "A more careful evaluation of the role of WSD is needed to know if this is good enough for our purposes .", "Anyway , we have only emulated a WSD algorithm that just picks up one sense and discards the rest .", "A more reasonable approach here could be giving different probabilities for each sense of a word , and use them to weight synsets in the vectorial representation of documents and queries .", "In Figure 3 we have plot the results of runs with a non disambiguated version of the queries , both for word sense indexing and synset indexing , against the manually disambiguated collection experiment 6 .", "The synset run performs approximately as the basic SMART run .", "It seems therefore useless to apply conceptual indexing if no disambiguation of the query is feasible .", "This is not a major problem in an interactive system that may help the user to disambiguate his query , but it must be taken into account if the process is not interactive and the query is too short to do reliable disambiguation .", "We have experimented with a retrieval approach based on indexing in terms of WordNet synsets instead of word forms , trying to address two questions 1 what potential does WordNet offer for text retrieval , abstracting from the problem of sense disambiguation , and 2 what is the sensitivity of retrieval performance to disambiguation errors .", "The answer to the first question is that indexing by synsets can be very helpful for text retrieval ; our experiments give up to a 29 improvement over a standard SMART run indexing with words .", "We believe that these results have to be further contrasted , but they strongly suggest that WordNet can be more useful to Text Retrieval than it was previously thought .", "The second question needs further , more finegrained , experiences to be clearly answered .", "However , for our test collection , we find that error rates below 30 still produce better results than standard word indexing , and that from 30 to 60 error rates , it does not behave worse than the standard SMART run .", "We also find that the queries have to be disambiguated to take advantage of the approach ; otherwise , the best possible results with synset indexing does not improve the performance of standard word indexing .", "Our first goal now is to improve our retrieval system in many ways . studying how to enrich the query with semantically related synsets , how to cornpare documents and queries using semantic information beyond the cosine measure , and how to obtain weights for synsets according to their position in the WordNet hierarchy , among other issues .", "A second goal is to apply synset indexing in a Cross Language environment , using the Euro WordNet multilingual database Gonzalo et al . , In press .", "Indexing by synsets offers a neat way of performing language independent retrieval , by mapping synsets into the EuroWordNet InterLingual Index that links monolingual wordnets for all the languages covered by EuroWordNet .", "This research is being supported by the European Community , project LE 4003 and also partially by the Spanish government , project TIC 96 1243 0O3 01 .", "We are indebted to Renee Pohlmann for giving us good pointers at an early stage of this work , and to AnseImo Peilas and David Fernandez for their help finishing up the test collection ."], "summary_lines": ["Indexing With WordNet Synsets Can Improve Text Retrieval\n", "The classical, vector space model for text retrieval is shown to give better results (up to 29% better in our experiments) if WordNet synsets are chosen as the indexing space, instead of word forms.\n", "This result is obtained for a manually disambiguated test collection (of queries and documents) derived from the SEMCOR semantic concordance.\n", "The sensitivity of retrieval performance to (automatic) disambiguation errors when indexing documents is also measured.\n", "Finally, it is observed that if queries are not disambiguated, indexing by synsets performs (at best) only as good as standard word indexing.\n", "We point out some more weaknesses of WordNet for Information Retrieval purposes, in particular the lack of domain information and the fact that sense distinctions are excessively fine-grained for the task.\n"]}
{"article_lines": ["Models For The Semantic Classification Of Noun Phrases", "Roles .", "In 28 3 .", "Relation no .", "1 2 3 6 7 11 13 15 16 21 25 the rest 0 . 06103 0 . 11268 0 . 00939 0 . 04225 0 . 39437 0 . 01878 0 . 03286 0 . 25822 0 . 04694 0 . 01878 0 . 00469 0 Table 5 Sample row from the conditional probability table where the feature pair is entity entity .", "The numbers in the top row identify the semantic relations as in Table 4 .", "Level Level 1 Level 2 Level 3 Level 4 Number of 9 52 70 122 features Number 9 46 47 47 features No . of feature pairs 57 out of 81 189 out of 2392 204 out of 3290 250 out of 5734 Number of 1 152 181 225 with only one relation Average number 2 . 7692 1 . 291 1 . 1765 1 . 144 non zero relations per line Table 6 Statistics for the semantic class features by level of specialization .", "This paper is about the automatic labeling of semantic relations in noun phrases NPs .", "The semantic relations are the underlying relations between two concepts expressed by words or phrases .", "We distinguish here between semantic relations and semantic roles .", "Semantic roles are always between verbs or nouns derived from verbs and other constituents run quickly , went to the store , computer maker , whereas semantic relations can occur between any constituents , for example in complex nominals malaria mosquito CAUSE , genitives girl s mouth PART WHOLE , prepositional phrases attached to nouns man at the store LOCATIVE , or discourse level The bus was late .", "As a result , I missed my appointment CAUSE .", "Thus , in a sense , semantic relations are more general than semantic roles and many semantic role types will appear on our list of semantic relations .", "The following NP level constructions are considered here cf . the classifications provided by Quirk et al . 1985 and Semmelmeyer and Bolander 1992 1 Compound Nominals consisting of two consecutive nouns eg night club a TEMPORAL relation indicating that club functions at night , 2 Adjective Noun constructions where the adjectival modifier is derived from a noun eg musical clock a MAKE PRODUCE relation , 3 Genitives eg the door of the car a PART WHOLE relation , and 4 Adjective phrases cf .", "Semmelmeyer and Bolander 1992 in which the modifier noun is expressed by a prepositional phrase which functions as an adjective eg toy in the box a LOCATION relation .", "Example Saturday s snowfall topped a one day record in Hartford , Connecticut , with the total of 12 . 5 inches , the weather service said .", "The storm claimed its fatality Thursday , when a car which was driven by a college student skidded on an interstate overpass in the mountains of Virginia and hit a concrete barrier , police said .", "www . cnn . com Record setting Northeast snowstorm winding down , Sunday , December 7 , 2003 .", "There are several semantic relations at the noun phrase level 1 Saturday s snowfall is a genitive encoding a TEMPORAL relation , 2 one day record is a TOPIC noun compound indicating that record is about one day snowing an ellipsis here , 3 record in Hartford is an adjective phrase in a LOCATION relation , 4 total of 12 . 5 inches is an of genitive that expresses MEASURE , 5 weather service is a noun compound in a TOPIC relation , 6 car which was driven by a college student encodes a THEME semantic role in an adjectival clause , 7 college student is a compound nominal in a PART WHOLE MEMBER OF relation , 8 interstate overpass is a LOCATION noun compound , 9 mountains of Virginia is an of genitive showing a PART WHOLE PLACE AREA and LOCATION relation , 10 concrete barrier is a noun compound encoding PART WHOLE STUFF OF .", "After many iterations over a period of time we identified a set of semantic relations that cover a large majority of text semantics .", "Table 1 lists these relations , their definitions , examples , and some references .", "Most of the time , the semantic relations are encoded by lexico syntactic patterns that are highly ambiguous .", "One pattern can express a number of semantic relations , its disambiguation being provided by the context or world knowledge .", "Often semantic relations are not disjoint or mutually exclusive , two or more appearing in the same lexical construct .", "This is called semantic blend Quirk et al . 1985 .", "For example , the expression Texas city contains both a LOCATION as well as a PART WHOLE relation .", "Other researchers have identified other sets of semantic relations Levi 1979 , Uanderwende 1994 , Sowa 1994 , Baker , Fillmore , and Lowe 1998 , Rosario and Hearst 2001 , Kingsbury , et al . 2002 , Blaheta and Charniak 2000 , Gildea and Jurafsky 2002 , Gildea and Palmer 2002 .", "Our list contains the most frequently used semantic relations we have observed on a large corpus .", "Besides the work on semantic roles , considerable interest has been shown in the automatic interpretation of complex nominals , and especially of compound nominals .", "The focus here is to determine the semantic relations that hold between different concepts within the same phrase , and to analyze the meaning of these compounds .", "Several approaches have been proposed for empirical noun compound interpretation , such as syntactic analysis based on statistical techniques Lauer and Dras 1994 , Pustejovsky et al . 1993 .", "Another popular approach focuses on the interpretation of the underlying semantics .", "Many researchers that followed this approach relied mostly on hand coded rules Finin 1980 , Uanderwende 1994 .", "More recently , Rosario and Hearst 2001 , Rosario , Hearst , and Fillmore 2002 , Lapata 2002 have proposed automatic methods that analyze and detect noun compounds relations from text .", "Rosario and Hearst 2001 focused on the medical domain making use of a lexical ontology and standard machine learning techniques .", "We approach the problem top down , namely identify and study first the characteristics or feature vectors of each noun phrase linguistic pattern , then develop models for their semantic classification .", "This is in contrast to our prior approach Girju , Badulescu , and Moldovan 2003a when we studied one relation at a time , and learned constraints to identify only that relation .", "We study the distribution of the semantic relations across different NP patterns and analyze the similarities and differences among resulting semantic spaces .", "We define a semantic space as the set of semantic relations an NP construction can encode .", "We aim at uncovering the general aspects that govern the NP semantics , and thus delineate the semantic space within clusters of semantic relations .", "This process has the advantage of reducing the annotation effort , a time consuming activity .", "Instead of manually annotating a corpus for each semantic relation , we do it only for each syntactic pattern and get a clear view of its semantic space .", "This syntactico semantic approach allows us to explore various NP semantic classification models in a unified way .", "This approach stemmed from our desire to answer questions such as It is well understood and agreed in linguistics that concepts can be represented in many ways using various constructions at different syntactic levels .", "This is in part why we decided to take the syntactico semantic approach that analyzes semantic relations at different syntactic levels of representation .", "In this paper we focus only on the behavior of semantic relations at NP level .", "A thorough understanding of the syntactic and semantic characteristics of NPs provides valuable insights into defining the most representative feature vectors that ultimately drive the discriminating learning models .", "Levi Levi 1979 defines complex nominals CNs as expressions that have a head noun preceded by one or more modifying nouns , or by adjectives derived from nouns usually called denominal adjectives .", "Most importantly for us , each sequence of nouns , or possibly adjectives and nouns , has a particular meaning as a whole carrying an implicit semantic relation ; for example , spoon handle PART WHOLE or musical clock MAKE PRODUCE .", "CNs have been studied intensively in linguistics , psycho linguistics , philosophy , and computational linguistics for a long time .", "The semantic interpretation of CNs proves to be very difficult for a number of reasons .", "1 Sometimes the meaning changes with the head eg musical clock MAKE PRODUCE , musical creation THEME , other times with the modifier eg GM car MAKE PRODUCE , family car POSSESSION .", "2 CNs interpretation is knowledge intensive and can be idiosyncratic .", "For example , in order to interpret correctly GM car we have to know that GM is a car producing company .", "3 There can be many possible semantic relations between a given pair of word constituents .", "For example , USA city can be regarded as a LOCATION as well as a PART WHOLE relation .", "4 Interpretation of CNs can be highly context dependent .", "For example , apple juice seat can be defined as seat with apple juice on the table in front of it cf .", "Downing 1977 .", "The semantic interpretation of genitive constructions is considered problematic by linguists because they involve an implicit relation that seems to allow for a large variety of relational interpretations ; for example John s car POSSESSOR POSSESSEE , Mary s brother KINSHIP , last year s exhibition TEMPORAL , a picture of my nice DEPICTION DEPICTED , and the desert s oasis PART WHOLE PLACE AREA .", "A characteristic of these constructions is that they are very productive , as the construction can be given various interpretations depending on the context .", "One such example is Kate s book that can mean the book Kate owns , the book Kate wrote , or the book Kate is very fond of .", "Thus , the features that contribute to the semantic interpretation of genitives are the nouns semantic classes , the type of genitives , discourse and pragmatic information .", "Adjective Phrases are prepositional phrases attached to nouns acting as adjectives cf .", "Semmelmeyer and Bolander 1992 .", "Prepositions play an important role both syntactically and semantically .", "Semantically speaking , prepositional constructions can encode various semantic relations , their interpretations being provided most of the time by the underlying context .", "For instance , the preposition with can encode different semantic relations 1 It was the girl with blue eyes MERONYMY , The conclusion for us is that in addition to the nouns semantic classes , the preposition and the context play important roles here .", "In order to focus our research , we will concentrate for now only on noun noun or adjective noun compositional constructions at NP level , ie those whose meaning can be derived from the meaning of the constituent nouns door knob , cup of wine .", "We don t consider metaphorical names eg , ladyfinger , metonymies eg , Vietnam veteran , proper names eg , John Doe , and NPs with coordinate structures in which neither noun is the head eg , player coach .", "However , we check if the constructions are non compositional lexicalized the meaning is a matter of convention ; e . g . , soap opera , sea lion , but only for statistical purposes .", "Fortunately , some of these can be identified with the help of lexicons .", "In order to provide a unified approach for the detection of semantic relations at different NP levels , we analyzed the syntactic and semantic behavior of these constructions on a large open domain corpora of examples .", "Our intention is to answer questions like 1 What are the semantic relations encoded by the NP level constructions ? , 2 What is their distribution on a large corpus ? , 3 Is there a common subset of semantic relations that can be fully paraphrased by all types ofNP constructions ? , 4 How many NPs are lexicalized ?", "We have assembled a corpus from two sources Wall Street Journal articles from TREC 9 , and eXtended WordNet glosses XWN http xwn . hlt . utdallas . edu .", "We used XWN 2 . 0 since all its glosses are syntactically parsed and their words semantically disambiguated which saved us considerable amount of time .", "Table 2 shows for each syntactic category the number of randomly selected sentences from each corpus , the number of instances found in these sentences , and finally the number of instances that our group managed to annotate by hand .", "The annotation of each example consisted of specifying its feature vector and the most appropriate semantic relation from those listed in Table 1 .", "The annotators , four PhD students in Computational Semantics worked in groups of two , each group focusing on one half of the corpora to annotate .", "Noun noun adjective noun , respectively sequences of words were extracted using the Lauer heuristic Lauer 1995 which looks for consecutive pairs of nouns that are neither preceded nor succeeded by a noun after each sentence was syntactically parsed with Charniak parser Charniak 2001 for XWN we used the gold parse trees .", "Moreover , they were provided with the sentence in which the pairs occurred along with their corresponding WordNet senses .", "Whenever the annotators found an example encoding a semantic relation other than those provided or they didn t know what interpretation to give , they had to tag it as OTHERS .", "Besides the type of relation , the annotators were asked to provide information about the order of the modifier and the head nouns in the syntactic constructions if applicable .", "For instance , in owner of car POSSESSION the possessor owner is followed by the possessee car , while in car ofJohn POSSESSION R the order is reversed .", "On average , 30 of the training examples had the nouns in reverse order .", "Most of the time , one instance was tagged with one semantic relation , but there were also situations in which an example could belong to more than one relation in the same context .", "For example , the genitive city of USA was tagged as a PART WHOLE PLACE AREA relation and as a LOCATION relation .", "Overall , there were 608 such cases in the training corpora .", "Moreover , the annotators were asked to indicate if the instance was lexicalized or not .", "Also , the judges tagged the NP nouns in the training corpus with their corresponding WordNet senses .", "The annotators agreement was measured using the Kappa statistics , one of the most frequently used measure of inter annotator agreement for classification tasks , where is the proportion of times the raters agree and is the probability of agreement by chance .", "The K coefficient is 1 if there is a total agreement among the annotators , and 0 if there is no agreement other than that expected to occur by chance .", "Table 3 shows the semantic relations inter annotator agreement on both training and test corpora for each NP construction .", "For each construction , the corpus was splint into 80 20 training testing ratio after agreement .", "We computed the K coefficient only for those instances tagged with one of the 35 semantic relations .", "For each pattern , we also computed the number of pairs that were tagged with OTHERS by both annotators , over the number of examples classified in this category by at least one of the judges , averaged by the number of patterns considered .", "The K coefficient shows a fair to good level of agreement for the training and testing data on the set of 35 relations , taking into consideration the task difficulty .", "This can be explained by the instructions the annotators received prior to annotation and by their expertise in lexical semantics .", "There were many heated discussions as well .", "Even noun phrase constructions are very productive allowing for a large number of possible interpretations , Table 4 shows that a relatively small set of 35 semantic relations covers a significant part of the semantic distribution of these constructions on a large open domain corpus .", "Moreover , the distribution of these relations is dependent on the type of NP construction , each type encoding a particular subset .", "For example , in the case of of genitives , there were 21 relations found from the total of 35 relations considered .", "The most frequently occurring relations were PART WHOLE , ATTRIBUTE HOLDER , POSSESSION , LOCATION , SOURCE , TOPIC , and THEME .", "By comparing the subsets of semantic relations in each column we can notice that these semantic spaces are not identical , proving our initial intuition that the NP constructions cannot be alternative ways of packing the same information .", "Table 4 also shows that there is a subset of semantic relations that can be fully encoded by all types of NP constructions .", "The statistics about the lexicalized examples are as follows N N 30 . 01 , Adj N 0 , s genitive 0 , of genitive 0 , adjective phrase 1 .", "From the 30 . 01 lexicalized noun compounds , 18 were proper names .", "This simple analysis leads to the important conclusion that the NP constructions must be treated separately as their semantic content is different .", "This observation is also partially consistent with other recent work in linguistics and computational linguistics on the grammatical variation of the English genitives , noun compounds , and adjective phrases .", "We can draw from here the following conclusions Given each NP syntactic construction considered , the goal is to develop a procedure for the automatic labeling of the semantic relations they encode .", "The semantic relation derives from the lexical , syntactic , semantic and contextual features of each NP construction .", "Semantic classification of syntactic patterns in general can be formulated as a learning problem , and thus benefit from the theoretical foundation and experience gained with various learning paradigms .", "This is a multi class classification problem since the output can be one of the semantic relations in the set .", "We cast this as a supervised learning problem where input output pairs are available as training data .", "An important first step is to map the characteristics of each NP construction usually not numerical into feature vectors .", "Let s define with the feature vector of an instance and let be the space of all instances ; ie .", "The multi class classification is performed by a function that maps the feature space into a semantic space , , where is the set of semantic relations from Table 1 , ie .", "Let be the training set of examples or instances where is the number of examples each accompanied by its semantic relation label .", "The problem is to decide which semantic relation to assign to a new , unseen example .", "In order to classify a given set of examples members of , one needs some kind of measure of the similarity or the difference between any two given members of .", "Most of the times it is difficult to explicitly define this function , since can contain features with numerical as well as non numerical values .", "Note that the features , thus space , vary from an NP pattern to another and the classification function will be pattern dependent .", "The novelty of this learning problem is the feature space and the nature ofthe discriminating An essential aspect of our approach below is the word sense disambiguation WSD of the content words nouns , verbs , adjectives and adverbs .", "Using a stateof the art open text WSD system , each word is mapped into its corresponding WordNet 2 . 0 sense .", "When disambiguating each word , the WSD algorithm takes into account the surrounding words , and this is one important way through which context gets to play a role in the semantic classification of NPs .", "So far , we have identified and experimented with the following NP features specifies the WordNet synset of the modifier noun .", "In case the modifier is a denominal adjective , we take the synset of the noun from which the adjective is derived .", "Example musical clock MAKE PRODUCE , and electric clock INSTRUMENT .", "Several learning models can be used to provide the discriminating function .", "So far we have experimented with three models 1 semantic scattering , 2 decision trees , and 3 naive Bayes .", "The first is described below , the other two are fairly well known from the machine learning literature .", "Semantic Scattering .", "This is a new model developed by us particularly useful for the classification of compound nominals without nominalization .", "The semantic relation in this case derives from the semantics of the two noun concepts participating in these constructions as well as the surrounding context .", "Model Formulation .", "Let us define with and the sets of semantic class features ie , function derived for each syntactic pattern .", "WordNet synsets of the NP modifiers and , respectively NP heads ie features 2 and 1 .", "The compound nominal semantics is distinctly specified by the feature pair , written shortly as .", "Given feature pair , the probability of a semantic relation r is , defined as the ratio between the number of occurrences of a relation r in the presence of feature pair over the number of occurrences of feature pair in the corpus .", "The most probable relation is Since the number of possible noun synsets combinations is large , it is difficult to measure the quantities and on a training corpus to calculate .", "One way of approximating the feature vector is to perform a semantic generalization , by replacing the synsets with their most general hypernyms , followed by a series of specializations for the purpose of eliminating ambiguities in the training data .", "There are 9 noun hierarchies , thus only 81 possible combinations at the most general level .", "Table 5 shows a row of the probability matrix for .", "Each entry , for which there is more than one relation , is scattered into other subclasses through an iterative process till there is only one semantic relation per line .", "This can be achieved by specializing the feature pair s semantic classes with their immediate WordNet hyponyms .", "The iterative process stops when new training data does not bring any improvements see Table 6 .", "The f measure results obtained so far are summarized in Table 7 .", "Overall , these results are very encouraging given the complexity of the problem .", "An important way of improving the performance of a system is to do a detailed error analysis of the results .", "We have analyzed the sources of errors in each case and found out that most of them are due to in decreasing order of importance 1 errors in automatic sense disambiguation , 2 missing combinations of features that occur in testing but not in the training data , 3 levels of specialization are too high , 4 errors caused by metonymy , 6 errors in the modifier head order , and others .", "These errors could be substantially decreased with more research effort .", "A further analysis of the data led us to consider a different criterion of classification that splits the examples into nominalizations and non nominalizations .", "The reason is that nominalization noun phrases seem to call for a different set of learning features than the non nominalization noun phrases , taking advantage of the underlying verbargument structure .", "Details about this approach are provided in Girju et al . 2004 .", "Semantic relations occur with high frequency in open text , and thus , their discovery is paramount for many applications .", "One important application is Question Answering .", "A powerful method of answering more difficult questions is to associate to each question the semantic relation that reflects the meaning of that question and then search for that semantic relation over the candidates of semantically tagged paragraphs .", "Here is an example .", "Q .", "Where have nuclear incidents occurred ?", "From the question stem word where , we know the question asks for a LOCATION which is found in the complex nominal Three Mile Island LOCATION of the sentence The Three Mile Island nuclear incident caused a DOE policy crisis , leading to the correct answer Three Mile Island .", "Q .", "What did the factory in Howell Michigan make ?", "The verb make tells us to look for a MAKE PRODUCE relation which is found in the complex nominal car factory MAKE PRODUCE of the text The car factory in Howell Michigan closed on Dec 22 , 1991 which leads to answer car .", "Another important application is building semantically rich ontologies .", "Last but not least , the discovery of text semantic relations can improve syntactic parsing and even WSD which in turn affects directly the accuracy of other NLP modules and applications .", "We consider these applications for future work ."], "summary_lines": ["Models For The Semantic Classification Of Noun Phrases\n", "This paper presents an approach for detecting semantic relations in noun phrases.\n", "A learning algorithm, called semantic scattering, is used to automatically label complex nominals, genitives and adjectival noun phrases with the corresponding semantic relation.\n", "We propose a 35 class scheme to classify relations in various phrases.\n", "We propose a method called semantic scattering for interpreting NCs.\n"]}
{"article_lines": ["Dependency Parsing and Domain Adaptation with LR Models and Parser Ensembles", "We present a data driven variant of the LR algorithm for dependency parsing , and extend it with a best first search for probabil istic generalized LR dependency parsing .", "Parser actions are determined by a classifier , based on features that represent the current state of the parser .", "We apply this pars ing framework to both tracks of the CoNLL 2007 shared task , in each case taking ad vantage of multiple models trained with different learners .", "In the multilingual track , we train three LR models for each of the ten languages , and combine the analyses obtained with each individual model with a maximum spanning tree voting scheme .", "In the domain adaptation track , we use two models to parse unlabeled data in the target domain to supplement the labeled out of domain training set , in a scheme similar to one iteration of co training .", "There are now several approaches for multilingual dependency parsing , as demonstrated in the CoNLL 2006 shared task Buchholz and Marsi , 2006 .", "The dependency parsing approach pre sented here extends the existing body of work mainly in four ways 1 .", "Although stepwise 1 dependency parsing has .", "commonly been performed using parsing algo 1 Stepwise parsing considers each step in a parsing algo rithm separately , while all pairs parsing considers entire rithms designed specifically for this task , such as those described by Nivre 2003 and Yamada and Matsumoto 2003 , we show that this can also be done using the well known LR parsing algorithm Knuth , 1965 , providing a connec tion between current research on shift reduce dependency parsing and previous parsing work using LR and GLR models ; wise framework to probabilistic parsing , with the use of a best first search strategy similar to the one employed in constituent parsing by Rat naparkhi 1997 and later by Sagae and Lavie 2006 ; 3 .", "We provide additional evidence that the parser .", "ensemble approach proposed by Sagae and Lavie 2006a can be used to improve parsing accuracy , even when only a single parsing algorithm is used , as long as variation can be ob tained , for example , by using different learning techniques or changing parsing direction from forward to backward of course , even greater gains may be achieved when different algo rithms are used , although this is not pursued here ; and , finally , 4 .", "We present a straightforward way to perform .", "parser domain adaptation using unlabeled data in the target domain .", "We entered a system based on the approach de scribed in this paper in the CoNLL 2007 shared trees .", "For a more complete definition , see the CoNLL X shared task description paper Buchholz and Marsi , 2006 .", "1044 task Nivre et al , 2007 , which differed from the 2006 edition by featuring two separate tracks , one in multilingual parsing , and a new track on domain adaptation for dependency parsers .", "In the multi lingual parsing track , participants train dependency parsers using treebanks provided for ten languages Arabic Hajic et al , 2004 , Basque Aduriz et al 2003 , Catalan Mart ?", "et al , 2007 , Chinese Chen et al , 2003 , Czech B ? hmova et al , 2003 , Eng lish Marcus et al , 1993 ; Johansson and Nugues , 2007 , Greek Prokopidis et al , 2005 , Hungarian Czendes et al , 2005 , Italian Montemagni et al , 2003 , and Turkish Oflazer et al , 2003 .", "In the domain adaptation track , participants were pro vided with English training data from the Wall Street Journal portion of the Penn Treebank Marcus et al , 1993 converted to dependencies Jo hansson and Nugues , 2007 to train parsers to be evaluated on material in the biological develop ment set and chemical test set domains Kulick et al , 2004 , and optionally on text from the CHILDES database MacWhinney , 2000 ; Brown , 1973 .", "Our system ? s accuracy was the highest in the domain adaptation track with labeled attachment score of 81 . 06 , and only 0 . 43 below the top scoring system in the multilingual parsing track our average labeled attachment score over the ten languages was 79 . 89 .", "We first describe our approach to multilingual dependency parsing , fol lowed by our approach for domain adaptation .", "We then provide an analysis of the results obtained with our system , and discuss possible improve ments .", "pendency Parsing Our overall parsing approach uses a best first probabilistic shift reduce algorithm based on the LR algorithm Knuth , 1965 .", "As such , it follows a bottom up strategy , or bottom up trees , as defined in Buchholz and Marsi 2006 , in contrast to the shift reduce dependency parsing algorithm described by Nivre 2003 , which is a bottom up top down hybrid , or bottom up spans .", "It is unclear whether the use of a bottom up trees algorithm has any advantage over the use of a bottom up spans algorithm or vice versa in practice , but the avail ability of different algorithms that perform the same parsing task could be advantageous in parser ensembles .", "The main difference between our pars er and a traditional LR parser is that we do not use an LR table derived from an explicit grammar to determine shift reduce actions .", "Instead , we use a classifier with features derived from much of the same information contained in an LR table the top few items on the stack , and the next few items of lookahead in the remaining input string .", "Addition ally , following Sagae and Lavie 2006 , we extend the basic deterministic LR algorithm with a bestfirst search , which results in a parsing strategy sim ilar to generalized LR parsing Tomita , 1987 ; 1990 , except that we do not perform Tomita ? s stack merging operations .", "The resulting algorithm is projective , and nonprojectivity is handled by pseudo projective trans formations as described in Nivre and Nilsson , 2005 .", "We use Nivre and Nilsson ? s PATH scheme2 .", "For clarity , we first describe the basic variant of the LR algorithm for dependency parsing , which is a deterministic stepwise algorithm .", "We then show how we extend the deterministic parser into a best first probabilistic parser .", "2 . 1 Dependency Parsing with a Data Driven .", "Variant of the LR Algorithm The two main data structures in the algorithm are a stack S and a queue Q . S holds subtrees of the fi nal dependency tree for an input sentence , and Q holds the words in an input sentence .", "S is initia lized to be empty , and Q is initialized to hold every word in the input in order , so that the first word in the input is in the front of the queue . 3 The parser performs two main types of actions shift and reduce .", "When a shift action is taken , a word is shifted from the front of Q , and placed on the top of S as a tree containing only one node , the word itself .", "When a reduce action is taken , the 2 The PATH scheme was chosen even though Nivre and Nilsson report slightly better results with the HEAD scheme because it does not result in a potentially qua dratic increase in the number of dependency label types , as observed with the HEAD and HEAD PATH schemes .", "Unfortunately , experiments comparing the use of the different pseudo projectivity schemes were not performed due to time constraints .", "3 We append a ? virtual root ?", "word to the beginning of every sentence , which is used as the head of every word in the dependency structure that does not have a head in the sentence .", "1045 two top items in S s1 and s2 are popped , and a new item is pushed onto S . This new item is a tree formed by making the root s1 of a dependent of the root of s2 , or the root of s2 a dependent of the root of s1 .", "Depending on which of these two cases oc cur , we call the action reduce left or reduce right , according to whether the head of the new tree is to the left or to the right its new dependent .", "In addi tion to deciding the direction of a reduce action , the label of the newly formed dependency arc must also be decided .", "Parsing terminates successfully when Q is emp ty all words in the input have been processed and S contains only a single tree the final dependency tree for the input sentence .", "If Q is empty , S contains two or more items , and no further reduce ac tions can be taken , parsing terminates and the input is rejected .", "In such cases , the remaining items in S contain partial analyses for contiguous segments of the input .", "2 . 2 A Probabilistic LR Model for Dependen cy Parsing In the traditional LR algorithm , parser states are placed onto the stack , and an LR table is consulted to determine the next parser action .", "In our case , the parser state is encoded as a set of features de rived from the contents of the stack S and queue Q , and the next parser action is determined according to that set of features .", "In the deterministic case described above , the procedure used for determin ing parser actions a classifier , in our case returns a single action .", "If , instead , this procedure returns a list of several possible actions with corresponding probabilities , we can then parse with a model simi lar to the probabilistic LR models described by Briscoe and Carroll 1993 , where the probability of a parse tree is the product of the probabilities of each of the actions taken in its derivation .", "To find the most probable parse tree according to the probabilistic LR model , we use a best first strategy .", "This involves an extension of the deter ministic shift reduce into a best first shift reduce algorithm .", "To describe this extension , we first in troduce a new data structure Ti that represents a parser state , which includes a stack Si , a queue Qi , and a probability Pi .", "The deterministic algorithm is a special case of the probabilistic algorithm where we have a single parser state T0 that contains S0 and Q0 , and the probability of the parser state is 1 .", "The best first algorithm , on the other hand , .", "keeps a heap H containing multiple parser states T0 . . .", "Tm .", "These states are ordered in the heap ac cording to their probabilities , which are determined by multiplying the probabilities of each of the parser actions that resulted in that parser state .", "The heap H is initialized to contain a single parser state T0 , which contains a stack S0 , a queue Q0 and prob ability P0 1 . 0 .", "S0 and Q0 are initialized in the same way as S and Q in the deterministic algo rithm .", "The best first algorithm then loops while H is non empty .", "At each iteration , first a state Tcurrent is popped from the top of H . If Tcurrent corresponds to a final state Qcurrent is empty and Scurrent contains a single item , we return the single item in Scurrent as the dependency structure corresponding to the input sentence .", "Otherwise , we get a list of parser actions act0 . . . actn with associated probabilities Pact0 . . . Pactn corresponding to state Tcurrent .", "For each of these parser actions actj , we create a new parser state Tnew by applying actj to Tcurrent , and set the probability Tnew to be Pnew Pcurrnet Pactj .", "Then , Tnew is inserted into the heap H . Once new states have been inserted onto H for each of the n parser actions , we move on to the next iteration of the algorithm .", "For each of the ten languages for which training data was provided in the multilingual track of the CoNLL 2007 shared task , we trained three LR models as follows .", "The first LR model for each language uses maximum entropy classification Berger et al , 1996 to determine possible parser actions and their probabilities4 .", "To control overfitting in the MaxEnt models , we used box type in equality constraints Kazama and Tsujii , 2003 .", "The second LR model for each language also uses MaxEnt classification , but parsing is performed backwards , which is accomplished simply by reversing the input string before parsing starts .", "Sa gae and Lavie 2006a and Zeman and ? abokrtsk ?", "2005 have observed that reversing the direction of stepwise parsers can be beneficial in parser combinations .", "The third model uses support vector machines 5 Vapnik , 1995 using the polynomial 4 Implementation by Yoshimasa Tsuruoka , available at http www tsujii . is . s . u tokyo . ac . jp tsuruoka maxent 5 Implementation by Taku Kudo , available at http chasen . org taku software TinySVM and all vs . all was used for multi class classification .", "1046 kernel with degree 2 .", "Probabilities were estimated for SVM outputs using the method described in Platt , 1999 , but accuracy improvements were not observed during development when these esti mated probabilities were used instead of simply the single best action given by the classifier with probability 1 . 0 , so in practice the SVM parsing models we used were deterministic .", "At test time , each input sentence is parsed using each of the three LR models , and the three result ing dependency structures are combined according to the maximum spanning tree parser combination scheme6 Sagae and Lavie , 2006a where each de pendency proposed by each of the models has the same weight it is possible that one of the more sophisticated weighting schemes proposed by Sa gae and Lavie may be more effective , but these were not attempted .", "The combined dependency tree is the final analysis for the input sentence .", "Although it is clear that fine tuning could provide accuracy improvements for each of the models in each language , the same set of meta parameters and features were used for all of the ten languages , due to time constraints during system development .", "The features used were7 ? For the subtrees in S 1 and S 2 ? the number of children of the root word of the subtrees ; ? the number of children of the root word of the subtree to the right of the root word ; ? the number of children of the root word of the subtree to the left of the root word ; ? the POS tag and DEPREL of the rightmost and leftmost children ; ? The POS tag of the word immediately to the right of the root word of S 2 ; ? The POS tag of the word immediately to the left of S 1 ; 6 Each dependency tree is deprojectivized before the combination occurs .", "7 S n denotes the nth item from the top of the stack where S 1 is the item on top of the stack , and Q n denotes the nth item in the queue .", "For a description of the features names in capital letters , see the shared task description Nivre et al , 2007 .", "The previous parser action ; ? The features listed for the root words of the subtrees in table 1 .", "In addition , the MaxEnt models also used selected combinations of these features .", "The classes used to represent parser actions were designed to encode all aspects of an action shift vs . reduce , right vs . left , and dependency label simultaneously .", "Results for each of the ten languages are shown in table 2 as labeled and unlabeled attachment scores , along with the average labeled attachment score and highest labeled attachment score for all participants in the shared task .", "Our results shown in boldface were among the top three scores for those particular languages five out of the ten lan guages .", "S 1 S 2 S 3 Q 0 Q 1 Q 3 WORD x x x x x LEMMA x x x POS x x x x x x CPOS x x x FEATS x x x Table 1 Additional features .", "Language LAS UAS Avg LAS Top LAS Arabic 74 . 71 84 . 04 68 . 34 76 . 52 Basque 74 . 64 81 . 19 68 . 06 76 . 94 Catalan 88 . 16 93 . 34 79 . 85 88 . 70 Chinese 84 . 69 88 . 94 76 . 59 84 . 69 Czech 74 . 83 81 . 27 70 . 12 80 . 19 English 89 . 01 89 . 87 80 . 95 89 . 61 Greek 73 . 58 80 . 37 70 . 22 76 . 31 Hungarian 79 . 53 83 . 51 71 . 49 80 . 27 Italian 83 . 91 87 . 68 78 . 06 84 . 40 Turkish 75 . 91 82 . 72 70 . 06 79 . 81 ALL 79 . 90 85 . 29 65 . 50 80 . 32 Table 2 Multilingual results .", "In a similar way as we used multiple LR models in the multilingual track , in the domain adaptation track we first trained two LR models on the out of 1047domain labeled training data .", "The first was a forward MaxEnt model , and the second was a back ward SVM model .", "We used these two models to perform a procedure similar to a single iteration of co training , except that selection of the newly au tomatically produced training instances was done by selecting sentences for which the two models produced identical analyses .", "On the development data we verified that sentences for which there was perfect agreement between the two models had labeled attachment score just above 90 on average , even though each of the models had accuracy be tween 78 and 79 over the entire development set .", "Our approach was as follows 1 .", "We trained the forward MaxEnt and backward .", "SVM models using the out of domain labeled training data ; 2 .", "We then used each of the models to parse the .", "first two of the three sets of domain specific unlabeled data that were provided we did not use the larger third set 3 .", "We compared the output for the two models , .", "and selected only identical analyses that were produced by each of the two separate models ; 4 .", "We added those analyses about 200k words in .", "the test domain to the original out of domain labeled training set ; the new larger training set ; and finally 6 .", "We used this model to parse the test data . .", "Following this procedure we obtained a labeled attachment score of 81 . 06 , and unlabeled attach ment score of 83 . 42 , both the highest scores for this track .", "This was done without the use of any additional resources closed track , but these re sults are also higher than the top score for the open track , where the use of certain additional resources was allowed .", "See Nivre et al , 2007 .", "One of the main assumptions in our use of differ ent models based on the same algorithm is that while the output generated by those models may often differ , agreement between the models is an indication of correctness .", "In our domain adapta tion approach , this was clearly true .", "In fact , the approach would not have worked if this assump tion was false .", "Experiments on the development set were encouraging .", "As stated before , when the parsers agreed , labeled attachment score was over 90 , even though the score of each model alone was lower than 79 .", "The domain adapted parser had a score of 82 . 1 , a significant improvement .", "Interes tingly , the ensemble used in the multilingual track also produced good results on the development set for the domain adaptation data , without the use of the unlabeled data at all , with a score of 81 . 9 al though the ensemble is more expensive to run .", "The different models used in each track were distinct in a few ways 1 direction forward or backward ; 2 learner MaxEnt or SVM ; and 3 search strategy best first or deterministic .", "Of those differences , the first one is particularly inter esting in single stack shift reduce models , as ours .", "In these models , the context to each side of a po tential dependency differs in a fundamental way .", "To one side , we have tokens that have already been processed and are already in subtrees , and to the other side we simply have a look ahead of the re maining input sentence .", "This way , the context of the same dependency in a forward parser may differ significantly from the context of the same de pendency in a backward parser .", "Interestingly , the accuracy scores of the MaxEnt backward models were found to be generally just below the accuracy of their corresponding forward models when tested on development data , with two exceptions Hunga rian and Turkish .", "In Hungarian , the accuracy scores produced by the forward and backward MaxEnt LR models were not significantly differ ent , with both labeled attachment scores at about 77 . 3 the SVM model score was 76 . 1 , and the final combination score on development data was 79 . 3 .", "In Turkish , however , the backward score was sig nificantly higher than the forward score , 75 . 0 and 72 . 3 , respectively .", "The forward SVM score was 73 . 1 , and the combined score was 75 . 8 .", "In expe riments performed after the official submission of results , we evaluated a backward SVM model which was trained after submission on the same development set , and found it to be significantly more accurate than the forward model , with a score of 75 . 7 .", "Adding that score to the combination raised the combination score to 77 . 9 a large im provement from 75 . 8 .", "The likely reason for this difference is that over 80 of the dependencies in the Turkish data set have the head to the right of 1048 the dependent , while only less than 4 have the head to the left .", "This means that the backward model builds much more partial structure in the stack as it consumes input tokens , while the for ward model must consume most tokens before it starts making attachments .", "In other words , context in general in the backward model has more struc ture , and attachments are made while there are still look ahead tokens , while the opposite is generally true in the forward model .", "Our results demonstrate the effectiveness of even small ensembles of parsers that are relatively similar using the same features and the same algorithm .", "There are several possible extensions and improvements to the approach we have described .", "For example , in section 3 we mention the use of different weighting schemes in dependency voting .", "We list additional ideas that were not attempted due to time constraints , but that are likely to produce improved results .", "One of the simplest improvements to our approach is simply to train more models with no oth er changes to our set up .", "As mentioned in section 5 , the addition of a backward SVM model did im prove accuracy on the Turkish set significantly , and it is likely that improvements would also be obtained in other languages .", "In addition , other learning approaches , such as memory based lan guage processing Daelemans and Van den Bosch , 2005 , could be used .", "A drawback of adding more models that became obvious in our experiments was the increased cost of both training for example , the SVM parsers we used required significant ly longer to train than the MaxEnt parsers and run time parsing with MBL models can be several times slower than with MaxEnt , or even SVM .", "A similar idea that may be more effective , but requires more effort , is to add parsers based on dif ferent approaches .", "For example , using MSTParser McDonald and Pereira , 2005 , a large margin all pairs parser , in our domain adaptation procedure results in significantly improved accuracy 83 . 2 LAS .", "Of course , the use of different approaches used by different groups in the CoNLL 2006 and 2007 shared tasks represents great opportunity for parser ensembles .", "Acknowledgements We thank the shared task organizers and treebank providers .", "We also thank the reviewers for their comments and suggestions , and Yusuke Miyao for insightful discussions .", "This work was supported in part by Grant in Aid for Specially Promoted Re search 18002007 ."], "summary_lines": ["Dependency Parsing and Domain Adaptation with LR Models and Parser Ensembles\n", "We present a data-driven variant of the LR algorithm for dependency parsing, and extend it with a best-first search for probabilistic generalized LR dependency parsing.\n", "Parser actions are determined by a classifier, based on features that represent the current state of the parser.\n", "We apply this parsing framework to both tracks of the CoNLL 2007 shared task, in each case taking advantage of multiple models trained with different learners.\n", "In the multilingual track, we train three LR models for each of the ten languages, and combine the analyses obtained with each individual model with a maximum spanning tree voting scheme.\n", "In the domain adaptation track, we use two models to parse unlabeled data in the target domain to supplement the labeled out-of-domain training set, in a scheme similar to one iteration of co-training.\n", "We use a combination of co-learning and active learning by training two different parsers on the labeled training data, parsing the unlabeled domain data with both parsers, and adding parsed sentences to the training data only if the two parsers agreed on their analysis.\n", "We generalize the standard deterministic framework to probabilistic parsing by using a best-first search strategy.\n"]}
{"article_lines": ["Investigating Regular Sense Extensions based on Intersective Levin Classes", "In this paper we specifically address questions of polysemy with respect to verbs , and how regular extensions of meaning can be achieved through the adjunction of particular syntactic phrases .", "We see verb classes as the key to making generalizations about regular extensions of meaning .", "Current approaches to English classification , Levin classes and WordNet , have limitations in their applicability that impede their utility as general classification schemes .", "We present a refinement of Levin classes , intersective sets , which are a more fine grained classification and have more coherent sets of syntactic frames and associated semantic components .", "We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the original Levin classes .", "We also have begun to examine related classes in Portuguese , and find that these verbs demonstrate similarly coherent syntactic and semantic properties .", "The difficulty of achieving adequate handcrafted semantic representations has limited the field of natural language processing to applications that can be contained within well defined subdomains .", "The only escape from this limitation will be through the use of automated or semi automated methods of lexical acquisition .", "However , the field has yet to develop a clear consensus on guidelines for a computational lexicon that could provide a springboard for such methods , although attempts are being made Pustejovsky , 1991 , Copestake and Sanfilippo , 1993 , Lowe et al . , 1997 , Dorr , 1997 .", "The authors would like to acknowledge the support of DARPA grant N66001 94C 6043 , ARO grant DAAH04 94G 0426 , and CAPES grant 0914 95 2 .", "One of the most controversial areas has to do with polysemy .", "What constitutes a clear separation into senses for any one verb , and how can these senses be computationally characterized and distinguished ?", "The answer to this question is the key to breaking the bottleneck of semantic representation that is currently the single greatest limitation on the general application of natural language processing techniques .", "In this paper we specifically address questions of polysemy with respect to verbs , and how regular extensions of meaning can be achieved through the adjunction of particular syntactic phrases .", "We base these regular extensions on a fine grained variation on Levin classes , intersective Levin classes , as a source of semantic components associated with specific adjuncts .", "We also examine similar classes in Portuguese , and the predictive powers of alternations in this language with respect to the same semantic components .", "The difficulty of determining a suitable lexical representation becomes multiplied when more than one language is involved and attempts are made to map between them .", "Preliminary investigations have indicated that a straightforward translation of Levin classes into other languages is not feasible Jones et al . , 1994 , Nomura et al . , 1994 , Saint Dizier , 1996 .", "However , we have found interesting parallels in how Portuguese and English treat regular sense extensions .", "Two current approaches to English verb classifications are WordNet Miller et al . , 1990 and Levin classes Levin , 1993 .", "WordNet is an online lexical database of English that currently contains approximately 120 , 000 sets of noun , verb , adjective , and adverb synonyms , each representing a lexicalized concept .", "A synset synonym set contains , besides all the word forms that can refer to a given concept , a definitional gloss and in most cases an example sentence .", "Words and synsets are interrelated by means of lexical and semantic conceptual links , respectively .", "Antonymy or semantic opposition links individual words , while the super subordinate relation links entire synsets .", "WordNet was designed principally as a semantic network , and contains little syntactic information .", "Levin verb classes are based on the ability of a verb to occur or not occur in pairs of syntactic frames that are in some sense meaning preserving diathesis alternations Levin , 1993 .", "The distribution of syntactic frames in which a verb can appear determines its class membership .", "The fundamental assumption is that the syntactic frames are a direct reflection of the underlying semantics .", "Levin classes are supposed to provide specific sets of syntactic frames that are associated with the individual classes .", "The sets of syntactic frames associated with a particular Levin class are not intended to be arbitrary , and they are supposed to reflect underlying semantic components that constrain allowable arguments .", "For example , break verbs and cut verbs are similar in that they can all participate in the transitive and in the middle construction , John broke the window , Glass breaks easily , John cut the bread , This loaf cuts easily .", "However , only break verbs can also occur in the simple intransitive , The window broke , The bread cut .", "In addition , cut verbs can occur in the conative , John valiantly cut hacked at the frozen loaf , but his knife was too dull to make a dent in it , whereas break verbs cannot , John broke at the window .", "The explanation given is that cut describes a series of actions directed at achieving the goal of separating some object into pieces .", "It is possible for these actions to be performed without the end result being achieved , but where the cutting manner can still be recognized , i . e . , John cut at the loaf .", "Where break is concerned , the only thing specified is the resulting change of state where the object becomes separated into pieces .", "If the result is not achieved , there are no attempted breaking actions that can still be recognized .", "It is not clear how much WordNet synsets should be expected to overlap with Levin classes , and preliminary indications are that there is a wide discrepancy Dorr and Jones , 1996 , Jones and Onyshkevych , 1997 , Dorr , 1997 .", "However , it would be useful for the WordNet senses to have access to the detailed syntactic information that the Levin classes contain , and it would be equally useful to have more guidance as to when membership in a Levin class does in fact indicate shared semantic components .", "Of course , some Levin classes , such as braid bob , braid , brush , clip , coldcream , comb , condition , crimp , crop , curl , etc . are clearly not intended to be synonymous , which at least partly explains the lack of overlap between Levin and WordNet .", "The association of sets of syntactic frames with individual verbs in each class is not as straightforward as one might suppose .", "For instance , carry verbs are described as not taking the conative , The mother carried at the baby , and yet many of the verbs in the carry class push , pull , tug , shove , kick are also listed in the push pull class , which does take the conative .", "This listing of a verb in more than one class many verbs are in three or even four classes is left open to interpretation in Levin .", "Does it indicate that more than one sense of the verb is involved , or is one sense primary , and the alternations for that class should take precedence over the alternations for the other classes in which the verb is listed ?", "The grounds for deciding that a verb belongs in a particular class because of the alternations that it does not take are elusive at best .", "We augmented the existing database of Levin semantic classes with a set of intersective classes , which were created by grouping together subsets of existing classes with overlapping members .", "All subsets were included which shared a minimum of three members .", "If only one or two verbs were shared between two classes , we assumed this might be due to homophony , an idiosyncrasy involving individual verbs rather than a systematic relationship involving coherent sets of verbs .", "This filter allowed us to reject the potential intersective class that would have resulted from combining the remove verbs with the scribble verbs , for example .", "The sole member of this intersection is the verb mantic classes such that ci n n cal c , where E is a relevance cut off .", "We then reclassified the verbs in the database as follows .", "A verb was assigned membership in an intersective class if it was listed in each of the existing classes that were combined to form the new intersective class .", "Simultaneously , the verb was removed from the membership lists of those existing classes .", "Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components , and could be divided into smaller subclasses .", "The split verbs cut , draw , kick , knock , push , rip , roll , shove , slip , split , etc . do not obviously form a homogeneous semantic class .", "Instead , in their use as split verbs , each verb manifests an extended sense that can be paraphrased as quot ; separate by V ing , quot ; where quot ; V quot ; is the basic meaning of that verb Levin , 1993 .", "Many of the verbs e . g . , draw , pull , push , shove , tug , yank that do not have an inherent semantic component of quot ; separating quot ; belong to this class because of the component of force in their meaning .", "They are interpretable as verbs of splitting or separating only in particular syntactic frames I pulled the twig and the branch apart , I pulled the twig off of the branch , but not I pulled the twig and the branch .", "The adjunction of the apart adverb adds a change of state semantic component with respect to the object which is not present otherwise .", "These fringe split verbs appear in several other intersective classes that highlight the force aspect of their meaning .", "Figure 2 depicts the intersection of split , carry and push pull .", "Figure 2 Intersective class formed from Levin carry , push pull and split verbs verbs in are not listed by Levin in all the intersecting classes but participate in all the alternations The intersection between the push pull verbs of exerting force , the carry verbs and the split verbs illustrates how the force semantic component of a verb can also be used to extend its meaning so that one can infer a causation of accompanied motion .", "Depending on the particular syntactic frame in which they appear , members of this intersective class pull , push , shove , tug , kick , draw , yank can be used to exemplify any one or more of the the component Levin classes . for each Levin verb class is not always complete , so to check if a particular verb belongs to a class it is better to check that the verb exhibits all the alternations that define the class .", "Since intersective classes were built using membership lists rather than the set of defining alternations , they were similarly incomplete .", "This is an obvious shortcoming of the current implementation of intersective classes , and might affect the choice of 3 as a relevance cut off in later implementations .", "verb of exerting force , no separation or causation of accompanied motion implied Although the Levin classes that make up an intersective class may have conflicting alternations e . g . , verbs of exerting force can take the conative alternation , while carry verbs cannot , this does not invalidate the semantic regularity of the intersective class .", "As a verb of exerting force , push can appear in the conative alternation , which emphasizes its force semantic component and ability to express an quot ; attempted quot ; action where any result that might be associated with the verb e . g . , motion is not necessarily achieved ; as a carry verb used with a goal or directional phrase , push cannot take the conative alternation , which would conflict with the core meaning of the carry verb class i . e . , causation of motion .", "The critical point is that , while the verb's meaning can be extended to either quot ; attempted quot ; action or directed motion , these two extensions cannot co occur they are mutually exclusive .", "However the simultaneous potential of mutually exclusive extensions is not a problem .", "It is exactly those verbs that are triple listed in the split push carry intersective class which have force exertion as a semantic component that can take the conative .", "The carry verbs that are not in the intersective class carry , drag , haul , heft , hoist , lug , tote , tow are more quot ; pure quot ; examples of the carry class and always imply the achievement of causation of motion .", "Thus they cannot take the conative alternation .", "Even though the Levin verb classes are defined by their syntactic behavior , many reflect semantic distinctions made by WordNet , a classification hierarchy defined in terms of purely semantic word relations synonyms , hypernyms , etc . .", "When examining in detail the intersective classes just described , which emphasize not only the individual classes , but also their relation to other classes , we see a rich semantic lattice much like WordNet .", "This is exemplified by the Levin cut verbs and the intersective class formed by the cut verbs and split verbs .", "The original intersective class cut , hack , hew , saw exhibits alternations of both parent classes , and has been augmented with chip , clip , slash , snip since these cut verbs also display the syntactic properties of split verbs .", "WordNet distinguishes two subclasses of cut , differentiated by the type of result This distinction appears in the second order Levin classes as membership vs . nonmembership in the intersective class with split .", "Levin verb classes are based on an underlying lattice of partial semantic descriptions , which are manifested indirectly in diathesis alternations .", "Whereas high level semantic relations synonym , hypernym are represented directly in WordNet , they can sometimes be inferred from the intersection between Levin verb classes , as with the cut split class .", "However , other intersective classes , such as the split push carry class , are no more consistent with WordNet than the original Levin classes .", "The most specific hypernym common to all the verbs in this intersective class is move , displace , which is also a hypernym for other carry verbs not in the intersection .", "In addition , only one verb pull has a WordNet sense corresponding to the change of state separation semantic component associated with the split class .", "The fact that the split sense for these verbs does not appear explicitly in WordNet is not surprising since it is only an extended sense of the verbs , and separation is inferred only when the verb occurs with an appropriate adjunct , such as apart .", "However , apart can also be used with other classes of verbs , including many verbs of motion .", "To explicitly list separation as a possible sense for all these verbs would be extravagant when this sense can be generated from the combination of the adjunct with the force potential cause of change of physical state or motion itself a special kind of change of state , i . e . , of position semantic component of the verb .", "WordNet does not currently provide a consistent treatment of regular sense extension some are listed as separate senses , others are not mentioned at all .", "It would be straightforward to augment it with pointers indicating which senses are basic to a class of verbs and which can be generated automatically , and include corresponding syntactic information .", "Figure 3 shows intersective classes involving two classes of verbs of manner of motion run and roll verbs and a class of verbs of existence meander verbs .", "Roll and run verbs have semantic components describing a manner of motion that typically , though not necessarily , involves change of location .", "In the absence of a goal or path adjunct they do not specify any direction of motion , and in some cases e . g . , float , bounce require the adjunct to explicitly specify any displacement at all .", "The two classes differ in that roll verbs relate to manners of motion characteristic of inanimate entities , while run verbs describe manners in which animate entities can move .", "Some manner of motion verbs allow a transitive alternation in addition to the basic intransitive .", "When a roll verb occurs in the transitive Bill moved the box across the room , the subject physically causes the object to move , whereas the subject of a transitive run verb merely induces the object to move the coach ran the athlete around the track .", "Some verbs can be used to describe motion of both animate and inanimate objects , and thus appear in both roll and run verb classes .", "The slide class partitions this roll run intersection into verbs that can take the transitive alternation and verbs that cannot drift and glide cannot be causative , because they are not typically externally controllable .", "Verbs in the slide roll run intersection are also allowed to appear in the dative alternation Carla slid the book to Dale , Carla slid Dale the book , in which the sense of change of location is extended to change of possession .", "When used intransitively with a path prepositional phrase , some of the manner of motion verbs can take on a sense of pseudo motional existence , in which the subject does not actually move , but has a shape that could describe a path for the verb e . g . , The stream twists through the valley .", "These verbs are listed in the intersective classes with meander verbs of existence .", "The Portuguese verbs we examined behaved much more similarly to their English counterparts than we expected .", "Many of the verbs participate in alternations that are direct translations of the English alternations .", "However , there are some interesting differences in which sense extensions are allowed .", "We have made a preliminary study of the Portuguese translation of the carry verb class .", "As in English , these verbs seem to take different alternations , and the ability of each to participate in an alternation is related to its semantic content .", "Table 1 shows how these Portuguese verbs naturally cluster into two different subclasses , based on their ability to take the conative and apart alternations as well as path prepositions .", "These subclasses correspond very well to the English subclasses created by the intersective class .", "The conative alternation in Portuguese is mainly contra against , and the apart alternation is mainly separando separating .", "For example , Eu puxei o ramo e o galho separando os I pulled the twig and the branch apart , and Ele empurrou contra a parede He pushed against the wall .", "We also investigated the Portuguese translation of some intersective classes of motion verbs .", "We selected the slide roll run , meander roll and roll run intersective classes .", "Most verbs have more than one translation into Portuguese , so we chose the translation that best described the meaning or that had the same type of arguments as described in Levin's verb classes .", "The elements of the slide roll run class are rebater bounce , flutuar float , rolar roll and deslizar slide .", "The resultative in Portuguese cannot be expressed in the same way as in English .", "It takes a gerund plus a reflexive , as in A porta deslizou abrindo se The door slid opening itself .", "Transitivity is also not always preserved in the translations .", "For example , flutuar does not take a direct object , so some of the alternations that are related to its transitive meaning are not present .", "For these verbs , we have the induced action alternation by using the light verb fazer make before the verb , as in Maria fez o barco flutuar Mary floated the boat .", "As can be seen in Table 2 the alternations for the Portuguese translations of the verbs in this intersective class indicate that they share similar properties with the English verbs , including the causative inchoative .", "The exception to this , as just noted , is flutuar float .", "The result of this is that flutuar should move out of the slide class , which puts it with derivar drift and planar glide in the closely related roll run class .", "As in English , derivar and planar are not externally controllable actions and thus don't take the causative inchoative alternation common to other verbs in the roll class .", "Planar doesn't take a direct object in Portuguese , and it shows the induced action alternation the same way as flutuar by using the light verb fazer .", "Derivar is usually said as quot ; estar a deriva quot ; quot ; to be adrift quot ; , showing its non controllable action more explicitly .", "We have presented a refinement of Levin classes , intersective classes , and discussed the potential for mapping them to WordNet senses .", "Whereas each WordNet synset is hierarchicalized according to only one aspect e . g . , Result , in the case of cut , Levin recognizes that verbs in a class may share many different semantic features , without designating one as primary .", "Intersective Levin sets partition these classes according to more coherent subsets of features force , force motion , force separation , in effect highlighting a lattice of semantic features that determine the sense of a verb .", "Given the incompleteness of the list of members of Levin classes , each verb must be examined to see whether it exhibits all the alternations of a class .", "This might be approximated by automatically extracting the syntactic frames in which the verb occurs in corpus data , rather than manual analysis of each verb , as was done in this study .", "We have also examined a mapping between the English verbs that we have discussed and their Portuguese translations , which have several of the same properties as the corresponding verbs in English .", "Most of these verbs take the same alternations as in English and , by virtue of these alternations , achieve the same regular sense extensions .", "There are still many questions that require further investigation .", "First , since our experiment was based on a translation from English to Portuguese , we can expect that other verbs in Portuguese would share the same alternations , so the classes in Portuguese should by no means be considered complete .", "We will be using resources such as dictionaries and on line corpora to investigate potential additional members of our classes .", "Second , since the translation mappings may often be many to many , the alternarebater flutuar rolar deslizar derivar planar bounce float roll slide drift glide dative yes yes yes conative no no no caus . inch . yes yes yes middle yes yes yes accept . coref . yes yes yes caus . inch . yes yes yes yes yes yes resultative yes yes yes Yee yes Yee adject . part . yes yes yes ind . action yes yes yes yes no yes locat . invers . yes yes yes yes yes yes measure Yee yes yes Yee yes yes adj . perf . no no no no no no cogn . object no no no no no no zero nom . yes yes no yes yes yes tions may depend on which translation is chosen , potentially giving us different clusters , but it is uncertain to what extent this is a factor , and it also requires further investigation .", "In this experiment , we have tried to choose the Portuguese verb that is most closely related to the description of the English verb in the Levin class .", "We expect these cross linguistic features to be useful for capturing translation generalizations between languages as discussed in the literature Palmer and Rosenzweig , 1996 , Copestake and Sanfilippo , 1993 , Dorr , 1997 .", "In pursuing this goal , we are currently implementing features for motion verbs in the English Tree Adjoining Grammar , TAG Bleam et al . , 1998 .", "TAGs have also been applied to Portuguese in previous work , resulting in a small Portuguese grammar Kipper , 1994 .", "We intend to extend this grammar , building a more robust TAG grammar for Portuguese , that will allow us to build an English Portuguese transfer lexicon using these features ."], "summary_lines": ["Investigating Regular Sense Extensions based on Intersective Levin Classes\n", "In this paper we specifically address questions of polysemy with respect to verbs, and how regular extensions of meaning can be achieved through the adjunction of particular syntactic phrases.\n", "We see verb classes as the key to making generalizations about regular extensions of meaning.\n", "Current approaches to English classification, Levin classes and WordNet, have limitations in their applicability that impede their utility as general classification schemes.\n", "We present a refinement of Levin classes, intersective sets, which are a more fine-grained classification and have more coherent sets of syntactic frames and associated semantic components.\n", "We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the original Levin classes.\n", "We also have begun to examine related classes in Portuguese, and find that these verbs demonstrate similarly coherent syntactic and semantic properties.\n", "We show that multiple listings could in some cases be interpreted as regular sense extensions and defined intersective Levin classes, which are a more fine-grained, syntactically and semantically coherent refinement of basic Levin classes.\n", "We argue that the use of syntactic frames and verb classes can simplify the definition of different verb senses.\n"]}
{"article_lines": ["Features And Values", "The paper discusses the linguistic aspects of a new general purpose facility for computing with features .", "The program was developed in connection with the course I taught at the University of Texas in the fall of 1983 .", "It is a generalized and expanded version of a system that Stuart Shieber originally designed for the PATR II project at SRI in the spring of 1983 with later modifications by Fernando Pereira and me .", "Like its predecessors , the new Texas version of the quot ; DG directed graph quot ; package is primarily intended for representing morphological and syntactic information but it may turn out to be very useful for semantic representa", "Most schools of linguistics use some type of feature notation in their phonological , morphological , syntactic , and semantic descriptions .", "Although the objects that appear in rules and conditions may have atomic names , such as quot ; k , quot ; quot ; NP , quot ; quot ; Subject , quot ; and the like , such high level terms typically stand for collections of features .", "Features , in this sense of the word , are usually thought of as attribute value pairs person 1st , number sg , although singleton features are also admitted in some theories .", "The values of phonological and morphological features are traditionally atomic ; e . g .", "1st , 2nd , 3rd ; they are often binary , Most current theories also allow features that have complex values .", "A complex value is a collection of features , for example", "number person 3rd In graphs of this sort , values are reached by traversing paths of attribute names .", "We use angle brackets to mark expressions that designate paths .", "With that convention , the above graph can also be represented as a set of equations agreement number sg agreement person 3rd Such equations also provide a convenient way to express conditions on features .", "This idea lies at the heart of UG , LFG , and the PATR II grammar for English Shieber , et al . , 831 constructed at SRI .", "For example , the equation subject agreement predicate agreement states that subject and predicate have the same value for agreement .", "In graph terms , this corresponds to a lattice where two vectors point to the same node agreement subj ect 1 predicate person 3rdn number sg Lexical Functional Grammar LFG Kaplan and Bres agreement agreement nan , 831 , Unification Grammar UG Kay , 79 , General number person ized Phrase Structure Grammar GPSG Gazdar and Pul sg 3rd lum , 821 , among others , use complex features .", "Another way to represent feature matrices is to think of them as directed graphs where values correspond to nodes and attributes to vectors In a case like this , the values of the two paths have been quot ; unified . quot ; To represent unification in terms of feature matrices we need to introduce some new convention to distinguish between identity and mere likeness .", "Even that would not quite suffice because the graph formalism also allows unification of values that have not yet been assigned .", "A third way to . view these structures is to think of them as partial functions that assign values to attributes Sag et . al . , Si .", "Several related grammar formalisms UG , LFG , PATHII , and GPS now exist that are based on a very similar conception of features and use unification as their basic operation .", "Because feature matrices lattice nodes are sets of attribute value pairs , unification is closely related to the operation of forming a union of two sets .", "However , while the latter always yields something at least the null set , unification is an operation that may fail or succeed .", "When it fails , no result . is produced and the operands remain unchanged ; when it succeeds , the operands are permanently altered in the process .", "They become the same object .", "This is an important characteristic .", "The result of unifying three or more graphs in pairs with one another does not depend on the order in which the operations are performed .", "They all become the same graph at the end .", "If graphs A and B contain the same attribute but have incompatible values for it , they cannot be unified .", "If A and B are compatible , then Unify A B contains every attribute that appears only in A or only in B with the value it has there .", "If some attribute appears both in A and B , then the value of that attribute in Unify A B is the unification of the two values .", "For example , agreement number pa Simple cases of grammatical concord , such as number , case and gender agreement between determiners and nouns in many languages , can be expressed straight forwardly by stating that the values of these features must unify .", "Another useful operation on feature matrices is generalization .", "It is closely related to set intersection .", "The generalization of two simple matrices A and B consists of the attribute value pairs that A and B have in common .", "If the values themselves are complex , we take the generalization of those values .", "For example , Generalization seems to be a very useful notion for expressing how number and gender agreement works in coordinate noun phrases .", "One curious fact about coordination is that conjunction of quot ; I quot ; with quot ; you quot ; or quot ; he quot ; in the subject position typically produces first person verb agreement .", "In sentences like quot ; he and I agree quot ; the verb has the same form as in quot ; we agree .", "quot ; The morphological equivalence of quot ; he and I , quot ; quot ; you and I , quot ; and quot ; we quot ; is partially obscured in English but very clear in many other languages .", "The problem is discussed in Section V below .", "Most current grammar formalisms for features have certain built in limitations .", "Three are relevant here The prohibition against cyclicity rules out structures that contain circular paths , as in the following example .", "Here the path a b c folds back onto itself , that is , a a b c .", "It is not clear whether such descriptions should be ruled out on theoretical grounds .", "Whatever the case might be , current implementations of LFG , UG , or GPSG with which I am familiar do not support them .", "The prohibition against negation makes it impossible to characterize a feature by saying that it does NOT have such and such a value .", "None of the above theories allows specifications such as the following .", "We use the symbol quot ; quot ; to mean 'not . ' riperson 3rol number sg The first statement says that case is quot ; not dative , quot ; the second says that the value of agreement is quot ; anything but 3rd person singular . quot ; Not allowing disjunctive specifications rules out matrices of the following sort .", "We indicate disjunction by enclosing the alternative values in .", "The first line describes the value of case as being quot ; either nominative or accusative . quot ; The value for agreement is given as quot ; either feminine singular or plural . quot ; Among the theories mentioned above , only Kay's UG allows disjunctive feature specifications in its formalism .", "In LFG , disjunctions are allowed in control equations but not in the specification of values .", "Of the three limitations , the first one may be theoretically justified since it has not been shown that there are phenomena in natural languages that involve circular structures cf .", "Kaplan and Bresnan , 831 , p . 281 .", "PATR II at SRI and its expanded version at the University of Texas allow such structures for practical reasons because they tend to arise , mostly inadvertently , in the course of grammar construction and testing .", "An implementation that does not handle unification correctly in such cases is too fragile to use .", "The other two restrictions are linguistically unmotivated .", "There are many cases , especially in morphology , in which the most natural feature specifications are negative or disjunctive .", "In fact , the examples given above all represent such cases .", "The first example , case dat , arises in the plural paradigm of words like quot ; Kind quot ; child in German .", "Such words have two forms in the plural quot ; Kinder quot ; and quot ; Kindern . quot ; The latter is used only in the plural dative , the former in the other three cases nominative , genitive , accusative .", "If we accept the view that there should be just one rather than three entries for the plural suffix quot ; er quot ; , we have the choice between number ; pi case oat The second alternative seems preferrable given the fact that there is , in this particular declension , a clear twoway contrast .", "The marked dative is in opposition with an unmarked form representing all the other cases .", "The Aecond example is from English .", "Although the features quot ; number quot ; and quot ; person quot ; are both clearly needed in English verb morphology , most verbs are very incompletely specified for them .", "In fact , the present tense paradigm of all regular verbs just has two forms of which one represents the 3rd person singular quot ; walks quot ; and the other quot ; walk quot ; is used for all other persons .", "Thus the most natural characterization for quot ; walk quot ; is that it is not 3rd person singular .", "The alternative is to say , in effect , that quot ; walk quot ; in the present tense has five different interpretations .", "The system of articles in German provides many examples that call for disjunctive feature specifications .", "The article quot ; die , quot ; for example , is used in the nominative and accusative cases of singular feminine nouns and all plural nouns .", "The entry given above succinctly encodes exactly this fact .", "There are many cases where disjunctive specifications seem necessary for reasons other than just descriptive elegance .", "Agreement conditions on conjunctions , for example , typically fail to exclude pairs where differences in case and number are not overtly marked .", "For example , in German Eisenberg , 73 noun phrases like des Dozenten gen sg the docent's der Dozenten gen pl the docents' . can blend as in der Antrag des oder der Dozenten the petition of the docent or docents .", "This is not possible when the noun is overtly marked for number , as in the case of quot ; des Professors quot ; gen sg and quot ; der Professoren quot ; gen pl der Antrag des oder der Professors der Antrag des oder der Professoren the petition of the professor or professors In the light of such cases , it seems reasonable to assume that there is a single form , quot ; Dozenten , quot ; which has a disjunctive feature specification , instead of postulating several fully specified , homonymous lexical entries .", "It is obvious that the grammaticality of the example crucially depends on the fact that quot ; Dozenten quot ; is not definitely singular or definitely plural but can be either .", "I sketch here briefly how the basic unification procedure can be modified to admit negative and disjunctive values .", "These ideas have been implemented in the new Texas version of the PATR II system for features .", "I am much indebted to Fernando Pereira for his advice on this topic .", "Negative values are created by the following operation .", "If A and B are distinct , i . e . contain a different value for some feature , then Negate A B does nothing to them .", "Otherwise both nodes acquire a quot ; negative constraint . quot ; In effect , A is marked with B and B with A .", "These constraints prevent the two nodes from ever becoming alike .", "When A is unified with C , unification succeeds only if the result is distinct from B .", "The result of Unify A C has to satisfy all the negative constraints of both A and C and it inherits all that could fail in some later unification .", "Disjunction is more complicated .", "Suppose A , B and C are all simple atomic values .", "In this situation C unifies with A B just in case it is identical to one or the other of the disjuncts .", "The result is C . Now suppose that A , B , and C are all complex .", "Furthermore , let us suppose that A and B are distinct but C is compatible with both of them as in the following What should be the result of Unify A B C ?", "Because A and B are incompatible , we cannot actually unify C with both of them .", "That operation would fail .", "Because there is no basis for choosing one , both alternatives have to be left open .", "Nevertheless . we need to take note of the fact that either A or B is to be unified with C . We can do this by making the result a complex disjunction .", "AC c .", "The new value of C , C' , is a disjunction of tuples which can be , but . have not yet been unified .", "Thus A C and B C are sets that consist of compatible structures .", "Furthermore , at least one of the tuples in the complex disjunction must remain consistent regardless of what happens to A and B .", "After the first unification we can still unify A with any structure that it . is compatible with , such as If this happens , then the tuple A C is no longer consistent .", "A side effect of A becoming A' render tem number sg case nom is that C' simultaniously reduces to B C .", "Since there is now only one viable alternative left , B and C can at this point be unified .", "The original result from Unify A Ill C now reduces to the same as Unify B C .", "C quot ; B C number Lease aco As the example shows , once C is unified with A B , A and B acquire a quot ; positive constraint . quot ; All later unifications involving them must keep at least one of the two pairs A C , B C unifieable .", "If at some later point one of the two tuples becomes inconsistent , the members of the sole remaining tuple finally can and should be unified .", "When that has happened , the positive constraint on A and B can also be discarded .", "A more elaborate example of this sort is given in the Appendix .", "Essentially the same procedure also works for more complicated cases .", "For example , unification of A B with C DI yields A C A D B C B D assuming that the two values in each tuple are compatible .", "Any pairs that could not be unified are left out .", "The complex disjunction is added as a positive constraint to all of the values that appear in it .", "The result of unifying A C B C with D F E F is A CD F A C E F B CD F B C E F , again assuming that no alternative can initially be ruled out .", "As for generalization , things are considerably simpler .", "The result of Generalize A B inherits both negative and positive constraints of A and B .", "This follows from the fact that the generalization of A and B is the maximal subgraph of A and B that will unify with either one them .", "Consequently , it is subject to any constraint that affects A or B .", "This is analogous to the fact that , in set theory , In our current implementation , negative constraints are dropped as soon as they become redundant as far as unification is concerned .", "For example , when case ace is unified with with case dal , the resulting matrix is simply case accl .", "The negative constraint is eliminated since there is no possibility that it could ever be violated later .", "This may be a wrong policy .", "It has to be modified to make generalization work as proposed in Section V for structures with negative constraints .", "If generalization is defined as we have suggested above , negative constraints must always be kept because they never become redundant for generalization .", "When negative or positive constraints are involved , unification obviously takes more time .", "Nevertheless , the basic algorithm remains pretty much the same .", "Allowing for constraints does not significantly reduce the speed at which values that do not have any get unified in the Texas implementation .", "In the course of working on the project , I gained one insight that perhaps should have been obvious from the very beginning the problems that arise in this connection are very similar to those that come up in logic programming .", "One can actually use the feature system for certain kind of inferencing .", "For example , let Mary , Jane , and John have the following values", "straints rather than additional features for establishing a markedness hierarchy .", "For example , the following feature specifications have the effect that we seek .", "If we now unify John with sister eyes blue both Jane and Mary get marked with the positive constraint that at least one of them has blue eyes .", "Suppose that we now learn that Mary has green eyes .", "This immediately gives us more information about John and Jane as well .", "Now we know that Jane's eyes are blue and that she definitely is John's sister .", "The role of positive constraints is to keep track of partial information in such a way that no inconsistencies are allowed and proper updating is done when more things become known .", "One problem of long standing for which the present system may provide a simple solution is person agreement in coordinate noun phrases .", "The conjunction of a 1st person pronoun with either 2nd or 3rd person pronoun invariably yields 1st person agreement .", "quot ; I and you quot ; is equivalent to quot ; we , quot ; as far as agreement is concerned .", "When a second person pronoun is conjoined with a third person NP , the resulting conjunction has the agreement properties of a second person pronoun .", "Schematically Assuming that generalization with negative constraints works as indicated above , i . e . negative constraints are always inherited , it immediately follows that the generalization of 1st person with any other person is compatible with only 1st person and that 2nd person wins over 3rd when they are combined .", "The results are as follows .", "Sag , Gazdar , Wasow , and Weisler 84 propose a solution which is based on the idea of deriving the person feature for a coordinate noun phrase by generalization intersection from the person features of its heads .", "It is obvious that the desired effect can be obtained in any feature system that uses the fewest features to mark 1st person , some additional feature for 2nd person , and yet another for 3rd person .", "Because generalization of 1st and 2nd , for example , yields only the features that two have in common , the one with fewest features wins .", "Any such solution can probably be implemented easily in the framework outlined above .", "However , this proposal has one very counterintuitive aspect markedness hierarchy is the reverse of what traditionally has been assumed .", "Designating something as 3rd person requires the greatest number of feature specifications .", "In the Sag et al . system , 3rd person is the most highly marked member and 1st person the least marked member of the trio .", "Traditionally , 3rd person has been regarded as the unmarked case .", "In our system , there is a rather simple solution under which the value of person feature in coordinate NPs is derived by generalization , just as Sag it et al . propose , which nevertheless preserves the traditional view of markedness .", "The desired result can be obtained by using negative con", "let 3rd conversant", "Note that the proper part of lst 2nd excludes 3rd person .", "It is compatible with both 1st and 2nd person but the negative constraint rules out the latter one .", "In tile case of lst 3rd , the negative constraint is compatible with 1st person but incompatible with 2nd and 3rd .", "In the last case , the specification speaker rules out 1st person and the negative constraint conversant 1 eliminates 3rd person .", "When negative constraints are counted in , 1st person is the most and 3rd person the least marked member of the three .", "In that respect , the proposed analysis is in line with traditional views on markedness .", "Another relevant observation is that the negative constraints on which the result crucially depends are themselves not too unnatural .", "In effect , they say of 1st person that it is quot ; neither 2nd nor 3rd quot ; and that 2nd person is quot ; not 3rd . quot ; It will be interesting to see whether other cases of markedness can be analyzed in the same way . speaker conversant", "I am indebted to Martin Kay for introducing me to unification and to Fernando Pereira , Stuart Shieber , Remo Pareschi , and Annie Zaenen for many insightful suggestions on the project ."], "summary_lines": ["Features And Values\n", "The paper discusses the linguistic aspects of a new general purpose facility for computing with features.\n", "The program was developed in connection with the course I taught at the University of Texas in the fall of 1983.\n", "It is a generalized and expanded version of a system that Stuart Shieber originally designed for the PATR-II project at SRI in the spring of 1983 with later modifications by Fernando Pereira and me.\n", "Like its predecessors, the new Texas version of the \"DG {directed graph}\" package is primarily intended for representing morphological and syntactic information but it may turn out to be very useful for semantic representations too.\n", "We provide examples of feature structures in which a negation operator might be useful.\n"]}
{"article_lines": ["A COMPUTATIONAL MODEL OF LANGUAGE DATA ORIENTED PARSING RENS BOlt Department of Computational I Jnguistics University of Amsterdmn Spuistraat 134 1012 VII Amsterdam The Netherlands rens alf . let . uva . nl PERFORMANCE Abstract 1 ata Oriented Parsing IX P is a model where no abstract rules , but language xt riences in the ti3ru of all , malyzed COlpUS , constitute the basis for langnage processing .", "Analyzing a new input means that the system attempts to find tile most probable way to reconstruct the input out of frugments that alr c y exist ill the corpus .", "Disambiguation occurs as a side effect .", "DOP can be implemented by using colivelllional parsing strategies .", "In oducfion This paper tommlizes the model for natural Imlgnage introduced m Sclm 199o .", "Since that article is written in Dutch , we will translate Some parts of it more or less literally in this introduction .", "According to Scba , the current radition of language processing systems is based on linguistically motivated competence models of natural Imlguages .", "llte problems that these systems lull iato , suggest file necessity of a more perfommnce oriented model of language processing , that takes into account the statistical properties of real language use .", "qllerefore Scha proposes a system ritat makes use of an annotated corpus .", "AnMyzing a new input means that the system attempts to find the most probable way to reconstruct the input out of fragments that already exist in the corpus .", "The problems with competence grammars that are mentioned in Schas aiticle , include the explosion of ambiguities , the fact tilat Itunmn judgemeats on grammaticality are not stable , that competence granunars do not account for language h alge , alld that no existing rule based grammar gives a descriptively adequate characterization of an actual language .", "According to Scha , tile deveh , pment of a fornml gnatunar fur natural latlguage gets more difficult , as tire grammar gets larger .", "When the number of phenotnena one has already takea into account gets larger , the number of iareractions that must be considered when , me tries to introduce all account of a new pllenomenon grows accordingly .", "As to tile problem of , mtbiguity , it has turned out that as soon as a formal gratmnar clmracterizes a non trivial part of a natural anguage , almost every input sentence of reasonable length gets ml re manageably large number of different structural analyses and The author wishes to thank his colleagues at the Department of Computational Linguistics of the Ilaiversity of Amsterdam for many fruitful discussions , and , in particular , Remko Scha , Martin van den Berg , Kwee Tjoe l , iong and Frodenk Somsen for valuable comments on earlier w rsions of this paper .", "semantical interpretations .", "I lids is problenmtic since most of these interpretations re not perceived as lVossible by a hunmn language user , while there are no systematic reasons 111 exclude tileln on syutactic or sematltic grounds .", "Often it is just a ntatter of relative implausibility tile only reason why a certain iarerpmtarion of a sentence is not perceived , is that aanther interprctatilm is much more plausible .", "Competence and Performance tale lhnriations of the current language procossing systerus are not suprising riley are the direct consequence of rile fact that these systems implement Chart skys notion of a coutpetence grmnmar .", "The formal grilnuuars that constitute the subject nmtter of theoretieal linguistics , aim at characterizing the clnnpetencc of tile langnage user .", "But the preferences language users have m the case of ambiguous entences , are paradigm instances of perfonatmce phenomena .", "In order to build effective lauguage processing systems we nmst intplement performanec grammars , rather than competence gratumars , qlaese performance granmuus houM not only contain information on the structural possibilities of file general I mgnage system , but also on details of actual language use in a language conmmnity , and of tile language experiences of an individual , which cause this individual to have certain expectations on what kinds of uUerances are going to occur , and what slractures and interpretations these utterances are going to have .", "Therc is all alternative linguistic tradition tluat has always focused on the concrete details of actual language use file statistical tradition .", "In this approach , syntactic structure is usually ignored ; only superficial stalistical properties of a large coqms are described file probability that a certain word is followed by a certain other word , the probability that a certain sequence of two words is followed by a ce ml word , etc .", "Markov cludns , see e . g .", "This approach bus perforumd succesfully ill certain practical tasks , such , as selecting the most probable sentence from the outputs of a speech recognition coruptment .", "It will be clear that this approach is not suitable for mmly other tasks , because no uotion of syntactic structme is used .", "Aud there are statistical dependencies within the sentences of a corpus , that cam extend over all arbitrarily long sequence of words ; this is ignored by file Markov approach .", "The challenge is now to develop a theory of language processlag that does justice to tile statistieM , as well as to tile structural aspects of langange .", "1 In Martin 19791 it is reported that their t ser generated 455 different lxuses for tile sentence lAst the sales of products produced in 1973 with the products produced in 1972 .", "ACRES DE COLING 92 , NANTES , 23 28 no tr 1992 8 5 5 PROC .", "NAN rES , AUG . 23 28 , 1992 The Synthesis of Syntax and Statistics The idea that a synthesis between syntactic and statistical approaches could be useful has incidentally been proposed before , but has not been worked out very well so far .", "The only technical elaboration of this idea that exists at the moment , the notion of a probabilisdc gtamnmr , is of a rather simplistic nature .", "A probabilistic grammar is simply a juxtaposition of the most fundamental syntactic notion and the most fundamental statistical notion it is an old fashioned context free grammar , that describes syntactic structures by means of a set of abstract rewrite rules that are now provided with probabilities that correspond to the application probabilities of the rules see e . g .", "Jeliuek 1990 .", "As long as a probabilistic grammar only assigns probabilities to individual rewrite rules , the grammar cannot account for all statistical properties of a language corpus .", "It is , for instance , not possible to indicate how the probability of syntactic structures or lexical items depends on their syntacticflexical context .", "As a consequence of this , it is not possible to recognize frequent phrases and figures of speech as such a disappointing property , for one would prefer that such phrases and figures of speech would get a high priority in the ranking of the possible syntactic analyses of a sentence .", "Some improvements can be made by applying the Markov approach to rewrite rules , as is found in the work of Salomaa 1969 and Magerman 1991 .", "Nevertheless , any approach which ties probabilities to rewrite rules will never be able to acconunodate all statistical dependencies .", "Optimal statistical estimations can only be achieved if tile statistics are applied to different kinds of units than rewrite rules .", "It is interesting to note that also in the field of theoretical linguistics tile necessity to use other kinds of structural units has been put forward .", "The clearest articulation of this idea is found in the work of Fillmore 1988 .", "From a linguistic point of view that emphasizes the syntactic complexities caused by idiomatic and semi idiomatic expressions , Fillmore et al .", "arrive at the proposal to describe language not by means of a set of rewrite rules , but by meaus of a set of constructions .", "A construction is a tree strncture a fragment of a constituent structure that can comprise more than one level .", "This tree is labeled with syntactic , semantic and pragnmtic ategories and feature values .", "Lexical items can be specified as part of a construction .", "Constructions can be idiomatic in nature the meaning of a larger constituent can be specified without being constructed front the meanings of its sub constituents .", "Fillmores ideas still show the influence of the tradition of formal grammars the constructions are schemata , and the combinatorics of putting the constructions together looks very much like a context free gramnmr .", "But the way in which Filhnore generalizes the notion of grmnmar resolves the problems we found in the current statistical grammars if a constrnction granunar is combined with statistical notions it is perhaps possible to represent all statistical information .", "This is one of the central ideas behind our approach .", "A New Approach Data Oriented Parsing The starting point of our approach is the idea indicated above , that when a human language user analyzes sentences , there is a strong preference for the recognition of sentences , constituents and patterns that occurred before in the experience of the language user .", "There is a statistical component in language processing that prefers more frequent structures and interpretations to less frequently perceived alternatives .", "The information we ideally would like to use in order to model the language performance uf a natural language user , comprises therefore an enumeration f all lexical items and syntactic semantic structures ever experieaced by the language user , with their frequency of occurrence .", "In practice this means a very large corpus of sentences with their syntactic analyses and semantic interpretatious .", "Every senteace comprises a large number of constructions not only the whole sentence and all its constituents , but also the patterns that can be abstracted from the analyzed sentence by introducing free variables for lexical elements or complex constituents .", "Parsing then does not happen by applying grammatical rules to rite input sentence , but by constructing an optinml analogy between the input sentence and as many corpus sentences , as possible .", "Sometimes the system shall need to abstract away from most of the properties of the trees in the corpus , and sometimes a part of tile input is found literally in the corpus , and can be treated as one unit in the parsing process .", "Thus the system tries to combine constructions from the corpus so as to reconstruct the input sentence as well as possible .", "llte preferred parse out of all parses of the input sentence is obtained by maximizing file conditional probability of a parse given the sentence .", "Finally , the preferred parse is added to the corpus , bringing it into a new state .", "To illustrate the basic idea , consider the following extremely simple exmnple .", "Assume that the whole corpus consists of only the following two trees S A M VP A Wa V NP . .", "pp w i x i l i l coting92 P Pr opec d Pr in Judy I I I in Nantes coling90 Then the input sentence who opened cohng 92 in Nantes in July can be analyzed as an S by combining the following constructions from file corpus NP VP Pr Pr PP V1 ?", "PP who coling 2 P Pr A A !", "j V NP P Pr i los 1 I I opened in July ACRES DE COLING 92 ?", "NANTES , 23 28 AOLr 1992 8 5 6 I ROC .", "71 COLING 92 , NANTES .", "AUG . 23 28 , 1992 The Model In order to come to fomml definitions of p , use and prefettedparse we tirst specify some basic notions .", "Labels We distinguish between file set of lexical l , lbels L and the t of non lexical labels IV .", "Lexical labels represent words .", "Non lexical l , fl els represent syi tactic and or semantic mid or i ragnlalie infonnatiou , depending on file kind of corpns being used .", "We write J for l , u l SUing Given a set of hlbcls , a string is all u tuple of elements of LI , . . . , L n u .", "All input string is ml nquple of elements of L l , t , . _ , Ln I , n . A Collckttellatio l Gill big defined OI sllil gS US usual ; l , . . . , b , c , . . . , ll a , . . . , b , c , . . . , d .", "Tree Given a set of labels J , the set of trees is defined as tile snmllest set Tree sucl that 1 i f I , , then l , , O Tree 2 i f L6 , tl , . . , , tneTice . , then l , , ll , . . . , tn eT ee For a set of trees 77cc over a t of labels , we define a function root .", "i ee 9 mid a tuuction le ; tves l ? ee L n by for n_ O , root L , tl , . . . , tn I , rot n O , le , ves L , tt , . . . , l t l ? , lves tl . . . le lves tn torn O , leaves L , O L Corpus A corpus C is a multiset of trees , ill file nse that ally tree can occur zero , nile or more times .", "File lt tves of every tree in a corpus is ml element of Ln it consfimtes the string of wo ds of which that tree is the amdysis that seemed most appropriate for understanding tile striug ill the context in which it was uttered .", "Construction8 Ill order to define the Constowtions of a tree , we need two additional notions Subffees and l tttems , Snbtrees L , tl , . . . , t n L , tl , . . , t u Snhtrees ? ll i Pattems L , t 1 , . . . , In L , O 1 ty l , ul , . . . , no .", "Vi 11 , , l nid attenls ti l Constructions T t 3beSubtrecs 1 teP , tttenls u We Slulll use tile lbllowing notation for a constnlction of a tree in a corpus tee tier nc tc . imstmctionsO0 .", "Example consider tree T . qhe trees T 1 and T 2 m conslnletions of T , while 3 is not .", "T S TI T T VP PP VP PP I v , I x op wwwi N Ju vi a po T 3 N , vp pp p Compos i t ion If t and u are trees , such Ilmt tile le tmost non lcxic ; ll leMof t is equal to the mot of n , then tou is the tree that results from substituting this leaf in t by tree u .", "The i mtial function o l eexTree 47ivc is called mlposJtion .", "We will write toU ov ; Ls touov , and ill general . . tloQ o o . . otn as tl t2o o . . . otn .", "Exmuple v t vp Np vp T VP pp t a l VP PP N Pr tr0 he I Palp e Tree 7 is a par of input slring s with respect o C , iff leaves 7 s and there me constructions tl , . , tn e , such that 1 tto . . . ot n . A tuple fl , . . . , t n of such constructions i said to generate par To f s . Note that different tuples of constructions Gm generate the . , vante parse .", "The set of par s of s will respect to C , P , use s , C , is given by I , use S , C 1 eTive lcaves T s A 3tl , . . . , t .", "tn File set of tuples of C nlstructions that generate a parse 7 ; lbples F , C , is given by luplcs L O tl , . . . , t p tl , . . . , t n C A tlo . . . otn T Probability All input string can have several parses and every such parse can be generated by veral different c mbinations f COllstruclious lrOlll tile corpus .", "What we are interested in , is , given an input string s , tile probability that arbiffury conlbinations of COllSIxuctions fro I tile colpus generate a celtain prose 25 of s . Thus we are interested ill tile colldJtkmal prolXlbility of a pmse 1 given s , with as probability space tile set of constructions of Oees in the corpus .", "l , et be a parse of iupet string s , and supl se timt 15 can exhaustively be generated by k tuples of constructions 1iqges 15 , C tl l , . . , thn , t21 , . . , 12n2 .", "Thell 7 occurs ill tll , . . . , tlnl or t21 , . . . , ten 2 or . . . . or Ikl , , . , tknk occur , aud thl , . . . , tlmt culs iff thl and th2 and . . . . ACrl ! s ol .", "23 28 AOt f 1992 8 5 7 l mlc .", "OF COI , ING 92 , NANTES , AU ? .", "I992 and t mh Occur hall , k .", "Thus the probability of Ti is given by P T i P t l l r . r3t lm u . . . . tglc3 . . . r tknvJ k p In shortened form P Ti P u el tlxl p l q l Tile events tpq are no__L mutually exclusive , since conslructions can overlap , and can include other constructions .", "The formula for tim joint probability of events E i is given by n n P , 3 E i 11 lOSilEi_l . , . hl i l i l Tile formula for the probability of combination of events E i that are not independent is given by see e . g .", "Harris 1966 k P L Ei X P Ei X l ldi1 Li2 X P hit , Ei2 Ei3 i l i i1 i2 i1 i2 . i3 . . . . P E I E2 . . . c lS k We will use Bayes decomposition formula to derive the conditional probability of 1 given s . Let 7 and Tj be parses of s ; the conditional probability of T i given s , is illen given by P Ti P sFI i P r P srl V 7 ts .", "P s z j P Tj P slTj Since P slTj is 1 for all j , we may write P T P Tils .", "p rj A parse 1 of s with nmxinml conditional probability P Tils is called a preferred parse of s . Implementation Several different implementations of DOP are possible .", "In Scholtes 1992 a neural net implementation f DOP is proposed , ltere we will show that conventional rule based parsing strategies can be applied tn DOP , by converting constructions into rules .", "A construction Can be seen as a production rule , where the lefthand side of the rule is constituted by the root of rite construction and the righthand side is constituted by the leaves of the construction .", "The only exmt condition is that of every such rule its corresponding construction should be remembered in order to generate a parse tree for the input string by composing the constructions that correspond to the rules ilmt are applied .", "For a construction t , the corresponding production rule is given by root t leaves O In order to calculate the pteterredparse of an input string by maximizing the conditional probability , all parses with all possible tuples of constructions must be generated , which becomes highly inefficient .", "Often we are not interested in all parses of an alnbiguous input string , neither in their exact probabilities , but only in which parse is the preferred parse .", "Thus we would like to have a strategy fllat estimates file top of the probability hierarchy of parses .", "llais call be achieved by using Monte Carlo techniques see e . g .", "Hammersley 1964 we estimate the preferred parse by taking random samples frotn the space of possibilities .", "This will give us a more effective approach dian exhaustively calculating the probabilities .", "Discussion Although DOP has not yet been tested thoroughly 2 , we call already predict sonic of its capabilities .", "In DOP , the probability of a parse depends on all tuples of coustructious that generate that parse .", "lhe more different ways in which a parse can be generated , the lligher its probability .", "This implies that a parse which can also be generated by relatively large constructions is favoured over a parse which can only be generated by relatively small constructions .", "This means that prepositiotml pluase attxichments arid figures of speech can be processed adequately by I OP .", "As 1o the problem of hmguage acquisition , this ntight seem problematic for DOP with all already analyzed corpus , only adult language behaviour can be simulated .", "The problem of language acquisition is itt our perspective the problem of the acquisition of an initial corpus , in which non linguistic input and pragmatics should play na important role .", "An additional remark should be devoted here to formal granlmars and disambiguation .", "Much work has been done to extend rule based granunars with selectional restrictions such that the explosion of ambiguities is constrabled considerably , llowever , to represent semantic and pragmatic onstraints i a very expensive task .", "No one has ever succeeded in doing so except in relatively small grammars .", "Furthermore , a basic question renmins as to whether it is possible to formally etlcode all of die syntactic , semantic alld pragmatic infomlation needed for disambiguation .", "In DOP , the additional infornmtion that one can draw from a corpus of hand marked structural annotations i that one can by pass the necessity for modelling world knowledge , since this will autonmtically enter into the disarnbiguation of structures by Imnd .", "Extracting constructions from these structures , and combining them in the most probable way , taking into account all possible statistical dependencies between them , preserves this world knowledge in the best possible way .", "In conclusion , it may be interesting to note that our idea of using past lallguage xperiences instead of rules , has much in cormnon with Stichs ideas about language Stich 1971 .", "lu Stichs view , judgements of gralnmaticality are not determined by applying a precompiled set of gratmuar rules , but rather have the character of a perceptual judgement on the question to what extent rite judged sentonce lotiks like the sentences the language user has in his head as examples of granlmaticality .", "The cot crete language xperiences of file past of a language user determine how a new utterance is processed ; there is no evidence for file assumption that past language experiences are generalized into a consistent heory that defines the 2 Corpora that will be used to lust DOP , mcude tile Tosca Corpus , built at the University of Nijmugen , and possibly the Penn Trcebm k , built at the Umversity of Pennsylvania .", "AcrEs Dr COLING 92 .", "23 28 AOt , q 1992 8 S 8 PROC .", "OF COLING 92 , NANTES , Aunt .", "1992 grammaticality and the structure of new utterances univocally .", "References Bahl 1983 Bahl , L . , Jelinek , F . and Mercer , R . , A Maximum Likelihood Approach to Continuous Speech Recognition , in EEE Transactions on Pattern Analysis and Machine Intelligence , Vol .", "Fillmore 1988 Fillmore , C . , Kay , P . mid OConnor , M . , Regularity and idiomaticity in grammatical constructions the ease of let alone , L , mguage 64 , p . 501 538 .", "Hanmlersley 1964 Hauunersley , J . M .", "and tlandscomb , D . C . , Monte C lo Methods , Chapnum and Hall , London .", "Hams 1966 Harris , 11 , lbeory of Probability , Addison Wesley , Reading Mass .", "Jelinek 1990 Jelinek , F . , l . afferty , J . D .", "and Mercer , R . I , . , B ic Methods of Probabilistic Context Free Granmuws , Yorktown tleights IBM RC 16374 72684 .", "Magerman 1991 Magemmn , D . and Marcus , M . , Pearl A Probabilistic Chart P user , in Proceedings of the European Chapter of the ACL91 , Berlin .", "Martin 1979 M , min , W . A . , Preliminary analysis of a bre . adth tirst parsing algorithin Theoretical , and experimental results Technical Report No .", "Salomaa 1969 Salomaa , A . , Probabilistic and weighted grmnmars , in lnfomJation and control 15 , p . 529 544 , Scha 1990 Scha , R . , Language Theory and Language Technology ; Competence and Perfomumce in Dutch , in Q . A . M .", "de Kort G . L . J .", ", Computertoepassingen in de Ncerlandistiek , Almere Landelijke Vereniging van Neerlandici .", "LVVN jaarboek Scholtes 1992 Scholtes , J . C . and Bloembergen , S . , The Design of a Neural Data Oriented Parsing 0DOP System , Proceedings of the Intonational Joint Conference on Neural Networks 1992 , Baltimore .", "Stich 1971 Stich , S . P . , What every speaker knows , in Philosophical Review 80 , p . 476 496 .", "ACRES DE COLING 92 , NANTES , 23 28 AOt l 1992 8 5 9 PROC .", "OF COLING 92 , NANTES , AUG . 23 28 , 1992"], "summary_lines": ["A Computational Model Of Language Performance: Data Oriented Parsing\n", "Data Oriented Parsing (DOP) is a model where no abstract rules, but language experiences in the form of an analyzed corpus, constitute the basis for language processing.\n", "Analyzing a new input means that the system attempts to find find the most probable way to reconstruct the input out of fragments that already exist in the corpus.\n", "Disambiguation occurs as a side-effect.\n", "DOP can be implemented by using conventional parsing strategies.\n", "In this work, super strong equivalence relations between other stochastic grammars are studied.\n", "We show that conventional context-free parsing techniques can be used in creating a parse forest for a sentence in DOP1.\n"]}
{"article_lines": ["Semantic Taxonomy Induction From Heterogenous Evidence", "We propose a novel algorithm for inducing semantic taxonomies .", "Previous algorithms for taxonomy induction have typically focused on independent classifiers for discovering new single relationships based on hand constructed or automatically discovered textual patterns .", "By contrast , our algorithm flexibly incorporates evidence from multiple classifiers over heterogenous relationships to optimize the entire structure of the taxonomy , using knowledge of a word s coordinate terms to help in determining its hypernyms , and vice versa .", "We apply our algorithm on the problem of sense disambiguated noun hyponym acquisition , where we combine the predictions of hypernym and coordinate term classifiers with the knowledge in a preexisting semantaxonomy WordNet 2 . 1 .", "We add to WordNet 2 . 1 at a relaerror reduction of a non joint algorithm using the same component classifiers .", "Finally , we show that a taxonomy built using our algorithm shows a 23 relative F score improvement over WordNet 2 . 1 on an independent testset of hypernym pairs .", "The goal of capturing structured relational knowledge about lexical terms has been the motivating force underlying many projects in lexical acquisition , information extraction , and the construction of semantic taxonomies .", "Broad coverage semantic taxonomies such as WordNet Fellbaum , 1998 and CYC Lenat , 1995 have been constructed by hand at great cost ; while a crucial source of knowledge about the relations between words , these taxonomies still suffer from sparse coverage .", "Many algorithms with the potential for automatically extending lexical resources have been proposed , including work in lexical acquisition Riloff and Shepherd , 1997 ; Roark and Charniak , 1998 and in discovering instances , named entities , and alternate glosses Etzioni et al . , 2005 ; Pasc a , 2005 .", "Additionally , a wide variety of relationship specific classifiers have been proposed , including pattern based classifiers for hyponyms Hearst , 1992 , meronyms Girju , 2003 , synonyms Lin et al . , 2003 , a variety of verb relations Chklovski and Pantel , 2004 , and general purpose analogy relations Turney et al . , 2003 .", "Such classifiers use hand written or automaticallyinduced patterns like Such NPy as NP , , or NPy like NP , , to determine , for example that NPy is a hyponym of NP , , i . e . , NPy IS A NP , , .", "While such classifiers have achieved some degree of success , they frequently lack the global knowledge necessary to integrate their predictions into a complex taxonomy with multiple relations .", "Past work on semantic taxonomy induction includes the noun hypernym hierarchy created in Caraballo , 2001 , the part whole taxonomies in Girju , 2003 , and a great deal of recent work described in Buitelaar et al . , 2005 .", "Such work has typically either focused on only inferring small taxonomies over a single relation , or as in Caraballo , 2001 , has used evidence for multiple relations independently from one another , by for example first focusing strictly on inferring clusters of coordinate terms , and then by inferring hypernyms over those clusters .", "Another major shortfall in previous techniques for taxonomy induction has been the inability to handle lexical ambiguity .", "Previous approaches have typically sidestepped the issue of polysemy altogether by making the assumption of only a single sense per word , and inferring taxonomies explicitly over words and not senses .", "Enforcing a false monosemy has the downside of making potentially erroneous inferences ; for example , collapsing the polysemous term Bush into a single sense might lead one to infer by transitivity that a rose bush is a kind of U . S . president .", "Our approach simultaneously provides a solution to the problems of jointly considering evidence about multiple relationships as well as lexical ambiguity within a single probabilistic framework .", "The key contribution of this work is to offer a solution to two crucial problems in taxonomy inProceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL , pages 801 808 , Sydney , July 2006 . c 2006 Association for Computational Linguistics duction and hyponym acquisition the problem of combining heterogenous sources of evidence in a flexible way , and the problem of correctly identifying the appropriate word sense of each new word added to the taxonomy . 1", "In section 2 . 1 we introduce our definitions for taxonomies , relations , and the taxonomic constraints that enforce dependencies between relations ; in section 2 . 2 we give a probabilistic model for defining the conditional probability of a set of relational evidence given a taxonomy ; in section 2 . 3 we formulate a local search algorithm to find the taxonomy maximizing this conditional probability ; and in section 2 . 4 we extend our framework to deal with lexical ambiguity .", "We define a taxonomy T as a set of pairwise relations R over some domain of objects DT .", "For example , the relations in WordNet include hypernymy , holonymy , verb entailment , and many others ; the objects of WordNet between which these relations hold are its word senses or synsets .", "We define that each relation R E R is a set of ordered or unordered pairs of objects i , j E DT ; we define Rij E T if relationship R holds over objects For the case of hyponym acquisition , the objects in our taxonomy are WordNet synsets .", "In this paper we focus on two of the many possible relationships between senses the hypernym relation and the coordinate term relation .", "We treat the hypernym or ISA relation as atomic ; we use the notation Hinj if a sense j is the n th ancestor of a sense i in the hypernym hierarchy .", "We will simply use Hij to indicate that j is an ancestor of i at some unspecified level .", "Two senses are typically considered to be coordinate terms or taxonomic sisters if they share an immediate parent in the hypernym hierarchy .", "We generalize this notion of siblinghood to state that two senses i and j are m , n cousins if their closest least common ij to denote that i and j are m , n cousins .", "Thus coordinate terms are 1 , 1 cousins ; technically the hypernym relation may also be seen as a specific case of this representation ; an immediate parent in the hypernym hierarchy is a 1 , 0 cousin , and the k th ancestor is a k , 0 cousin .", "A semantic taxonomy such as WordNet enforces certain taxonomic constraints which disallow particular taxonomies T . For example , the ISA transitivity constraint in WordNet requires that each synset inherits the hypernyms of its hypernym , and the part inheritance constraint requires that each synset inherits the meronyms of its hypernyms .", "For the case of hyponym acquisition we enforce the following two taxonomic constraints on the hypernym and m , n cousin relations Constraint 1 requires that the each synset inherits the hypernyms of its direct hypernym ; constraint 2 simply defines the m , n cousin relation in terms of the atomic hypernym relation .", "The addition of any new hypernym relation to a preexisting taxonomy will usually necessitate the addition of a set of other novel relations as implied by the taxonomic constraints .", "We refer to the full set of novel relations implied by a new link Rij as I Rij ; we discuss the efficient computation of the set of implied links for the purpose of hyponym acquisition in Section 3 . 4 .", "We propose that the event Rij E T has some prior probability P Rij E T , and P Rij E T P Rij E T 1 .", "We define the probability of the taxonomy as a whole as the joint probability of its component relations ; given a partition of all possible relations R A , B where A E T and We assume that we have some set of observed evidence E consisting of observed features over pairs of objects in some domain DE ; we ll begin with the assumption that our features are over pairs of words , and that the objects in the taxonomy also correspond directly to words . 4 Given a set of features ERij E E , we assume we have some model for inferring P Rij E T ERij , i . e . , the posterior probability of the event Rij E T given the corresponding evidence ERij for that relation .", "For example , evidence for the hypernym relation EHij might be the set of all observed lexico syntactic patterns containing i and j in all sentences in some corpus .", "For simplicity we make the following independence assumptions first , we assume that each item of observed evidence ERij is independent of all other observed evidence given the taxonomy T , i . e . , P E T 11ERijEE P ERij T .", "Further , we assume that each item of observed evidence ERij depends on the taxonomy T only by way of the corresponding relation Rij , i . e . , For example , if our evidence EHij is a set of observed lexico syntactic patterns indicative of hypernymy between two words i and j , we assume that whatever dependence the relations in T have on our observations may be explained entirely by dependence on the existence or non existence of the single hypernym relation H i , j .", "Applying these two independence assumptions we may express the conditional probability of our evidence given the taxonomy Within our model we define the goal of taxonomy induction to be to find the taxonomy T that maximizes the conditional probability of our observations E given the relationships of T , i . e . , to find We propose a search algorithm for finding T for the case of hyponym acquisition .", "We assume we begin with some initial possibly empty taxonomy T . We restrict our consideration of possible new taxonomies to those created by the single operation ADD RELATION Rij , T , which adds the single relation Rij to T . We define the multiplicative change OT Rij to the conditional probability P E T given the addition of a single relation Rij Here k is the inverse odds of the prior on the event Rij E T ; we consider this to be a constant independent of i , j , and the taxonomy T . To enforce the taxonomic constraints in T , for each application of the ADD RELATION operator we must add all new relations in the implied set I Rij not already in T . 5 Thus we define the multiplicative change of the full set of implied relations as the product over all new relations Rewriting the conditional probability in terms of our estimates of the posterior probabilities This definition leads to the following best first search algorithm for hyponym acquisition , which at each iteration defines the new taxonomy as the union of the previous taxonomy T and the set of novel relations implied by the relation Rij that maximizes AT I Rij and thus maximizes the conditional probability of the evidence over all possible single relations Since word senses are not directly observable , if the objects in the taxonomy are word senses as in WordNet , we must extend our model to allow for a many to many mapping e . g . , a word to sense mapping between DE and DT .", "For this setting we assume we know the function senses i , mapping from the word i to all of is possible corresponding senses .", "We assume that each set of word pair evidence ERij we possess is in fact sense pair evidence ERkl for a specific pair of senses k0 E senses i , l0 E senses j .", "Further , we assume that a new relation between two words is probable only between the correct sense pair , i . e .", "When computing the conditional probability of a specific new relation Rkl E I Rab , we assume that the relevant sense pair k0 , l0 is the one which maximizes the probability of the new relation , i . e . for k E senses i , l E senses j , Our independence assumptions for this extension need only to be changed slightly ; we now assume that the evidence ERij depends on the taxonomy T via only a single relation between sensepairs Rkl .", "Using this revised independence assumption the derivation for best first search over taxonomies for hyponym acquisition remains unchanged .", "One side effect of this revised independence assumption is that the addition of the single sense collapsed relation Rkl in the taxonomy T will explain the evidence ERij for the relation over words i and j now that such evidence has been revealed to concern only the specific senses k and l .", "We demonstrate the ability of our model to use evidence from multiple relations to extend WordNet with novel noun hyponyms .", "While in principle we could use any number of relations , for simplicity we consider two primary sources of evidence the probability of two words in WordNet being in a hypernym relation , and the probability of two words in WordNet being in a coordinate relation .", "In sections 3 . 1 and 3 . 2 we describe the construction of our hypernym and coordinate classifiers , respectively ; in section 3 . 3 we outline the efficient algorithm we use to perform local search over hyponym extended WordNets ; and in section 3 . 4 we give an example of the implicit structure based word sense disambiguation performed within our framework .", "Our classifier for the hypernym relation is derived from the hypernym only classifier described in Snow et al . , 2005 .", "The features used for predicting the hypernym relationship are obtained by parsing a large corpus of newswire and encyclopedia text with MINIPAR Lin , 1998 .", "From the resulting dependency trees the evidence EHij for each word pair i , j is constructed ; the evidence takes the form of a vector of counts of occurrences that each labeled syntactic dependency path was found as the shortest path connecting i and j in some dependency tree .", "The labeled training set is constructed by labeling the collected feature vectors as positive known hypernym or negative known non hypernym examples using WordNet 2 . 0 ; 49 , 922 feature vectors were labeled as positive training examples , and 800 , 828 noun pairs were labeled as negative training examples .", "The model for predicting P Hij EHij is then trained using logistic regression , predicting the noun pair hypernymy label from WordNet from the feature vector of lexico syntactic patterns .", "The hypernym classifier described above predicts the probability of the generalized hypernymancestor relation over words P Hij EHij .", "For the purposes of taxonomy induction , we would prefer an ancestor distance specific set of classifiers over senses , i . e . , for k E senses i , l E senses j , the set of classifiers estimating P H kl EH ij , P Hkl EHij , . . . .", "One problem that arises from directly assigning the probability P Hn ij EH ij a P Hij EHij for all n is the possibility of adding a novel hyponym to an overly specific hypernym , which might still satisfy P Hn ij EH ij for a very large n . In order to discourage unnecessary overspecification , we penalize each probability P Hk ij EH ij by a factor Ak 1 for some A 1 , and renormalize P Hkij EHij a Ak 1P Hij EHij .", "In our experiments we set A 0 . 95 .", "The classifier for learning coordinate terms relies on the notion of distributional similarity , i . e . , the idea that two words with similar meanings will be used in similar contexts Hindle , 1990 .", "We extend this notion to suggest that words with similar meanings should be near each other in a semantic taxonomy , and in particular will likely share a hypernym as a near parent .", "Our classifier for m , n cousins is derived from the algorithm and corpus given in Ravichandran et al . , 2005 .", "In that work an efficient randomized algorithm is derived for computing clusters of similar nouns .", "We use a set of more than 1000 distinct clusters of English nouns collected by their algorithm over 70 million webpages6 , with each noun i having a score representing its cosine similarity to the centroid c of the cluster to which it belongs , cos B i , c .", "We use the cluster scores of noun pairs as input to our own algorithm for predicting the m , n cousin relationship between the senses of two words i and j .", "If two words i and j appear in a cluster together , with cluster centroid c , we set our single coordinate input feature to be the minimum cluster score min cos B i , c , cos B j , c , and zero otherwise .", "For each such noun pair feature , we construct a labeled training set of m , n cousin relation labels from WordNet 2 . 1 .", "We define a noun pair i , j to be a known m , n cousin if for some senses k E senses i , l E senses j , Cmn ij E WordNet ; if more than one such relation exists , we assume the relation with smallest sum m n , breaking ties by smallest absolute difference m n .", "We consider all such labeled relationships from WordNet with 0 m , n 7 ; pairs of words that have no corresponding pair of synsets connected in the hypernym hierarchy , or with min m , n 7 , are assigned to a single class C' .", "Further , due to the symmetry of the similarity score , we merge each class Cmn Cmn U Cnm ; this implies that the resulting classifier will predict , as expected given a symmetric input , P Cmn We find 333 , 473 noun synset pairs in our training set with similarity score greater than 0 . 15 .", "We next apply softmax regression to learn a classifier that predicts P Cmn ij EC ij , predicting the WordNet class labels from the single similarity score derived from the noun pair s cluster similarity .", "Hyponym acquisition is among the simplest and most straightforward of the possible applications of our model ; here we show how we efficiently implement our algorithm for this problem .", "First , we identify the set of all the word pairs i , j over which we have hypernym and or coordinate evidence , and which might represent additions of a novel hyponym to the WordNet 2 . 1 taxonomy i . e . , that has a known noun hypernym and an unknown hyponym , or has a known noun coordinate term and an unknown coordinate term .", "This yields a list of 95 , 000 single links over threshold P Rij 0 . 12 .", "For each unknown hyponym i we may have several pieces of evidence ; for example , for the unknown term continental we have 21 relevant pieces of hypernym evidence , with links to possible hypernyms carrier , airline , unit , . . . ; and we have 5 pieces of coordinate evidence , with links to possible coordinate terms airline , american eagle , airbus , . . . .", "For each proposed hypernym or coordinate link involved with the novel hyponym i , we compute the set of candidate hypernyms for i ; in practice we consider all senses of the immediate hypernym j for each potential novel hypernym , and all senses of the coordinate term k and its first two hypernym ancestors for each potential coordinate .", "In the continental example , from the 26 individual pieces of evidence over words we construct the set of 99 unique synsets that we will consider as possible hypernyms ; these include the two senses of the word airline , the ten senses of the word carrier , and so forth .", "Next , we iterate through each of the possible hypernym synsets l under which we might add the new word i ; for each synset l we compute the change in taxonomy score resulting from adding the implied relations I H1il required by the taxonomic constraints of T . Since typically our set of all evidence involving i will be much smaller than the set of possible relations in I H1il , we may efficiently check whether , for each sense s E senses w , for all words where we have some evidence ERiw , whether s participates in some relation with i in the set of implied relations I H1il . 7 If there is more than one sense s E senses w , we add to I H1il the single relationship Ris that maximizes the taxonomy likelihood , i . e . arg maxsEsenses w AT Ris .", "A major strength of our model is its ability to correctly choose the sense of a hypernym to which to add a novel hyponym , despite collecting evidence over untagged word pairs .", "In our algorithm word sense disambiguation is an implicit side effect of our algorithm ; since our algorithm chooses to add the single link which , with its implied links , yields the most likely taxonomy , and since each distinct synset in WordNet has a different immediate neighborhood of relations , our algorithm simply disambiguates each node based on its surrounding structural information .", "As an example of sense disambiguation in practice , consider our example of continental .", "Suppose we are iterating through each of the 99 possible synsets under which we might add continental as a hyponym , and we come to the synset airline n 2 in WordNet 2 . 1 , i . e .", "a commercial organization serving as a common carrier . In this case we will iterate through each piece of hypernym and coordinate evidence ; we find that the relation H continental , carrier is satisfied with high probability for the specific synset carrier n S , the grandparent of airline n 2 ; thus the factor AT H3 continental , carrier n S is included in the factor of the set of implied relations AT I H1 continental , airline n 2 .", "Suppose we instead evaluate the first synset of airline , i . e . , airline n 1 , with the gloss a hose that carries air under pressure . For this synset none of the other 20 relationships directly implied by hypernym evidence or the 5 relationships implied by the coordinate evidence are implied by adding the single link H1 continental , airline n 1 ; thus the resulting change in the set of implied links given by the correct carrier sense of airline is much higher than that of the hose sense .", "In fact it is the largest of all the 99 considered hypernym links for continental ; H1 continental , airline n 2 is link 18 , 736 added to the taxonomy by our algorithm .", "In order to evaluate our framework for taxonomy induction , we have applied hyponym acquisition to construct several distinct taxonomies , starting with the base of WordNet 2 . 1 and only adding novel noun hyponyms .", "Further , we have constructed taxonomies using a baseline algorithm , which uses the identical hypernym and coordinate classifiers used in our joint algorithm , but which does not combine the evidence of the classifiers .", "In section 4 . 1 we describe our evaluation methodology ; in sections 4 . 2 and 4 . 3 we analyze the fine grained precision and disambiguation precision of our algorithm compared to the baseline ; in section 4 . 4 we compare the coarse grained precision of our links motivated by categories defined by the WordNet supersenses against the baseline algorithm and against an oracle for named entity recognition .", "Finally , in section 4 . 5 we evaluate the taxonomies inferred by our algorithm directly against the WordNet 2 . 1 taxonomy ; we perform this evaluation by testing each taxonomy on a set of human judgments of hypernym and non hypernym noun pairs sampled from newswire text .", "We evaluate the quality of our acquired hyponyms by direct judgment .", "In four separate annotation sessions , two judges labeled 50 , 100 , 100 , 100 samples uniformly generated from the first 100 , 1000 , 10000 , 20000 single links added by our algorithm .", "For the direct measure of fine grained precision , we simply ask for each link H X , Y added by the system , is X a Y ?", "In addition to the fine grained precision , we give a coarse grained evaluation , inspired by the idea of supersense tagging in Ciaramita and Johnson , 2003 .", "The 26 supersenses used in WordNet 2 . 1 are listed in Table 1 ; we label a hyponym link as correct in the coarse grained evaluation if the novel hyponym is placed under the appropriate supersense .", "This evaluation task is similar to a fine grained Named Entity Recognition Fleischman and Hovy , 2002 task with 26 categories ; for example , if our algorithm mistakenly inserts a novel non capital city under the hyponym state capital , it will inherit the correct supersense location .", "Finally , we evaluate the ability of our algorithm to correctly choose the appropriate sense of the hypernym under which a novel hyponym is being added .", "Our labelers categorize each candidate sense disambiguated hypernym synset suggested by our algorithm into the following categories A single hyponym hypernym pair is allowed to be simultaneously labeled 2 and 3 .", "Table 2 displays the results of our evaluation of fine grained precision for the baseline non joint algorithm Base and our joint algorithm Joint , as well as the relative error reduction ER of our algorithm over the baseline .", "We use the minimum of the two judges scores .", "Here we define fine grained precision as c1 total .", "We see that our joint algorithm strongly outperforms the baseline , and has high precision for predicting novel hyponyms up to 10 , 000 links .", "Also in Table 2 we compare the sense disambiguation precision of our algorithm and the baseline .", "Here we measure the precision of sense disambiguation among all examples where each algorithm found a correct hyponym word ; our calculation for disambiguation precision is c1 c1 c2 .", "Again our joint algorithm outperforms the baseline algorithm at all levels of recall .", "Interestingly the baseline disambiguation precision improves with higher recall ; this may be attributed to the observation that the highestconfidence hypernyms predicted by individual classifiers are likely to be polysemous , whereas hypernyms of lower confidence are more frequently monosemous and thus trivially easy to disambiguate .", "We compute coarse grained precision as c1 c3 total .", "Inferring the correct coarse grained supersense of a novel hyponym can be viewed as a fine grained 26 category Named Entity Recognition task ; our algorithm for taxonomy induction can thus be viewed as performing high accuracy fine grained NER .", "Here we compare against both the baseline non joint algorithm as well as an oracle algorithm for Named Entity Recognition , which perfectly classifies the supersense of all nouns that fall under the four supersenses person , group , location , quantity , but works only for those supersenses .", "Table 3 shows the results of this coarse grained evaluation .", "We see that the baseline non joint algorithm has higher precision than the NER oracle as 10 , 000 and 20 , 000 links ; however , both are significantly outperformed by our joint algorithm , which maintains high coarse grained precision 92 even at 20 , 000 links .", "For our final evaluation we compare our learned taxonomies directly against the currently existing hypernym links in WordNet 2 . 1 .", "In order to compare taxonomies we use a hand labeled test set of over 5 , 000 noun pairs , randomly sampled from newswire corpora described in Snow et al . , 2005 .", "We measured the performance of both our inferred taxonomies and WordNet against this test set . 8 The performance and comparison of the best WordNet classifier vs . our taxonomies is given in Table 4 .", "Our best performing inferred taxonomy on this test set is achieved after adding 30 , 000 novel hyponyms , achieving an 23 relative improvement in F score over the WN2 . 1 classifier .", "We have presented an algorithm for inducing semantic taxonomies which attempts to globally optimize the entire structure of the taxonomy .", "Our probabilistic architecture also includes a new model for learning coordinate terms based on m , n cousin classification .", "The model s ability to integrate heterogeneous evidence from different classifiers offers a solution to the key problem of choosing the correct word sense to which to attach a new hypernym .", "Thanks to Christiane Fellbaum , Rajat Raina , Bill MacCartney , and Allison Buckley for useful discussions and assistance annotating data .", "Rion Snow is supported by an NDSEG Fellowship sponsored by the DOD and AFOSR .", "This work was supported in part by the Disruptive Technology Office DTO s Advanced Question Answering for Intelligence AQUAINT Program ."], "summary_lines": ["Semantic Taxonomy Induction From Heterogenous Evidence\n", "We propose a novel algorithm for inducing semantic taxonomies.\n", "Previous algorithms for taxonomy induction have typically focused on independent classifiers for discovering new single relationships based on hand-constructed or automatically discovered textual patterns.\n", "By contrast, our algorithm flexibly incorporates evidence from multiple classifiers over heterogenous relationships to optimize the entire structure of the taxonomy, using knowledge of a word's coordinate terms to help in determining its hypernyms, and vice versa.\n", "We apply our algorithm on the problem of sense-disambiguated noun hyponym acquisition, where we combine the predictions of hypernym and coordinate term classifiers with the knowledge in a preexisting semantic taxonomy (WordNet 2.1).\n", "We add 10,000 novel synsets to WordNet 2.1 at 84% precision, a relative error reduction of 70% over a non-joint algorithm using the same component classifiers.\n", "Finally, we show that a taxonomy built using our algorithm shows a 23% relative F-score improvement over WordNet 2.1 on an independent testset of hypernym pairs.\n", "We generate tens of thousands of hypernym patterns and combine these with noun clusters to generate high-precision suggestions for unknown noun insertion into WordNet.\n", "We use known hypernym / hyponym pairs to generate training data for a machine-learning system, which then learns many lexico-syntactic patterns.\n", "We add novel terms by greedily maximizing the conditional probability of a set of relational evidence given a taxonomy.\n", "We use syntactic path patterns as features for supervised hyponymy and synonymy classifiers, whose training examples are derived automatically from WordNet.\n"]}
{"article_lines": ["Integrated Annotation For Biomedical Information Extraction", "We describe an approach to two areas of biomedical information extraction , drug development and cancer genomics .", "We have developed a framework which includes corpus annotation integrated at multiple levels a Treebank containing syntactic structure , a Propbank containing predicate argument structure , and annotation of entities and relations among the entities .", "Crucial to this approach is the proper characterization of entities as relation components , which allows the integration of the entity annotation with the syntactic structure while retaining the capacity to annotate and extract more complex events .", "We are training statistical taggers using this annotation for such extraction as well as using them for improving the annotation process .", "Work over the last few years in literature data mining for biology has progressed from linguistically unsophisticated models to the adaptation of Natural Language Processing NLP techniques that use full parsers Park et al . , 2001 ; Yakushiji et al . , 2001 and coreference to extract relations that span multiple sentences Pustejovsky et al . , 2002 ; Hahn et al . , 2002 For an overview , see Hirschman et al . , 2002 .", "In this work we describe an approach to two areas of biomedical information extraction , drug development and cancer genomics , that is based on developing a corpus that integrates different levels of semantic and syntactic annotation .", "This corpus will be a resource for training machine learning algorithms useful for information extraction and retrieval and other datamining applications .", "We are currently annotating only abstracts , although in the future we plan to expand this to full text articles .", "We also plan to make publicly available the corpus and associated statistical taggers .", "We are collaborating with researchers in the Division of Oncology at The Children s Hospital of Philadelphia , with the goal of automatically mining the corpus of cancer literature for those associations that link specified variations in individual genes with known malignancies .", "In particular we are interested in extracting three entities Gene , Variation Event , and Malignancy in the following relationship Gene X with genomic Variation Event Y is correlated with Malignancy Z .", "For example , WT1 is deleted in Wilms Tumor S . Such statements found in the literature represent individual gene variation malignancy observables .", "A collection of such observables serves two important functions .", "First , it summarizes known relationships between genes , variation events , and malignancies in the cancer literature .", "As such , it can be used to augment information available from curated public databases , as well as serve as an independent test for accuracy and completeness of such repositories .", "Second , it allows inferences to be made about gene , variation , and malignancy associations that may not be explicitly stated in the literature , both at the fact and entity instance levels .", "Such inferences provide testable hypotheses and thus future research targets .", "The other major area of focus , in collaboration with researchers in the Knowledge Integration and Discovery Systems group at GlaxoSmithKline GSK , is the extraction of information about enzymes , focusing initially on compounds that affect the activity of the cytochrome P450 CYP family of proteins .", "For example , the goal is to see a phrase like Amiodarone weakly inhibited CYP2C9 , CYP2D6 , and CYP3A4 mediated activities with Ki values of 45 . 1 271 . 6 and extract the facts amiodarone inhibits CYP2C9 with Ki 45 . 1 271 . 6 amiodarone inhibits CYP2D6 with Ki 45 . 1 271 . 6 amiodarone inhibits CYP3A4 with Ki 45 . 1 271 . 6 Previous work at GSK has used search algorithms that are based on pattern matching rules filling template slots .", "The rules rely on identifying the relevant passages by first identifying compound names and then associating them with a limited number of relational terms such as inhibit or inactivate .", "This is similar to other work in biomedical extraction projects Hirschman et al . , 2002 .", "Creating good pattern action rules for an IE problem is far from simple .", "There are many complexities in the different ways that a relation can be expressed in language , such as syntactic alternations and the heavy use of coordination .", "While sufficiently complex patterns can deal with these issues , it requires a good amount of time and effort to build such hand crafted rules , particularly since such rules are developed for each specific problem .", "A corpus that is annotated with sufficient syntactic and semantic structure offers the promise of training taggers for quicker and easier information extraction .", "The corpus that we are developing for the two different application demands consists of three levels of annotation the entities and relations among the entities for the oncology or CYP domain , syntactic structure Treebank , and predicate argument structure Propbank .", "This is a novel approach from the point of view of NLP since previous efforts at Treebanking and Propbanking have been independent of the special status of any entities , and previous efforts at entity annotation have been independent of corresponding layers of syntactic and semantic structure .", "The decomposition of larger entities into components of a relation , worthwhile by itself on conceptual grounds for entity definition , also allows the component entities to be mapped to the syntactic structure .", "These entities can be viewed as semantic types associated with syntactic constituents , and so our expectation is that automated analyses of these related levels will interact in a mutually reinforcing and beneficial way for development of statistical taggers .", "Development of such statistical taggers is proceeding in parallel with the annotation effort , and these taggers help in the annotation process , as well as being steps towards automatic extraction .", "In this paper we focus on the aspects of this project that have been developed and are in production , while also trying to give enough of the overall vision to place the work that has been done in context .", "Section 2 discusses some of the main issues around the development of the guidelines for entity annotation , for both the oncology and inhibition domains .", "Section 3 first discusses the overall plan for the different levels of annotation , and then focuses on the integration of the two levels currently in production , entity annotation and syntactic structure .", "Section 4 describes the flow of the annotation process , including the development of the statistical taggers mentioned above .", "Section 5 is the conclusion .", "Annotation has been proceeding for both the oncology and the inhibition domains .", "Here we give a summary of the main features of the annotation guidelines that have been developed .", "We have been influenced in this by previous work in annotation for biomedical information extraction Ohta et al . , 2002 ; Gaizauskas et al . , 2003 .", "However , we differ in the domains we are annotating and the design philosophy for the entity guidelines .", "For example , we have been concentrating on explicit concepts for entities like genes rather than developing a wide range ontology for the various physical instantiations .", "Gene Entity For the sake of this project the definition for Gene Entity has two significant characteristics .", "First , Gene refers to a composite entity as opposed to the strict biological definition .", "As has been noted by others , there are often ambiguities in the usage of the entity names .", "For example , it is sometimes unclear as to whether it is the gene or protein being referenced , or the same name might refer to the gene or the protein at different locations in the same document .", "Our approach to this problem is influenced by the named entity annotation in the Automatic Content Extraction ACE project Consortium , 2002 , in which geopolitical entities can have different roles , such as location or organization .", "Analogously , we consider a gene to be a composite entity that can have different roles throughout a document .", "Standardization of Gene references between different texts and between gene synonyms is handled by externally referencing each instance to a standard ontology Ashburner et al . , 2000 .", "In the context of this project , Gene refers to a conceptual entity as opposed to the specific manifestation of a gene i . e . an allele or nucleotide sequence .", "Therefore , we consider genes to be abstract concepts identifying genomic regions often associated with a function , such as MYC or TrkB ; we do not consider actual instances of such genes within the gene entity domain .", "Since we are interested in the association between Gene entities and malignancies , for this project genes are of interest to us when they have an associated variation event .", "Therefore , the combination of Gene entities and Variation events provides us with an evoked entity representing the specific instance of a gene .", "Variation Events as Relations Variations comprise a relationship between the following entities Type e . g . point mutation , translocation , or inversion , Location e . g . codon 14 , 1p36 . 1 , or base pair 278 , Original State e . g .", "Alanine , and Altered State e . g .", "Thymine .", "These four components represent the key elements necessary to describe any genomic variation event .", "Variations are often underspecified in the literature , frequently having only two or three of these specifications .", "Characterizing individual variations as a relation among such components provides us with a great deal of flexibility 1 it allows us to capture the complete variation event even when specific components are broadly spaced in the text , often spanning multiple sentences or even paragraphs ; 2 it provides us with a convenient means of tracking anaphora between detailed descriptions e . g . a point mutation at codon 14 and summary references e . g . this variation ; and 3 it provides a single structure capable of capturing the breadth of variation specifications e . g .", "A T point mutation at base pair 47 , A48 G or t 11 ; 14 q13 ; 32 .", "Malignancy The guidelines for malignancy annotation are under development .", "We are planning to define it in a manner analogous to variation , whereby a Malignancy is composed of various attribute types such as developmental stage , behavior , topographic site , and morphology .", "In the CYP Inhibition annotation task we are tagging three types of entities Each category has its own questions and uncertainties .", "Names like CYP2C19 and cytochrome P450 enzymes proclaim their membership , but there are many aliases and synonyms that do not proclaim themselves , such as 17 , 20 lyase .", "We are compiling a list of such names .", "Other substances is a potentially huge and vaguelydelimited set , which in the current corpus includes grapefruit juice and red wine as well as more obviously biochemical entities like polyunsaturatedfatty acids and erythromycin .", "The quantitative measurements we are directly interested in are those directly related to inhibition , such as IC50 and K i .", "We tag the name of the measurement , the numerical value , and the unit .", "For example , in the phrase . . . was inhibited by troleandomycin ED50 1 microM , ED50 is the name , 1 the value , and microM the unit .", "We are also tagging other measurements , since it is easy to do and may provide valuable information for future IE work .", "As has been noted in the literature on biomedical IE e . g . , Pustejovsky et al . , 2002 ; Yakushiji et al . , 2001 , the same relation can take a number of syntactic forms .", "For example , the family of words based on inhibit occurs commonly in MEDLINE abstracts about CYP enzymes as in the example in the introduction in patterns like A inhibited B , A inhibited the catalytic activity of B , inhibition of B by A , etc .", "Such alternations have led to the use of patternmatching rules often hand written to match all the relevant configurations and fill in template slots based on the resulting pattern matches .", "As discussed in the introduction , dealing with such complications in patterns can take much time and effort .", "Our approach instead is to build an annotated corpus in which the predicate argument information is annotated on top of the parsing annotations in the Treebank , the resulting corpus being called a proposition bank or Propbank .", "This newly annotated corpus is then used for training processors that will automatically extract such structures from new examples .", "In a Propbank for biomedical text , the types of inhibit examples listed above would consistently have their compounds labeled as Arg0 and their enzymes labeled as Arg1 , for nominalized forms such as A is an inhibitor of B , A caused inhibition of B , inhibition of B by A , as well the standard A inhibits B .", "We would also be able to label adjuncts consistently , such as the with prepositional phrase in CYP3A4 activity was decreased by L , S and F with IC 50 values of about 200 mM .", "In accordance with other Calibratable verbs such as rise , fall , decline , etc . , this phrase would be labeled as an Arg2 EXTENT , regardless of its syntactic role .", "A Propbank has been built on top of the Penn Treebank , and has been used to train semantic taggers , for extracting argument roles for the predicates of interest , regardless of the particular syntactic context . 1 Such semantic taggers have been developed by using machine learning techniques trained on the Penn Propbank Surdeanu et al . , 2003 ; Gildea and Palmer , 2002 ; Kingsbury and Palmer , 2002 .", "However , the Penn Treebank and Propbank involve the annotation of Wall Street Journal text .", "This text , being a financial domain , differs in significant ways from the biomedical text , and so it is necessary for this approach to have a corpus of biomedical texts such as MEDLINE articles annotated for both syntactic structure Treebanking and shallow semantic structure Propbanking .", "In this project , the syntactic and semantic annotation is being done on a corpus which is also being annotated for entities , as described in Section 2 .", "Since semantic taggers of the sort described above result in semantic roles assigned to syntactic tree constituents , it is desirable to have the entities correspond to syntactic constituents so that the semantic roles are assigned to entities .", "The entity information can function as type information and be taken advantage of by learning algorithms to help characterize the properties of the terms filling specified roles in a given predicate .", "This integration of these three different annotation levels , including the entities , is being done for the first time2 , and we discuss here three main challenges to this correspondence between entities and constituents 1 entities that are large enough to cut across multiple constituents , 2 entities within prenominal modifiers , and 3 coordination . 3 Relations and Large Entities One major area of concern is the possibility of entities that contain more than one syntactic constituent and do not match any node in the syntax tree .", "For example , as discussed in Section 2 , a variation event includes material on a variation s type , location , and state , and can cut not only across constituents , but even sentences and paragraphs .", "A simple example is point mutations at codon 12 , containing both the nominal the type of mutation and following NP the location .", "Note that while in isolation this could also be considered one syntactic constituent , the NP and PP together , the actual context is . . . point mutations at codon 12 in duodenal lavage fluid . . . .", "Since all PPs are attached at the same level , at codon 12 and in duodenal lavage fluid are sisters , and so there is no constituent consisting ofjust point mutations at codon 12 .", "Casting the variation event as a relation between different component entities allows the component entities to correspond to tree constituents , while retaining the capacity to annotate and search for more complex events .", "In this case , one component entity point mutations cor2An influential precursor to this integration is the system described in Miller et al . , 1996 .", "Our work is in much the same spirit , although the representation of the predicate argument structure via Propbank and the linkage to the entities is quite different , as well as of course the domain of annotation .", "3There are cases where the entities are so minimal that they are contained within a NP , not including the determiner , such as CpG site in the NP a CpG site . entities .", "We are not as concerned about these cases since we expect that such entity information properly contained within a base NP can be associated with the full base NP . responds to a base NP node , and at codon 12 is corresponds to the PP node that is the NP s sister .", "At the same time , the relation annotation contains the information relating these two constituents .", "Similarly , while the malignancy entity definition is currently under development , as mentioned in Section 2 . 1 , a guiding principle is that it will also be treated as a relation and broken down into component entities .", "While this also has conceptual benefits for the annotation guidelines , it has the fortunate effect of making such otherwise syntaxunfriendly malignancies as colorectal adenomas containing early cancer and acute myelomonocytic leukemia in remission amenable for mapping the component parts to syntactic nodes .", "Entities within Prenominal Modifiers While we are for the most part following the Penn Treebank guidelines Bies et al . , 1995 , we are modifying them in two important aspects .", "One concerns the prenominal modifiers , which in the Penn Treebank were left flat , with no structure , but in this biomedical domain contain much of the information e . g . , cancer associated autoimmune antigen .", "Not only would this have had no annotation for structure , but even more bizarrely , cancer associated would have been a single token in the Penn Treebank , thus making it impossible to capture the information as to what is associated with what .", "We have developed new guidelines to assign structure to prenominal entities such as breast cancer , as well as changed the tokenization guidelines to break up tokens such as cancer associated .", "Coordination We have also modified the treebank annotation to account for the well known problem of entities that are discontinuous within a coordination structure e . g . , K and H ras , where the entities are K ras and Hras .", "Our annotation tool allows for discontinuous entities , so that both K ras and H ras are annotated as genes .", "Under standard Penn Treebank guidelines for tokenization and syntactic structure , this would receive the flat structure", "K and H ras in which there is no way to directly associate the entity K ras with a constituent node .", "We have modified the treebank guidelines so that K ras and H ras are both constituents , with the ras part ofK ras represented with an empty category co indexed with ras in H ras 4 .", "As mentioned in the introduction , statistical taggers are being developed in parallel with the annotation effort .", "While such taggers are part of the final goal of the project , providing the building blocks for extracting entities and relations , they are also useful in the annotation process itself , so that the annotators only need to perform correction of automatically tagged data , instead of starting from scratch .", "We are currently annotating MEDLINE abstracts for both the oncology and CYP domains .", "The flowchart for the annotation process is shown in Figure 1 .", "Tokenization , POS tagging , entity annotation both domains , and treebanking are in full production .", "Propbank annotation and the merging of the entities and treebanking remain to be integrated into the current workflow .", "The table in Figure 2 shows the number of abstracts completed for each annotation area .", "The annotation sequence begins with tokenization and part of speech annotating .", "While both aspects are similar to those used for the Penn Treebank , there are some differences , partly alluded to in Section 3 .", "Tokens are somewhat more fine grained than in the Penn Treebank , so that H ras , e . g . , would consist of three tokens H , , and ras .", "Tokenized and part of speech annotated files are then sent to the entity annotators , either for oncology or CYP , depending on which domain the abstract has been chosen for .", "The entities described in Section 2 are annotated at this step .", "We are using WordFreak , a Java based linguistic annotation tool5 , for annotation of tokenization , POS , and entities .", "Figure 3 is a screen shot of the oncology domain annotation , here showing a variation relation being created out of component entities for type and location .", "In parallel with the entity annotation , a file is treebanked i . e . , annotated for its syntactic structure .", "Note that this is done independently of the entity annotation .", "This is because the treebanking guidelines are relatively stable once they were adjusted for the biomedical domain as described in Section 3 , while the entity definitions can require a significant period of study before stabilizing , and with the parallel treatment the treebanking can proceed without waiting for the entity annotation .", "However , this does mean that to produce the desired integrated annotation , the entity and treebanking annotations need to be merged into one representation .", "The consideration of the issues described in Section 3 has been carried out for the purpose of allowing this integration of the treebanking and entity annotation .", "This has been completed for some pilot documents , but the full merging remains to be integrated into the workflow system .", "Until recently Feb . 10 , the part of speech annotation was done by hand correcting the results of tagging the data with a part of speech tagger trained on a modified form of the Penn Treebank . 6 The tagger is a maximumentropy model utilizing the opennlp package available athttp www . sf . net projects opennlp . It has now been retrained using 315 files 122 from the oncology domain , 193 from the cyp domain .", "Figure 4 shows the improvement of the new vs . the old POS tagger on the same 294 files that have been hand corrected .", "These results are based on testing files that have already been tokenized , and thus are an evaluation only of the POS tagger and not the tokenizer .", "While not directly comparable to results such as Tateisi and Tsujii , 2004 , due to the different tag sets and tokenization , they are in the same general range . 7 The oncology and cyp entity annotation , as well as the treebanking are still being done fully manually , although that will change in the near future .", "Initial results for a tagger to identify the various components of a variation relation are promising , although not yet integrated into annotation process .", "The tagger is based on the implementation of Conditional Random Fields Lafferty et al . , 2001 in the Mallet toolkit McCallum , 2002 .", "Briefly , Conditional Random Fields are log linear models that rely on weighted features to make predictions on the input .", "Features used by our system include standard pattern matching and word features as well as some expert created regular expression features8 .", "Using 10 fold cross validation on 264 labelled abstracts containing 551 types , 1064 locations and 557 states , we obtained the following results An entity is considered correctly identified if and only if it matches the human labeling by both category type , location or state and span from position a to position b .", "At this stage we have not distinguished between initial and final states .", "While it is difficult to compare taggers that tag different types of entities e . g . , Friedman et al . , 2001 ; Gaizauskas et al . , 2003 , CRFs have been utilized for state of the art results in NP chunking and gene and protein tagging Sha and Pereira , 2003 ; McDonald and Pereira , 2004 Currently , we are beginning to investigate methods to identify relations over the variation components that are extracted using the entity tagger .", "We have described here an integrated annotation approach for two areas of biomedical information extraction .", "We discussed several issues that have arisen for this integration of annotation layers .", "Much effort has been spent on the entity definitions and how they relate to the higher level concepts which are desired for extraction .", "There are promising initial results for training taggers to extract these entities .", "Next steps in the project include 1 continued annotation of the layers we are currently doing , 2 integration of the level of predicate argument annotation , and 3 further development of the statistical taggers , including taggers for identifying relations over their component entities .", "The project described in this paper is based at the Institute for Research in Cognitive Science at the University of Pennsylvania and is supported by grant EIA0205448 from the National Science Foundation s Information Technology Research ITR program .", "We would like to thank Aravind Joshi , Jeremy Lacivita , Paula Matuszek , Tom Morton , and Fernando Pereira for their comments ."], "summary_lines": ["Integrated Annotation For Biomedical Information Extraction\n", "We describe an approach to two areas of biomedical information extraction, drug development and cancer genomics.\n", "We have developed a framework which includes corpus annotation integrated at multiple levels: a Treebank containing syntactic structure, a Propbank containing predicate-argument structure, and annotation of entities and relations among the entities.\n", "Crucial to this approach is the proper characterization of entities as relation components, which allows the integration of the entity annotation with the syntactic structure while retaining the capacity to annotate and extract more complex events.\n", "We are training statistical taggers using this annotation for such extraction as well as using them for improving the annotation process.\n"]}
{"article_lines": ["Paraphrasing For Automatic Evaluation", "This paper studies the impact of paraphrases on the accuracy of automatic evaluation .", "Given a reference sentence and a machine generated sentence , we seek to find a paraphrase of the reference sentence that is closer in wording to the machine output than the original reference .", "We apply our paraphrasing method in the context of machine translation evaluation .", "Our experiments show that the use of a paraphrased synthetic reference refines the accuracy of automatic evaluation .", "We also found a strong connection between the quality of automatic paraphrases as judged by humans and their contribution to automatic evaluation .", "The use of automatic methods for evaluating machine generated text is quickly becoming mainstream in natural language processing .", "The most notable examples in this category include measures such as BLEU and ROUGE which drive research in the machine translation and text summarization communities .", "These methods assess the quality of a machine generated output by considering its similarity to a reference text written by a human .", "Ideally , the similarity would reflect the semantic proximity between the two .", "In practice , this comparison breaks down to n gram overlap between the reference and the machine output . machine translation from the NIST 2004 MT evaluation .", "Consider the human written translation and the machine translation of the same Chinese sentence shown in Table 1 .", "While the two translations convey the same meaning , they share only auxiliary words .", "Clearly , any measure based on word overlap will penalize a system for generating such a sentence .", "The question is whether such cases are common phenomena or infrequent exceptions .", "Empirical evidence supports the former .", "Analyzing 10 , 728 reference translation pairs1 used in the NIST 2004 machine translation evaluation , we found that only 21 less than 0 . 2 of them are identical .", "Moreover , 60 of the pairs differ in at least 11 words .", "These statistics suggest that without accounting for paraphrases , automatic evaluation measures may never reach the accuracy of human evaluation .", "As a solution to this problem , researchers use multiple references to refine automatic evaluation .", "Papineni et al . 2002 shows that expanding the number of references reduces the gap between automatic and human evaluation .", "However , very few human annotated sets are augmented with multiple references and those that are available are relatively 'Each pair included different translations of the same sentence , produced by two human translators . small in size .", "Moreover , access to several references does not guarantee that the references will include the same words that appear in machine generated sentences .", "In this paper , we explore the use of paraphrasing methods for refinement of automatic evaluation techniques .", "Given a reference sentence and a machine generated sentence , we seek to find a paraphrase of the reference sentence that is closer in wording to the machine output than the original reference .", "For instance , given the pair of sentences in Table 1 , we automatically transform the reference sentence 1a . into However , Israel s answer failed to completely remove the U . S . suspicions .", "Thus , among many possible paraphrases of the reference , we are interested only in those that use words appearing in the system output .", "Our paraphrasing algorithm is based on the substitute in context strategy .", "First , the algorithm identifies pairs of words from the reference and the system output that could potentially form paraphrases .", "We select these candidates using existing lexico semantic resources such as WordNet .", "Next , the algorithm tests whether the candidate paraphrase is admissible in the context of the reference sentence .", "Since even synonyms cannot be substituted in any context Edmonds and Hirst , 2002 , this filtering step is necessary .", "We predict whether a word is appropriate in a new context by analyzing its distributional properties in a large body of text .", "Finally , paraphrases that pass the filtering stage are used to rewrite the reference sentence .", "We apply our paraphrasing method in the context of machine translation evaluation .", "Using this strategy , we generate a new sentence for every pair of human and machine translated sentences .", "This synthetic reference then replaces the original human reference in automatic evaluation .", "The key findings of our work are as follows 1 Automatically generated paraphrases improve the accuracy of the automatic evaluation methods .", "Our experiments show that evaluation based on paraphrased references gives a better approximation of human judgments than evaluation that uses original references .", "2 The quality of automatic paraphrases determines their contribution to automatic evaluation .", "By analyzing several paraphrasing resources , we found that the accuracy and coverage of a paraphrasing method correlate with its utility for automatic MT evaluation .", "Our results suggest that researchers may find it useful to augment standard measures such as BLEU and ROUGE with paraphrasing information thereby taking more semantic knowledge into account .", "In the following section , we provide an overview of existing work on automatic paraphrasing .", "We then describe our paraphrasing algorithm and explain how it can be used in an automatic evaluation setting .", "Next , we present our experimental framework and data and conclude by presenting and discussing our results .", "Automatic Paraphrasing and Entailment Our work is closely related to research in automatic paraphrasing , in particular , to sentence level paraphrasing Barzilay and Lee , 2003 ; Pang et al . , 2003 ; Quirk et al . , 2004 .", "Most of these approaches learn paraphrases from a parallel or comparable monolingual corpora .", "Instances of such corpora include multiple English translations of the same source text written in a foreign language , and different news articles about the same event .", "For example , Pang et al . 2003 expand a set of reference translations using syntactic alignment , and generate new reference sentences that could be used in automatic evaluation .", "Our approach differs from traditional work on automatic paraphrasing in goal and methodology .", "Unlike previous approaches , we are not aiming to produce any paraphrase of a given sentence since paraphrases induced from a parallel corpus do not necessarily produce a rewriting that makes a reference closer to the system output .", "Thus , we focus on words that appear in the system output and aim to determine whether they can be used to rewrite a reference sentence .", "Our work also has interesting connections with research on automatic textual entailment Dagan et al . , 2005 , where the goal is to determine whether a given sentence can be inferred from text .", "While we are not assessing an inference relation between a reference and a system output , the two tasks face similar challenges .", "Methods for entailment recognition extensively rely on lexico semantic resources Haghighi et al . , 2005 ; Harabagiu et al . , 2001 , and we believe that our method for contextual substitution can be beneficial in that context .", "Automatic Evaluation Measures A variety of automatic evaluation methods have been recently proposed in the machine translation community NIST , 2002 ; Melamed et al . , 2003 ; Papineni et al . , 2002 .", "All these metrics compute n gram overlap between a reference and a system output , but measure the overlap in different ways .", "Our method for reference paraphrasing can be combined with any of these metrics .", "In this paper , we report experiments with BLEU due to its wide use in the machine translation community .", "Recently , researchers have explored additional knowledge sources that could enhance automatic evaluation .", "Examples of such knowledge sources include stemming and TF IDF weighting Babych and Hartley , 2004 ; Banerjee and Lavie , 2005 .", "Our work complements these approaches we focus on the impact of paraphrases , and study their contribution to the accuracy of automatic evaluation .", "The input to our method consists of a reference sentence R r1 . . . rm and a system generated sentence W w1 . . . wp whose words form the sets R and W respectively .", "The output of the model is a synthetic reference sentence SRW that preserves the meaning of R and has maximal word overlap with W . We generate such a sentence by substituting words from R with contextually equivalent words from W . Our algorithm first selects pairs of candidate word paraphrases , and then checks the likelihood of their substitution in the context of the reference sentence .", "Candidate Selection We assume that words from the reference sentence that already occur in the system generated sentence should not be considered for substitution .", "Therefore , we focus on unmatched pairs of the form r , w r E R W , w E W R .", "From this pool , we select candidate pairs whose members exhibit high semantic proximity .", "In our experiments we compute semantic similarity using WordNet , a large scale lexico semantic resource employed in many NLP applications for similar pur2a .", "It is hard to believe that such tremendous changes have taken place for those people and lands that I have never stopped missing while living abroad .", "2b .", "For someone born here but has been sentimentally attached to a foreign country far from home , it is difficult to believe this kind of changes . poses .", "We consider a pair as a substitution candidate if its members are synonyms in WordNet .", "Applying this step to the two sentences in Table 2 , we obtain two candidate pairs home , place and difficult , hard .", "Contextual Substitution The next step is to determine for each candidate pair ri , wj whether wj is a valid substitution for ri in the context of r1 . . . ri_1 ri 1 . . . rm .", "This filtering step is essential because synonyms are not universally substitutable2 .", "Consider the candidate pair home , place from our example see Table 2 .", "Words home and place are paraphrases in the sense of habitat , but in the reference sentence place occurs in a different sense , being part of the collocation take place .", "In this case , the pair home , place cannot be used to rewrite the reference sentence .", "We formulate contextual substitution as a binary classification task given a context r1 . . . ri_1 ri 1 . . . rm , we aim to predict whether wj can occur in this context at position i .", "For each candidate word wj we train a classifier that models contextual preferences of wj .", "To train such a classifier , we collect a large corpus of sentences that contain the word wj and an equal number of randomly extracted sentences that do not contain this word .", "The former category forms positive instances , while the latter represents the negative .", "For the negative examples , a random position in a sentence is selected for extracting the context .", "This corpus is acquired automatically , and does not require any manual annotations .", "We represent context by n grams and local collocations , features typically used in supervised word sense disambiguation .", "Both n grams and collocations exclude the word wj .", "An n gram is a sequence of n adjacent words appearing in r1 . . . ri 10ri 1 . . . rm .", "A local collocation also takes into account the position of an n gram with respect to the target word .", "To compute local collocations for a word at position i , we extract all n grams n 1 . . . 4 beginning at position i 2 and ending at position i 2 .", "To make these position dependent , we prepend each of them with the length and starting position .", "Once the classifier3 for wj is trained , we apply it to the context r1 . . . ri 1 ri 1 . . . rm .", "For positive predictions , we rewrite the string as r1 . . . ri 1wjri 1 . . . rm .", "In this formulation , all substitutions are tested independently .", "For the example from Table 2 , only the pair difficult , hard passes this filter , and thus the system produces the following synthetic reference For someone born here but has been sentimentally attached to a foreign country far from home , it is hard to believe this kind of changes .", "The synthetic reference keeps the meaning of the original reference , but has a higher word overlap with the system output .", "One of the implications of this design is the need to develop a large number of classifiers to test contextual substitutions .", "For each word to be inserted into a reference sentence , we need to train a separate classifier .", "In practice , this requirement is not a significant burden .", "The training is done off line and only once , and testing for contextual substitution is instantaneous .", "Moreover , the first filtering step effectively reduces the number of potential candidates .", "For example , to apply this approach to the 71 , 520 sentence pairs from the MT evaluation set described in Section 4 . 1 . 2 , we had to train 2 , 380 classifiers .", "We also discovered that the key to the success of this approach is the size of the corpus used for training contextual classifiers .", "We derived training corpora from the English Gigaword corpus , and the average size of a corpus for one classifier is 255 , 000 sentences .", "We do not attempt to substitute any words that have less that 10 , 000 appearances in the Gigaword corpus .", "Our primary goal is to investigate the impact of machine generated paraphrases on the accuracy of automatic evaluation .", "We focus on automatic evaluation of machine translation due to the availability of human annotated data in that domain .", "The hypothesis is that by using a synthetic reference translation , automatic measures approximate better human evaluation .", "In section 4 . 2 , we test this hypothesis by comparing the performance of BLEU scores with and without synthetic references .", "Our secondary goal is to study the relationship between the quality of paraphrases and their contribution to the performance of automatic machine translation evaluation .", "In section 4 . 3 , we present a manual evaluation of several paraphrasing methods and show a close connection between intrinsic and extrinsic assessments of these methods .", "We begin by describing relevant background information , including the BLEU evaluation method , the test data set , and the alternative paraphrasing methods considered in our experiments .", "BLEU is the basic evaluation measure that we use in our experiments .", "It is the geometric average of the n gram precisions of candidate sentences with respect to the corresponding reference sentences , times a brevity penalty .", "The BLEU score is computed as follows where pn is the n gram precision , c is the cardinality of the set of candidate sentences and r is the size of the smallest set of reference sentences .", "To augment BLEU evaluation with paraphrasing information , we substitute each reference with the corresponding synthetic reference .", "We use the Chinese portion of the 2004 NIST MT dataset .", "This portion contains 200 Chinese documents , subdivided into a total of 1788 segments .", "Each segment is translated by ten machine translation systems and by four human translators .", "A quarter of the machine translated segments are scored by human evaluators on a one to five scale along two dimensions adequacy and fluency .", "We use only adequacy scores , which measure how well content is preserved in the translation .", "To investigate the effect of paraphrase quality on automatic evaluation , we consider two alternative paraphrasing resources Latent Semantic Analysis LSA , and Brown clustering Brown et al . , 1992 .", "These techniques are widely used in NLP applications , including language modeling , information extraction , and dialogue processing Haghighi et al . , 2005 ; Serafin and Eugenio , 2004 ; Miller et al . , 2004 .", "Both techniques are based on distributional similarity .", "The Brown clustering is computed by considering mutual information between adjacent words .", "LSA is a dimensionality reduction technique that projects a word co occurrence matrix to lower dimensions .", "This lower dimensional representation is then used with standard similarity measures to cluster the data .", "Two words are considered to be a paraphrase pair if they appear in the same cluster .", "We construct 1000 clusters employing the Brown method on 112 million words from the North American New York Times corpus .", "We keep the top 20 most frequent words for each cluster as paraphrases .", "To generate LSA paraphrases , we used the Infomap software4 on a 34 million word collection of articles from the American News Text corpus .", "We used the default parameter settings a 20 , 000 word vocabulary , the 1000 most frequent words minus a stoplist for features , a 15 word context window on either side of a word , a 100 feature reduced representation , and the 20 most similar words as paraphrases .", "While we experimented with several parameter settings for LSA and Brown methods , we do not claim that the selected settings are necessarily optimal .", "However , these methods present sensible comcompared to BLEU as well as our method for one reference .", "Two triangles indicates significant at the 99 confidence level , one triangle at the 95 confidence level and X not significant .", "Triangles point towards the better method . parison points for understanding the relationship between paraphrase quality and its impact on automatic evaluation .", "Table 3 shows synthetic references produced by the different paraphrasing methods .", "The standard way to analyze the performance of an evaluation metric in machine translation is to compute the Pearson correlation between the automatic metric and human scores Papineni et al . , 2002 ; Koehn , 2004 ; Lin and Och , 2004 ; Stent et al . , 2005 .", "Pearson correlation estimates how linearly dependent two sets of values are .", "The Pearson correlation values range from 1 , when the scores are perfectly linearly correlated , to 1 , in the case of inversely correlated scores .", "To calculate the Pearson correlation , we create a document by concatenating 300 segments .", "This strategy is commonly used in MT evaluation , because of BLEU s well known problems with documents of small size Papineni et al . , 2002 ; Koehn , 2004 .", "For each of the ten MT system translations , the evaluation metric score is calculated on the document and the corresponding human adequacy score is calculated as the average human score over the segments .", "The Pearson correlation is calculated over these ten pairs Papineni et al . , 2002 ; Stent et al . , 2005 .", "This process is repeated for ten different documents created by the same process .", "Finally , a paired t test is calculated over these ten different correlation scores to compute statistical significance .", "Table 4 shows Pearson correlation scores for BLEU and the four paraphrased augmentations , averaged over ten runs . 5 In all ten tests , our method based on contextual rewriting ContextWN improves the correlation with human scores over BLEU .", "Moreover , in nine out of ten tests ContextWN outperforms the method based on WordNet .", "The results of statistical significance testing are summarized in Table 5 .", "All the paraphrasing methods except LSA , exhibit higher correlation with human scores than plain BLEU .", "Our method significantly outperforms BLEU , and all the other paraphrasebased metrics .", "This consistent improvement confirms the importance of contextual filtering .", "5Depending on the experimental setup , correlation values can vary widely .", "Our scores fall within the range of previous researchers Papineni et al . , 2002 ; Lin and Och , 2004 .", "The third column in Table 4 shows that automatic paraphrasing continues to improve correlation scores even when two human references are paraphrased using our method .", "In the last section , we saw significant variations in MT evaluation performance when different paraphrasing methods were used to generate a synthetic reference .", "In this section , we examine the correlation between the quality of automatically generated paraphrases and their contribution to automatic evaluation .", "We analyze how the substitution frequency and the accuracy of those substitutions contributes to a method s performance .", "We compute the substitution frequency of an automatic paraphrasing method by counting the number of words it rewrites in a set of reference sentences .", "Table 6 shows the substitution frequency and the corresponding BLEU score .", "The substitution frequency varies greatly across different methods LSA is by far the most prolific rewriter , while Brown produces very few substitutions .", "As expected , the more paraphrases identified , the higher the BLEU score for the method .", "However , this increase does well as the Kappa coefficient of agreement . not translate into better evaluation performance .", "For instance , our contextual filtering method removes approximately a quarter of the paraphrases suggested by WordNet and yields a better evaluation measure .", "These results suggest that the substitution frequency cannot predict the utility value of the paraphrasing method .", "Accuracy measures the correctness of the proposed substitutions in the context of a reference sentence .", "To evaluate the accuracy of different paraphrasing methods , we randomly extracted 200 paraphrasing examples from each method .", "A paraphrase example consists of a reference sentence , a reference word to be paraphrased and a proposed paraphrase of that reference that actually occurred in a corresponding system translation .", "The judge was instructed to mark a substitution as correct only if the substitution was both semantically and grammatically correct in the context of the original reference sentence .", "Paraphrases produced by the four methods were judged by two native English speakers .", "The pairs were presented in random order , and the judges were not told which system produced a given pair .", "We employ a commonly used measure , Kappa , to assess agreement between the judges .", "We found that on all the four sets the Kappa value was around 0 . 7 , which corresponds to substantial agreement Landis and Koch , 1977 .", "As Table 7 shows , the ranking between the accuracy of the different paraphrasing methods mirrors the ranking of the corresponding MT evaluation methods shown in Table 4 .", "The paraphrasing method with the highest accuracy , ContextWN , contributes most significantly to the evaluation performance of BLEU .", "Interestingly , even methods with moderate accuracy , i . e .", "63 for WordNet , have a positive influence on the BLEU metric .", "At the same time , poor paraphrasing accuracy , such as LSA with 30 , does hurt the performance of automatic evaluation .", "To further understand the contribution of contextual filtering , we compare the substitutions made by WordNet and ContextWN on the same set of sentences .", "Among the 200 paraphrases proposed by WordNet , 73 36 . 5 were identified as incorrect by human judges .", "As the confusion matrix in Table 8 shows , 40 54 . 5 were eliminated during the filtering step .", "At the same time , the filtering erroneously eliminates 27 positive examples 21 .", "Even at this level of false negatives , the filtering has an overall positive effect .", "This paper presents a comprehensive study of the impact of paraphrases on the accuracy of automatic evaluation .", "We found a strong connection between the quality of automatic paraphrases as judged by humans and their contribution to automatic evaluation .", "These results have two important implications 1 refining standard measures such as BLEU with paraphrase information moves the automatic evaluation closer to human evaluation and 2 applying paraphrases to MT evaluation provides a task based assessment for paraphrasing accuracy .", "We also introduce a novel paraphrasing method based on contextual substitution .", "By posing the paraphrasing problem as a discriminative task , we can incorporate a wide range of features that improve the paraphrasing accuracy .", "Our experiments show improvement of the accuracy of WordNet paraphrasing and we believe that this method can similarly benefit other approaches that use lexicosemantic resources to obtain paraphrases .", "Our ultimate goal is to develop a contextual filtering method that does not require candidate selection based on a lexico semantic resource .", "One source of possible improvement lies in exploring more powerful learning frameworks and more sophisticated linguistic representations .", "Incorporating syntactic dependencies and class based features into the context representation could also increase the accuracy and the coverage of the method .", "Our current method only implements rewriting at the word level .", "In the future , we would like to incorporate substitutions at the level of phrases and syntactic trees .", "The authors acknowledge the support of the National Science Foundation Barzilay ; CAREER grant IIS 0448168 and DARPA Kauchak ; grant HR0011 06 C 0023 .", "Thanks to Michael Collins , Charles Elkan , Yoong Keok Lee , Philip Koehn , Igor Malioutov , Ben Snyder and the anonymous reviewers for helpful comments and suggestions .", "Any opinions , findings and conclusions expressed in this material are those of the author s and do not necessarily reflect the views of DARPA or NSF ."], "summary_lines": ["Paraphrasing For Automatic Evaluation\n", "This paper studies the impact of paraphrases on the accuracy of automatic evaluation.\n", "Given a reference sentence and a machine-generated sentence, we seek to find a paraphrase of the reference sentence that is closer in wording to the machine output than the original reference.\n", "We apply our paraphrasing method in the context of machine translation evaluation.\n", "Our experiments show that the use of a paraphrased synthetic reference refines the accuracy of automatic evaluation.\n", "We also found a strong connection between the quality of automatic paraphrases as judged by humans and their contribution to automatic evaluation.\n", "We show that creating synthetic reference sentences by substituting synonyms from Wordnet into the original reference sentences can increase the number of exact word matches with an MT system's output and yield significant improvements in correlations of BLEU (Papineni et al., 2002) scores with human judgments of translation adequacy.\n"]}
{"article_lines": ["Using Emoticons To Reduce Dependency In Machine Learning Techniques For Sentiment Classification", "Sentiment Classification seeks to identify a piece of text according to its author s general feeling toward their subject , be it positive or negative .", "Traditional machine learning techniques have been applied to this problem with reasonable success , but they have been shown to work well only when there is a good match between the training and test data with respect to topic .", "This paper demonstrates that match with respect to domain and time is also important , and presents preliminary experiments with training data labeled with emoticons , which has the potential of being independent of domain , topic and time .", "Recent years have seen an increasing amount of research effort expended in the area of understanding sentiment in textual resources .", "A sub topic of this research is that of Sentiment Classification .", "That is , given a problem text , can computational methods determine if the text is generally positive or generally negative ?", "Several diverse applications exist for this potential technology , ranging from the automatic filtering of abusive messages Spertus , 1997 to an in depth analysis of market trends and consumer opinions Dave et al . , 2003 .", "This is a complex and challenging task for a computer to achieve consider the difficulties involved in instructing a computer to recognise sarcasm , for example .", "Previous work has shown that traditional text classification approaches can be quite effective when applied to the sentiment analysis problem .", "Models such as Naive Bayes NB , Maximum Entropy ME and Support Vector Machines SVM can determine the sentiment of texts .", "Pang et al . 2002 used a bagof features framework based on unigrams and bigrams to train these models from a corpus of movie reviews labelled as positive or negative .", "The best accuracy achieved was 82 . 9 , using an SVM trained on unigram features .", "A later study Pang and Lee , 2004 found that performance increased to 87 . 2 when considering only those portions of the text deemed to be subjective .", "However , Engstr om 2004 showed that the bagof features approach is topic dependent .", "A classifier trained on movie reviews is unlikely to perform as well on for example reviews of automobiles .", "Turney 2002 noted that the unigram unpredictable might have a positive sentiment in a movie review e . g . unpredictable plot , but could be negative in the review of an automobile e . g . unpredictable steering .", "In this paper , we demonstrate how the models are also domain dependent how a classifier trained on product reviews is not effective when evaluating the sentiment of newswire articles , for example .", "Furthermore , we show how the models are temporally dependent how classifiers are biased by the trends of sentiment apparent during the time period represented by the training data .", "We propose a novel source of training data based on the language used in conjunction with emoticons in Usenet newsgroups .", "Training a classifier using this data provides a breadth of features that , while it does not perform to the state of the art , could function independent of domain , topic and time .", "In this section , we describe experiments we have carried out to determine the influence of domain , topic and time on machine learning based sentiment classification .", "The experiments use our own implementation of a Naive Bayes classifier and Joachim s 1999 5VMlight implementation of a Support Vector Machine classifier .", "The models were trained using unigram features , accounting for the presence of feature types in a document , rather than the frequency , as Pang et al . 2002 found that this is the most effective strategy for sentiment classification .", "When training and testing on the same set , the mean accuracy is determined using three fold crossvalidation .", "In each case , we use a paired sample t test over the set of test documents to determine whether the results produced by one classifier are statistically significantly better than those from another , at a confidence interval of at least 95 .", "Engstr om 2004 demonstrated how machinelearning techniques for sentiment classification can be topic dependent .", "However , that study focused on a three way classification positive , negative and neutral .", "In this paper , for uniformity across different data sets , we focus on only positive and negative sentiment .", "This experiment also provides an opportunity to evaluate the Naive Bayes classifier as the previous work used SVMs .", "We use subsets of a Newswire dataset kindly provided by Roy Lipski of Infonic Ltd . that relate to the topics of Finance FIN , Mergers and Aquisitions M A and a mixture of both topics MIX .", "Each subset contains further subsets of articles of positive and negative sentiment selected by independent trained annotators , each containing 100 stories .", "We trained a model on a dataset relating to one topic and tested that model using the other topics .", "Figure 1 shows the results of this experiment .", "The tendency seems to be that performance in a given topic is best if the training data is from the same topic .", "For example , the Finance trained SVM classifier achieved an accuracy of 78 . 8 against articles from Finance , but only 72 . 7 when predicting the sentiment of articles from M A .", "However , statistical testing showed that the results are not significantly different when training on one topic and testing on another .", "It is interesting to note , though , that providing a dataset of mixed topics the sub corpus MIX does not necessarily reduce topic dependency .", "Indeed , the performance of the classifiers suffers a great deal when training on mixed data confidence interval 95 .", "We conducted an experiment to compare the accuracy when training a classifier on one domain newswire articles or movie reviews from the Polarity 1 . 0 dataset used by Pang et al . 2002 and testing on the other domain .", "In Figure 2 , we see a clear indication that models trained on one domain do not perform as well on another domain .", "All differences are significant at a confidence interval of 99 . 9 .", "To investigate the effect of time on sentiment classification , we constructed a new set of movie reviews , following the same approach used by Pang et al . 2002 when they created the Polarity 1 . 0 dataset .", "The data source was the Internet Movie Review Database archive1 of movie reviews .", "The reviews were categorised as positive or negative using automatically extracted ratings .", "A review was ignored if it was not written in 2003 or 2004 ensuring that the review was written after any in the Polarity 1 . 0 dataset .", "This procedure yielded a corpus of 716 negative and 2 , 669 positive reviews .", "To create the Polarity 20042 dataset we randomly selected 700 negative reviews and 700 positive reviews , matching the size and distribution of the Polarity 1 . 0 dataset .", "The next experiment evaluated the performance of the models first against movie reviews from the same time period as the training set and then against reviews from the other time period .", "Figure 3 shows the resulting accuracies .", "These results show that while the models perform well on reviews from the same time period as the training set , they are not so effective on reviews from other time periods confidence interval 95 .", "It is also apparent that the Polarity 2004 dataset performs worse than the Polarity 1 . 0 dataset confidence interval 99 . 9 .", "A possible reason for this is that Polarity 2004 data is from a much smaller time period than that represented by Polarity 1 . 0 .", "One way of overcoming the domain , topic and time problems we have demonstrated above would be to find a source of much larger and diverse amounts of general text , annotated for sentiment .", "Users of electronic methods of communication have developed visual cues that are associated with emotional states in an attempt to state the emotion that their text represents .", "These have become known as smileys or emoticons and are glyphs constructed using the characters available on a standard keyboard , representing a facial expression of emotion see Figure 4 for some examples .", "When the author of an electronic communication uses an emoticon , they are effectively marking up their own text with an emotional state .", "This marked up text can be used to train a sentiment classifier if we assume that a smile indicates generally positive text and a frown indicates generally negative text .", "We collected a corpus of text marked up with emoticons by downloading Usenet newsgroups and saving an article if it contained an emoticon listed in Figure 4 .", "This process resulted in 766 , 730 articles being stored , from 10 , 682 , 455 messages in 49 , 759 newsgroups inspected .", "Figure 4 also lists the percentage of documents containing each emoticon type , as observed in the Usenet newsgroups .", "We automatically extracted the paragraph s containing the emoticon of interest a smile or a frown from each message and removed any superfluous formatting characters such as those used to indicate article quotations in message threads .", "In order to prevent quoted text from being considered more than once , any paragraph that began with exactly the same thirty characters as a previously observed paragraph was disregarded .", "Finally , we used the classifier developed by Cavnar and Trenkle 1994 to filter out any paragraphs of non English text .", "This process yielded a corpus of 13 , 000 article extracts containing frown emoticons .", "As investigating skew between positive and negative distributions is outside the scope of this work , we also extracted 13 , 000 article extracts containing smile emoticons .", "The dataset is referred to throughout this paper as Emoticons and contains 748 , 685 words .", "This section describes how the Emoticons corpus3 was optimised for use as sentiment classification training data .", "2 , 000 articles containing smiles and 2 , 000 articles containing frowns were held out as optimising test data .", "We took increasing amounts of articles from the remaining dataset from 2 , 000 to 22 , 000 in increments of 1 , 000 , an equal number being taken from the positive and negative sets as optimising training data .", "For each set of training data we extracted a context of an increasing number of tokens from 10 to 1 , 000 in increments of 10 both before and in a window4 around the smile or frown emoticon .", "The models were trained using this extracted context and tested on the held out dataset .", "The optimisation process revealed that the bestperforming settings for the Naive Bayes classifier was a window context of 130 tokens taken from the largest training set of 22 , 000 articles .", "Similarly , the best performance for the SVM classifier was found using a window context of 150 tokens taken from 20 , 000 articles .", "The classifiers performance in predicting the smiles and frowns of article extracts was verified using these optimised parameters and ten fold crossvalidation .", "The mean accuracy of the Naive Bayes classifier was 61 . 5 , while the SVM classifier was 70 . 1 .", "Using these same classifiers to predict the sentiment of movie reviews in Polarity 1 . 0 resulted in accuracies of 59 . 1 Naive Bayes and 52 . 1 SVM .", "We repeated the optimisation process using a held out set of 100 positive and 100 negative reviews from the Polarity 1 . 0 dataset , as it is possible that this test needs different parameter settings .", "This revealed an optimum context of a window of 50 tokens taken from a training set of 21 , 000 articles for the Naive Bayes classifier .", "Interestingly , the optimum context for the SVM classifier appeared to be a window of only 20 tokens taken from a mere 2 , 000 training examples .", "This is clearly an anomaly , as these parameters resulted in an accuracy of 48 . 9 when testing against the reserved reviews of Polarity 1 . 0 .", "We attribute this to the presence of noise , both in the training set and in the held out set , and discuss this below Section 4 . 2 .", "The second best parameters according to the optimisation process were a context of 510 tokens taken before an emoticon , from a training set of 20 , 000 examples .", "We used these optimised parameters to evaluate the sentiments of texts in the test sets used to evaluate dependency in Section 2 .", "Figures 5 , 6 and 7 show the final , optimised results across topics , domains and time periods respectively .", "These tables report the average accuracies over three folds , with the standard deviation as a measure of error .", "The emoticon trained classifiers perform well up to 70 accuracy when predicting the sentiment of article extracts from the Emoticons dataset , which is encouraging when one considers the high level of noise that is likely to be present in the dataset .", "However , they perform only a little better than one would expect by chance when classifying movie reviews , and are not effective in predicting the sentiment of newswire articles .", "This is perhaps due to the nature of the datasets one would expect language to be informal in movie reviews , and even more so in Usenet articles .", "In contrast , language in newswire articles is far more formal .", "We might therefore infer a further type of dependence in sentiment classification , that of language style dependency .", "Also , note that neither machine learning model consistently out performs the other .", "We speculate that this , and the generally mediocre performance of the classifiers , is due at least to two factors ; poor coverage of the features found in the test domains and a high level of noise found in Usenet article extracts .", "We investigate these factors below .", "Figure 8 shows the coverage of the Emoticon trained classifiers on the various test sets .", "In these experiments , we are interested in the coverage in terms of unique token types rather than the frequency of features , as this more closely reflects the training of the models see Section 2 . 1 .", "The mean coverage of the Polarity 1 . 0 dataset during three fold crossvalidation is also listed as an example of the coverage one would expect from a better performing sentiment classifier .", "The Emoticon trained classifier has much worse coverage in the test sets .", "We analysed the change in coverage of the Emoticon trained classifiers on the Polarity 1 . 0 dataset .", "We found that the coverage continued to improve as more training data was provided ; the coverage of unique token types was improving by about 0 . 6 per 1 , 000 training examples when the Emoticons dataset was exhausted .", "It appears possible that more training data will improve the performance of the Emoticon trained classifiers by increasing the coverage .", "Potential sources for this include online bulletin boards , chat forums , and further newsgroup data from Usenet and Google Groups5 .", "Future work will utilise these sources to collect more examples of emoticon use and analyse any improvement in coverage and accuracy .", "The article extracts collected in the Emoticons dataset may be noisy with respect to sentiment .", "The SVM classifier seems particularly affected by this noise .", "Figure 9 depicts the change in performance of the SVM classifier when varying the training set size and size of context extracted .", "There are significant spikes apparent for the training sizes of 2 , 000 , 3 , 000 and 6 , 000 article extracts as noted in Section 3 . 2 , where the accuracy suddenly increases for the training set size , then quickly decreases for the next set size .", "This implies that the classifier is discovering features that are useful in classifying the heldout set , but the addition of more , noisy , texts soon makes the information redundant .", "Some examples of noise taken from the Emoticons dataset are mixed sentiment , e . g .", "Sorry about venting my frustration here but I just lost it .", "Happy thanks giving everybody , sarcasm , e . g .", "Thank you so much , that s really encouraging , and spelling mistakes , e . g .", "The movies where for me a major desapointment .", "In future work we will investigate ways to remove noisy data from the Emoticons dataset .", "This paper has demonstrated that dependency in sentiment classification can take the form of domain , topic , temporal and language style .", "One might suppose that dependency is occurring because classifiers are learning the semantic sentiment of texts rather than the general sentiment of language used .", "That is , the classifiers could be learning authors sentiment towards named entities e . g . actors , directors , companies , etc . .", "However , this does not seem to be the case .", "In a small experiment , we part ofspeech tagged the Polarity 2004 dataset and automatically replaced proper nouns with placeholders .", "Retraining on this modified text did not significantly affect performance .", "But it may be that something more subtle is happening .", "Possibly , the classifiers are learning the words associated with the semantic sentiment of entities .", "For example , suppose that there has been a well received movie about mountaineering .", "During this movie , there is a particularly stirring scene involving an ice axe and most of the reviewers mention this scene .", "During training , the word ice axe would become associated with a positive sentiment , whereas one would suppose that this word does not in general express any kind of sentiment .", "In future work we will perform further tests to determine the nature of dependency in machine learning techniques for sentiment classification .", "One way of evaluating the ice axe effect could be to build a pseudo ontology of the movie reviews a map of the sentiment bearing relations that would enable the analysis of the dependencies created by the training process .", "Other extensions of this work are to collect more text marked up with emoticons , and to experiment with techniques to automatically remove noisy examples from the training data .", "This research was funded by a UK EPSRC studentship .", "I am very grateful to Thorsten Joachims , Roy Lipski , Bo Pang and John Trenkle for kindly making their data or software available , and to the anonymous reviewers for their constructive comments .", "Thanks also to Nick Jacobi for his discussion of the ice axe effect .", "Special thanks to my supervisor , John Carroll , for his continued advice and encouragement ."], "summary_lines": ["Using Emoticons To Reduce Dependency In Machine Learning Techniques For Sentiment Classification\n", "Sentiment Classification seeks to identify a piece of text according to its author's general feeling toward their subject, be it positive or negative.\n", "Traditional machine learning techniques have been applied to this problem with reasonable success, but they have been shown to work well only when there is a good match between the training and test data with respect to topic.\n", "This paper demonstrates that match with respect to domain and time is also important, and presents preliminary experiments with training data labeled with emoticons, which has the potential of being independent of domain, topic and time.\n", "In sentiment analysis research, we use emoticons in newsgroup articles to extract instances relevant for training polarity classifiers.\n", "We find that when authors of an electronic communication use an emotion, they are effectively marking up their own text with an emotional state.\n"]}
{"article_lines": ["Centering A Framework For Modeling The Local Coherence Of Discourse", "the original motivations for centering , the basic definitions underlying the centering framework , and the original theoretical claims .", "This paper attempts to meet that need .", "To accomplish this goal , we have chosen to remove descriptions of many open research questions posed in Grosz , Joshi , and Weinstein 1986 as well as solutions that were only partially developed .", "We have also greatly shortened the discussion of criteria for and constraints on a possible semantic theory as a foundation for this work .", "This paper concerns relationships among focus of attention , choice of referring expression , and perceived coherence of utterances within a discourse segment .", "It presents a framework and initial theory of centering intended to model the local component of attentional state .", "The paper examines interactions between local coherence and choice of referring expressions ; it argues that differences in coherence correspond in part to the inference demands made by different types of referring expressions , given a particular attentional state .", "It demonstrates that the attentional state properties modeled by centering can account for these differences .", "Preface Our original paper Grosz , Joshi , and Weinstein 1983 on centering claimed that certain entities mentioned in an utterance were more central than others and that this property imposed constraints on a speaker's use of different types of referring expressions .", "Centering was proposed as a model that accounted for this phenomenon .", "We argued that the coherence of discourse was affected by the compatibility between centering properties of an utterance and choice of referring expression .", "Subsequently , we revised and expanded the ideas presented therein .", "We defined various centering constructs and proposed two centering rules in terms of these constructs .", "A draft manuscript describing this elaborated centering framework and presenting some initial theoretical claims has been in wide circulation since 1986 .", "This draft Grosz , Joshi , and Weinstein 1986 has led to a number of papers by others on this topic and has been extensively cited , but has never been published . '", "We have been urged to publish the more detailed description of the centering framework and theory proposed in Grosz , Joshi , and Weinstein 1986 so that an official version would be archivally available .", "The task of completing and revising this draft became more daunting as time passed and more and more papers appeared on centering .", "Many of these papers proposed extensions to or revisions of the theory and attempted to answer questions posed in Grosz , Joshi , and Weinstein 1986 .", "It has become ever more clear that it would be useful to have a quot ; definitive quot ; statement of the original motivations for centering , the basic definitions underlying the centering framework , and the original theoretical claims .", "This paper attempts to meet that need .", "To accomplish this goal , we have chosen to remove descriptions of many open research questions posed in Grosz , Joshi , and Weinstein 1986 as well as solutions that were only partially developed .", "We have also greatly shortened the discussion of criteria for and constraints on a possible semantic theory as a foundation for this work .", "This paper presents an initial attempt to develop a theory that relates focus of attention , choice of referring expression , and perceived coherence of utterances within a discourse segment .", "The research described here is a further development of several strands of previous research .", "It fits within a larger effort to provide an overall theory of discourse structure and meaning .", "In this section we describe the larger research context of this work and then briefly discuss the previous work that led to it .", "Centering fits within the theory of discourse structure developed by Grosz and Sidner 1986 .", "Grosz and Sidner distinguish among three components of discourse structure a linguistic structure , an intentional structure , and an attentional state .", "At the level of linguistic structure , discourses divide into constituent discourse segments ; an embedding relationship may hold between two segments .", "The intentional structure comprises intentions and relations among them .", "The intentions provide the basic rationale for the discourse , and the relations represent the connections among these intentions .", "Attentional state models the discourse participants' focus of attention at any given point in the discourse .", "Changes in attentional state depend on the intentional structure and on properties of the utterances in the linguistic structure .", "Each discourse segment exhibits both local coherence i . e . , coherence among the utterances in that segment and global coherence i . e . , coherence with other segments in the discourse .", "Corresponding to these two levels of coherence are two components of attentional state ; the local level models changes in attentional state within a discourse segment , and the global level models attentional state properties at the intersegmental level .", "Grosz and Sidner argue that global coherence depends on the intentional structure .", "They propose that each discourse has an overall communicative purpose , the discourse purpose DP ; and each discourse segment has an associated intention , its discourse segment purpose DSP .", "The DP and DSP are speaker intentions ; they are correlates at the discourse level of the intentions Grice argued underlay utterance meaning Grice 1969 .", "If a discourse is multi party e . g . , a dialogue , then the DSP for a given segment is an intention of the conversational participant who initiates that segment .", "Lochbaum 1994 employs collaborative plans Grosz and Kraus 1993 to model intentional structure , and is thus able to integrate intentions of different participants .", "Satisfaction of the DSPs contributes to the satisfaction of the DP .", "Relationships between DSPs provide the basic structural relationships for the discourse ; embeddings in the linguistic structure are derived from these relationships .", "The global coherence of a discourse depends on relationships among its DP and DSPs .", "Grosz and Sidner model the global level component of the attentional state with a stack ; pushes and pops of focus spaces on the stack depend on intentional relationships .", "This paper is concerned with local coherence and its relationship to attentional state at the local level .", "Centering is proposed as a model of the local level component of attentional state .", "We examine the interactions between local coherence and choices of referring expressions , and argue that differences in coherence correspond in part to the different demands for inference made by different types of referring expressions , given a particular attentional state .", "We describe how the attentional state properties modeled by centering can account for these differences .", "Three pieces of previous research provide the background for this work .", "Grosz 1977 defined two levels of focusing in discourse global and immediate .", "Participants were said to be globally focused on a set of entities relevant to the overall discourse .", "These entities may either have been explicitly introduced into the discourse or sufficiently closely related to such entities to be considered implicitly in focus Grosz 1981 .", "In contrast , immediate focusing referred to a more local focusing process one that relates to identifying the entity that an individual utterance most centrally concerns .", "Sidner 1979 provided a detailed analysis of immediate focusing , including a distinction between the current discourse focus and potential foci .", "She gave algorithms for tracking immediate focus and rules that stated how the immediate focus could be used to identify the referents of pronouns and demonstrative noun phrases e . g . , quot ; this party , quot ; quot ; that party quot ; .", "Joshi and Kuhn 1979 and Joshi and Weinstein 1981 provided initial results on the connection between changes in immediate focus and the complexity of inferences required to integrate a representation of the meaning of an individual utterance into a representation of the meaning of the discourse of which it was a part .", "To avoid confusion with previous uses of the term quot ; focus quot ; in linguistics , they introduced the centering terminology .", "Their notions of quot ; forward looking quot ; and quot ; backward looking quot ; centers correspond approximately to Sidner's potential foci and discourse focus .", "In all of this work , focusing , whether global or immediate , was seen to function to limit the inferences required for understanding utterances in a discourse .", "Grosz and Sidner were concerned with the inferences needed to interpret anaphoric expressions of various sorts e . g . pronouns , definite descriptions , ellipsis .", "They used focusing to order candidates ; as a result the need for search was greatly reduced and the use of inference could be restricted to determining whether a particular candidate was appropriate given the embedding utterance interpretation .", "Joshi , Kuhn , and Weinstein were concerned with reducing the inferences required to integrate utterance meaning into discourse meaning .", "They used centering to determine an almost monadic predicate representation of an utterance in discourse ; they then used this representation to reduce the complexity of inference .", "In this paper , we generalize and clarify certain of Sidner's results , but adopt the quot ; centering quot ; terminology .", "We also abstract from Sidner's focusing algorithm to specify constraints on the centering process .", "We consider the relationship between coherence and inference load and examine how both interact with attentional state and choices in linguistic expression .", "The remainder of this paper is organized as follows in Section 2 , we briefly describe the phenomena motivating the development of centering that this paper aims to explain .", "Section 3 provides the basic definitions of centers and related definitions needed to present the theoretical claims of the paper .", "In Section 4 , we state the main properties of the centering framework and the major claims of centering theory .", "In Section 5 , we discuss several factors that affect centering constraints and govern the centering rules given in Section 6 .", "In Section 7 , we discuss applications of the rules and their ability to explain several discourse coherence phenomena .", "In Section 8 , we briefly outline the properties of an underlying semantic framework that are required by centering .", "Finally , in Section 9 we conclude with a brief comparison of centering with the research that preceded it and a summary of research that expands on Grosz , Joshi , and Weinstein 1986 .", "In particular , Section 9 provides references to subsequent investigations of additional factors that control centering and examinations of its cross linguistic applicability and empirical validity .", "Discourses are more than mere sequences of utterances .", "For a sequence of utterances to be a discourse , it must exhibit coherence .", "In this paper , we investigate linguistic and attentional state factors that contribute to coherence among utterances within a discourse segment .", "These factors contribute to the difference in coherence between the following two discourse segments 2 Discourse 1 is intuitively more coherent than Discourse 2 .", "This difference may be seen to arise from different degrees of continuity in what the discourse is about .", "Discourse 1 centers around a single individual , describing various actions he took and his reactions to them .", "In contrast , Discourse 2 seems to flip back and forth among several different entities .", "More specifically , the initial utterance a in each segment could begin a segment about an individual named 'John' or one about John's favorite music store or one about the fact that John wants to buy a piano .", "Whereas Discourse 1 is clearly about John , Discourse 2 has no single clear center of attention .", "Utterance 2b seems to be about the store .", "If a reader inferred that utterance 2a was about John , then that reader would perceive a change in the entity which the discourse seems to be about in going from 2a to 2b ; on the other hand , if the reader took 2a to be about the store then in going to 2b , there is no change .", "In either case , in utterance 2c John seems to be central , requiring a shift from utterance 2b , while the store becomes central again in utterance 2d , requiring yet another shift .", "This changing of 'aboutness' in fact , flipping it back and forth makes discourse 2 less coherent than discourse 1 .", "Discourses 1 and 2 convey the same information , but in different ways .", "They differ not in content or what is said , but in expression or how it is said .", "The variation in 'aboutness' they exhibit arises from different choices of the way in which they express the same propositional content .", "The differences can only be explained , however , by looking beyond the surface form of the utterances in the discourse ; different types of referring expressions and different syntactic forms make different inference demands on a hearer or reader .", "These differences in inference load underlie certain differences 2 This example and the others in this paper are single speaker texts .", "However , centering also applies to dialogue and multi party conversations .", "Issues of the interaction between turn taking and changes in centering status remain to be investigated . in coherence .", "The model of local attentional state described in this paper provides a basis for explaining these differences .", "Thus , the focus of our investigation is on interactions among choice of referring expression , attentional state , the inferences required to determine the interpretation of an utterance in a discourse segment , and coherence .", "Pronouns and definite descriptions are not equivalent with respect to their effect on coherence .", "We conjecture that this is so because they engender different inferences on the part of a hearer or reader .", "In the most pronounced cases , the wrong choice will mislead a hearer and force backtracking to a correct interpretation . '", "The following variations of a discourse sequence illustrate this problem and provide additional evidence for our conjecture .", "By using a pronoun to refer to Tony in utterance e the speaker may confuse the hearer .", "Through utterance d Terry has been the center of attention , and hence is the most likely referent of quot ; he quot ; in utterance e .", "It is only when one gets to the word quot ; sick quot ; that it is clear that it must be Tony and not Terry who is sick , and hence that the pronoun in utterance e refers to Tony not Terry .", "A much more natural sequence results if quot ; Tony quot ; is used , as the sequence 4a 4e illustrates .", "We conjecture that the form of expression in a discourse substantially affects the resource demands made upon a hearer in discourse processing and through this influences the perceived coherence of the discourse .", "It is well known from the study of complexity theory that the manner in which a class of problems is represented can significantly affect the time or space resources required by any procedure that solves the problem .", "Here too we conjecture that the manner , i . e . , linguistic form , in which a discourse represents a particular propositional content can affect the resources required by any procedure that processes that discourse .", "We use the phrase inference load placed upon the hearer to refer to the resources required to extract information from a discourse because of particular choices of linguistic expression used in the discourse .", "We conjecture that one psychological reflex of this inference load is a difference in perceived coherence among discourses that express the same propositional content using different linguistic forms .", "One of the tasks a hearer must perform in processing a discourse is to identify the referents of noun phrases in the discourse .", "It is commonly accepted , and is a hypothesis under which our work on centering proceeds , that a hearer's determination of noun phrase reference involves some process of inference .", "Hence a particular claim of centering theory is that the resource demands of this inference process are affected by the form of expression of the noun phrase .", "In Section 7 , we discuss the effect on perceived coherence of the use of pronouns and definite descriptions by relating different choices to the inferences they require the hearer or reader to make .", "We use the term centers of an utterance to refer to those entities serving to link that utterance to other utterances in the discourse segment that contains it .", "It is an utterance i . e . , the uttering of a sequence of words at a certain point in the discourse and not a sentence in isolation that has centers .", "The same sentence uttered in different discourse situations may have different centers .", "Centers are thus discourse constructs .", "Furthermore , centers are semantic objects , not words , phrases , or syntactic forms .", "Each utterance U in a discourse segment DS is assigned a set of forward looking centers , Cf U , DS ; each utterance other than the segment initial utterance is assigned a single backward looking center , Cb U , DS .", "To simplify notation , when the relevant discourse segment is clear , we will drop the associated DS and use Cb U and Cf U .", "The backward looking center of utterance Un 1 connects with one of the forwardlooking centers of utterance U .", "The connection between the backward looking center of utterance Un i and the forward looking centers of utterance Un may be of several types .", "To describe these types , we need to introduce two new relations , realizes and directly realizes , that relate centers to linguistic expressions .", "We will say that U directly realizes c if U is an utterance of some phrase' for which c is the semantic interpretation .", "Realizes is a generalization of directly realizes .", "This generalization is important for capturing certain regularities in the use of definite descriptions and pronouns .", "The precise definition of depends on the semantic theory one adopts . '", "One feature that distinguishes centering from other treatments of related discourse phenomena is that the realization relation combines syntactic , semantic , discourse , and intentional factors .", "That is , the centers of an utterance in general , and the backward looking center specifically , are determined on the basis of a combination of properties of the utterance , the discourse segment in which it occurs , and various aspects of the cognitive state of the participants of that discourse .", "Thus , for a semantic theory to support centering , it must provide an adequate basis for computing the realization relation .", "For example , NP directly realizes c may hold in cases where NP is a definite description and c is its denotation , its value free interpretation discussed in Section 8 , or an object related to it by quot ; speaker's reference quot ; Kripke 1977 .", "More importantly , when NP is a pronoun , the principles that determine the c's for which it is the case that NP directly realizes c do not derive exclusively from syntactic , semantic , or pragmatic factors .", "They are principles that must be elicited from the study of discourse itself .", "An initial formulation of some such principles is given in Section 8 . 6 The forward looking centers of Un depend only on the expressions that constitute that utterance ; they are not constrained by features of any previous utterance in the segment .", "The elements of Cf Un are partially ordered to reflect relative prominence in U .", "In Section 5 , we discuss a number of factors that may affect the ordering on the elements of Cf .", "The more highly ranked an element of Cf Un , the more likely it is to be Cb Un 1 .", "The most highly ranked element of Cf Un that is realized in Un i is the Cb U0 1 .", "Because Cf U0 is only partially ordered , some elements may , from Cf Un information alone , be equally likely to be Cb U0 1 .", "In such cases , additional criteria are needed for deciding which single entity is the Cb Un i .", "Some recent psycholinguistic evidence suggests that the syntactic role in Un i may determine this choice Gordon , Grosz , and Gilliom 1993 .", "In the remainder of the paper we will use a notation such that the elements of Cf are ranked in the order in which they are listed . '", "In particular , for presentational 4 U need not be a full clause .", "We use U here to stress again that it is the utterance , not the string of words .", "5 In the original manuscript , we defined realize in terms of situation semantics Barwise and Perry 1983 and said the relation held quot ; if either c is an element of the situation described by the utterance U or c is directly realized by some subpart of U . quot ; We discuss this further in Section 7 .", "6 In the examples in this paper , we will be concerned with the realization relationship that holds between a center and a singular definite noun phrase ; i . e . , cases where an NP directly realizes a center c . Several extensions to the theory presented here are needed to handle plural , quantified noun phrases and indefinites .", "It is also important to note that not all noun phrases in an utterance contribute centers to Cf U and not only noun phrases do so .", "More generally , events and other entities that are more often directly realized by verb phrases can also be centers , whereas negated noun phrases typically do not contribute centers ; the study of these issues is , however , beyond the scope of this paper .", "7 To simplify the presentation in the remainder of this paper , we will assume in most of the discussion that there is a total order with strict ordering between any two elements ; at those places where the partial ordering makes a significant difference we will discuss that . purposes , we will use the following schematic to refer to the centers of utterances in a sequence For Ur , COO a , Cf Un ei , ez , ep , a ek , for some k . For Und_i Cb Un i realizes em and , for all j , j m , ei is not realized in U0 1 ; i . e . , em is realized in Un 1 , and no higher ranked ej is realized in Finally , we also define three types of transition relations across pairs of utterances .", "The coherence of a segment is affected by the kinds of centering transitions engendered by a speaker's choices of linguistic realizations in the utterances constituting the segment .", "Of particular concern are choices among 1 CONTINUATION of the center from one utterance not only to the next , but also to subsequent utterances ; 2 RETENTION of the center from one utterance to the next ; 3 SHIFTING the center , if it is neither retained nor continued . '", "The centering framework described above provides the basis for stating a number of specific claims about the relationship between discourse coherence , inference load , and choice of referring expression .", "Underlying these claims is the most fundamental claim of centering theory , that to the extent a discourse adheres to centering constraints , its coherence will increase and the inference load placed upon the hearer will decrease .", "We briefly list several major claims in this section , and elaborate on the evidence or motivation for each in subsequent sections .", "Cb Un 1 ; because Cf Un is only partially ordered , additional factors may constrain the choice . '", "Before we can examine the linguistic features that contribute to an entity's being the backward looking center of an utterance , it is necessary to provide support for the claim that there is only a single backward looking center .", "In the definitions in Section 3 , there is a basic asymmetry between the Cf , which is a set , and the Cb , which is a singleton .", "Sequences like those in 6 seem to suggest that there might be multiple Cb's , analogous to the partially ordered set of Cf's .", "A priori there is no reason to think that either Susan or Betsy alone is the Cb of utterance 6b .", "If both Susan and Betsy were equally likely backward looking centers in the second utterance of these sequences , then all of these variants would be equally good or , perhaps , there would be a preference for variants 7 and 9 , which exhibit continuity of grammatical subject and object .", "However , this is not the case .", "There is a marked decrease in acceptability from version 7 to version 10 , and for many people version 10 is completely unacceptable .", "The problem is not merely a change from a pronoun back to a proper name , as this happens to the same extent in all four variants .", "It also cannot be attributed solely to a change from grammatical subject to grammatical object position , as variant 8 involves such a change and yet is better than variant 9 , which does not .", "Rather , it must be the case that Susan is the Cb at utterance b at each of the variants .", "Variants 9 and 10 can be shown to be worse than 7 and 8 because they violate the centering rules presented in the next section .", "This example suggests that pronominalization and subject position are possible linguistic mechanisms for establishing and continuing some entity as the Cb .", "In the second utterance of these sequences , Susan is realized by a pronoun in subject position ; 'she' is the Cb of this utterance .", "Utterance 7c continues Susan as Cb , whereas utterance 8c merely retains her .", "Utterances like 8c may be used to provide a basis for a shift in Cb . 11 However , this leaves open questions of the independence of syntactic role and pronominalization , and the predominance of either , for controlling centering .", "The fact that being in subject position contributes in and of itself to the likelihood an entity will be the highest ranked Cf i . e . likely to be the next Cb can be seen by contrasting the following two sequences , which differ only in their final utterances 11 The effect of various linguistic constructions on center movement and the interactions of centering shifts with global discourse structure are active areas of research .", "Section 9 provides references to such work .", "In the c utterance of each sequence , Susan is the Cb .", "Either Susan or Betsy might be the referent of the subject pronoun in the fourth utterance ; however , there appears to be a strong preference for Susan i . e . , for the reading quot ; Susan told Betsy quot ; . 12 Because this preference might be attributable to parallelism , the last utterance in 12 provides a crucial test .", "If the Cf ranking depended on pronominalization alone , the fourth utterance would allow either Susan or Betsy to be the highest ranked Cf .", "Parallelism would suggest different preferences for the Cb 12e in the two sequences .", "However , the preferred reading of the pronoun respectively , quot ; she quot ; and quot ; her quot ; in utterance e of both sequences is Susan , who is realized in the subject position of the d utterances .", "This preference holds regardless of syntactic position in the e utterances .", "Thus , we can establish a preference for subject position .", "In other circumstances , however , as the examples below illustrate , the Cb may be realized in other grammatical roles .", "In the first clause of both utterances 13d and 14d , the direct object is pronominalized ; the pronoun quot ; it quot ; refers to the green plastic tugboat .", "In 13 taking the boat to be the highest ranked Cf and hence the most likely referent for quot ; the silly thing quot ; in the second clause of utterance d yields a coherent and easily comprehensible discourse . '", "In 14 , however , pragmatic information leads to a preference for the bear , not the boat , to be the referent of quot ; the silly thing quot ; in the last utterance ; this preference is in conflict with the boat's being the most likely CID .", "That 13 is a more coherent discourse than implies report of a dialogue e . g . , quot ; She thanked her and told her she appreciated that the wine was quite rare . quot ; may lead to interpretations in which the subject pronoun is taken as referring to Betsy ; accentuation of the subject may also be used to achieve this result .", "The first of these suggests a strong interaction between dialogue verbs and centering , which is also apparent in direct speech dialogue examples .", "The relationship between this kind of lexical semantic influence over centering and that of so called 'empathy' verbs , e . g . , Kameyama 1985 , Walker , Iida , and Cote 1994 , remains to be determined .", "The second would appear to provide additional evidence for subject preference in centering , based on results of Hirschberg and Ward 1991 showing that accenting served to flip preferences in their study from either strict to sloppy or sloppy to strict readings for anaphors in the antecedent clause in VP ellipsis constructions .", "Thus , the discourses in 11 14 suggest that grammatical role is a major determinant of the ranking on the Cf , With SUBJECT OBJECT S OTHER .", "The effect of factors such as word order especially fronting , clausal subordination , and lexical semantics , as well as the interaction among these factors are areas of active investigation ; Section 9 again provides references to such work .", "In summary , these examples provide support for the claim that there is only a single Cb , that grammatical role affects an entity's being more highly ranked in Cf , and that lower ranked elements of the Cf cannot be pronominalized unless higher ranked ones are .", "Kameyama 1985 was the first to argue that grammatical role , rather than thematic role , which Sidner used , affected the Cf ranking .", "Psycholinguistic research since 1986 Hudson D'Zmura 1988 ; Gordon , Grosz , and Gilliom 1993 supports the claims that there is a single Cb and that grammatical role plays a determining role in identifying the Cb .", "It furthermore suggests that neither thematic role nor surface position is a determinant of the Cb .", "In contrast , both grammatical role and surface position were shown to affect the Cf ordering .", "Although there are as yet no psycholinguistic results related to the effect of pronominalization on determining Cb Un_i , cross linguistic work Kameyama 1985 ; Prince and Walker 1995 ; Walker , Iida , and Cote 1994 argues that it plays such a role .", "Section 9 lists several papers appearing after Grosz , Joshi , and Weinstein 1986 that investigate factors affecting the Cf ordering .", "The basic constraint on center realization is given by Rule 1 , which is stated in terms of the definitions and schematic in Section 3 .", "Rule 1 If any element of Cf Un is realized by a pronoun in Un i , then the Cb Un i must be realized by a pronoun also .", "In particular , this constraint stipulates that no element in an utterance can be realized as a pronoun unless the backward looking center of the utterance is realized as a pronoun also . '", "Rule 1 represents one function of pronominal reference the use of a pronoun to realize the Cb signals the hearer that the speaker is continuing to talk about the same thing .", "Note that Rule 1 does not preclude using pronouns for other entities so long as the Cb is realized with a pronoun .", "This is illustrated in examples 7 10 in Section 5 .", "Psychological research Gordon , Grosz , and Gilliom 1993 ; Hudson D'Zmura 1988 and cross linguistic research Di Eugenio 1990 ; Kameyama 1985 , 1986 , 1988 ; Walker , Iida , and Cote 1990 , 1994 have validated that the Cb is preferentially realized by a pronoun in English and by equivalent forms i . e . , zero pronouns in other languages .", "The basic constraint on center movement is given by Rule 2 .", "Sequences of continuation are preferred over sequences of retaining ; and sequences of retaining are to be preferred over sequences of shifting .", "In particular , a pair continuations across Un and across Un 1 , represented as Cont Un , Un i and Cont Un 1 , Un 2 respectively , is preferred over a pair of retentions , Ret Un , Un i and Ret Un i , Un 2 .", "The case is analogous for pair of retentions and a pair of shifts .", "Rule 2 reflects our intuition that continuation of the center and the use of retentions when possible to produce smooth transitions to a new center provides a basis for local coherence .", "In a locally coherent discourse segment , shifts are followed by a sequence of continuations characterizing another stretch of locally coherent discourse .", "Frequent shifting leads to a lack of local coherence , as was illustrated by the contrast between Discourse 1 and Discourse 2 in Section 2 .", "Thus , Rule 2 provides a constraint on speakers , and on natural language generation systems .", "They should plan ahead to minimize the number of shifts .", "This rule does not have the same direct implementation for interpretation systems ; rather it predicts that certain sequences produce a higher inference load than others .", "To empirically test the claim made by Rule 2 requires examination of differences in inference load of alternative multi utterance sequences that differentially realize the same content .", "Although several cross linguistic studies have investigated Rule 2 see Section 9 , there are as yet no psycholinguistic results empirically validating it .", "The two centering rules along with the partial ordering on the forward looking centers described in Section 5 constitute the basic framework of center management .", "These rules can explain a range of variations in local coherence . '", "A violation of Rule 1 occurs if a pronoun is not used for the backward looking center and some other entity is realized by a pronoun .", "Such a violation occurs in the following sequence presumed to be in a longer segment that is currently centered on John cf . also examples 9 and 10 in Section 5 The violation of Rule 1 leads to the incoherence of the sequence .", "The only possible interpretation is that the quot ; John quot ; referred to in 15c is a second person named quot ; John , quot ; not the one referred to in the preceding utterances in 15 ; however , even under this interpretation the sequence is very odd .", "The next example illustrates that this effect is 16 These rules and constraints have also been used by others as the basis for pronoun resolution algorithms based on centering .", "The earliest such attempt Brennen , Friedman , and Pollard 1987 used the uniqueness and locality of Cb constraints and ranked the Cf by grammatical role ; it employed a variant of Rule 2 in which the stated preferences on transitions were restricted to transitions between individual pairs of utterances rather than the longer sequences in the original formulation and used to decide between possible interpretations of pronouns .", "Section 9 provides references to other work on centering algorithms . independent of the grammatical position of the Cb and also demonstrates that Rule 1 operates independently of the type of centering transition .", "Without utterance 16c , this sequence , like the sequence in 15 , is unacceptable unless it is possible to consider the introduction of a second person named quot ; John . quot ; The intervening utterance c here provides for a shift in center from John to Mike , making the full sequence coherent . '", "It is important to notice that Rule 1 constrains the realization of the most highly ranked element of the Cf Un that is realized in Un 1 given that pronominalization is used .", "Obviously any entities realized in Un that are not realized in Un i , including the Cb Un as well as the highest ranked element of Cf WO , do not affect the applicability of Rule 1 .", "Likewise , if no pronouns are used , then Rule 1 is not applicable .", "Two particular ways in which such situations may hold have been noticed in previous research .", "Each leads to a different type of inference load on the hearer , both of which we believe relate to Rule 1 ; however , neither constitutes a violation of Rule 1 .", "The resulting discourses are coherent , but the determination of local coherence in the first case or the detection of a global shift in the second case requires additional inferences .", "The first case concerns realization of the Cb by a nonpronominal expression .", "Rule 1 does not preclude using a proper name or definite description for the Cb if there are no pronouns in an utterance .", "However , it appears that such uses are best when the full definite noun phrases that realize the centers do more than just refer .", "They convey some additional information , i . e . , lead the hearer or reader to draw additional inferences .", "The hearer or reader not only infers that the Cb has not changed even though no pronoun has been used , but also recognizes that the description holds of the old Cb .", "Sequences 17 and 18 are typical cases . '", "The second case concerns the use of a pronoun to realize an entity not in the Cf Un ; such uses are strongly constrained .", "The particular cases that have been identified involve instances where attention is shifted globally back to a previously centered entity e . g .", "Grosz 1977 , Reichman 19851 .", "In such cases additional inferences are required to determine that the pronoun does not refer to a member of the current forward looking centers and to identify the context back to which attention is shifting .", "Further investigation is required to determine the linguistic cues e . g . , intonation or cue phrases Grosz and Hirschberg 19921 and intentional information that are required to enable such shifts while preserving coherence , as well as the effect on inference load .", "A third complication arises in the application of Rule 1 in sequences in which the Cb of an utterance is realized but not directly realized in that utterance .", "This situation typically holds when an utterance directly realizes an entity implicitly focused by an element of the Cf of the previous utterance .", "For instance , it arises in utterances containing noun phrases that express functional relations e . g . , quot ; the door , quot ; quot ; the owner quot ; whose arguments have been directly realized in previous utterances e . g . , a house as occurs in the sequence , 19 a .", "The house appeared to have been burgled . b .", "The door was ajar . c . The furniture was in disarray .", "In this segment , the house referred to in 19a is an element of the Cf 19a .", "This house is the Cb 19b ; it is realized but not directly realized in 19b .", "Because the house is the Cb , the Cf 19b includes it as well as the door that is directly realized in the utterance .", "The Cb 19c is thus again quot ; house . quot ; We assume here that the door ranks above the house in Cf 19b .", "For example , if 19b is followed by a sentence with 'it' in the subject position , then 'it' is more likely to refer to the door . '", "This is consistent with the ranking of the door ahead of the house in Cf 19b .", "However , continuity of the house as a potential Cb for 19c is reflected in the discourse segment being interpreted to be quot ; about quot ; the house and 19c being interpreted in the same way as 19b with respect to the house .", "In Grosz , Joshi , and Weinstein 1986 we did not explore this issue further ; the general issue of the roles of functional dependence and implicit focus in centering remain open . '", "The use of different types of transitions following the rankings in Rule 2 are illustrated by the discourse below .", "Utterance 20b establishes John both as the Cb and as the most highly ranked Cf .", "In utterance 20c John continues as the Cb , but in utterance 20d he is only retained ; Mike has become the most highly ranked element of the Cf .", "Finally , in utterance 20e the backward looking center shifts to being Mike .", "Rule 1 is satisfied throughout 20 .", "Rule 1 depends only on the ordering of elements of Cf , and not on the notions of retaining and continuation .", "Different semantic theories make different commitments with respect to the completeness or definiteness required of an interpretation .", "Because the information needed to compute a unique interpretation for an utterance is not always available at the time the utterance occurs in the discourse , the ways in which a theory treats partial information affects its computational tractability as the basis for discourse interpretation .", "It is not merely that utterances themselves contain only partial information , but that it may only be subsequent to an utterance that sufficient information is available for computing a unique interpretation .", "No matter how rich a model of context one has , it will not be possible to fully constrain the interpretation of an utterance when it occurs .", "This is especially true for definite noun phrase interpretation .", "For example , several interpretations are possible for the noun phrase quot ; the Vice President of the United States , quot ; in the utterance 21 The Vice President of the United States is also President of the Senate .", "One interpretation , namely the individual who is currently Vice President , provides the appropriate basis for the interpretation of quot ; he quot ; in the subsequent utterance given in 22 However , a different interpretation , one which retains some descriptive content , provides the appropriate basis for an interpretation of the pronoun quot ; he quot ; in the slightly different subsequent utterance 23 Historically , he is the President's key person in negotiations with Congress .", "A semantic theory that forces a unique interpretation of utterance 21 will require that a computational theory or system either manage several alternatives simultaneously or provide some mechanism for retracting one choice and trying another later .", "On the other hand , a theory that allows for a partially specified interpretation must provide for refining that interpretation on the basis of subsequent utterances .", "Additional utterances may provide further constraints on an interpretation , and sequences of utterances may not be coherent , if they do not allow for a consistent choice of interpretation .", "For example , the utterance in 24 is perfectly fine after 22 , but yields an incoherent sequence after 23 . 2' 21 These examples were first written in 1986 when George Bush was Vice President .", "They remain useful for illustrating the original points if the time of original writing is taken into account .", "As we discuss later , taken as spoken now they illustrate new points .", "24 As Ambassador to China , he handled many tricky negotiations , so he does well in this job .", "To summarize , given that one purpose of discourse is to increase the information shared by speaker and hearer , it is not surprising that individual utterances convey only partial information .", "However , the lack of complete information at the time of processing an utterance means that a unique interpretation cannot be definitely determined .", "In constructing a computational model , we are then left with three choices compute all possible interpretations and filter out possibilities as more information is received ; choose on some basis a most likely interpretation and provide for quot ; backtracking quot ; and computing others later ; compute a partial interpretation .", "We conjecture that this third choice is the appropriate one for noun phrase interpretation .", "Centering theory and the centering framework rely on a certain picture of the ways in which utterances function to convey information about the world .", "One role of a semantic theory is to give substance to such a picture .", "At the time Grosz , Joshi , and Weinstein 1986 was written , it struck us that situation semantics Barwise and Perry 1983 provided a particularly convenient setting in which to frame our own theory of discourse phenomena , though our account relied only on general features of this approach and not on details of the theory as then articulated .", "The two most important features of situation semantics from the standpoint of the theory of discourse interpretation we wished to develop were 1 that it allows for the partial interpretation of utterances as they occur in discourse , and 2 that it provides a framework in which a rich theory of the dependence of interpretation on abstract features of context may be elaborated .", "There is now a large situation semantics literature that contains many extensions and refinements of the theory to which we refer the interested reader .", "The original book Barwise and Perry 1983 may be consulted for an account of the distinction between value free and value loaded interpretations used below .", "In the discussion and examples in previous sections , the Cb and the elements of Cf have all been the denotations of various noun phrases in an utterance .", "The actual situation is more complicated even if we ignore for the moment quantifiers and other syntactic complexities cf .", "Webber 1978 as well as cases in which the center is functionally dependent on , or otherwise implicitly focused by , an element of the Cf of the previous utterance cf .", "Section 7 .", "A singular definite noun phrase may contribute a number of different interpretations to Cf .", "In particular , not only the valuefree interpretation , but also various loadings may be contributed .", "For example , in the utterance , quot ; The Vice President of the United States is also President of the Senate , quot ; the noun phrase quot ; the Vice President quot ; contributes both a value loaded and a value free interpretation .", "The value free interpretation is needed in the sequence 25a c , whereas the value loaded interpretation is needed in 26a c .", "The Cb 25b and the Cb 26b are both directly realized by the anaphoric element quot ; he . quot ; But Cb 25b is the value free interpretation of the noun phrase , quot ; the Vice President quot ; as in , quot ; The Vice President of the United States is the President's key man in negotiations with Congress quot ; , whereas Cb 26b is the value loaded interpretation as in quot ; the person who now is Vice President of the United States quot ; .", "That this is so is demonstrated by the fact that 25c is true in 1994 , whereas 26c is not .", "Centering accommodates these differences by allowing the noun phrase quot ; the Vice President of the United States quot ; potentially to contribute both its value free interpretation and its value loading at the world type to Cf 25a .", "Cb 25b is then the value free interpretation , and Cb 26b is the value loaded one at the time of the writing of Grosz , Joshi , and Weinstein 1986 , George Bush , but now 1995 Al Gore .", "In each sequence , the a utterance underdetermines what element to add to Cf .", "This underdetermination may continue in a subsequent utterance with the pronoun .", "For example , that would be the case if the introductory adverbials were left off the b utterances .", "We conjecture that the correct approach to take in these cases is to add the valuefree interpretation to Cf and then load it for the interpretation of subsequent utterances if this is necessary .", "This conjecture derives from a belief that this approach will most effectively limit the inferences required .", "These loading situations thus constitute a component of the centering constituent of the discourse situation .", "It remains an open question how long to retain these loading situations , although those corresponding to elements of Cf that are not carried forward either as the Cb or as Cf s of the subsequent utterance can , obviously , be dropped .", "It is possible for an utterance to prefer either a value free VF or value loaded VL interpretation but not force it .", "For example , the second utterance in the following sequence prefers a VF interpretation but allows for the VL interpretation that is needed in the third utterance .", "In a similar way the second utterance in the following sequencen prefers the VL interpretation , but allows for the VF .", "The third utterance requires the VF interpretation .", "In these examples , both value free and value loaded interpretations are shown to stem from the same full definite noun phrase .", "There appear to be strong constraints on the kinds of transitions that are allowed , however .", "In particular , if a given utterance forces either the VF or the VL interpretation , then only this interpretation is possible in the immediately subsequent utterance .", "However , if some utterance only prefers one interpretation in a given context , but allows the other , then the subsequent utterance may pick up on either one .", "For example , the sequence , in which quot ; he quot ; may be interpreted either VF , or VL , may be followed by either 30 or 31 However , if we change 29b to force the value loaded interpretation , as in 26 , then only the value loaded interpretation 30 is possible .", "Similarly , if 29b is changed to force the value free interpretation , as in 25b , then only the value free interpretation 31 is possible .", "Speaker intentions may also enter into the determination of which entities are in the Cf .", "The referential uses of descriptions , of which Donnellan 1966 gives examples , demonstrate cases in which the quot ; referential intentions quot ; of the speaker in his use of the description play a role in determining Cb U .", "For example , consider the following sequence In these examples , ' the speaker uses a description to refer to something other than the semantic denotation of that description , i . e . the unique thing that satisfies the description if there is one .", "There are several alternative explanations of such examples , involving various accounts of speaker's intentions , mutual belief , and the like .", "A complete discussion of these issues is beyond the scope of this paper .", "The importance of these cases resides in showing that Cf U may include more than one entity that is realized by a single NP in U .", "In this case , the noun phrase quot ; her husband quot ; contributes two individuals , the husband and the lover , to Cf 32a and Cf 33a .", "This can be seen by observing that both discourses seem equally appropriate and that the backward looking centers of 32b and 33b are respectively the husband and the lover , which are realized by their anaphoric elements .", "These examples introduce a number of research issues concerning the representation and management of the Cb and Cf discourse entities .", "The account given here depends on a semantic theory that permits minimal commitment in interpretations .", "The open question is which constraints on centers are introduced at which points during processing .", "We must leave this as a topic for future work .", "This theory can be contrasted with two previous research efforts that spurred this work Sidner's 1979 original work on immediate focusing and pronouns , and Joshi and Weinstein's 1981 subsequent work on centering and inferences .", "The centering theory discussed here is quite close to Sidner's original theory , both in attacking local discourse issues and in the general outline of approach .", "However , it differs in several details .", "In Sidner's theory , each utterance provides an immediate discourse focus , an actor focus , and a set of potential foci .", "The discourse and actor foci may coincide , but need not .", "Her potential foci are roughly analogous to our Cf .", "The Cb for an utterance sometimes coincides with her actor focus and sometimes with her discourse focus .", "She distinguishes these two to handle various cases of multiple pronouns .", "However , as we have shown , utterances do not have multiple Cbs .", "Furthermore , utterances can have more than two pronouns , so merely adding a second kind of immediate focus is of limited use .", "The difference between these two theories can be seen from the following example from Sidner 19791 On Sidner's account , Carl is the actor focus after 34b and Jeff is the discourse focus .", "Because the actor focus is preferred as the referent of pronominal expressions , Carl is the leading candidate for the entity referred to by he in 34c .", "It is difficult to rule this , case out without invoking fairly special domain specific rules .", "On our account , Jeff is the Cb at 34b and there is no problem .", "The type of example Sidner was concerned about would occur if utterance 34c were replaced by quot ; He thinks he studies too much . quot ; However , the centering rules would still hold in this case .", "They provide no constraints on additional pronouns so long as the highest ranked Cf is realized by a pronoun .", "However , the rules are incomplete ; in particular , as given they do not specify which pronoun in a multipronoun utterance refers to the Cb .", "The center management rules are based solely on the Cb and the highest ranked member of the Cf .", "As a result , while there are cases of multiple pronouns for which the theory makes incomplete predictions , having both an actor and a discourse focus will not handle these cases in general .", "Joshi and Kuhn 1979 and Joshi and Weinstein 1981 presented a preliminary report on their research regarding the connection between the computational complexity of the inferences required to process a discourse and the coherence of that discourse as assessed by measures that invoke centering phenomena .", "However , their basic definitions conflate the centers of an utterance with the linguistic expressions that realize those centers .", "In some of their examples it is unclear whether the shift in center or the particular expression used to realize the center is responsible for differences in coherence and inference load .", "Our present work has clarified these differences while maintaining Joshi and Weinstein's basic focus on the interaction between inference load and center management .", "Since Grosz , Joshi , and Weinstein 1986 was first circulated a number of researchers have tested and developed aspects of the theory presented here . '", "This follow on research can be roughly grouped in a few main areas", "We want to thank Breck Baldwin , Felicia Hurewitz , Andy Kehler , Karen Lochbaum , Christine Nakatani , Ellen Prince , and Lyn Walker for their valuable comments , which helped us improve both the content and the presentation of our paper .", "We are also grateful to Carolyn Elken for helping us keep track of the various drafts of this paper and for providing valuable editorial help .", "Partial support for the first author was provided by grants NSF IRI 90 09018 and IRI 93 08173 ; the second author was partially supported by the ARO Grant DAAL03 89 0031 and ARPA Grant N00014 90 1863 ."], "summary_lines": ["Centering: A Framework For Modeling The Local Coherence Of Discourse\n", "This paper concerns relationships among focus of attention, choice of referring expression, and perceived coherence of utterances within a discourse segment.\n", "It presents a framework and initial theory of centering intended to model the local component of attentional state.\n", "The paper examines interactions between local coherence and choice of referring expressions; it argues that differences in coherence correspond in part to the inference demands made by different types of referring expressions, given a particular attentional state.\n", "It demonstrates that the attentional state properties modeled by centering can account for these differences.\n", "Our centering model uses a ranking of discourse entities realized in particular sentence sand computes transitions between adjacent sentences to provide insight in the felicity of texts.\n", "Our centering theory postulates strong links between the center of attention in comprehension of adjacent sentences and syntactic position and form of reference.\n", "Our centering Theory is an entity-based theory of local coherence, which claims that certain entities mentioned in an utterance are more central than others and that this property constrains a speaker's use of certain referring expressions.\n", "Our centering Theory is an influential framework for modelling entity coherence in computational linguistics in the last two decades.\n"]}
{"article_lines": ["A Discriminative Global Training Algorithm For Statistical MT", "This paper presents a novel training algorithm for a linearly scored block sequence translation model .", "The key component is a new procedure to directly optimize the global scoring function used by a SMT decoder .", "No translation , language , or distortion model probabilities are used as in earlier work on SMT .", "Therefore our method , which employs less domain specific knowledge , is both simpler and more extensible than previous approaches .", "Moreover , the training procedure treats the decoder as a black box , and thus can be used to optimize any decoding scheme .", "The training algorithm is evaluated on a standard Arabic English translation task .", "This paper presents a view of phrase based SMT as a sequential process that generates block orientation sequences .", "A block is a pair of phrases which are translations of each other .", "For example , Figure 1 shows an Arabic English translation example that uses four blocks .", "During decoding , we view translation as a block segmentation process , where the input sentence is segmented from left to right and the target sentence is generated from bottom to top , one block at a time .", "A monotone block sequence is generated except for the possibility to handle some local phrase re ordering .", "In this local re ordering model Tillmann and Zhang , 2005 ; Kumar and Byrne , 2005 a block with orientation is generated relative to its predecessor block .", "During decoding , we maximize the score of a block orientation sequence where is a block , is its predecessor block , and eft ight eutral is a threevalued orientation component linked to the block a block is generated to the left or the right of its predecessor block , where the orientation of the predecessor block is ignored .", "Here , is the number of blocks in the translation .", "We are interested in learning the weight vector from the training data . is a high dimensional binary feature representation of the block orientation pair .", "The block orientation se quence is generated under the restriction that the concatenated source phrases of the blocks yield the input sentence .", "In modeling a block sequence , we emphasize adjacent block neighbors that have right or left orientation , since in the current experiments only local block swapping is handled neutral orientation is used for detached blocks as described in Tillmann and Zhang , 2005 .", "This paper focuses on the discriminative training of the weight vector used in Eq .", "The decoding process is decomposed into local decision steps based on Eq .", "1 , but the model is trained in a global setting as shown below .", "The advantage of this approach is that it can easily handle tens of millions of features , e . g . up to million features for the experiments in this paper .", "Moreover , under this view , SMT becomes quite similar to sequential natural language annotation problems such as part of speech tagging and shallow parsing , and the novel training algorithm presented in this paper is actually most similar to work on training algorithms presented for these task , e . g . the on line training algorithm presented in McDonald et al . , 2005 and the perceptron training algorithm presented in Collins , 2002 .", "The current approach does not use specialized probability features as in Och , 2003 in any stage during decoder parameter training .", "Such probability features include language model , translation or distortion probabilities , which are commonly used in current SMT approaches 1 .", "We are able to achieve comparable performance to Tillmann and Zhang , 2005 .", "The novel algorithm differs computationally from earlier work in discriminative training algorithms for SMT Och , 2003 as follows No additional development data set is necessary as the weight vector is trained on bilingual training data only .", "The paper is structured as follows Section 2 presents the baseline block sequence model and the feature representation .", "Section 3 presents the discriminative training algorithm that learns a good global ranking function used during decoding .", "Section 4 presents results on a standard Arabic English translation task .", "Finally , some discussion and future work is presented in Section 5 .", "This paper views phrase based SMT as a block sequence generation process .", "Blocks are phrase pairs consisting of target and source phrases and local phrase re ordering is handled by including so called block orientation .", "Starting point for the block based translation model is a block set , e . g . about million Arabic English phrase pairs for the experiments in this paper .", "This block set is used to decode training sentence to obtain block orientation sequences that are used in the discriminative parameter training .", "Nothing but the block set and the parallel training data is used to carry out the training .", "We use the block set described in Al Onaizan et al . , 2004 , the use of a different block set may effect translation results .", "Rather than predicting local block neighbors as in Tillmann and Zhang , 2005 , here the model parameters are trained in a global setting .", "Starting with a simple model , the training data is decoded multiple times the weight vector is trained to discriminate block sequences with a high translation score against block sequences with a high BLEU score 2 .", "The high BLEU scoring block sequences are obtained as follows the regular phrase based decoder is modified in a way that it uses the BLEU score as optimization criterion independent of any translation model .", "Here , searching for the highest BLEU scoring block sequence is restricted to local re ordering as is the model based decoding as shown in Fig .", "The BLEU score is computed with respect to the single reference translation provided by the parallel training data .", "A block sequence with an average BLEU score of about is obtained for each training sentence 3 .", "The true maximum BLEU block sequence as well as the high scoring No computationally expensive best lists are generated during training for each input sentence a single block sequence is generated on each iteration over the training data .", "block sequences are represented by high dimensional feature vectors using the binary features defined below and the translation process is handled as a multi class classification problem in which each block sequence represents a possible class .", "The effect of this training procedure can be seen in Figure 2 each decoding step on the training data adds a high scoring block sequence to the discriminative training and decoding performance on the training data is improved after each iteration along with the test data decoding performance .", "A theoretical justification for the novel training procedure is given in Section 3 .", "We now define the feature components for the block bigram feature vector in Eq .", "Although the training algorithm can handle realvalued features as used in Och , 2003 ; Tillmann and Zhang , 2005 the current paper intentionally excludes them .", "The current feature functions are similar to those used in common phrase based translation systems for them it has been shown that good translation performance can be achieved 4 .", "A systematic analysis of the novel training algorithm will allow us to include much more sophisticated features in future experiments , i . e .", "POSbased features , syntactic or hierarchical features Chiang , 2005 .", "The dimensionality of the feature vector depends on the number of binary features .", "For illustration purposes , the binary features are chosen such that they yield on the example block sequence in Fig .", "There are phrase based and word based features The feature is a unigram phrase based feature capturing the identity of a block .", "Additional phrase based features include block orientation , target and source phrase bigram features .", "Word based features are used as well , e . g . feature captures word to word translation dependencies similar to the use of Model probabilities in Koehn et al . , 2003 .", "Additionally , we use distortion features involving relative source word position and gram features for adjacent target words .", "These features correspond to the use of a language model , but the weights for theses features are trained on the parallel training data only .", "For the most complex model , the number of features is about million ignoring all features that occur only once .", "Throughout the section , we let .", "Each block sequence corresponds to a candidate translation .", "In the training data where target translations are given , a BLEU score can be calculated for each against the target translations .", "In this set up , our goal is to find a weight vector such that the higher the higher the corresponding BLEU score should be .", "If we can find such a weight vector , then block decoding by searching for the highest will lead to good translation with high BLEU score .", "Formally , we denote a source sentence by , and let be the set of possible candidate oriented block sequences that the decoder can generate from .", "For example , in a monotone decoder , the set contains block sequences that cover the source sentence in the same order .", "For a decoder with local re ordering , the candidate set also includes additional block sequences with re ordered block configurations that the decoder can efficiently search .", "Therefore depending on the specific implementation of the decoder , the set can be different .", "In general , is a subset of all possible oriented block sequences that are consistent with input sentence .", "Given a scoring function and an input sentence , we can assume that the decoder implements the following decoding rule 2 Let be a set of training sentences .", "Each sentence is associated with a set of possible translation block sequences that are searchable by the decoder .", "Each translation block sequence induces a translation , which is then assigned a BLEU score obtained by comparing against the target translations .", "The block consists of target phrase violate and source phrase tnthk otherwise Lebanese is a word in the target phrase of block and AllbnAny is a word in the source phrase otherwise \u277bgoal of the training is to find a weight vector such that for each training sentence , the corresponding decoder outputs which has the maximum BLEU score among all based on Eq .", "In other words , if maximizes the scoring function , then also maximizes the BLEU metric .", "Based on the description , a simple idea is to learn the BLEU score for each candidate block sequence .", "That is , we would like to estimate such that .", "This can be achieved through least squares regression .", "It is easy to see that if we can find a weight vector that approximates , then the decoding rule in Eq .", "2 automatically maximizes the BLEU score .", "However , it is usually difficult to estimate reliably based only on a linear combination of the feature vector as in Eq .", "We note that a good decoder does not necessarily employ a scoring function that approximates the BLEU score .", "Instead , we only need to make sure that the top ranked block sequence obtained by the decoder scoring function has a high BLEU score .", "To formulate this idea , we attempt to find a decoding parameter such that for each sentence in the training data , sequences in with the highest BLEU scores should get scores higher than those with low BLEU scores .", "Denote by a set of block sequences in with the highest BLEU scores .", "Our decoded result should lie in this set .", "We call them the truth .", "The set of the remaining sequences is , which we shall refer to as the alternatives .", "We look for a weight vector that minimize the following training criterion where are BLEU scores , are translation scores , and .", "We refer to this formulation as costMargin cost sensitive margin method for each training sentence the costMargin between the true block sequence set and the alternative block sequence set is maximized .", "Note that due to the truth and alternative set up , we always have .", "This loss function gives an upper bound of the error we will suffer if the order of and is wrongly predicted that is , if we predict instead of .", "It also has the property that if for the BLEU scores holds , then the loss value is small proportional to .", "A major contribution of this work is a procedure to solve Eq .", "3 approximately .", "The main difficulty is that the search space covered by the decoder can be extremely large .", "It cannot be enumerated for practical purposes .", "Our idea is to replace this large space by a small subspace which we call relevant set .", "The possibility of this reduction is based on the following theoretical result .", "If is a convex function of as in our choice , then we know that the global optimal solution remains the same if the whole decoding space is replaced by the relevant set where is a non negative real valued loss function whose specific choice is not critical for the purposes of this paper , and is a regularization parameter .", "In our experiments , results are obtained using the following convex loss .", "Each subspace will be significantly smaller than .", "This is because it only includes those alternatives with score close to one of the selected truth .", "These are the most important alternatives that are easily confused with the truth .", "Essentially the lemma says that if the decoder works well on these difficult alternatives relevant points , then it works well on the whole space .", "The idea is closely related to active learning in standard classification problems , where we selectively pick the most important samples often based on estimation uncertainty for labeling in order to maximize classification performance Lewis and Catlett , 1994 .", "In the active learning setting , as long as we do well on the actively selected samples , we do well on the whole sample space .", "In our case , as long as we do well on the relevant set , the decoder will perform well .", "Since the relevant set depends on the decoder parameter , and the decoder parameter is optimized on the relevant set , it is necessary to estimate them jointly using an iterative algorithm .", "The basic idea is to start with a decoding parameter , and estimate the corresponding relevant set ; we then update based on the relevant set , and iterate this process .", "The procedure is outlined in Table 1 .", "We intentionally leave the implementation details of the step and step open .", "Moreover , in this general algorithm , we do not have to assume that has the form of Eq .", "A natural question concerning the procedure is its convergence behavior .", "It can be shown that under mild assumptions , if we pick in an alternative for each such that then the procedure converges to the solution of Eq .", "Moreover , the rate of convergence depends only on the property of the loss function , and not on the size of .", "This property is critical as it shows that as long as Eq .", "6 can be computed efficiently , then the Approximate Relevant Set algorithm is efficient .", "Moreover , it gives a bound on the size of an approximate relevant set with a certain accuracy . 5 The approximate solution of Eq .", "5 in can be implemented using stochastic gradient descent SGD , where we may simply update as The parameter is a fixed constant often referred to as learning rate .", "Again , convergence results can be proved for this procedure .", "Due to the space limitation , we skip the formal statement as well as the corresponding analysis .", "Up to this point , we have not assumed any specific form of the decoder scoring function in our algorithm .", "Now consider Eq .", "1 used in our model .", "We may express it as where .", "Using this feature representation and the loss function in Eq .", "4 , we obtain the following costMargin SGD update rule for each training data point and", "We applied the novel discriminative training approach to a standard Arabic to English translation task .", "The training data comes from UN news sources .", "Some punctuation tokenization and some number classing are carried out on the English and the Arabic training data .", "We show translation results in terms of the automatic BLEU evaluation metric Papineni et al . , 2002 on the MT03 Arabic English DARPA evaluation test set consisting of sentences with Arabic words with reference translations .", "In order to speed up the parameter training the original training data is filtered according to the test set all the Arabic substrings that occur in the test set are computed and the parallel training data is filtered to include only those training sentence pairs that contain at least one out of these phrases the resulting pre filtered training data contains about thousand sentence pairs million Arabic words and million English words .", "The block set is generated using a phrase pair selection algorithm similar to Koehn et al . , 2003 ; Al Onaizan et al . , 2004 , which includes some heuristic filtering to mal statement here .", "A detailed theoretical investigation of the method will be given in a journal paper . for each data point initialize truth and alternative for each decoding iteration for each data point increase phrase translation accuracy .", "Blocks that occur only once in the training data are included as well .", "The training algorithm in Table 2 is adapted from Table 1 .", "The training is carried out by running times over the parallel training data , each time decoding all the training sentences and generating a single block translation sequence for each training sentence .", "The top five block sequences with the highest BLEU score are computed up front for all training sentence pairs and are stored separately as described in Section 2 .", "The score based decoding of the training sentence pairs is carried out in parallel on Bit Opteron machines .", "Here , the monotone decoding is much faster than the decoding with block swapping the monotone decoding takes less than hours and the decoding with swapping takes about an hour .", "Since the training starts with only the parallel training data and a block set , some initial block sequences have to be generated in order to initialize the global model training for each input sentence a simple bag of blocks translation is generated .", "For each input interval that is matched by some block , a single block is added to the bag of blocks translation .", "The order in which the blocks are generated is ignored .", "For this block set only block and word identity features are generated , i . e . features of type and in Section 2 .", "This step does not require the use of a decoder .", "The initial block sequence training data contains only a single alternative .", "The training procedure proceeds by iteratively decoding the training data .", "After each decoding step , the resulting translation block sequences are stored on disc in binary format .", "A block sequence generated at decoding step is used in all subsequent training steps , where .", "The block sequence training data after the th decoding step is given as , where the size of the relevant alternative set is .", "Although in order to achieve fast convergence with a theoretical guarantee , we should use Eq .", "6 to update the relevant set , in reality , this idea is difficult to implement because it requires a more costly decoding step .", "Therefore in Table 2 , we adopt an approximation , where the relevant set is updated by adding the decoder output at each stage .", "In this way , we are able to treat the decoding scheme as a black box .", "One way to approximate Eq .", "6 is to generate multiple decoding outputs and pick the most relevant points based on Eq .", "Since the best list generation is computationally costly , only a single block sequence is generated for each training sentence pair , reducing the memory requirements for the training algorithm as well .", "Although we are not able to rigorously prove fast convergence rate for this approximation , it works well in practice , as Figure 2 shows .", "Theoretically this is because points achieving large values in Eq .", "6 tend to have higher chances to become the top ranked decoder output as well .", "The SGDbased on line training algorithm described in Section 3 , is carried out after each decoding step to generate the weight vector for the subsequent decoding step .", "Since this training step is carried out on a single machine , it dominates the overall computation time .", "Since each iteration adds a single relevant alternative to the set , computation time increases with the number of training iterations the initial model is trained in a few minutes , while training the model after the th iteration takes up to hours for the most complex models .", "Table 3 presents experimental results in terms of uncased BLEU 6 .", "Two re ordering restrictions are tested , i . e . monotone decoding MON , and local block re ordering where neighbor blocks can be swapped SWAP .", "The SWAP re ordering uses the same features as the monotone models plus additional orientation based and distortionBLEU on the training data sentences and the MT03 test data 670 sentences . based features .", "Different feature sets include word based features , phrase based features , and the combination of both .", "For the results with word based features , the decoder still generates phrase to phrase translations , but all the scoring is done on the word level .", "Line shows a BLEU score of for the best performing system which uses all word based and phrase based features 7 .", "Line and line of Table 3 show the training data averaged BLEU score obtained by searching for the highest BLEU scoring block sequence for each training sentence pair as described in Section 2 .", "Allowing local block swapping in this search procedure yields a much improved BLEU score of .", "The experimental results show that word based models significantly outperform phrase based models , the combination of wordbased and phrase based features performs better than those features types taken separately .", "Additionally , swap based re ordering slightly improves performance over monotone decoding .", "For all experiments , the training BLEU score remains significantly lower than the maximum obtainable BLEU score shown in line and line .", "In this respect , there is significant room for improvements in terms of feature functions and alternative set generation .", "The word based models perform surprisingly well , i . e . the model in line uses only three feature types model features like in Section 2 , distortion features , and target language m gram features up to .", "Training speed varies depending on the feature types used for the simplest model shown in line of Table 3 , the training takes about hours , for the models usence and the test set lower graph ; BLEU with four references as a function of the training iteration for the model corresponding to line in Table 3 . ing word based features shown in line and line training takes less than days .", "Finally , the training for the most complex model in line takes about days .", "Figure 2 shows the BLEU performance for the model corresponding to line in Table 3 as a function of the number of training iterations .", "By adding top scoring alternatives in the training algorithm in Table 2 , the BLEU performance on the training data improves from about for the initial model to about for the best model after iterations .", "After each training iteration the test data is decoded as well .", "Here , the BLEU performance improves from for the initial model to about for the final model we do not include the test data block sequences in the training .", "Table 3 shows a typical learning curve for the experiments in Table 3 the training BLEU score is much higher than the test set BLEU score despite the fact that the test set uses reference translations .", "The work in this paper substantially differs from previous work in SMT based on the noisy channel approach presented in Brown et al . , 1993 .", "While error driven training techniques are commonly used to improve the performance of phrasebased translation systems Chiang , 2005 ; Och , 2003 , this paper presents a novel block sequence translation approach to SMT that is similar to sequential natural language annotation problems such as part of speech tagging or shallow parsing , both in modeling and parameter training .", "Unlike earlier approaches to SMT training , which either rely heavily on domain knowledge , or can only handle a small number of features , this approach treats the decoding process as a black box , and can optimize tens millions of parameters automatically , which makes it applicable to other problems as well .", "The choice of our formulation is convex , which ensures that we are able to find the global optimum even for large scale problems .", "The loss function in Eq .", "4 may not be optimal , and using different choices may lead to future improvements .", "Another important direction for performance improvement is to design methods that better approximate Eq .", "Although at this stage the system performance is not yet better than previous approaches , good translation results are achieved on a standard translation task .", "While being similar to Tillmann and Zhang , 2005 , the current procedure is more automated with comparable performance .", "The latter approach requires a decomposition of the decoding scheme into local decision steps with the inherent difficulty acknowledged in Tillmann and Zhang , 2005 .", "Since such limitation is not present in the current model , improved results may be obtained in the future .", "A perceptronlike algorithm that handles global features in the context of re ranking is also presented in Shen et al . , 2004 .", "The computational requirements for the training algorithm in Table 2 can be significantly reduced .", "While the global training approach presented in this paper is simple , after iterations or so , the alternatives that are being added to the relevant set differ very little from each other , slowing down the training considerably such that the set of possible block translations might not be fully explored .", "As mentioned in Section 2 , the current approach is still able to handle real valued features , e . g . the language model probability .", "This is important since the language model can be trained on a much larger monolingual corpus .", "This work was partially supported by the GALE project under the DARPA contract No .", "HR001106 2 0001 .", "The authors would like to thank the anonymous reviewers for their detailed criticism on this paper ."], "summary_lines": ["A Discriminative Global Training Algorithm For Statistical MT\n", "This paper presents a novel training algorithm for a linearly-scored block sequence translation model.\n", "The key component is a new procedure to directly optimize the global scoring function used by a SMT decoder.\n", "No translation, language, or distortion model probabilities are used as in earlier work on SMT.\n", "Therefore our method, which employs less domain specific knowledge, is both simpler and more extensible than previous approaches.\n", "Moreover, the training procedure treats the decoder as a black-box, and thus can be used to optimize any decoding scheme.\n", "The training algorithm is evaluated on a standard Arabic-English translation task.\n", "We use a BLEU oracle decoder for discriminative training of a local reordering model.\n", "We use a perceptron style algorithm for training a large number of features.\n", "We compute high BLEU hypotheses by running a conventional decoder so as to maximize a per-sentence approximation of BLEU-4, under a simple, local reordering models.\n", "We present a procedure to directly optimize the global scoring function used by a phrase based decoder on the accuracy of the translations.\n"]}
{"article_lines": ["Nymble A High Performance Learning Name Finder", "This paper presents a statistical , learned approach to finding names and other nonrecursive entities in text as per the MUC 6 definition of the NE task , using a variant of the standard hidden Markov model .", "We present our justification for the problem and our approach , a detailed discussion of the model itself and finally the successful results of this new approach .", "In the past decade , the speech recognition community has had huge successes in applying hidden Markov models , or HMM's to their problems .", "More recently , the natural language processing community has effectively employed these models for part ofspeech tagging , as in the seminal Church , 1988 and other , more recent efforts Weischedel et al . , 1993 .", "We would now propose that HMM's have successfully been applied to the problem of name finding .", "We have built a named entity NE recognition system using a slightly modified version of an HMM ; we call our system quot ; Nymble quot ; .", "To our knowledge , Nymble out performs the best published results of any other learning name finder .", "Furthermore , it performs at or above the 90 accuracy level , often considered quot ; near human performance quot ; .", "The system arose from the NE task as specified in the last Message Understanding Conference MUC , where organization names , person names , location names , times , dates , percentages and money amounts were to be delimited in text using SGML markup .", "We will describe the various models employed , the methods for training these models and the method for quot ; decoding quot ; on test data the term quot ; decoding quot ; borrowed from the speech recognition community , since one goal of traversing an HMM is to recover the hidden state sequence .", "To date , we have successfully trained and used the model on both English and Spanish , the latter for MET , the multi lingual entity task .", "The basic premise of the approach is to consider the raw text encountered when decoding as though it had passed through a noisy channel , where it had been originally marked with named entities . '", "The job of the generative model is to model the original process which generated the name class annotated words , before they went through the noisy channel .", "More formally , we must find the most likely sequence of name classes NC given a sequence of words W Pr NC I W 2 . 1 In order to treat this as a generative model where it generates the original , name class annotated words , and since the a priori probability of the word sequence the denominator is constant for any given sentence , we can maxi mize Equation 2 . 2 by maximizing the numerator alone .", "I See Cover and Thomas , 1991 , ch .", "2 , for an excellent overview of the principles of information theory .", "2 . 2 Previous approaches have typically used manually constructed finite state patterns Weischodel , 1995 , Appelt et al . , 1995 .", "For every new language and every new class of new information to spot , one has to write a new set of rules to cover the new language and to cover the new class of information .", "A finite state pattern rule attempts to match against a sequence of tokens words , in much the same way as a general regular expression matcher .", "In addition to these finitestate pattern approaches , a variant of Brill rules has been applied to the problem , as outlined in Aberdeen et al . , 1995 .", "The atomic elements of information extraction indeed , of language as a whole could be considered the who , where , when and how much in a sentence .", "A name finder performs what is known as surface or lightweight parsing , delimiting sequences of tokens that answer these important questions .", "It can be used as the first step in a chain of processors a next level of processing could relate two or more named entities , or perhaps even give semantics to that relationship using a verb .", "In this way , further processing could discover the quot ; what quot ; and quot ; how quot ; of a sentence or body of text .", "Furthermore , name finding can be useful in its own right an Internet query system might use namefinding to construct more appropriately formed queries quot ; When was Bill Gates born ? quot ; could yield the query quot ; Bill Gates quot ; born .", "Also , name finding can be directly employed for link analysis and other information retrieval problems .", "We will present the model twice , first in a conceptual and informal overview , then in a moredetailed , formal description of it as a type of HMM .", "The model bears resemblance to Scott Miller's novel work in the Air Traffic Information System ATIS task , as documented in Miller et al . , 1994 .", "Figure 3 . 1 is a pictorial overview of our model .", "Informally , we have an ergodic HMM with only eight internal states the name classes , including the NOT A NAME class , with two special states , the START and END OF SENTENCE states .", "Within each of the name class states , we use a statistical bigram language model , with the usual one word per state emission .", "This means that the number of states in each of the name class states is equal to the vocabulary size , I VI .", "The generation of words and name classes proceeds in three steps These three steps are repeated until the entire observed word sequence is generated .", "Using the Viterbi algorithm , we efficiently search the entire space of all possible name class assignments , maximizing the numerator of Equation 2 . 2 , Pr W , NC .", "Informally , the construction of the model in this manner indicates that we view each type of quot ; name quot ; to be its own language , with separate bigram probabilities for generating its words .", "While the number of word states within each name class is equal to I VI , this quot ; interior quot ; bigram language model is ergodic , i . e . , there is a probability associated with every one of the 1V12transitions .", "As a parameterized , trained model , if such a transition were never observed , the model quot ; backs off' to a less powerful model , as described below , in 3 . 3 . 3 on p . 4 .", "Throughout most of the model , we consider words to be ordered pairs or two element vectors , composed of word and word feature , denoted w , f .", "The word feature is a simple , deterministic computation performed on each word as it is added to or feature computation is an extremely small part of the implementation , at roughly ten lines of code .", "Also , most of the word features are used to distinguish types of numbers , which are language independent . 2 The rationale for having such features is clear in Roman languages , capitalization gives good evidence of names . 3 This section describes the model formally , discussing the transition probabilities to the wordstates , which quot ; generate quot ; the words of each name class .", "As with most trained , probabilistic models , we looked up in the vocabulary .", "It produces one of the fourteen values in Table 3 . 1 .", "These values are computed in the order listed , so that in the case of non disjoint feature classes , such as containsDigitAndAlpha and containsDigitAndDash , the former will take precedence .", "The first eight features arise from the need to distinguish and annotate monetary amounts , percentages , times and dates .", "The rest of the features distinguish types of capitalization and all other words such as punctuation marks , which are separate tokens .", "In particular , the f irstWord feature arises from the fact that if a word is capitalized and is the first word of the sentence , we have no good information as to why it is capitalized but note that allCaps and capPeriod are computed before f irstWord , and therefore take precedence .", "The word feature is the one part of this model which is language dependent .", "Fortunately , the word have a most accurate , most powerful model , which will quot ; back off' to a less powerful model when there is insufficient training , and ultimately back off to unigram probabilities .", "In order to generate the first word , we must make a transition from one name class to another , as well as calculate the likelihood of that word .", "Our intuition was that a word preceding the start of a name class such as quot ; Mr . quot ; , quot ; President quot ; or other titles preceding the PERSON name class and the word following a name class would be strong indicators of the subsequent and preceding name classes , respectively .", "2 Non english languages tend to use the comma and period in the reverse way in which English does , i . e . , the comma is a decimal point and the period separates groups of three digits in large numbers .", "However , the re ordering of the precedence of the two relevant word features had little effect when decoding Spanish , so they were left as is .", "3 Although Spanish has many lower case words in organization names .", "See 4 . 1 on p . 6 for more details .", "Accordingly , the probabilitiy for generating the first word of a name class is factored into two parts Pr NC I NC_ , , w_1 Pr w , f firs , I NC , NC_ , .", "3 . 1 The top level model for generating all but the first word in a name class is Pr w , NC .", "3 . 2 There is also a magical quot ; end quot ; word , so that the probability may be computed for any current word to be the final word of its name class , i . e . , Pr end , o the r I w , f find' NC .", "3 . 3 As one might imagine , it would be useless to have the first factor in Equation 3 . 1 be conditioned off of the end word , so the probability is conditioned on the previous real word of the previous name class , i . e . , we compute W 1 last observed word otherwise NC , START OF SENTENCE 3 . 4 Note that the above probability is not conditioned on the word feature of w_1 , the intuition of which is that in the cases where the previous word would help the model predict the next name class , the world feature capitalization in particular is not important quot ; Mr . quot ; is a good indicator of the next word beginning the PERSON name class , regardless of capitalization , especially since it is almost never seen as quot ; mr . quot ; .", "The calculation of the above probabilities is straightforward , using events sample size where c represents the number of times the events occurred in the training data the count .", "Ideally , we would have sufficient training or at least one observation of ! every event whose conditional probability we wish to calculate .", "Also , ideally , we would have sufficient samples of that upon which each conditional probability is conditioned , e . g . , for Pr NC I NC 1 , w_ , , we would like to have seen sufficient numbers of NC_ , , w1 .", "Unfortunately , there is rarely enough training data to compute accurate probabilities when quot ; decoding quot ; on new data .", "3 . 1 Unknown Words The vocabulary of the system is built as it trains .", "Necessarily , then , the system knows about all words for which it stores bigram counts in order to compute the probabilities in Equations 3 . 1 3 . 3 .", "The question arises how the system should deal with unknown words , since there are three ways in which they can appear in a bigram as the current word , as the previous word or as both .", "A good answer is to train a separate , unknown word model off of held out data , to gather statistics of unknown words occurring in the midst of known words .", "Typically , one holds out 10 20 of one's training for smoothing or unknown word training .", "In order to overcome the limitations of a small amount of training data particularly in Spanish we hold out 50 of our data to train the unknown word model the vocabulary is built up on the first 50 , save these counts in training data file , then hold out the other 50 and concatentate these bigram counts with the first unknown word training file .", "This way , we can gather likelihoods of an unknown word appearing in the bigram using all available training data .", "This approach is perfectly valid , as we art trying to estimate that which we have not legitimately seen in training .", "When decoding , if either word of the bigram is unknown , the model used to estimate the probabilities of Equations 3 . 1 3 is the unknown word model , otherwise it is the model from the normal training .", "The unknown word model can be viewed as a first level of back off , therefore , since it is used as a backup model when an unknown word is encountered , and is necessarily not as accurate as the bigram model formed from the actual training .", "3 . 2 Further Back off Models and Smoothing Whether a bigram contains an unknown word or not , it is possible that either model may not have seen this bigram , in which case the model backs off to a less powerful , less descriptive model .", "Table 3 . 2 shows a graphic illustration of the back off scheme The weight for each back off model is computed onthe fly , using the following formula If computing Pr XIY , assign weight of A to the direct computation using one of the formulae of 3 . 3 . 2 and a weight of 1 A to the back off model , where 3 . 8 where quot ; old c Y quot ; is the sample size of the model from which we are backing off .", "This is a rather simple method of smoothing , which tends to work well when there are only three or four levels of back off . 4 This method also overcomes the problem when a back off model has roughly the same amount of training as the current model , via the first factor of Equation 3 . 8 , which essentially ignores the back off model and puts all the weight on the primary model , in such an equi trained situation .", "As an example disregarding the first factor if we saw the bigram quot ; come hither quot ; once in training and we saw quot ; come here quot ; three times , and nowhere else did we see the word quot ; come quot ; in the NOT A NAME class , when computing Pr quot ; hither quot ; I quot ; come quot ; , NOT A NAME , we would back off to the unigram probability Pr quot ; hither quot ; I NOT A NAME with a weight of , since the number of unique outcomes for the word state for quot ; come quot ; would be two , and the total number of times quot ; come quot ; had been the preceding word in a bigram would be four a 4 Any more levels of back off might require a more sophisticated smoothing technique , such as deleted interpolation .", "No matter what smoothing technique is used , one must remember that smoothing is the art of estimating the probability of that which is unknown i . e . , not seen in training .", "Unlike a traditional HMM , the probability of generating a particular word is 1 for each word state inside each of the name class states .", "An alternative and more traditional model would have a small number of states within each name class , each having , perhaps , some semantic signficance , e . g . , three states in the PERSON name class , representing a first , middle and last name , where each of these three states would have some probability associated with emitting any word from the vocabulary .", "We chose to use a bigram language model because , while less semantically appealing , such n gram language models work remarkably well in practice .", "Also , as a first research attempt , an n gram model captures the most general significance of the words in each name class , without presupposing any specifics of the structure of names , a la the PERSON name class example , above .", "More important , either approach is mathematically valid , as long as all transitions out of a given state sum to one .", "All of this modeling would be for naught were it not for the existence of an efficient algorithm for finding the optimal state sequence , thereby quot ; decoding quot ; the original sequence of name classes .", "The number of possible state sequences for N states in an ergodic model for a sentence of m words is Alm , but , using dynamic programming and an appropriate merging of multiple theories when they converge on a particular state the Viterbi decoding algorithm a sentence can be quot ; decoded quot ; in time linear to the number of tokens in the sentence , 0 m Viterbi , 1967 .", "Since we are interested in recovering the name class state sequence , we pursue eight theories at every given step of the algorithm .", "Initially , the word feature was not in the model ; instead the system relied on a third level back off partof speech tag , which in turn was computed by our stochastic part of speech tagger .", "The tags were taken at face value there were not k best tags ; the system treated the part of speech tagger as a quot ; black box quot ; .", "Although the part of speech tagger used capitalization to help it determine proper noun tags , this feature was only implicit in the model , and then only after two levels of back off !", "Also , the capitalization of a word was submerged in the muddiness of part of speech tags , which can quot ; smear quot ; the capitalization probability mass over several tags .", "Because it seemed that capitalization would be a good name predicting feature , and that it should appear earlier in the model , we eliminated the reliance on part of speech altogether , and opted for the more direct , word feature model described above , in 3 .", "Originally , we had a very small number of features , indicating whether the word was a number , the first word of a sentence , all uppercase , inital capitalized or lower case .", "We then expanded the feature set to its current state in order to capture more subtleties related mostly to numbers ; due to increased performance although not entirely dramatic on every test , we kept the enlarged feature set .", "Contrary to our expectations which were based on our experience with English , Spanish contained many examples of lower case words in organization and location names .", "For example , departamento quot ; Department quot ; could often start an organization name , and adjectival place names , such as coreana quot ; Korean quot ; could appear in locations and by convention are not capitalized .", "The entire system is implemented in C , atop a quot ; home brewed quot ; , general purpose class library , providing a rapid code compile train test cycle .", "In fact , many NLP systems suffer from a lack of software and computer science engineering effort runtime efficiency is key to performing numerous experiments , which , in turn , is key to improving performance .", "A system may have excellent performance on a given task , but if it takes long to compile and or run on test data , the rate of improvement of that system will be miniscule compared to that which can run very efficiently .", "On a Sparc20 or SGI Indy with an appropritae amount of RAM , Nymble can compile in 10 minutes , train in 5 minutes and run at 6MB hr .", "There were days in which we had as much as a 15 reduction in error rate , to borrow the performance measure used by the speech community , where error rate 100 Fmeasure .", "See 4 . 3 for the definition of F measure .", "In this section we report the results of evaluating the final version of the learning software .", "We report the results for English and for Spanish and then the results of a set of experiments to determine the impact of the training set size on the algorithm's performance in both English and Spanish .", "For each language , we have a held out development test set and a held out , blind test set .", "We only report results on the blind test set for each respective language .", "The scoring program measures both precision I recall , terms borrowed from the information retrieval community , where number of correct responses and number responses number of correct responses number correct in key Put informally , recall measures the number of quot ; hits quot ; vs . the number of possible correct answers as specified in the key file , whereas precision measures how many answers were correct ones compared to the number of answers delivered .", "These two measures of performance combine to form one measure of performance , the F measure , which is computed by the weighted harmonic mean of precision and recall 32 1 RP where if represents the relative weight of recall to precision and typically has the value 1 .", "To our knowledge , our learned name finding system has achieved a higher F measure than any other learned system when compared to state of the art manual rule based systems on similar data .", "Our test set of English data for reporting results is that of the MUC 6 test set , a collection of 30 WSJ documents we used a different test set during development .", "Our Spanish test set is that used for MET , comprised of articles from the news agency AFP .", "Table 4 . 1 illustrates Nymble's performance as compared to the best reported scores for each category .", "With any learning technique one of the important questions is how much training data is required to get acceptable performance .", "More generally how does performance vary as the training set size is increased or decreased ?", "We ran a sequence of experiments in English and in Spanish to try to answer this question for the final model that was implemented .", "For English , there were 450 , 000 words of training data .", "By that we mean that the text of the document itself including headlines but not including SGML tags was 450 , 000 words long .", "Given this maximum size of training available to us , we successfully divided the training material in half until we were using only one eighth of the original training set size or a training set of 50 , 000 words for the smallest experiment .", "To give a sense of the size of 450 , 000 words , that is roughly half the length of one edition of the Wall Street Journal .", "The results are shown in a histogram in Figure 4 . 1 below .", "The positive outcome of the experiment is that half as much training data would have given almost equivalent performance .", "Had we used only one quarter of the data or approximately 100 , 000 words , performance would have degraded slightly , only about 1 2 percent .", "Reducing the training set size to 50 , 000 words would have had a more significant decrease in the performance of the system ; however , the performance is still impressive even with such a small training set .", "On the other hand , the result also shows that merely annotating more data will not yield dramatic improvement in the performance .", "With increased training data it would be possible to use even more detailed models that require more data and could achieve significantly improved overall system performance with those more detailed models .", "For Spanish we had only 223 , 000 words of training data .", "We also measured the performance of the system with half the training data or slightly more than 100 , 000 words of text .", "Figure 4 . 2 shows the results .", "There is almost no change in performance by using as little as 100 , 000 words of training data .", "Therefore the results in both languages were comparable .", "As little as 100 , 000 words of training data produces performance nearly comparable to handcrafted systems .", "While our initial results have been quite favorable , there is still much that can be done potentially to improve performance and completely close the gap between learned and rule based name finding systems .", "We would like to incorporate the following into the current model", "We have shown that using a fairly simple probabilistic model , finding names and other numerical entities as specified by the MUC tasks can be performed with quot ; near human performance quot ; , often likened to an F of 90 or above .", "We have also shown that such a system can be trained efficiently and that , given appropriately and consistently marked answer keys , it can be trained on languages foreign to the trainer of the system ; for example , we do not speak Spanish , but trained Nymble on answer keys marked by native speakers .", "None of the formalisms or techniques presented in this paper is new ; rather , the approach to this task the model itself is wherein lies the novelty .", "Given the incredibly difficult nature of many NLP tasks , this example of a learned , stochastic approach to name finding lends credence to the argument that the NLP community ought to push these approaches , to find the limit of phenomena that may be captured by probabilistic , finite state methods ."], "summary_lines": ["Nymble: A High-Performance Learning Name-Finder\n", "This paper presents a statistical, learned approach to finding names and other non-recursive entities in text (as per the MUC-6 definition of the NE task), using a variant of the standard hidden Markov model.\n", "We present our justification for the problem and our approach, a detailed discussion of the model itself and finally the successful results of this new approach.\n", "We develop Nymble, an HMM-based name tagging system operating in English and Spanish.\n", "Nymble uses statistical learning to acquire a Hidden Markov Model (HMM) that recognises NEs in text.\n"]}
{"article_lines": ["Corpus Variation And Parser Performance", "Most work in statistical parsing has focused on a single corpus the Wall Street Journal portion of the Penn Treebank .", "While this has allowed for quantitative comparison of parsing techniques , it has left open the question of how other types of text might affect parser performance , and how portable parsing models are across corpora .", "We examine these questions by comparing results for the Brown and WSJ corpora , and also consider which parts of the parser's probability model are particularly tuned to the corpus on which it was trained .", "This leads us to a technique for pruning parameters to reduce the size of the parsing model .", "The past several years have seen great progress in the field of natural language parsing , through the use of statistical methods trained using large corpora of hand parsed training data .", "The techniques of Charniak 1997 , Collins 1997 , and Ratnaparkhi 1997 achieved roughly comparable results using the same sets of training and test data .", "In each case , the corpus used was the Penn Treebank's hand annotated parses of Wall Street Journal articles .", "Relatively few quantitative parsing results have been reported on other corpora though see Stolcke et al . 1996 for results on Switchboard , as well as Collins et al .", "1999 for results on Czech and Hwa 1999 for bootstrapping from WSJ to ATIS .", "The inclusion of parses for the Brown corpus in the Penn Treebank allows us to compare parser performance across corpora .", "In this paper we examine the following questions Our investigation of these questions leads us to a surprising result about parsing the WSJ corpus over a third of the model's parameters can be eliminated with little impact on performance .", "Aside from cross corpus considerations , this is an important finding if a lightweight parser is desired or memory usage is a consideration .", "A great deal of work has been done outside of the parsing community analyzing the variations between corpora and different genres of text .", "Biber 1993 investigated variation in a number syntactic features over genres , or registers , of language .", "Of particular importance to statistical parsers is the investigation of frequencies for verb subcategorizations such as Roland and Jurafsky 1998 .", "Roland et al . 2000 find that subcategorization frequencies for certain verbs vary significantly between the Wall Street Journal corpus and the mixed genre Brown corpus , but that they vary less so between genre balanced British and American corpora .", "Argument structure is essentially the task that automatic parsers attempt to solve , and the frequencies of various structures in training data are reflected in a statistical parser's probability model .", "The variation in verb argument structure found by previous research caused us to wonder to what extent a model trained on one corpus would be useful in parsing another .", "The probability models of modern parsers include not only the number and syntactic type of a word's arguments , but lexical information about their fillers .", "Although we are not aware of previous comparisons of the frequencies of argument fillers , we can only assume that they vary at least as much as the syntactic subcategorization frames .", "We take as our baseline parser the statistical model of Model 1 of Collins 1997 .", "The model is a historybased , generative model , in which the probability for a parse tree is found by expanding each node in the tree in turn into its child nodes , and multiplying the probabilities for each action in the derivation .", "It can be thought of as a variety of lexicalized probabilistic context free grammar , with the rule probabilities factored into three distributions .", "The first distribution gives probability of the syntactic category H of the head child of a parent node with category P , head word Hhw with the head tag the part of speech tag of the head word Hht The head word and head tag of the new node H are defined to be the same as those of its parent .", "The remaining two distributions generate the non head children one after the other .", "A special STOP symbol is generated to terminate the sequence of children for a given parent .", "Each child is generated in two steps first its syntactic category C and head tag Cht are chosen given the parent's and head child's features and a function A representing the distance from the head child Then the new child's head word Chw is chosen For each of the three distributions , the empirical distribution of the training data is interpolated with less specific backoff distributions , as we will see in Section 5 .", "Further details of the model , including the distance features used and special handling of punctuation , conjunctions , and base noun phrases , are described in Collins 1999 .", "The fundamental features of used in the probability distributions are the lexical heads and head tags of each constituent , the co occurrences of parent nodes and their head children , and the cooccurrences of child nodes with their head siblings and parents .", "The probability models of Charniak 1997 , Magerman 1995 and Ratnaparkhi 1997 differ in their details but are based on similar features .", "Models 2 and 3 of Collins 1997 add some slightly more elaborate features to the probability model , as do the additions of Charniak 2000 to the model of Charniak 1997 .", "Our implementation of Collins' Model 1 performs at 86 precision and recall of labeled parse constituents on the standard Wall Street Journal training and test sets .", "While this does not reflect the state of the art performance on the WSJ task achieved by the more the complex models of Charniak 2000 and Collins 2000 , we regard it as a reasonable baseline for the investigation of corpus effects on statistical parsing .", "We conducted separate experiments using WSJ data , Brown data , and a combination of the two as training material .", "For the WSJ data , we observed the standard division into training sections 2 through 21 of the treebank and test section 23 sets .", "For the Brown data , we reserved every tenth sentence in the corpus as test data , using the other nine for training .", "This may underestimate the difficulty of the Brown corpus by including sentences from the same documents in training and test sets .", "However , because of the variation within the Brown corpus , we felt that a single contiguous test section might not be representative .", "Only the subset of the Brown corpus available in the Treebank II bracketing format was used .", "This subset consists primarily of various fiction genres .", "Corpus sizes are shown in Results for the Brown corpus , along with WSJ results for comparison , are shown in Table 2 .", "The basic mismatch between the two corpora is shown in the significantly lower performance of the WSJtrained model on Brown data than on WSJ data rows 1 and 2 .", "A model trained on Brown data only does significantly better , despite the smaller size of the training set .", "Combining the WSJ and Brown training data in one model improves performance further , but by less than 0 . 5 absolute .", "Similarly , adding the Brown data to the WSJ model increased performance on WSJ by less than 0 . 5 .", "Thus , even a large amount of additional data seems to have relatively little impact if it is not matched to the test material .", "The more varied nature of the Brown corpus also seems to impact results , as all the results on Brown are lower than the WSJ result .", "The parsers cited above all use some variety of lexical dependency feature to capture statistics on the cooccurrence of pairs of words being found in parentchild relations within the parse tree .", "These word pair relations , also called lexical bigrams Collins , 1996 , are reminiscent of dependency grammars such as Melcuk 1988 and the link grammar of Sleator and Temperley 1993 .", "In Collins' Model 1 , the word pair statistics occur in the distribution where Hhw represent the head word of a parent node in the tree and Chw the head word of its non head child .", "The head word of a parent is the same as the head word of its head child .", "Because this is the only part of the model that involves pairs of words , it is also where the bulk of the parameters are found .", "The large number of possible pairs of words in the vocabulary make the training data necessarily sparse .", "In order to avoid assigning zero probability to unseen events , it is necessary to smooth the training data .", "The Collins model uses linear interpolation to estimate probabilities from empirical distributions of varying specificities where P represents the empirical distribution derived directly from the counts in the training data .", "The interpolation weights A1 , A2 are chosen as a function of the number of examples seen for the conditioning events and the number of unique values seen for the predicted variable .", "Only the first distribution in this interpolation scheme involves pairs of words , and the third component is simply the probability of a word given its part of speech .", "Because the word pair feature is the most specific in the model , it is likely to be the most corpusspecific .", "The vocabularies used in corpora vary , as do the word frequencies .", "It is reasonable to expect word co occurrences to vary as well .", "In order to test this hypothesis , we removed the distribution P ChwlP , H , Hht , Hhw , C , Cht from the parsing model entirely , relying on the interpolation of the two less specific distributions in the parser We performed cross corpus experiments as before to determine whether the simpler parsing model might be more robust to corpus effects .", "Results are shown in Table 3 .", "Perhaps the most striking result is just how little the elimination of lexical bigrams affects the baseline system performance on the WSJ corpus decreases by less than 0 . 5 absolute .", "Moreover , the performance of a WSJ trained system without lexical bigrams on Brown test data is identical to the WSJtrained system with lexical bigrams .", "Lexical cooccurrence statistics seem to be of no benefit when attempting to generalize to a new corpus .", "The relatively high performance of a parsing model with no lexical bigram statistics on the WSJ task led us to explore whether it might be possible to significantly reduce the size of the parsing model by selectively removing parameters without sacrificing performance .", "Such a technique reduces the parser's memory requirements as well as the overhead of loading and storing the model , which could be desirable for an application where limited computing resources are available .", "Significant effort has gone into developing techniques for pruning statistical language models for speech recognition , and we borrow from this work , using the weighted difference technique of Seymore and Rosenfeld 1996 .", "This technique applies to any statistical model which estimates probabilities by backing off , that is , using probabilities from a less specific distribution when no data are available are available for the full distribution , as the following equations show for the general case Here e is the event to be predicted , h is the set of conditioning events or history , a is a backoff weight , and h' is the subset of conditioning events used for the less specific backoff distribution .", "BO is the backoff set of events for which no data are present in the specific distribution P1 .", "In the case of n gram language modeling , e is the next word to be predicted , and the conditioning events are the n 1 preceding words .", "In our case the specific distribution P1 of the backoff model is Pcw of equation 1 , itself a linear interpolation of three empirical distributions from the training data .", "The less specific distribution P2 of the backoff model is Pcw2 of equation 2 , an interpolation of two empirical distributions .", "The backoff weight a is simply 1 A1 in our linear interpolation model .", "The Seymore Rosenfeld pruning technique can be used to prune backoff probability models regardless of whether the backoff weights are derived from linear interpolation weights or discounting techniques such as Good Turing .", "In order to ensure that the model's probabilities still sum to one , the backoff weight a must be adjusted whenever a parameter is removed from the model .", "In the Seymore Rosenfeld approach , parameters are pruned according to the following criterion where p' elh' represents the new backed off probability estimate after removing p eIh from the model and adjusting the backoff weight , and N e , h is the count in the training data .", "This criterion aims to prune probabilities that are similar to their backoff estimates , and that are not frequently used .", "As shown by Stolcke 1998 , this criterion is an approximation of the relative entropy between the original and pruned distributions , but does not take into account the effect of changing the backoff weight on other events' probabilities .", "Adjusting the threshold 0 below which parameters are pruned allows us to successively remove more and more parameters .", "Results for different values of 0 are shown in Table 4 .", "The complete parsing model derived from the WSJ training set has 735 , 850 parameters in a total of nine distributions three levels of backoff for each of the three distributions Ph , P , and P , , , , .", "The lexical bigrams are contained in the most specific distribution for P , , , , .", "Removing all these parameters reduces the total model size by 43 .", "The results show a gradual degradation as more parameters are pruned .", "The ten lexical bigrams with the highest scores for the pruning metric are shown in Table 5 for WSJ and Table 6 .", "The pruning metric of equation 3 has been normalized by corpus size to allow comparison between WSJ and Brown .", "The only overlap between the two sets is for pairs of unknown word tokens .", "The WSJ bigrams are almost all specific to finance , are all word pairs that are likely to appear immediately adjacent to one another , and are all children of the base NP syntactic category .", "The Brown bigrams , which have lower correlation values by our metric , include verb subject and preposition object relations and seem more broadly applicable as a model of English .", "However , the pairs are not strongly related semantically , no doubt because the first term of the pruning criterion favors the most frequent words , such as forms of the verbs quot ; be quot ; and quot ; have quot ; .", "Our results show strong corpus effects for statistical parsing models a small amount of matched training data appears to be more useful than a large amount of unmatched data .", "The standard WSJ task seems to be simplified by its homogenous style .", "Adding training data from from an unmatched corpus doesn't hurt , but doesn't help a great deal either .", "In particular , lexical bigram statistics appear to be corpus specific , and our results show that they are of no use when attempting to generalize to new training data .", "In fact , they are of surprisingly little benefit even for matched training and test data removing them from the model entirely reduces performance by less than 0 . 5 on the standard WSJ parsing task .", "Our selective pruning technique allows for a more fine grained tuning of parser model size , and would be particularly applicable to cases where large amounts of training data are available but memory usage is a consideration .", "In our implementation , pruning allowed models to run within 256MB that , unpruned , required larger machines .", "The parsing models of Charniak 2000 and Collins 2000 add more complex features to the parsing model that we use as our baseline .", "An area for future work is investigation of the degree to which such features apply across corpora , or , on the other hand , further tune the parser to the peculiarities of the Wall Street Journal .", "Of particular interest are the automatic clusterings of lexical co occurrences used in Charniak 1997 and Magerman 1995 .", "Cross corpus experiments could reveal whether these clusters uncover generally applicable semantic categories for the parser's use .", "Acknowledgments This work was undertaken as part of the FrameNet project at ICSI , with funding from National Science Foundation grant ITR HCI 0086132 ."], "summary_lines": ["Corpus Variation And Parser Performance\n", "Most work in statistical parsing has focused on a single corpus: the Wall Street Journal portion of the Penn Treebank.\n", "While this has allowed for quantitative comparison of parsing techniques, it has left open the question of how other types of text might affect parser performance, and how portable parsing models are across corpora.\n", "We examine these questions by comparing results for the Brown and WSJ corpora, and also consider which parts of the parser's probability model are particularly tuned to the corpus on which it was trained.\n", "This leads us to a technique for pruning parameters to reduce the size of the parsing model.\n", "We show that the accuracy of parsers trained on the Penn Treebank degrades when applied to different genres and domains.\n", "We report results on sentences of 40 or less words on all the Brown corpus sections combined, for which we obtain 80.3%/81.0% recall/precision when training only on data from the WSJ corpus, and 83.9%/84.8% when training on data from the WSJ corpus and all sections of the Brown corpus.\n"]}
{"article_lines": ["Experiments in Domain Adaptation for Statistical Machine Translation", "2 Test set performance of our systems and output reference length ratio .", "4 . 3 Training and decoding parameters We tried to improve performance by increasing some of the limits imposed on the training and decoding setup .", "During training , long sentences are removed from the training data to speed up the GIZA word alignment process .", "Traditionally , we worked with a sentence length limit of 40 .", "We found that increasing this limit to about 80 gave better results without causing undue problems with running the word alignment GIZA increasingly fails and runs much slower with long sentences .", "We also tried to increase beam sizes and the limit on the number of translation options per coverage span ttable limit .", "This has shown to be successful in our experiments with Arabic English and Chinese English systems .", "Surprisingly , increasing the maximum stack size to 1000 from 200 and ttable limit to 100 from 20 has barely any efon translation performance .", "The changed only by less than 0 . 05 , and often worsened .", "4 . 4 German English system The German English language pair is especially challenging due to the large differences in word order .", "Collins et al . 2005 suggest a method to reorder the German input before translating using a set of manually crafted rules .", "In our German English submissions , this is done both to the training data and the input to the machine translation system .", "5 Conclusions Our submission to the WMT 2007 shared task is a fairly straight forward use of the Moses MT system using default parameters .", "In a sense , we submitted baseline performance of this system . for all our systems on the test sets are displayed in Table 2 .", "Compared to other submitted systems , these are very good scores , often the best or second highest scores for these tasks .", "We made a special effort in two areas We explored domain adaptation methods for the News Commentary test sets and we used reordering rules for the German English language pair .", "The open source Moses Koehn et al . , 2007 MT system was originally developed at the University of Edinburgh and received a major boost through a 2007 Johns Hopkins workshop .", "It is now used at several academic institutions as the basic infrastructure for statistical machine translation research .", "The Moses system is an implementation of the phrase based machine translation approach Koehn et al . , 2003 .", "In this approach , an input sentence is first split into text chunks so called phrases , which are then mapped one to one to target phrases using a large phrase translation table .", "Phrases may be reordered , but typically a reordering limit in our experiments a maximum movement over 6 words is used .", "See Figure 1 for an illustration .", "Phrase translation probabilities , reordering probabilities and language model probabilities are combined to give each possible sentence translation a score .", "The best scoring translation is searched for by the decoding algorithm and outputted by the system as the best translation .", "The different system components hi phrase translation probabilities , language model , etc . are combined in a log linear model to obtain the score for the translation e for an input sentence f The weights of the components Ai are set by a discriminative training method on held out development data Och , 2003 .", "The basic components used in our experiments are a two phrase translation probabilities both p e f and p f e , b two word translation probabilities both p e f and p f e , c phrase count , d output word count , e language model , f distance based reordering model , and g lexicalized reordering model .", "For a more detailed description of this model , please refer to Koehn et al . , 2005 .", "Since training data for statistical machine translation is typically collected opportunistically from wherever it is available , the application domain for a machine translation system may be very different from the domain of the system s training data .", "For the WMT 2007 shared task , the challenge was to use a large amount of out of domain training data Proceedings of the Second Workshop on Statistical Machine Translation , pages 224 227 , Prague , June 2007 . c 2007 Association for Computational Linguistics about 40 million words combined with a much smaller amount of in domain training data about 1 million words to optimize translation performance on that particular domain .", "We carried out these experiments on French English .", "The first baseline system is trained only on the outof domain Europarl corpus , which has the following corpus statistics The second baseline system is trained only on the in domain NewsCommentary corpus .", "This corpus is much smaller French English Sentences 42 , 884 Words 1 , 198 , 041 1 , 018 , 503 To make use of all the training data , the straightforward way is to simply concatenate the two training corpora and use the combined data for both translation model and language model training .", "In our situation , however , the out of domain training data overwhelms the in domain training data due to the sheer relative size .", "Hence , we do not expect the best performance from this simplistic approach .", "One way to force a drift to the jargon of the target domain is the use of the language model .", "In our next setup , we used only in domain data for training the language model .", "This enables the system to use all the translation knowledge from the combined corpus , but it gives a preference to word choices that are dominant in the in domain training data .", "Essentially , the goal of our subsequent approaches is to make use of all the training data , but to include a preference for the in domain jargon by giving more weight to the in domain training data .", "This and the next approach explore methods to bias the language model , while the final approach biases the translation model .", "We trained two language models , one for each the out of domain and the in domain training data .", "Language modeling software such as the SRILM toolkit we used Stolke , 2002 allows the interpolation of these language models .", "When interpolating , we give the out of domain language model a weight in respect to the in domain language model .", "Since we want to obtain a language model that gives us the best performance on the target domain , we set this weight so that the perplexity of the development set from that target domain is optimized .", "We searched for the optimal weight setting by simply testing a set of weights and focusing on the most promising range of weights .", "Figure 2 displays all the weights we explored during this process and the corresponding perplexity of the resulting language model on the development set nc dev2007 .", "The optimal weight can be picked out easily from this very smooth curve .", "The log linear modeling approach of statistical machine translation enables a straight forward combination of the in domain and out of domain language models .", "We included them as two separate features , whose weights are set with minimum error rate training .", "The relative weight for each model is set directly by optimizing translation performance .", "Finally , besides biasing the language model to a specific target domain , we may also bias the translation model .", "Here , we take advantage of a feature of the Moses decoder s factored translation model framework .", "In factored translation models , the representation of words is extended to a vector of factors e . g . , surface form , lemma , POS , morphology .", "The mapping of an input phrase to an output phrase is decomposed into several translation and generation steps , each using a different translation or generation table , respectively .", "Such a decomposition is called a decoding path .", "A more recent feature of the factored translation model framework is the possible use of multiple alternative decoding paths .", "This alternate decoding path model was developed by Birch et al . 2007 .", "For our purposes , we use two decoding paths , each consisting of only one translation step .", "One decoding path is the in domain translation table , and the other decoding path is the out of domain translation table .", "Again , respective weights are set with minimum error rate training .", "Table 1 shows results of our domain adaptation experiments on the development test set nc devtest2007 .", "The results suggest that the language model is a useful tool for domain adaptation .", "While training on all the data is essential for good performance , using an in domain language model alone already gives fairly high performance 27 . 46 .", "The performance with the interpolated language model 27 . 12 and two language models 27 . 30 are similar .", "All perform better than the three baseline approaches .", "The results also suggest that higher performance can be obtained by using two translation models through the Moses decoder s alternative decoding path framework .", "We saw our best results under this condition 27 . 64 .", "We participated in all categories .", "Given the four language pairs , with two translation directions and except for Czech two test domains , this required us to build 14 translation systems .", "We had access to a fairly large computer cluster to carry out our experiments over the course of a few weeks .", "However , speed issues with the decoder and load issues on the crowded cluster caused us to take a few shortcuts .", "Also , a bug crept in to our EnglishFrench experiments where we used the wrong detokenizer , resulting drop of 2 3 points in BLEU .", "Minimum error rate training is the most timeconsuming aspects of the training process .", "Due to time constraints , we did not carry out this step for all but the Czech systems a new language for us .", "For the other systems , we re used weight settings from our last year s submission .", "One of the most crucial outcomes of tuning is a proper weight setting for output length , which is especially important for the BLEU score .", "Since the training corpus and tokenization changed , our reused weights are not always optimal in this respect .", "But only in one case we felt compelled to manually adjust the weight for the word count feature , since the original setup led to a output reference length ratio of 0 . 88 on the development test set .", "For the Europarl test sets , we did not use any domain adaptation techniques , but simply used either just the Europarl training data or the combined data whatever gave the higher score on the development test set , although scores differed by only about 0 . 1 0 . 2 BLEU .", "In order to be able to re use the old weights , we were limited to domain adaptation methods that did not change the number of components .", "We decided to use the interpolated language model method described in Section 2 . 5 .", "For the different language pairs , optimal interpolation weights differed We tried to improve performance by increasing some of the limits imposed on the training and decoding setup .", "During training , long sentences are removed from the training data to speed up the GIZA word alignment process .", "Traditionally , we worked with a sentence length limit of 40 .", "We found that increasing this limit to about 80 gave better results without causing undue problems with running the word alignment GIZA increasingly fails and runs much slower with long sentences .", "We also tried to increase beam sizes and the limit on the number of translation options per coverage span ttable limit .", "This has shown to be successful in our experiments with Arabic English and Chinese English systems .", "Surprisingly , increasing the maximum stack size to 1000 from 200 and ttable limit to 100 from 20 has barely any effect on translation performance .", "The BLEU score changed only by less than 0 . 05 , and often worsened .", "The German English language pair is especially challenging due to the large differences in word order .", "Collins et al . 2005 suggest a method to reorder the German input before translating using a set of manually crafted rules .", "In our German English submissions , this is done both to the training data and the input to the machine translation system .", "Our submission to the WMT 2007 shared task is a fairly straight forward use of the Moses MT system using default parameters .", "In a sense , we submitted a baseline performance of this system .", "BLEU and NIST scores for all our systems on the test sets are displayed in Table 2 .", "Compared to other submitted systems , these are very good scores , often the best or second highest scores for these tasks .", "We made a special effort in two areas We explored domain adaptation methods for the NewsCommentary test sets and we used reordering rules for the German English language pair .", "This work was supported in part under the GALE program of the Defense Advanced Research Projects Agency , Contract No .", "HR0011 06 C 0022 and in part under the EuroMatrix project funded by the European Commission 6th Framework Programme ."], "summary_lines": ["Experiments in Domain Adaptation for Statistical Machine Translation\n", "The special challenge of the WMT 2007 shared task was domain adaptation.\n", "We took this opportunity to experiment with various ways of adapting a statistical machine translation systems to a special domain (here: news commentary), when most of the training data is from a different domain (here: European Parliament speeches).\n", "This paper also gives a description of the submission of the University of Edinburgh to the shared task.\n", "Factored translation models is used for the integration of domain adaptation.\n", "We use two language models and two translation models: one in-domain and other out-of-domain to adapt the system.\n", "We learn mixture weights for language models trained with in-domain and out of-domain data respectively by minimizing the perplexity of a tuning (development) set and interpolating the models.\n"]}
{"article_lines": ["Simple Coreference Resolution with Rich Syntactic and Semantic Features", "Coreference systems are driven by syntactic , semantic , and discourse constraints .", "We present a simple approach which completely modularizes these three aspects .", "In contrast to much current work , which focuses on learning and on the discourse component , our system is deterministic and is driven entirely by syntactic and semantic compatibility as learned from a large , unlabeled corpus .", "Despite its simplicity and discourse naivete , our system substantially outperforms all unsupervised systems and most supervised ones .", "Primary contributions include 1 the presentation of a simpleto reproduce , high performing baseline and 2 the demonstration that most remaining errors can be attributed to syntactic and semantic factors external to the coreference phenomenon and perhaps best addressed by non coreference systems .", "The resolution of entity reference is influenced by a variety of constraints .", "Syntactic constraints like the binding theory , the i within i filter , and appositive constructions restrict reference by configuration .", "Semantic constraints like selectional compatibility e . g . a spokesperson can announce things and subsumption e . g .", "Microsoft is a company rule out many possible referents .", "Finally , discourse phenomena such as salience and centering theory are assumed to heavily influence reference preferences .", "As these varied factors have given rise to a multitude of weak features , recent work has focused on how best to learn to combine them using models over reference structures Culotta et al . , 2007 ; Denis and Baldridge , 2007 ; Klenner and Ailloud , 2007 .", "In this work , we break from the standard view .", "Instead , we consider a vastly more modular system in which coreference is predicted from a deterministic function of a few rich features .", "In particular , we assume a three step process .", "First , a selfcontained syntactic module carefully represents syntactic structures using an augmented parser and extracts syntactic paths from mentions to potential antecedents .", "Some of these paths can be ruled in or out by deterministic but conservative syntactic constraints .", "Importantly , the bulk of the work in the syntactic module is in making sure the parses are correctly constructed and used , and this module s most important training data is a treebank .", "Second , a self contained semantic module evaluates the semantic compatibility of headwords and individual names .", "These decisions are made from compatibility lists extracted from unlabeled data sources such as newswire and web data .", "Finally , of the antecedents which remain after rich syntactic and semantic filtering , reference is chosen to minimize tree distance .", "This procedure is trivial where most systems are rich , and so does not need any supervised coreference data .", "However , it is rich in important ways which we argue are marginalized in recent coreference work .", "Interestingly , error analysis from our final system shows that its failures are far more often due to syntactic failures e . g . parsing mistakes and semantic failures e . g . missing knowledge than failure to model discourse phenomena or appropriately weigh conflicting evidence .", "One contribution of this paper is the exploration of strong modularity , including the result that our system beats all unsupervised systems and approaches the state of the art in supervised ones .", "Another contribution is the error analysis result that , even with substantial syntactic and semantic richness , the path to greatest improvement appears to be to further improve the syntactic and semantic modules .", "Finally , we offer our approach as a very strong , yet easy to implement , baseline .", "We make no claim that learning to reconcile disparate features in a joint model offers no benefit , only that it must not be pursued to the exclusion of rich , nonreference analysis .", "In coreference resolution , we are given a document which consists of a set of mentions ; each mention is a phrase in the document typically an NP and we are asked to cluster mentions according to the underlying referent entity .", "There are three basic mention types proper Barack Obama , nominal president , and pronominal he . 1 For comparison to previous work , we evaluate in the setting where mention boundaries are given at test time ; however our system can easily annotate reference on all noun phrase nodes in a parse tree see Section 3 . 1 . 1 .", "In this work we use the following data sets Development see Section 3 We will present evaluations on multiple coreference resolution metrics , as no single one is clearly superior", "In this section we develop our system and report developmental results on ACE2004 ROTHDEV see Section 2 . 1 ; we report pairwise F1 figures here , but report on many more evaluation metrics in Section 4 .", "At a high level , our system resembles a pairwise coreference model Soon et al . , 1999 ; Ng and Cardie , 2002 ; Bengston and Roth , 2008 ; for each mention mi , we select either a single best antecedent amongst the previous mentions m1 , . . . , mi 1 , or the NULL mention to indicate the underlying entity has not yet been evoked .", "Mentions are linearly ordered according to the position of the mention head with ties being broken by the larger node coming first .", "While much research Ng and Cardie , 2002 ; Culotta et al . , 2007 ; Haghighi and Klein , 2007 ; Poon and Domingos , 2008 ; Finkel and Manning , 2008 has explored how to reconcile pairwise decisions to form coherent clusters , we simply take the transitive closure of our pairwise decision as in Ng and Cardie 2002 and Bengston and Roth 2008 which can and does cause system errors .", "In contrast to most recent research , our pairwise decisions are not made with a learned model which outputs a probability or confidence , but instead for each mention mi , we select an antecedent amongst m1 , . . . , mi_1 or the NULL mention as follows Initially , there is no syntactic constraint improved in Section 3 . 1 . 3 , the antecedent compatibility filter allows proper and nominal mentions to corefer only with mentions that have the same head improved in Section 3 . 2 , and pronouns have no compatibility constraints improved in Section 3 . 1 . 2 .", "Mention heads are determined by parsing the given mention span with the Stanford parser Klein and Manning , 2003 and using the Collins head rules Collins , 1999 ; Poon and Domingos 2008 showed that using syntactic heads strongly outperformed a simple rightmost headword rule .", "The mention type is determined by the head POS tag proper if the head tag is NNP or NNPS , pronoun if the head tag is PRP , PRP , WP , or WP , and nominal otherwise .", "For the selection phase , we order mentions m1 , . . . , mi_1 according to the position of the head word and select the closest mention that remains after constraint and filtering are applied .", "This choice reflects the intuition of Grosz et al . 1995 that speakers only use pronominal mentions when there are not intervening compatible mentions .", "This system yields a rather low 48 . 9 pairwise F1 see BASE FLAT in Table 2 .", "There are many , primarily recall , errors made choosing antecedents for all mention types which we will address by adding syntactic and semantic constraints .", "In this section , we enrich the syntactic representation and information in our system to improve results .", "We first focus on fixing the pronoun antecedent choices .", "A common error arose from the use of mention head distance as a poor proxy for discourse salience .", "For instance consider the example in Figure 1 , the mention America is closest to its in flat mention distance , but syntactically Nintendo ofAmerica holds a more prominent syntactic position relative to the pronoun which , as Hobbs 1977 argues , is key to discourse salience .", "Mapping Mentions to Parse Nodes In order to use the syntactic position of mentions to determine anaphoricity , we must associate each mention in the document with a parse tree node .", "We parse all document sentences with the Stanford parser , and then for each evaluation mention , we find the largest span NP which has the previously determined mention head as its head . 5 Often , this results in a different , typically larger , mention span than annotated in the data .", "Now that each mention is situated in a parse tree , we utilize the length of the shortest tree path between mentions as our notion of distance .", "In by agreement constraints see Section 3 . 1 . 2 .", "The pronoun them is closest to the site mention , but has an incompatible number feature with it .", "The closest in tree distance , see Section 3 . 1 . 1 compatible mention is The Israelis , which is correct particular , this fixes examples such as those in Figure 1 where the true antecedent has many embedded mentions between itself and the pronoun .", "This change by itself yields 51 . 7 pairwise F1 see BASE TREE in Table 2 , which is small overall , but reduces pairwise pronoun antecedent selection error from 51 . 3 to 42 . 5 .", "We now refine our compatibility filtering to incorporate simple agreement constraints between coreferent mentions .", "Since we currently allow proper and nominal mentions to corefer only with matching head mentions , agreement is only a concern for pronouns .", "Traditional linguistic theory stipulates that coreferent mentions must agree in number , person , gender , and entity type e . g . animacy .", "Here , we implement person , number and entity type agreement . 6 A number feature is assigned to each mention deterministically based on the head and its POS tag .", "For entity type , we use NER labels .", "Ideally , we would like to have information about the entity type of each referential NP , however this information is not easily obtainable .", "Instead , we opt to utilize the Stanford NER tagger Finkel et al . , 2005 over the sentences in a document and annotate each NP with the NER label assigned to that mention head .", "For each mention , when its NP is assigned an NER label we allow it to only be compatible with that NER label . 7 For pronouns , we deterministically assign a set of compatible NER values e . g . personal pronouns can only be a PERpositive and i within i constraint .", "The i withini constraint disallows coreference between parent and child NPs unless the child is an appositive .", "Hashed numbers indicate ground truth but are not in the actual trees .", "SON , but its can be an ORGANIZATION or LOCATION .", "Since the NER tagger typically does not label non proper NP heads , we have no NER compatibility information for nominals .", "We incorporate agreement constraints by filtering the set of possible antecedents to those which have compatible number and NER types with the target mention .", "This yields 53 . 4 pairwise F1 , and reduces pronoun antecedent errors to 42 . 5 from 34 . 4 .", "An example of the type of error fixed by these agreement constraints is given by Figure 2 .", "Our system has so far focused only on improving pronoun anaphora resolution .", "However , a plurality of the errors made by our system are amongst nonpronominal mentions . 8 We take the approach that in order to align a non pronominal mention to an antecedent without an identical head , we require evidence that the mentions are compatible .", "Judging compatibility of mentions generally requires semantic knowledge , to which we return later .", "However , some syntactic configurations guarantee coreference .", "The one exploited most in coreference work Soon et al . , 1999 ; Ng and Cardie , 2002 ; Luo et al . , 2004 ; Culotta et al . , 2007 ; Poon and Domingos , 2008 ; Bengston and Roth , 2008 is the appositive construction .", "Here , we represent apposition as a syntactic feature of an NP indicating that it is coreferent with its parent NP e . g . it is an exception to the i within i constraint that parent and child NPs cannot be coreferent .", "We deterministically mark a node as NP APPOS see Figure 3 when it is the third child in of a parent NP whose expansion begins with NP , NP , and there is not a conjunction in the expansion to avoid marking elements in a list as appositive .", "Role Appositives During development , we discovered many errors which involved a variant of appositives which we call role appositives see painter in Figure 3 , where an NP modifying the head NP describes the role of that entity typically a person entity .", "There are several challenges to correctly labeling these role NPs as being appositives .", "First , the NPs produced by Treebank parsers are flat and do not have the required internal structure see Figure 3 a .", "While fully solving this problem is difficult , we can heuristically fix many instances of the problem by placing an NP around maximum length sequences of NNP tags or NN and JJ tags within an NP ; note that this will fail for many constructions such as U . S . President Barack Obama , which is analyzed as a flat sequence of proper nouns .", "Once this internal NP structure has been added , whether the NP immediately to the left of the head NP is an appositive depends on the entity type .", "For instance , Rabbi Ashi is an apposition but Iranian army is not .", "Again , a full solution would require its own model , here we mark as appositions any NPs immediately to the left of a head child NP where the head child NP is identified as a person by the NER tagger . 9 We incorporate NP appositive annotation as a constraint during filtering .", "Any mention which corresponds to an appositive node has its set of possible antecedents limited to its parent .", "Along with the appositive constraint , we implement the i within i constraint that any non appositive NP cannot be be coreferent with its parent ; this constraint is then propagated to any node its parent is forced to agree with .", "The order in which these constraints are applied is important , as illustrated by the example in Figure 4 First the list of possible antecedents for the appositive NP is constrained to only its parent .", "Now that all appositives have been constrained , we apply the i withini constraint , which prevents its from having the NP headed by brand in the set of possible antecedents , and by propagation , also removes the NP headed by Gitano .", "This leaves the NP Wal Mart as the closest compatible mention .", "Adding these syntactic constraints to our system yields 55 . 4 F1 , a fairly substantial improvement , but many recall errors remain between mentions with differing heads .", "Resolving such cases will require external semantic information , which we will automatically acquire see Section 3 . 2 .", "Predicate Nominatives Another syntactic constraint exploited in Poon and Domingos 2008 is the predicate nominative construction , where the object of a copular verb forms of the verb be is constrained to corefer with its subject e . g .", "Microsoft is a company in Redmond .", "While much less frequent than appositive configurations there are only 17 predicate nominatives in our devel9Arguably , we could also consider right modifying NPs e . g . , Microsoft Company 1 1 to be role appositive , but we do not do so here . opment set , predicate nominatives are another highly reliable coreference pattern which we will leverage in Section 3 . 2 to mine semantic knowledge .", "As with appositives , we annotate object predicate nominative NPs and constrain coreference as before .", "This yields a minor improvement to 55 . 5 F1 .", "While appositives and related syntactic constructions can resolve some cases of non pronominal reference , most cases require semantic knowledge about the various entities as well as the verbs used in conjunction with those entities to disambiguate references Kehler et al . , 2008 .", "However , given a semantically compatible mention head pair , say AOL and company , one might expect to observe a reliable appositive or predicative nominative construction involving these mentions somewhere in a large corpus .", "In fact , the Wikipedia page for AOL10 has a predicate nominative construction which supports the compatibility of this head pair AOL LLC formerly America Online is an American global Internet services and media company operated by Time Warner .", "In order to harvest compatible head pairs , we utilize our BLIPP and WIKI data sets see Section 2 , and for each noun proper or common and pronoun , we assign a maximal NP mention node for each nominal head as in Section 3 . 1 . 1 ; we then annotate appositive and predicate nominative NPs as in Section 3 . 1 . 3 .", "For any NP which is annotated as an appositive or predicate nominative , we extract the head pair of that node and its constrained antecedent .", "The resulting set of compatible head words , while large , covers a little more than half of the examples given in Table 1 .", "The problem is that these highly reliable syntactic configurations are too sparse and cannot capture all the entity information present .", "For instance , the first sentence of Wikipedia abstract for Al Gore is Albert Arnold Al Gore , Jr . is an American environmental activist who served as the 45th Vice President of the United States from 1993 to 2001 under President Bill Clinton .", "The required lexical pattern X who served as Y is a general appositive like pattern that almost surely indicates coreference .", "Rather than opt to manually create a set of these coreference patterns as in Hearst 1992 , we instead opt to automatically extract these patterns from large corpora as in Snow et al . 2004 and Phillips and Riloff 2007 .", "We take a simple bootstrapping technique given a set of mention pairs extracted from appositives and predicate nominative configurations , we extract counts over tree fragments between nodes which have occurred in this set of head pairs see Figure 5 ; the tree fragments are formed by annotating the internal nodes in the tree path with the head word and POS along with the subcategorization .", "We limit the paths extracted in this way in several ways paths are only allowed to go between adjacent sentences and have a length of at most 10 .", "We then filter the set of paths to those which occur more than a hundred times and with at least 10 distinct seed head word pairs .", "The vast majority of the extracted fragments are variants of traditional appositives and predicatenominatives with some of the structure of the NPs specified .", "However there are some tree fragments which correspond to the novel coreference patterns see Figure 5 of parenthetical alias as well as conjunctions of roles in NPs .", "We apply our extracted tree fragments to our BLIPP and WIKI data sets and extract a set of compatible word pairs which match these fragments ; these words pairs will be used to relax the semantic compatibility filter see the start of the section ; mentions are compatible with prior mentions with the same head or with a semantically compatible head word .", "This yields 58 . 5 pairwise F1 see SEMCOMPAT in Table 2 as well as similar improvements across other metrics .", "By and large the word pairs extracted in this way are correct in particular we now have coverage for over two thirds of the head pair recall errors from Table 1 .", "There are however wordpairs which introduce errors .", "In particular citystate constructions e . g .", "Los Angeles , California appears to be an appositive and incorrectly allows our system to have angeles as an antecedent for california .", "Another common error is that the symbol is made compatible with a wide variety of common nouns in the financial domain .", "We present formal experimental results here see Table 2 .", "We first evaluate our model on the ACE2004 CULOTTA TEST dataset used in the state of the art systems from Culotta et al . 2007 and Bengston and Roth 2008 .", "Both of these systems were supervised systems discriminatively trained to maximize b3 and used features from many different structured resources including WordNet , as well as domain specific features Culotta et al . , 2007 .", "Our best b3 result of 79 . 0 is broadly in the range of these results .", "We should note that in our work we use neither the gold mention types we do not model pre nominals separately nor do we use the gold NER tags which Bengston and Roth 2008 does .", "Across metrics , the syntactic constraints and semantic compatibility components contribute most to the overall final result .", "On the MUC6 TEST dataset , our system outpersion made by the system .", "Each row is a mention type and the column the predicted mention type antecedent .", "The majority of errors are made in the NOMINAL category . forms both Poon and Domingos 2008 an unsupervised Markov Logic Network system which uses explicit constraints and Finkel and Manning 2008 a supervised system which uses ILP inference to reconcile the predictions of a pairwise classifier on all comparable measures . 11 Similarly , on the ACE2004 NWIRE dataset , we also outperform the state of the art unsupervised system of Poon and Domingos 2008 .", "Overall , we conclude that our system outperforms state of the art unsupervised systems12 and is in the range of the state of the art systems of Culotta et al . 2007 and Bengston and Roth 2008 .", "There are several general trends to the errors made by our system .", "Table 3 shows the number of pairwise errors made on MUC6 TEST dataset by mention type ; note these errors are not equally weighted in the final evaluations because of the transitive closure taken at the end .", "The most errors are made on nominal mentions with pronouns coming in a distant second .", "In particular , we most frequently say a nominal is NULL when it has an antecedent ; this is typically due to not having the necessary semantic knowledge to link a nominal to a prior expression .", "In order to get a more thorough view of the cause of pairwise errors , we examined 20 random errors made in aligning each mention type to an antecedent .", "We categorized the errors as follows we incorrectly aligned a pronoun to a mention with which it is not semantically compatible e . g . he aligned to board . mentions with the same head are always compatible .", "Includes modifier and specificity errors such as allowing Lebanon and Southern Lebanon to corefer .", "This also includes errors of definiteness in nominals e . g . the people in the room and Chinese people .", "Typically , these errors involve a combination of missing syntactic and semantic information .", "The result of this error analysis is given in Table 4 ; note that a single error may be attributed to more than one cause .", "Despite our efforts in Section 3 to add syntactic and semantic information to our system , the largest source of error is still a combination of missing semantic information or annotated syntactic structure rather than the lack of discourse or salience modeling .", "Our error analysis suggests that in order to improve the state of the art in coreference resolution , future research should consider richer syntactic and semantic information than typically used in current systems .", "Our approach is not intended as an argument against the more complex , discourse focused approaches that typify recent work .", "Instead , we note that rich syntactic and semantic processing vastly reduces the need to rely on discourse effects or evidence reconciliation for reference resolution .", "Indeed , we suspect that further improving the syntactic and semantic modules in our system may produce greater error reductions than any other route forward .", "Of course , a system which is rich in all axes will find some advantage over any simplified approach .", "Nonetheless , our coreference system , despite being relatively simple and having no tunable parameters or complexity beyond the non reference complexity of its component modules , manages to outperform state of the art unsupervised coreference resolution and be broadly comparable to state of the art supervised systems ."], "summary_lines": ["Simple Coreference Resolution with Rich Syntactic and Semantic Features\n", "Coreference systems are driven by syntactic, semantic, and discourse constraints.\n", "We present a simple approach which completely modularizes these three aspects.\n", "In contrast to much current work, which focuses on learning and on the discourse component, our system is deterministic and is driven entirely by syntactic and semantic compatibility as learned from a large, unlabeled corpus.\n", "Despite its simplicity and discourse naivete, our system substantially outperforms all unsupervised systems and most supervised ones.\n", "Primary contributions include (1) the presentation of a simple-to-reproduce, high-performing baseline and (2) the demonstration that most remaining errors can be attributed to syntactic and semantic factors external to the coreference phenomenon (and perhaps best addressed by non-coreference systems).\n", "We show that coreference errors in state-of-the art systems are frequently due to poor models of semantic compatibility.\n", "In our SYN-CONSTR setting, each referring mention is coreferent with any past mention with the same head or in a deterministic syntactic configuration (appositives or predicative nominatives constructions).\n", "When searching for an antecedent for mk, its candidate antecedents are visited in an order determined by their positions in the associated parse tree.\n"]}
{"article_lines": ["A General Framework For Distributional Similarity", "We present a general framework for distributional similarity based on the concepts of precision and recall .", "Different parameter settings within this framework approximate different existing similarity measures as well as many more which have , until now , been unexplored .", "We show that optimal parameter settings outperform two existing state of the art similarity measures on two evaluation tasks for high and low frequency nouns .", "There are many potential applications of sets of distributionally similar words .", "In the syntactic domain , language models , which can be used to evaluate alternative interpretations of text and speech , require probabilistic information about words and their co occurrences which is often not available due to the sparse data problem .", "In order to overcome this problem , researchers e . g .", "Pereira et al . 1993 have proposed estimating probabilities based on sets of words which are known to be distributionally similar .", "In the semantic domain , the hypothesis that words which mean similar things behave in similar ways Levin , 1993 , has led researchers e . g .", "Lin 1998 to propose that distributional similarity might be used as a predictor of semantic similarity .", "Accordingly , we might automatically build thesauruses which could be used in tasks such as malapropism correction Budanitsky and Hirst , 2001 and text summarization Silber and McCoy , 2002 .", "However , the loose definition of distributional similarity that two words are distributionally similar if they appear in similar contexts has led to many distributional similarity measures being proposed ; for example , the L1 Norm , the Euclidean Distance , the Cosine Metric Salton and McGill , 1983 , Jaccard's Coefficient Frakes and Baeza Yates , 1992 , the Dice Coefficient Frakes and Baeza Yates , 1992 , the KullbackLeibler Divergence Cover and Thomas , 1991 , the Jenson Shannon Divergence Rao , 1983 , the a skew Divergence Lee , 1999 , the Confusion Probability Essen and Steinbiss , 1992 , Hindle's Mutual Information MI Based Measure Hindle , 1990 and Lin's MI Based Measure Lin , 1998 .", "Further , there is no clear way of deciding which is the best measure .", "Application based evaluation tasks have been proposed , yet it is not clear Weeds and Weir , 2003 whether there is or should be one distributional similarity measure which outperforms all other distributional similarity measures on all tasks and for all words .", "We take a generic approach that does not directly reduce distributional similarity to a single dimension .", "The way dimensions are combined together will depend on parameters tuned to the demands of a given application .", "Further , different parameter settings will approximate different existing similarity measures as well as many more which have , until now , been unexplored .", "The contributions of this paper are four fold .", "First , we propose a general framework for distributional similarity based on the concepts of precision and recall Section 2 .", "Second , we evaluate the framework at its optimal parameter settings for two different applications Section 3 , showing that it outperforms existing state ofthe art similarity measures for both high and low frequency nouns .", "Third , we begin to investigate to what extent existing similarity measures might be characterised in terms of parameter settings within the framework Section 4 .", "Fourth , we provide an understanding of why a single existing measure cannot achieve optimal results in every application of distributional similarity measures .", "In this section , we introduce the relevance of the Information Retrieval IR concepts of precision and recall in the context of word similarity .", "We provide combinatorial , probabilistic and mutual information based models for precision and recall and discuss combining precision and recall to provide a single number in the context of a particular application .", "The similarity' of two nouns can be viewed as a measure of how appropriate it is to use one noun or its distribution in place of the other .", "If we are using the distribution of one noun in place of the distribution the other noun , we can consider the precision and recall of the prediction made .", "Precision tells us how much of what has been predicted is correct whilst recall tells us how much of what is required has been predicted .", "In order to calculate precision and recall , we first need to consider for each noun n which verb co occurrences will be predicted by it and , conversely , required in a description of it .", "We will refer to these verbs as the features of n , F n where D n , v , is the degree of association between noun n and verb v . Possible association functions will be defined in the context of each model described below .", "If we are considering the ability of noun A to predict noun B then it follows that the set of True Positives is TP F A n F B and precision and recall can be defined as Precision and recall both lie in the range 0 , 1 and are both equal to one when each noun has exactly the same features .", "It should also be noted that RA , B P B , A .", "We will now consider some different possibilities for measuring the degree of association between a noun n and a verb v . In the combinatorial model , we simply consider whether a verb has ever been seen to co occur with the noun .", "In other words , the degree of association D between a noun n and a verb v is 1 if they have co occurred together and 0 otherwise .", "In this case , it should be noted that the definitions of precision and recall can be simplified as follows In the probabilistic model , more probable or more frequent co occurrences are considered more significant .", "The degree of association between a noun n and verb v is defined in the probabilistic model as The definitions for feature set membership , TP , precision and recall all remain the same except for the use of the new association function .", "Using the probabilistic model , the precision of A's prediction of B is the probability that a verb picked at random from those co occurring with A will also co occur with B ; and the recall of A's prediction of B is the probability that a verb picked at random from those those cooccurring with B will also co occur with A .", "Mutual information MI allows us to capture the idea that a co occurrence of low probability events is more informative than a co occurrence of high probability events .", "In this model , as before , we retain the definitions for feature set membership , TP , precision and recall but again change the association function .", "Here , the degree of association between a noun n and a verb v is their MI .", "Accordingly , verb v will be considered to be a feature of noun n if the probability of their cooccurrence is greater than would be expected if verbs and nouns occurred independently .", "Although we have defined a pair of numbers for similarity , in applications it will still be necessary to compute a single number in order to determine neighbourhood or cluster membership .", "There are two obvious ways to optimise a pair of numbers such as precision and recall .", "The first is to use an arithmetic mean , which optimises the sum of the numbers , and the second is to use a harmonic mean2 , which optimises the product of the numbers .", "In an attempt to retain generality , we can allow both alternatives by computing an arithmetic mean of the harmonic mean and the arithmetic mean , noting that the relative importance of each term in an arithmetic mean is controlled by weights which sum to 1 where both and y lie in the range 0 , 1 .", "The resulting similarity sim A , B will also lie in the range 0 , 1 where 0 represents complete lack of similarity and 1 represents equivalence .", "This formula can be used in combination with any of the models for precision and recall outlined above .", "Further , the generality allows us to investigate empirically the relative significance of the different terms and thus whether one or more might be omitted in future work .", "Precision and recall can be computed once for every pair of words whereas similarity is something which will be computed for a specific task and will depend on the values of 3 and y .", "Table 1 summarizes some special parameter settings .", "In this section , we evaluate the performance of the framework , using the combinatorial and MI based models of precision and recall , at two application based tasks against Lin's MIbased Measure simun and the a skew Divergence Measure simasd .", "The formulae for these measures are given in Figure 1 .", "For the askew divergence measure we set a 0 . 99 since this most closely approximates the KullbackLeibler divergence measure .", "The two evaluation tasks used pseudo disambiguation and WordNet Fellbaum , 1998 prediction are fairly standard for distributional similarity measures .", "However , in the future we wish to extend our evaluation to other tasks such as malapropism correction Budanitsky and Hirst , 2001 and PP attachment ambiguity resolution Resnik , 1993 and also to the probabilistic model .", "Since we use the same data and methodology as in earlier work , some detail is omitted in the subsequent discussion but full details and rationale can be found in Weeds and Weir 2003 .", "Pseudo disambiguation tasks e . g .", "Lee , 1999 have become a standard evaluation technique and , in the current context , we may use a word's neighbours to decide which of two cooccurrences is the most likely .", "Although pseudo disambiguation itself is an artificial task , it has relevance in at least two real application areas .", "First , by replacing occurrences of a particular word in a test suite with a pair or set of words from which a technique must choose , we recreate a simplified version of the word sense disambiguation task ; that is , choosing between a fixed number of homonyms based on local context .", "The second is in language modelling where we wish to estimate the probability of co occurrences of events but , due to the sparse data problem , it is often the case that a possible co occurrence has not been seen in the training data .", "As is common in this field e . g .", "Lee , 1999 , we study similarity between nouns based on their co occurrences with verbs in the direct object relation .", "We study similarity between high and low frequency nouns since we want to investigate any associations between word frequency and quality of neighbours found by the measures but it is impractical to evaluate a large number of similarity measures over all nouns .", "2 , 852 , 300 lemmatised noun verb directobject pairs were extracted from the BNC using a shallow parser Briscoe and Carroll , 1995 ; Carroll and Briscoe , 1996 .", "From those nouns also occurring in WordNet , we selected the 1000 most frequent3 nouns and a set of 1000 low frequency4 nouns .", "For each noun , 80 of the available data was randomly selected as training data and the other 20 set aside as test data .", "Precision and recall were computed for each pair of nouns using the combinatorial and MI models .", "This data is then available to the application task which will first have to compute the similarity for each pair of nouns based on current parameter settings and select nearest neighbours accordingly .", "We converted each noun verb pair n , vi in the set aside test data into a noun verb verb triple n , vi , v2 where P vi is approximately equal to P v2 over all the training data and n , v2 has not been seen in the test or training data .", "A high frequency noun test set and a low frequency noun test set , each containing 10 , 000 test instances , were then constructed by selecting ten test instances for each noun in a two step process of 1 whilst more than ten triples remained , discarding duplicate triples and 2 randomly selecting ten triples from those remaining after step 1 .", "Each set of test triples was split into five disjoint subsets , containing two triples for each noun , so that average performance and standard error could be computed .", "Additionally , three of the five subsets were used as a development set to optimise parameters k , , i3 and y and the remaining two used as a test set to find error rates at these optimal settings .", "The task is then for the nearest neighbours of noun n to decide which of n , vi and n , v2 was the original co occurrence .", "Each of n's neighbours , m , is given a vote which is equal to the difference in frequencies of the co occurrences m , vi and m , v2 and which it casts to the cooccurrence in which it appears most frequently .", "The votes for each co occurrence are summed over all of the k nearest neighbours of n and the co occurrence with the most votes wins .", "Performance is measured as error rate .", "of ties , error T 1 of incorrect choices 2 where T is the number of test instances . is that the hyponymy relation in WordNet is a gold standard for semantic similarity which is , of course , not true .", "However , we believe that a distributional similarity measure which more closely predicts WordNet , is more likely to be a good predictor of semantic similarity .", "We will first explain the WordNet based distance measure Lin , 1997 and then explain how we determine the similarity between neighbour sets generated using different measures .", "The similarity of two nouns in WordNet is defined as the similarity of their maximally similar senses .", "The commonality of two concepts is defined as the maximally specific superclass of those concepts .", "So , if syn n is the set of senses of the noun n in WordNet , sup c is the set of possibly indirect superclasses of concept c in WordNet and P c is the probability that a randomly selected noun refers to an instance of c , then the similarity between ni and n2 can be calculated using the formula for simwn in Figure 1 .", "The probabilities P c are estimated by the frequencies of concepts in SemCor Miller et al . , 1994 , a sense tagged subset of the Brown corpus , noting that the occurrence of a concept refers to instances of all the superclasses of that concept i . e .", "P root of tree6 1 .", "The k nearest neighbours7 of each noun , computed using each distributional similarity measure at each parameter setting , are then compared with the k nearest neighbours of the noun according to the WordNet based measure .", "In order to compute the similarity of two neighbour sets , we transform each neighbour set so that each neighbour is given a rank score of k rank .", "We do not use the similarity scores directly since these require normalization if different similarity measures using different scales are to be compared .", "Having performed this transformation , the neighbour sets for the same word w may be represented by two ordered sets of words wk , w1 and w , wl .", "The similarity between such sets is computed using the same calculation as used by Lin 1998 except for simplifications due to the use of ranks where i and j are the rank scores of the words within each neighbour set .", "Table 3 summarizes the optimal mean similarities and parameter settings for the general framework using both the combinatorial sim and the MI based simmt models .", "Results for Lin's MI based measure simun and the a skew divergence measure simasd are also given and results are divided into those for high frequency nouns and those for low frequency nouns .", "Standard errors in the optimal mean similarities are not given but were of the order of 0 . 1 .", "Our first observation is that the general framework using the MI based model for precision and recall outperforms all of the other distributional similarity measures .", "We also observe that lower values of y produce better results , particularly for low frequency nouns .", "For example , when y 1 , similarity for low frequency nouns drops to 0 . 147 using the combinatorial model and 0 . 177 using the MI based model .", "Third , from Figure 3 , it appears that this WordNet prediction task favours measures which select high recall neighbours .", "Although optimum similarity for the combinatorial model occurs at , 8 0 . 5 , similarity is always higher for lower values of than for higher values of 3 . ing the a skew divergence measure and those found using the MI Based model .", "Optimal similarity 0 . 760 and 0 . 725 respectively was found at y 0 . 0 and , i3 0 . 0 for high frequency nouns and at y 0 . 25 and 3 0 . 0 for low frequency nouns .", "Further , similarity between the measures drops rapidly once , i3 rises above 0 . 3 .", "Using the MI based model for precision and recall and with a parameter setting of y 1 . 0 , the general framework for distributional similarity proposed herein closely approximates Lin's 1998 Measure .", "However , we have shown that using a much lower value of y so that the combination of precision and recall is closer to a weighted arithmetic mean than a harmonic mean yields better results in the two application tasks considered here .", "This is because the relative importance of precision and recall can be tuned to the task at hand .", "Further , we have shown that pseudodisambiguation is a task which requires high precision neighbours whereas WordNet prediction is a task which requires high recall neighbours .", "Accordingly , it is not clear how a single unparameterised similarity measure could give optimum results on both tasks .", "In the future , we intend to extend the work to the characterisation of other tasks and other existing similarity measures .", "As well as their , usually implicit , use of precision and recall , the main difference between existing similarity measures will be the models in which precision and recall are defined .", "We have explored two such models here a combinatorial model and a MIbased model and have shown that the MIbased model achieves significantly improved results over the combinatorial model .", "We propose to investigate other models such as the probabilistic one given in Section 2 . 3 .", "We would like to thank John Carroll for the use of his parser , Adam Kilgarriff and Bill Keller for valuable discussions and the UK EPSRC for its studentship to the first author ."], "summary_lines": ["A General Framework For Distributional Similarity\n", "We present a general framework for distributional similarity based on the concepts of precision and recall.\n", "Different parameter settings within this framework approximate different existing similarity measures as well as many more which have, until now, been unexplored.\n", "We show that optimal parameter settings outperform two existing state-of-the-art similarity measures on two evaluation tasks for high and low frequency nouns.\n", "We propose a general framework for distributional similarity that consists of notions of precision and recall.\n"]}
{"article_lines": ["Automatic Evaluation Of Summaries Using N Gram Co Occurrence Statistics", "Following the recent adoption by the machine translation community of automatic evaluation using the BLEU NIST scoring process , we conduct an in depth study of a similar idea for evaluating summaries .", "The results show that automatic evaluation using unigram cooccurrences between summary pairs correlates surprising well with human evaluations , based on various statistical metrics ; while direct application of the BLEU evaluation procedure does not always give good results .", "Automated text summarization has drawn a lot of interest in the natural language processing and information retrieval communities in the recent years .", "A series of workshops on automatic text summarization WAS 2000 , 2001 , 2002 , special topic sessions in ACL , COLING , and SIGIR , and government sponsored evaluation efforts in the United States DUC 2002 and Japan Fukusima and Okumura 2001 have advanced the technology and produced a couple of experimental online systems Radev et al . 2001 , McKeown et al .", "Despite these efforts , however , there are no common , convenient , and repeatable evaluation methods that can be easily applied to support system development and just in time comparison among different summarization methods .", "The Document Understanding Conference DUC 2002 run by the National Institute of Standards and Technology NIST sets out to address this problem by providing annual large scale common evaluations in text summarization .", "However , these evaluations involve human judges and hence are subject to variability Rath et al . 1961 .", "For example , Lin and Hovy 2002 pointed out that 18 of the data contained multiple judgments in the DUC 2001 single document evaluation1 .", "To further progress in automatic summarization , in this paper we conduct an in depth study of automatic evaluation methods based on n gram co occurrence in the context of DUC .", "Due to the setup in DUC , the evaluations we discussed here are intrinsic evaluations Sparck Jones and Galliers 1996 .", "Section 2 gives an overview of the evaluation procedure used in DUC .", "Section 3 discusses the IBM BLEU Papineni et al . 2001 and NIST 2002 n gram co occurrence scoring procedures and the application of a similar idea in evaluating summaries .", "Section 4 compares n gram cooccurrence scoring procedures in terms of their correlation to human results and on the recall and precision of statistical significance prediction .", "Section 5 concludes this paper and discusses future directions .", "The 2002 Document Understanding Conference2 included the follow two main tasks given a set of documents about a single subject , participants were required to create 4 generic summaries of the entire set , containing 50 , 100 , 200 , and 400 words respectively .", "The document sets were of four types a single natural disaster event ; a single event ; multiple instances of a type of event ; and information about an individual .", "The training set comprised 30 sets of approximately 10 documents , each provided with their 50 , 100 , 200 , and 400 word human written summaries .", "The test set comprised 30 unseen sets .", "A total of 11 systems participated in the singledocument summarization task and 12 systems participated in the multi document task .", "For each document or document set , one human summary was created as the ideal' model summary at each specified length .", "Two other human summaries were also created at each length .", "In addition , baseline summaries were created automatically for each length as reference points .", "For the multi document summarization task , one baseline , lead baseline , took the first 50 , 100 , 200 , and 400 words in the last document in the collection .", "A second baseline , coverage baseline , took the first sentence in the first document , the first sentence in the second document and so on until it had a summary of 50 , 100 , 200 , or 400 words .", "Only one baseline baseline1 was created for the single document summarization task .", "To evaluate system performance NIST assessors who created the ideal' written summaries did pairwise comparisons of their summaries to the system generated summaries , other assessors' summaries , and baseline summaries .", "They used the Summary Evaluation Environment SEE 2 . 0 developed by Lin 2001 to support the process .", "Using SEE , the assessors compared the system's text the peer text to the ideal the model text .", "As shown in Figure 1 , each text was decomposed into a list of units and displayed in separate windows .", "SEE 2 . 0 provides interfaces for assessors to judge both the content and the quality of summaries .", "To measure content , assessors step through each model unit , mark all system units sharing content with the current model unit green dark gray highlight in the model summary window , and specify that the marked system units express all , most , some , or hardly any of the content of the current model unit .", "To measure quality , assessors rate grammaticality3 , cohesion4 , and coherence5 at five different levels all , most , some , hardly any , or none6 .", "For example , as shown in Figure 1 , an assessor marked system units 1 . 1 and 10 . 4 red dark underlines in the left pane as sharing some content with the current model unit 2 . 2 highlighted green dark gray in the right .", "Recall at different compression ratios has been used in summarization research to measure how well an automatic system retains important content of original documents Mani et al . 1998 .", "However , the simple sentence recall measure cannot differentiate system performance appropriately , as is pointed out by Donaway et al . 2000 .", "Therefore , instead of pure sentence recall score , we use coverage score C . We define it as follows7 Total number of MUs in the model summary E , the ratio of completeness , ranges from 1 to 0 1 for all , 3 4 for most , 1 2 for some , 1 4 for hardly any , and 0 for none .", "If we ignore E set it to 1 , we obtain simple sentence recall score .", "We use average coverage scores derived from human judgments as the references to evaluate various automatic scoring methods in the following sections .", "To automatically evaluate machine translations the machine translation community recently adopted an n gram co occurrence scoring procedure BLEU Papineni et al . 2001 .", "The NIST NIST 2002 scoring metric is based on BLEU .", "The main idea of BLEU is to measure the translation closeness between a candidate translation and a set of reference translations with a numerical metric .", "To achieve this goal , they used a weighted average of variable length n gram matches between system translations and a set of human reference translations and showed that a weighted average metric , i . e .", "BLEU , correlating highly with human assessments .", "Similarly , following the BLEU idea , we assume that the closer an automatic summary to a professional human summary , the better it is .", "The question is quot ; Can we apply BLEU directly without any modifications to evaluate summaries as well ? quot ; .", "We first ran IBM's BLEU evaluation script unmodified over the DUC 2001 model and peer summary set .", "The resulting Spearman rank order correlation coefficient \u03c1 between BLEU and the human assessment for the single document task is 0 . 66 using one reference summary and 0 . 82 using three reference summaries ; while Spearman \u03c1 for the multidocument task is 0 . 67 using one reference and 0 . 70 using three .", "These numbers indicate that they positively correlate at \u03b1 0 . 018 .", "Therefore , BLEU seems a promising automatic scoring metric for summary evaluation .", "According to Papineni et al . 2001 , BLEU is essentially a precision metric .", "It measures how well a machine translation overlaps with multiple human translations using n gram co occurrence statistics .", "N gram precision in BLEU is computed as follows Where Countclip n gram is the maximum number of ngrams co occurring in a candidate translation and a reference translation , and Count n gram is the number of n grams in the candidate translation .", "To prevent very short translations that try to maximize their precision scores , BLEU adds a brevity penalty , BP , to the formula Where c is the length of the candidate translation and r is the length of the reference translation .", "The BLEU formula is then written as follows N is set at 4 and wn , the weighting factor , is set at 1 N .", "For summaries by analogy , we can express equation 1 in terms of n gram matches following equation 2 Where Countmatch n gram is the maximum number of n grams co occurring in a peer summary and a model unit and Count n gram is the number of n grams in the model unit .", "Notice that the average n gram coverage score , Cn , as shown in equation 5 is a recall metric 8 The number of instances is 14 11 systems , 2 humans , and 1 baseline for the single document task and is 16 12 systems , 2 humans , and 2 baselines for the multi document task . ings versus human ranking for the multidocument task data from DUC 2001 .", "The same system is at each vertical line with ranking given by different Ngram 1 , 4 n scores .", "The straight line AvgC is the human ranking and n marks summaries of different sizes .", "Ngram 1 , 4 all combines results from all sizes . instead of a precision one as pn .", "Since the denominator of equation 5 is the total sum of the number of n grams occurring at the model summary side instead of the peer side and only one model summary is used for each evaluation ; while there could be multiple references used in BLEU and Count lip n gram could come from matching different reference translations .", "Furthermore , instead of a brevity penalty that punishes overly short translations , a brevity bonus , BB , should be awarded to shorter summaries that contain equivalent content .", "In fact , a length adjusted average coverage score was used as an alternative performance metric in DUC 2002 .", "However , we set the brevity bonus or penalty to 1 for all our experiments in this paper .", "In summary , the ngram co occurrence statistics we use in the following sections are based on the following formula Where j i , i and j range from 1 to 4 , and wn is 1 ji 1 .", "Ngram 1 , 4 is a weighted variable length n gram match score similar to the IBM BLEU score ; while Ngram k , k , i . e . i j k , is simply the average k gram coverage score Ck .", "With these formulas , we describe how to evaluate them in the next section .", "In order to evaluate the effectiveness of automatic evaluation metrics , we propose two criteria cients of different DUC 2001 data between Ngram 1 , 4 n rankings and human rankings including S and excluding SX stopwords .", "SD 100 is for single document summaries of 100 words and MD 50 , 100 , 200 , and 400 are for multi document summaries of 50 , 100 , 200 , and 400 words .", "MD All averages results from summaries of all sizes . should be a good predictor of the statistical significance of human assessments with high reliability .", "The first criterion ensures whenever a human recognizes a good summary translation system , an automatic evaluation will do the same with high probability .", "This enables us to use an automatic evaluation procedure in place of human assessments to compare system performance , as in the NIST MT evaluations NIST 2002 .", "The second criterion is critical in interpreting the significance of automatic evaluation results .", "For example , if an automatic evaluation shows there is a significant difference between run A and run B at \u03b1 0 . 05 using the z test t test or bootstrap resampling , how does this translate to quot ; real quot ; significance , i . e . the statistical significance in a human assessment of run A and run B ?", "Ideally , we would like there to be a positive correlation between them .", "If this can be asserted with strong reliability high recall and precision , then we can use the automatic evaluation to assist system development and to be reasonably sure that we have made progress .", "As stated in Section 3 , direct application of BLEU on the DUC 2001 data showed promising results .", "However , BLEU is a precision based metric while the human evaluation protocol in DUC is essentially recall based .", "We therefore prefer the metric given by equation 6 and use it in all our experiments .", "Using DUC 2001 data , we compute average Ngram 1 , 4 scores for each peer system at different summary sizes and rank systems according to their scores .", "We then compare the Ngram 1 , 4 ranking with the human ranking .", "Figure 2 shows the result of DUC 2001 multi document data .", "Stopwords are ignored during the computation of Ngram 1 , 4 scores and words are stemmed using a Porter stemmer Porter 1980 .", "The x axis is the human ranking and the y axis gives the corresponding Ngram 1 , 4 rankings for summaries of difference sizes .", "The straight line marked by AvgC is the ranking given by human assessment .", "For example , a system at 5 , 8 means that human ranks its performance at the 5th rank while Ngram 1 , 4 400 ranks it at the 8th .", "If an automatic ranking fully matches the human ranking , its plot will coincide with the heavy diagonal .", "A line with less deviation from the heavy diagonal line indicates better correlation with the human assessment .", "To quantify the correlation , we compute the Spearman rank order correlation coefficient p for each Ngram 1 , 4 n run at different summary sizes n .", "We also test the effect of inclusion or exclusion of stopwords .", "The results are summarized in Table 1 .", "Although these results are statistically significant \u03b1 _ 0 . 025 and are comparable to IBM BLEU's correlation figures shown in Section 3 , they are not consistent across summary sizes and tasks .", "For example , the correlations of the single document task are at the 60 level ; while they range from 50 to 80 for the multidocument task .", "The inclusion or exclusion of stopwords also shows mixed results .", "In order to meet the requirement of the first criterion stated in Section 3 , we need better results .", "The Ngram 1 , 4 n score is a weighted average of variable length n gram matches .", "By taking a log sum of the ngram matches , the Ngram 1 , 4 n favors match of longer n grams .", "For example , if quot ; United States of America quot ; occurs in a reference summary , while one peer summary , A , uses quot ; United States quot ; and another summary , B , uses the full phrase quot ; United States of America quot ; , summary B gets more contribution to its overall score simply due to the longer version of the name .", "However , intuitively one should prefer a short version of the name in summarization .", "Therefore , we need to change the weighting scheme to not penalize or even reward shorter equivalents .", "We conduct experiments to understand the effect of individual n gram co occurrence scores in approximating human assessments .", "Tables 2 and 3 show the results of these runs without and with stopwords respectively .", "For each set of DUC 2001 data , single document 100word summarization task , multi document 50 , 100 , 200 , and 400 word summarization tasks , we compute 4 different correlation statistics Spearman rank order correlation coefficient Spearman p , linear regression t test LRt , 11 degree of freedom for single document task and 13 degree of freedom for multi document task , Pearson product moment coefficient of correlation Pearson p , and coefficient of determination CD for each Ngram i , evaluation metric .", "Among them Spearman p is a nonparametric test , a higher number indicates higher correlation ; while the other three tests are parametric tests .", "Higher LRt , Pearson p , and CD also suggests higher linear correlation .", "Analyzing all runs according to Tables 2 and 3 , we make the following observations outperform 0 . 99 Spearman p 0 . 75 the weighted average of n gram of variable length Ngram 1 , 4 0 . 88 Spearman p 0 . 55 in single and multiple document tasks when stopwords are ignored .", "Importantly , unigram performs especially well with Spearman p ranging from 0 . 88 to 0 . 99 that is better than the best case in which weighted average of variable length n gram matches is used and is consistent across different data sets .", "2 The performance of weighted average n gram scores is in the range between bi gram and tri gram co occurrence scores .", "This might suggest some summaries are over penalized by the weighted average metric due to the lack of longer n gram matches .", "For example , given a model string quot ; United States , Japan , and Taiwan quot ; , a candidate string quot ; United States , Taiwan , and Japan quot ; has a unigram score of 1 , bi gram score of 0 . 5 , and trigram and 4 gram scores of 0 when the stopword quot ; and' is ignored .", "The weighted average n gram score for the candidate string is 0 .", "3 Excluding stopwords in computing n gram cooccurrence statistics generally achieves better correlation than including stopwords .", "We have shown that simple unigram , Ngram 1 , 1 , or bigram , Ngram 2 , 2 , co occurrence statistics based on equation 6 outperform the weighted average of n gram matches , Ngram 1 , 4 , in the previous section .", "To examine how well the statistical significance in the automatic Ngram i , metrics translates to real significance when human assessments are involved , we set up the following test procedures A good automatic metric should have high recall and precision .", "This implies that if a statistical test indicates a significant difference between two runs using the automatic metric then very probably there is also a significant difference in the manual evaluation .", "This would be very useful during the system development cycle to gauge if an improvement is really significant or not .", "Figure 3 shows the recall and precision curves for the DUC 2001 single document task at different \u03b1 levels and Figure 4 is for the multi document task with differFigure 3 .", "Recall and precision curves of Ngram co occurrence statistics versus human assessment for DUC 2001 single document task .", "The 5 points on each curve represent values for the 5 levels .", "Figure 4 .", "Recall and precision curves of N gram co occurrence statistics versus human assessment for DUC 2001 multi document task .", "Dark black solid lines are for average of all summary sizes , light red solid lines are for 50 word summaries , dashed green lines are for 100 word summaries , dash dot lines blue are for 200 word summaries , and dotted magenta lines are for 400 word summaries . ent summary sizes .", "Both of them exclude stopwords .", "We use z test in all the significance tests with level at 0 . 10 , 0 . 05 , 0 . 25 , 0 . 01 , and 0 . 005 .", "From Figures 3 and 4 , we can see Ngram 1 , 1 and Ngram 2 , 2 reside on the upper right corner of the recall and precision graphs .", "Ngram 1 , 1 has the best overall behavior .", "These graphs confirm Ngram 1 , 1 simple unigram is a good automatic scoring metric with good statistical significance prediction power .", "In this paper , we gave a brief introduction of the manual summary evaluation protocol used in the Document Understanding Conference .", "We then discussed the IBM BLEU MT evaluation metric , its application to summary evaluation , and the difference between precisionbased BLEU translation evaluation and recall based DUC summary evaluation .", "The discrepancy led us to examine the effectiveness of individual n gram cooccurrence statistics as a substitute for expensive and error prone manual evaluation of summaries .", "To evaluate the performance of automatic scoring metrics , we proposed two test criteria .", "One was to make sure system rankings produced by automatic scoring metrics were similar to human rankings .", "This was quantified by Spearman's rank order correlation coefficient and three other parametric correlation coefficients .", "Another was to compare the statistical significance test results between automatic scoring metrics and human assessments .", "We used recall and precision of the agreement between the test statistics results to identify good automatic scoring metrics .", "According to our experiments , we found that unigram co occurrence statistics is a good automatic scoring metric .", "It consistently correlated highly with human assessments and had high recall and precision in significance test with manual evaluation results .", "In contrast , the weighted average of variable length n gram matches derived from IBM BLEU did not always give good correlation and high recall and precision .", "We surmise that a reason for the difference between summarization and machine translation might be that extraction based summaries do not really suffer from grammar problems , while translations do .", "Longer n grams tend to score for grammaticality rather than content .", "It is encouraging to know that the simple unigram cooccurrence metric works in the DUC 2001 setup .", "The reason for this might be that most of the systems participating in DUC generate summaries by sentence extraction .", "We plan to run similar experiments on DUC 2002 data to see if unigram does as well .", "If it does , we will make available our code available via a website to the summarization community .", "Although this study shows that unigram co occurrence statistics exhibit some good properties in summary evaluation , it still does not correlate to human assessment 100 of the time .", "There is more to be desired in the recall and precision of significance test agreement with manual evaluation .", "We are starting to explore various metrics suggested in Donaway et al . 2000 .", "For example , weight n gram matches differently according to their information content measured by tf , tfidf , or SVD .", "In fact , NIST MT automatic scoring metric NIST 2002 already integrates such modifications .", "One future direction includes using an automatic question answer test as demonstrated in the pilot study in SUMMAC Mani et al . 1998 .", "In that study , an automatic scoring script developed by Chris Buckley showed high correlation with human evaluations , although the experiment was only tested on a small set of 3 topics .", "According to Over 2003 , NIST spent about 3 , 000 man hours each in DUC 2001 and 2002 for topic and document selection , summary creation , and manual evaluation .", "Therefore , it would be wise to use these valuable resources , i . e . manual summaries and evaluation results , not only in the formal evaluation every year but also in developing systems and designing automatic evaluation metrics .", "We would like to propose an annual automatic evaluation track in DUC that encourages participants to invent new automated evaluation metrics .", "Each year the human evaluation results can be used to evaluate the effectiveness of the various automatic evaluation metrics .", "The best automatic metric will be posted at the DUC website and used as an alternative in house and repeatable evaluation mechanism during the next year .", "In this way the evaluation technologies can advance at the same pace as the summarization technologies improve ."], "summary_lines": ["Automatic Evaluation Of Summaries Using N-Gram Co-Occurrence Statistics\n", "Following the recent adoption by the machine translation community of automatic evaluation using the BLEU/NIST scoring process, we conduct an in-depth study of a similar idea for evaluating summaries.\n", "The results show that automatic evaluation using unigram co-occurrences between summary pairs correlates surprising well with human evaluations, based on various statistical metrics; while direct application of the BLEU evaluation procedure does not always give good results.\n", "We are the first to systematically point out problems with the large scale DUC evaluation and to look to solutions by seeking more robust automatic alternatives.\n", "We propose ROUGE, a semi automatic approach,  which is primarily based on n gram co-occurrence between automatic and human summaries.\n"]}
{"article_lines": ["Prepositional Phrase Attachment Through A Backed Off Model", "Recent work has considered corpus based or statistical approaches to the problem of prepositional attachment ambiguity .", "Typically , ambiguous verb phrases of the form v p np2 through a model which considers values of the four head words v , nl , paper shows that the problem is analogous to n gram language models in speech recognition , and that one of the most common methods for language modeling , the backed off estimate , is applicable .", "Results on Wall Street Journal data of 84 . 5 accuracy are obtained using this method .", "Prepositional phrase attachment is a common cause of structural ambiguity in natural language .", "For example take the following sentence Pierre Vinken , 61 years old , joined the board as a nonexecutive director .", "The PP 'as a nonexecutive director' can either attach to the NP 'the board' or to the VP 'joined' , giving two alternative structures .", "In this case the VP attachment is correct NP attach joined the board as a nonexecutive director VP attach joined the board as a nonexecutive director Work by Ratnaparkhi , Reynar and Roukos RRR94 and Brill and Resnik BR94 has considered corpus based approaches to this problem , using a set of examples to train a model which is then used to make attachment decisions on test data .", "Both papers describe methods which look at the four head words involved in the attachment the VP head , the first NP head , the preposition and the second NP head in this case joined , board , as and director respectively .", "This paper proposes a new statistical method for PP attachment disambiguation based on the four head words .", "The training and test data were supplied by IBM , being identical to that used in RRR94 .", "Examples of verb phrases containing a v np pp sequence had been taken from the Wall Street Journal Treebank MSM93 .", "For each such VP the head verb , first head noun , preposition and second head noun were extracted , along with the attachment decision 1 for noun attachment , 0 for verb .", "For example the verb phrase joined the board as a nonexecutive director would give the quintuple 0 joined board as director The elements of this quintuple will from here on be referred to as the random variables A , V , Ni , P , and N2 .", "In the above verb phrase A 0 , V joined , Ni board , P as , and N2 director .", "The data consisted of training and test files of 20801 and 3097 quintuples respectively .", "In addition , a development set of 4039 quintuples was also supplied .", "This set was used during development of the attachment algorithm , ensuring that there was no implicit training of the method on the test set itself .", "A PP attachment algorithm must take each quadruple V v , Ni nl , P p , N2 n2 in test data and decide whether the attachment variable A . 0 or 1 .", "The accuracy of the algorithm is then the percentage of attachments it gets 'correct' on test data , using the A values taken from the treebank as the reference set .", "The probability of the attachment variable A being 1 or 0 signifying noun or verb attachment respectively is a probability , p , which is conditional on the values of the words in the quadruple .", "In general a probabilistic algorithm will make an estimate , . 75 , of this probability The decision can then be made using the test If this is true the attachment is made to the noun , if not then it is made to the verb .", "When evaluating an algorithm it is useful to have an idea of the lower and upper bounds on its performance .", "Some key results are summarised in the table below .", "All results in this section are on the IBM training and test data , with the exception of the two 'average human' results .", "'Always noun attachment' means attach to the noun regardless of v , n1 , p , n2 .", "'Most likely for each preposition' means use the attachment seen most often in training data for the preposition seen in the test quadruple .", "The human performance results are taken from RRR94 , and are the average performance of 3 treebanking experts on a set of 300 randomly selected test events from the WSJ corpus , first looking at the four head words alone , then using the whole sentence .", "A reasonable lower bound seems to be 72 . 2 as scored by the 'Most likely for each preposition' method .", "An approximate upper bound is 88 . 2 it seems unreasonable to expect an algorithm to perform much better than a human .", "We will use the symbol f to denote the number of times a particular tuple is seen in training data .", "For example f 1 , is , revenue , from , research is the number of times the quadruple is , revenue , from , research is seen with a noun attachment .", "Counts of lower order tuples can also be made for example 1 1 , P from is the number of times P from is seen with noun attachment in training data , f V is , N2 research is the number of times V is , N2 research is seen with either attachment and any value of Ni and P . A maximum likelihood method would use the training data to give the following estimation for the conditional probability Unfortunately sparse data problems make this estimate useless .", "A quadruple may appear in test data which has never been seen in training data . ie . f v , nl , p , n2 0 .", "The above estimate is undefined in this situation , which happens extremely frequently in a large vocabulary domain such as WSJ .", "In this experiment about 95 of those quadruples appearing in test data had not been seen in training data .", "Even if f v , nl , p , n2 0 , it may still be very low , and this may make the above MLE estimate inaccurate .", "Unsmoothed MLE estimates based on low counts are notoriously bad in similar problems such as n gram language modeling GC90 .", "However later in this paper it is shown that estimates based on low counts are surprisingly useful in the PP attachment problem .", "Hindle and Rooth HR93 describe one of the first statistical approaches to the prepositional phrase attachment problem .", "Over 200 , 000 v , nl , p triples were extracted from 13 million words of AP news stories .", "The attachment decisions for these triples were unknown , so an unsupervised training method was used section 5 . 2 describes the algorithm in more detail .", "Two human judges annotated the attachment decision for 880 test examples , and the method performed at 80 accuracy on these cases .", "Note that it is difficult to compare this result to results on Wall Street Journal , as the two corpora may be quite different .", "The Wall Street Journal Treebank MSM93 enabled both RRR94 and BR94 to extract a large amount of supervised training material for the problem .", "Both of these methods consider the second noun , n2 , as well as v , n1 and p , with the hope that this additional information will improve results .", "BR94 use 12 , 000 training and 500 test examples .", "A greedy search is used to learn a sequence of 'transformations' which minimise the error rate on training data .", "A transformation is a rule which makes an attachment decision depending on up to 3 elements of the v , nl , p , n2 quadruple .", "Typical examples would be 'If P of then choose noun attachment' or 'If V buy and P for choose verb attachment' .", "A further experiment incorporated word class information from WordNet into the model , by allowing the transformations to look at classes as well as the words .", "An example would be 'If N2 is in the time semantic class , choose verb attachment' .", "The method gave 80 . 8 accuracy with words only , 81 . 8 with words and semantic classes , and they also report an accuracy of 75 . 8 for the metric of HR93 on this data .", "Transformations using words only score 81 . 9 1 on the IBM data used in this paper .", "'Personal communication from Brill .", "RRR94 use the data described in section 2 . 1 of this paper 20801 training and 3097 test examples from Wall Street Journal .", "They use a maximum entropy model which also considers subsets of the quadruple .", "Each sub tuple predicts noun or verb attachment with a weight indicating its strength of prediction the weights are trained to maximise the likelihood of training data .", "For example P of might have a strong weight for noun attachment , while V buy , P for would have a strong weight for verb attachment .", "RRR94 also allow the model to look at class information , this time the classes were learned automatically froin a corpus .", "Results of 77 . 7 words only and 81 . 6 words and classes are reported .", "Crucially they ignore low count events in training data by imposing a frequency cut off somewhere between 3 and 5 .", "KATZ87 describes backed off n gram word models for speech recognition .", "There the task is to estimate the probability of the next word in a text given the n 1 preceding words .", "The NILE estimate of this probability would be But again the denominator f wi , w2 . . . . wn_1 will frequently be zero , especially for large n . The backed off estimate is a method of combating the sparse data problem .", "It is defined recursively as follows Else backing off continues in the same way .", "The idea here is to use MLE estimates based on lower order n grams if counts are not high enough to make an accurate estimate at the current level .", "The cut off frequencies c1 , c2 . . . . are thresholds determining whether to back off or not at each level counts lower than ci at stage i are deemed to be too low to give an accurate estimate , so in this case backing off continues .", "al , a2 , . . . . are normalisation constants which ensure that conditional probabilities sum to one .", "Note that the estimation of w w1 , w2 . . . . wn_i is analogous to the estimation of P 1I v , nl , p , n2 , and the above method can therefore also be applied to the PP attachment problem .", "For example a simple method for estimation of nl , p , n2 would go from MLE estimates of Ally , n1 , p , n2 to nl , p to j3 1 v , iii to polo to pm .", "However a crucial difference between the two problems is that in the n gram task the words w1 to Tun are sequential , giving a natural order in which backing off takes place from Awnlwi , w2 . . . . tun_i to P wn1w2 , w3 . . . . tun_i to P wnlw3 , w4 . . . . wn_i and so on .", "There is no such sequence in the PP attachment problem , and because of this there are four possible triples when backing off from quadruples v , nl , p , v , p , n2 , nl , p , n2 and v , nl , n2 and six possible pairs when backing off from triples v , p , nl , p , p , n2 , v , n1 , v , n2 and nl , n2 .", "A key observation in choosing between these tuples is that the preposition is particularly important to the attachment decision .", "For this reason only tuples which contained the preposition were used in backed off estimates this reduces the problem to a choice between 3 triples and 3 pairs at each respective stage .", "Section 6 . 2 describes experiments which show that tuples containing the preposition are much better indicators of attachment .", "The following method of combining the counts was found to work best in practice Note that this method effectively gives more weight to tuples with high overall counts .", "Another obvious method of combination , a simple average2 , gives equal weight to the three tuples regardless of their total counts and does not perform as well .", "The cut off frequencies must then be chosen .", "A surprising difference from language modeling is that a cut off frequency of 0 is found to be optimum at all stages .", "This effectively means however low a count is , still use it rather than backing off a level .", "2eg .", "A simple average for triples would be defined as", "The figure below shows the results for the method on the 3097 test sentences , also giving the total count and accuracy at each of the backed off stages .", "In an effort to reduce sparse data problems the following processing was run over both test and training data These modifications are similar to those performed on the corpus used by BR94 .", "The result using this modified corpus was 84 . 5 , an improvement of 0 . 4 on the previous result .", "Results from RRR94 , BR94 and the backed off method are Shown in the table below4 .", "All results are for the IBM data .", "These figures should be taken in the context of the lower and upper bounds of 72 . 2 88 . 2 proposed in section 2 . 3 .", "On the surface the method described in 11R93 looks very similar to the backed off estimate .", "For this reason the two methods deserve closer comparison .", "Hindle and Rooth used a partial parser to extract head nouns from a corpus , together with a preceding verb and a following preposition , giving a table of v , n1 , p triples .", "An iterative , unsupervised method was then used to decide between noun and verb attachment for each triple .", "The decision was made as follows' If we ignore n2 then the IBM data is equivalent to Hindle and Rooth's v , n1 , p triples . with the advantage of the attachment decision being known , allowing a supervised algorithm .", "The test used in HR93 can then be stated as follows in our notation This is effectively a comparison of the maximum likelihood estimates of Ill and , a different measure from the backed off estimate which gives 73 iiv , p , n1 .", "The backed off method based on just the f v , p and Anil , p counts would be If P 11v , n1 , p 0 . 5 then choose noun attachment , else choose verb attachment , where f 1 , v , f 1 , n1 , p fi liv , n1 , P f v , p f nl , p An experiment was implemented to investigate the difference in performance between these two methods .", "The test set was restricted to those cases where f 1 , n1 0 , f 0 , v 0 , and Hindle and Rooth's method gave a definite decision .", "ie . the above inequality is strictly less than or greater than .", "This gave 1924 test cases .", "Hindle and Rooth's method scored 82 . 1 accuracy 1580 correct on this set , whereas the backed off measure scored 86 . 5 1665 correct .", "A possible criticism of the backed off estimate is that it uses low count events without any smoothing , which has been shown to be a mistake in similar problems such as n gram language models .", "In particular , quadruples and triples seen in test data will frequently be seen only once or twice in training data .", "An experiment was made with all counts less than 5 being put to zero , 6 effectively making the algorithm ignore low count events .", "In RRR94 a cut off 'between 3 and 5' is used for all events .", "The training and test data were both the unprocessed , original data sets .", "The results were as follows We have excluded tuples which do not contain a preposition from the model .", "This section gives results which justify this .", "The table below gives accuracies for the sub tuples at each stage of backing off .", "The accuracy figure for a particular tuple is obtained by modifying the algorithm in section 4 . 1 to use only information from that tuple at the appropriate stage .", "For example for v , nl , n2 , stage 2 would be modified to read If f v , nl , n2 0 , All other stages in the algorithm would be unchanged .", "The accuracy figure is then the percentage accuracy on the test cases where the v , nl , n2 counts were used .", "The development set with no morphological processing was used for these tests .", "At each stage there is a sharp difference in accuracy between tuples with and without a preposition .", "Moreover , if the 14 tuples in the above table were ranked by accuracy , the top 7 tuples would be the 7 tuples which contain a preposition .", "The backed off estimate scores appreciably better than other methods which have been tested on the Wall Street Journal corpus .", "The accuracy of 84 . 5 is close to the human performance figure of 88 using the 4 head words alone .", "A particularly surprising result is the significance of low count events in training data .", "The algorithm has the additional advantages of being conceptually simple , and computationally inexpensive to implement .", "There are a few possible improvements which may raise performance further .", "Firstly , while we have shown the importance of low count events , some kind of smoothing may improve performance further this needs to be investigated .", "Word classes of semantically similar words may be used to help the sparse data problem both RRR94 and BR94 report significant improvements through the use of word classes .", "Finally , more training data is almost certain to improve results ."], "summary_lines": ["Prepositional Phrase Attachment Through A Backed-Off Model\n", "Recent work has considered corpus-based or statistical approaches to the problem of prepositional phrase attachment ambiguity.\n", "Typically, ambiguous verb phrases of the form v rip1 p rip2 are resolved through a model which considers values of the four head words (v, nl, p and 77,2).\n", "This paper shows that the problem is analogous to n-gram language models in speech recognition, and that one of the most common methods for language modeling, the backed-off estimate, is applicable.\n", "Results on Wall Street Journal data of 84.5% accuracy are obtained using this method.\n", "We use a Back-Off model, which enables them to take low frequency effects into account on the Ratnaparkhi dataset (with good results).\n", "We introduce modifications to the Ratnaparkhi et al (1994) dataset meant to combat data sparsity and used the modified version to train their backed-off model.\n"]}
{"article_lines": ["A New Statistical Parser Based On Bigram Lexical Dependencies", "This paper describes a new statistical parser which is based on probabilities of dependencies between head words in the parse tree .", "Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words .", "Tests using Wall Street Journal data show that the method performs at least as well as SPATTER Magerman 95 ; Jelinek et al . 94 , which has the best published results for a statistical parser on this task .", "The simplicity of the approach means the model trains on 40 , 000 sentences in under 15 minutes .", "With a beam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss in accuracy .", "Lexical information has been shown to be crucial for many parsing decisions , such as prepositional phrase attachment for example Hindle and Rooth 93 .", "However , early approaches to probabilistic parsing Pereira and Schabes 92 ; Magerman and Marcus 91 ; Briscoe and Carroll 93 conditioned probabilities on non terminal labels and part of speech tags alone .", "The SPATTER parser Magerman 95 ; Jelinek et al . 94 does use lexical information , and recovers labeled constituents in Wall Street Journal text with above 84 accuracy as far as we know the best published results on this task .", "This paper describes a new parser which is much simpler than SPATTER , yet performs at least as well when trained and tested on the same Wall Street Journal data .", "The method uses lexical information directly by modeling head modifier' relations between pairs of words .", "In this way it is similar to 'By 'modifier' we mean the linguistic notion of either an argument or adjunct .", "Link grammars Lafferty et al . 92 , and dependency grammars in general .", "The aim of a parser is to take a tagged sentence as input for example Figure 1 a and produce a phrase structure tree as output Figure 1 b .", "A statistical approach to this problem consists of two components .", "First , the statistical model assigns a probability to every candidate parse tree for a sentence .", "Formally , given a sentence S and a tree T , the model estimates the conditional probability P TIS .", "The most likely parse under the model is then Second , the parser is a method for finding Zest .", "This section describes the statistical model , while section 3 describes the parser .", "The key to the statistical model is that any tree such as Figure 1 b can be represented as a set of baseNPs2 and a set of dependencies as in Figure 1 c .", "We call the set of baseNPs B , and the set of dependencies D ; Figure 1 d shows B and D for this example .", "For the purposes of our model , S is the sentence with words tagged for part of speech .", "That is , S to' , ti , w2 , t2 . . . w7 4 , 4 , .", "For POS tagging we use a maximum entropy tagger described in Ratnaparkhi 96 .", "The tagger performs at around 97 accuracy on Wall Street Journal Text , and is trained on the first 40 , 000 sentences of the Penn Treebank Marcus et al . 93 .", "Given S and B , the reduced sentence S' is defined as the subsequence of S which is formed by removing punctuation and reducing all baseNPs to their head word alone .", "heads of baseNPs are marked in bold .", "Arrows show modifier head dependencies .", "Section 2 . 1 describes how arrows are labeled with non terminal triples from the parse tree .", "Non head words within baseNPs are excluded from the dependency structure ; d B , the set of baseNPs , and D , the set of dependencies , are extracted from c .", "Thus the reduced sentence is an array of word tag Sections 2 . 1 to 2 . 4 describe the dependency model .", "Section 2 . 5 then describes the baseNP model , which uses bigram tagging techniques similar to Ramshaw and Marcus 95 ; Church 88 .", "The dependency model is limited to relationships between words in reduced sentences such as Example 1 .", "The mapping from trees to dependency structures is central to the dependency model .", "It is defined in two steps 1 .", "For each constituent P in the parse tree a simple set of rules' identifies which of the children Ci is the 'head child' of P . For example , NN would be identified as the head child of NP DET . 13 33 NN , VP would be identified as the head child of S NP VP .", "Head words 2 .", "Head modifier relationships are now extracted from the tree in Figure 2 .", "Figure 3 illustrates how each constituent contributes a set of dependency relationships .", "VBD is identified as the head child of VP VBD NP NP .", "The head words of the two NPs , resignation and yesterday , both modify the head word of the VBD , announced .", "Dependencies are labeled by the modifier non terminal , NP in both of these cases , the parent non terminal , VP , and finally the head child non terminal , VBD .", "The triple of nonterminals at the start , middle and end of the arrow specify the nature of the dependency relationship NP , S , VP represents a subject verb dependency , PP , NP , NP denotes prepositional phrase modification of an NP , and so on4 .", "Each word in the reduced sentence , with the exception of the sentential head 'announced' , modifies exactly one other word .", "We use the notation to state that the jth word in the reduced sentence is a modifier to the hith word , with relationship R35 .", "AF stands for 'arrow from' .", "Ri is the triple of labels at the start , middle and end of the arrow .", "For example , ti1 Smith in this sentence , 4The triple can also be viewed as representing a semantic predicate argument relationship , with the three elements being the type of the argument , result and functor respectively .", "This is particularly apparent in Categorial Grammar formalisms Wood 93 , which make an explicit link between dependencies and functional application .", "5For the head word of the entire sentence h , 0 , with R3 Label of the root of the parse tree .", "So in this case , AF 5 0 , S . and 1155 announced , so AF 1 5 , NP , S , VP .", "D is now defined as the m tuple of dependencies D AF 1 , AF 2 . . . AF m .", "The model assumes that the dependencies are independent , so that This section describes the way P AF AS , B is estimated .", "The same sentence is very unlikely to appear both in training and test data , so we need to back off from the entire sentence context .", "We believe that lexical information is crucial to attachment decisions , so it is natural to condition on the words and tags .", "Let V be the vocabulary of all words seen in training data , T be the set of all part of speech tags , and TRAIN be the training set , a set of reduced sentences .", "We define the following functions where h z is an indicator function which is 1 if x is true , 0 if x is false . where P is the set of all triples of non terminals .", "The denominator is a normalising factor which ensures that The denominator of 9 is constant , so maximising P DIS , B over D for fixed S , B is equivalent to maximising the product of the numerators , H DIS , B .", "This considerably simplifies the parsing process An estimate based on the identities of the two tokens alone is problematic .", "Additional context , in particular the relative order of the two words and the distance between them , will also strongly influence the likelihood of one word modifying the other .", "For example consider the relationship between 'sales' and the three tokens of 'of' Example 2 Shaw , based in Dalton , Ga . , has annual sales of about 1 . 18 billion , and has economies of scale and lower raw material costs that are expected to boost the profitability of Armstrong 's brands , sold under the Armstrong and Evans Black names .", "In this sentence 'sales' and 'of' co occur three times .", "The parse tree in training data indicates a relationship in only one of these cases , so this sentence would contribute an estimate of that the two words are related .", "This seems unreasonably low given that 'sales of' is a strong collocation .", "The latter two instances of 'of' are so distant from 'sales' that it is unlikely that there will be a dependency .", "This suggests that distance is a crucial variable when deciding whether two words are related .", "It is included in the model by defining an extra 'distance' variable , A , and extending C , F and P to include this variable .", "For example , C a , b , c , d , 6 . is the number of times a , 6 and c , d appear in the same sentence at a distance A apart .", "11 is then maximised instead of 10 A simple example of Aj , hi would be Aj , hi hj j .", "However , other features of a sentence , such as punctuation , are also useful when deciding if two words are related .", "We have developed a heuristic 'distance' measure which takes several such features into account The current distance measure Ai , h , is the combination of 6 features , or questions we motivate the choice of these questions qualitatively section 4 gives quantitative results showing their merit Question 1 Does the kith word precede or follow the jth word ?", "English is a language with strong word order , so the order of the two words in surface text will clearly affect their dependency statistics .", "Question 2 Are the hjth word and the jth word adjacent ?", "English is largely right branching and head initial , which leads to a large proportion of dependencies being between adjacent words 7 .", "Table 1 shows just how local most dependencies are .", "Question 3 Is there a verb between the kith word and the jth word ?", "Conditioning on the exact distance between two words by making Aj , h , hj j leads to severe sparse data problems .", "But Table 1 shows the need to make finer distance distinctions than just whether two words are adjacent .", "Consider the prepositions 'to' , 'in' and 'of' in the following sentence Example 3 Oil stocks escaped the brunt of Friday 's selling and several were able to post gains , including Chevron , which rose 5 8 to 66 3 8 in Big Board composite trading of 2 . 4 million shares .", "The prepositions' main candidates for attachment would appear to be the previous verb , 'rose' , and the baseNP heads between each preposition and this verb .", "They are less likely to modify a more distant verb such as 'escaped' .", "Question 3 allows the parser to prefer modification of the most recent verb effectively another , weaker preference for right branching structures .", "Table 2 shows that 94 of dependencies do not cross a verb , giving empirical evidence that question 3 is useful .", "Questions 4 , 5 and 6 People find that punctuation is extremely useful for identifying phrase structure , and the parser described here also relies on it heavily .", "Commas are not considered to be words or modifiers in the dependency model but they do give strong indications about the parse structure .", "Questions 4 , 5 and 6 allow the parser to use this information .", "The maximum likelihood estimator in 7 is likely to be plagued by sparse data problems C t77j , ti 7T7h , th , , Ai , h3 may be too low to give a reliable estimate , or worse still it may be zero leaving the estimate undefined .", "Collins 95 describes how a backed off estimation strategy is used for making prepositional phrase attachment decisions .", "The idea is to back off to estimates based on less context .", "In this case , less context means looking at the POS tags rather than the specific words .", "There are four estimates , El E2 E3 and E4 based respectively on 1 both words and both tags ; 2 cui and the two POS tags ; 3 . thh , and the two POS tags ; 4 the two POS tags alone . where V is the set of all words seen in training data the other definitions of C follow similarly .", "Estimates 2 and 3 compete for a given pair of words in test data both estimates may exist and they are equally 'specific' to the test case example .", "Collins 95 suggests the following way of combining them , which favours the estimate appearing more often in training data This gives three estimates E1 , E23 and E4 , a similar situation to trigram language modeling for speech recognition Jelinek 90 , where there are trigram , bigram and unigram estimates .", "Jelinek 90 describes a deleted interpolation method which combines these estimates to give a 'smooth' estimate , and the model uses a variation of this idea Jelinek 90 describes how to find A values in 15 and 16 which maximise the likelihood of held out data .", "We have taken a simpler approach , namely These A values have the desired property of increasing as the denominator of the more 'specific' estimator increases .", "We think that a proper implementation of deleted interpolation is likely to improve results , although basing estimates on co occurrence counts alone has the advantage of reduced training times .", "The overall model would be simpler if we could do without the baseNP model and frame everything in terms of dependencies .", "However the baseNP model is needed for two reasons .", "First , while adjacency between words is a good indicator of whether there is some relationship between them , this indicator is made substantially stronger if baseNPs are reduced to a single word .", "Second , it means that words internal to baseNPs are not included in the co occurrence counts in training data .", "Otherwise , in a phrase like 'The Securities and Exchange Commission closed yesterday' , pre modifying nouns like 'Securities' and 'Exchange' would be included in cooccurrence counts , when in practice there is no way that they can modify words outside their baseNP .", "The baseNP model can be viewed as tagging the gaps between words with S tart , C ontinue , E nd , B etween or N u11 symbols , respectively meaning that the gap is at the start of a BaseNP , continues a BaseNP , is at the end of a BaseNP , is between two adjacent baseNPs , or is between two words which are both not in BaseNPs .", "We call the gap before the ith word Gi a sentence with n words has n 1 gaps .", "For example , John Smith the president of IBM has announced his resignation yesterday John C Smith B the C president E of S IBM E has N announced S his C resignation B yesterday The baseNP model considers the words directly to the left and right of each gap , and whether there is a comma between the two words we write ci 1 if there is a comma , ci 0 otherwise .", "Probability estimates are based on counts of consecutive pairs of words in unreduced training data sentences , where baseNP boundaries define whether gaps fall into the S , C , E , B or N categories .", "The probability of a baseNP sequence in an unreduced sentence S is then The estimation method is analogous to that described in the sparse data section of this paper .", "The method is similar to that described in Ramshaw and Marcus 95 ; Church 88 , where baseNP detection is also framed as a tagging problem .", "The probability of a parse tree T , given a sentence S , is The denominator in Equation 9 is not actually constant for different baseNP sequences , but we make this approximation for the sake of efficiency and simplicity .", "In practice this is a good approximation because most baseNP boundaries are very well defined , so parses which have high enough P BIS to be among the highest scoring parses for a sentence tend to have identical or very similar baseNPs .", "Parses are ranked by the following quantity9 Equations 19 and 11 define i BIS and N DIS , B .", "The parser finds the tree which maximises 20 subject to the hard constraint that dependencies cannot cross .", "91n fact we also model the set of unary productions , U , in the tree , which are of the form P .", "This introduces an additional term , P UP , S , into 20 .", "This section describes two modifications which improve the model's performance . in the chart Z . . X Y . . two of its children X and Y are separated by a comma , then the last word in Y must be directly followed by a comma , or must be the last word in the sentence .", "In training data 96 of commas follow this rule .", "The rule also has the benefit of improving efficiency by reducing the number of constituents in the chart . where T is the set of all tags .", "Hence C a , c is the number of times that the words a and c occur in the same sentence , ignoring their tags .", "The other definitions in 13 are similarly redefined , with POS tags only being used when backing off from lexical information .", "This makes the parser less sensitive to tagging errors .", "Second , for each word wi the tagger can provide the distribution of tag probabilities P ti IS given the previous two words are tagged as in the best overall sequence of tags rather than just the first best tag .", "The score for a parse in equation 20 then has an additional term , nin_i P tilS , the product of probabilities of the tags which it contains .", "Ideally we would like to integrate POS tagging into the parsing model rather than treating it as a separate stage .", "This is an area for future research .", "The parsing algorithm is a simple bottom up chart parser .", "There is no grammar as such , although in practice any dependency with a triple of nonterminals which has not been seen in training data will get zero probability .", "Thus the parser searches through the space of all trees with nonterminal triples seen in training data .", "Probabilities of baseNPs in the chart are calculated using 19 , while probabilities for other constituents are derived from the dependencies and baseNPs that they contain .", "A dynamic programming algorithm is used if two proposed constituents span the same set of words , have the same label , head , and distance from with the punctuation rule described in section 2 . 7 ; 3 is model 2 with POS tags ignored when lexical information is present ; 4 is model 3 with probability distributions from the POS tagger .", "LR LP labeled recall precision .", "CBs is the average number of crossing brackets per sentence .", "0 CBs , 2 CBs are the percentage of sentences with 0 or 2 crossing brackets respectively . join to form a new constituent .", "Each operation gives two new probability terms one for the baseNP gap tag between the two constituents , and the other for the dependency between the head words of the two constituents . the head to the left and right end of the constituent , then the lower probability constituent can be safely discarded .", "Figure 4 shows how constituents in the chart combine in a bottom up manner .", "The parser was trained on sections 02 21 of the Wall Street Journal portion of the Penn Treebank Marcus et al . 93 approximately 40 , 000 sentences , and tested on section 23 2 , 416 sentences .", "For comparison SPATTER Magerman 95 ; Jelinek et al . 94 was also tested on section 23 .", "We use the PARSEVAL measures Black et al . 91 to compare performance number of correct constituents in proposed parse number of constituents in proposed parse number of correct constituents in proposed parse number of constituents in treebank parse of constituents which violate constituent boundaries with a constituent in the treebank parse .", "For a constituent to be 'correct' it must span the same set of words ignoring punctuation , i . e . all tokens tagged as commas , colons or quotes and have the same labell as a constituent in the treebank the model .", "The results are for all sentences of 100 words in section 23 using model 3 .", "For 'no lexical information' all estimates are based on POS tags alone .", "For 'no distance measure' the distance measure is Question 1 alone i . e . whether tb 3 precedes Or follows ti h , parse .", "Four configurations of the parser were tested 1 The basic model ; 2 The basic model with the punctuation rule described in section 2 . 7 ; 3 Model 2 with tags ignored when lexical information is present , as described in 2 . 7 ; and 4 Model 3 also using the full probability distributions for POS tags .", "We should emphasise that test data outside of section 23 was used for all development of the model , avoiding the danger of implicit training on section 23 .", "Table 3 shows the results of the tests .", "Table 4 shows results which indicate how different parts of the system contribute to performance .", "All tests were made on a Sun SPARCServer 1000E , using 100 of a 60Mhz SuperSPARC processor .", "The parser uses around 180 megabytes of memory , and training on 40 , 000 sentences essentially extracting the co occurrence counts from the corpus takes under 15 minutes .", "Loading the hash table of bigram counts into memory takes approximately 8 minutes .", "Two strategies are employed to improve parsing efficiency .", "First , a constant probability threshold is used while building the chart any constituents with lower probability than this threshold are discarded .", "If a parse is found , it must be the highest ranked parse by the model as all constituents discarded have lower probabilities than this parse and could not , therefore , be part of a higher probability parse .", "If no parse is found , the threshold is lowered and parsing is attempted again .", "The process continues until a parse is found .", "Second , a beam search strategy is used .", "For each span of words in the sentence the probability , Ph , of the highest probability constituent is recorded .", "All other constituents spanning the same words must have probability greater than 9 for some constant beam size constituents which fall out of this beam are discarded .", "The method risks introducing search errors , but in practice efficiency can be greatly improved with virtually no loss of accuracy .", "Table 5 shows the trade off between speed and accuracy as the beam is narrowed . as the beam size is varied .", "Model 3 was used for this test on all sentences 100 words in section 23 .", "We have shown that a simple statistical model based on dependencies between words can parse Wall Street Journal news text with high accuracy .", "The method is equally applicable to tree or dependency representations of syntactic structures .", "There are many possibilities for improvement , which is encouraging .", "More sophisticated estimation techniques such as deleted interpolation should be tried .", "Estimates based on relaxing the distance measure could also be used for smoothing at present we only back off on words .", "The distance measure could be extended to capture more context , such as other words or tags in the sentence .", "Finally , the model makes no account of valency .", "I would like to thank Mitch Marcus , Jason Eisner , Dan Melamed and Adwait Ratnaparkhi for many useful discussions , and for comments on earlier versions of this paper .", "I would also like to thank David Magerman for his help with testing SPATTER ."], "summary_lines": ["A New Statistical Parser Based On Bigram Lexical Dependencies\n", "This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree.\n", "Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words.\n", "Tests using Wall Street Journal data show that the method performs at least as well as SPATTER (Magerman 95; Jelinek et al. 94), which has the best published results for a statistical parser on this task.\n", "The simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes.\n", "With a beam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss in accuracy.\n", "We make use of a backed-off smoothing technique to alleviate sparse data problems.\n"]}
{"article_lines": ["Robust Bilingual Word Alignment For Machine Aided Translation", "We have developed a new program called aligning parallel text , text such as the Canadian Hansards that are available in two or more languages .", "The program takes the of 1993 , a robust alternative to sentence based alignment programs , and applies word level constraints usa version of Brown Model 2 Brown et al . , 1993 , modified and extended to deal robustness issues . tested on a subset of Canadian Hansards supplied by Simard Simard et al . , 1992 .", "The combination of word_align plus char_align reduces the variance average square error by a factor of over More importantly , because word_align and char_align were designed to work robustly on texts that are smaller and more noisy than the Hansards , it has been possible to successfully deploy the programs at AT T Language Line Services , a commercial translation service , to help them with difficult terminology .", "Aligning parallel texts has recently received considerable attention Warwick et al . , 1990 ; Brown et al . , 1991a ; Gale and Church , 1991b ; Gale and Church , 1991a ; Kay and Rosenschein , 1993 ; Simard et al . , 1992 ; Church , 1993 ; Kupiec , 1993 ; Matsumoto et al . , 1993 .", "These methods have been used in machine translation Brown et al . , 1990 ; Sadler , 1989 , terminology research and translation aids Isabelle , 1992 ; Ogden and Gonzales , 1993 , bilingual lexicography Klavans and Tzoukermann , 1990 , collocation studies Smadja , 1992 , word sense disambiguation Brown et al . , 1991b ; Gale et al . , 1992 and information retrieval in a multilingual environment Landauer and Littman , 1990 .", "The information retrieval application may be of particular relevance to this audience .", "It would be highly desirable for users to be able to express queries in whatever language they chose and retrieve documents that may or may not have been written in the same language as the query .", "Landauer and Littman used SVD analysis or Latent Semantic Indexing on the Canadian Hansards , parliamentary debates that are published in both English and French , in order to estimate a kind of soft thesaurus .", "They then showed that these estimates could be used to retrieve documents appropriately in the bilingual condition where the query and the document were written in different languages .", "We have been most interested in the terminology application .", "How does Microsoft , or some other software vendor , want quot ; dialog box , quot ; quot ; text box , quot ; and quot ; menu box quot ; to be translated in their manuals ?", "Considerable time is spent on terminology questions , many of which have already been solved by other translators working on similar texts .", "It ought to be possible for a translator to point at an instance of quot ; dialog box quot ; in the English version of the Microsoft Windows manual and see how it was translated in the French version of the same manual .", "Alternatively , the translator can ask for a bilingual concordance as shown in Figure 1 .", "A PCbased terminology reuse tool is being developed to do just exactly this .", "The tool depends crucially on the results of an alignment program to determine which parts of the source text correspond with which parts of the target text .", "In working with the translators at AT T Language Line Services , a commercial translation service , we discovered that we needed to completely redesign our alignment programs in order to deal more effectively with texts supplied by Language Line's customers .", "All too often the texts are not available in electronic form , and may need to be scanned in and processed by an OCR optical character recognition device .", "Even if the texts are available in electronic form , it may not be worth the effort to clean them up by hand .", "Real texts are not like the Hansards ; real texts are much smaller and not nearly as clean as the ideal texts that have been used in previous studies .", "To deal with these robustness issues , Church 1993 developed a character based alignment method called char_align .", "The method was intended as a replacement for sentence based methods e . g . , Brown et al . , 1991a ; Gale and Church , 1991b ; Kay and Rosenschein , 1993 , which are very sensitive to noise .", "This paper describes a new program , called word_align , that starts with an initial quot ; rough quot ; alignment e . g . , the output of char_a ign or a sentence based alignment method , and produces improved alignments by exploiting constraints at the word level .", "The alignment algorithm consists of two steps 1 estimate translation probabilities , and 2 use these probabilities to search for most probable alignment path .", "The two steps are described in the following section .", "The translation probabilities are estimated using a method based on Brown et al . 's Model 2 1993 , which is summarized in the following subsection , 2 . 1 . 1 .", "Then , in subsection 2 . 1 . 2 , we describe modifications that achieve three goals 1 enable word_align to accept input which may not be aligned by sentence e . g . char_al gn's output , 2 reduce the number of parameters that need to be estimated , and 3 prepare the ground for the second step , the search for the best alignment described in section 2 . 2 .", "In the context of their statistical machine translation project Brown et al . , 1990 , Brown et al . estimate Pr f le , the probability that f , a sentence in one language say French , is the translation of e , a sentence in the other language say English .", "Pr f le is computed using the concept of altgnment , denoted by a , which is a set of connections between each French word in f and the corresponding English word in e . A connection , which we will write f e as con specifies that position j in f is connected to position i in e . If a French word in f does not correspond to any English word in e , then it is connected to the special word null position 0 in e .", "Notice that this model is directional , as each French position is connected to exactly one position in the English sentence which might be the null word , and accordingly the number of connections in an alignment is equal to the length of the French sentence .", "However , an English word may be connected to several words in the French sentence , or not connected at all .", "Using alignments , the translation probability for a pair of sentences is expressed as where A is the set of all combinatorially possible alignments for the sentences f and e calligraphic font will be used to denote sets .", "In their paper , Brown et al . present a series of 5 models of Pr f le .", "The first two of these 5 models are summarized here .", "Model 1 Model 1 assumes that Pr f , ale depends primarily on t f le , the probability that an occurrence of the English word e is translated as the French word f . That is , 2 where Cf e , an irrelevant constant , accounts for certain dependencies on sentence lengths , which are not important for our purposes here .", "Except for Cf e' most of the notation is borrowed from Brown et al . .", "The variable , j , is used to refer to a position in a French sentence , and the variable , i , is used to refer to a position in an English sentence .", "The expression , f , is used to refer to the French word in position j of a French sentence , and e , is used to refer to the English word in position i of an English sentence .", "An alignment , a , is a set of pairs j , i , each of which connects a position in a French sentence with a corresponding position in an English sentence .", "The expression , ai , is used to refer to the English position that is connected to the French position j , and the expression , is used to refer to the English word in position ai .", "The variable , m , is used to denote the length of the French sentence and the variable , I , is used to denote the length of the English sentence .", "There are quite a number of constraints that could be used to estimate Pr f , ale .", "Model 1 depends primarily on the translation probabilities , t f e , and does not make use of constraints involving the positions within an alignment .", "These constraints will be exploited in Model 2 .", "Brown et al . estimate t fle on the basis of a training set , a set of English and French sentences that have been aligned at the sentence level .", "Those values of t fle that maximize the probability of the training set are called the maximum likelihood estimates .", "Brown et a . show that the maximum likelihood estimates satisfy It follows from the definition of Model 1 that the probability of a connection satisfies Recall that h refers to the French word in position j of the French sentence f of length m , and that ei refers to the English word in position i of the English sentence e of length I .", "Also , remember that position 0 is reserved for the null word .", "Equations 3 and 4 are used iteratively to estimate t f le .", "That is , we start with an initial guess for f le .", "We then evaluation the right hand side of equation 4 , and compute the probability of the connections in the training set .", "Then we evaluate equation 3 , obtain new estimates for the translation probabilities , and repeat the process , until it converges .", "This iterative process is known as the EM algorithm and has been shown to converge to a stationary point Baum , 1972 ; Dempster et al . , 1977 .", "Moreover , Brown et a . show that Model 1 has a unique maximum , and therefore , in this special case , the EM algorithm is guaranteed to converge to the maximum likelihood solution , and does not depend on the initial guess .", "Model 2 Model 2 improves upon model 1 by making use of the positions within an alignment .", "For instance , it is much more likely that the first word of an English sentence will be connected to a word near the beginning of the corresponding French sentence , than to some word near the end of the French sentence .", "Model 2 enhances Model 1 with the assumption that the probability of a connection , con , depends also on j and i the positions in f and e , as well as on m and 1 the lengths of the two sentences .", "This dependence is expressed through the term a iIj , m , 1 , which denotes the probability of connecting position j in a French sentence of length m with position i in an English sentence of length I .", "Since each French position is connected to exactly one English position , the constraint El c where e C'f is an irrelevant constant .", ", As in Model 1 , equation 3 holds for the maximum likelihood estimates of the translation probabilities .", "The corresponding equation for the maxwhere CO tile and CON . , , denote sets of connections the set CO . Arf , , contains all connections in the training data between f and e , and the set CON . , , contains all connections between some French word and e . The probability of a connection , con' . e , is the sum of the probabilities of all alignments that contain it .", "Notice that equation 3 satisfies the constraint E t fle 1 , for each English word e . where CONV denotes the set of connections in the training data between positions j and i in French and English sentences of lengths in and 1 , respectively .", "Similarly , CO . A7 . 3 denotes the set of connections between position j and some English position , in sentences of these lengths .", "Instead of equation 4 , we obtain the following equation for the probability of a connection Notice that Model 1 is a special case of Model 2 , where a i1j , m , l is held fixed at As before , the EM algorithm is used to compute maximum likelihood estimates for t fle and a i1j , m , 1 using first equation 7 , and then equations 3 and 6 .", "However , in this case , Model 2 does not have a unique maximum , and therefore the results depend on the initial guesses .", "Brown et al . therefore use Model 1 to obtain estimates for t f le which do not depend on the initial guesses .", "These values are then used as the initial guesses of t f e in Model 2 .", "As mentioned in the introduction , we are interested in aligning corpora that are smaller and noisier than the Hansards .", "This implies severe practical constraints on the word alignment algorithm .", "As mentioned earlier , we chose to start with the output of char_align because it is more robust than alternative sentence based methods .", "This choice , of course , requires certain modifications to the model of Brown et al . to accommodate as input an initial rough alignment such as produced by char_align instead of pairs of aligned sentences .", "It is also useful to reduce the number of parameters that we are trying to estimate , because we have much less data and much more noise .", "The paragraphs below describe our modifications which are intended to meet these somewhat different requirements .", "The two major modifications are a replacing the sentence by sentence alignment with a single global alignment for the entire corpus , and b replacing the set of probabilities a ijj , in , ! with a small set of offset probabilities .", "Word_align starts with an initial rough alignment , I , which maps French positions to English positions if the mapping is partial , we use linear extrapolation to make it complete .", "Our goal is to find a global alignment .", "A , which is more accurate than I .", "To achieve this goal , we first use I to determine which connections will be considered for A .", "Let coni , i denote a connection between position j in the French corpus and position i in the English corpus the super scripts in cof e are omitted , as there is no notion of sentences .", "We assume that coni , i is a possible connection only if i falls within a limited window which is centered around 1 j , such that where w is a predetermined parameter specifying the size of the window we typically set w to 20 words .", "Connections that fall outside this window are assumed to have a zero probability .", "This assumption replaces the assumption of Brown et al . that connections which cross boundaries of aligned sentences have a zero probability .", "In this new framework , equation 3 becomes where CONL , and CON .", ", e are taken from the set of possible connections , as defined by 8 .", "Turning to Model 2 , the parameters of the form a i1j , m , l are somewhat more problematic .", "First , since there are no sentence boundaries , there are no direct equivalents for i , j , in and 1 .", "Secondly , there are too many parameters to be estimated , given the limited size of our corpora one parameter for each combination of i , j , in and 1 .", "Fortunately , these parameters are highly redundant .", "For example , it is likely that a i1j , m , 1 will be very close to a i 11j 1 , m , and a i1j , m 1 , 1 1 .", "In order to deal with these concerns , we replace probabilities of the form a iIj , in , 1 with a small set of offset probabilities .", "We use k to denote the offset between i , an English position which corresponds to the French position j , and the English position which the input alignment I connects to j k i I j .", "An offset probability , o k , is the probability of having an offset k for some arbitrary connection .", "According to 8 , k ranges between w and w . Thus , instead of equation 6 , we have where CON is the set of all connections and COArk is the set of all connections with offset k . Instead of equation 7 , we have The last three equations are used in the EM algorithm in an iterative fashion as before to estimate the translation probabilities and the offset probabilities .", "Table 1 and Figure 2 show some values that were estimated in this way .", "The input consisted of a pair of Microsoft Windows manuals in English 125 , 000 words and its equivalent in French 143 , 000 words .", "Table 1 shows four French words and the four most likely translations , sorted by t el f 1 .", "Note that the correct translation s are usually near the front of the list , though there is a tendency for the program to be confused by collocates such as quot ; information about quot ; .", "Figure 2 shows the probability estimates for offsets from the initial alignment I .", "Note that smaller offsets are more likely than larger ones , as we would expect .", "Moreover , the distribution is reasonably close to normal , as indicated by the dotted line , which was generated by a Gaussian with a mean of 0 and standard deviation of 102 .", "We have found it useful to make use of three filters to deal with robustness issues .", "Empirically , we found that both high frequency and low frequency words caused difficulties and therefore connections involving these words are filtered out .", "The thresholds are set to exclude the most frequent function words and punctuations , as well as words with less than 3 occurrences .", "In addition , following a similar filter by Brown et al . , small values of t f le are set to 0 after each iteration of the EM algorithm because these small values often correspond to inappropriate translations .", "Finally , connections to null are ignored .", "Such connections model French words that are often omitted in the English translation .", "However , because of OCR errors and other sources of noise , it was decided that this phenomenon was too difficult to model .", "Some words will not be aligned because of these heuristics .", "It may not be necessary , however , to align all words in order to meet the goal of helping translators and lexicographers with difficult terminology .", "The EM algorithm produces two sets of maximum likelihood probability estimates translation probabilities , 1 1 le , and offset probabilities , o k .", "Brown et al . select their preferred alignment simply by choosing the most probable alignment according to the maximum likelihood probabilities , relative to the given sentence alignment .", "In the terms of our 'In this example , French is used as the source language and English as the target .", "2The center of the estimated distribution seems more fiat than in a normal distribution .", "This might be explained by a higher tendency for local changes of word order within phrases than for order changes among phrases .", "This is merely a hypothesis , though , which requires further testing .", "Unfortunately , this method does not model the dependence between connections for French words that are near one another .", "For example , the fact that the French position j was connected to the English position i will not increase the probability that j 1 will be connected to an English position near i .", "The absence of such dependence can easily confuse the program , mainly in aligning adjacent occurrences of the same word , which are common in technical texts .", "Brown et al . introduce such dependence in their Model 4 .", "We have selected a simpler alternative defined in terms of offset probabilities .", "The first step in finding the most probable alignment is to determine the relevant connectzons for each French position .", "Relevant connections are required to be reasonably likely , that is , their translation probability t f le should exceed some minimal threshold .", "Moreover , they are required to fall within a window between 1 j w and 1 j w in the English corpus , as in the previous step parameter estimation .", "We call a French position relevant if it has at least one relevant connection .", "Each alignment A then consists of exactly one connection for each relevant French position the irrelevant positions are ignored .", "To model the dependency between connections in an alignment , we assume that the offset of a connection is determined relative to the preceding connection in A , instead of relative to the initial alignment , I .", "For this purpose , we define A' j as a linear extrapolation from the preceding connection in A where previ is the last French position before j which is aligned by A and NE and NF are the lengths of the English and French corpora .", "A' j thus predicts the connection of j , knowing the connection of jprey and assuming that the two languages have the same word order .", "Instead of 12 , the most probable alignment maximizes French word zone fermer informations insertion English translations with probabilities box 0 . 58 area 0 . 28 want 0 . 04 In 0 . 02 close 0 . 44 when 0 . 08 Close 0 . 07 selected 0 . 06 information 0 . 66 about 0 . 15 For 0 . 12 see 0 . 04 insertion 0 . 61 point 0 . 23 Edit 0 . 06 To 0 . 05 We approximate the offset probabilities , o k , relative to A' , using the maximum likelihood estimates which were computed relative to I as described in Section 2 . 1 . 2 .", "We use a dynamic programming algorithm to find the most probable alignment .", "This enables us to know the value A 1 when dealing with position j .", "To avoid connections with very low probability due to a large offset we require that i f , lei o i j exceeds a pre specified threshold T3 .", "If the threshold is not exceeded , the connection is dropped from the alignment , and i file o i j for that connection is set to T when computing 14 .", "T can therefore be interpreted as a global setting of the probability that a random position will be connected to the null 'In fact , the threshold on t hle , , which is used to determine the relevant connections described in the previous subsection , is used just as an efficient early application of the threshold T . This early application is possible when t hle , o k , x T , where kmax is the value of k with maximal o k .", "English word' .", "A similar dynamic programming approach was used by Gale and Church for word alignment Gale and Church , 1991a , to handle dependency between connections .", "Word_olign was first evaluated on a representative sample of Canadian Hansards 160 , 000 words in English and French .", "The sample was kindly provided by Simard et al . , along with alignments of sentence boundaries as determined by their panel of 8 judges Simard et al . , 1992 .", "Ten iterations of the EM algorithm were computed to estimate the parameters of the model .", "The window size was set to 20 words in each direction , and the minimal threshold for t fle was set to 0 . 005 .", "We considered connections whose source and target words had frequencies between 3 and 1700 1700 is the highest frequency of a content word in the corpus .", "We thus excluded as many 'As mentioned earlier , we do not estimate directly translation probabilities for the null English word . function words as possible , but no content words .", "In this experiment , we used French as the source language and English as the target language .", "Figure 3 presents the alignment error rate of word_align .", "It is compared with the error rate of word_align's input , i . e . the initial rough alignment which is produced by char_align .", "The errors are sampled at sentence boundaries , and are measured as the relative distance between the output of the alignment program and the quot ; true quot ; alignment , as defined by the human judges5 .", "The histograms present errors in the range of 20 20 , which covers about 95 of the data . 6 .", "It can be seen that word_align decreases the error rate significantly notice the different scales of the vertical axes .", "In 55 of the cases , there is no error in word_align's output distance of 0 , in 73 the distance from the correct alignment is at most 1 , and in 84 the distance is at most 3 .", "A second evaluation of word_align was performed on noisy technical documents , of the type typically available for AT T Language Line Services .", "We used the English and French versions of a manual of monitoring equipment about 65 , 000 words , both scanned by an OCR device .", "We sampled the English vocabulary with frequency between three and 450 occurrences , the same vocabulary that was used for alignment .", "We sampled 100 types from the top fifth by frequency of the vocabulary quintile , 80 types from the second quintile , 60 from the third , 40 from the fourth , and 20 from the bottom quintile .", "We used this stratified sampling because we wanted to make more accurate statements about our error rate by tokens than we would have obtained from random sampling , or even from equal weighting of the quintiles .", "After choosing the 300 types from the vocabulary list , one token for each type was chosen at random from the corpus .", "By hand , the best corresponding position in the French version was chosen , to be compared with word_align's output .", "Table 2 summarizes the results of the second experiment .", "The figures indicate the expected relative frequency of each offset from the correct alignment .", "This relative frequency was computed according to the word frequencies in the stratified sample .", "As shown in the table , for 60 . 5 of the tokens the alignment is accurate , and in 84 the offset from the correct alingment is at most 3 .", "These figures demonstrate the usefulness of word_align for constructing bilingual lexicons , and its impact on 'As explained earlier , word_align produces a partial alignment .", "For the purpose of the evaluation , we used linear interpolation to get alignments for all the positions in the sample .", "6Recall that the window size we used is 20 words in each direction , which means that word_align cannot recover from larger errors in char_align . char align errors in words Figure 3 Word_align reduces the variance average square error by a factor of 5 over char_align alone notice the vertical scales . the quality of bilingual concordances as in Figure 1 .", "Indeed , using bilingual concordances which are based on word_align's output , the translators at AT T Language Line Services are now producing bilingual terminology lexicons at a rate of 60 100 terms per hour !", "This is compared with the previous rate of about 30 terms per hour using char_al gn's output , and an extremely lower rate before alignment tools were available .", "Compared with other word alignment algorithms Brown et al . , 1993 ; Gale and Church , 1991a , word_align does not require sentence alignment as input , and was shown to produce useful alignments for small and noisy corpora .", "Its robustness was achieved by modifying Brown et al . 's Model 2 to handle an initial quot ; rough quot ; alignment , reducing the number of parameters and introducing a dependency between alignments of adjacent words .", "Taking the output of char_align as input , word_align produces significantly better , wordlevel , alignments on the kind of corpora that are typically available to translators .", "This improvement increased the rate of constructing bilingual terminology lexicons at AT T Language Line Services by a factor of 2 3 .", "In addition , the alignments may also be helpful to developers of lexicons for machine translation systems .", "Word_align thus provides an example how a model such as Brown et al . 's Model 2 , that was originally designed for research in statistical machine translation , can be modified to achieve practical , though less ambitious , goals in the near term ."], "summary_lines": ["Robust Bilingual Word Alignment For Machine Aided Translation\n", "We have developed a new program called word_align for aligning parallel text, text such as the Canadian Hansards that are available in two or more languages.\n", "The program takes the output of char_align (Church, 1993), a robust alternative to sentence-based alignment programs, and applies word-level constraints using a version of Brown el al.'s Model 2 (Brown et al., 1993), modified and extended to deal with robustness issues.\n", "Word_align was tested on a subset of Canadian Hansards supplied by Simard (Simard et al., 1992).\n", "The combination of word_align plus char_align reduces the variance (average square error) by a factor of 5 over char_align alone.\n", "More importantly, because word_align and char_align were designed to work robustly on texts that are smaller and more noisy than the Hansards, it has been possible to successfully deploy the programs at AT&T Language Line Services, a commercial translation service, to help them with difficult terminology.\n", "We show that knowledge of target-text length is not crucial to the model's performance.\n"]}
{"article_lines": ["Efficient Normal Form Parsing For Combinatory Categorial Grammar", "Under categorial grammars that have powerful rules like composition , a simple n word sentence can have exponentially many parses .", "Generating all parses is inefficient and obscures whatever true semantic ambiguities are in the input .", "This paper addresses the problem for a fairly general form of Combinatory Categorial Grammar , by means of an efficient , correct , and easy to implement normal form parsing tech The parser is proved to find exone in each semantic equivalence class of allowable parses ; that is , spurious ambiguity as carefully defined is shown to be both safely and completely eliminated .", "Combinatory Categorial Grammar Steedman , 1990 , like other quot ; flexible quot ; categorial grammars , suffers from spurious ambiguity Wittenburg , 1986 .", "The non standard constituents that are so crucial to CCG's analyses in 1 , and in its account of intonational focus Prevost Steedman , 1994 , remain available even in simpler sentences .", "This renders 2 syntactically ambiguous .", "The practical problem of quot ; extra quot ; parses in 2 becomes exponentially worse for longer strings , which can have up to a Catalan number of parses .", "An exhaustive parser serves up 252 CCG parses of 3 , which must be sifted through , at considerable cost , in order to identify the two distinct meanings for further processing . '", "This paper presents a simple and flexible CCG parsing technique that prevents any such explosion of redundant CCG derivations .", "In particular , it is proved in 4 . 2 that the method constructs exactly one syntactic structure per semantic reading e . g . , just two parses for 3 .", "All other parses are suppressed by simple normal form constraints that are enforced throughout the parsing process .", "This approach works because CCG's spurious ambiguities arise as is shown in only a small set of circumstances .", "Although similar work has been attempted in the past , with varying degrees of success Karttunen , 1986 ; Wittenburg , 1986 ; Pareschi Steedman , 1987 ; Bouma , 1989 ; Hepple Morrill , 1989 ; Ki5nig , 1989 ; Vijay Shanker Weir , 1990 ; Hepple , 1990 ; Moortgat , 1990 ; Hendriks , 1993 ; Niv , 1994 , this appears to be the first full normal form result for a categorial formalism having more than contextfree power .", "CCG may be regarded as a generalization of contextfree grammar CFG one where a grammar has infinitely many nonterminals and phrase structure rules .", "In addition to the familiar atomic nonterminal categories typically S for sentences , N for nouns , NP for noun phrases , etc .", ", CCG allows in signs different types to quot ; John likes quot ; and quot ; Mary prefinitely many slashed categories .", "If x and y are tends to like , quot ; thus losing the ability to conjoin such categories , then x y respectively x y is the cat constituents or subcategorize for them as a class . egory of an incomplete x that is missing a y at its Pareschi Steedman , 1987 do tackle the CCG right respectively left .", "Thus verb phrases are an case , but Hepple , 1987 shows their algorithm to alyzed as subjectless sentences S NP , while quot ; John be incomplete . likes quot ; is an objectless sentence or S NP .", "A complex 3 Overview of the Parsing Strategy category like S NP S NP N may be written as As is well known , general CFG parsing methods S NP S NP N , under a convention that slashes are can be applied directly to CCG .", "Any sort of chart left associative . parser or non deterministic shift reduce parser will The results herein apply to the TAG equivalent do .", "Such a parser repeatedly decides whether two CCG formalization given in Joshi et al . , 1991 . 2 adjacent constituents , such as S NP and NP N , should In this variety of CCG , every non lexical phrase be combined into a larger constituent such as S N . structure rule is an instance of one of the following The role of the grammar is to state which combibinary rule templates where n 0 nations are allowed .", "The key to efficiency , we will 4 Forward generalized composition Bn see , is for the parser to be less permissive than the xly Y Inzn 12z2 lizi Inn I2z2 lizi grammar for it to say quot ; no , redundant quot ; in some Backward generalized composition Bn cases where the grammar says quot ; yes , grammatical . quot ; Y Inzn I2z2 lizi Y x Inzn I I 5 shows the constituents that untrammeled , 2z2 , izi CCG will find in the course of parsing quot ; John likes Instances with n 0 are called application rules , and Mary . quot ; The spurious ambiguity problem is not that instances with n 1 are called composition rules .", "In the grammar allows 5c , but that the grammar ala given rule , x , y , z1 . . . zn would be instantiated as lows both 5f and 5g distinct parses of the same categories like NP , S NP , or S NP S NP N .", "Each of string , with the same meaning .", "11 through ln would be instantiated as either or 5 a .", "John si s Np A fixed CCG grammar need not include every b .", "likes swp Np phrase structure rule matching these templates .", "In c . John likes siNp deed , Joshi et al . , 1991 place certain restrictions d . Mary Np on the rule set of a CCG grammar , including a re e . likes Mary s Np quirement that the rule degree n is bounded over the f . John likes Mary s to be disallowed set .", "The results of the present paper apply to such g . John likes Mary s restricted grammars and also more generally , to any The proposal is to construct all constituents CCG style grammar with a decidable rule set . shown in 5 except for 5f .", "If we slightly conEven as restricted by Joshi et al . , 1991 , CCGs strain the use of the grammar rules , the parser will have the quot ; mildly context sensitive quot ; expressive power still produce 5c and 5d constituents that are of Tree Adjoining Grammars TAGs .", "Most work indispensable in contexts like 1 while refusing to on spurious ambiguity has focused on categorial for combine those constituents into 51 .", "The relevant malisms with substantially less power .", "Hepple , rule S NP NP S will actually be blocked when it 1990 and Hendriks , 1993 , the most rigorous pieces attempts to construct 5f .", "Although rule blocking of work , each establish a normal form for the syn may eliminate an analysis of the sentence , as it does tactic calculus of Lambek , 1958 , which is weakly here , a semantically equivalent analysis such as 5g context free .", "Konig , 1989 ; Moortgat , 1990 have will always be derivable along some other route . also studied the Lambek calculus case .", "Hepple In general , our goal is to discover exactly one analMorrill , 1989 , who introduced the idea of normal ysis for each substring , meaning pair .", "By pracform parsing , consider only a small CCG frag ticing quot ; birth control quot ; for each bottom up generation ment that lacks backward or order changing com of constituents in this way , we avoid a population position ; Niv , 1994 extends this result but does explosion of parsing options .", "quot ; John likes Mary quot ; has not show completeness .", "Wittenburg , 1987 assumes only one reading semantically , so just one of its anala CCG fragment lacking order changing or higher yses 5f 5g is discovered while parsing 6 .", "Only order composition ; furthermore , his revision of the that analysis , and not the other , is allowed to concombinators creates new , conjoinable constituents tinue on and be built into the final parse of 6 . that conventional CCG rejects .", "Bouma , 1989 pro 6 that galoot in the corner that thinks John poses to replace composition with a new combina likes MarAs tor , but the resulting product grammar scheme as For a chart parser , where each chart cell stores the analyses of some substring , this strategy says that 2This formalization sweeps any type raising into the 80 lexicon , as has been proposed on linguistic grounds Dowty , 1988 ; Steedman , 1991 , and others .", "It also treats conjunction lexically , by giving quot ; and quot ; the generalized category x x x and barring it from composition . all analyses in a cell are to be semantically distinct .", "Karttunen , 1986 suggests enforcing that property directly by comparing each new analysis semantically with existing analyses in the cell , and refusing to add it if redundant but Hepple Morrill , 1989 observe briefly that this is inefficient for large charts . 3 The following sections show how to obtain effectively the same result without doing any semantic interpretation or comparison at all .", "It is convenient to begin with a special case .", "Suppose the CCG grammar includes not some but all instances of the binary rule templates in 4 .", "As always , a separate lexicon specifies the possible categories of each word .", "If we group a sentence's parses into semantic equivalence classes , it always turns out that exactly one parse in each class satisfies the following simple declarative constraints The notation here is from 4 .", "More colloquially , 7 says that the output of rightward leftward composition may not compose or apply over anything to is right left .", "A parse tree or subtree that satisfies 7 is said to be in normal form NF .", "As an example , consider the effect of these restrictions on the simple sentence quot ; John likes Mary . quot ; Ignoring the tags OT , FC , and BC for the moment , 8a is a normal form parse .", "Its competitor 813 is not , nor is any larger tree containing 8b .", "But non3How inefficient ?", "i has exponentially many semantically distinct parses n 10 yields 82 , 756 , 612 parses in 2 48 , 620 equivalence classes .", "Karttunen's io method must therefore add 48 , 620 representative parses to the appropriate chart cell , first comparing each one against all the previously added parses of which there are 48 , 620 2 on average to ensure it is not semantically redundant .", "Additional comparisons are needed to reject parses other than the lucky 48 , 620 .", "Adding a parse can therefore take exponential time .", "Structure sharing does not appear to help parses that are grouped in a parse forest have only their syntactic category in common , not their meaning .", "Karttunen's approach must tease such parses apart and compare their various meanings individually against each new candidate .", "By contrast , the method proposed below is purely syntactic just like any quot ; ordinary quot ; parser so it never needs to unpack a subforest , and can run in polynomial time . standard constituents are allowed when necessary 8c is in normal form cf .", "It is not hard to see that 7a eliminates all but right branching parses of quot ; forward chains quot ; like A B B C C or A B C C D D E F G G H , and that 7b eliminates all but left branching parses of quot ; backward chains . quot ; Thus every functor will get its arguments , if possible , before it becomes an argument itself .", "But it is hardly obvious that 7 eliminates all of CCG 's spurious ambiguity .", "One might worry about unexpected interactions involving crossing composition rules like A B B C 4 A C . Significantly , it turns out that 7 really does suffice ; the proof is in 4 . 2 .", "It is trivial to modify any sort of CCG parser to find only the normal form parses .", "No semantics is necessary ; simply block any rule use that would violate 7 .", "In general , detecting violations will not hurt performance by more than a constant factor .", "Indeed , one might implement 7 by modifying CCG's phrase structure grammar .", "Each ordinary CCG category is split into three categories that bear the respective tags from 9 .", "The 24 templates schematized in 10 replace the two templates of 4 .", "Any CFG style method can still parse the resulting spuriosity free grammar , with tagged parses as in 8 .", "In particular , the polynomial time , polynomialspace CCG chart parser of Vijay Shanker Weir , 1993 can be trivially adapted to respect the constraints by tagging chart entries .", "It is interesting to note a rough resemblance between the tagged version of CCG in 10 and the tagged Lambek calculus L , which Hendriks , 1993 developed to eliminate spurious ambiguity from the Lambek calculus L . Although differences between CCG and L mean that the details are quite different , each system works by marking the output of certain rules , to prevent such output from serving as input to certain other rules .", "We wish to establish that each semantic equivalence class contains exactly one NF parse .", "But what does quot ; semantically equivalent quot ; mean ?", "Let us adopt a standard model theoretic view .", "For each leaf i . e . , lexeme of a given syntax tree , the lexicon specifies a lexical interpretation from the model .", "CCG then provides a derived interpretation in the model for the complete tree .", "The standard CCG theory builds the semantics compositionally , guided by the syntax , according to 11 .", "We may therefore regard a syntax tree as a static quot ; recipe quot ; for combining word meanings into a phrase meaning .", "One might choose to say that two parses are semantically equivalent if they derive the same phrase meaning .", "However , such a definition would make spurious ambiguity sensitive to the fine grained semantics of the lexicon .", "Are the two analyses of VP VP VP VP VP semantically equivalent ?", "If the lexemes involved are quot ; softly knock twice , quot ; then yes , as softly twice knock and twice softly knock arguably denote a common function in the semantic model .", "Yet for quot ; intentionally knock twice quot ; this is not the case these adverbs do not commute , and the semantics are distinct .", "It would be difficult to make such subtle distinctions rapidly .", "Let us instead use a narrower , quot ; intensional quot ; definition of spurious ambiguity .", "The trees in 12a b will be considered equivalent because they specify the same quot ; recipe , quot ; shown in 12c .", "No matter what lexical interpretations f , g , h , k are fed into the leaves A B , B C D , D E , E F , both the trees end up with the same derived interpretation , namely a model element that can be determined from f , g , h , k by calculating AxAy . f g h k x y .", "By contrast , the two readings of quot ; softly knock twice quot ; are considered to be distinct , since the parses specify different recipes .", "That is , given a suitably Q fit NF T NF a free choice of meanings for the words , the two parses 131 32 132 7 can be made to pick out two different VP type func This construction resembles a well known normaltions in the model .", "The parser is therefore conser form reduction procedure that Hepple 8 . 6 Morrill , vative and keeps both parses . '", "1989 propose without proving completeness for a 4 . 2 Normal form parsing is safe Sz complete small fragment of CCG .", "The motivation for producing only NF parses as The proof of theorem 2 completeness is longer defined by 7 lies in the following existence and and more subtle .", "First it shows , by a simple inducuniqueness theorems for CCG . tion , that since a and a' disagree they must disagree Theorem 1 Assuming quot ; pure CCG , quot ; where all pos in at least one of these ways sible rules are in the grammar , any parse tree a is se a There are trees 3 , y and rules R R' such that mantically equivalent to some NF parse tree NF a .", "R , , 7 is a subtree of a and R' , 0 , 7 is a This says the NF parser is safe for pure CCG we subtree of a' .", "For example , S S S S may form will not lose any readings by generating just normal a constituent by either Blx or Bix . forms .", "b There is a tree 7 that appears as a subtree of Theorem 2 Given distinct NF trees a 0 a' on the both a and a' , but combines to the left in one same sequence of leaves .", "Then a and a' are not case and to the right in the other . semantically equivalent .", "Either condition , the proof shows , leads to different This says that the NF parser is complete generat quot ; immediate scope quot ; relations in the full trees a and a' ing only normal forms eliminates all spurious ambi in the sense in which f takes immediate scope over guity . g in f g x but not in f h g x or g f x .", "ConDetailed proofs of these theorems are available on dition a is straightforward .", "Condition b splits the al T 1g archive , but can only be sketched here . into a case where y serves as a secondary argument Theorem 1 is proved by a constructive induction on inside both a and a' , and a case where it is a primary the order of a , given below and illustrated in 13 argument in a or a' .", "The latter case requires consid For a a leaf , put NF a a . eration of 7's ancestors ; the NF properties crucially R , 0 , 7 denotes the parse tree formed by com rule out counterexamples here . bining subtrees 13 , 7 via rule R . The notion of scope is relevant because semantic If a R , 3 , 7 , then take NF a interpretations for CCG constituents can be written R , NF , NF y , which exists by inductive as restricted lambda terms , in such a way that conhypothesis , unless this is not an NF tree .", "In stituents having distinct terms must have different the latter case , WLOG , R is a forward rule and interpretations in the model for suitable interpretaNF Q , 13i , 32 for some forward com tions of the words , as in 4 . 1 .", "Theorem 2 is proved position rule Q .", "Pure CCG turns out to pro by showing that the terms for a and a' differ somevide forward rules S and T such that a' where , so correspond to different semantic recipes .", "S , , NF T , , 7 is a constituent and Similar theorems for the Lambek calculus were is semantically equivalent to a .", "Moreover , since previously shown by Hepple , 1990 ; Hendriks , 1993 .", "131 serves as the primary subtree of the NF tree The present proofs for CCG establish a result that NF , fi1 . cannot be the output of forward com has long been suspected the spurious ambiguity position , and is NF besides .", "Therefore a' is NF problem is not actually very widespread in CCG . take NF a a' .", "Theorem 2 says all cases of spurious ambiguity Theorem 2 remains true 1 NF per reading . that their NFs have been previously computed .", "Whether theorem 1 1 NF per reading remains Figure 1 gives an efficient CKY style algorithm true depends on what set of rules is removed .", "For based on this insight .", "Parsing strategies besides most linguistically reasonable choices , the proof of CKY would also work , in particular Vijay Shanker theorem 1 will go through , ' so that the normal form Si Weir , 1993 The management of cached NFs in parser of 4 remains safe .", "But imagine removing steps 9 , 12 , and especially 16 ensures that duplicate only the rule B C C B this leaves the string A B NFs never enter the oldNFs array thus any alterB C C with a left branching parse that has no legal native copy of a . nf has the same array coordinates NF equivalent . used for a . nf itself , because it was built from identiIn the sort of restricted grammar where theorem 1 cal subtrees . does not obtain , can we still find one possibly non The function PreferableTo c , r step 15 proNF parse per equivalence class ?", "Yes a different vides flexibility about which parse represents its kind of efficient parser can be built for this case . class .", "PreferableTo may be defined at whim to Since the new parser must be able to generate a choose the parse discovered first , the more leftnon NF parse when no equivalent NF parse is avail branching parse , or the parse with fewer nonable , its method of controlling spurious ambiguity standard constituents .", "Alternatively , PreferableTo cannot be to enforce the constraints 7 .", "The old may call an intonation or discourse module to pick parser refused to build non NF constituents ; the new the parse that better reflects the topic focus diviparser will refuse to build constituents that are se sion of the sentence .", "A variant algorithm ignores mantically equivalent to already built constituents .", "PreferableTo and constructs one parse forest per This idea originates with Karttunen , 1986 . reading .", "Each forest can later be unpacked into inHowever , we can take advantage of the core result dividual equivalent parse trees , if desired . of this paper , theorems 1 and 2 , to do Karttunen's Vijay Shanker Sz Weir , 1990 also give a method redundancy check in 0 1 time no worse than the for removing quot ; one well known source quot ; of spurious normal form parser's check for FC and BC tags . ambiguity from restricted CCGs ; 4 . 2 above shows Karttunen's version takes worst case exponential that this is in fact the only source .", "However , their time for each redundancy check see footnote 3 . method relies on the grammaticality of certain interThe insight is that theorems 1 and 2 estab mediate forms , and so can fail if the CCG rules can lish a one to one map between semantic equivalence be arbitrarily restricted .", "In addition , their method classes and normal forms of the pure unrestricted is less efficient than the present one it considers CCG parses in pairs , not singly , and does not remove any 15 Two parses a , a' of the pure CCG are parse until the entire parse forest has been built . semantically equivalent if they have the 6 Extensions to the CCG Formalism same normal form NF a NF a' .", "In addition to the Bn quot ; generalized composition quot ; The NF function is defined recursively by 4 . 2's rules given in 2 , which give CCG power equivalent proof of theorem 1 ; semantic equivalence is also to TAG , rules based on the S quot ; substitution quot ; and defined independently of the grammar .", "So 15 is T quot ; type raising quot ; combinators can be linguistically meaningful and true even if a , a' are produced by useful .", "S provides another rule template , used in a restricted CCG .", "The tree NF a may not be a the analysis of parasitic gaps Steedman , 1987 ; Szlegal parse under the restricted grammar .", "How abolcsi , 1989 ever , it is still a perfectly good data structure that 16 a .", "s x y liz y liz x liz can be maintained outside the parse chart , to serve 11 f g Az . f z g z b .", "S y liz x Y liz x liz Although S interacts with Bn to produce another source of spurious ambiguity , illustrated in 17 , the additional ambiguity is not hard to remove .", "It can be shown that when the restriction 18 is used together with 7 , the system again finds exactly one 84 'For the proof to work , the rules S and T must be available in the restricted grammar , given that R and Q are .", "This is usually true since 7 favors standard constituents and prefers application to composition , most grammars will not block the NF derivation while allowing a non NF one .", "On the other hand , the NF parse of A B B C C D E uses B2 twice , while the non NF parse gets by with B2 and B1 .", "Type raising presents a greater problem .", "Various new spurious ambiguities arise if it is permitted freely in the grammar .", "In principle one could proceed without grammatical type raising Dowty , 1988 ; Steedman , 1991 have argued on linguistic grounds that type raising should be treated as a mere lexical redundancy property .", "That is , whenever the lexicon contains an entry of a certain category X , with semantics x , it also contains one with say category T T X and interpretation Ap . p x .", "As one might expect , this move only sweeps the problem under the rug .", "If type raising is lexical , then the definitions of this paper do not recognize 19 as a spurious ambiguity , because the two parses are now , technically speaking , analyses of different sentences .", "Nor do they recognize the redundancy in 20 , because just as for the example quot ; softly knock twice quot ; in 4 . 1 it is contingent on a kind of lexical coincidence , namely that a type raised subject commutes with a generically type raised object .", "Such ambiguities are left to future work .", "The main contribution of this work has been formal to establish a normal form for parses of quot ; pure quot ; Cornbinatory Categorial Grammar .", "Given a sentence , every reading that is available to the grammar has exactly one normal form parse , no matter how many parses it has in toto .", "A result worth remembering is that , although TAG equivalent CCG allows free interaction among forward , backward , and crossed composition rules of any degree , two simple constraints serve to eliminate all spurious ambiguity .", "It turns out that all spurious ambiguity arises from associative quot ; chains quot ; such as A B B C C or A B C C D D E F G G H .", "Wit8 5 tenburg , 1987 ; Hepple Morrill , 1989 anticipate this result , at least for some fragments of CCG , but leave the proof to future work .", "These normal form results for pure CCG lead directly to useful parsers for real , restricted CCG grammars .", "Two parsing algorithms have been presented for practical use .", "One algorithm finds only normal forms ; this simply and safely eliminates spurious ambiguity under most real CCG grammars .", "The other , more complex algorithm solves the spurious ambiguity problem for any CCG grammar , by using normal forms as an efficient tool for grouping semantically equivalent parses .", "Both algorithms are safe , complete , and efficient .", "In closing , it should be repeated that the results provided are for the TAG equivalent Bn generalized composition formalism of Joshi et al . , 1991 , optionally extended with the S substitution rules of Szabolcsi , 1989 .", "The technique eliminates all spurious ambiguities resulting from the interaction of these rules .", "Future work should continue by eliminating the spurious ambiguities that arise from grammatical or lexical type raising ."], "summary_lines": ["Efficient Normal-Form Parsing For Combinatory Categorial Grammar\n", "Under categorial grammars that have powerful rules like composition, a simple n-word sentence can have exponentially many parses.\n", "Generating all parses is inefficient and obscures whatever true semantic ambiguities are in the input.\n", "This paper addresses the problem for a fairly general form of Combinatory Categorial Grammar, by means of an efficient, correct, and easy to implement normal-form parsing technique.\n", "The parser is proved to find exactly one parse in each semantic equivalence class of allowable parses; that is, spurious ambiguity (as carefully defined) is shown to be both safely and completely eliminated.\n", "We provide a safe and complete parsing algorithm which can return non-NF derivations when necessary to preserve an interpretation if composition is bounded or the grammar is restricted in other ways.\n"]}
{"article_lines": ["Minimally Supervised Morphological Analysis By Multimodal Alignment", "with genetic algorithms .", "Workshop on Empirical Learning of NLP Tasks .", "K . Koskenniemi , 1983 .", "A general computation model word form recognition and production .", "11 , of General Linguistics . of Helsinki .", "C . X .", "Ling , 1994 .", "Learning the past tense of English verbs The symbolic pattern associator vs . connecmodels .", "Art .", "Intel .", "Res . , R . Mooney and M . Califf , 1995 .", "Induction of firstorder decision lists Results on learning the past of English verbs .", "Art .", "Intel .", "Res . , K . Oflazer and S . Nirenburg , 1999 .", "Practical bootof morphological analyzers . of the Conference on Natural Language Learning .", "D . Rumelhart and J . McClelland , 1986 .", "On learning the past tense of English verbs .", "In J . McClel D . Rumelhart , and the Group , Parallel distributed processing Explorations in the of cognition , 2 .", "MIT Press .", "P . Theron and I . Cloete , 1997 .", "Automatic acquisition two level morphological rules . of the Fifth Conference on Applied Natural Language Propages 103 110 .", "A . Voutilainen , 1995 .", "Morphological disambiguation .", "In F . Karlsson , A . Voutilainen , J . Heikkila , and A .", "eds . grammar A language independent system for parsing unrestricted text , pages 165 284 .", "The Hague Mouton de Gruyter .", "This paper presents an original and successful algorithm for the nearly unsupervised induction of inflectional morphological analyzers , with a focus on highly irregular forms not typically handled by other morphology induction algorithms .", "It is useful to consider this task as three separate steps The target output of Step 1 is an inflection root mapping such as shown in Table 1 , with optional columns giving the hypothesized stem change and suffix analysis as well as part of speech .", "This suffix focused transformational model is not , as given , sufficient for languages with prefixal , infixal and reduplicative morphologies .", "But it is remarkably productive across Indo European languages in its current form and can be extended to other affixational schema when appropriate .", "For many applications , once the vocabulary list achieves sufficiently broad coverage , this alignment table effectively becomes a morphological analyzer simply by table lookup independent of necessary contextual ambiguity resolution .", "While the probabilistic analyzer trained in Step 2 remains useful for previously unseen words , such words are typically quite regular and most of the difficult substance of the lemmatization problem can often be captured by a large root Posinflection mapping table and a simple transducer to handle residual forms .", "This is not the case for agglutinative languages such as Turkish or Finnish , or for very highly inflected languages such as Czech , where sparse data becomes an issue .", "But for many languages , and to a quite practical degree , inflectional morphological analysis and generation can be viewed primarily as an alignment task on a broad coverage wordlist .", "Thus , while this paper will discuss our implementation of a stand alone probabilistic analyzer and retraining process in Steps 2 and 3 , the challenge of large coverage inflection root alignment expressed in Step 1 is the core of this work .", "In further clarification of the task description , the morphological induction described in this paper assumes , and is based on , only the following limited set of often optional available resources a A table such as Table 2 of the inflectional parts of speech of the given language , along with a list of the canonical suffixes for each part of speech .", "These suffixes not only serve as mnemonic tags for the POS labels , but they can also be used to obtain a noisy set of candidate examples for each part of speech . ' the given language is useful to the extraction of context similarity features .", "f If available , the various distance similarity tables generated by this algorithm on previously studied languages can be useful as seed information , especially if these languages are closely related e . g .", "Spanish and Italian .", "There is a rich tradition of supervised and unsupervised learning in the domain of morphology .", "Rumelhart and McClelland 1986 , Egedi and Sproat 1988 , Ling 1994 and Mooney and Calif 1995 have each investigated the supervised learning of the English past tense from paired training data , the first two using phonologicallybased connectionist models and the latter two performing comparative studies with ID3 decision trees and first order decision lists respectively .", "Brent 1993 , 1999 , de Marcken 1995 , Kazakov 1997 and Goldsmith 2000 have each focused on the problem of unsupervised learning of morphological systems as essentially a segmentation task , yielding a morphologically plausible and statistically motivated partition of stems and affixes .", "Brent and de Marcken both have used a minimum description length framework , with the primary goal of inducing lexemes from boundaryless speech like streams .", "Goldsmith specifically sought to induce suffix paradigm classes e . g .", "NULL . ed . ing vs . e . ed . ing vs . e . ed . es . ing vs . ted . tion from raw text .", "However , handling of irregular words was largely excluded from this work , as Goldsmith assumed a strictly concatenative morphology without models for stem changes .", "Morphology induction in agglutenative languages such as Turkish and Finnish presents a problem similar to parsing or segmenting a sentence , given the long strings of affixations allowed and the relatively free affix order .", "Voutilainen 1995 has approached this problem in a finitestate framework , and Hakkani Thr et al . 2000 have done so using a trigram tagger , with the assumption of a concatenative affixation model .", "The two level model of morphology Koskenniemi , 1983 has been extremely successful in manually capturing the morphological processes of the world's languages .", "The context sensitive stem change models used in this current paper have been partially inspired by this framework .", "For example , a two level equivalent capturing happy er happier is y i p p _ , quite similar in spirit and function to our probabilistic model P y xil . . . app , er .", "Theron and Cloete 1997 sought to learn a 2 level rule set for English , Xhosa and Afrikaans by supervision from 0 4000 aligned inflection root pairs extracted from dictionaries .", "Single character insertion and deletions were allowed , and the learned rules supported both prefixation and suffixation .", "Their supervised learning approach could be applied directly to the aligned pairs induced in this paper .", "Finally , Oflazer and Nirenburg 1999 have developed a framework to learn two level morphological analyzers from interactive supervision in a Elicit Build Test loop under the Boas project .", "Humans provide as needed feedback regarding errors and omissions .", "Recently applied to Polish , the model also assumes concatenative morphology and treats non concatenative irregular forms through table lookup .", "Thus there is a notable gap in the research literature for induction of analyzers for irregular morphological processes , including significant stem changing .", "The algorithm described below directly addresses this gap , while successfully inducing more regular analyses without supervision as well .", "The motivating dilemma behind our approach to morphological alignment is the question of how one determines that the past tense of sing is sang and not singed .", "The pairing sing x singed requires only simple concatenation with the canonical suffix , ed , and singed is indeed a legal word in our vocabulary the past tense of singe .", "And while few irregular verbs have a true word occupying the slot that would be generated by a regular morphological rule , a large corpus is filled with many spelling mistakes or dysfluencies such as taked observed with a frequency of 1 , and such errors can wreak havoc in na\u00efve alignment based methods .", "How can we overcome this problem ?", "Relative corpus frequency is one useful evidence source .", "Observe in Table 3 that in an 80 million word collection of newswire text the relative frequency distribution of sang sing is 1427 1204 or 1 . 19 1 , which indicates a reasonably close frequency match , while the singed sing ratio is 0 . 007 1 , a substantial disparity .", "However , simply looking for close relative frequencies between an inflection and its candidate root is inappropriate , given that some inflections are relatively rare and expected to occur much less frequently than the root form .", "Thus in order to be able to rank the sang sing and singed sing candidates effectively , it is necessary to be able to quantify how well each fits or deviates from expected frequency distributions .", "To do so , we use simple non parametric statistics to calculate the probability of a particularvBD ratio by examining how frequently other such ratios in a similar range have been seen in the corpus .", "Figure 1 illustrates such a histogram based on the log of the ratios to focus more attention on the extrema .", "The histogram is then smoothed and normalized as an approximation of the probability density function for this estimator g vvBBD which we can then use to quantify to what extent a given candidate log such as og sang sing . 17 , fits our empirically motivated expectations .", "The relative position of the candidate pairings on the graph suggests that this estimator is indeed informative given the task of ranking potential root inflection pairings .", "However , estimating these distributions presents a problem given that the true alignments and hence frequency ratios between inflections The third alignment similarity function considers overall stem edit distance using a weighted Levenshtein measure .", "In morphological systems worldwide , vowels and vowel clusters are relatively mutable through morphological processes , while consonants generally tend to have a lower probability of change during inflection .", "Rather than treating all string edits as equal , a cost matrix of the form shown in Table 6 is utilized , with initial distance costs 61 v v , 62 v v , 63 c c and 64 c v , initially set to 0 . 5 , 0 . 6 , 1 . 0 , 0 . 98 , a relatively arbitrary assignment reflecting this tendency .", "However , as subsequent algorithm iterations proceed , this matrix is re estimated with empirically observed character to character stem change probabilities from the algorithm's current best weighted alignments . a o ue m n . . . a 0 61 .", "62 64 64 More optimally , the initial state of this matrix could be seeded with values partially borrowed from previously trained matrices from other related languages .", "Alternately , the initial distances could be set partially sensitive to phonological similarities , with dist d , t dist d , f for example , although this particular distinction emerges readily via iterative re estimation from the baseline model .", "The goal of this research is not only to extract an accurate table of inflection root alignments , but also to generalize this mapping function via a generative probabilistic model .", "The following section describes the creation of this model , as well as how the context sensitive probability of each morphological transformation can be used as the fourth alignment similarity measure .", "At each iteration of the algorithm , this probabilistic mapping function is trained on the table output of the previous iteration , equivalent to the information in Table 1 e . g .", "root , inflection pairs with optional part of speech tags , confidence scores and stemchange suffix analysis . 6 From this output , we cluster the observed stem changes by the variable length root context in which they were applied , as illustrated in Table 7 .", "First note that because the triple of root stemchange suffix uniquely determines a resulting inflection , one can effectively compute P inflection I root , suffix , POS by P stemchange I root , suffix , POS , i . e . for any root ya , suffix o and inflection 0a , Using statistics such as shown in Table 7 , it is thus possible to compute the generation or alignment probability for an inflection given root and suffix using the simple interpolated backoff model in 1 where Ai is a function of the relative sample size of the conditioning event , and lastk root indicates the final k characters of the root .", "We only backoff to the extent necessary .", "Furthermore , note that for English and most inflections in Spanish , the stem changes observed when adding suffixes are independent of part of speech i . e .", "8 behaves the same on suffixation for both nouns and verbs , so these probabilities can often be further simplified by deleting the conditioning variable POS , as illustrated in 2 .", "We have further generalized these variablelength context models via a full hierarchicallysmoothed trie architecture , allowing robust specialization to very long root contexts if sample sizes are sufficient .", "6 . 1 Baseline Model for Morphological Transformation Probabilities On the first iteration , no inflection root pairs are available for estimating the above models .", "As prior knowledge is not available regarding a x stem change probabilities , an assumption is made that the cost of each is proportional to the previously described Levenshtein distance between a and 3 , with the cost of a change increasing geometrically as the distance from the end of the root increases .", "The rate of this cost increase ultimately depends on the tendency of the language to allow word internal spelling changes as in Spanish or Arabic , or strongly favor changes at the point of affixation as in English .", "The primary goal of iterative retraining is to refine the core morphological transformation model , which not only serves as one of the four similarity models , but is also a primary deliverable of the learning process .", "As subsequent iterations proceed , the stemchange probability models are retrained on the output of the prior iteration , weighting each training example with its alignment confidence , and filtering out a x 3 changes without a minimum level of support to help reduce noise .", "The final stem change probabilities then are an interpolation with the trained model Pi and the initial baseline P0 model described in Section 6 . 1 P c x 13 I root , suffix , POS Ai P0 a x 3 I suffix 1 Ai Pi a x 3 I root , suffix , POS The Levenshtein distance models are reestimated as observed in Section 5 , while the context similarity model can be improved through better self learned lemmatization of the modelled context words .", "7 Lemma Alignment by Model Combination and the Pigeonhole Principle As shown empirically below , no single model is sufficiently effective on its own .", "We applied traditional classifier combination techniques to merge the four models' scores , scaling each to achieve compatible dynamic range .", "The Frequency , Levenshtein and Context similarity models retain equal relative weight as training proceeds , while the Morphological Transformation MorphTrans similarity model increases in relative weight as it becomes better trained .", "Table 8 demonstrates the combined measures in action , showing the relative rankings of candidate roots for the inflections took , shook and juegan by the four similarity models after the first iteration in Columns 2 4 .", "The overall consensus similarity measure at the end of Iteration 1 is shown in Column 1 . 7 Note that even though only one of the four estimators independently ranked shake as the most likely root of shook , after only the first iteration the consensus choice is correct .", "The final column of Table 8 shows the retrained MorphTrans similarity measure after convergence .", "Based on training evidence from the confidently aligned pairs took take , shook shake and undertook undertake from previous iterations , the probability of ake xook has increased significantly , further increasing the confidence in the overall alignments at convergence not shown , but not changing the previously correct ranking in these cases .", "The final alignment constraint that we pursued was based on the pigeonhole principle .", "This principle suggests that for a given part of speech , a root should not have more than one inflection nor should multiple inflections in the same part of speech share the same root .", "There are , of course , exceptions to this tendency , such as travelled traveled and dreamed ! dreamt , which are observed as variant forms of their respected roots .", "71n addition to the consensus similarity score in subcolumn 2 , subcolumn 3 shows the average of the ranks of the candidate root given the inflection and the ranks of the candidate inflection given the root .", "This bidirectional average ranking score favors cases where attraction between root and inflection is mutual , and disfavors cases where higher ranked competition exists for a root's attentions , effectively capturing a weak form of the pigeonhole principle .", "Thus it was used as the primary ranking criteria over raw similarity score . shake . 00149 5 . 5 1 shake . 854 share . 073 shoo . 500 shoot . 002593 shake . 465578 shoot . 00126 9 . 3 2 shave . 323 ship . 068 shoot . 333 shoo . 002593 shoot . 001296 ship . 00104 16 . 3 3 shape . 210 shift . 062 shoe . 310 shock . 000096 shoo . 001296 shatter . 00061 18 . 9 4 shore . 194 shop . 060 shake . 290 short . 000096 shock . 000048 shop . 00094 19 . 8 5 shower . 184 shake . 058 shop . 236 shout . 000095 short . 000048 shut . 00081 20 . 6 6 shoot . 162 shut . 052 shout . 236 . . . . . . shove . 000048 shun . 00039 20 . 7 7 shock . 154 shoot . 051 show . 236 shake . 000003 shore . 000048 The extent to which such overlaps should be penalized depends on the probability of seeing variant inflections in the morphology , but for Spanish and English this is relatively low .", "We exploited the pigeonhole principle in two ways simultaneously .", "The first is a greedy algorithm , in which candidates are aligned in order of decreasing score , and when the the first choice root for a given inflection has already been taken by another inflection of the same part of speech , the algorithm continues until a free slot is found .", "The exception is when the highest ranking free form is several orders of magnitude lower than the first choice ; here the first choice alignment is assumed to be correct , but a variant form .", "Current empirical evaluation of this work focuses on its accuracy in analyzing the often highly irregular past tense of English verbs .", "Consistent with prior empirical studies in this field , evaluation was performed on a test set of 3888 inflected words , including 128 highly irregular inflections , 1877 cases where the past tense was formed by simple concatenative suffixation , and 1883 inflections exhibiting a non concatenative stem change such as gemination or elision .", "In execution , for each test inflected form , the analysis algorithm was free to consider alignment to any word in the corpus which had been identified as a potential root verb by the part of speech tagging process or occurrence in a dictionaryderived rootlist , not just those roots in the test set .", "It is thus a more challenging evaluation than testing simple alignment accuracy between two clean and extraneous entry free wordlists .", "Table 9 shows the performance of several of the investigated similarity measures .", "Frequency similarity FS , enhanced Levenshtein LS , and Context similarity CS alone achieve only 10 , 31 and 28 overall accuracy respectively .", "However , the hypothesis that these measures model independent and complementary evidence sources is supported by the roughly additive combined accuracy of 71 . 6 . 8 The final performance of the full converged CS FS LS MS model at 99 . 2 accuracy on the full test set , and 99 . 7 accuracy on inflections requiring analysis beyond simple concatenative suffixation , is quite remarkable given that the algorithm had absolutely no inflection , root examples as training data , and had no prior inventory of stem changes available , with only a slight statistical bias in favor of shorter stem changes with smaller Levenshtein distance , and with the minimal search simplifying assumption in all the models that candidate alignments must begin with a the same VC prefix . 9 Given a starting point where all single character X 17 changes at the point of suffixation are equally likely , the processes of elison e e , gemination e . g .", "E xd in the context of d , and y xi shift in the context of a preceding consonant , not vowel all emerge by the end of the first iteration with high probability in their appropriate contexts , and low probability elsewhere .", "Table 10 shows how each of the models perform on a randomly selected 30 of the highly irregular forms , with correctly selected roots identified in bold .", "The residual errors are primarily of three types Two inflections , went and ate , were not alignable with their correct roots due to different first character .", "The largest class of errors are due to the pigeonhole principle strongly disfavoring two inflections from sharing the same root . 10 9To put the Table 9 results in perspective , Mooney and Calif 1995 achieved 82 . 5 overall accuracy using a fully supervised decision list learner trained on 250 paired past tense root verb pairs in plain text form .", "Although they don't breakdown this performance by word type , their included FOILDL program trained from 250 pairs and applied to our evaluation set achieved 100 accuracy on the pairs with simple ed concatenation , 84 accuracy on stem changing non concat pairs and 5 accuracy on the highly irregular pairs , with 89 overall accuracy .", "Other available supervised learning results e . g .", "Ling ; Rumelhart and McClelland are only given for phonological word representations .", "While not directly comparable with our text based data , their performance is significantly worse than Mooney and Calif's FOILDL on common phonological paired data , suggesting that FOILDL is a generally competitive reference point for our results .", "19This was previously noted in the case of dream dreamed and dreamt , or burned burned and burnt , with the higher probability analysis typically occupying the root slot and the lower probability form typically forced to seek alignment elsewhere .", "Indeed , the pigeonhole principle is the most problematic of all the The remaining errors typically are due to sparse statistics for the lower frequency irregular forms .", "Mappings such as slew slay are particularly difficult because , with a corpus frequency of only 4 , there is too little data to estimate a good context profile or an effectively discriminatory frequency profile .", "Enlarging the raw corpus size should improve performance in both of these cases .", "This paper has presented an original algorithm capable of inducing the accurate morphological analysis of even highly irregular verbs , starting with no paired inflection , root examples for training and no prior seeding of legal morphological transformations .", "It does so by treating morphological analysis predominantly as an alignment task in a large corpus , performing the effective collaboration of four original similarity measures based on expected frequency distributions , context , morphologically weighted Levenshtein similarity and an iteratively bootstrapped model of affixation and stem change probabilities .", "This constitutes a significant achievement in that previous approaches to morphology acquisition have either focused on unsupervised induction of quasiregular concatenative affixation , or handled irregular forms with fully supervised training .", "In contrast , this paper's essentially unsupervised algorithm achieves over 80 accuracy on the most highly irregular forms , and 99 . 7 accuracy on analyses requiring some stem change , outperforming Mooney and Califf's fully supervised learning algorithm overall and on both of these measures . alignment principles used , as it creates nearly as many problems as it fixes .", "The overall performance advantage is slightly in its favor with 59 misalignments avoided for 50 problems created , but the cost of this approach is borne heavily by the irregular verbs , and a probabilistic model of when variant forms should be expected allowed is necessary to fix these cases while preserving the advantages of the principle in downweighting clashing analyses in the more regular verbs .", "Test True Convg CS FS LS MS Itr 1 CS FS LS CS FS LS only Word Root Score Itr 1 Itr 1 Itr 1 got get go 1 . 30 go go go gut knew know know 1 . 35 know know know know took take take 1 . 50 take take take toot blew blow blow 1 . 80 blow blow blow blow became become become 2 . 35 become become become become made make make 2 . 40 make make make mate clung cling cling 2 . 55 cling cling cling cling drew draw draw 2 . 65 draw draw draw draw swore swear swear 2 . 80 swear swear swear store wore wear wear 3 . 10 wear wear wear wire came come come 3 . 55 come come come come thought think think 3 . 60 think think think thump flung fling fling 4 . 60 fling fling fling fling brought bring bring 5 . 35 bring bring bring brighten strove strive strive 5 . 85 strive strive straddle strive stuck stick stick 6 . 00 stick stick stabilize stock swept sweep sweep 6 . 20 sweep sweep sweep swap shone shine shine 6 . 55 shine shine shine shine woke wake wake 6 . 95 wake wake wind wake clove cleave cleave 7 . 35 cleave cleave cleave close bore bear bear 7 . 75 bear bar bear bare meant mean mean 8 . 20 mean mean manage mount lent lend lend 9 . 25 lend lend lend lend slew slay slit 10 . 06 slit slight slight slow struck strike strike 11 . 60 strike strike strike strut bought buy buy 12 . 20 buy buy buy bound bit bite bite 13 . 60 bite bite betray bet dove dive dive 17 . 25 dive dive dash dive burnt burn burp 17 . 30 burp burp burp burn went go want 18 . 29 want want want want caught catch catch 18 . 35 catch cut catch cough dealt deal deal 21 . 45 deal deal disagree deal"], "summary_lines": ["Minimally Supervised Morphological Analysis By Multimodal Alignment\n", "This paper presents a corpus-based algorithm capable of inducing inflectional morphological analyses of both regular and highly irregular forms (such as brought\u2192bring) from distributional patterns in large monolingual text with no direct supervision.\n", "The algorithm combines four original alignment models based on relative corpus frequency, contextual similarity, weighted string similarity and incrementally retrained inflectional transduction probabilities.\n", "Starting with no paired <inflection,root> examples for training and no prior seeding of legal morphological transformations, accuracy of the induced analyses of 3888 past-tense test cases in English exceeds 99.2% for the set, with currently over 80% accuracy on the most highly irregular forms and 99.7% accuracy on forms exhibiting non-concatenative suffixation.\n", "We obtain outstanding results at inducing English past tense after beginning with a list of the open class roots in the language, a table of a language's inflectional parts of speech, and the canonical suffixes for each part of speech.\n", "We propose an algorithm that extracts morphological rules relating roots and inflected forms of verbs (but the algorithm can be extended to other morphological relations).\n", "The supervised morphological learner presented in this paper models lemmatization as a word-final stem change plus a suffix taken from a (possibly empty) list of potential suffixes.\n"]}
{"article_lines": ["SemEval 2007 Task 07 Coarse Grained English All Words Task", "This paper presents the coarse grained En glish all words task at SemEval 2007 .", "We describe our experience in producing acoarse version of the WordNet sense inven tory and preparing the sense tagged corpusfor the task .", "We present the results of participating systems and discuss future direc tions .", "It is commonly thought that one of the major obstacles to high performance Word Sense Disambiguation WSD is the fine granularity of sense inventories .", "State of the art systems attained a disam biguation accuracy around 65 in the Senseval 3 all words task Snyder and Palmer , 2004 , whereWordNet Fellbaum , 1998 was adopted as a ref erence sense inventory .", "Unfortunately , WordNet is a fine grained resource , encoding sense distinctionsthat are difficult to recognize even for human an notators Edmonds and Kilgarriff , 2002 .", "MakingWSD an enabling technique for end to end applications clearly depends on the ability to deal with rea sonable sense distinctions .", "The aim of this task was to explicitly tackle the granularity issue and study the performance of WSD systems on an all words basis when a coarser set of senses is provided for the target words .", "Given the need of the NLP community to work on freelyavailable resources , the solution of adopting a dif ferent computational lexicon is not viable .", "On the other hand , the production of a coarse grained sense inventory is not a simple task .", "The main issue is certainly the subjectivity of sense clusters .", "To overcome this problem , different strategies can be adopted .", "For instance , in the OntoNotes project Hovy et al , 2006 senses are grouped until a 90 inter annotator agreement is achieved .", "In contrast , as we describe in this paper , our approach is based on a mapping to a previously existing inventory which encodes sense distinctions at different levelsof granularity , thus allowing to induce a sense clus tering for the mapped senses . We would like to mention that another SemEval 2007 task dealt with the issue of sense granularityfor WSD , namely Task 17 subtask 1 Coarse grained English Lexical Sample WSD .", "In this paper , we report our experience in organizing Task 07 .", "The task required participating systems to annotate open class words i . e . nouns , verbs , adjectives , and adverbs in a test corpus with the most appropriate sense from a coarse grained version of the WordNet sense inventory .", "2 . 1 Test Corpus .", "The test data set consisted of 5 , 377 words of run ning text from five different articles the first three in common with Task 17 were obtained from the WSJ corpus , the fourth was the Wikipedia entry for computer programming1 , the fifth was an excerpt of Amy Steedman ? s Knights of the Art , biographies of Italian painters2 .", "We decided to add the last two 1http en . wikipedia . org wiki Computer programming 2http www . gutenberg . org etext 529 30 article domain words annotated d001 JOURNALISM 951 368 d002 BOOK REVIEW 987 379 d003 TRAVEL 1311 500 d004 COMPUTER SCIENCE 1326 677 d005 BIOGRAPHY 802 345 total 5377 2269 Table 1 Statistics about the five articles in the test data set .", "texts to the initial dataset as we wanted the corpus to have a size comparable to that of previous editions of all words tasks . In Table 1 we report the domain , number of run ning words , and number of annotated words for the five articles .", "We observe that articles d003 and d004 are the largest in the corpus they constitute 51 . 87 of it .", "2 . 2 Creation of a Coarse Grained Sense .", "Inventory To tackle the granularity issue , we produced acoarser grained version of the WordNet sense inven tory3 based on the procedure described by Navigli 2006 .", "The method consists of automatically map ping WordNet senses to top level , numbered entries in the Oxford Dictionary of English ODE , Soanesand Stevenson , 2003 .", "The semantic mapping be tween WordNet and ODE entries was obtained intwo steps first , we disambiguated with the SSI algo rithm Navigli and Velardi , 2005 the definitions ofthe two dictionaries , together with additional infor mation hypernyms and domain labels ; second , foreach WordNet sense , we determined the best match ing ODE coarse entry .", "As a result , WordNet senses mapped to the same ODE entry were assigned to the same sense cluster .", "WordNet senses with no match were associated with a singleton sense .", "In contrast to the automatic method above , thesense mappings for all the words in our test cor pus were manually produced by the third author , anexpert lexicographer , with the aid of a mapping in terface .", "Not all the words in the corpus could be mapped directly for several reasons lacking entries in ODE e . g . adjectives underlying and shivering , 3We adopted WordNet 2 . 1 , available from http wordnet . princeton . edu different spellings e . g . after effect vs . aftereffect , halfhearted vs . half hearted , etc . , derivatives e . g . procedural , gambler , etc . .", "In most of the cases , weasked the lexicographer to map senses of the orig inal word to senses of lexically related words e . g . WordNet senses of procedural were mapped to ODE senses of procedure , etc . .", "When this mapping was not straightforward , we just adopted the WordNet sense inventory for that word . We released the entire sense groupings those in duced from the manual mapping for words in the test set plus those automatically derived on the other words and made them available to the participants .", "2 . 3 Sense Annotation .", "All open class words i . e . nouns , verbs , adjectives , and adverbs with an existing sense in the WordNetinventory were manually annotated by the third author .", "Multi word expressions were explicitly iden tified in the test set and annotated as such this wasmade to allow a fair comparison among systems independent of their ability to identify multi word ex pressions .", "We excluded auxiliary verbs , uncovered phrasal and idiomatic verbs , exclamatory uses , etc . The annotator was allowed to tag words with multiple coarse senses , but was asked to make a single sense assignment whenever possible .", "The lexicographer annotated an overall numberof 2 , 316 content words .", "47 2 of them were excluded because no WordNet sense was deemed ap propriate .", "The remaining 2 , 269 content words thusconstituted the test data set .", "Only 8 of them were as signed more than one sense specifically , two coarse senses were assigned to a single word instance4 and two distinct fine grained senses were assigned to 7 word instances .", "This was a clear hint that the sense clusters were not ambiguous for the vast majority of words . In Table 2 we report information about the polysemy of the word instances in the test set .", "Over all , 29 . 88 678 2269 of the word instances weremonosemous according to our coarse sense inven tory .", "The average polysemy of the test set with the coarse grained sense inventory was 3 . 06 compared to an average polysemy with the WordNet inventory 4d005 . s004 . t015 31 polysemy N V A R all monosemous 358 86 141 93 678 polysemous 750 505 221 115 1591 total 1108 591 362 208 2269 Table 2 Statistics about the test set polysemy N nouns , V verbs , A adjectives , R adverbs .", "of 6 . 18 .", "2 . 4 Inter Annotator Agreement .", "Recent estimations of the inter annotator agreement when using the WordNet inventory report figures of 72 . 5 agreement in the preparation of the English all words test set at Senseval 3 Snyder and Palmer , 2004 and 67 . 3 on the Open Mind Word Expert an notation exercise Chklovski and Mihalcea , 2002 . As the inter annotator agreement is often considered an upper bound for WSD systems , it was de sirable to have a much higher number for our task , given its coarse grained nature .", "To this end , besidethe expert lexicographer , a second author indepen dently performed part of the manual sense mapping 590 word senses described in Section 2 . 2 .", "The pairwise agreement was 86 . 44 .", "We repeated the same agreement evaluation onthe sense annotation task of the test corpus .", "A sec ond author independently annotated part of the test set 710 word instances .", "The pairwise agreement between the two authors was 93 . 80 .", "This figure , compared to those in the literature for fine grained human annotations , gives us a clear indication that the agreement of human annotators strictly depends on the granularity of the adopted sense inventory .", "We calculated two baselines for the test corpus a random baseline , in which senses are chosen at random , and the most frequent baseline MFS , in which we assign the first WordNet sense to each word in the dataset .", "Formally , the accuracy of the random baseline was calculated as follows BLRand 1 T T ?", "i 1 1 CoarseSenses wi where T is our test corpus , wi is the i th word instance in T , and CoarseSenses wi is the set ofcoarse senses for wi according to the sense cluster ing we produced as described in Section 2 . 2 .", "The accuracy of the MFS baseline was calculated as BLMFS 1 T T ?", "i 1 ? wi , 1 where ? wi , k equals 1 when the k th sense ofword wi belongs to the cluster s manually associ ated by the lexicographer to word wi 0 otherwise .", "Notice that our calculation of the MFS is based on the frequencies in the SemCor corpus Miller et al , 1993 , as we exploit WordNet sense rankings .", "12 teams submitted 14 systems overall plus two systems from a 13th withdrawn team that we will not report .", "According to the SemEval policy for task organizers , we remark that the system labelled as UOR SSI was submitted by the first author thesystem is based on the Structural Semantic Inter connections algorithm Navigli and Velardi , 2005 with a lexical knowledge base composed by Word Net and approximately 70 , 000 relatedness edges . Even though we did not specifically enrich the al gorithm ? s knowledge base on the task at hand , we list the system separately from the overall ranking . The results are shown in Table 3 .", "We calculated a MFS baseline of 78 . 89 and a random baseline of 52 . 43 .", "In Table 4 we report the F1 mea sures for all systems where we used the MFS as abackoff strategy when no sense assignment was at tempted this possibly reranked 6 systems marked in bold in the table which did not assign a sense to all word instances in the test set .", "Comparedto previous results on fine grained evaluation exer cises Edmonds and Kilgarriff , 2002 ; Snyder and Palmer , 2004 , the systems ?", "results are much higher .", "On the other hand , the difference in performancebetween the MFS baseline and state of the art sys tems around 5 on coarse grained disambiguationis comparable to that of the Senseval 3 all words ex ercise .", "However , given the novelty of the task webelieve that systems can achieve even better perfor 32 System A P R F1 NUS PT 100 . 0 82 . 50 82 . 50 82 . 50 NUS ML 100 . 0 81 . 58 81 . 58 81 . 58 LCC WSD 100 . 0 81 . 45 81 . 45 81 . 45 GPLSI 100 . 0 79 . 55 79 . 55 79 . 55 BLMFS 100 . 0 78 . 89 78 . 89 78 . 89 UPV WSD 100 . 0 78 . 63 78 . 63 78 . 63 TKB UO 100 . 0 70 . 21 70 . 21 70 . 21 PU BCD 90 . 1 69 . 72 62 . 80 66 . 08 RACAI SYNWSD 100 . 0 65 . 71 65 . 71 65 . 71 SUSSX FR 72 . 8 71 . 73 52 . 23 60 . 44 USYD 95 . 3 58 . 79 56 . 02 57 . 37 UOFL 92 . 7 52 . 59 48 . 74 50 . 60 SUSSX C WD 72 . 8 54 . 54 39 . 71 45 . 96 SUSSX CR 72 . 8 54 . 30 39 . 53 45 . 75 UOR SSI ?", "100 . 0 83 . 21 83 . 21 83 . 21 Table 3 System scores sorted by F1 measure A attempted , P precision , R recall , F1 F1 mea sure , ? system from one of the task organizers .", "mance by heavily exploiting the coarse nature of the sense inventory .", "In Table 5 we report the results for each of the five articles .", "The interesting aspect of the table is that documents from some domains seem to havepredominant senses different from those in Sem Cor .", "Specifically , the MFS baseline performs more poorly on documents d004 and d005 , from the COMPUTER SCIENCE and BIOGRAPHY domains respectively .", "We believe this is due to the fact that these documents have specific predominant senses , which correspond less often to the most frequent sense in SemCor than for the other three documents .", "It is also interesting to observe that different systemsperform differently on the five documents we high light in bold the best performing systems on each article .", "Finally , we calculated the systems ?", "performance by part of speech .", "The results are shown in Table 6 .", "Again , we note that different systems show dif .", "ferent performance depending on the part of speechtag .", "Another interesting aspect is that the performance of the MFS baseline is very close to state of the art systems for adjectives and adverbs , whereas it is more than 3 points below for verbs , and around 5 for nouns .", "System F1 NUS PT 82 . 50 NUS ML 81 . 58 LCC WSD 81 . 45 GPLSI 79 . 55 BLMFS 78 . 89 UPV WSD 78 . 63 SUSSX FR 77 . 04 TKB UO 70 . 21 PU BCD 69 . 72 RACAI SYNWSD 65 . 71 SUSSX C WD 64 . 52 SUSSX CR 64 . 35 USYD 58 . 79 UOFL 54 . 61 UOR SSI ?", "83 . 21 Table 4 System scores sorted by F1 measure with MFS adopted as a backoff strategy when no sense assignment is attempted ? system from one of the task organizers .", "Systems affected are marked in bold .", "System N V A R NUS PT 82 . 31 78 . 51 85 . 64 89 . 42 NUS ML 81 . 41 78 . 17 82 . 60 90 . 38 LCC WSD 80 . 69 78 . 17 85 . 36 87 . 98 GPLSI 80 . 05 74 . 45 82 . 32 86 . 54 BLMFS 77 . 44 75 . 30 84 . 25 87 . 50 UPV WSD 79 . 33 72 . 76 84 . 53 81 . 25 TKB UO 70 . 76 62 . 61 78 . 73 74 . 04 PU BCD 71 . 41 59 . 69 66 . 57 55 . 67 RACAI SYNWSD 64 . 02 62 . 10 71 . 55 75 . 00 SUSSX FR 68 . 09 51 . 02 57 . 38 49 . 38 USYD 56 . 06 60 . 43 58 . 00 54 . 31 UOFL 57 . 65 48 . 82 25 . 87 60 . 80 SUSSX C WD 52 . 18 35 . 64 42 . 95 46 . 30 SUSSX CR 51 . 87 35 . 44 42 . 95 46 . 30 UOR SSI ?", "84 . 12 78 . 34 85 . 36 88 . 46 Table 6 System scores by part of speech tag N nouns , V verbs , A adjectives , R adverbs sorted by overall F1 measure best scores are marked in bold , ? system from one of the task organizers .", "33 d001 d002 d003 d004 d005 System P R P R P R P R P R NUS PT 88 . 32 88 . 32 88 . 13 88 . 13 83 . 40 83 . 40 76 . 07 76 . 07 81 . 45 81 . 45 NUS ML 86 . 14 86 . 14 88 . 39 88 . 39 81 . 40 81 . 40 76 . 66 76 . 66 79 . 13 79 . 13 LCC WSD 87 . 50 87 . 50 87 . 60 87 . 60 81 . 40 81 . 40 75 . 48 75 . 48 80 . 00 80 . 00 GPLSI 83 . 42 83 . 42 86 . 54 86 . 54 80 . 40 80 . 40 73 . 71 73 . 71 77 . 97 77 . 97 BLMFS 85 . 60 85 . 60 84 . 70 84 . 70 77 . 80 77 . 80 75 . 19 75 . 19 74 . 20 74 . 20 UPV WSD 84 . 24 84 . 24 80 . 74 80 . 74 76 . 00 76 . 00 77 . 11 77 . 11 77 . 10 77 . 10 TKB UO 78 . 80 78 . 80 72 . 56 72 . 56 69 . 40 69 . 40 70 . 75 70 . 75 58 . 55 58 . 55 PU BCD 77 . 16 67 . 94 75 . 52 67 . 55 64 . 96 58 . 20 68 . 86 61 . 74 64 . 42 60 . 87 RACAI SYNWSD 71 . 47 71 . 47 72 . 82 72 . 82 66 . 80 66 . 80 60 . 86 60 . 86 59 . 71 59 . 71 SUSSX FR 79 . 10 57 . 61 73 . 72 53 . 30 74 . 86 52 . 40 67 . 97 48 . 89 65 . 20 51 . 59 USYD 62 . 53 61 . 69 59 . 78 57 . 26 60 . 97 57 . 80 60 . 57 56 . 28 47 . 15 45 . 51 UOFL 61 . 41 59 . 24 55 . 93 52 . 24 48 . 00 45 . 60 53 . 42 47 . 27 44 . 38 41 . 16 SUSSX C WD 66 . 42 48 . 37 61 . 31 44 . 33 55 . 14 38 . 60 50 . 72 36 . 48 42 . 13 33 . 33 SUSSX CR 66 . 05 48 . 10 60 . 58 43 . 80 59 . 14 41 . 40 48 . 67 35 . 01 40 . 29 31 . 88 UOR SSI ?", "86 . 14 86 . 14 85 . 49 85 . 49 79 . 60 79 . 60 86 . 85 86 . 85 75 . 65 75 . 65 Table 5 System scores by article best scores are marked in bold , ? system from one of the task organizers .", "In order to allow for a critical and comparative inspection of the system results , we asked the partici pants to answer some questions about their systems .", "These included information about whether 1 .", "the system used semantically annotated and unannotated resources ; 2 .", "the system used the MFS as a backoff strategy ; 3 .", "the system used the coarse senses provided by the organizers ; 4 .", "the system was trained on some corpus .", "We believe that this gives interesting information to provide a deeper understanding of the results .", "Wesummarize the participants ?", "answers to the questionnaires in Table 7 .", "We report about the use of semantic resources as well as semantically annotated corpora SC SemCor , DSO Defence Science Organ isation Corpus , SE Senseval corpora , OMWE Open Mind Word Expert , XWN eXtended Word Net , WN WordNet glosses and or relations , WND WordNet Domains , as well as information about the use of unannotated corpora UC , training TR , MFS based on the SemCor sense frequencies , and the coarse senses provided by the organizers CS .", "As expected , several systems used lexico semantic information from the WordNet semantic networkand or were trained on the SemCor semantically annotated corpus . Finally , we point out that all the systems perform ing better than the MFS baseline adopted it as a backoff strategy when they were not able to output a sense assignment .", "It is commonly agreed that Word Sense Disambiguation needs emerge and show its usefulness in end to end applications after decades of research in the field it is still unclear whether WSD can provide a relevant contribution to real world applications , such as Information Retrieval , Question Answering , etc . In previous Senseval evaluation exercises , state of the art systems achieved performance far below70 and even the agreement between human annotators was discouraging .", "As a result of the discus sion at the Senseval 3 workshop in 2004 , one of the aims of SemEval 2007 was to tackle the problems at the roots of WSD .", "In this task , we dealt with the granularity issue which is a major obstacle to bothsystem and human annotators .", "In the hope of over coming the current performance upper bounds , we 34 System SC DSO SE OMWE XWN WN WND OTHER UC TR MFS CS GPLSI ? ?", "LCC WSD ? ?", "NUS ML ? ?", "NUS PT ? ?", "Parallel corpus ? ?", "PU BCD ? ?", "RACAI SYNWSD ? ?", "SUSSX C WD ? ?", "SUSSX CR ? ?", "SUSSX FR ? ?", "TKB UO ? ?", "UOFL ? ?", "UOR SSI ?", "SSI LKB ? ?", "UPV WSD ? ?", "USYD ? ?", "Table 7 Information about participating systems SC SemCor , DSO Defence Science Organisation Corpus , SE Senseval corpora , OMWE Open Mind Word Expert , XWN eXtended WordNet , WN WordNet glosses and or relations , WND WordNet Domains , UC use of unannotated corpora , TR use of training , MFS most frequent sense backoff strategy , CS use of coarse senses from the organizers , ? system from one of the task organizers . proposed the adoption of a coarse grained sense inventory .", "We found the results of participating sys tems interesting and stimulating .", "However , some questions arise .", "First , it is unclear whether , given the novelty of the task , systems really achieved the state of the art or can still improve their performancebased on a heavier exploitation of coarse and finegrained information from the adopted sense inven tory .", "We observe that , on a technical domain suchas computer science , most supervised systems per formed worse due to the nature of their training set .", "Second , we still need to show that coarse senses can be useful in real applications .", "Third , a full coarse sense inventory is not yet available this is a majorobstacle to large scale in vivo evaluations .", "We be lieve that these aspects deserve further investigation in the years to come .", "Acknowledgments This work was partially funded by the Interop NoE 508011 , 6th European Union FP .", "We would like to thank Martha Palmer for providing us the first three texts of the test corpus ."], "summary_lines": ["SemEval-2007 Task 07: Coarse-Grained English All-Words Task\n", "This paper presents the coarse-grained English all-words task at SemEval-2007.\n", "We describe our experience in producing a coarse version of the WordNet sense inventory and preparing the sense-tagged corpus for the task.\n", "We present the results of participating systems and discuss future directions.\n", "We show that the performance of WSD systems clearly indicates that WSD is not easy unless one adopts a coarse-grained approach, and then systems tagging all words at best perform a few percentage points above the most frequent sense heuristic (Navigli et al, 2007).\n"]}
{"article_lines": ["Fast Methods For Kernel Based Text Analysis", "Kernel based learning e . g . , Support Vector Machines has been successfully applied to many hard problems in Natural Language Processing NLP .", "In NLP , although feature combinations are crucial to improving performance , they are heuristically selected .", "Kernel methods change this situation .", "The merit of the kernel is that feature combinaimplicitly expanded without loss of generality and increasing the computational costs .", "Kernel based text analysis shows an excellent performance in terms in accuracy ; however , these methods are usually too slow to apply to large scale text analysis .", "In this paper , we extend Mining to convert a kernel based classifier into a simple and fast linear classifier .", "Experimental results on English BaseNP Chunking , Japanese Word Segmentation and Japanese Dependency Parsing show that our new classifiers are about 30 to 300 times faster than the standard kernel based classifiers .", "Kernel methods e . g . , Support Vector Machines Vapnik , 1995 attract a great deal of attention recently .", "In the field of Natural Language Processing , many successes have been reported .", "Examples include Part of Speech tagging Nakagawa et al . , 2002 Text Chunking Kudo and Matsumoto , 2001 , Named Entity Recognition Isozaki and Kazawa , 2002 , and Japanese Dependency Parsing Kudo and Matsumoto , 2000 ; Kudo and Matsumoto , 2002 .", "It is known in NLP that combination of features contributes to a significant improvement in accuracy .", "For instance , in the task of dependency parsing , it would be hard to confirm a correct dependency relation with only a single set of features from either a head or its modifier .", "Rather , dependency relations should be determined by at least information from both of two phrases .", "In previous research , feature combination has been selected manually , and the performance significantly depended on these selections .", "This is not the case with kernel based methodology .", "For instance , if we use a polynomial kernel , all feature combinations are implicitly expanded without loss of generality and increasing the computational costs .", "Although the mapped feature space is quite large , the maximal margin strategy Vapnik , 1995 of SVMs gives us a good generalization performance compared to the previous manual feature selection .", "This is the main reason why kernel based learning has delivered great results to the field of NLP .", "Kernel based text analysis shows an excellent performance in terms in accuracy ; however , its inefficiency in actual analysis limits practical application .", "For example , an SVM based NE chunker runs at a rate of only 85 byte sec , while previous rulebased system can process several kilobytes per second Isozaki and Kazawa , 2002 .", "Such slow execution time is inadequate for Information Retrieval , Question Answering , or Text Mining , where fast analysis of large quantities of text is indispensable .", "This paper presents two novel methods that make the kernel based text analyzers substantially faster .", "These methods are applicable not only to the NLP tasks but also to general machine learning tasks where training and test examples are represented in a binary vector .", "More specifically , we focus on a Polynomial Kernel of degree d , which can attain feature combinations that are crucial to improving the performance of tasks in NLP .", "Second , we introduce two fast classification algorithms for this kernel .", "One is PKI Polynomial Kernel Inverted , which is an extension of Inverted Index in Information Retrieval .", "The other is PKE Polynomial Kernel Expanded , where all feature combinations are explicitly expanded .", "By applying PKE , we can convert a kernel based classifier into a simple and fast liner classifier .", "In order to build PKE , we extend the PrefixSpan Pei et al . , 2001 , an efficient Basket Mining algorithm , to enumerate effective feature combinations from a set of support examples .", "Experiments on English BaseNP Chunking , Japanese Word Segmentation and Japanese Dependency Parsing show that PKI and PKE perform respectively 2 to 13 times and 30 to 300 times faster than standard kernel based systems , without a discernible change in accuracy .", "Suppose we have a set of training data for a binary classification problem x1 , y1 , .", ", xL , yL xj E RN , yj E 1 , 1 , where xj is a feature vector of the j th training sample , and yj is the class label associated with this training sample .", "The decision function of SVMs is defined by where A \u03c6 is a non liner mapping function from RN to RH N H .", "B \u03b1j , b E R , \u03b1j 0 .", "The mapping function \u03c6 should be designed such that all training examples are linearly separable in RH space .", "Since H is much larger than N , it requires heavy computation to evaluate the dot products \u03c6 xi \u03c6 x in an explicit form .", "This problem can be overcome by noticing that both construction of optimal parameter \u03b1i we will omit the details of this construction here and the calculation of the decision function only require the evaluation of dot products \u03c6 xi \u03c6 x .", "This is critical , since , in some cases , the dot products can be evaluated by a simple Kernel Function K x1 , x2 \u03c6 x1 \u03c6 x2 .", "Substituting kernel function into 1 , we have the following decision function .", "One of the advantages of kernels is that they are not limited to vectorial object x , but that they are applicable to any kind of object representation , just given the dot products .", "For many tasks in NLP , the training and test examples are represented in binary vectors ; or sets , since examples in NLP are usually represented in socalled Feature Structures .", "Here , we focus on such cases 1 .", "Suppose a feature set F 1 , 2 , . . . , N and training examples Xj j 1 , 2 , . . . , L , all of which are subsets of F i . e . , Xj C_ F .", "In this case , Xj can be regarded as a binary vector xj xj1 , xj2 , . . . , xjN where xji 1 if i E Xj , xji 0 otherwise .", "The dot product of x1 and x2 is given by x1 x2 X1 n X2 .", "Kd x , y Kd X , Y 1 X n Y d , 3 where d 1 , 2 , 3 , . . . .", "In this paper , 3 will be referred to as an implicit form of the Polynomial Kernel .", "It is known in NLP that a combination of features , a subset of feature set F in general , contributes to overall accuracy .", "In previous research , feature combination has been selected manually .", "The use of a polynomial kernel allows such feature expansion without loss of generality or an increase in computational costs , since the Polynomial Kernel of degree d implicitly maps the original feature space F into Fd space .", "i . e . , 0 F Fd .", "This property is critical and some reports say that , in NLP , the polynomial kernel outperforms the simple linear kernel Kudo and Matsumoto , 2000 ; Isozaki and Kazawa , 2002 .", "Here , we will give an explicit form of the Polynomial Kernel to show the mapping function 0 .", "Lemma 1 Explicit form ofPolynomial Kernel .", "The Polynomial Kernel of degree d can be rewritten as", "In this section , we introduce two fast classification algorithms for the Polynomial Kernel of degree d . Before describing them , we give the baseline classifier PKB where 3 X cd r will be referred as a subset weight of the Polynomial Kernel of degree d . This function gives a prior weight to the subset s , where s r . Example 1 Quadratic and Cubic Kernel Given sets X a , b , c , d and Y a , b , d , e , the Quadratic Kernel K2 X , Y and the Cubic Kernel K3 X , Y can be calculated in an implicit form as K2 X , Y 1 X n Y 2 1 3 2 16 , K3 X , Y 1 X n Y 3 1 3 3 64 .", "Using Lemma 1 , the subset weights of the Quadratic Kernel and the Cubic Kernel can be calculated as c2 0 1 , c2 1 3 , c2 2 2 and c3 0 1 , c3 1 7 , c3 2 12 , c3 3 6 .", "In addition , subsets Pr X n Y r 0 , 1 , 2 , 3 are given as follows P0 X n Y The complexity of PKB is O X SV , since it takes O X to calculate 1 Xj n X d and there are a total of SV support examples .", "Given an item i E F , if we know in advance the set of support examples which contain item i E F , we do not need to calculate Xj n X for all support examples .", "This is a naive extension of Inverted Indexing in Information Retrieval .", "Figure 1 shows the pseudo code of the algorithm PKI .", "The function h i is a pre compiled table and returns a set of support examples which contain item i .", "The complexity of the PKI is O X B SV , where B is an average of h i over all item i E F . The PKI can make the classification speed drastically faster when B is small , in other words , when feature space is relatively sparse i . e . , B SV .", "The feature space is often sparse in many tasks in NLP , since lexical entries are used as features .", "The algorithm PKI does not change the final accuracy of the classification .", "Using Lemma 1 , we can represent the decision function 5 in an explicit form where \u0393d X udr 0 Pr X .", "The classification algorithm given by 7 will be referred to as PKE .", "The complexity of PKE is O \u0393d X O X d , independent on the number of support examples SV .", "To apply the PKE , we first calculate \u0393d F degree of vectors This calculation is trivial only when we use a Quadratic Kernel , since we just project the original feature space F into F x F space , which is small enough to be calculated by a naive exhaustive method .", "However , if we , for instance , use a polynomial kernel of degree 3 or higher , this calculation becomes not trivial , since the size of feature space exponentially increases .", "Here we take the following strategy Definition 2 w' An approximation of w An approximation of w is given by w' w' s1 , w' s2 , . . . , w' s \u0393d F , where w' s is set to 0 if w s is trivially close to 0 .", "i . e . , \u03c3neg w s \u03c3pos \u03c3neg 0 , \u03c3pos 0 , where \u03c3pos and \u03c3neg are predefined thresholds .", "The algorithm PKE is an approximation of the PKB , and changes the final accuracy according to the selection of thresholds \u03c3pos and \u03c3neg .", "The calculation of w' is formulated as the following mining problem Given a set of support examples and subset weight cd r , extract all subsets s and their weights w s if w s holds w s \u03c3pos or w s 5 \u03c3neg .", "In this paper , we apply a Sub Structure Mining algorithm to the feature combination mining problem .", "Generally speaking , sub structures mining algorithms efficiently extract frequent sub structures e . g . , subsets , sub sequences , sub trees , or subgraphs from a large database set of transactions .", "In this context , frequent means that there are no less than \u03be transactions which contain a sub structure .", "The parameter \u03be is usually referred to as the Minimum Support .", "Since we must enumerate all subsets of F , we can apply subset mining algorithm , in some times called as Basket Mining algorithm , to our task .", "There are many subset mining algorithms proposed , however , we will focus on the PrefixSpan algorithm , which is an efficient algorithm for sequential pattern mining , originally proposed by Pei et al . , 2001 .", "The PrefixSpan was originally designed to extract frequent sub sequence not subset patterns , however , it is a trivial difference since a set can be seen as a special case of sequences i . e . , by sorting items in a set by lexicographic order , the set becomes a sequence .", "The basic idea of the PrefixSpan is to divide the database by frequent sub patterns prefix and to grow the prefix spanning pattern in a depth first search fashion .", "We now modify the PrefixSpan to suit to our feature combination mining .", "We only enumerate up to subsets of size d . when we plan to apply the Polynomial Kernel of degree d . In the original PrefixSpan , the frequency of each subset does not change by its size .", "However , in our mining task , it changes i . e . , the frequency of subset s is weighted by cd s .", "Here , we process the mining algorithm by assuming that each transaction support example Xj has its frequency Cdyj\u03b1j , where Cd max cd 1 , cd 2 , . . . , cd d .", "The weight w s is calculated by w s \u03c9 s x cd s Cd , where \u03c9 s is a frequency of s , given by the original PrefixSpan .", "We first divide the support examples into positive yi 0 and negative yi 0 examples , and process mining independently .", "The result can be obtained by merging these two results .", "In the original PrefixSpan , minimum support is an integer .", "In our mining task , we can give a real number to minimum support , since each transaction support example Xj has possibly non integer frequency Cdyj\u03b1j .", "Minimum supports \u03c3pos and \u03c3neg control the rate of approximation .", "For the sake of convenience , we just give one parameter \u03c3 , and calculate \u03c3pos and \u03c3neg as follows After the process of mining , a set of tuples Q s , w s I is obtained , where s is a frequent subset and w s is its weight .", "We use a TRIE to efficiently store the set Q .", "The example of such TRIE compression is shown in Figure 2 .", "Although there are many implementations for TRIE , we use a Double Array Aoe , 1989 in our task .", "The actual classification of PKE can be examined by traversing the TRIE for all subsets s E Fd X .", "To demonstrate performances of PKI and PKE , we examined three NLP tasks English BaseNP Chunking EBC , Japanese Word Segmentation JWS and Japanese Dependency Parsing JDP .", "A more detailed description of each task , training and test data , the system parameters , and feature sets are presented in the following subsections .", "Table 1 summarizes the detail information of support examples e . g . , size of SVs , size of feature set etc . .", "Our preliminary experiments show that a Quadratic Kernel performs the best in EBC , and a Cubic Kernel performs the best in JWS and JDP .", "The experiments using a Cubic Kernel are suitable to evaluate the effectiveness of the basket mining approach applied in the PKE , since a Cubic Kernel projects the original feature space F into F3 space , which is too large to be handled only using a naive exhaustive method .", "All experiments were conducted under Linux using XEON 2 . 4 Ghz dual processors and 3 . 5 Gbyte of main memory .", "All systems are implemented in C .", "Text Chunking is a fundamental task in NLP dividing sentences into non overlapping phrases .", "BaseNP chunking deals with a part of this task and recognizes the chunks that form noun phrases .", "Here is an example sentence He reckons the current account deficit will narrow to only 1 . 8 billion .", "A BaseNP chunk is represented as sequence of words between square brackets .", "BaseNP chunking task is usually formulated as a simple tagging task , where we represent chunks with three types of tags B beginning of a chunk .", "I non initial word .", "O outside of the chunk .", "In our experiments , we used the same settings as Kudo and Matsumoto , 2002 .", "We use a standard data set Ramshaw and Marcus , 1995 consisting of sections 15 19 of the WSJ corpus as training and section 20 as testing .", "Since there are no explicit spaces between words in Japanese sentences , we must first identify the word boundaries before analyzing deep structure of a sentence .", "Japanese word segmentation is formalized as a simple classification task .", "Let s c1c2 cm be a sequence of Japanese characters , t t1t2 tm be a sequence of Japanese character types 3 associated with each character , and yi 1 , 1 , i 1 , 2 , . . . , m 1 be a boundary marker .", "If there is a boundary between ci and ci 1 , yi 1 , otherwise yi 1 .", "The feature set of example xi is given by all characters as well as character types in some constant window e . g . , 5 ci 2 , ci 1 , , ci 2 , ci 3 , ti 2 , ti 1 , , ti 2 , ti 3 .", "Note that we distinguish the relative position of each character and character type .", "We use the Kyoto University Corpus Kurohashi and Nagao , 1997 , 7 , 958 sentences in the articles on January 1st to January 7th are used as training data , and 1 , 246 sentences in the articles on January 9th are used as the test data .", "The task of Japanese dependency parsing is to identify a correct dependency of each Bunsetsu base phrase in Japanese .", "In previous research , we presented a state of the art SVMs based Japanese dependency parser Kudo and Matsumoto , 2002 .", "We combined SVMs into an efficient parsing algorithm , Cascaded Chunking Model , which parses a sentence deterministically only by deciding whether the current chunk modifies the chunk on its immediate right hand side .", "The input for this algorithm consists of a set of the linguistic features related to the head and modifier e . g . , word , part of speech , and inflections , and the output from the algorithm is either of the value 1 dependent or 1 independent .", "We use a standard data set , which is the same corpus described in the Japanese Word Segmentation .", "3Usually , in Japanese , word boundaries are highly constrained by character types , such as hiragana and katakana both are phonetic characters in Japanese , Chinese characters , English alphabets and numbers .", "Tables 2 , 3 and 4 show the execution time , accuracy4 , and Q size of extracted subsets , by changing \u03c3 from 0 . 01 to 0 . 0005 .", "The PKI leads to about 2 to 12 times improvements over the PKB .", "In JDP , the improvement is significant .", "This is because B , the average of h i over all items i F , is relatively small in JDP .", "The improvement significantly depends on the sparsity of the given support examples .", "The improvements of the PKE are more significant than the PKI .", "The running time of the PKE is 30 to 300 times faster than the PKB , when we set an appropriate \u03c3 , e . g . , \u03c3 0 . 005 for EBC and JWS , \u03c3 0 . 0005 for JDP .", "In these settings , we could preserve the final accuracies for test data .", "The PKE with a Cubic Kernel tends to make Q large e . g . , Q 2 . 32 million for JWS , Q 8 . 26 million for JDP .", "To reduce the size of Q , we examined simple frequency based pruning experiments .", "Our extension is to simply give a prior threshold \u03be 1 , 2 , 3 , 4 . . . , and erase all subsets which occur in less than \u03be support examples .", "The calculation of frequency can be similarly conducted by the PrefixSpan algorithm .", "Tables 5 and 6 show the results of frequency based pruning , when we fix \u03c3 0 . 005 for JWS , and \u03c3 0 . 0005 for JDP .", "In JDP , we can make the size of set Q about one third of the original size .", "This reduction gives us not only a slight speed increase but an improvement of accuracy 89 . 29 89 . 34 .", "Frequency based pruning allows us to remove subsets that have large weight and small frequency .", "Such subsets may be generated from errors or special outliers in the training examples , which sometimes cause an overfitting in training .", "In JWS , the frequency based pruning does not work well .", "Although we can reduce the size of Q by half , the accuracy is also reduced 97 . 94 97 . 83 .", "It implies that , in JWS , features even with frequency of one contribute to the final decision hyperplane . classification ; building Kx K 1 2 classifiers considering all pairs of classes , and final class decision was given by majority voting .", "The values in this column are averages over all pairwise classifiers .", "There have been several studies for efficient classification of SVMs .", "Isozaki et al . propose an XQK eXpand the Quadratic Kernel which can make their Named Entity recognizer drastically fast Isozaki and Kazawa , 2002 .", "XQK can be subsumed into PKE .", "Both XQK and PKE share the basic idea ; all feature combinations are explicitly expanded and we convert the kernel based classifier into a simple linear classifier .", "The explicit difference between XQK and PKE is that XQK is designed only for Quadratic Kernel .", "It implies that XQK can only deal with feature combination of size up to two .", "On the other hand , PKE is more general and can also be applied not only to the Quadratic Kernel but also to the general style of polynomial kernels 1 X Y d . In PKE , there are no theoretical constrains to limit the size of combinations .", "In addition , Isozaki et al . did not mention how to expand the feature combinations .", "They seem to use a naive exhaustive method to expand them , which is not always scalable and efficient for extracting three or more feature combinations .", "PKE takes a basket mining approach to enumerating effective feature combinations more efficiently than their exhaustive method .", "We focused on a Polynomial Kernel of degree d , which has been widely applied in many tasks in NLP and can attain feature combination that is crucial to improving the performance of tasks in NLP .", "Then , we introduced two fast classification algorithms for this kernel .", "One is PKI Polynomial Kernel Inverted , which is an extension of Inverted Index .", "The other is PKE Polynomial Kernel Expanded , where all feature combinations are explicitly expanded .", "The concept in PKE can also be applicable to kernels for discrete data structures , such as String Kernel Lodhi et al . , 2002 and Tree Kernel Kashima and Koyanagi , 2002 ; Collins and Duffy , 2001 .", "For instance , Tree Kernel gives a dot product of an ordered tree , and maps the original ordered tree onto its all sub tree space .", "To apply the PKE , we must efficiently enumerate the effective sub trees from a set of support examples .", "We can similarly apply a sub tree mining algorithm Zaki , 2002 to this problem .", "\u00b5d \u00b5"], "summary_lines": ["Fast Methods For Kernel-Based Text Analysis\n", "Kernel-based learning (e.g., Support Vector Machines) has been successfully applied to many hard problems in Natural Language Processing (NLP).\n", "In NLP, although feature combinations are crucial to improving performance, they are heuristically selected.\n", "Kernel methods change this situation.\n", "The merit of the kernel methods is that effective feature combination is implicitly expanded without loss of generality and increasing the computational costs.\n", "Kernel-based text analysis shows an excellent performance in terms in accuracy; however, these methods are usually too slow to apply to large-scale text analysis.\n", "In this paper, we extend a Basket Mining algorithm to convert a kernel-based classifier into a simple and fast linear classifier.\n", "Experimental results on English BaseNP Chunking, Japanese Word Segmentation and Japanese Dependency Parsing show that our new classifiers are about 30 to 300 times faster than the standard kernel-based classifiers.\n", "We propose polynomial kernel inverted (PKI).\n", "PKI - Inverted Indexing, stores for each feature the support vectors in which it appears.\n", "The PKE approach uses a basket mining approach to prune many features from the expansion.\n", "An extension of the PrefixSpan algorithm (Pei et al, 2001) is used to efficiently mine the features in a low degree polynomial kernel space.\n"]}
{"article_lines": ["First Order Probabilistic Models for Coreference Resolution", "Traditional noun phrase coreference resolution systems represent features only of pairs of noun phrases .", "In this paper , we propose a machine learning method enables features over noun phrases , resulting in a first order probabilistic model for coreference .", "We outline a set of approximations that make this approach practical , and apply our method to the ACE coreference dataset , achieving a 45 error reduction over a comparable method that only considers features of pairs of noun phrases .", "This result demonstrates an example of how a firstorder logic representation can be incorporated into a probabilistic model and scaled efficiently .", "Noun phrase coreference resolution is the problem of clustering noun phrases into anaphoric sets .", "A standard machine learning approach is to perform a set of independent binary classifications of the form Is mention a coreferent with mention b ? This approach of decomposing the problem into pairwise decisions presents at least two related difficulties .", "First , it is not clear how best to convert the set of pairwise classifications into a disjoint clustering of noun phrases .", "The problem stems from the transitivity constraints of coreference If a and b are coreferent , and b and c are coreferent , then a and c must be coreferent .", "This problem has recently been addressed by a number of researchers .", "A simple approach is to perform the transitive closure of the pairwise decisions .", "However , as shown in recent work McCallum and Wellner , 2003 ; Singla and Domingos , 2005 , better performance can be obtained by performing relational inference to directly consider the dependence among a set of predictions .", "For example , McCallum and Wellner 2005 apply a graph partitioning algorithm on a weighted , undirected graph in which vertices are noun phrases and edges are weighted by the pairwise score between noun phrases .", "A second and less studied difficulty is that the pairwise decomposition restricts the feature set to evidence about pairs of noun phrases only .", "This restriction can be detrimental if there exist features of sets of noun phrases that cannot be captured by a combination of pairwise features .", "As a simple example , consider prohibiting coreferent sets that consist only of pronouns .", "That is , we would like to require that there be at least one antecedent for a set of pronouns .", "The pairwise decomposition does not make it possible to capture this constraint .", "In general , we would like to construct arbitrary features over a cluster of noun phrases using the full expressivity of first order logic .", "Enabling this sort of flexible representation within a statistical model has been the subject of a long line of research on first order probabilistic models Gaifman , 1964 ; Halpern , 1990 ; Paskin , 2002 ; Poole , 2003 ; Richardson and Domingos , 2006 .", "Conceptually , a first order probabilistic model can be described quite compactly .", "A configuration of the world is represented by a set of prediChoosing the closest preceding phrase is common because nearby phrases are a priori more likely to be coreferent .", "We refer to the training and inference methods described in this section as the Pairwise Model .", "where ZX ; is a normalizer that sums over the two settings of yj .", "Note that this model gives us the representational power of recently proposed Markov logic networks Richardson and Domingos , 2006 ; that is , we can construct arbitrary formulae in first order logic to characterize the noun coreference task , and can learn weights for instantiations of these formulae .", "However , naively grounding the corresponding Markov logic network results in a combinatorial explosion of variables .", "Below we outline methods to scale training and prediction with this representation .", "As in the Pairwise Model , we must decide how to sample training examples and how to combine independent classifications at testing time .", "It is important to note that by moving to the First Order Logic Model , the number of possible predictions has increased exponentially .", "In the Pairwise Model , the number of possible y variables is O x 2 , where x is the set of noun phrases .", "In the First Order Logic Model , the number of possible y variables is O 21 quot ; l There is a y variable for each possible element of the powerset of x .", "Of course , we do not enumerate this set ; rather , we incrementally instantiate y variables as needed during prediction .", "A simple method to generate training examples is to sample positive and negative cluster examples uniformly at random from the training data .", "Positive examples are generated by first sampling a true cluster , then sampling a subset of that cluster .", "Negative examples are generated by sampling two positive examples and merging them into the same cluster .", "At testing time , we perform standard greedy agglomerative clustering , where the score for each merger is proportional to the probability of the newly formed clustering according to the model .", "Clustering terminates when there exists no additional merge that improves the probability of the clustering .", "We refer to the system described in this section as First Order Uniform .", "In this section we propose two enhancements to the training procedure for the First Order Uniform model .", "First , because each training example consists of a subset of noun phrases , the number of possible training examples we can generate is exponential in the number of noun phrases .", "We propose an errordriven sampling method that generates training examples from errors the model makes on the training data .", "The algorithm is as follows Given initial parameters A , perform greedy agglomerative clustering on training document i until an incorrect cluster is formed .", "Update the parameter vector according to this mistake , then repeat for the next training document .", "This process is repeated for a fixed number of iterations .", "Exactly how to update the parameter vector is addressed by the second enhancement .", "We propose modifying the optimization criterion of training to perform ranking rather than classification of clusters .", "Consider a training example cluster with a negative label , indicating that not all of the noun phrases it contains are coreferent .", "A classification training algorithm will penalize all the features associated with this cluster , since they correspond to a negative example .", "However , because there may exists subsets of the cluster that are coreferent , features representing these positive subsets may be unjustly penalized .", "To address this problem , we propose constructing training examples consisting of one negative examWe propose augmenting the Pairwise Model to enable classification decisions over sets of noun phrases .", "Given a set of noun phrases xj xi , let the binary random variable yj be 1 if all the noun phrases xi E xj are coreferent .", "The features fk and weights Ak are defined as before , but now the features can represent arbitrary attributes over the entire set xj .", "This allows us to use the full flexibility of first order logic to construct features about sets of nouns .", "The First Order Logic Model is ple and one nearby positive example .", "In particular , when agglomerative clustering incorrectly merges two clusters , we select the resulting cluster as the negative example , and select as the positive example a cluster that can be created by merging other existing clusters . 1 We then update the weight vector so that the positive example is assigned a higher score than the negative example .", "This approach allows the update to only penalize the difference between the two features of examples , thereby not penalizing features representing any overlapping coreferent clusters .", "To implement this update , we use MIRA Margin Infused Relaxed Algorithm , a relaxed , online maximum margin training algorithm Crammer and Singer , 2003 .", "It updates the parameter vector with two constraints 1 the positive example must have a higher score by a given margin , and 2 the change to A should be minimal .", "This second constraint is to reduce fluctuations in A .", "Let s A , xj be the unnormalized score for the positive example and s A , xk be the unnormalized score of the negative example .", "Each update solves the following 1Of the possible positive examples , we choose the one with the highest probability under the current model to guard against large fluctuations in parameter updates fc Figure 3 An example noun coreference factor graph for the First Order Model in which factors f , model the coreference between sets of nouns , and ft enforce the transitivity among related decisions .", "Here , the additional node y123 indicates whether nouns x1 , x2 , x3 are all coreferent .", "The number of y variables increases exponentially in the number of x variables .", "In this case , MIRA with a single constraint can be efficiently solved in one iteration of the Hildreth and D Esopo method Censor and Zenios , 1997 .", "Additionally , we average the parameters calculated at each iteration to improve convergence .", "We refer to the system described in this section as First Order MIRA .", "In this section , we describe the Pairwise and FirstOrder models in terms of the factor graphs they approximate .", "For the Pairwise Model , a corresponding undirected graphical model can be defined as where Zx is the input dependent normalizer and factor fc parameterizes the pairwise noun phrase compatibility as fc yij , xij exp Ek \u03bbkfk yij , xij .", "Factor ft enforces the transitivity constraints by ft oc if transitivity is not satisfied , 1 otherwise .", "This is similar to the model presented in McCallum and Wellner 2005 .", "A factor graph for the Pairwise Model is presented in Figure 2 for three noun phrases .", "For the First Order model , an undirected graphical model can be defined as where Zx is the input dependent normalizer and factor fc parameterizes the cluster wise noun phrase compatibility as fc yj , xj exp Ek \u03bbkfk yj , xj .", "Again , factor ft enforces the transitivity constraints by ft oc if transitivity is not satisfied , 1 otherwise .", "Here , transitivity is a bit more complicated , since it also requires that if yj 1 , then for any subset xk C_ xj , yk 1 .", "A factor graph for the First Order Model is presented in Figure 3 for three noun phrases .", "The methods described in Sections 2 , 3 and 4 can be viewed as estimating the parameters of each factor fc independently .", "This approach can therefore be viewed as a type of piecewise approximation of exact parameter estimation in these models Sutton and McCallum , 2005 .", "Here , each fc is a piece of the model trained independently .", "These pieces are combined at prediction time using clustering algorithms to enforce transitivity .", "Sutton and McCallum 2005 show that such a piecewise approximation can be theoretically justified as minimizing an upper bound of the exact loss function .", "We apply our approach to the noun coreference ACE 2004 data , containing 443 news documents with 28 , 135 noun phrases to be coreferenced .", "336 documents are used for training , and the remainder for testing .", "All entity types are candidates for coreference pronouns , named entities , and nominal entities .", "We use the true entity segmentation , and parse each sentence in the corpus using a phrase structure grammar , as is common for this task .", "We follow Soon et al . 2001 and Ng and Cardie 2002 to generate most of our features for the Pairwise Model .", "These include The First Order Model includes the following features Enumerate each pair of noun phrases and compute the features listed above .", "All X is true if all pairs share a feature X , Most True X is true if the majority of pairs share a feature X , and Most False X is true if most of the pairs do not share feature X .", "In addition to the listed features , we also include conjunctions of size 2 , for example Genders match AND numbers match .", "We use the B3 algorithm to evaluate the predicted coreferent clusters Amit and Baldwin , 1998 .", "B3 is common in coreference evaluation and is similar to the precision and recall of coreferent links , except that systems are rewarded for singleton clusters .", "For each noun phrase xi , let ci be the number of mentions in xi s predicted cluster that are in fact coreferent with xi including xi itself .", "Precision for xi is defined as ci divided by the number of noun phrases in xi s cluster .", "Recall for xi is defined as the ci divided by the number of mentions in the gold standard cluster for xi .", "F1 is the harmonic mean of recall and precision .", "In addition to Pairwise , First Order Uniform , and First Order MIRA , we also compare against Pairwise MIRA , which differs from First Order MIRA only by the fact that it is restricted to pairwise features . ence .", "FIRST ORDER MIRA is our proposed model that takes advantage of first order features of the data and is trained with error driven and rank based methods .", "We see that both the first order features and the training enhancements improve performance consistently . wise Model in F1 measure for both standard training and error driven training .", "We attribute some of this improvement to the capability of the First Order model to capture features of entire clusters that may indicate some phrases are not coreferent .", "Also , we attribute the gains from error driven training to the fact that training examples are generated based on errors made on the training data .", "However , we should note that there are also small differences in the feature sets used for error driven and standard training results .", "Error analysis indicates that often noun xi is correctly not merged with a cluster xj when xj has a strong internal coherence .", "For example , if all 5 mentions of France in a document are string identical , then the system will be extremely cautious of merging a noun that is not equivalent to France into xj , since this will turn off the All String Match feature for cluster xj .", "To our knowledge , the best results on this dataset were obtained by the meta classification scheme of Ng 2005 .", "Although our train test splits may differ slightly , the best B Cubed F1 score reported in Ng 2005 is 69 . 3 , which is considerably lower than the 79 . 3 obtained with our method .", "Also note that the Pairwise baseline obtains results similar to those in Ng and Cardie 2002 .", "There has been a recent interest in training methods that enable the use of first order features Paskin , 2002 ; Daum e III and Marcu , 2005b ; Richardson and Domingos , 2006 .", "Perhaps the most related is learning as search optimization LASO Daum e III and Marcu , 2005b ; Daum e III and Marcu , 2005a .", "Like the current paper , LASO is also an error driven training method that integrates prediction and training .", "However , whereas we explicitly use a ranking based loss function , LASO uses a binary classification loss function that labels each candidate structure as correct or incorrect .", "Thus , each LASO training example contains all candidate predictions , whereas our training examples contain only the highest scoring incorrect prediction and the highest scoring correct prediction .", "Our experiments show the advantages of this ranking based loss function .", "Additionally , we provide an empirical study to quantify the effects of different example generation and loss function decisions .", "Collins and Roark 2004 present an incremental perceptron algorithm for parsing that uses early update to update the parameters when an error is encountered .", "Our method uses a similar early update in that training examples are only generated for the first mistake made during prediction .", "However , they do not investigate rank based loss functions .", "Others have attempted to train global scoring functions using Gibbs sampling Finkel et al . , 2005 , message propagation , Bunescu and Mooney , 2004 ; Sutton and McCallum , 2004 , and integer linear programming Roth and Yih , 2004 .", "The main distinctions of our approach are that it is simple to implement , not computationally intensive , and adaptable to arbitrary loss functions .", "There have been a number of machine learning approaches to coreference resolution , traditionally factored into classification decisions over pairs of nouns Soon et al . , 2001 ; Ng and Cardie , 2002 .", "Nicolae and Nicolae 2006 combine pairwise classification with graph cut algorithms .", "Luo et al . 2004 do enable features between mention cluster pairs , but do not perform the error driven and ranking enhancements proposed in our work .", "Denis and Baldridge 2007 use a ranking loss function for pronoun coreference ; however the examples are still pairs of pronouns , and the example generation is not error driven .", "Ng 2005 learns a meta classifier to choose the best prediction from the output of several coreference systems .", "While in theory a metaclassifier can flexibly represent features , they do not explore features using the full flexibility of firstorder logic .", "Also , their method is neither errordriven nor rank based .", "McCallum and Wellner 2003 use a conditional random field that factors into a product of pairwise decisions about pairs of nouns .", "These pairwise decisions are made collectively using relational inference ; however , as pointed out in Milch et al . 2004 , this model has limited representational power since it does not capture features of entities , only of pairs of mention .", "Milch et al . 2005 address these issues by constructing a generative probabilistic model , where noun clusters are sampled from a generative process .", "Our current work has similar representational flexibility as Milch et al . 2005 but is discriminatively trained .", "We have presented learning and inference procedures for coreference models using first order features .", "By relying on sampling methods at training time and approximate inference methods at testing time , this approach can be made scalable .", "This results in a coreference model that can capture features over sets of noun phrases , rather than simply pairs of noun phrases .", "This is an example of a model with extremely flexible representational power , but for which exact inference is intractable .", "The simple approximations we have described here have enabled this more flexible model to outperform a model that is simplified for tractability .", "A short term extension would be to consider features over entire clusterings , such as the number of clusters .", "This could be incorporated in a ranking scheme , as in Ng 2005 .", "Future work will extend our approach to a wider variety of tasks .", "The model we have described here is specific to clustering tasks ; however a similar formulation could be used to approach a number of language processing tasks , such as parsing and relation extraction .", "These tasks could benefit from first order features , and the present work can guide the approximations required in those domains .", "Additionally , we are investigating more sophisticated inference algorithms that will reduce the greediness of the search procedures described here .", "We thank Robert Hall for helpful contributions .", "This work was supported in part by the Defense Advanced Research Projects Agency DARPA , through the Department of the Interior , NBC , Acquisition Services Division , under contract NBCHD030010 , in part by U . S . Government contract NBCH040171 through a subcontract with BBNT Solutions LLC , in part by The Central Intelligence Agency , the National Security Agency and National Science Foundation under NSF grant IIS 0326249 , in part by Microsoft Live Labs , and in part by the Defense Advanced Research Projects Agency DARPA under contract HR0011 06 C 0023 .", "Any opinions , findings and conclusions or recommendations expressed in this material are the author s and do not necessarily reflect those of the sponsor ."], "summary_lines": ["First-Order Probabilistic Models for Coreference Resolution\n", "Traditional noun phrase coreference resolution systems represent features only of pairs of noun phrases.\n", "In this paper, we propose a machine learning method that enables features over sets of noun phrases, resulting in a first-order probabilistic model for coreference.\n", "We outline a set of approximations that make this approach practical, and apply our method to the ACE coreference dataset, achieving a 45% error reduction over a comparable method that only considers features of pairs of noun phrases.\n", "This result demonstrates an example of how a first-order logic representation can be incorporated into a probabilistic model and scaled efficiently.\n", "We present a system which uses an online learning approach to train a classifier to judge whether two entities are coreferential or not.\n", "We introduce a first-order probabilistic model which implements features over sets of mentions and thus operates directly on entities.\n"]}
{"article_lines": ["Text Translation Alignment", "No Figure 4 SAT after where 254 is a translation of the latter part of 218 and the early part of 219 When a proton strikes a gas nucleus , it produces three kinds of pion , of which one kind decays into two gamma rays .", "The gamma rays travel close to the original trajectory of the proton , and the model predicts they will be beamed toward the earth at just two points on the pulsars orbit around the companion star .", "Trifft em Proton auf einen Atomkern in dieser Gashfille , werden drei Arten von Pionen erzeugt .", "Die neutralen Pionen zerfallen in jeweils zwei Gammaquanten , die sich beinahe in dieselbe Richtung wie das urspriingliche Proton bewegen .", "Nach der Modellvorstellung gibt es gerade zwei Positionen im Umlauf des Pulsars urn semen Begleitstern , bei denen die Strahlung in Richtung zum Beobachter auf der Erde ausgesandt wird .", "Another example is provided by English sentences 19 and 20 , which appear in German as sentences 21 and 22 .", "However the latter part of English sentence 19 is in fact transferred to sentence 22 in the German .", "This is also unmistakable in the final results .", "Notice also , in this example , that the definition of quot ; photon quot ; has become a parenthetical expression at the beginning of the second German sentence , a fact which is not reflected .", "The other end of the cosmic ray energy spectrum is defined somearbitrarily any quantum greater than electron volts arriving from space is considered a cosmic ray .", "The definition encompasses not only particles but also gamma ray photons , which are quanta of electromagnetic radiation .", "2 4 4 4 5 ; 136 Martin Kay and Martin Riischeisen Text Translation Alignment Le Correctness of sentence alignment in the various passes of the algorithm .", "Pass Correctness Coverage Constraint in SAT of SAT by AST 1 100 12 4 2 100 47 17 3 100 89 38 4 99 . 7 96 41 Das untere Ende des Spektrums der kosmischen Strahlen ist verhaltnismai3ig unscharf definiert .", "Jedes Photon Quant der elektromagnetischen Strahlung Teilchen mit einer Energie von mehr als Elektronenvolt , das aus dem Weltraum eintrifft , bezeichnet man als kosmischen Strahl . frequently occurred in our data that sentences that separated by colons or semicolons in the original appeared as completely distinct sentences in the German translation .", "Indeed , the common usage in the two languages would probably have been better represented if we had treated colons and semicolons as sentence separators , along with periods , question marks , and the like .", "There are , of course , situations in which these punctuation marks are used in other ways , but they are considerably less frequent and , in any case , it seems that our program would almost always make the right associations .", "An example involving the colon is to be found in sentence 142 of the original , translated as sentences 163 and 164 The absorption lines established a lower limit on the distance of Cygnus X 3 it must be more distant than the farthest hydrogen cloud , which is believed to lie about 37 , 000 light years away , near the edge of the galaxy . dieser Absorptionslinie kann eine untere Grenze der Entvon Cygnus X bestimmen .", "Die Quelle mu13 jenseits am weitesten entfernten Wasserstoff Wolke sein , also weiter als ungefahr 37000 Lichtjahre entfernt , am Rande der Milchstrai3e . sentence 197 , containing semicolon , is translated by German sentences 228 and 229 The estimate is conservative ; because it is based on the gamma rays observed arriving at the earth , it does not take into account the likelihood that Cygnus X emits cosmic rays in all directions .", "Dies ist eine vorsichtige Absch5tzung .", "Sie ist nur aus den Gammastrahlen Daten abgeleitet , die auf der Erde gemessen werden ; daI3 Cygnus X 3 wahrscheirtlich kosmische Strahlung in alle Richtungen aussendet , ist dabei noch nicht beriicksichtigt .", "137 Computational Linguistics Volume 19 , Number 1 German Sentence No CA e tc ' x 10 20 30 40 50 English Sentence No Figure 5 alignment of the first of the test texts true alignment dots and hypothesis of the SAT after the first pass circles and after the second pass crosses .", "Table 3 summarizes the accuracy of the algorithm as a function of the number of passes .", "The thresholded SAT is evaluated by two criteria the number of correct by the total number alignments , and since the SAT does not necessarily give an alignment for every sentence the coverage , i . e . , the number of sentences with at least one entry relative to the total number of sentences .", "An alignment is said to be correct if the SAT contains exactly the numbers of the sentences that are complete or partial translations of the original sentence .", "The coverage of 96 of the SAT in pass 4 is as much as one would expect , since the remaining nonaligned sentences are one zero alignments , most of them due to the German subheadings that are not part of the English version .", "The table also shows that the AST always provides a significant number of candidates for alignment with each sentence before a pass the fourth column gives the number of true sentence alignments relative to the total number of candidates in the AST .", "Recall that the final alignment is always a subset of the hypotheses in the AST in every preceding pass .", "Figure 5 shows the true sentence alignment for the first 50 sentences dots , and how the algorithm discovered them in the first pass , only a few sentences are set into correspondence circles ; after the second pass crosses already almost half of the correspondences are found .", "Note that there are no wrong alignments in the first two passes .", "In the third pass , almost all of the remaining alignments are found for the first 50 sentences in the figure all , and a final pass usually completes the alignment .", "Our algorithm produces very favorable results when allowed to converge gradually .", "Processing time in the original LISP implementation was high , typically several hours for each pass .", "By trading CPU time for memory massively , the time needed by a C implementation on a Sun 4 75 was reduced to 1 . 7 mm for the first pass , 0 . 8 mm for the second , and 0 . 5 min for the third pass in an application to this pair of articles .", "Initialization , i . e . , reading the files and building up the data structures , takes another 0 . 6 min in the beginning .", "It should be noted that a naive implementation of 138 Martin Kay and Martin Roscheisen Text Translation Alignment the algorithm without using the appropriate data structures can easily lead to times that are a factor of 30 higher and do not scale up to larger texts .", "The application of our method to a text that we put together from the Hansard corpus had essentially no problem in identifying the correct sentence alignment in a process of five passes .", "The alignments for the first 1000 sentences of the English text were checked by hand , and seven errors were found ; five of them occurred in sentences where sentence boundaries were not correctly identified by the program of periods that did not mark a sentence boundary and were identified such by very simple preprocessing program .", "The other two errors involved two short sentences for which the SAT did not give an alignment .", "Processing time increased essentially linearly per pass the first pass took 8 . 3 min , the second 3 . 2 mm , and it further decreased until the last pass , which took 2 . 1 min .", "Initialization took 4 . 2 min .", "Note that the error rate depends crucially on the kind of quot ; annealing schedule quot ; used if the thresholds that allow a word pair in the WAT to influence the SAT are lowered too fast , only a few passes are needed , but accuracy deteriorates .", "For example , in an application where the process terminated after only three passes , the accuracy was only in the eighties estimated on the basis of the first 120 sentences of the English Hansard text checked by hand .", "Since processing time after the first pass is usually already considerably lower , we have found that a high accuracy can safely be attained when more passes are allowed than are actually necessary .", "In order to evaluate the sensitivity of the algorithm to the lengths of the texts that are to be aligned , we applied it to text samples that ranged in length from 10 to 1000 sentences , and examined the accuracy of the WAT after the first pass ; that is , more precisely , the number of word pairs in the WAT that are valid translations relative to the total number of word pairs with a similarity of not less than 0 . 7 the measurements are cross validated over different texts .", "The result is that this accuracy increases asymptotically to 1 with the text length , and is already higher than 80 for text length of 100 sentences which sufficient to reach an almost perfect alignment in the end .", "Roughly speaking , the accuracy is almost 1 for texts longer than 150 sentences , and around 0 . 5 for text length in the lower range from 20 to 60 .", "In other words , texts of a length of more than 150 sentences are suitable to be processed in this way ; text fragments shorter than 80 sentences do not have a high proportion of correct word pairs in the first WAT , but further experiments showed that the final alignment for texts of this length is , on average , again almost perfect the drawback of a less accurate initial WAT is apparently largely compensated for by the fact that the AST is also narrower for these texts ; however , the variance in the alignment accuracies is significantly higher .", "Related Work Since we addressed the text translation alignment problem in 1988 , a number of researchers , among them Gale and Church 1991 and Brown , Lai , and Mercer 1991 , have worked on the problem .", "Both methods are based on the observation that the length of text unit is highly correlated to the length of the translation of this unit , no matter whether length is measured in number of words or in number of characters see Figure 6 .", "Consequently , they are both easier to implement than ours , though not necessarily more efficient .", "The method of Brown , Lai , and Mercer 1991 is based on a hidden Markov model for the generation of aligned pairs of corpora , whose parameters are estimated from a large text .", "For an application of this method to the Canadian Hansard , good results are reported .", "However , the problem was also considerably facilitated by the way the implementation made use of Hansard specific comments 139 Linguistics Volume 19 , Number German Length in wards CO German Length in chars 40 60 80 120 200 400 600 800 English Length in words English Length in chars Figure 6 Lengths of Aligned Paragraphs are Correlated Robust regression between lengths of aligned paragraphs .", "Left length measured in words .", "Right length measured in characters . and annotations these are used in a preprocessing step to find anchors for sentence alignment such that , on average , there are only ten sentences in between .", "Moreover , corpus is for the near literalness of its translations , and it is therefore unclear to what extent the good results are due to the relative ease of the problem .", "This would be an important consideration when comparing various algorithms ; when the algorithms are actually applied , it is clearly very desirable to incorporate as much prior knowledge say , on potential anchors as possible .", "Moreover , long texts can almost always be expected to contain natural anchors , such as chapter section headings , at which to make an priori Gale and Church 1991 note that their method performed considerably better when lengths of sentences were measured in number of characters instead of in number of words .", "Their method is based on a probabilistic model of the distance between and a dynamic programming algorithm is used to minimize the total aligned Their implementation assumes that character in one language gives rise to , on average , one character in the other language . '", "In our texts , one character in English on average gives rise to somewhat more than 1 . 2 characters in German , and the correlation between the lengths in characters of aligned paragraphs in the two languages was with 0 . 952 lower than the 0 . 991 that are menin Gale and Church 1991 , which supports our impression that the we used are hard texts to align , but it is not clear to what extent this would deteriorate the results .", "In applications to economic reports from the Union Bank of Switzerland , the method performs very well on simple alignments one to one , oneto two , but has at the moment problems with complex matches .", "The method has the 8 Recall that , in a similar way , we assumed in our implementation that one sentence in one language gives rise to , on average , n rn sentences in the other language see first footnote in Section 2 . 3 .", "140 Martin Kay and Martin Riischeisen Text Translation Alignment advantage of associating a score with pairs of sentences so that it is easy to extract a subset for which there is a high likelihood that the alignments are correct .", "Given the simplicity of the methods proposed by Brown , Lai , and Mercer and Gale and Church , either of them could be used as a heuristic in the construction of the initial AST in our algorithm .", "In the current version , the number of candidate sentence pairs that are considered in the first pass near the middle of a text contributes disproportionally to the cost of the computation .", "In fact , as we remarked earlier , the of this step is proposed modification would effectively make it linear .", "Future Work For most practical purposes , the alignment algorithm we have described produces very satisfactory results , even when applied to relatively free translations .", "There are doubtless many places in which the algorithm itself could be improved .", "For example , it is clear that the present method of building the SAT favors associations between long sentences , and this is not surprising , because there is more information in long sentences .", "But we have not investigated the extent of this bias and we do not therefore know it as appropriate .", "The present algorithm rests on being able to identify one to one associations between certain words , notably technical terms and proper names .", "It is clear from a brief inspection of Table 2 that very few correspondences are noticed among everyday words and , when they are , it is usually because those words also have precise technical uses .", "The very few exceptions include quot ; only quot ; quot ; nur quot ; and 'the quot ; quot ; die . quot ; The pair quot ; per quot ; quot ; pro quot ; might also qualify but if the languages afford any example of a scientific preposition , this is surely it .", "The most interesting further developments would be in the direction of loosening up this dependence on one to one associations both because this would present a very significant challenge and also because we are convinced that our present method identifies essentially all the significant one to one associations .", "There are two obvious kinds of looser associations that could be investigated .", "One would consist of connections between a single vocabulary item in one language and two or more in the other , or even between several items in one language and several in the other .", "The other would involve connections one one , one many , or many many between phrases or recurring sequences .", "We have investigated the first of these enough to satisfy ourselves that there is latent information on one to many associations in the text , and that it can be revealed by suitable extensions of our methods .", "However , it is clear that the combinatorial problems associated with this approach are severe , and pursuing it would require much fine tuning of the program and designing much more effective ways of indexing the most important data structures .", "The key to reducing the combinatorial explosion probably lies in using tables of similarities such as those the present algorithm uses to suggest combinations of items that would be worth considering .", "If such an approach could be made efficient enough , it is even possible that it would provide a superior way of solving the problem for which our heuristic methods of morphological analysis were introduced .", "Its superiority would come from the fact that it would not depend on words being formed by concatenation , but would also accommodate such phenomena as umlaut , ablaut , vowel harmony , and the nonconcatenative process of Semitic morphology .", "The problems of treating recurring sequences are less severe .", "Data structures , such as the Patricia tree Knuth 1973 ; pp .", "490 493 provide efficient means of identifying all such sequences and , once identified , the data they provide could be added to 141 Computational Linguistics Volume 19 , Number 1 the WAT much as we now add the results of morphological analysis .", "Needless to say , this would only allow for uninterrupted sequences .", "Any attempt to deal with discontinuous sequences would doubtless also involve great combinatorial problems .", "These avenues for further development are intriguing and would surely lead to interesting results .", "But it is unlikely that they would lead to much better sets of associations among sentences than are to be found in the SATs that our present program produces , and it was mainly these results that we were interested in from the outset .", "The other avenues we have mentioned concern improvements in the WAT which , for us , was always a secondary interest .", "We present an algorithm for aligning texts with their translations that is based only on internal evidence .", "The relaxation process rests on a notion of which word in one text corresponds to which word in the other text that is essentially based on the similarity of their distributions .", "It exploits a partial alignment of the word level to induce a maximum likelihood alignment of the sentence level , which is in turn used , in the next iteration , to refine the word level estimate .", "The algorithm appears to converge to the correct sentence alignment in only a few iterations .", "To align a text with a translation of it in another language is , in the terminology of this paper , to show which of its parts are translated by what parts of the second text .", "The result takes the form of a list of pairs of items words , sentences , paragraphs , or whatever from the two texts .", "A pair a , b is on the list if a is translated , in whole or in part , by b .", "If a , 17 and a , c are on the list , it is because a is translated partly by b , and partly by c . We say that the alignment is partial if only some of the items of the chosen kind from one or other of the texts are represented in the pairs .", "Otherwise , it is complete .", "It is notoriously difficult to align good translations on the basis of words , because it is often difficult to decide just which words in an original are responsible for a given one in a translation and , in any case , some words apparently translate morphological or syntactic phenomena rather than other words .", "However , it is relatively easy to establish correspondences between such words as proper nouns and technical terms , so that partial alignment on the word level is often possible .", "On the other hand , it is also easy to align texts and translations on the sentence or paragraph levels , for there is rarely much doubt as to which sentences in a translation contain the material contributed by a given one in the original .", "The growing interest in the possibility of automatically aligning Large texts is attested to by independent work that has been done on it since the first description of our methods was made available Kay and Roscheisen 1988 .", "In recent years it has been possible for the first time to obtain machine readable versions of large corpora of text with accompanying translations .", "The most striking example is the Canadian quot ; Hansard , quot ; the transcript of the proceedings of the Canadian parliament .", "Such bilingual corpora make it possible to undertake statistical , and other kinds of empirical , studies of translation on a scale that was previously unthinkable .", "Alignment makes possible approaches to partially , or completely , automatic translation based on a large corpus of previous translations that have been deemed acceptable .", "Perhaps the best known example of this approach is to be found in Sato and Nagao 199W .", "The method proposed there requires a database to be maintained of the syntactic structures of sentences together with the structures of the corresponding translations .", "This database is searched in the course of making a new translation for examples of previous sentences that are like the current one in ways that are relevant for the method .", "Another example is the completely automatic , statistical approach to translation taken by the research group at IBM Brown et al . 1990 , which takes a large corpus of text with aligned translations as its point of departure .", "It is widely recognized that one of the most important sources of information to which a translator can have access is a large body of previous translations .", "No dictionary or terminology bank can provide information of comparable value on topical matters of possibly intense though only transitory interest , or on recently coined terms in the target language , or on matters relating to house style .", "But such a body of data is useful only if , once a relevant example has been found in the source language , the corresponding passage can be quickly located in the translation .", "This is simple only if the texts have been previously aligned .", "Clearly , what is true of the translator is equally true of others for whom translations are a source of primary data , such as students of translation , the designers of translations systems , and lexicographers .", "Alignment would also facilitate the job of checking for consistency in technical and legal texts where consistency constitutes a large part of accuracy .", "In this paper , we provide a method for aligning texts and translations based only on internal evidence .", "In other words , the method depends on no information about the languages involved beyond what can be derived from the texts themselves .", "Furthermore , the computations on which it is based are straightforward and robust .", "The plan rests on a relationship between word and sentence alignments arising from the observation that a pair of sentences containing an aligned pair of words must themselves be aligned .", "It follows that a partial alignment on the word level could induce a much more complete alignment on the sentence level .", "A solution to the alignment problem consists of a subset of the Cartesian product of the sets of source and target sentences .", "The process starts from an initial subset excluding pairs whose relative positions in their respective texts is so different that the chance of their being aligned is extremely low .", "This potentially alignable set of sentences forms the basis for a relaxation process that proceeds as follows .", "An initial set of candidate word alignments is produced by choosing pairs of words that tend to occur in possibly aligned sentences .", "The idea is to propose a pair of words for alignment if they have similar distributions in their respective texts .", "The distributions of a pair of words are similar if most of the sentences in which the first word occurs are alignable with sentences in which the second occurs , and vice versa .", "The most apparently reliable of these word alignments are then used to induce a set of sentence alignments that will be a subset of the eventual result .", "A new estimate is now made of what sentences are alignable based on the fact that we are now committed to aligning certain pairs , Because sentence pairs are never removed from the set of alignments , the process converges to the point when no new ones can be found ; then it stops .", "In the next section , we describe the algorithm , In Section 3 we describe additions to the basic technique required to provide for morphology , that is , relatively superficial variations in the forms of words .", "In Section 4 we show the results of applying a program that embodies these techniques to an article from Scientific American and its German translation in Spektrurn der Wissenschaft .", "In Section 5 we discuss other approaches to the alignment problem that were subsequently undertaken by other researchers Gale and Church 1991 ; Brown , Lai , and Mercer 1991 .", "Finally , in Section 6 , we consider ways in which our present methods might be extended and improved ,", "The principal data structures used in the algorithm are the following Word Sentence Index WSI .", "One of these is prepared for each of the texts .", "It is a table with an entry for each different word in the text showing the sentences in which that word occurs .", "For the moment , we may take a word as being simply a distinct sequence of letters .", "If a word occurs more than once in a sentence , that sentence occurs on the list once for each occurrence .", "Alignable Sentence Table AST .", "This is a table of pairs of sentences , one from each text .", "A pair is included in the table at the beginning of a pass if that pair is a candidate for association by the algorithm in that pass .", "Word Alignment Table WAT .", "This is a list of pairs of words , together with similarities and frequencies in their respective texts , that have been aligned by comparing their distributions in the texts .", "Sentence Alignment Table SAT .", "This is a table that records for each pair of sentences how many times the two sentences were set in correspondence by the algorithm .", "Some additional data structures were used to improve performance in our implementation of the algorithm , but they are not essential to an understanding of the method as a whole .", "At the beginning of each cycle , an AST is produced that is expected to contain the eventual set of alignments , generally amongst others .", "It pairs the first and last sentences of the two texts with a small number of sentences from the beginning and end of the other text .", "Generally speaking , the closer a sentence is to the middle of the text , the larger the set of sentences in the other text that are possible correspondents for it .", "The next step is to hypothesize a set of pairs of words that are assumed to correspond based on similarities between their distributions in the two texts .", "For this purpose , a word in the first text is deemed to occur at a position corresponding to a word in the second text if they occur in a pair of sentences that is a member of the AST .", "Similarity of distribution is a function of the number of corresponding sentences in which they occur and the total number of occurrences of each .", "Pairs of words are entered in the WAT if the association between them is so close that it is not likely to be the result of a random event .", "In our algorithm , the closeness of the association is estimated on the basis of the similarity of their distributions and the total number of occurrences .", "The next step is to construct the SAT , which , in the last pass , will essentially become the output of the program as a whole .", "The idea here is to associate sentences that contain words paired in the WAT , giving preference to those word pairs that appear to be more reliable .", "Multiple associations are recorded .", "If there are to be further passes of the main body of the algorithm , a new AST is then constructed in light of the associations in the SAT .", "Associations that are supported some minimum number of times are treated just as the first and last sentences of the texts were initially ; that is , as places at which there is known to be a correspondence .", "Possible correspondences are provided for the intervening sentences by the same interpolation method initially used for all sentences in the middle of the texts .", "In preparation for the next pass , a new set of corresponding words is now hypothesized using distributions based on the new AST , and the cycle repeats .", "The main algorithm is a relaxation process that leaves at the end of each pass a new WAT and SAT , each presumably more refined than the one left at the end of the preceding pass .", "The input to the whole process consists only of the WSIs of the two texts .", "Before the first pass of the relaxation process , an initial AST is computed simply from the lengths of the two texts Construct Initial AST .", "If the texts contain m and n sentences respectively , then the table can be thought of as an m x n array of ones and zeros .", "The average number of sentences in the second text corresponding to a given one in the first text is n rn , and the average position of the sentence in the second text corresponding to the i th sentence in the first text is therefore i n m .", "In other words , the expectation is that the true correspondences will lie close to the diagonal .", "Empirically , sentences typically correspond one for one ; correspondences of one sentence to two are much rarer , and correspondences of one to three or more , though they doubtless occur , are very rare and were unattested in our data .", "The maximum deviation can be stochastically modeled as 0 NAT , the factor by which the standard deviation of a sum of n independent and identically distributed random variables multiplies We construct the initial AST using a function that pairs single sentences near the middle of the text with as many as 0 Vii sentences in the other text ; it is generously designed to admit all but the most improbable associations .", "Experience shows that because of this policy the results are highly insensitive to the particular function used to build this initial table . '", "The main body of the relaxation process consists of the following steps Build the WAT .", "For all sentences 5A in the first text , each word in sA is compared with each word in those sentences 53 of the second text that are considered as candidates for correspondence , Le . , for which 0 , 58 E AST .", "A pair of words is entered into the WAT if the distributions of the two words in their texts are sufficiently similar and if the total number of occurrences indicates that this pair is unlikely to be the result of a spurious match .", "Note that the number of comparisons of the words in two sentences is quadratic only in the number of words in a sentence , which can be assumed to be not a function of the length of the text .", "Because of the constraint on the maximum deviation from the diagonal as outlined above , the computational complexity of the algorithm is bound by 0 n 7 in each pass .", "Our definition of the similarity between a pair of words is complicated by the fact that the two texts have unequal lengths and that the AST allows more than one correspondence , which means that we cannot simply take the inner product of the vector representations of the word's occurrences .", "Instead , we use as a measure of similarity 3 where c is the number of corresponding positions , and NT x is the number of occurrences of the word x in the text T . This is essentially Dice's coefficient Rijsbergen 1979 .", "Technically , the value of c is the cardirtality of the largest set of pairs i , j such that Suppose that the word quot ; dog quot ; occurs in sentences 50 , 52 , 75 , and 200 of the English text , and quot ; Hund quot ; in sentences 40 and 180 of the German , and that the AST contains the pairs 50 , 40 , 52 , 40 , and 200 , 180 , among others , but not 75 , 40 .", "There are two sets that meet the requirements , namely 1 , 1 , 42 and 2 , 1 , 4 , 2 .", "The set 1 1 , 1 , 2 , 1 , 4 , 2 is excluded on the grounds that 1 , 1 and 2 , 1 overlap in the above sense the first occurrence of quot ; Hund quot ; is represented twice .", "In the example , the similarity would be computed as 4 _2 12 , regardless of the ambiguity between 1 , 1 and 2 , 1 .", "The result of the comparisons of the words in all of the sentences of one text with those in the other text is that the word pairs with the highest similarity are located .", "Comparing the words in a sentence of one text with those in a sentence of the other text carries with it an amortized cost of constant computational complexity , 4 if the usual memory processing tradeoff on serial machines is exploited by maintaining redundant data structures such as multiple hash tables and ordered indexed trees . '", "The next task is to determine for each word pair , whether it will actually be entered into the WAT the WAT is a sorted table where the more reliable pairs are put before less reliable ones .", "For this purpose , each entry contains , as well as the pair of words themselves , the frequencies of those words in their respective texts and the similarity between them .", "The closeness of the association between two words , and thus their rank in the WAT , is evaluated with respect to their similarity and the total number of their occurrences .", "To understand why similarity cannot be used alone , note that there are far more one frequency words than words of higher frequency .", "Thus , a pair of words with a similarity of 1 , each of them occurring only once , may well be the result of a random event .", "If such a pair was proposed for entry into the WAT it should only be added with a low priority .", "The exact stochastic relation is depicted in Figure 1 , where the probability is shown that a word of a frequency k that was aligned with a word in the other text with a certain similarity s is just the result of a random process . '", "Note that , for a high frequency word that has a high similarity with some other word right front corner , it is very unlikely negligible plateau height that this association has to be attributed to chance .", "On the other hand , low similarities back can easily be attained by just associating arbitrary words .", "Low frequency words because there are so many of them in a text can also achieve a high similarity with some other words without having to be related in an interesting way .", "This can be intuitively explained by the fact that the similarity of a high frequency word is based on a pattern made up of a large number of instances .", "It is therefore a pattern that is unlikely to be replicated by chance .", "Furthermore , since there are relatively few high frequency words , and they can only contract high similarities with other high frequency words , the number of possible correspondents for them is lower , and the chance of spurious associations is therefore less on these grounds also .", "Note that low frequency words with low similarity back left corner have also a low probability of being spuriously associated to some other word .", "This is because low frequency words can achieve a low similarity only with words of a high frequency , which in turn are rare in a text , and are therefore unlikely to be associated spuriously . '", "Our algorithm does not use all the detail in Figure 1 , but only a simple discrete heuristic a word pair whose similarity exceeds some threshold is assigned to one of two or three segments of the WAT , depending on the word frequency .", "A segment with words of higher frequency is preferred to lower frequency segments .", "Within each segment , the entries are sorted in order of decreasing similarity and , in case of equal similarities , in order of decreasing frequency .", "In terms of Figure 1 , we take a rectangle from the right front .", "We place the left boundary as far to the left as possible , because this is where most of the words are .", "Build the SAT .", "In this step , the correspondences in the WAT are used to establish a mapping between sentences of the two texts .", "In general , these new 6 The basis for this graph is an analytic derivation of the probability that a word with a certain frequency in a 300 sentence text matches some random pattern with a particular similarity .", "The analytic formula relies on word frequency data derived from a large corpus instead of on a stochastic model for word frequency distribution such as Zipf's law , which states that the frequency with which words occur in a text is indirectly proportional to the number of words with this frequency ; for a recent discussion of more accurate models , see also Baayen 19911 .", "Clearly , the figure is dependent on the state of the AST e . g . lower similarities become more acceptable as the AST becomes more and more narrow , but the thresholds relevant to our algorithm can be precomputed at compile time .", "The figure shown would be appropriate to pass 3 in our experiment .", "In the formula used , there are a few reasonable simplifications concerning the nature of the AST ; however , a Monte Carlo simulation that is exactly in accordance with our algorithm confirmed the depicted figure in every essential detail .", "7 This discussion could also be cast in an information theoretic framework using the notion of quot ; mutual information quot ; Fano 1961 , estimating the variance of the degree of match in order to find a frequency threshold see Church and Hanks 1990 .", "Likelihood that a word pair is a spurious match as a function of a word's frequency and its similarity with a word in the other text maximum 0 . 94 . associations are added to the ones inherited from the preceding pass .", "It is an obvious requirement of the mapping that lines of association should not cross .", "At the beginning of the relaxation process , the SAT is initialized such that the first sentences of the two texts , and the last sentences , are set in correspondence with one another , regardless of any words they may contain .", "The process that adds the remaining associations scans the WAT in order and applies a three part process to each pair v , w .", "Build a New AST .", "If there is to be another pass of the relaxation algorithm , a new AST must be constructed as input to it .", "This is based on the current SAT and is derived from it by supplying associations for sentences for which it provides none .", "The idea is to fill gaps between associated pairs of sentences in the same manner that the gap between the first and the last sentence was filled before the first pass .", "However , only sentence associations that are represented more than some minimum number of times in the SAT are transferred to the AST .", "In what follows , we will refer to these sentence pairs as anchors .", "As before , it is convenient to think of the AST as a rectangular array , even though it is represented more economically in the program .", "Consider a maximal sequence of empty AST entries , that is , a sequence of sentences in one text for which there are no associated sentences in the other , but which is bounded above and below by an anchor .", "The new associations that are added lie on and adjacent to the diagonal joining the two anchors .", "The distance from the diagonal is a function of the distance of the current candidate sentence pair and the nearest anchor .", "The function is the same one used in the construction of the initial AST .", "As we said earlier , the basic alignment algorithm treats words as atoms ; that is , it treats strings as instances of the same word if they consist of identical sequences of letters , and otherwise as totally different .", "The effect of this is that morphological variants of a word are not seen as related to one another .", "This might not be seen as a disadvantage in all circumstances .", "For example , nouns and verbs in one text might be expected to map onto nouns with the same number and verbs with the same tense much of the time .", "But this is not always the case and , more importantly , some languages make morphological distinctions that are absent in the other .", "German , for example , makes a number of case distinctions , especially in adjectives , that are not reflected in the morphology of English .", "For these reasons , it seems desirable to allow words to contract associations with other words both in the form in which they actually occur , and in a more normalized form that will throw them together with morphologically related other words in the text .", "The strategy we adopted was to make entries in the WSI , not only for maximal strings of alphabetic characters occurring in the texts , but also for other strings that could usefully be regarded as normalized forms of these .", "Clearly , one way to obtain normalized forms of words is to employ a fully fledged morphological analyzer for each of the languages .", "However , we were concerned that our methods should be as independent as possible of any specific facts about the languages being treated , since this would make them more readily usable .", "Furthermore , since our methods attend only to very gross features of the texts , it seemed unreasonable that their success should turn on a very fine analysis at any level .", "We argue that , by adding a guess as to how a word should be normalized to the WSI , we remove no associations that could have been formed on the basis of the original word , but only introduce the possibility of some additional associations .", "Also , it is unlikely that an incorrect normalization will contract any associations at all , especially in view of the fact that these forms , because they normalize several original forms , tend to occur more often .", "They will therefore rarely be misleading .", "For us , a normalized form of a word is always an initial or a final substring of that word no attention is paid to morphographemic or word internal changes .", "A word is broken into two parts , one of which becomes the normalized form , if there is evidence that the resulting prefix and suffix belong to a paradigm .", "In particular , both must occur as prefixes and suffixes of other forms .", "The algorithm proceeds in two stages .", "First a data structure , called the trie , is constructed in which information about the occurrences of potential prefixes and suffixes in the text is stored .", "Second , words are split , where the trie provides evidence for doing so , and one of the resulting parts is chosen as the normalization . information with strings of characters .", "It is particularly economical in situations where many of the strings of interest are substrings of others in the set .", "A trie is in fact a tree , with a branch at the root node for every character that begins a string in the set .", "To look up a string , one starts at the root , and follows the branch corresponding to its first character to another node .", "From there , the branch for the second character is followed to a third node , and so on , until either the whole string has been matched , or it has been discovered not to be in the set .", "If it is in the set , then the node reached after matching its last character contains whatever information the structure contains for it .", "The economy of the scheme lies in the fact that a node containing information about a string also serves as a point on the way to longer strings of which the given one is a prefix .", "In this application , two items of information are stored with a string , namely the number of textual words in which it occurs as a prefix and as a suffix .", "There is a function from potential break points in words to numbers whose value is maximized to choose the best point at which to break .", "If p and s are the potential prefix and suffix , respectively , and P p and S s are the number of words in the text in which they occur as such , the value of the function is kP p S s .", "The quantity k is introduced to enable us to prefer certain kinds of breaks over others .", "For the English and German texts used in our experiments , k length p so as to favor long prefixes on the grounds that both languages are primarily suffixing .", "If the function has the same value for more than one potential break point , the one farthest to the right is preferred , also for the reason that we prefer to maximize the lengths of prefixes .", "Once it has been decided to divide a word , and at what place , one of the two parts is selected as the putative canonical form of the word , namely , whichever is longer , and the prefix if both are of equal length .", "Finally , any other words in the same text that share the chosen prefix suffix are split at the corresponding place , and so assigned to the same canonical form .", "The morphological algorithm treats words that appear hyphenated in the text specially .", "The hyphenated word is treated as a unit , just as it appears , and so are the strings that result from breaking the word at the hyphens .", "In addition , the analysis procedure described above is applied to these components , and any putative normal forms found are also used .", "It is worth pointing out that we received more help from hyphens than one might normally expect in our analysis of the German texts because of a tendency on the part of the Spektrum der Wissenschaft translators , following standard practice for technical writing , of hyphenating compounds ,", "In this section , we show some of the results of our experiments with these algorithms , and also data produced at some of the intermediate stages .", "We applied the methods described here to two pairs of articles from Scientific American and their German translations in Spektrum der Wissenschaft see references .", "The English and German articles about human powered flight had 214 and 162 sentences , respectively ; the ones about cosmic rays contained 255 and 300 sentences , respectively .", "The first pair was primarily used to develop the algorithm and to determine the various parameters of the program .", "The performance of the algorithm was finally tested on the latter pair of articles .", "We chose these journals because of a general impression that the translations were of very high quality and were sufficiently quot ; free quot ; to be a substantial challenge for the algorithm .", "Furthermore , we expected technical translators to adhere to a narrow view of semantic accuracy in their work , and to rate the importance of this above stylistic considerations .", "Later we also give results for another application of our algorithm to a larger text of 1257 sentences that was put together from two days from the French English Hansard corpus .", "Table 1 shows the first 50 entries of the WAT after pass 1 of the algorithm .", "It shows part of the first section of the WAT lines 1 23 and the beginning of the second lines 24 50 .", "The first segment contains words or normalized forms with more than 7 occurrences and a similarity not less than 0 . 8 .", "Strings shown with a following hyphen are prefixes arising from the morphological procedure ; strings with an initial hyphen are suffixes .", "Naturally , some of the word divisions are made in places that do not accurately reflect linguistic facts .", "For example , English quot ; proto quot ; 1 comes from quot ; proton quot ; and quot ; protons quot ; ; German quot ; eilchen quot ; 17 is the normalization for words ending in quot ; teilchen quot ; and , in the same way , quot ; eistung quot ; 47 comes from quot ; leistung . quot ; Of these 50 word pairs , 42 have essentially the same meanings .", "We take it that quot ; erg quot ; and quot ; Joule , quot ; in line 4 , mean the same , modulo a change in units .", "Also , it is not unreasonable to associate pairs like quot ; primary quot ; quot ; sekunclaren quot ; 26 and quot ; electric quot ; quot ; Feld quot ; 43 , on the grounds that they tend to be used together .", "The pair quot ; rapid quot ; quot ; Pulsare quot ; 49 is made because a pulsar is a rapidly spinning neutron star and some such phrase occurs with it five out of six times .", "Notice , however , that the association quot ; pulsar quot ; quot ; Pulsar quot ; is also in table 6 .", "Furthermore , the German strings quot ; Pulsar quot ; and quot ; Pulsar quot ; are both given correct associations in the next pass lines 17 and 20 of Table 2 .", "The table shows two interesting effects of the morphological analysis procedure .", "The word quot ; shower quot ; is wrongly associated with the word quot ; Gammaquant quot ; 25 with a frequency of 6 , but the prefix quot ; shower quot ; is correctly associated with quot ; Luftschauer quot ; The SAT after pass 1 18 with a frequency of 20 .", "On the other hand , the incorrect association of quot ; element quot ; with quot ; usammensetzung quot ; 39 is on the basis of a normalized form for words ending in quot ; Zusammensetzung quot ; , whereas quot ; Zusammensetzung , quot ; unnormalized , is correctly associated with quot ; composition quot ; 35 .", "Totally unrelated words are associated in a few instances , as in quot ; Observatory quot ; quot ; diesem quot ; 24 , quot ; detectors quot ; quot ; primare quot ; 36 , and quot ; bright quot ; quot ; Astronona quot ; 48 .", "Of these only the second remains at the end of the third pass .", "The English quot ; Observatory quot ; is then properly associated with the German word quot ; Observatorium . quot ; At that stage , quot ; bright quot ; has no association .", "Figure 2 shows part of the SAT at the end of pass 1 of the relaxation cycle .", "Sentences in the English text and in the German text are identified by numbers on the abscissa and the ordinate respectively .", "Entries in the array indicate that the sentences are considered to correspond .", "The numbers show how often a particular association is supported , which is essentially equivalent to how many word pairs in the WAT support such an association .", "If there are no such numbers , then no associations have been found for it at this stage .", "For example , the association of English sentence 148 with German sentence 170 is supported by three different word pairs .", "It is already very striking how strongly occupied entries in this table constrain the possible entries in the unoccupied slots .", "Figure 3 shows part of the AST before pass 2 .", "This is derived directly from the material illustrated in Figure 2 .", "The abscissa gives the English sentence number and in direction of the ordinate the associated German sentences are shown bullet .", "Those sentence pairs in Figure 2 supported by at least three word pairs , namely those shown on lines 148 , 192 , 194 , and 196 , are assumed to be reliable , and they are the only associations shown for these sentences in Figure 3 .", "Candidate associations have been provided for the intervening sentences by the interpolation method described above .", "Notice that the greatest number of candidates are shown against sentences occurring midway between a pair assumed to have been reliably connected English sentence numbers 169 to 171 .", "Table 2 shows the first 100 entries of the WAT after pass 3 , where the threshold for the similarity was lowered to 0 . 5 .", "As we pointed out earlier , most of the incorrect associations in Table 1 have been eliminated .", "German quot ; MilchstraSe quot ; 19 is not a translation of the English quot ; galaxy , quot ; but the Milky Way is indeed a galaxy and quot ; the galaxy quot ; is sometimes used in place of quot ; Milky Way quot ; where the reference is clear .", "The association between quot ; period quot ; and quot ; Stunden quot ; 38 is of a similar kind .", "The words are strongly associated because of recurring phrases of the form quot ; in a 4 . 8 hour period . quot ; Figure 4 gives the SAT after pass 3 .", "It is immediately apparent , first , that the majority of the sentences have been associated with probable translations and , second , that many of these associations are very strongly supported .", "For example , note that the correspondence between English sentence 190 and German sentence 219 is supported 21 times .", "Using this table , it is in fact possible to locate the translation of a given English sentence to within two or three sentences in the German text , and usually more closely than that .", "However , some ambiguities remain .", "Some of the apparent anomalies come The AST before pass 2 . from stylistic differences in the way the texts were presented in the two journals .", "The practice of Scientific American is to collect sequences of paragraphs into a logical unit by beginning the first of them with an oversized letter .", "This is not done in Spektrum der Wissenschaf t , which instead provides a subheading at these points .", "This therefore appears as an insertion in the translation .", "Two such are sentences number 179 and 233 , but our procedure has not created incorrect associations for them .", "Recall that the alignment problem derives its interest from the fact that single sentences are sometimes translated as sequences of sentences and conversely .", "These cases generally stand out strongly in the output that our method delivers .", "For example , the English sentence pair 5 , 6 Yet whereas many of the most exciting advances in astronomy have come from the detailed analysis of X ray and radio sources , until recently the source of cosmic rays was largely a matter of speculation .", "They seem to come from everywhere , raining down on the earth from all directions at a uniform rate . is rendered in German by the single sentence 5 Dennoch blieben die Quellen der kosmischen Strahlung , die aus alien Richtungen gleichmaSig auf die Erde zu treffen scheint , bis vor kurzem reine Spekulation , wahrend einige der aufregendsten Fortschritte in der Astronomic aus dem detaillierten Studium von R\u00f6ntgen und Radiowellen herriihrten .", "The second English sentence becomes a relative clause in the German .", "More complex associations also show up clearly in the results .", "For example , English sentences 218 and 219 are translated by German sentences 253 , 254 , and 255 , The SAT after pass 3 . where 254 is a translation of the latter part of 218 and the early part of 219 When a proton strikes a gas nucleus , it produces three kinds of pion , of which one kind decays into two gamma rays .", "The gamma rays travel close to the original trajectory of the proton , and the model predicts they will be beamed toward the earth at just two points on the pulsars orbit around the companion star .", "Trifft em Proton auf einen Atomkern in dieser Gashfille , werden drei Arten von Pionen erzeugt .", "Die neutralen Pionen zerfallen in jeweils zwei Gammaquanten , die sich beinahe in dieselbe Richtung wie das urspriingliche Proton bewegen .", "Nach der Modellvorstellung gibt es gerade zwei Positionen im Umlauf des Pulsars urn semen Begleitstern , bei denen die Strahlung in Richtung zum Beobachter auf der Erde ausgesandt wird .", "Another example is provided by English sentences 19 and 20 , which appear in German as sentences 21 and 22 .", "However the latter part of English sentence 19 is in fact transferred to sentence 22 in the German .", "This is also unmistakable in the final results .", "Notice also , in this example , that the definition of quot ; photon quot ; has become a parenthetical expression at the beginning of the second German sentence , a fact which is not reflected .", "The other end of the cosmic ray energy spectrum is defined somewhat arbitrarily any quantum greater than 108 electron volts arriving from space is considered a cosmic ray .", "The definition encompasses not only particles but also gamma ray photons , which are quanta of electromagnetic radiation .", "Das untere Ende des Spektrums der kosmischen Strahlen ist verhaltnismai3ig unscharf definiert .", "Jedes Photon Quant der elektromagnetischen Strahlung oder Teilchen mit einer Energie von mehr als 108 Elektronenvolt , das aus dem Weltraum eintrifft , bezeichnet man als kosmischen Strahl .", "It frequently occurred in our data that sentences that were separated by colons or semicolons in the original appeared as completely distinct sentences in the German translation .", "Indeed , the common usage in the two languages would probably have been better represented if we had treated colons and semicolons as sentence separators , along with periods , question marks , and the like .", "There are , of course , situations in English in which these punctuation marks are used in other ways , but they are considerably less frequent and , in any case , it seems that our program would almost always make the right associations .", "An example involving the colon is to be found in sentence 142 of the original , translated as sentences 163 and 164 The absorption lines established a lower limit on the distance of Cygnus X 3 it must be more distant than the farthest hydrogen cloud , which is believed to lie about 37 , 000 light years away , near the edge of the galaxy .", "Aus dieser Absorptionslinie kann man eine untere Grenze der Entfernung von Cygnus X bestimmen .", "Die Quelle mu13 jenseits der am weitesten entfernten Wasserstoff Wolke sein , also weiter als ungefahr 37000 Lichtjahre entfernt , am Rande der Milchstrai3e .", "English sentence 197 , containing a semicolon , is translated by German sentences 228 and 229 The estimate is conservative ; because it is based on the gamma rays observed arriving at the earth , it does not take into account the likelihood that Cygnus X emits cosmic rays in all directions .", "Dies ist eine vorsichtige Absch5tzung .", "Sie ist nur aus den Gammastrahlen Daten abgeleitet , die auf der Erde gemessen werden ; daI3 Cygnus X 3 wahrscheirtlich kosmische Strahlung in alle Richtungen aussendet , ist dabei noch nicht beriicksichtigt .", "Sentence alignment of the first 50 sentences of the test texts true alignment dots and hypothesis of the SAT after the first pass circles and after the second pass crosses .", "Table 3 summarizes the accuracy of the algorithm as a function of the number of passes .", "The thresholded SAT is evaluated by two criteria the number of correct alignments divided by the total number of alignments , and since the SAT does not necessarily give an alignment for every sentence the coverage , i . e . , the number of sentences with at least one entry relative to the total number of sentences .", "An alignment is said to be correct if the SAT contains exactly the numbers of the sentences that are complete or partial translations of the original sentence .", "The coverage of 96 of the SAT in pass 4 is as much as one would expect , since the remaining nonaligned sentences are one zero alignments , most of them due to the German subheadings that are not part of the English version .", "The table also shows that the AST always provides a significant number of candidates for alignment with each sentence before a pass the fourth column gives the number of true sentence alignments relative to the total number of candidates in the AST .", "Recall that the final alignment is always a subset of the hypotheses in the AST in every preceding pass .", "Figure 5 shows the true sentence alignment for the first 50 sentences dots , and how the algorithm discovered them in the first pass , only a few sentences are set into correspondence circles ; after the second pass crosses already almost half of the correspondences are found .", "Note that there are no wrong alignments in the first two passes .", "In the third pass , almost all of the remaining alignments are found for the first 50 sentences in the figure all , and a final pass usually completes the alignment .", "Our algorithm produces very favorable results when allowed to converge gradually .", "Processing time in the original LISP implementation was high , typically several hours for each pass .", "By trading CPU time for memory massively , the time needed by a C implementation on a Sun 4 75 was reduced to 1 . 7 mm for the first pass , 0 . 8 mm for the second , and 0 . 5 min for the third pass in an application to this pair of articles .", "Initialization , i . e . , reading the files and building up the data structures , takes another 0 . 6 min in the beginning .", "It should be noted that a naive implementation of the algorithm without using the appropriate data structures can easily lead to times that are a factor of 30 higher and do not scale up to larger texts .", "The application of our method to a text that we put together from the Hansard corpus had essentially no problem in identifying the correct sentence alignment in a process of five passes .", "The alignments for the first 1000 sentences of the English text were checked by hand , and seven errors were found ; five of them occurred in sentences where sentence boundaries were not correctly identified by the program because of periods that did not mark a sentence boundary and were not identified as such by a very simple preprocessing program .", "The other two errors involved two short sentences for which the SAT did not give an alignment .", "Processing time increased essentially linearly per pass the first pass took 8 . 3 min , the second 3 . 2 mm , and it further decreased until the last pass , which took 2 . 1 min .", "Initialization took 4 . 2 min .", "Note that the error rate depends crucially on the kind of quot ; annealing schedule quot ; used if the thresholds that allow a word pair in the WAT to influence the SAT are lowered too fast , only a few passes are needed , but accuracy deteriorates .", "For example , in an application where the process terminated after only three passes , the accuracy was only in the eighties estimated on the basis of the first 120 sentences of the English Hansard text checked by hand .", "Since processing time after the first pass is usually already considerably lower , we have found that a high accuracy can safely be attained when more passes are allowed than are actually necessary .", "In order to evaluate the sensitivity of the algorithm to the lengths of the texts that are to be aligned , we applied it to text samples that ranged in length from 10 to 1000 sentences , and examined the accuracy of the WAT after the first pass ; that is , more precisely , the number of word pairs in the WAT that are valid translations relative to the total number of word pairs with a similarity of not less than 0 . 7 the measurements are cross validated over different texts .", "The result is that this accuracy increases asymptotically to 1 with the text length , and is already higher than 80 for a text length of 100 sentences which is sufficient to reach an almost perfect alignment in the end .", "Roughly speaking , the accuracy is almost 1 for texts longer than 150 sentences , and around 0 . 5 for text length in the lower range from 20 to 60 .", "In other words , texts of a length of more than 150 sentences are suitable to be processed in this way ; text fragments shorter than 80 sentences do not have a high proportion of correct word pairs in the first WAT , but further experiments showed that the final alignment for texts of this length is , on average , again almost perfect the drawback of a less accurate initial WAT is apparently largely compensated for by the fact that the AST is also narrower for these texts ; however , the variance in the alignment accuracies is significantly higher .", "Since we addressed the text translation alignment problem in 1988 , a number of researchers , among them Gale and Church 1991 and Brown , Lai , and Mercer 1991 , have worked on the problem .", "Both methods are based on the observation that the length of text unit is highly correlated to the length of the translation of this unit , no matter whether length is measured in number of words or in number of characters see Figure 6 .", "Consequently , they are both easier to implement than ours , though not necessarily more efficient .", "The method of Brown , Lai , and Mercer 1991 is based on a hidden Markov model for the generation of aligned pairs of corpora , whose parameters are estimated from a large text .", "For an application of this method to the Canadian Hansard , good results are reported .", "However , the problem was also considerably facilitated by the way the implementation made use of Hansard specific comments and annotations these are used in a preprocessing step to find anchors for sentence alignment such that , on average , there are only ten sentences in between .", "Moreover , this particular corpus is well known for the near literalness of its translations , and it is therefore unclear to what extent the good results are due to the relative ease of the problem .", "This would be an important consideration when comparing various algorithms ; when the algorithms are actually applied , it is clearly very desirable to incorporate as much prior knowledge say , on potential anchors as possible .", "Moreover , long texts can almost always be expected to contain natural anchors , such as chapter and section headings , at which to make an a priori segmentation .", "Gale and Church 1991 note that their method performed considerably better when lengths of sentences were measured in number of characters instead of in number of words .", "Their method is based on a probabilistic model of the distance between two sentences , and a dynamic programming algorithm is used to minimize the total distance between aligned units .", "Their implementation assumes that each character in one language gives rise to , on average , one character in the other language . '", "In our texts , one character in English on average gives rise to somewhat more than 1 . 2 characters in German , and the correlation between the lengths in characters of aligned paragraphs in the two languages was with 0 . 952 lower than the 0 . 991 that are mentioned in Gale and Church 1991 , which supports our impression that the Scientific American texts we used are hard texts to align , but it is not clear to what extent this would deteriorate the results .", "In applications to economic reports from the Union Bank of Switzerland , the method performs very well on simple alignments one to one , oneto two , but has at the moment problems with complex matches .", "The method has the advantage of associating a score with pairs of sentences so that it is easy to extract a subset for which there is a high likelihood that the alignments are correct .", "Given the simplicity of the methods proposed by Brown , Lai , and Mercer and Gale and Church , either of them could be used as a heuristic in the construction of the initial AST in our algorithm .", "In the current version , the number of candidate sentence pairs that are considered in the first pass near the middle of a text contributes disproportionally to the cost of the computation .", "In fact , as we remarked earlier , the complexity of this step is 0 n ii .", "The proposed modification would effectively make it linear .", "For most practical purposes , the alignment algorithm we have described produces very satisfactory results , even when applied to relatively free translations .", "There are doubtless many places in which the algorithm itself could be improved .", "For example , it is clear that the present method of building the SAT favors associations between long sentences , and this is not surprising , because there is more information in long sentences .", "But we have not investigated the extent of this bias and we do not therefore know it as appropriate .", "The present algorithm rests on being able to identify one to one associations between certain words , notably technical terms and proper names .", "It is clear from a brief inspection of Table 2 that very few correspondences are noticed among everyday words and , when they are , it is usually because those words also have precise technical uses .", "The very few exceptions include quot ; only quot ; quot ; nur quot ; and 'the quot ; quot ; die . quot ; The pair quot ; per quot ; quot ; pro quot ; might also qualify but if the languages afford any example of a scientific preposition , this is surely it .", "The most interesting further developments would be in the direction of loosening up this dependence on one to one associations both because this would present a very significant challenge and also because we are convinced that our present method identifies essentially all the significant one to one associations .", "There are two obvious kinds of looser associations that could be investigated .", "One would consist of connections between a single vocabulary item in one language and two or more in the other , or even between several items in one language and several in the other .", "The other would involve connections one one , one many , or many many between phrases or recurring sequences .", "We have investigated the first of these enough to satisfy ourselves that there is latent information on one to many associations in the text , and that it can be revealed by suitable extensions of our methods .", "However , it is clear that the combinatorial problems associated with this approach are severe , and pursuing it would require much fine tuning of the program and designing much more effective ways of indexing the most important data structures .", "The key to reducing the combinatorial explosion probably lies in using tables of similarities such as those the present algorithm uses to suggest combinations of items that would be worth considering .", "If such an approach could be made efficient enough , it is even possible that it would provide a superior way of solving the problem for which our heuristic methods of morphological analysis were introduced .", "Its superiority would come from the fact that it would not depend on words being formed by concatenation , but would also accommodate such phenomena as umlaut , ablaut , vowel harmony , and the nonconcatenative process of Semitic morphology .", "The problems of treating recurring sequences are less severe .", "Data structures , such as the Patricia tree Knuth 1973 ; pp .", "490 493 provide efficient means of identifying all such sequences and , once identified , the data they provide could be added to the WAT much as we now add the results of morphological analysis .", "Needless to say , this would only allow for uninterrupted sequences .", "Any attempt to deal with discontinuous sequences would doubtless also involve great combinatorial problems .", "These avenues for further development are intriguing and would surely lead to interesting results .", "But it is unlikely that they would lead to much better sets of associations among sentences than are to be found in the SATs that our present program produces , and it was mainly these results that we were interested in from the outset .", "The other avenues we have mentioned concern improvements in the WAT which , for us , was always a secondary interest ."], "summary_lines": ["Text-Translation Alignment\n", "We present an algorithm for aligning texts with their translations that is based only on internal evidence.\n", "The relaxation process rests on a notion of which word in one text corresponds to which word in the other text that is essentially based on the similarity of their distributions.\n", "It exploits a partial alignment of the word level to induce a maximum likelihood alignment of the sentence level, which is in turn used, in the next iteration, to refine the word level estimate.\n", "The algorithm appears to converge to the correct sentence alignment in only a few iterations.\n", "Our morphology algorithm is applied for splitting potential suffixes and prefixes and for obtaining the normalised word forms.\n"]}
{"article_lines": ["Incorporating Non Local Information Into Information Extraction Systems By Gibbs Sampling", "Most current statistical natural language processing models use only local features so as to permit dynamic programming in inference , but this makes them unable to fully account for the long distance structure that is prevalent in language use .", "We how to solve this dilemma with sama simple Monte Carlo method used to perform approximate inference in factored probabilistic models .", "By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs , CMMs , and CRFs , it is possible to incorporate non local structure while preserving tractable inference .", "We use this technique to augment an existing CRF based information extraction system with long distance dependency models , enforcing label consistency and extraction template consistency constraints .", "This technique results in an error reduction of up to 9 over state of the art systems on two established information extraction tasks .", "Most statistical models currently used in natural language processing represent only local structure .", "Although this constraint is critical in enabling tractable model inference , it is a key limitation in many tasks , since natural language contains a great deal of nonlocal structure .", "A general method for solving this problem is to relax the requirement of exact inference , substituting approximate inference algorithms instead , thereby permitting tractable inference in models with non local structure .", "One such algorithm is Gibbs sampling , a simple Monte Carlo algorithm that is appropriate for inference in any factored probabilistic model , including sequence models and probabilistic context free grammars Geman and Geman , 1984 .", "Although Gibbs sampling is widely used elsewhere , there has been extremely little use of it in natural language processing . 1 Here , we use it to add non local dependencies to sequence models for information extraction .", "Statistical hidden state sequence models , such as Hidden Markov Models HMMs Leek , 1997 ; Freitag and McCallum , 1999 , Conditional Markov Models CMMs Borthwick , 1999 , and Conditional Random Fields CRFs Lafferty et al . , 2001 are a prominent recent approach to information extraction tasks .", "These models all encode the Markov property decisions about the state at a particular position in the sequence can depend only on a small local window .", "It is this property which allows tractable computation the Viterbi , Forward Backward , and Clique Calibration algorithms all become intractable without it .", "However , information extraction tasks can benefit from modeling non local structure .", "As an example , several authors see Section 8 mention the value of enforcing label consistency in named entity recognition NER tasks .", "In the example given in Figure 1 , the second occurrence of the token Tanjug is mislabeled by our CRF based statistical NER system , because by looking only at local evidence it is unclear whether it is a person or organization .", "The first occurrence of Tanjug provides ample evidence that it is an organization , however , and by enforcing label consistency the system should be able to get it right .", "We show how to incorporate constraints of this form into a CRF model by using Gibbs sampling instead of the Viterbi algorithm as our inference procedure , and demonstrate that this technique yields significant improvements on two established IE tasks .", "In hidden state sequence models such as HMMs , CMMs , and CRFs , it is standard to use the Viterbi algorithm , a dynamic programming algorithm , to infer the most likely hidden state sequence given the input and the model see , e . g . , Rabiner 1989 .", "Although this is the only tractable method for exact computation , there are other methods for computing an approximate solution .", "Monte Carlo methods are a simple and effective class of methods for approximate inference based on sampling .", "Imagine we have a hidden state sequence model which defines a probability distribution over state sequences conditioned on any given input .", "With such a model M we should be able to compute the conditional probability PM s o of any state sequence s s0 , . . . , sN given some observed input sequence o o0 , . . . , oN .", "One can then sample sequences from the conditional distribution defined by the model .", "These samples are likely to be in high probability areas , increasing our chances of finding the maximum .", "The challenge is how to sample sequences efficiently from the conditional distribution defined by the model .", "Gibbs sampling provides a clever solution Geman and Geman , 1984 .", "Gibbs sampling defines a Markov chain in the space of possible variable assignments in this case , hidden state sequences such that the stationary distribution of the Markov chain is the joint distribution over the variables .", "Thus it is called a Markov Chain Monte Carlo MCMC method ; see Andrieu et al . 2003 for a good MCMC tutorial .", "In practical terms , this means that we can walk the Markov chain , occasionally outputting samples , and that these samples are guaranteed to be drawn from the target distribution .", "Furthermore , the chain is defined in very simple terms from each state sequence we can only transition to a state sequence obtained by changing the state at any one position i , and the distribution over these possible transitions is just where s i is all states except si .", "In other words , the transition probability of the Markov chain is the conditional distribution of the label at the position given the rest of the sequence .", "This quantity is easy to compute in any Markov sequence model , including HMMs , CMMs , and CRFs .", "One easy way to walk the Markov chain is to loop through the positions i from 1 to N , and for each one , to resample the hidden state at that position from the distribution given in Equation 1 .", "By outputting complete sequences at regular intervals such as after resampling all N positions , we can sample sequences from the conditional distribution defined by the model .", "This is still a gravely inefficient process , however .", "Random sampling may be a good way to estimate the shape of a probability distribution , but it is not an efficient way to do what we want find the maximum .", "However , we cannot just transition greedily to higher probability sequences at each step , because the space is extremely non convex .", "We can , however , borrow a technique from the study of non convex optimization and use simulated annealing Kirkpatrick et al . , 1983 .", "Geman and Geman 1984 show that it is easy to modify a Gibbs Markov chain to do annealing ; at time t we replace the distribution in 1 with where c c0 , . . . , cT defines a cooling schedule .", "At each step , we raise each value in the conditional distribution to an exponent and renormalize before sampling from it .", "Note that when c 1 the distribution is unchanged , and as c 0 the distribution becomes sharper , and when c 0 the distribution places all of its mass on the maximal outcome , having the effect that the Markov chain always climbs uphill .", "Thus if we gradually decrease c from 1 to 0 , the Markov chain increasingly tends to go uphill .", "This annealing technique has been shown to be an effective technique for stochastic optimization Laarhoven and Arts , 1987 .", "To verify the effectiveness of Gibbs sampling and simulated annealing as an inference technique for hidden state sequence models , we compare Gibbs and Viterbi inference methods for a basic CRF , without the addition of any non local model .", "The results , given in Table 1 , show that if the Gibbs sampler is run long enough , its accuracy is the same as a Viterbi decoder .", "Our basic CRF model follows that of Lafferty et al . 2001 .", "We choose a CRF because it represents the state of the art in sequence modeling , allowing both discriminative training and the bi directional flow of probabilistic information across the sequence .", "A CRF is a conditional sequence model which represents the probability of a hidden state sequence given some observations .", "In order to facilitate obtaining the conditional probabilities we need for Gibbs sampling , we generalize the CRF model in a way that is consistent with the Markov Network literature see Cowell et al . 1999 we create a linear chain of cliques , where each clique , c , represents the probabilistic relationship between an adjacent pair of states2 using a clique potential \u03c6c , which is just a table containing a value for each possible state assignment .", "The table is not a true probability distribution , as it only accounts for local interactions within the clique .", "The clique potentials themselves are defined in terms of exponential models conditioned on features of the observation sequence , and must be instantiated for each new observation sequence .", "The sequence of potentials in the clique chain then defines the probability of a state sequence given the observation sequence as where \u03c6i si 1 , si is the element of the clique potential at position i corresponding to states si 1 and si . 3 Although a full treatment of CRF training is beyond the scope of this paper our technique assumes the model is already trained , we list the features used by our CRF for the two tasks we address in Table 2 .", "During training , we regularized our exponential models with a quadratic prior and used the quasi Newton method for parameter optimization .", "As is customary , we used the Viterbi algorithm to infer the most likely state sequence in a CRF .", "The clique potentials of the CRF , instantiated for some observation sequence , can be used to easily compute the conditional distribution over states at a position given in Equation 1 .", "Recall that at position i we want to condition on the states in the rest of the sequence .", "The state at this position can be influenced by any other state that it shares a clique with ; in particular , when the clique size is 2 , there are 2 such cliques .", "In this case the Markov blanket of the state the minimal set of states that renders a state conditionally independent of all other states consists of the two neighboring states and the observation sequence , all of which are observed .", "The conditional distribution at position i can then be computed simply as where the factor tables F in the clique chain are already conditioned on the observation sequence .", "We test the effectiveness of our technique on two established datasets the CoNLL 2003 English named entity recognition dataset , and the CMU Seminar Announcements information extraction dataset .", "This dataset was created for the shared task of the Seventh Conference on Computational Natural Language Learning CoNLL , 4 which concerned named entity recognition .", "The English data is a collection of Reuters newswire articles annotated with four entity types person PER , location LOC , organization ORG , and miscellaneous MISC .", "The data is separated into a training set , a development set testa , and a test set testb .", "The training set contains 945 documents , and approximately 203 , 000 tokens .", "The development set has 216 documents and approximately 51 , 000 tokens , and the test set has 231 documents and approximately 46 , 000 tokens .", "We evaluate performance on this task in the manner dictated by the competition so that results can be properly compared .", "Precision and recall are evaluated on a per entity basis and combined into an F1 score .", "There is no partial credit ; an incorrect entity boundary is penalized as both a false positive and as a false negative .", "This dataset was developed as part of Dayne Freitag s dissertation research Freitag 1998 . 5 It consists of 485 emails containing seminar announcements at Carnegie Mellon University .", "It is annotated for four fields speaker , location , start time , and end time .", "Sutton and McCallum 2004 used 5 fold cross validation when evaluating on this dataset , so we obtained and used their data splits , so that results can be properly compared .", "Because the entire dataset is used for testing , there is no development set .", "We also used their evaluation metric , which is slightly different from the method for CoNLL data .", "Instead of evaluating precision and recall on a per entity basis , they are evaluated on a per token basis .", "Then , to calculate the overall F1 score , the F1 scores for each class are averaged .", "Our models of non local structure are themselves just sequence models , defining a probability distribution over all possible state sequences .", "It is possible to flexibly model various forms of constraints in a way that is sensitive to the linguistic structure of the data e . g . , one can go beyond imposing just exact identity conditions .", "One could imagine many ways of defining such models ; for simplicity we use the form where the product is over a set of violation types A , and for each violation type A we specify a penalty parameter \u03b8\u03bb .", "The exponent A , s , o is the count of the number of times that the violation A occurs in the state sequence s with respect to the observation sequence o .", "This has the effect of assigning sequences with more violations a lower probability .", "The particular violation types are defined specifically for each task , and are described in the following two sections .", "This model , as defined above , is not normalized , and clearly it would be expensive to do so .", "This doesn t matter , however , because we only use the model for Gibbs sampling , and so only need to compute the conditional distribution at a single position i as defined in Equation 1 .", "One inefficient way to compute this quantity is to enumerate all possible sequences differing only at position i , compute the score assigned to each by the model , and renormalize .", "Although it seems expensive , this computation can be made very efficient with a straightforward memoization technique at all times we maintain data structures representing the relationship between entity labels and token sequences , from which we can quickly compute counts of different types of violations .", "Label consistency structure derives from the fact that within a particular document , different occurrences of a particular token sequence are unlikely to be labeled as different entity types .", "Although any one occurrence may be ambiguous , it is unlikely that all instances are unclear when taken together .", "The CoNLL training data empirically supports the strength of the label consistency constraint .", "Table 3 shows the counts of entity labels for each pair of identical token sequences within a document , where both are labeled as an entity .", "Note that inconsistent labelings are very rare . 6 In addition , we also want to model subsequence constraints having seen Geoff Woods earlier in a document as a person is a good indicator that a subsequent occurrence of Woods should also be labeled as a person .", "However , if we examine all cases of the labelings of other occurrences of subsequences of a labeled entity , we find that the consistency constraint does not hold nearly so strictly in this case .", "As an example , one document contains references to both The China Daily , a newspaper , and China , the country .", "Counts of subsequence labelings within a document are listed in Table 4 .", "Note that there are many offdiagonal entries the China Daily case is the most common , occurring 328 times in the dataset .", "The penalties used in the long distance constraint model for CoNLL are the Empirical Bayes estimates taken directly from the data Tables 3 and 4 , except that we change counts of 0 to be 1 , so that the distribution remains positive .", "So the estimate of a PER also being an ORG is 5 3151 ; there were 5 instance of an entity being labeled as both , PER appeared 3150 times in the data , and we add 1 to this for smoothing , because PER MISC never occured .", "However , when we have a phrase labeled differently in two different places , continuing with the PER ORG example , it is unclear if we should penalize it as PER that is also an ORG or an ORG that is also a PER .", "To deal with this , we multiply the square roots of each estimate together to form the penalty term .", "The penalty term is then multiplied in a number of times equal to the length of the offending entity ; this is meant to encourage the entity to shrink . 7 For example , say we have a document with three entities , Rotor Volgograd twice , once labeled as PER and once as ORG , and Rotor , labeled as an ORG .", "The likelihood of a PER also being an ORG is 5 3151 , and of an ORG also Due to the lack of a development set , our consistency model for the CMU Seminar Announcements is much simpler than the CoNLL model , the numbers where selected due to our intuitions , and we did not spend much time hand optimizing the model .", "Specifically , we had three constraints .", "The first is that all entities labeled as start time are normalized , and are penalized if they are inconsistent .", "The second is a corresponding constraint for end times .", "The last constraint attempts to consistently label the speakers .", "If a phrase is labeled as a speaker , we assume that the last word is the speaker s last name , and we penalize for each occurrance of that word which is not also labeled speaker .", "For the start and end times the penalty is multiplied in based on how many words are in the entity .", "For the speaker , the penalty is only multiplied in once .", "We used a hand selected penalty of exp 4 . 0 .", "In the previous section we defined two models of non local structure .", "Now we would like to incorporate them into the local model in our case , the trained CRF , and use Gibbs sampling to find the most likely state sequence .", "Because both the trained CRF and the non local models are themselves sequence models , we simply combine the two models into a factored sequence model of the following form where M is the local CRF model , L is the new nonlocal model , and F is the factored model . 8 In this form , the probability again looks difficult to compute because of the normalizing factor , a sum over all hidden state sequences of length N .", "However , since we are only using the model for Gibbs sampling , we never need to compute the distribution explicitly .", "Instead , we need only the conditional probability of each position in the sequence , which can be computed as", "In our experiments we compare the impact of adding the non local models with Gibbs sampling to our baseline CRF implementation .", "In the CoNLL named entity recognition task , the non local models increase the F1 accuracy by about 1 . 3 .", "Although such gains may appear modest , note that they are achieved relative to a near state of the art NER system the winner of the CoNLL English task reported an F1 score of 88 . 76 .", "In contrast , the increases published by Bunescu and Mooney 2004 are relative to a baseline system which scores only 80 . 9 on the same task .", "Our performance is similar on the CMU Seminar Announcements dataset .", "We show the per field F1 results that were reported by Sutton and McCallum 2004 for comparison , and note that we are again achieving gains against a more competitive baseline system .", "For all experiments involving Gibbs sampling , we used a linear cooling schedule .", "For the CoNLL dataset we collected 200 samples per trial , and for the CMU Seminar Announcements we collected 100 samples .", "We report the average of all trials , and in all cases we outperform the baseline with greater than 95 confidence , using the standard t test .", "The trials had low standard deviations 0 . 083 and 0 . 007 and high minimun F scores 86 . 72 , and 92 . 28 for the CoNLL and CMU Seminar Announcements respectively , demonstrating the stability of our method .", "The biggest drawback to our model is the computational cost .", "Taking 100 samples dramatically increases test time .", "Averaged over 3 runs on both Viterbi and Gibbs , CoNLL testing time increased from 55 to 1738 seconds , and CMU Seminar Announcements testing time increases from 189 to 6436 seconds .", "Several authors have successfully incorporated a label consistency constraint into probabilistic sequence model named entity recognition systems .", "Mikheev et al . 1999 and Finkel et al .", "2004 incorporate label consistency information by using adhoc multi stage labeling procedures that are effective but special purpose .", "Malouf 2002 and Curran and Clark 2003 condition the label of a token at a particular position on the label of the most recent previous instance of that same token in a prior sentence of the same document .", "Note that this violates the Markov property , but is achieved by slightly relaxing the requirement of exact inference .", "Instead of finding the maximum likelihood sequence over the entire document , they classify one sentence at a time , allowing them to condition on the maximum likelihood sequence of previous sentences .", "This approach is quite effective for enforcing label consistency in many NLP tasks , however , it permits a forward flow of information only , which is not sufficient for all cases of interest .", "Chieu and Ng 2002 propose a solution to this problem for each token , they define additional features taken from other occurrences of the same token in the document .", "This approach has the added advantage of allowing the training procedure to automatically learn good weightings for these global features relative to the local ones .", "However , this approach cannot easily be extended to incorporate other types of non local structure .", "The most relevant prior works are Bunescu and Mooney 2004 , who use a Relational Markov Network RMN Taskar et al . , 2002 to explicitly models long distance dependencies , and Sutton and McCallum 2004 , who introduce skip chain CRFs , which maintain the underlying CRF sequence model which Bunescu and Mooney , 2004 lack while adding skip edges between distant nodes .", "Unfortunately , in the RMN model , the dependencies must be defined in the model structure before doing any inference , and so the authors use crude heuristic part of speech patterns , and then add dependencies between these text spans using clique templates .", "This generates a extremely large number of overlapping candidate entities , which then necessitates additional templates to enforce the constraint that text subsequences cannot both be different entities , something that is more naturally modeled by a CRF .", "Another disadvantage of this approach is that it uses loopy beliefpropagation and a voted perceptron for approximate learning and inference ill founded and inherently unstable algorithms which are noted by the authors to have caused convergence problems .", "In the skip chain CRFs model , the decision of which nodes to connect is also made heuristically , and because the authors focus on named entity recognition , they chose to connect all pairs of identical capitalized words .", "They also utilize loopy belief propagation for approximate learning and inference .", "While the technique we propose is similar mathematically and in spirit to the above approaches , it differs in some important ways .", "Our model is implemented by adding additional constraints into the model at inference time , and does not require the preprocessing step necessary in the two previously mentioned works .", "This allows for a broader class of long distance dependencies , because we do not need to make any initial assumptions about which nodes should be connected , and is helpful when you wish to model relationships between nodes which are the same class , but may not be similar in any other way .", "For instance , in the CMU Seminar Announcements dataset , we can normalize all entities labeled as a start time and penalize the model if multiple , nonconsistent times are labeled .", "This type of constraint cannot be modeled in an RMN or a skip CRF , because it requires the knowledge that both entities are given the same class label .", "We also allow dependencies between multi word phrases , and not just single words .", "Additionally , our model can be applied on top of a pre existing trained sequence model .", "As such , our method does not require complex training procedures , and can instead leverage all of the established methods for training high accuracy sequence models .", "It can indeed be used in conjunction with any statistical hidden state sequence model HMMs , CMMs , CRFs , or even heuristic models .", "Third , our technique employs Gibbs sampling for approximate inference , a simple and probabilistically well founded algorithm .", "As a consequence of these differences , our approach is easier to understand , implement , and adapt to new applications .", "We have shown that a constraint model can be effectively combined with an existing sequence model in a factored architecture to successfully impose various sorts of long distance constraints .", "Our model generalizes naturally to other statistical models and other tasks .", "In particular , it could in the future be applied to statistical parsing .", "Statistical context free grammars provide another example of statistical models which are restricted to limiting local structure , and which could benefit from modeling nonlocal structure .", "This work was supported in part by the Advanced Researchand Development Activity ARDA s Advanced Question Answeringfor Intelligence AQUAINT Program .", "Additionally , we would like to that our reviewers for their helpful comments ."], "summary_lines": ["Incorporating Non-Local Information Into Information Extraction Systems By Gibbs Sampling\n", "Most current statistical natural language processing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use.\n", "We show how to solve this dilemma with Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models.\n", "By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference.\n", "We use this technique to augment an existing CRF-based information extraction system with long-distance dependency models, enforcing label consistency and extraction template consistency constraints.\n", "This technique results in an error reduction of up to 9% over state-of-the-art systems on two established information extraction tasks.\n"]}
{"article_lines": ["An Unsupervised Morpheme Based HMM For Hebrew Morphological Disambiguation", "Morphological disambiguation is the process of assigning one set of morphological features to each individual word in a text .", "When the word is ambiguous there are several possible analyses for the word , a disambiguation procedure based on the word context must be applied .", "This paper deals with morphological disambiguation of the Hebrew language , which combines morphemes into a word in both agglutinative and fusional ways .", "We present an unsupervised stochastic model the only resource we use is a morphological analyzer which deals with the data sparseness problem caused by the affixational morphology of the Hebrew language .", "We present a text encoding method for languages with affixational morphology in which the knowledge of word formation rules which are quite restricted in Hebrew helps in the disambiguation .", "We adapt HMM algorithms for learning and searching this text representation , in such a way that segmentation and tagging can be learned in parallel in one step .", "Results on a large scale evaluation indicate that this learning improves disambiguation for complex tag sets .", "Our method is applicable to other languages with affix morphology .", "Morphological disambiguation is the process of assigning one set of morphological features to each individual word in a text , according to the word context .", "In this work , we investigate morphological disambiguation in Modern Hebrew .", "We explore unsupervised learning method , which is more challenging than the supervised case .", "The main motivation for this approach is that despite the development of annotated corpora in Hebrew' , there is still not enough data available for supervised training .", "The other reason , is that unsupervised methods can handle the dynamic nature of Modern Hebrew , as it evolves over time .", "In the case of English , because morphology is simpler , morphological disambiguation is generally covered under the task of part of speech tagging .", "The main morphological variations are embedded in the tag name for example , Ns and Np for noun singular or plural .", "The tagging accuracy of supervised stochastic taggers is around 96 97 Manning and Schutze , 1999 , 10 . 6 . 1 .", "Merialdo 1994 reports an accuracy of 86 . 6 for an unsupervised word based HMM , trained on a corpus of 42 , 186 sentences about 1M words , over a tag set of 159 different tags .", "Elworthy 1994 , in contrast , reports an accuracy of 75 . 49 , 80 . 87 and 79 . 12 for unsupervised word based HMM trained on parts of the LOB corpora , with a tagset of 134 tags .", "With good initial conditions , such as good approximation of the tag distribution for each word , Elworthy reports an improvement to 94 . 6 , 92 . 27 and 94 . 51 on the same data sets .", "Merialdo , on the other hand , reports an improvement to 92 . 6 and 94 . 4 for the case where 100 and 2000 sentences of the training corpus are manually tagged .", "Modern Hebrew is characterized by rich morphology , with a high level of ambiguity .", "On average , in our corpus , the number of possible analyses per word reached 2 . 4 in contrast to 1 . 4 for English .", "In Hebrew , several morphemes combine into a single word in both agglutinative and fusional ways .", "This results in a potentially high number of tags for each word .", "In contrast to English tag sets whose sizes range from 48 to 195 , the number of tags for Hebrew , based on all combinations of the morphological attributes part of speech , gender , number , person , tense , status , and the affixes' properties2 , can grow theoretically to about 300 , 000 tags .", "In practice , we found only 1 , 934 tags in a corpus of news stories we gathered , which contains about 6M words .", "The large size of such a tag set about 10 times larger than the most comprehensive English tag set is problematic in term of data sparseness .", "Each morphological combination appears rarely , and more samples are required in order to learn the probabilistic model .", "In this paper , we hypothesize that the large set of morphological features of Hebrew words , should be modeled by a compact morpheme model , based on the segmented words into prefix , baseform , and suffix .", "Our main result is that best performance is obtained when learning segmentation and morpheme tagging in one step , which is made possible by an appropriate text representation .", "Several works have dealt with Hebrew tagging in the past decade .", "In Hebrew , morphological analysis requires complex processing according to the rules of Hebrew word formation .", "The task of a morphological analyzer is to produce all possible analyses for a given word .", "Recent analyzers provide good performance and documentation of this process Yona and Wintner , 2005 ; Segal , 2000 .", "Morphological analyzers rely on a dictionary , and their performance is , therefore , impacted by the occurrence of unknown words .", "The task of a morphological disambiguation system is to pick the most likely analysis produced by an analyzer in the context of a full sentence .", "Levinger et al . 1995 developed a context free method in order to acquire the morpho lexical probabilities , from an untagged corpus .", "Their method handles the data sparseness problem by using a set of similar words for each word , built according to a set of rules .", "The rules produce variations of the morphological properties of the word analyses .", "Their tests indicate an accuracy of about 88 for context free analysis selection based on the approximated analysis distribution .", "In tests we reproduced on a larger data set 30K tagged words , the accuracy is only 78 . 2 .", "In order to improve the results , the authors recommend merging their method together with other morphological disambiguation methods which is the approach we pursue in this work .", "Levinger's morphological disambiguation system Levinger , 1992 combines the above approximated probabilities with an expert system , based on a manual set of 16 syntactic constraints .", "In the first phase , the expert system is applied , dis24 86 . ambiguating 35 of the ambiguous words with an accuracy of 99 . 6 .", "In order to increase the applicability of the disambiguation , approximated probabilities are used for words that were not disambiguated in the first stage .", "Finally , the expert system is used again over the new probabilities that were set in the previous stage .", "Levinger reports an accuracy of about 94 for disambiguation of 85 of the words in the text overall 80 disambiguation .", "The system was also applied to prune out the least likely analyses in a corpus but without , necessarily , selecting a single analysis for each word .", "For this task , an accuracy of 94 was reported while reducing 92 of the ambiguous analyses .", "Carmel and Maarek 1999 use the fact that on average 45 of the Hebrew words are unambiguous , to rank analyses , based on the number of disambiguated occurrences in the text , normalized by the total number of occurrences for each word .", "Their application indexing for an information retrieval system does not require all of the morphological attributes but only the lemma and the PoS of each word .", "As a result , for this case , 75 of the words remain with one analysis with 95 accuracy , 20 with two analyses and 5 with three analyses .", "Segal 2000 built a transformation based tagger in the spirit of Brill 1995 .", "In the first phase , the analyses of each word are ranked according to the frequencies of the possible lemmas and tags in a training corpus of about 5 , 000 words .", "Selection of the highest ranked analysis for each word gives an accuracy of 83 of the test text which consists of about 1 , 000 words .", "In the second stage , a transformation learning algorithm is applied in contrast to Brill , the observed transformations are not applied , but used for re estimation of the word couples probabilities .", "After this stage , the accuracy is about 93 .", "The last stage uses a bottomup parser over a hand crafted grammar with 150 rules , in order to select the analysis which causes the parsing to be more accurate .", "Segal reports an accuracy of 95 .", "Testing his system over a larger test corpus , gives poorer results Lembersky 2001 reports an accuracy of about 85 .", "Bar Haim et al . 2005 developed a word segmenter and PoS tagger for Hebrew .", "In their architecture , words are first segmented into morphemes , and then , as a second stage , these morphemes are tagged with PoS .", "The method proceeds in two sequential steps segmentation into morphemes , then tagging over morphemes .", "The segmentation is based on an HMM and trained over a set of 30K annotated words .", "The segmentation step reaches an accuracy of 96 . 74 .", "PoS tagging , based on unsupervised estimation which combines a small annotated corpus with an untagged corpus of 340K words by using smoothing technique , gives an accuracy of 90 . 51 .", "As noted earlier , there is as yet no large scale Hebrew annotated corpus .", "We are in the process of developing such a corpus , and we have developed tagging guidelines Elhadad et al . , 2005 to define a comprehensive tag set , and assist human taggers achieve high agreement .", "The results discussed above should be taken as rough approximations of the real performance of the systems , until they can be re evaluated on such a large scale corpus with a standard tag set .", "Arabic is a language with morphology quite similar to Hebrew .", "Theoretically , there might be 330 , 000 possible morphological tags , but in practice , Habash and Rambow 2005 extracted 2 , 200 different tags from their corpus , with an average number of 2 possible tags per word .", "As reported by Habash and Rambow , the first work on Arabic tagging which used a corpus for training and evaluation was the work of Diab et al . 2004 .", "Habash and Rambow were the first to use a morphological analyzer as part of their tagger .", "They developed a supervised morphological disambiguator , based on training corpora of two sets of 120K words , which combines several classifiers of individual morphological features .", "The accuracy of their analyzer is 94 . 8 96 . 2 depending on the test corpus .", "An unsupervised HMM model for dialectal Arabic which is harder to be tagged than written Arabic , with accurracy of 69 . 83 , was presented by Duh and Kirchhoff 2005 .", "Their supervised model , trained on a manually annotated corpus , reached an accuracy of 92 . 53 .", "Arabic morphology seems to be similar to Hebrew morphology , in term of complexity and data sparseness , but comparison of the performances of the baseline tagger used by Habash and Rambow which selects the most frequent tag for a given word in the training corpus for Hebrew and Arabic , shows some intriguing differences 92 . 53 for Arabic and 71 . 85 for Hebrew .", "Furthermore , as mentioned above , even the use of a sophisticated context free tagger , based on Levinger et al . , 1995 , gives low accuracy of 78 . 2 .", "This might imply that , despite the similarities , morphological disambiguation in Hebrew might be harder than in Arabic .", "It could also mean that the tag set used for the Arabic corpora has not been adapted to the specific nature of Arabic morphology a comment also made in Habash and Rambow , 2005 .", "We propose an unsupervised morpheme based HMM to address the data sparseness problem .", "In contrast to Bar Haim et al . , our model combines segmentation and morphological disambiguation , in parallel .", "The only resource we use in this work is a morphological analyzer .", "The analyzer itself can be generated from a word list and a morphological generation module , such as the HSpell wordlist Har'el and Kenigsberg , 2004 .", "The lexical items of word based models are the words of the language .", "The implication of this decision is that both lexical and syntagmatic relations of the model , are based on a word oriented tagset .", "With such a tagset , it must be possible to tag any word of the language with at least one tag .", "Let us consider , for instance , the Hebrew phrase bclm hn im3 , which contains two words .", "The word bclm has several possible morpheme segmentations and analyses as described in Table 1 .", "In wordbased HMM , we consider such a phrase to be generated by a Markov process , based on the wordoriented tagset of N 1934 tags states and about M 175K word types .", "Line W of Table 2 describes the size of a first order word based HMM , built over our corpus .", "In this model , we found 834 entries for the II vector which models the distribution of tags in first position in sentences out of possibly N 1934 , about 250K entries for the A matrix which models the transition probabilities from tag to tag out of possibly N2 ; Li 3 . 7M , and about 300K entries for the B matrix which models the emission probabilities from tag to word out of possibly M N ; Li 350M .", "For the case of a secondorder HMM , the size of the A2 matrix which models the transition probabilities from two tags to the third one , grows to about 7M entries , where the size of the B2 matrix which models the emission probabilities from two tags to a word is about 5M .", "Despite the sparseness of these matrices , the number of their entries is still high , since we model the whole set of features of the complex word forms .", "Let us assume , that the right segmentation for the sentence is provided to us for example b clm hn im as is the case for English text .", "In such a way , the observation is composed of morphemes , generated by a Markov process , based on a morpheme based tagset .", "The size of such a tagset for Hebrew is about 200 , where the size of the II , A , B , A2 and B2 matrices is reduced to 145 , 16K , 140K , 700K , and 1 . 7M correspondingly , as described in line M of Table 2 a reduction of 90 when compared with the size of a word based model .", "The problem in this approach , is that someone along the way , agglutinates the morphemes of each word leaving the observed morphemes uncertain .", "For example , the word bclm can be segmented in four different ways in Table 1 , as indicated by the placement of the ' ' in the Segmentation column , while the word hn im can be segmented in two different ways .", "In the next section , we adapt the parameter estimation and the searching algorithms for such uncertain output observation .", "In contrast to standard HMM , the output observations of the above morpheme based HMM are ambiguous .", "We adapted Baum Welch Baum , 1972 and Viterbi Manning and Schutze , 1999 , 9 . 3 . 2 algorithms for such uncertain observation .", "We first formalize the output representation and then describe the algorithms .", "Output Representation The learning and searching algorithms of HMM are based on the output sequence of the underlying Markov process .", "For the case of a morpheme based model , the output sequence is uncertain we don t see the emitted morphemes but the words they form .", "If , for instance , the Markov process emitted the morphemes b clm h nim , we would see two words bclm hn im instead .", "In order to handle the output ambiguity , we use static knowledge of how morphemes are combined into a word , such as the four known combinations of the word bclm , the two possible combinations of the word hn im , and their possible tags within the original words .", "Based on this information , we encode the sentence into a structure that represents all the possible readings of the sentence , according to the possible morpheme combinations of the words , and their possible tags .", "The representation consists of a set of vectors , each vector containing the possible morphemes and their tags for each specific time sequential position within the morpheme expansion of the words of the sentence .", "A morpheme is represented by a tuple symbol , state , prev , next , where symbol denotes a morpheme , state is one possible tag for this morpheme , prev and next are sets of indexes , denoting the indexes of the morphemes of the previous and the next vectors that precede and follow the current morpheme in the overall lattice , representing the sentence .", "Fig .", "2 describes the representation of the sentence bclm hn im .", "An emission is denoted in this figure by its symbol , its state index , directed edges from its previous emissions , and directed edges to its next emissions .", "In order to meet the condition of Baum Eagon inequality Baum , 1972 that the polynomial P O \u00b5 which represents the probability of an observed sequence O given a model \u00b5 be homogeneous , we must add a sequence of special EOS end of sentence symbols at the end of each path up to the last vector , so that all the paths reach the same length .", "The above text representation can be used to model multi word expressions MWEs .", "Consider the Hebrew sentence hw' wrk dyn gdwl , which can be interpreted as composed of 3 units he lawyer great he is a great lawyer or as 4 units he edits law big he is editing an important legal decision .", "In order to select the correct interpretation , we must determine whether wrk dyn is an MWE .", "This is another case of uncertain output observation , which can be represented by our text encoding , as done in Fig .", "This representation seems to be expensive in term of the number of emissions per sentence .", "However , we observe in our data that most of the words have only one or two possible segmentations , and most of the segmentations consist of at most one affix .", "In practice , we found the average number of emissions per sentence in our corpus where each symbol is counted as the number of its predecessor emissions to be 455 , where the average number of words per sentence is about 18 .", "That is , the cost of operating over an ambiguous sentence representation increases the size of the sentence from 18 to 455 , but on the other hand , it reduces the probabilistic model by a factor of 10 as discussed above .", "Morphological disambiguation over such a sequence of vectors of uncertain morphemes is similar to words extraction in automatic speech recognition ASR Jurafsky and Martin , 2000 , chp .", "The states of the ASR model are phones , where each observation is a vector of spectral features .", "Given a sequence of observations for a sentence , the encoding based on the lattice formed by the phones distribution of the observations , and the language model searches for the set of words , made of phones , which maximizes the acoustic likelihood and the language model probabilities .", "In a similar manner , the supervised training of a speech recognizer combines a training corpus of speech wave files , together with word transcription , and language model probabilities , in order to learn the phones model .", "There are two main differences between the typical ASR model and ours 1 an ASR decoder deals with one aspect segmentation of the observations into a set of words , where this segmentation can be modeled at several levels subphones , phones and words .", "These levels can be trained individually such as training a language model from a written corpus , and training the phones model for each word type , given transcripted wave file , and then combined together in a hierarchical model .", "Morphological disambiguation over uncertain morphemes , on the other hand , deals with both morpheme segmentation and the tagging of each morpheme with its morphological features .", "Modeling morpheme segmentation , within a given word , without its morphology features would be insufficient .", "2 The supervised resources of ASR are not available for morphological disambiguation we don t have a model of morphological features sequences equivalent to the language model of ASR nor a tagged corpus equivalent to the transcripted wave files of ASR .", "These two differences require a design which combines the two dimensions of the problem , in order to support unsupervised learning and searching of morpheme sequences and their morphological features , simultaneously .", "Parameter Estimation We present a variation of the Baum Welch algorithm Baum , 1972 which operates over the lattice representation we have defined above .", "The algorithm starts with a probabilistic model \u00b5 which can be chosen randomly or obtained from good initial conditions , and at each iteration , a new model \u00b5 is derived in order to better explain the given output observations .", "For a given sentence , we define T as the number of words in the sentence , and T as the number of vectors of the output representation O ot , 1 t T , where each item in the output is denoted by olt sym , state , prev , next , 1 t T , 1 l ot .", "We define a t , l as the probability to reach olt at time t , and 0 t , l as the probability to end the sequence from olt .", "Fig .", "3 describes the expectation and the maximization steps of the learning algorithm for a first order HMM .", "The algorithm works in O T time complexity , where T is the total number of symbols in the output sequence encoding , where each symbol is counted as the size of its prev set .", "Searching for best state sequence The searching algorithm gets an observation sequence O and a probabilistic model \u00b5 , and looks for the best state sequence that generates the observation .", "We define S t , l as the probability of the best state sequence that leads to emission olt , and t , l as the index of the emission at time t 1 that precedes olt in the best state sequence that leads to it .", "Fig .", "4 describes the adaptation of the Viterbi Manning and Schutze , 1999 , 9 . 3 . 2 algorithm to our text representation for first order HMM , which works in O T time .", "We ran a series of experiments on a Hebrew corpus to compare various approaches to the full morphological disambiguation and PoS tagging tasks .", "The training corpus is obtained from various newspaper sources and is characterized by the following statistics 6M word occurrences , 178 , 580 distinct words , 64 , 541 distinct lemmas .", "Overall , the ambiguity level is 2 . 4 average number of analyses per word .", "We tested the results on a test corpus , manually annotated by 2 taggers according to the guidelines we published and checked for agreement .", "The test corpus contains about 30K words .", "We compared two unsupervised models over this data set Word model W , and Morpheme model M .", "We also tested two different sets of initial conditions .", "Uniform distribution Uniform For each word , each analysis provided by the analyzer is estimated with an equal likelihood .", "Context Free approximation CF We applied the CF algorithm of Levinger et al . 1995 to estimate the likelihood of each analysis .", "Table 3 reports the results of full morphological disambiguation .", "For each morpheme and word models , three types of models were tested 1 First order HMM , 2 Partial second order HMM only state transitions were modeled excluding B2 matrix , 2 Second order HMM including the B2 matrix .", "Analysis If we consider the tagger which selects the most probable morphological analysis for each word in the text , according to Levinger et al . 1995 approximations , with accuracy of 78 . 2 , as the baseline tagger , four steps of error reduction can be identified .", "1 Contextual information The simplest first order word based HMM with uniform initial conditions , achieves error reduction of 17 . 5 78 . 2 82 . 01 .", "2 Initial conditions Error reductions in the range 11 . 5 37 . 8 82 . 01 84 . 08 for word model 1 , and 81 . 53 88 . 5 for morhpeme model 2 were achieved by initializing the various models with context free approximations .", "While this observation confirms Elworthy 1994 , the impact of error reduction is much less than reported there for English about 70 79 94 .", "The key difference beside the unclear characteristic of Elworthy initial condition since he made use of an annotated corpus is the much higher quality of the uniform distribution for Hebrew .", "3 Model order The partial second order HMM 2 produced the best results for both word 85 . 75 and morpheme 88 . 5 models over the initial condition .", "The full second order HMM 2 didn t upgrade the accuracy of the partial second order , but achieved the best results for the uniform distribution morpheme model .", "This is because the context free approximation does not take into account the tag of the previous word , which is part of model 2 .", "We believe that initializing the morpheme model over a small set of annotated corpus will set much stronger initial condition for this model .", "4 Model type The main result of this paper is the error reduction of the morpheme model with respect to the word model about 19 . 3 85 . 75 88 . 5 .", "In addition , we apply the above models for the simpler task of segmentation and PoS tagging , as reported in Table 4 .", "The task requires picking the correct morphemes of each word with their correct PoS excluding all other morphological features .", "The best result for this task is obtained with the morpheme model 2 92 . 32 .", "For this simpler task , the improvement brought by the morpheme model over the word model is less significant , but still consists of a 5 error reduction .", "Unknown words account for a significant chunk of the errors .", "Table 5 shows the distribution of errors contributed by unknown words words that cannot be analyzed by the morphological analyzer .", "7 . 5 of the words in the test corpus are unknown 4 are not recognized at all by the morphological analyzer marked as None in the table , and for 3 . 5 , the set of analyses proposed by the analyzer does not contain the correct analysis Missing .", "We extended the lexicon to include missing and none lexemes of the closed sets .", "In addition , we modified the analyzer to extract all possible segmentations of unknown words , with all the possible tags for the segmented affixes , where the remaining unknown baseforms are tagged as UK .", "The model was trained over this set .", "In the next phase , the corpus was automatically tagged , according to the trained model , in order to form a tag distribution for each unknown word , according to its context and its form .", "Finally , the tag for each unknown word were selected according to its tag distribution .", "This strategy accounts for about half of the 7 . 5 unknown words .", "Table 6 shows the confusion matrix for known words 5 and up .", "The key confusions can be attributed to linguistic properties of Modern Hebrew most Hebrew proper names are also nouns and they are not marked by capitalization which explains the PN N confusion .", "The verb noun and verb adjective confusions are explained by the nature of the participle form in Hebrew beinoni participles behave syntactically almost in an identical manner as nouns .", "In this work , we have introduced a new text encoding method that captures rules of word formation in a language with affixational morphology such as Hebrew .", "This text encoding method allows us to learn in parallel segmentation and tagging rules in an unsupervised manner , despite the high ambiguity level of the morphological data average number of 2 . 4 analyses per word .", "Reported results on a large scale corpus 6M words with fully unsupervised learning are 92 . 32 for PoS tagging and 88 . 5 for full morphological disambiguation .", "In this work , we used the backoff smoothing method , suggested by Thede and Harper 1999 , with an extension of additive smoothing Chen , 1996 , 2 . 2 . 1 for the lexical probabilities B and B2 matrices .", "To complete this study , we are currently investigating several smoothing techniques Chen , 1996 , in order to check whether the morpheme model is critical for the data sparseness problem , or whether it can be handled with smoothing over a word model .", "We are currently investigating two major methods to improve our results first , we have started gathering a larger corpus of manually tagged text and plan to perform semi supervised learning on a corpus of 100K manually tagged words .", "Second , we plan to improve the unknown word model , such as integrating it with named entity recognition system Ben Mordechai , 2005 ."], "summary_lines": ["An Unsupervised Morpheme-Based HMM For Hebrew Morphological Disambiguation\n", "Morphological disambiguation is the process of assigning one set of morphological features to each individual word in a text.\n", "When the word is ambiguous (there are several possible analyses for the word), a disambiguation procedure based on the word context must be applied.\n", "This paper deals with morphological disambiguation of the Hebrew language, which combines morphemes into a word in both agglutinative and fusional ways.\n", "We present an unsupervised stochastic model - the only resource we use is a morphological analyzer - which deals with the data sparseness problem caused by the affixational morphology of the Hebrew language.\n", "We present a text encoding method for languages with affixational morphology in which the knowledge of word formation rules (which are quite restricted in Hebrew) helps in the disambiguation.\n", "We adapt HMM algorithms for learning and searching this text representation, in such a way that segmentation and tagging can be learned in parallel in one step.\n", "Results on a large scale evaluation indicate that this learning improves disambiguation for complex tag sets.\n", "Our method is applicable to other languages with affix morphology.\n", "We provide for each word not only the PoS, but also full morphological features, such as Gender, Number, Person, Construct, Tense, and the affixes' properties.\n", "We present a lattice-based modification of the BaumWelch algorithm to handle the segmentation ambiguity.\n"]}
{"article_lines": ["An Efficient Method For Determining Bilingual Word Classes", "In statistical natural language processing we always face the problem of sparse data .", "One way to reduce this problem is to group words into equivalence classes which is a standard method in statistical language modeling .", "In this paper we describe a method to determine bilingual word classes suitable for statistical machine translation .", "We develop an optimization criterion based on a maximumlikelihood approach and describe a clustering algorithm .", "We will show that the usage of the bilingual word classes we get can improve statistical machine translation .", "Word classes are often used in language modelling to solve the problem of sparse data .", "Various clustering techniques have been proposed Brown et al . , 1992 ; Jardino and Adda , 1993 ; Martin et al . , 1998 which perform automatic word clustering optimizing a maximum likelihood criterion with iterative clustering algorithms .", "In the field of statistical machine translation we also face the problem of sparse data .", "Our aim is to use word classes in statistical machine translation to allow for more robust statistical translation models .", "A naive approach for doing this would be the use of mono lingually optimized word classes in source and target language .", "Unfortunately we can not expect these independently optimized classes to be correspondent .", "Therefore mono lingually optimized word classes do not seem to be useful for machine translation see also Fung and Wu , 1995 .", "We define bilingual word clustering as the process of forming corresponding word classes suitable for machine translation purposes for a pair of languages using a parallel training corpus .", "The described method to determine bilingual word classes is an extension and improvement of the method mentioned in Och and Weber , 1998 .", "Our approach is simpler and computationally more efficient than Wang et al . , 1996 .", "The task of a statistical language model is to estimate the probability Pr wiev of a sequence of words wiv wi 'WN .", "A simple approximation of Pr 41 is to model it as a product of bigram probabilities Pr wPI HiN_ , p wi 2_1 .", "If we want to estimate the bigram probabilities p wlw' using a realistic natural language corpus we are faced with the problem that most of the bigrams are rarely seen .", "One possibility to solve this problem is to partition the set of all words into equivalence classes .", "The function C maps words w to their classes C w .", "Rewriting the corpus probability using classes we arrive at the following probability model p wiv IC In this model we have two types of probabilities the transition probability p C1C1 for class C given its predecessor class C' and the membership probability p wIC for word w given class C . To determine the optimal classes C for a given number of classes M we perform a maximumlikelihood approach arg mrc p wiv IC 2 We estimate the probabilities of Eq .", "1 by relative frequencies p CIC quot ; n C1C1' In C' , p wIC n w In C .", "The function n provides the frequency of a uni or bigram in the training corpus .", "If we insert this into Eq .", "2 and apply the negative logarithm and change the summation order we arrive at the following optimization Proceedings of EACL '99 criterion LP , .", "Kneser and Ney , 1991 The function h n is a shortcut for n log n .", "It is necessary to fix the number of classes in C in advance as the optimum is reached if every word is a class of its own .", "Because of this it is necessary to perform an additional optimization process which determines the number of classes .", "The use of leaving one out in a modified optimization criterion as in Kneser and Ney , 1993 could in principle solve this problem .", "An efficient optimization algorithm for LPI is described in section 4 .", "In bilingual word clustering we are interested in classes F and E which form partitions of the vocabulary of two languages .", "To perform bilingual word clustering we use a maximum likelihood approach as in the monolingual case .", "We maximize the joint probability of a bilingual training corpus To perform the maximization of Eq .", "6 we have to model the monolingual a priori probability p ef1E and the translation probability p f lel ; e , F .", "For the first we use the class based bigram probability from Eq .", "To model p fillef ; E , . 7 we assume the existence of an alignment af .", "We assume that every word fj is produced by the word ea , at position a3 in the training corpus with the probability P filea , The word alignment ail is trained automatically using statistical translation models as described in Brown et al . , 1993 ; Vogel et al . , 1996 .", "The idea is to introduce the unknown alignment a as hidden variable into a statistical model of the translation probability p glef .", "By applying the EMalgorithm we obtain the model parameters .", "The alignment cif that we use is the Viterbi Alignment of an HMM alignment model similar to Vogel et al . , 1996 .", "By rewriting the translation probability using word classes , we obtain corresponding to Eq .", "1 8 The variables F and E denote special classes in and E . We use relative frequencies to estimate p FIE and p fIF The function nt FIE counts how often the words in class F are aligned to words in class E . If we insert these relative frequencies into Eq .", "8 and apply the same transformations as in the monolingual case we obtain a similar optimization criterion for the translation probability part of Eq .", "Thus the full optimization criterion for bilingual word classes is The two count functions n E1E and nt FIE can be combined into one count function ng X1Y n X1Y nt X1Y as for all words f and all words e and e' holds n f le 0 and nt ele1 0 .", "Using the function n9 we arrive at the following optimization criterion Here we defined ng , i X Ex , ng XIX' and n9 , 2 X Ex , n9 X11X .", "The variable X runs over the classes in and F . In the optimization process it cannot be allowed that words of different languages occur in one class .", "It can be seen that Eq .", "3 is a special case of Eq .", "9 with ng , 1 n9 , 2 .", "Another possibility to perform bilingual word clustering is to apply a two step approach .", "In a first step we determine classes S optimizing only the monolingual part of Eq .", "6 and secondly we determine classes F optimizing the bilingual part without changing 6 By using these two optimization processes we enforce that the classes E are mono lingually 'good' classes and that the classes . 7 correspond to 6 .", "Interestingly enough this results in a higher translation quality see section 5 .", "An efficient optimization algorithm for LPI is the exchange algorithm Martin et al . , 1998 .", "For the optimization of LP2 we can use the same algorithm with small modifications .", "Our starting point is a random partition of the training corpus vocabulary .", "This initial partition is improved iteratively by moving a single word from one class to another .", "The algorithm to determine bilingual classes is depicted in Figure 1 .", "If only one word w is moved between the partitions C and C' the change LP C , n9 LP C' , n9 can be computed efficiently looking only at classes C for which ng w , C 0 or ng C , w 0 .", "We define Mc , to be the average number of seen predecessor and successor word classes .", "With the notation I for the number of iterations needed for convergence , B for the number of word bigrams , M for the number of classes and V for the vocabulary size the computational complexity of this algorithm is roughly I B log2 B IV V M Mo .", "A detailed analysis of the complexity can be found in Martin et al . , 1998 .", "The algorithm described above provides only a local optimum .", "The quality of the resulting local optima can be improved if we accept a short term degradation of the optimization criterion during the optimization process .", "We do this in our implementation by applying the optimization method threshold accepting Dueck and Scheuer , 1990 which is an efficient simplification of simulated annealing .", "The statistical machine translation method described in Och and Weber , 1998 makes use of bilingual word classes .", "The key element of this approach are the alignment templates originally referred to as translation rules which are pairs of phrases together with an alignment between the words of the phrases .", "Examples of alignment templates are shown in Figure 2 .", "The advantage of the alignment template approach against word based statistical translation models is that word context and local re orderings are explicitly taken into account .", "The alignment templates are automatically trained using a parallel training corpus .", "The translation of a sentence is done by a search process which determines the set of alignment templates which optimally cover the source sentence .", "The bilingual word classes are used to generalize the applicability of the alignment templates in search .", "If there exists a class which contains all cities in source and target language it is possible that an alignment template containing a special city can be generalized to all cities .", "More details are given in Och and Weber , 1998 ; Och and Ney , 1999 .", "We demonstrate results of our bilingual clustering method for two different bilingual corpora see Tables 1 and 2 .", "The EuTRANs I corpus is a subtask of the quot ; Traveller Task quot ; Vidal , 1997 which is an artificially generated Spanish English corpus .", "The domain of the corpus is a humanto human communication situation at a reception Table 3 Example of bilingual word classes corpus EuTRANs I , method BIL 2 .", "El how it pardon what when where which who why E2 my our E3 today tomorrow E4 ask call make E5 carrying changing giving looking moving putting sending showing waking E6 full half quarter Si c'omo cu'al cu'ando cu'anta d'onde dice dicho hace qu'e qui'en tiene desk of a hotel .", "The EuTRANs II corpus is a natural German English corpus consisting of different text types belonging to the domain of tourism bilingual Web pages of hotels , bilingual touristic brochures and business correspondence .", "The target language of our experiments is English .", "We compare the three described methods to generate bilingual word classes .", "The classes MONO are determined by monolingually optimizing source and target language classes with Eq .", "The classes BIL are determined by bilingually optimizing classes with Eq .", "The classes BIL 2 are determined by first optimizing mono lingually classes for the target language English and afterwards optimizing classes for the source language Eq .", "11 and Eq .", "For EuTRANs I we used 60 classes and for EuTRANs II we used 500 classes .", "We chose the number of classes in such a way that the final performance of the translation system was optimal .", "The CPU time for optimization of bilingual word classes on an Alpha workstation was under 20 seconds for EuTRANs I and less than two hours for EuTRANs II .", "Table 3 provides examples of bilingual word classes for the EuTRANs I corpus .", "It can be seen that the resulting classes often contain words that are similar in their syntactic and semantic functions .", "The grouping of words with a different meaning like today and tomorrow does not imply that these words should be translated by the same Spanish word , but it does imply that the translations of these words are likely to be in the same Spanish word class .", "To measure the quality of our bilingual word classes we applied two different evaluation measures exp J 1 E maxi log p C f j ei 3 1 Both measures determine the extent to which the translation probability is spread out .", "A small value means that the translation probability is very focused and that the knowledge of the source language class provides much information about the target language class . sertions deletions substitutions relative to a reference translation .", "As expected the translation quality improves using classes .", "For the small EuTRANs I task the word error rates reduce significantly .", "The word error rates for the EuTRANs II task are much larger because the task has a very large vocabulary and is more complex .", "The bilingual classes show better results than the monolingual classes MONO .", "One explanation for the improvement in translation quality is that the bilingually optimized classes result in an increased average size of used alignment templates .", "For example the average length of alignment templates with the EuTRANs I corpus using WORD is 2 . 85 and using BIL 2 it is 5 . 19 .", "The longer the average alignment template length , the more context is used in the translation and therefore the translation quality is higher .", "An explanation for the superiority of BIL 2 over BIL is that by first optimizing the English classes mono lingually , it is much more probable that longer sequences of classes occur more often thereby increasing the average alignment template size .", "By applying a maximum likelihood approach to the joint probability of a parallel corpus we obtained an optimization criterion for bilingual word classes which is very similar to the one used in monolingual maximum likelihood word clustering .", "For optimization we used the exchange algorithm .", "The obtained word classes give a low translation lexicon perplexity and improve the quality of staProceedings of EACL '99 tistical machine translation .", "We expect improvements in translation quality by allowing that words occur in more than one class and by performing a hierarchical clustering .", "Acknowledgements This work has been partially supported by the European Community under the ESPRIT project number 30268 EuTrans ."], "summary_lines": ["An Efficient Method For Determining Bilingual Word Classes\n", "In statistical natural language processing we always face the problem of sparse data.\n", "One way to reduce this problem is to group words into equivalence classes which is a standard method in statistical language modeling.\n", "In this paper we describe a method to determine bilingual word classes suitable for statistical machine translation.\n", "We develop an optimization criterion based on a maximum-likelihood approach and describe a clustering algorithm.\n", "We will show that the usage of the bilingual word classes we get can improve statistical machine translation.\n", "We show improvements on perplexity of bilingual corpus, and word translation accuracy using a template-based translation model.\n", "We describe a method for determining bilingual word classes, used to improve the extraction of alignment templates through alignments between classes, not only between words.\n"]}
{"article_lines": ["Phrasal Cohesion And Statistical Machine Translation", "There has been much interest in using phrasal movement to improve statistical machine translation .", "We explore how well phrases cohere across two languages , specifically English and French , and examine the particular conditions under which they do not .", "We demonstrate that while there are cases where coherence is poor , there are many regularities which can be exploited by a statistical machine translation system .", "We also compare three variant syntactic representations to determine which one has the best properties with respect to cohesion .", "Statistical machine translation SMT seeks to develop mathematical models of the translation process whose parameters can be automatically estimated from a parallel corpus .", "The first work in SMT , done at IBM Brown et al . , 1993 , developed a noisy channel model , factoring the translation process into two portions the translation model and the language model .", "The translation model captures the translation of source language words into the target language and the reordering of those words .", "The language model ranks the outputs of the translation model by how well they adhere to the syntactic constraints of the target language . 1 The prime deficiency of the IBM model is the reordering component .", "Even in the most complex of 'Though usually a simple word n gram model is used for the language model . the five IBM models , the reordering operation pays little attention to context and none at all to higherlevel syntactic structures .", "Many attempts have been made to remedy this by incorporating syntactic information into translation models .", "These have taken several different forms , but all share the basic assumption that phrases in one language tend to stay together i . e . cohere during translation and thus the word reordering operation can move entire phrases , rather than moving each word independently .", "Yarowsky et al . , 2001 states that during their work on noun phrase bracketing they found a strong cohesion among noun phrases , even when comparing English to Czech , a relatively free word order language .", "Other than this , there is little in the SMT literature to validate the coherence assumption .", "Several studies have reported alignment or translation performance for syntactically augmented translation models Wu , 1997 ; Wang , 1998 ; Alshawi et al . , 2000 ; Yamada and Knight , 2001 ; Jones and Havrilla , 1998 and these results have been promising .", "However , without a focused study of the behavior of phrases across languages , we cannot know how far these models can take us and what specific pitfalls they face .", "The particulars of cohesion will clearly depend upon the pair of languages being compared .", "Intuitively , we expect that while French and Spanish will have a high degree of cohesion , French and Japanese may not .", "It is also clear that if the cohesion between two closely related languages is not high enough to be useful , then there is no hope for these methods when applied to distantly related languages .", "For this reason , we have examined phrasal cohesion for French and English , two languages which are fairly close syntactically but have enough differences to be interesting .", "An alignment is a mapping between the words in a string in one language and the translations of those words in a string in another language .", "Given an English string , , and a French string , , an alignment a can be represented by .", "Each is a set of indices into where indicates that word in the French sentence is aligned with word in the English sentence . indicates that English word has no corresponding French word .", "Given an alignment and an English phrase covering words , the span is a pair where the first element is and the second element is .", "Thus , the span includes all words between the two extrema of the alignment , whether or not they too are part of the translation .", "If phrases cohere perfectly across languages , the span of one phrase will never overlap the span of another .", "If two spans do overlap , we call this a crossing .", "Figure 1 shows an example of an English parse along with the alignment between the English and French words shown with dotted lines .", "The English word not is aligned to the two French words ne and pas and thus has a span of 1 , 3 .", "The main English verb change is aligned to the French modifie and has a span of 2 , 2 .", "The two spans overlap and thus there is a crossing .", "This definition is asymmetric i . e . what is a crossing when moving from English to French is not guaranteed to be a crossing when moving from French to English .", "However , we only pursue translation direction since that is the one for which we have parsed data .", "To calculate spans , we need aligned pairs of English and French sentences along with parses for the English sentences .", "Our aligned data comes from a corpus described in Och and Ney , 2000 which contains 500 sentence pairs randomly selected from the Canadian Hansard corpus and manually aligned .", "The alignments are of two types sure S and possible P .", "S alignments are those which are unambiguous while P alignments are those which are less certain .", "P alignments often appear when a phrase in one language translates as a unit into a phrase in the other language e . g . idioms , free translations , missing function words but can also be the result of genuine ambiguity .", "When two annotators disagree , the union of the P alignments produced by each annotator is recorded as the P alignment in the corpus .", "When an S alignment exists , there will always also exist a P alignment such that P S . The English sentences were parsed using a state of the art statistical parser Charniak , 2000 trained on the University of Pennsylvania Treebank Marcus et al . , 1993 . je invoque le R eglement Since P alignments often align phrasal translations , the number of crossings when P alignments are used will be artificially inflated .", "For example , in Figure 2 note that every pair of English and French words under the verb phrase is aligned .", "This will generate five crossings , one each between the pairs VBP PP , IN NP , NP PP , NN DT , and IN NP .", "However , what is really happening is that the whole verb phrase is first being moved without crossing anything else and then being translated as a unit .", "For our purposes we want to count this example as producing zero crossings .", "To accomplish this , we defined a simple heuristic to detect phrasal translations so we can filter them if desired .", "After calculating the French spans from the English parses and alignment information , we counted crossings for all pairs of child constituents in each constituent in the sentence , maintaining separate counts for those involving the head constituent of the phrase and for crossings involving modifiers only .", "We did this while varying conditions along two axes alignment type and phrasal translation filtering .", "Recalling the two different types of alignments , S and P , we examined three different conditions S alignments only , P alignments only , or S alignments where present falling back to P alignments S P .", "For each of these conditions , we counted crossings both with and without using the phrasal translation filter .", "For a given alignment type S , S P , P , let if phrases and cross each other and otherwise .", "Let if the phrasal translation filter is turned off .", "If the filter is on , , modifier constituents , and child constituents and for a particular alignment type , the number of head crossings and modifier crossings can be calculated recursively", "Table 1 shows the average number of crossings per sentence .", "The table is split into two sections , one for results when the phrasal filter was used and one for when it was not .", "Alignment Type refers to whether we used S , P or S P as the alignment data .", "The Head Crossings line shows the results when comparing the span of the head constituent of a phrase with the spans of its modifier constituents , and Modifier Crossings refers to the case where we compare the spans of pairs of modifiers .", "The Phrasal Translations line shows the average number of phrasal translations detected per sentence .", "For S alignments , the results are quite promising , with an average of only 0 . 236 head crossings per sentence and an even smaller average for modifier crossings 0 . 056 .", "However , these results are overly optimistic since often many words in a sentence will not have an S alignment at all , such as coming , in , and before in following example the full report will be coming in before the fall le rapport complet sera d epos e de ici le automne prochain When we use P alignments for these unaligned words the S P case , we get a more meaningful result .", "Both types of crossings are much more frequent 4 . 790 for heads and 0 . 88 for modifiers and Then , for a given phrase with head constituent if and are part of a phrasal translation in alignment otherwise phrasal translation filtering has a much larger effect reducing head average to 2 . 772 and modifier average to 0 . 516 .", "Phrasal translations account for almost half of all crossings , on average .", "This effect is even more pronounced in the case where we use P alignments only .", "This reinforces the importance of phrasal translation in the development of any translation system .", "Even after filtering , the number of crossings in the S P case is quite large .", "This is discouraging , however there are reasons why this result should be looked on as more of an upper bound than anything precise .", "For one thing , there are cases of phrasal translation which our heuristic fails to recognize , an example of which is shown in Figure 3 .", "The alignment of explorer with this and matter seems to indicate that the intention of the annotator was to align the phrase work this matter out , as a unit , to de explorer la question .", "However , possibly due to an error during the coding of the alignment , work and out align with de indicated by the solid lines while this and matter do not .", "This causes the phrasal translation heuristic to fail resulting in a crossing where there should be none .", "Also , due to the annotation guidelines , P alignments are not as consistent as would be ideal .", "Recall that in cases of annotator disagreement , the P alignment is taken to be the union of the P alignments of both annotators .", "Thus , it is possible for the P alignment to contain two mutually conflicting alignments .", "These composite alignments will likely generate crossings even where the alignments of each individual annotator would not .", "While reflecting genuine ambiguity , an SMT system would likely pursue only one of the alternatives and only a portion of the crossings would come into play .", "Our results show a significantly larger number of head crossings than modifier crossings .", "One possibility is that this is due to most phrases having a head and modifier pair to test , while many do not have multiple modifiers and therefore there are fewer opportunities for modifier crossings .", "Thus , it is informative to examine how many potential crossings actually turn out to be crossings .", "Table 2 provides this result in the form of the percentage of crossing tests which result in detection of a crossing .", "To calculate this , we kept totals for the number of head and modifier crossing tests performed as well as the number of phrasal translations detected .", "Note that when the phrasal translation filter is turned on , these totals differ for each of the different alignment types S , S P , and P .", "The percentages are calculated after summing over all sentencesin the corpus There are still many more crossings in the S P and P alignments than in the S alignments .", "The S alignment has 1 . 58 head crossings while the S P and P alignments have 32 . 16 and 35 . 47 respectively , with similar relative percentages for modifier crossings .", "Also as before , half to two thirds of crossings in the S P and P alignments are due to phrasal translations .", "More interestingly , we see that modifier crossings remain significantly less prevalent than head crossings e . g .", "14 . 45 vs . 32 . 16 for the S P case and that this is true uniformly across all parameter settings .", "This indicates that heads are more intimately involved with their modifiers than modifiers are with each other and therefore are more likely to be involved in semi phrasal constructions .", "Since it is clear that crossings are too prevalent to ignore , it is informative to try to understand exactly what constructions give rise to them .", "To that end , we examined by hand all of the head crossings produced using the S alignments with phrasal filtering .", "Table 3 shows the results of this analysis .", "The first thing to note is that by far most of the crossings do not reflect lack of phrasal cohesion between the two languages .", "Instead , they are caused either by errors in the syntactic analysis or the fact that translation as done by humans is a much richer process than just replication of the source sentence in another language .", "Sentences are reworded , clauses are reordered , and sometimes human translators even make mistakes .", "Errors in syntactic analysis consist mostly of attachment errors .", "Rewording and reordering accounted for a large number of crossings as well .", "In most of the cases of rewording see Figure 4 or relorsque nous avons pr epar e le budget , nous avons pris cela en consid eration ordering see Figure 5 a more parallel translation would also be valid .", "Thus , while it would be difficult for a statistical model to learn from these examples , there is nothing to preclude production of a valid translation from a system using phrasal movement in the reordering phase .", "The rewording and reordering examples were so varied that we were unable to find any regularities which might be exploited by a translation model .", "Among the cases which do result from language differences , the most common is the ne . . . pas construction e . g .", "Figure 1 .", "Fifteen percent of the 86 total crossings are due to this construction .", "Because ne . . . pas wraps around the verb , it will always result in a crossing .", "However , the types of syntactic structures categorized as context free grammar rules which are present in cases of negation are rather restricted .", "Of the 47 total distinct syntactic structures which resulted in crossings , only three of them involved negation .", "In addition , the crossings associated with these particular structures were unambiguously caused by negation i . e . for each structure , only negation related crossings were present .", "Next most common is the case where the English contains a modal verb which is aligned with the main verb in the French .", "In the example in Figure 6 , will be is aligned to sera indicated by the solid lines and because of the constituent structure of the English parse there is a crossing .", "As with negation , this type of crossing is quite regular , resulting uniquely from only two different syntactic structures .", "Many of the causes listed above are related to verb phrases .", "In particular , some of the adverb related crossings e . g .", "Figure 1 and all of the modal related crossings e . g .", "Figure 6 are artifacts of the nested verb phrase structure of our parser .", "This nesting usually does not provide any extra information beyond what could be gleaned from word order .", "Therefore , we surmised that flattening verb phrases would eliminate some types of crossings without reducing the utility of the parse .", "The flattening operation consists of identifying all nested verb phrases and splicing the children of the nested phrase into the parent phrase in its place .", "This procedure is applied recursively until there are no nested verb phrases .", "An example is shown in Figure 8 .", "Crossings can be calculated as before .", "Adverbs are a third common cause , as they typically follow the verb in French while preceding it in English .", "Figure 7 shows an example where the span of simplement overlaps with the span of the verb phrase beginning with tells indicated by the solid lines .", "Unlike negation and modals , this case is far less regular .", "It arises from six different syntactic constructions and two of those constructions are implicated in other types of crossings as well .", "Flattening reduces the number of potential head crossings while increasing the number of potential modifier crossings .", "Therefore , we would expect to see a comparable change to the number of crossings measured , and this is exactly what we find , as shown in Tables 4 and 5 .", "For example , for S P alignments , the average number of head crossings decreases from 2 . 772 to 2 . 252 , while the average number of modifier crossings increases from 0 . 516 to 0 . 86 .", "We see similar behavior when we look at the percentage of crossings per chance Tables 6 and 7 .", "For the same alignment type , the percentage of head crossings decreases from 18 . 61 to 15 . 12 , while the percentage of modifier crossings increases from 8 . 47 to 10 . 59 .", "One thing to note , however , is that the total number of crossings of both types detected in the corpus decreases as compared to the baseline , and thus the benefits to head crossings outweigh the detriments to modifier crossings .", "Our intuitions about the cohesion of syntactic structures follow from the notion that translation , as a meaning preserving operation , preserves the dependencies between words , and that syntactic structures encode these dependencies .", "Therefore , dependency structures should cohere as well as , or better than , their corresponding syntactic structures .", "To examine the validity of this , we extracted dependency structures from the parse trees with flattened verb phrases and calculated crossings for them .", "Figure 9 shows a parse tree and its corresponding dependency structure .", "The procedure for counting modifier crossings in a dependency structure is identical to the procedure for parse trees .", "For head crossings , the only difference is that rather than comparing spans of two siblings , we compare the spans of a child and its parent .", "Again focusing on the S P alignment case , we see that the average number of head crossings see Table 4 continues to decrease compared to the previous case from 2 . 252 to 1 . 88 , and that the average number of modifier crossings see Table 5 continues to increase from 0 . 86 to 1 . 498 .", "This time , however , the percentages for both types of crossings see Tables 6 and 7 decrease relative to the case of flattened verb phrases from 15 . 12 to 12 . 62 for heads and from 10 . 59 to 9 . 22 for modifiers .", "The percentage of modifier crossings is still higher than in the base case 9 . 22 vs . 8 . 47 .", "Overall , however , the dependency representation has the best cohesion properties . ernment .", "We would like to thank Franz Och for providing us with the manually annotated data used in these experiments .", "We have examined the issue of phrasal cohesion between English and French and discovered that while there is less cohesion than we might desire , there is still a large amount of regularity in the constructions where breakdowns occur .", "This reassures us that reordering words by phrasal movement is a reasonable strategy .", "Many of the initially daunting number of crossings were due to non linguistic reasons , such as rewording during translation or errors in syntactic analysis .", "Among the rest , there are a small number of syntactic constructions which result in the majority of the crossings examined in our analysis .", "One practical result of this skewed distribution is that one could hope to discover the major problem areas for a new language pair by manually aligning a small number of sentences .", "This information could be used to filter a training corpus to remove sentences which would cause problems in training the translation model , or for identifying areas to focus on when working to improve the model itself .", "We are interested in examining different language pairs as the opportunity arises .", "We have also examined the differences in cohesion between Treebank style parse trees , trees with flattened verb phrases , and dependency structures .", "Our results indicate that the highest degree of cohesion is present in dependency structures .", "Therefore , in an SMT system which is using some type of phrasal movement during reordering , dependency structures should produce better results than raw parse trees .", "In the future , we plan to explore this hypothesis in an actual translation system .", "The work reported here was supported in part by the Defense Advanced Research Projects Agency under contract number N66001 00 C 8008 .", "The views and conclusions contained in this document are those of the author and should not be interpreted as necessarily representing the official policies , either expressed or implied , of the Defense Advanced Research Projects Agency or the United States Gov"], "summary_lines": ["Phrasal Cohesion And Statistical Machine Translation\n", "There has been much interest in using phrasal movement to improve statistical machine translation.\n", "We explore how well phrases cohere across two languages, specifically English and French, and examine the particular conditions under which they do not.\n", "We demonstrate that while there are cases where coherence is poor, there are many regularities which can be exploited by a statistical machine translation system.\n", "We also compare three variant syntactic representations to determine which one has the best properties with respect to cohesion.\n", "We measure phrasal cohesion in gold standard alignments by counting crossings.\n", "We compare tree-bank parser style analyses, a variant with flattened VPs and dependency structures.\n"]}
{"article_lines": ["Deterministic Parsing Of Syntactic Non Fluencies", "Discussion the rules for Fidditch are written as deterministic pattern action rules of the same sort as the rules in the parsing grammar , their operation is in a sense isolable .", "The patterns of the self correction rules are checked first , before any of the grammar rule patterns are checked , at each step in the parse .", "Despite this independence in terms of rule ordering , the operation of the self corr , ction component is closely tied to the grammar of the parser ; for it is the parsing grammar that specifies what sort of constituents count as the same for copying . example , if the grammar did not treat a noun phrase when it is subject of a sentence , the self correction rules could not properly resolve a sentence like People a people from Kennsington the editing rules would never recognize that the same sort of element .", "Note 13 treated as a Restart because lexical trigger is not present .", "Thus , the observed pattern of self correction introduces empirical constraints on the set of features that are available for syntactic rules .", "The self correction rules impose constraints not only on what linguistic elements must count as the same , but also on what must count as different .", "For example , in sentence be recognized as different sorts of elements in the grammar for the AUX node to be correctly the grammar assigned the words exactly the same part of speech , then the Category C ; . 7y Editor necessarily apply , incorrectly expunging 14 Kid could be a brain in school .", "It appears therefore that the pattern of self corrections that occur represents a potentially rich source of evidence about the nature of syntactic categories . the patterns of self correction count as about the nature of categories for the linguist , then this data must be equally available to the language learner .", "This would suggest that , far from being an impediment to language learning , non fluencies may in fact facilitate language acquisition by highlighting equivalent classes . expunction of edit signal only surface copy category copy stack copy restart failures remaining unclear and ungrammatical 127 This raises the general question of how children can acquire a language in the face of unrestrained non fluency .", "How can a language learner sort out the grammatical from the ungrammatical strings ?", "The non fluencies of speech are of course but one aspect of the degeneracy of input that makes language acquisition a puzzle .", "The self correction system I have described suggests that many non fluent strings can be resolved with little detailed linguistic knowledge .", "As Table 1 shows , about a quarter of the editing signals result in expunction of only non linguistic material .", "This requires only an ability to distinguish linguistic from nonlinguistic stuff , and it introduces the idea that edit signals signal an expunction site .", "Almost a third are resolved by the Surface Copying rule , which can be viewed simply as an instance of the general non linguistic rule that multiple instances of the same thing count as a single instance .", "The category copying rules are generalizations of simple copying , applied to a knowledge of linguistic categories .", "Making the transition from surface copies to category copies is aided by the fact that there is considerable overlap in coverage , defining a path of expanding generalization .", "Thus at the earliest stages of learning , only the simplest , non linguistic self correction rules would come into play , and gradually the more syntactically integrated would be acquired .", "Contrast this self correction system to an approach that handles non fluencies by some general problem solving routines , for example Granger 1982 , who proposes reasoning from what a speaker might be expected to say .", "Besides the obvious inefficiencies of general problem solving approaches , it is worth giving special emphasis to the problem with learnability .", "A general problem solving approach depends crucially on evaluating the likelihood of possible deviations from the norms .", "But a language learner has by definition only partial and possibly incorrect knowledge of the syntax , and is therefore unable to consistently identify deviations from the grammatical system .", "With the editing system I describe , the learner need not have the ability to recognize deviations from grammatical norms , but merely the non linguistic ability to recognize copies of the same thing . far , I have considered the selfcorrection component from the standpoint of parsing .", "However , it is clear that the origins are in the process of generation .", "The mechanism for editing self corrections that I have proposed has as its essential operation expunging one two identical is unable to expunge a sequence of two elements .", "The Surface Copy Editor might be viewed as a counterexample to this claim , but see below .", "Consider expunction now from the standpoint of the generator .", "Suppose self correction bears a one to one relationship to a possible action of the generator initiated by some monitoring component which could be called ABANDON CONSTRUCT X .", "And suppose that this action can be initiated at any time up until CONSTRUCT X is completed , when a signal is returned that the construction is complete .", "Further suppose that ABANDON CONSTRUCT X causes an editing signal .", "When the speaker decides in the middle of some linguistic element to abandon it and start again , an editing signal is produced .", "If this is an appropriate model , then the elements which are self corrected should be exactly those elements that xist at some stage in the generation process .", "Thus , we should be able to find evidence for the units involved in generation by looking at the data of self correction .", "And indeed , such evidence should be available to the language learner as well .", "Summary I have described the nature of self corrected speech which is a major source of spoken non fluencies and how it can be resolved by simple editing rules within the context of a deterministic parser .", "Two features are essential to the self correction system 1 every self correction site whether it results in the expunction of words or not is marked by a phonetically identifiable signal placed at the right edge of the potential expunction site ; and 2 the expunged part is the left hand member of a pair of copies , one on each side of the editing signal .", "The copies may be of three types 1 identical surface strings , which are edited by a matching rule that applies before syntactic analysis begins ; 2 complete constituents , when two constituents of the same type appear in the parser's buffer ; or 3 incomplete constituents , when the parser finds itself trying to complete a constituent of the same type as a constituent it has just completed .", "Whenever two such copies appear in such a configuration , and the first one ends with an editing signal , the first is expunged from further analysis .", "This editing system has been implemented as part of a deterministic parser , and tested on a wide range of sentences from transcribed speech .", "Further study of the self correction system promises to provide insights into the units of production and the nature of linguistic categories .", "Bell Laboratories Murray Hill , New Jersey 07974 It is often remarked that natural language , used naturally , is unnaturally ungrammatical .", "Spontaneous speech contains all manner of false starts , hesitations , and self corrections that disrupt the well formedness of strings .", "It is a mystery then , that despite this apparent wide deviation from grammatical norms , people have little difficulty understanding the non fluent speech that is the essential medium of everyday life .", "And it is a still greater mystery that children can succeed in acquiring the grammar of a language on the basis of evidence provided by a mixed set of apparently grammatical and ungrammatical strings .", "In this paper I present a system of rules for resolving the non fluencies of speech , implemented as part of a computational model of syntactic processing .", "The essential idea is that non fluencies occur when a speaker corrects something that he or she has already said out loud .", "Since words once said cannot be unsaid , a speaker can only accomplish a self correction by saying something additional namely the intended words .", "The intended words are supposed to substitute for the wrongly produced words .", "For example , in sentence 1 , the speaker initially said but meant we .", "The problem for the hearer , as for any natural language understanding system , is to determine what words are to be expunged from the actual words said to find the intended sentence .", "Labov 1966 provided the key to solving this problem when he noted that a phonetic signal specifically , a markedly abrupt cut off of the speech signal always marks the site where self correction takes place .", "Of course , finding the site of a self correction is only half the problem ; it remains to specify what should be removed .", "A first guess suggests that this must be a non deterministic problem , requiring complex reasoning about what the speaker meant to say .", "Labov claimed that a simple set of rules operating on the surface string would specify exactly what should be changed , transforming nearly all non fluent strings into fully grammatical sentences .", "The specific set of transformational rules Labov proposed were not formally adequate , in part because they were surface transformations which ignored syntactic constituenthood .", "But his work forms the basis of this current analysis .", "This research was done for the roost part at the University of Pennsylvania . supported by the National Institute of Education under grants G78 0169 and G80 0163 .", "Labov's claim was not of course that ungrammatical sentences are never produced in speech , for that clearly would be false .", "Rather , it seems that truly ungrammatical productions represent only a tiny fraction of the spoken output , and in the preponderance of cases , an apparent ungrammaticality can be resolved by simple editing rules .", "In order to make sense of non fluent speech , it is essential that the various types of grammatical deviation be distinguished .", "This point has sometimes been missed , and fundamentally different kinds of deviation from standard grammaticality have been treated together because they all present the same sort of problem for a natural language understanding system .", "For example , Hayes and Mouradian 1981 mix together speaker initiated self corrections with fragmentary sentences of all sorts people often leave out or repeat words or phrases , break off what they are saying and rephrase or replace it , speak in fragments , or otherwise use incorrect grammar 1981 231 .", "Ultimately , it will be essential to distinguish between nonfluent productions on the one hand , and constructions that are fully grammatical though not yet understood , on the other .", "Although we may not know in detail the correct characterization of such processes as ellipsis and conjunction , they are without doubt fully productive grammatical processes .", "Without an understanding of the differences in the kinds of non fluencies that occur , we are left with a kind cf grab bag of grammatical deviation that can never be analyzed except by some sort of general purpose mechanisms .", "In this paper , I want to characterize the subset of spoken non fluencies that can be treated as self corrections , and to describe how they are handled in the context of a deterministic parser .", "I assume that a system for dealing with self corrections similar to the one I describe roust be a part of the competence of any natural language user .", "I will begin by discussing the range of non fluencies that occur in speech .", "Then , after reviewing the notion of deterministic parsing , I will describe the model of parsing self corrections in detail , and report results from a sample of 1500 sentences .", "Finally , I discuss some implications of this theory of self correction , particularly for the problem of language acquisition .", "Linguists have been of less help in describing the nature of spoken non fluencies than might have been hoped ; relatively little attention has been devoted to the actual performance of speakers , and studies that claim to be based on performance data seem to ignore the problem of nonfluencies .", "Notable exceptions include Frornkin 1980 , and Thompson 1980 .", "For the discussion of self correction , I want to distinguish three types of non fluencies that typically occur in speech .", "Sentence 2a is an example of non standard subject relative clauses that are common in speech .", "Sentence 2b , which seems to have two tensed quot ; be quot ; verbs in one clause is a productive sentence type that occurs regularly , though rarely , in all sorts of spoken discourse see Kroch and Hindle 1981 .", "I assume that a correct and complete grammar for a parser will have to deal with all grammatical processes , marginal as well as central .", "I have nothing further to say about unusual constructions here .", "True Ungrammaticalities .", "A small percentage of spoken utterances are truly ungrammatical .", "That is , they do not result from any regular grammatical process however rare , nor are they instances of successful self correction .", "Unexceptionable examples are hard to find , but the following give the flavor . focus of this paper .", "Self corrected strings all have the characteristic that some extraneous material was apparently inserted , and that expunging some substring results in a well formed syntactic structure , which is apparently consistent with the meaning that is intended .", "In the degenerate case , self correction inserts non lexical material , which the syntactic processor ignores , as in 4 .", "The minimal non lexical material that self correction might insert is the editing signal itself .", "Other cases examples 610 below are only interpretable given the assumption that certain words , which are potentially part of the syntactic structure , are to be removed from the syntactic analysis .", "The status of the material that is corrected by selfcorrection and is expunged by the editing rules is somewhat odd .", "I use the term expunction to mean that it is removed from any further syntactic analysis .", "This does not mean however that a self corrected string is unavailable for semantic processing .", "Although the self corrected string is edited from the syntactic analysis , it is nevertheless available for semantic interpretation .", "Jefferson 1974 discusses the example 5 . . . thuh thiy officer . . . where the initial , self corrected string with the preconsonantal form of the rather than the pre vocalic form makes it clear that the speaker originally intended to refer to the police by some word other than officer .", "I should also note that the problems addressed by the self correction component that I am concerned with are only part of the kind of deviance that occurs in natural language use .", "Many types of naturally occurring errors are not part of this system , for example , phonological and semantic errors .", "It is reasonable to hope that much of this dreck will be handled by similar subsystems .", "Of course , there will always remain errors that are outside of any system .", "But we expect that the apparent chaos is much more regular than it at first appears and that it can be modeled by the interaction of components that are themselves simple .", "In the following discussion , I use the terms self . correction and editing more or less interchangeably , though the two terms emphasize the generation and interpretation aspects of the same process .", "The editing system that I will describe is implemented on top of a deterministic parser , called Fidditch . based on the processing principles proposed by Marcus 1980 .", "It takes as input a sentence of standard words and returns a labeled bracketing that represents the syntactic structure as an annotated tree structure .", "Fidditch was 'designed to process transcripts of spontaneous speech , and to produce an analysis , partial if necessary , for a large corpus of interview transcripts .", "Because it is a deterministic parser , it produces only one analysis for each sentence .", "When Fidditch is unable to build larger constituents out of subphrases , it moves on to the next constituent of the sentence .", "In brief , the parsing process proceeds as follows .", "The words in a transcribed sentence where sentence means one tensed clause together with all subordinate clauses are assigned a lexical category or set of lexical categories on the basis of a 2000 word lexicon and a morphological analyzer .", "The lexicon contains , for each word , a list of possible lexical categories , subcategorization information , and in a few cases , information on compound words .", "For example , the entry for round states that it is a noun , verb , adjective or preposition , that as a verb it is subcategorized for the movable particles out and up and for NP , and that it may be part of the compound adjective preposition round about .", "Once the lexical analysis is complete , The phrase structure tree is constructed on the basis of pattern action rules using two internal data structures 1 a push down stack of incomplete nodes , and 2 a buffer of complete constituents , into which the grammar rules can look through a window of three constituents .", "The parser matches rule patterns to the configuration of the window and stack .", "Its basic actions include starting to build a new node by pushing a category onto the stack attaching the first element of the window to the stack dropping subtrees from the stack into the first position in the window when they are complete .", "The parser proceeds deterministically in the sense that no aspect of the tree structure , once built may be altered by any rule .", "See Marcus 1980 for a comprehensive discussion of this theory of parsing .", "The self correction rules specify how much , if anything , to expunge when an editing signal is detected .", "The rules depend crucially on being able to recognize an editing signal , for that marks the right edge of an expunction site .", "For the present discussion , I will assume little about the phonetic nature of the signal except that it is phonetically recognizable , and that , whatever their phonetic nature , all editing signals are , for the self correction system , equivalent .", "Specifying the nature of the editing signal is , obviously , an area where further research is needed .", "The only action that the editing rules can perform is expunction , by which I mean removing an element from the view of the parser .", "The rules never replace one element with another or insert an element in the parser data structures .", "However , both replacements and insertions can be accomplished within the self correction system by expunction of partially identical strings .", "For example , in The self correction rules will expunge the I am which precedes the editing signal , thereby in effect replacing am with was and inserting really .", "Self corrected strings can be viewed formally as having extra material inserted , but not involving either deletion or replacement of material .", "The linguistic system does seem to make use of both deletions and replacements in other subsystems of grammar however , namely in ellipsis and rank shift .", "As with the editing system , these are not errors but formal systems that interact with the central features of the syntax .", "True errors do of course occur involving all three logical possibilities insertion , deletion , and replacement but these are relatively rare .", "The self correction rules have access to the internal data structures of the parser , and like the parser itself , they operate deterministically .", "The parser views the editing signal as occurring at the end of a constituent , because it marks the right edge of an expunged element .", "There are two types of editing rules in the system expunction of copies , for which there are three rules , and lexically triggered restarts , for which there is one rule .", "The copying rules say that if you have two elements which are the same and they are separated by an editing signal , the first should be expunged from the structure .", "Obviously the trick here is to determine what counts as copies .", "There are three specific places where copy editing applies .", "SURFACE COPY EDITOR .", "This is essentially a nonsyntactic rule that matches the surface string on either side of the editing signal , and expunges the first copy .", "It applies to the surface string i . e . , for transcripts , the orthographic string before any syntactic procc . . 6 .", "For example , in 7 , the underlined strings are expunged before parsing begins .", "7a Well if they'd if they'd had a knife wou I wouldn't be here today .", "7b If they if they could do it .", "Typically , the Surface Copy Editor expunges a string of words that would later be analyzed as a constituent or partial constituent , and would be expunged by the Category or the Stack Editors as in 7a .", "However , the string that is expunged by the Surface Copy Editor need not be dominated by a single node ; it can be a sequence of unrelated constituents .", "For example , in 7b the parser will not analyze the first if they as an SBAR node since there is no AUX node to trigger the start of a sentence , and therefore , the words will not be expunged by either the Category or the Stack editor .", "Such cases where the Surface Copy Editor must apply are rare , and it may therefore be that there exists an optimal parser grammar that would make the Surface Copy Editor redundant ; all strings would be edited by the syntactically based Category and Stack Copy rules .", "However , it seems that the Surface Copy Editor must exist at some stage in the process of syntactic acquisition .", "The overlap between it and the other rules may be essential in learning .", "CATEGORY COPY EDITOR .", "This copy editor matches syntactic constituents in the first two positions in the parser's buffer of complete constituents .", "When the first window position ends with an editing signal and the first and second constituents in the window are of the same type , the first is expunged .", "For example , in sentence 8 the first of two determiners separated by an editing signal is expunged and the first of two verbs is similarly expunged .", "8 I was just that the kind of guy that didn't have like to have people worrying .", "STACK COPY EDITOR .", "If the first constituent in the window is preceded by an editing signal , the Stack Copy Editor looks into the stack for a constituent of the same type , and expunges any copy it finds there along with all descendants .", "In the current implementation , the Stack Copy Editor is allowed to look at successive nodes in the stack , back to the first COMP node or attention shifting boundary .", "If it finds a copy , it expunges that copy along with any nodes that are at a shallower level in the stack .", "If Fidditch were allowed to attach of incomplete constituents , the Stack Copy Editor could be implemented to delete the copy only , without searching through the stack .", "The specifics of the implementation seems not to matter for this discussion of the editing rules .", "In sentence 9 , the initial embedded sentence is expunged by the Stack Copy Editor .", "It will be useful to look a little more closely at the operation of the parser to see the editing rules at work .", "Sentence 10 includes three editing signals which trigger the copy editors .", "note also that the complement of were is ellipted .", "I will show a trace of the parser at each of these correction stages .", "The first editor that comes into play is the Surface Copy Editor , which searches for identical strings on either side of an editing signal , and expunges the first copy .", "This is done once for each sentence , before any lexical category assignments are made .", "Thus in effect , the Surface Copy Editor corresponds to a phonetic phonological matching operation , although it is in fact an orthographic procedure because we are dealing with transcriptions .", "Obviously , a full understanding of the self correction system calls for detailed phonetic phonological investigations .", "After the Surface Copy Editor has applied , the string that the lexical analyzer sees is 11 rather than 10 .", "Lexical assignments are made , and the parser proceeds to build the tree structures .", "After some processing , the configuration of the data structures is that shown in Figure 1 .", "Before determining what next rule to apply , the two editing rules come into play , the Category Editor and the Stack Editor .", "At this pulse , the Stack Editor will apply because the first constituent in the window is the same an AUX node as the current active node , and the current node ends with an edit signal .", "As a result , the first window element is popped into another dimension , leaving the the parser data structures in the state shown in Figure 2 .", "Parsing of the sentence proceeds , and eventually reaches the state shown in Figure 3 . where the Stack Editor conditions are again met .", "The current active node and the first element in the window are both NPs , and the active node ends with an edit signal .", "This causes the current node to be expunged , leaving only a single NP node , the one in the window .", "The final analysis of the sentence , after some more processing is the tree shown in Figure 4 .", "I should reemphasize that the status of the edited elements is special .", "The copy editing rules remove a constituent , no matter how large , from the view of the parser .", "The parser continues as if those words had not been said .", "Although the expunged constituents may be available for semantic interpretation , they do not form part of the main predication .", "A somewhat different sort of self correction , less sensitive to syntactic structure and flagged not only by the editing signal but also by a lexical item , is the restart .", "A restart triggers the expunction of all words from the edit signal back to the beginning of the sentence .", "It is signaled by a standard edit signal followed by a specific lexical item drawn from a set including well , ok , see , you know , like 1 said , etc .", "For example , It seems likely that , in addition to the lexical signals , specific intonational signals may also be involved in restarts .", "The editing system I have described has been applied to a corpus of over twenty hours of transcribed speech , in the process of using the parser to search for various syntactic constructions .", "Thc transcripts are of sociolinguistic interviews of the sort developed by Labov and designed to elicit unreflecting speech that approximates natural conversation . They are conversational interviews covering a range of topics , and they typically include considerable non fluency .", "Over half the sentences in one 90 minute interview contained at least one non fluency .", "The transcriptions are in standard orthography , with sentence boundaries indicated .", "The alternation of speakers' turns is indicated , but overlap is not .", "Editing signals , when noted by the transcriber , are indicated in the transcripts with a double dash .", "It is clear that this approach to transcription oniy imperfectly reflects the phonetics of editing signals ; we can't be sure to what extent the editing signals in our transcripts represent facts about production and to what extent they represent facts about perception .", "Nevertheless , except for a general tendency toward underrepresentation , there seems to be no systematic bias in our transcriptions of the editing signals , and therefore our findings are not likely to be undone by a better understanding of the phonetics of self correction .", "One major problem in analyzing the syntax of English is the multiple category membership of words .", "In general , most decisions about category membership can be made on the basis of local context .", "However , by its nature , selfcorrection disrupts the local context , and therefore the disambiguation of lexical categories becomes a more difficult problem .", "It is not clear whether the rules for category disambiguation extend across an editing signal or not .", "The results I present depend on a successful disambiguation of the syntactic categories , though the algorithm to accomplish this is not completely specified .", "Thus , to test the self correction routines I have , where necessary , imposed the proper category assignment .", "Table 1 shows the result of this editing system in the parsing of the interview transcripts from one speaker .", "All in all this shows the editing system to be quite successful in resolving non fluencies .", "Although the editing rules for Fidditch are written as deterministic pattern action rules of the same sort as the rules in the parsing grammar , their operation is in a sense isolable .", "The patterns of the self correction rules are checked first , before any of the grammar rule patterns are checked , at each step in the parse .", "Despite this independence in terms of rule ordering , the operation of the self corr , ction component is closely tied to the grammar of the parser ; for it is the parsing grammar that specifies what sort of constituents count as the same for copying .", "For example , if the grammar did not treat there as a noun phrase when it is subject of a sentence , the self correction rules could not properly resolve a sentence like because the editing rules would never recognize that people and there are the same sort of element .", "Note that 13 cannot be treated as a Restart because the lexical trigger is not present .", "Thus , the observed pattern of self correction introduces empirical constraints on the set of features that are available for syntactic rules .", "The self correction rules impose constraints not only on what linguistic elements must count as the same , but also on what must count as different .", "For example , in sentence 14 , could and be must be recognized as different sorts of elements in the grammar for the AUX node to be correctly resolved .", "If the grammar assigned the two words exactly the same part of speech , then the Category C ; . 7y Editor would necessarily apply , incorrectly expunging could .", "It appears therefore that the pattern of self corrections that occur represents a potentially rich source of evidence about the nature of syntactic categories .", "Learnability .", "If the patterns of self correction count as evidence about the nature of syntactic categories for the linguist , then this data must be equally available to the language learner .", "This would suggest that , far from being an impediment to language learning , non fluencies may in fact facilitate language acquisition by highlighting equivalent classes .", "This raises the general question of how children can acquire a language in the face of unrestrained non fluency .", "How can a language learner sort out the grammatical from the ungrammatical strings ?", "The non fluencies of speech are of course but one aspect of the degeneracy of input that makes language acquisition a puzzle .", "The self correction system I have described suggests that many non fluent strings can be resolved with little detailed linguistic knowledge .", "As Table 1 shows , about a quarter of the editing signals result in expunction of only non linguistic material .", "This requires only an ability to distinguish linguistic from nonlinguistic stuff , and it introduces the idea that edit signals signal an expunction site .", "Almost a third are resolved by the Surface Copying rule , which can be viewed simply as an instance of the general non linguistic rule that multiple instances of the same thing count as a single instance .", "The category copying rules are generalizations of simple copying , applied to a knowledge of linguistic categories .", "Making the transition from surface copies to category copies is aided by the fact that there is considerable overlap in coverage , defining a path of expanding generalization .", "Thus at the earliest stages of learning , only the simplest , non linguistic self correction rules would come into play , and gradually the more syntactically integrated would be acquired .", "Contrast this self correction system to an approach that handles non fluencies by some general problem solving routines , for example Granger 1982 , who proposes reasoning from what a speaker might be expected to say .", "Besides the obvious inefficiencies of general problem solving approaches , it is worth giving special emphasis to the problem with learnability .", "A general problem solving approach depends crucially on evaluating the likelihood of possible deviations from the norms .", "But a language learner has by definition only partial and possibly incorrect knowledge of the syntax , and is therefore unable to consistently identify deviations from the grammatical system .", "With the editing system I describe , the learner need not have the ability to recognize deviations from grammatical norms , but merely the non linguistic ability to recognize copies of the same thing .", "Generation .", "Thus far , I have considered the selfcorrection component from the standpoint of parsing .", "However , it is clear that the origins are in the process of generation .", "The mechanism for editing self corrections that I have proposed has as its essential operation expunging one of two identical elements .", "It is unable to expunge a sequence of two elements .", "The Surface Copy Editor might be viewed as a counterexample to this claim , but see below .", "Consider expunction now from the standpoint of the generator .", "Suppose self correction bears a one to one relationship to a possible action of the generator initiated by some monitoring component which could be called ABANDON CONSTRUCT X .", "And suppose that this action can be initiated at any time up until CONSTRUCT X is completed , when a signal is returned that the construction is complete .", "Further suppose that ABANDON CONSTRUCT X causes an editing signal .", "When the speaker decides in the middle of some linguistic element to abandon it and start again , an editing signal is produced .", "If this is an appropriate model , then the elements which are self corrected should be exactly those elements that xist at some stage in the generation process .", "Thus , we should be able to find evidence for the units involved in generation by looking at the data of self correction .", "And indeed , such evidence should be available to the language learner as well .", "I have described the nature of self corrected speech which is a major source of spoken non fluencies and how it can be resolved by simple editing rules within the context of a deterministic parser .", "Two features are essential to the self correction system 1 every self correction site whether it results in the expunction of words or not is marked by a phonetically identifiable signal placed at the right edge of the potential expunction site ; and 2 the expunged part is the left hand member of a pair of copies , one on each side of the editing signal .", "The copies may be of three types 1 identical surface strings , which are edited by a matching rule that applies before syntactic analysis begins ; 2 complete constituents , when two constituents of the same type appear in the parser's buffer ; or 3 incomplete constituents , when the parser finds itself trying to complete a constituent of the same type as a constituent it has just completed .", "Whenever two such copies appear in such a configuration , and the first one ends with an editing signal , the first is expunged from further analysis .", "This editing system has been implemented as part of a deterministic parser , and tested on a wide range of sentences from transcribed speech .", "Further study of the self correction system promises to provide insights into the units of production and the nature of linguistic categories .", "My thanks to Tony Kroch , Mitch Marcus , and Ken Church for helpful comments on this work ."], "summary_lines": ["Deterministic Parsing Of Syntactic Non-Fluencies\n", "It is often remarked that natural language, used naturally, is unnaturally ungrammatical.\n", "Spontaneous speech contains all manner of false starts, hesitations, and self-corrections that disrupt the well-formedness of strings.\n", "It is a mystery then, that despite this apparent wide deviation from grammatical norms, people have little difficulty understanding the non-fluent speech that is the essential medium of everyday life.\n", "And it is a still greater mystery that children can succeed in acquiring the grammar of a language on the basis of evidence provided by a mixed set of apparently grammatical and ungrammatical strings.\n", "We address the problem of correcting self repairs by adding rules to a deterministic parser that would remove the necessary text.\n", "We define a typology of repairs and associated correction strategies in terms of extensions to a deterministic parser.\n"]}
{"article_lines": ["An IR Approach for Translating New Words from Nonparallel Comparable Texts", "In recent years , there is a phenomenal growth in the amount of online text material available from the greatest information repository known as the World Wide Web .", "Various traditional information retrieval IR techniques combined with natural language processing NLP techniques have been re targeted to enable efficient access of the WWW search engines , indexing , relevance feedback , query term and keyword weighting , document analysis , document classification , etc .", "Most of these techniques aim at efficient online search for information already on the Web .", "Meanwhile , the corpus linguistic community regards the WWW as a vast potential of corpus resources .", "It is now possible to download a large amount of texts with automatic tools when one needs to compute , for example , a list of synonyms ; or download domain specific monolingual texts by specifying a keyword to the search engine , and then use this text to extract domain specific terms .", "It remains to be seen how we can also make use of the multilingual texts as NLP resources .", "In the years since the appearance of the first papers on using statistical models for bilingual lexicon compilation and machine translation Brown et al . , 1993 ; Brown et al . , 1991 ; Gale and Church , 1993 ; Church , 1993 ; Simard et al . , 1992 , large amount of human effort and time has been invested in collecting parallel corpora of translated texts .", "Our goal is to alleviate this effort and enlarge the scope of corpus resources by looking into monolingual , comparable texts .", "This type of texts are known as nonparallel corpora .", "Such nonparallel , monolingual texts should be much more prevalent than parallel texts .", "However , previous attempts at using nonparallel corpora for terminology translation were constrained by the inadequate availability of same domain , comparable texts in electronic form .", "The type of nonparallel texts obtained from the LDC or university libraries were often restricted , and were usually out of date as soon as they became available .", "For new word translation , the timeliness of corpus resources is a prerequisite , so is the continuous and automatic availability of nonparallel , comparable texts in electronic form .", "Data collection effort should not inhibit the actual translation effort .", "Fortunately , nowadays the World Wide Web provides us with a daily increase of fresh , up to date multilingual material , together with the archived versions , all easily downloadable by software tools running in the background .", "It is possible to specify the URL of the online site of a newspaper , and the start and end dates , and automatically download all the daily newspaper materials between those dates .", "In this paper , we describe a new method which combines IR and NLP techniques to extract new word translation from automatically downloaded English Chinese nonparallel newspaper texts .", "To improve the performance of a machine translation system , it is often necessary to update its bilingual lexicon , either by human lexicographers or statistical methods using large corpora .", "Up until recently , statistical bilingual lexicon compilation relies largely on parallel corpora .", "This is an undesirable constraint at times .", "In using a broad coverage English Chinese MT system to translate some text recently , we discovered that it is unable to translate Mel ! liougan which occurs very frequently in the text .", "Other words which the system cannot find in its 20 , 000 entry lexicon include proper names such as the Taiwanese president Lee Teng Hui , and the Hong Kong Chief Executive Tung CheeHwa .", "To our disappointment , we cannot locate any parallel texts which include such words since they only start to appear frequently in recent months .", "A quick search on the Web turned up archives of multiple local newspapers in English and Chinese .", "Our challenge is to find the translation of 1 . , 133 I liougan and other words from this online nonparallel , comparable corpus of newspaper materials .", "We choose to use issues of the English newspaper Hong Kong Standard and the Chinese newspaper Mingpao , from Dec . 12 , 97 to Dec . 31 , 97 , as our corpus .", "The English text contains about 3 Mb of text whereas the Chinese text contains 8 . 8 Mb of 2 byte character texts .", "So both texts are comparable in size .", "Since they are both local mainstream newspapers , it is reasonable to assume that their contents are comparable as well .", "Unlike in parallel texts , the position of a word in a text does not give us information about its translation in the other language .", "Rapp , 1995 ; Fung and McKeown , 1997 suggest that a content word is closely associated with some words in its context .", "As a tutorial example , we postulate that the words which appear in the context ofMet , Iliougan should be similar to the words appearing in the context of its English translation , flu .", "We can form a vector space model of a word in terms of its context word indices , similar to the vector space model of a text in terms of its constituent word indices Salton and Buckley , 1988 ; Salton and Yang , 1973 ; Croft , 1984 ; Turtle and Croft , 1992 ; Bookstein , 1983 ; Korfhage , 1995 ; Jones , 1979 .", "The value of the i th dimension of a word vector W is f if the i th word in the lexicon appears f times in the same sentences as W . Left columns in Table 1 and Table 2 show the list of content words which appear most frequently in the context of flu and Africa respectively .", "The right column shows those which occur most frequently in the context of bv , g .", "We can see that the context of At is more similar to that of flu than to that of Africa .", "So the first clue to the similarity between a word and its translation number of common words in their contexts .", "In a bilingual corpus , the quot ; common word quot ; is actually a bilingual word pair .", "We use the lexicon of the MT system to quot ; bridge quot ; all bilingual word pairs in the corpora .", "These word pairs are used as seed words .", "We found that the contexts of flu and 'MI Iliougan share 233 quot ; common quot ; context words , whereas the contexts of Africa and W Iff Iliougan share only 121 common words , even though the context of flu has 491 unique words and the context of Africa has 328 words .", "In the vector space model , W flu and W liougan has 233 overlapping dimensions , whereas there are 121 overlapping dimensions between W flu and W Africa .", "The flu example illustrates that the actual ranking of the context word frequencies provides a second clue to the similarity between a bilingual word pair .", "For example , virus ranks very high for both flu and ME Iliougan and is a strong quot ; bridge quot ; between this bilingual word pair .", "This leads us to use the term frequency TF measure .", "The TF of a context word is defined as the frequency of the word in the context of W . e . g .", "TF of virus in flu is 26 , in MM . is 147 .", "However , the TF of a word is not independent of its general usage frequency .", "In an extreme case , the function word the appears most frequently in English texts and would have the highest TF in the context of any W . In our HKStandard Mingpao corpus , Hong Kong is the most frequent content word which appears everywhere .", "So in the flu example , we would like to reduce the significance of Hong Kong's TF while keeping that of virus .", "A common way to account for this difference is by using the inverse document frequency IDF .", "Among the variants of IDF , we choose the following representation from Jones , 1979", "Next , a ranking algorithm is needed to match the unknown word vectors to their counterparts in the other language .", "A ranking algorithm selects the best target language candidate for a source language word according to direct comparison of some similarity measures Frakes and Baeza Yates , 1992 .", "We modify the similarity measure proposed by Salton and Buckley , 1988 into the following SO where maxn ni the maximum frequency of any word in the corpus the total number of occurrences of word i in the corpus SO We , We where wic wie wieX wie VELiwic2 x ELiwie2 TFic TFie The IDF of virus is 1 . 81 and that of Hong Kong is 1 . 23 in the English text .", "The IDF of WS is 1 . 92 and that of Hong Kong is 0 . 83 in Chinese .", "So in both cases , virus is a stronger quot ; bridge quot ; for biglliougan than Hong Kong .", "Hence , for every context seed word i , we assign a word weighting factor Salton and Buckley , 1988 w TFiw x IDFi where TFiw is the TF of word i in the context of word W . The updated vector space model of word W has wi in its i th dimension .", "The ranking of the 20 words in the contexts of W , , , f3 Iliougan is rearranged by this weighting factor as shown in Table3 .", "Variants of similarity measures such as the above have been used extensively in the IR community Frakes and Baeza Yates , 1992 .", "They are mostly based on the Cosine Measure of two vectors .", "For different tasks , the weighting factor might vary .", "For example , if we add the IDF into the weighting factor , we get the following measure Si where wic TFic x IDF , wie TFie x IDFi In addition , the Dice and Jaccard coefficients are also suitable similarity measures for document comparison Frakes and Baeza Yates , 1992 .", "We also implement the Dice coefficient into similarity measure S2 where wic TFie x IDFi Si is often used in comparing a short query with a document text , whereas S2 is used in comparing two document texts .", "Reasoning that our objective falls somewhere in between we are comparing segments of a document , we also multiply the above two measures into a third similarity measure S3 .", "In using bilingual seed words such asN , Et virus as quot ; bridges quot ; for terminology translation , the quality of the bilingual seed lexicon naturally affects the system output .", "In the case of European language pairs such as French English , we can envision using words sharing common cognates as these quot ; bridges quot ; .", "Most importantly , we can assume that the word boundaries are similar in French and English .", "However , the situation is messier with English and Chinese .", "First , segmentation of the Chinese text into words already introduces some ambiguity of the seed word identities .", "Secondly , English Chinese translations are complicated by the fact that the two languages share very little stemming properties , or part of speech set , or word order .", "This property causes every English word to have many Chinese translations and vice versa .", "In a source target language translation scenario , the translated text can be quot ; rearranged quot ; and cleaned up by a monolingual language model in the target language .", "However , the lexicon is not very reliable in establishing quot ; bridges quot ; between nonparallel English Chinese texts .", "To compensate for this ambiguity in the seed lexicon , we introduce a confidence weighting to each bilingual word pair used as seed words .", "If a word ie is the k th candidate for word ic , then wite wite ki .", "The similarity scores then become S4 and S5 and S6 S4 x S5 We also experiment with other combinations of the similarity scores such as S7 SO x S5 .", "All similarity measures S3 S7 are used in the experiment for finding a translation for ME", "In order to apply the above algorithm to find the translation for blgt , I liougan from the HKStandard Mingpao corpus , we first use a script to select the 118 English content words which are not in the lexicon as possible candidates .", "Using similarity measures S3 S7 , the highest ranking candidates of MS are shown in Table 6 .", "S6 and S7 appear to be the best similarity measures .", "We then test the algorithm with S7 on more Chinese words which are not found in the lexicon but which occur frequently enough in the Mingpao texts .", "A statistical new word extraction tool can be used to find these words .", "The unknown Chinese words and their English counterparts , as well as the occurrence frequencies of these words in HKStandard Mingpao are shown in Table 4 .", "Frequency numbers with a indicates that this word does not occur frequent enough to be found .", "Chinese words with a indicates that it is a word with segmentation and translation ambiguities .", "For example , 44 Lam could be a family name , or part of another word meaning forest .", "When it is used as a family name , it could be transliterated into Lam in Cantonese or Lin in Mandarin .", "Disregarding all entries with a in the above table , we apply the algorithm to the rest of the Chinese unknown words and the 118 English unknown words from HKStandard .", "The output is ranked by the similarity scores .", "The highest ranking translated pairs are shown in Table 5 .", "The only Chinese unknown words which are not correctly translated in the above list are ik6 fl Lunar and MR Yeltsin 1 .", "Tung CheeHwa is a pair of collocates which is actually the full name of the Chief Executive .", "Poultry in Chinese is closely related to flu because the Chinese name for bird flu is poultry flu .", "In fact , almost all unambiguous Chinese new words find their translations in the first 100 of the ranked list .", "Six of the Chinese words have correct translation as their first candidate .", "Using vector space model and similarity measures for ranking is a common approach in IR for query text and text text comparisons Salton and Buckley , 1988 ; Salton and Yang , 1973 ; Croft , 1984 ; Turtle and Croft , 1992 ; Bookstein , 1983 ; Korfhage , 1995 ; Jones , 1979 .", "This approach has also been used by Dagan and Itai , 1994 ; Gale et al . , 1992 ; Shfitze , 1992 ; Gale et al . , 1993 ; Yarowsky , 1995 ; Gale and Church , 1994 for sense disambiguation between multiple usages of the same word .", "Some of the early statistical terminology translation methods are Brown et al . , 1993 ; Wu and Xia , 1994 ; Dagan and Church , 1994 ; Gale and Church , 1991 ; Kupiec , 1993 ; Smadja et al . , 1996 ; Kay and Roscheisen , 1993 ; Fung and Church , 1994 ; Fung , 1995b .", "These algorithms all require parallel , translated texts as input .", "Attempts at exploring nonparallel corpora for terminology translation are very few Rapp , 1995 ; Fung , 1995a ; Fung and McKeown , 1997 .", "Among these , Rapp , 1995 proposes that the association between a word and its close collocate is preserved in any language . and Fung and McKeown , 1997 suggests that the associations between a word and many seed words are also preserved in another language .", "In this paper , we have demonstrated that the associations between a word and its context seed words are well preserved in nonparallel , comparable texts of different languages .", "Our algorithm is the first to have generated a collocation bilingual lexicon , albeit small , from a nonparallel , comparable corpus .", "We have shown that the algorithm has good precision , but the recall is low due to the difficulty in extracting unambiguous Chinese and English words .", "Better results can be obtained when the following changes are made We will test the precision and recall of the algorithm on a larger set of unknown words .", "We have devised an algorithm using context seed word TF IDF for extracting bilingual lexicon from nonparallel , comparable corpus in English Chinese .", "This algorithm takes into account the reliability of bilingual seed words and is language independent .", "This algorithm can be applied to other language pairs such as English French or English German .", "In these cases , since the languages are more similar linguistically and the seed word lexicon is more reliable , the algorithm should yield better results .", "This algorithm can also be applied in an iterative fashion where high ranking bilingual word pairs can be added to the seed word list , which in turn can yield more new bilingual word pairs ."], "summary_lines": ["An IR Approach for Translating New Words from Nonparallel Comparable Texts\n", "We demonstrate that the associations between a word and its context seed words are preserved in comparable texts of different languages.\n", "We propose to represent the contexts of a word or phrase with a real-valued vector, which one element corresponds to one word in the contexts.\n"]}
{"article_lines": ["The SemEval 2007 WePS Evaluation Establishing a benchmark for the Web People Search Task", "This paper presents the task definition , resources , participation , and comparative re sults for the Web People Search task , which was organized as part of the SemEval 2007 evaluation exercise .", "This task consists of clustering a set of documents that mention an ambiguous person name according to the actual entities referred to using that name .", "Finding information about people in the World Wide Web is one of the most common activities of Internetusers .", "Person names , however , are highly ambigu ous .", "In most cases , the results for a person name search are a mix of pages about different peoplesharing the same name .", "The user is then forced ei ther to add terms to the query probably losing recall and focusing on one single aspect of the person , orto browse every document in order to filter the infor mation about the person he is actually looking for .", "In an ideal system the user would simply type aperson name , and receive search results clustered ac cording to the different people sharing that name .", "And this is , in essence , the WePS Web People Search task we have proposed to SemEval 2007 participants systems receive a set of web pages which are the result of a web search for a per son name , and they have to cluster them in as many sets as entities sharing the name .", "This task has close links with Word Sense Disambiguation WSD , which is generally formulated as the taskof deciding which sense a word has in a given context .", "In both cases , the problem addressed is the resolution of the ambiguity in a natural language expression .", "A couple of differences make our problem different .", "WSD is usually focused on open class words common nouns , adjectives , verbs andadverbs .", "The first difference is that boundaries be tween word senses in a dictionary are often subtle or even conflicting , making binary decisions harderand sometimes even useless depending on the ap plication .", "In contrast , distinctions between people should be easier to establish .", "The second differenceis that WSD usually operates with a dictionary con taining a relatively small number of senses that can be assigned to each word .", "Our task is rather a case of Word Sense Discrimination , because the number of ? senses ?", "actual people is unknown a priori , and it is in average much higher than in the WSD task there are 90 , 000 different names shared by 100 mil lion people according to the U . S . Census Bureau .", "There is also a strong relation of our proposedtask with the Co reference Resolution problem , fo cused on linking mentions including pronouns ina text .", "Our task can be seen as a co reference resolution problem where the focus is on solving inter document co reference , disregarding the linking of all the mentions of an entity inside each document .", "An early work in name disambiguation Baggaand Baldwin , 1998 uses the similarity between doc uments in a Vector Space using a ? bag of words ?", "representation .", "An alternative approach by Mann and Yarowsky 2003 is based on a rich feature space of automatically extracted biographic information .", "Fleischman and Hovy 2004 propose a Maximum Entropy model trained to give the probability that 64 two names refer to the same individual 1 . The paper is organized as follows .", "Section 2 provides a description of the experimental methodology , the training and test data provided to the par ticipants , the evaluation measures , baseline systemsand the campaign design .", "Section 3 gives a description of the participant systems and provides the evaluation results .", "Finally , Section 4 presents some con clusions .", "2 . 1 Data .", "Following the general SemEval guidelines , we have prepared trial , training and test data sets for the task , which are described below .", "2 . 1 . 1 Trial dataFor this evaluation campaign we initially deliv ered a trial corpus for the potential participants .", "The trial data consisted of an adapted version of the WePS corpus described in Artiles et al , 2006 .", "The predominant feature of this corpus is a high number of entities in each document set , due to the fact that the ambiguous names were extracted from the most common names in the US Census .", "This corpus did not completely match task specifications because it did not consider documents with internal ambiguity , nor it did consider non person entities ; but it was , however , a cost effective way of releasing data toplay around with .", "During the first weeks after releasing this trial data to potential participants , some annotation mistakes were noticed .", "We preferred , how ever , to leave the corpus ? as is ? and concentrate our efforts in producing clean training and test datasets , rather than investing time in improving trial data .", "2 . 1 . 2 Training data In order to provide different ambiguity scenarios , we selected person names from different sources US Census .", "We reused the Web03 corpus Mann , 2006 , which contains 32 names randomly picked from the US Census , and was well suited for the task .", "Wikipedia .", "Another seven names were sampledfrom a list of ambiguous person names in the En glish Wikipedia .", "These were expected to have a1For a comprehensive bibliography on person name disam biguation refer to http nlp . uned . es weps few predominant entities popular or historical , and therefore a lower ambiguity than the previous set . ECDL .", "Finally , ten additional names were ran domly selected from the Program Committee listing of a Computer Science conference ECDL 2006 . This set offers a scenario of potentially low am biguity computer science scholars usually have a stronger Internet presence than other professionalfields with the added value of the a priori knowl edge of a domain specific type of entity scholar present in the data .", "All datasets consist of collections of web pages obtained from the 100 top results for a person name query to an Internet search engine 2 .", "Note that 100 is an upper bound , because in some occasions the URL returned by the search engine no longer exists . The second and third datasets developed explic itly for our task consist of 17 person names and 1685 associated documents in total 99 documentsper name in average .", "Each web page was down loaded and stored for off line processing .", "We also stored the basic metadata associated to each search result , including the original URL , title , position in the results ranking and the corresponding snippet generated by the search engine . In the process of generating the corpus , the selection of the names plays an important role , poten tially conditioning the degree of ambiguity that will be found later in the Web search results .", "The reasonsfor this variability in the ambiguity of names are diverse and do not always correlate with the straight forward census frequency .", "A much more decisivefeature is , for instance , the presence of famous en tities sharing the ambiguous name with less popular people .", "As we are considering top search results , these can easily be monopolized by a single entity that is popular in the Internet .", "After the annotation of this data see section 2 . 1 . 4 . we found our predictions about the averageambiguity of each dataset not to be completely ac curate .", "In Table 1 we see that the ECDL 06 average ambiguity is indeed relatively low except for the documents for ? Thomas Baker ?", "standing as the most ambiguous name in the whole training .", "Wikipedia names have an average ambiguity of 23 , 14 entities2We used the Yahoo !", "API from Yahoo !", "Search Web Ser vices http developer . yahoo . com search web .", "65 Name entities documents discarded Wikipedia names John Kennedy 27 99 6 George Clinton 27 99 6 Michael Howard 32 99 8 Paul Collins 37 98 6 Tony Abbott 7 98 9 Alexander Macomb 21 100 14 David Lodge 11 100 9 Average 23 , 14 99 , 00 8 , 29 ECDL 06 Names Edward Fox 16 100 36 Allan Hanbury 2 100 32 Donna Harman 7 98 6 Andrew Powell 19 98 48 Gregory Crane 4 99 17 Jane Hunter 15 99 59 Paul Clough 14 100 35 Thomas Baker 60 100 31 Christine Borgman 7 99 11 Anita Coleman 9 99 28 Average 15 , 30 99 , 20 30 , 30 WEB03 Corpus Tim Whisler 10 33 8 Roy Tamashiro 5 23 6 Cynthia Voigt 1 405 314 Miranda Bollinger 2 2 0 Guy Dunbar 4 51 34 Todd Platts 2 239 144 Stacey Doughty 1 2 0 Young Dawkins 4 61 35 Luke Choi 13 20 6 Gregory Brennan 32 96 38 Ione Westover 1 4 0 Patrick Karlsson 10 24 8 Celeste Paquette 2 17 2 Elmo Hardy 3 55 15 Louis Sidoti 2 6 3 Alexander Markham 9 32 16 Helen Cawthorne 3 46 13 Dan Rhone 2 4 2 Maile Doyle 1 13 1 Alice Gilbreath 8 74 30 Sidney Shorter 3 4 0 Alfred Schroeder 35 112 58 Cathie Ely 1 2 0 Martin Nagel 14 55 31 Abby Watkins 13 124 35 Mary Lemanski 2 152 78 Gillian Symons 3 30 6 Pam Tetu 1 4 2 Guy Crider 2 2 0 Armando Valencia 16 79 20 Hannah Bassham 2 3 0 Charlotte Bergeron 5 21 8 Average 5 , 90 47 , 20 18 , 00 Global average 10 , 76 71 , 02 26 , 00 Table 1 Training Data per name , which is higher than for the ECDL set .", "The WEB03 Corpus has the lowest ambiguity 5 , 9 entities per name , for two reasons first , randomly picked names belong predominantly to the long tail of unfrequent person names which , per se , have low ambiguity .", "Being rare names implies that in averagethere are fewer documents returned by the search engine 47 , 20 per name , which also reduces the pos sibilities to find ambiguity .", "2 . 1 . 3 Test data For the test data we followed the same process described for the training .", "In the name selection wetried to maintain a similar distribution of ambiguity degrees and scenario .", "For that reason we ran domly extracted 10 person names from the English Wikipedia and another 10 names from participantsin the ACL 06 conference .", "In the case of the US census names , we decided to focus on relatively com mon names , to avoid the problems explained above .", "Unfortunately , after the annotation was finished once the submission deadline had expired , wefound a major increase in the ambiguity degrees Ta ble 2 of all data sets .", "While we expected a raise in the case of the US census names , the other two cases just show that there is a high and unpredictable variability , which would require much larger data sets to have reliable population samples .", "This has made the task particularly challenging for participants , because naive learning strategies such as empirical adjustment of distance thresholds to optimize standard clustering algorithms might be misleaded by the training set .", "2 . 1 . 4 AnnotationThe annotation of the data was performed separately in each set of documents related to an ambiguous name .", "Given this set of approximately 100 documents that mention the ambiguous name , the an notation consisted in the manual clustering of eachdocument according to the actual entity that is re ferred on it . When non person entities were found for in stance , organization or places named after a person the annotation was performed without any special rule .", "Generally , the annotator browses documents following the original ranking in the search results ; after reading a document he will decide whether thementions of the ambiguous name refer to a new en tity or to a entity previously identified .", "We asked the annotators to concentrate first on mentions that strictly contained the search string , and then to pay attention to the co referent variations of the name .", "For instance ? John Edward Fox ?", "or ? Edward Fox Smith ?", "would be valid mentions .", "? Edward J . Fox ? , however , breaks the original search string , and we do not get into name variation detection , so it will be considered valid only if it is co referent to a valid 66 Name entities documents discarded Wikipedia names Arthur Morgan 19 100 52 James Morehead 48 100 11 James Davidson 59 98 16 Patrick Killen 25 96 4 William Dickson 91 100 8 George Foster 42 99 11 James Hamilton 81 100 15 John Nelson 55 100 25 Thomas Fraser 73 100 13 Thomas Kirk 72 100 20 Average 56 , 50 99 , 30 17 , 50 ACL06 Names Dekang Lin 1 99 0 Chris Brockett 19 98 5 James Curran 63 99 9 Mark Johnson 70 99 7 Jerry Hobbs 15 99 7 Frank Keller 28 100 20 Leon Barrett 33 98 9 Robert Moore 38 98 28 Sharon Goldwater 2 97 4 Stephen Clark 41 97 39 Average 31 , 00 98 , 40 12 , 80 US Census Names Alvin Cooper 43 99 9 Harry Hughes 39 98 9 Jonathan Brooks 83 97 8 Jude Brown 32 100 39 Karen Peterson 64 100 16 Marcy Jackson 51 100 5 Martha Edwards 82 100 9 Neil Clark 21 99 7 Stephan Johnson 36 100 20 Violet Howard 52 98 27 Average 50 , 30 99 , 10 14 , 90 Global average 45 , 93 98 , 93 15 , 07 Table 2 Test Data mention .", "In order to perform the clustering , the annotatorwas asked to pay attention to objective facts bi ographical dates , related names , occupations , etc . and to be conservative when making decisions .", "Thefinal result is a complete clustering of the docu ments , where each cluster contains the documentsthat refer to a particular entity .", "Following the pre vious example , in documents for the name ? Edward Fox ?", "the annotator found 16 different entities with that name .", "Note that there is no a priori knowledge about the number of entities that will be discovered in a document set .", "This makes the task specially difficult when there are many different entities and a high volume of scattered biographical information to take into account .", "In cases where the document does not offer enough information to decide whether it belongs to a cluster or is a new entity , it is discarded from the evaluation process not from the dataset .", "Another common reason for discarding documents was theabsence of the person name in the document , usu ally due to a mismatch between the search engine cache and the downloaded URL .", "We found that , in many cases , different entities were mentioned using the ambiguous name within asingle document .", "This was the case when a doc ument mentions relatives with names that contain the ambiguous string for instance ? Edward Fox ?", "and ? Edward Fox Jr . ? .", "Another common case ofintra document ambiguity is that of pages contain ing database search results , such as book lists from Amazon , actors from IMDB , etc . A similar case is that of pages that explicitly analyze the ambiguity of a person name Wikipedia ? disambiguation ?", "pages .", "The way this situation was handled , in terms of the annotation , was to assign each document to as many clusters as entities were referred to on it with the ambiguous name .", "2 . 2 Evaluation measures .", "Evaluation was performed in each document set web pages mentioning an ambiguous person name of the data distributed as test .", "The human annotation was used as the gold standard for the evaluation . Each system was evaluated using the standard pu rity and inverse purity clustering measures Purity isrelated to the precision measure , well known in In formation Retrieval .", "This measure focuses on the frequency of the most common category in eachcluster , and rewards the clustering solutions that in troduce less noise in each cluster .", "Being C the set of clusters to be evaluated , L the set of categories manually annotated and n the number of clustered elements , purity is computed by taking the weighted average of maximal precision values Purity ? i Ci n max Precision Ci , Lj where the precision of a cluster Ci for a given cat egory Lj is defined as Precision Ci , Lj Ci ? Lj Ci Inverse Purity focuses on the cluster with maximum recall for each category , rewarding the clus tering solutions that gathers more elements of each category in a corresponding single cluster .", "Inverse Purity is defined as 67 Inverse Purity ? i Li n max Precision Li , Cj For the final ranking of systems we used the har monic mean of purity and inverse purity F ? 0 , 5 . The F measure is defined as follows F 1 ? 1Purity 1 ?", "? 1 Inverse PurityF ? 0 , 2 is included as an additional measure giv ing more importance to the inverse purity aspect .", "The rationale is that , for a search engine user , it should be easier to discard a few incorrect web pages in a cluster containing all the informationneeded , than having to collect the relevant infor mation across many different clusters .", "Therefore , achieving a high inverse purity should be rewarded more than having high purity .", "2 . 3 Baselines .", "Two simple baseline approaches were applied to the test data .", "The ALL IN ONE baseline provides aclustering solution where all the documents are assigned to a single cluster .", "This has the effect of always achieving the highest score in the inverse purity measure , because all classes have their docu ments in a single cluster .", "On the other hand , the purity measure will be equal to the precision of thepredominant class in that single cluster .", "The ONE IN ONE baseline gives another extreme clusteringsolution , where every document is assigned to a dif ferent cluster .", "In this case purity always gives its maximum value , while inverse purity will decrease with larger classes .", "2 . 4 Campaign design .", "The schedule for the evaluation campaign was set by the SemEval organisation as follows i release task description and trial data set ; ii release of training and test ; iii participants send their answers to the task organizers ; iv the task organizers evaluate the answers and send the results .", "The task description and the initial trial data set were publicly released before the start of the official evaluation . The official evaluation period started with the simultaneous release of both training and test data , to gether with a scoring script with the main evaluation measures to be used .", "This period spanned five weeksin which teams were allowed to register and down load the data .", "During that period , results for a giventask had to be submitted no later than 21 days af ter downloading the training data and no later than 7days after downloading the test data .", "Only one sub mission per team was allowed .", "Training data included the downloaded webpages , their associated metadata and the human clustering of each document set , providing a develop ment test bed for the participant ? s systems .", "We also specified the source of each ambiguous name in the training data Wikipedia , ECDL conference and US Census .", "Test data only included the downloaded web pages and their metadata .", "This section of thecorpus was used for the systems evaluation .", "Partici pants were required to send a clustering for each test document set .", "Finally , after the evaluation period was finishedand all the participants sent their data , the task orga nizers sent the evaluation for the test data .", "29 teams expressed their interest in the task ; thisnumber exceeded our expectations for this pilot ex perience , and confirms the potential interest of theresearch community in this highly practical prob lem .", "Out of them , 16 teams submitted results within the deadline ; their results are reported below .", "3 . 1 Results and discussion .", "Table 3 presents the macro averaged results ob tained by the sixteen systems plus the two baselineson the test data .", "We found macro average 3 preferable to micro average 4 because it has a clear inter pretation if the evaluation measure is F , then we should calculate F for every test case person name and then average over all trials .", "The interpretation of micro average F is less clear .", "The systems are ranked according to the scores obtained with the harmonic mean measure F ? 0 , 5 of 3Macro average F consists of computing F for every test set person name and then averaging over all test sets .", "4Micro average F consists of computing the average P and IP over all test sets and then calculating F with these figures .", "68 Macro averaged Scores F measures rank team id ? , 5 ? , 2 Pur Inv Pur 1 CU COMSEM , 78 , 83 , 72 , 88 .", "2 IRST BP , 75 , 77 , 75 , 80 .", "3 PSNUS , 75 , 78 , 73 , 82 .", "5 SHEF , 66 , 73 , 60 , 82 .", "6 FICO , 64 , 76 , 53 , 90 .", "7 UNN , 62 , 67 , 60 , 73 .", "8 ONE IN ONE , 61 , 52 1 , 00 , 47 .", "9 AUG , 60 , 73 , 50 , 88 .", "10 SWAT IV , 58 , 64 , 55 , 71 .", "11 UA ZSA , 58 , 60 , 58 , 64 .", "12 TITPI , 57 , 71 , 45 , 89 .", "13 JHU1 13 , 53 , 65 , 45 , 82 .", "14 DFKI2 , 50 , 63 , 39 , 83 .", "15 WIT , 49 , 66 , 36 , 93 .", "16 UC3M 13 , 48 , 66 , 35 , 95 .", "17 UBC AS , 40 , 55 , 30 , 91 .", "18 ALL IN ONE , 40 , 58 , 29 1 , 00 .", "Table 3 Team rankingpurity and inverse purity .", "Considering only the par ticipant systems , the average value for the ranking measure was 0 , 60 and its standard deviation 0 , 11 .", "Results with F ? 0 , 2 are not substantially different except for the two baselines , which roughly swappositions .", "There are some ranking swaps , but gen erally only within close pairs . The good performance of the ONE IN ONE baseline system is indicative of the abundance of singleton entities entities represented by only one doc ument .", "This situation increases the inverse purity score for this system giving a harmonic measure higher than the expected .", "The WEPS task ended with considerable success in terms of participation , and we believe that a careful analysis of the contributions made by participants which is not possible at the time of writing this report will be an interesting reference for future re search .", "In addition , all the collected and annotated dataset will be publicly available 5 as a benchmark for Web People Search systems . At the same time , it is clear that building a re liable test bed for the task is not simple .", "First ofall , the variability across test cases is large and un predictable , and a system that works well with the 5http nlp . uned . es wepsnames in our test bed may not be reliable in practi cal , open search situations .", "Partly because of that , our test bed happened to be unintentionally challenging for systems , with a large difference be tween the average ambiguity in the training and test datasets .", "Secondly , it is probably necessary to think about specific evaluation measures beyond standard clustering metrics such as purity and inverse purity , which are not tailored to the task and do not be have well when multiple classification is allowed . We hope to address these problems in a forthcom ing edition of the WEPS task .", "This research was supported in part by the National Science Foundation of United States under GrantIIS 00325657 and by a grant from the Spanish government under project Text Mess TIN2006 15265C06 .", "This paper does not necessarily reflect the po sition of the U . S . Government ."], "summary_lines": ["The SemEval-2007 WePS Evaluation: Establishing a benchmark for the Web People Search Task\n", "This paper presents the task definition, resources, participation, and comparative results for the Web People Search task, which was organized as part of the SemEval-2007 evaluation exercise.\n", "This task consists of clustering a set of documents that mention an ambiguous person name according to the actual entities referred to using that name.\n", "We consider the problem of disambiguating person names in a Web searching scenario.\n", "The goal of the Web People Search task is to assign Web pages to groups, where each group contains all (and only those) pages that refer to one unique entity.\n", "Our Web Persona Search (WePS) task has created a benchmark dataset.\n"]}
{"article_lines": ["Extraposition Grammars", "Extraposition grammars are an extension of definite clause grammars , and are similarly defined in terms of logic clauses .", "The extended formalism makes it easy to describe left extraposition of constituents , an important feature of natural language syntax .", "Edinburgh EH1 1JZ SCOTLAND Extraposition grammars are an extension of definite clause grammars , and are similarly defined in terms of logic clauses .", "The extended formalism makes it easy to describe left extraposition of constituents , an important feature of natural language syntax .", "This paper presents a grammar formalism for natural language analysis , called extraposition grammars XGs , based on the subset of predicate calculus known as definite , or Horn , clauses .", "It is argued that certain important linguistic phenomena , collectively known in transformational grammar as left extraposition , can be described better in XGs than in earlier grammar formalisms based on definite clauses .", "The XG formalism is an extension of the definite clause grammar DCG 6 formalism , which is itself a restriction of Colmerauer's formalism of metamorphosis grammars MGs 2 .", "Thus XGs and MGs may be seen as two alternative extensions of the same basic formalism , DCGs .", "The argument for XGs will start with a comparison with DCGs .", "I should point out , however , that the motivation for the development of XGs came from studying large MGs for natural language 4 , 7 .", "The relationship between MGs and DCGs is analogous to that between type 0 grammars and contextfree grammars .", "So , some of the linguistic phenomena which are seen as rewriting one sequence of constituents into another might be described better in a MG than in a DCG .", "However , it will be shown that rewritings such as the one involved in left extraposition cannot easily be described in either of the two formalisms .", "Left extraposition has been used by grammarians to describe the form of interrogative sentences and relative clauses , at least in languages such as English , French , Spanish and Portuguese .", "The importance of these constructions , even in simplified subsets of natural language , such as those used in database interfaces , suggests that a grammar formalism should be able to express them in a clear and concise manner .", "This is the purpose of XGs .", "This section summarises the concepts of definite clause grammars DCGs , and of the underlying system of logic , definite clauses , needed for the rest of the paper .", "A fuller discussion can be found elsewhere 6 .", "A definite clause has either the form to be read as quot ; P is true if Q1 , . . . , Qn are true quot ; , or the form P . to be read as quot ; P is true quot ; .", "P is the head of the clause , are goals , forming the body of the clause .", "The symbols P , Qi , Qn stand for literals .", "A literal has a predicate symbol , and possibly some arguments in parentheses , separated by commas , e . g .", "A literal is to be interpreted as denoting a relation between its arguments ; e . g .", "quot ; father X , Y quot ; denotes the relation 'father' between X and Y .", "Arguments are terms , standing for partially specified objects .", "Terms may be A compound term has a functor and some arguments , which are terms .", "Compound terms are best seen as Copyright 1981 by the Association for Computational Linguistics .", "Permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the Journal reference and this copyright notice are included on the first page .", "To copy otherwise , or to republish , requires a fee and or specific permission .", "A particular type of term , the list , has a simplified notation .", "The binary functor . ' makes up nonempty lists , and the atom ' denotes the empty list .", "In the special list notation , may be read as quot ; X is grandfather of Z if X is father of Y and Y is a parent of Z quot ; ; the clause father john , mary . may be read as quot ; John is father of Mary quot ; note the use of lower case for the constants in the clause .", "A set of definite clauses forms a program .", "A program defines the relations denoted by the predicates appearing on the head of clauses .", "When using a definite clause interpreter , such as PROLOG 9 , a goal statement ? P . specifies that the relation instances that match P are required .", "Now , any context free rule , such as sentence noun phrase , verb_phrase .", "I use , ' for concatenation , and . ' to terminate a rule may be translated into a definite clause which says quot ; there is a sentence between points SO and S in a string if there is a noun phrase between points SO and Si , and a verb phrase between points Si and S quot ; .", "A context free rule like determiner the .", "where the square brackets mark a terminal can be translated into determiner SO , S connects SO , the , S . which may be read as quot ; there is a determiner between points SO and S in a string if SO is joined to S by the word the quot ; .", "The predicate 'connects' is used to relate terms denoting points in a string to the words which join those points .", "Depending on the application , different definitions of 'connects' might be used .", "In particular , if a point in a string is represented by the list of words after that point , 'connects' has the very simple definition connects WordIS1 , Word , S . which may be read as quot ; a string point represented by a list of words with first element Word and rest S is connected by the word Word to the string point represented by list S . quot ; DCGs are the natural extension of context free grammars CFGs obtained through the translation into definite clauses outlined above .", "A DCG nonterminal may have arguments , of the same form as those of a predicate , and a terminal may be any term .", "For instance , the rule is made of a noun phrase with structure NP and number N which can be either 'singular' or 'plural' , followed by a verb phrase with structure VP agreeing with the number N quot ; .", "A DCG rule is just quot ; syntactic sugar quot ; for a definite clause .", "The clause for the example above is In general , a DCG non terminal with n arguments is translated into a predicate of n 2 arguments , the last two of which are the string points , as in the translation of context free rules into definite clauses .", "The main idea of DCGs is then that grammar symbols can be general logic terms rather than just atomic symbols .", "This makes DCGs a general purpose grammar formalism , capable of describing any type 0 language .", "The first grammar formalism with logic terms as grammar symbols was Colmerauer's metamorphosis grammars 2 .", "Where a DCG is a CFG with logic terms for grammar symbols , a MG is a somewhat restricted type 0 grammar with logic terms for grammar symbols .", "However , the very simple translation of DCGs into definite clauses presented above does not carry over directly to MGs .", "Roughly speaking , left extraposition occurs in a natural language sentence when a subconstituent of some constituent is missing , and some other constituent , to the left of the incomplete one , represents the missing constituent in some way .", "It is useful to think that an empty constituent , the trace , occupies the quot ; hole quot ; left by the missing constituent , and that the constituent to the left , which represents the missing part , is a marker , indicating that a constituent to its right contains a trace 1 .", "One can then say that the constituent in whose place the trace stands has been extraposed to the left , and , in its new position , is represented by the marker .", "For instance , relative clauses are formed by a marker , which in the simpler cases is just a relative pronoun , followed by a sentence where some noun phrase has been replaced by a trace .", "This is represented in the following annotated surface structure In this example , t stands for the trace , 'that' is the surface form of the marker , and the connection between the two is indicated by the common index i .", "The concept of left extraposition plays an essential role , directly or indirectly , in many formal descriptions of relative and interrogative clauses .", "Related to this concept , there are several quot ; global constraints quot ; , the quot ; island constraints quot ; , that have been introduced to restrict the situations in which left extraposition can be applied .", "For instance , the Ross complex NP constraint 8 , implies that any relative pronoun occurring outside a given noun phrase cannot be bound to a trace occurring inside a relative clause which is a subconstituent of the noun phrase .", "This means that it is not possible to have a configuration like Xi . . . np rel X2 s t2 tl 1 . . .", "Note that here I use the concept of left extraposition in a loose sense , without relating it to transformations as in transformational grammar .", "In XGs , and also in other formalisms for describing languages for instance the context free rule schemas of Gazdar 5 , the notion of transformation is not used , but a conceptual operation of some kind is required for instance to relate a relative pronoun to a quot ; hole quot ; in the structural representation of the constituent following the pronoun .", "To describe a fragment of language where left extraposition occurs , one might start with a CFG which gives a rough approximation of the fragment .", "The grammar may then be refined by adding arguments to non terminals , to carry extraposed constituents across phrases .", "This method is analogous to the introduction of quot ; derived quot ; rules by Gazdar 5 .", "Take for example the CFG in Figure 4 . 1 .", "In this grammar it is possible to use rule 1 to expand a noun phrase into a trace , even outside a relative clause .", "To prevent this , I will add arguments to all non terminals from Which a noun phrase might be extraposed .", "The modified grammar , now a DCG , is given in Figure 4 . 2 .", "A variable 'Hole . . . ' will have the value 'trace' if an extraposed noun phrase occurs somewhere to the right , 'nil' otherwise .", "The parse tree of Figure 4 . 3 shows the variable values when the grammar of Figure 4 . 2 is used to analyse the noun phrase quot ; the man that John met quot ; .", "Intuitively , we either can see noun phrases moving to the left , leaving traces behind , or traces appearing from markers and moving to the right .", "In a phrase quot ; noun phrase Hole 1 , Hole2 quot ; , Holel will have the value 'trace' when a trace occurs somewhere to the right of the left end of the phrase .", "In that case , Hole2 will be 'nil' if the noun phrase contains the trace , 'trace' if the trace appears to the right of the right end of this noun phrase .", "Thus , rule 2 in Figure 4 . 2 specifies that a noun phrase expands into a trace if a trace appears from the left , and as this trace is now placed , it will not be found further to the right .", "The non terminal 'relative' has no arguments , because the complex NP constraint prevents noun phrases from moving out of a relative clause .", "However , that constraint does not apply to prepositional phrases , so 'prep_phrase' has arguments .", "The non terminal 'sentence' and consequently 'verb_phrase' has a single argument , because in a relative clause the trace must occur in the sentence immediately to the right of the relative pronoun .", "It is obvious that in a more extensive grammar , many non terminals would need extraposition arguments , and the increased complication would make the grammar larger and less readable .", "Colmerauer's MG formalism allows an alternative way to express left extraposition .", "It involves the use of rules whose left hand side is a non terminal followed by a string of quot ; dummy quot ; terminal symbols which do not occur in the input vocabulary .", "An example of such a rule is rel_marker , t rel pronoun .", "Its meaning is that 'rel pronoun' can be analysed as a 'rel marker' provided that the terminal 't' is added to the front of the input remaining after the rule application .", "Subsequent rule applications will have to cope explicitly with such dummy terminals .", "This method has been used in several published grammars 2 , 4 , 7 , but in a large grammar it has the same if not worse problems of size and clarity as the previous method .", "It also suffers from a theoretical problem in general , the language defined by such a grammar will contain extra sentences involving the dummy terminals .", "For parsing , however , no problem arises , because the input sentences are not supposed to contain dummy terminals .", "These inadequacies of MGs were the main motivation for the development of XGs .", "To describe left extraposition , we need to relate non contiguous parts of a sentence .", "But neither DCGs nor MGs have means of representing such a relationship by specific grammar rules .", "Rather ; the relationship can only be described implicitly , by adding extra information to many unrelated rules in the grammar .", "That is , one cannot look at a grammar and find a set of rules specific to the constructions which involve left extraposition .", "With extraposition grammars , I attempt to provide a formalism in which such rules can be written .", "In this informal introduction to the XG formalism , I will avoid the extra complications of non terminal arguments .", "So , in the discussion that follows , we may look at XGs as an extension of CFGs .", "Sometimes it is easier to look at grammar rules in the left to right , or synthesis , direction .", "I will say then that a rule is being used to expand or rewrite a string .", "In other cases , it is easier to look at a rule in the rightto left , or analysis , direction .", "I will say then that the rule is being used to analyse a string .", "Let us first look at the following XG fragment sentence noun_phrase , verb_phrase . noun_phrase determiner , noun , relative . noun_phrase trace . relative 1 . relative rel marker , sentence . rel marker . . . trace rel pronoun .", "All rules but the last are context free .", "The last rule expresses the extraposition in simple relative clauses .", "It states that a relative pronoun is to be analysed as a marker , followed by some unknown constituents denoted by . . . ' , followed by a trace .", "This is shown in Figure 5 . 1 .", "As in the DCG example of the previous section , the extraposed noun phrase is expanded into a trace .", "However , instead of the trace being rewritten into the empty string , the trace is used as part of the analysis of rel marker' .", "The difference between XG rules and DCG rules is then that the left hand side of an XG rule may contain several symbols .", "Where a DCG rule is seen as expressing the expansion of a single non terminal into a string , an XG rule is seen as expanding together several non contiguous symbols into a string .", "More precisely , an XG rule has the general form Here each segment s , separated from other segments by . . . ' is a sequence of terminals and non terminals written in DCG notation , with , ' for concatenation .", "The first symbol in s 1 , the leading symbol , is restricted to be a non terminal .", "The right hand side r is as in a DCG rule .", "Leaving aside the constraints discussed in the next section , the meaning of a rule like 3 is that any sequence of symbols of the form sixis 2x 2 etc . sk_ ixk_isk with arbitrary xi's , can be rewritten into rx ix 2 . . . xk_ 1 .", "Thinking procedurally , one can say that a nonterminal may be expanded by matching it to the leading symbol on the left hand side of a rule , and the rest of the left hand side is quot ; put aside quot ; to wait for the derivation of symbols which match each of its symbols in sequence .", "This sequence of symbols can be interrupted by arbitrary strings , paired to the occurrences of . . . ' on the left hand side of the rule .", "When several XG rules are involved , the derivation of a surface string becomes more complicated than in the single rule example of the previous section , because rule applications interact in the way now to be described .", "To represent the intermediate stages in an XG derivation , I will use bracketed strings , made up of A bracketed string is balanced if the brackets in it balance in the usual way .", "Now , an XG rule etc .", ". . . un V . can be applied to bracketed string s if s x0u1x1u2 etc . xn_ unxn and each of the gaps x1 , . . . , xn_1 is balanced .", "The substring of s between xo and xn is the span of the rule application .", "The application rewrites s into new string t , replacing u1 by v followed by n 1 open brackets , and replacing each of u2 , un by a close bracket ; in short , s is replaced by xov . . . x1 x . . .", "The relation between the original string s and the derived string t is abbreviated as s t . In the new string t , the substring between xo and xn is the result of the application .", "In particular , the application of a rule with a single segment in its left hand side is no different from what it would be in a type 0 grammar Taking again the rule rel marker . . . trace rel pronoun . its application to rel marker John likes trace produces rel _pronoun John likes After this rule application , it is not possible to apply any rule with a segment matching inside a bracketed portion and another segment matching outside it .", "The use of the above rule has divided the string into two isolated portions , each of which must be independently expanded .", "Given an XG with initial symbol s , a sentence t is in the language defined by the XG if there is a sequence of rule applications that transforms s into a string from which t can be obtained by deleting all brackets .", "I shall refer to the restrictions on XG rule application which I have just described as the bracketing constraint .", "The effect of the bracketing constraint is independent of the order of application of rules , because if two rules are used in a derivation , the brackets introduced by each of them must be compatible in the way described above .", "As brackets are added and never deleted , it is clear that the order of application is irrelevant .", "For similar reasons , any two applications in a derivation where the rules involved have more than one segment in their left hand sides , one and only one of the two following situations arises If one follows to the letter the definitions in this section , then checking , in a parsing procedure , whether an XG rule may be applied , would require a scan of the whole intermediate string .", "However , we will see in Section 10 that this check may be done quot ; on the fly quot ; as brackets are introduced , with a cost independent of the length of the current intermediate string in the derivation .", "In the same way as parse trees are used to visualise context free derivations , I use derivation graphs to represent XG derivations .", "In a derivation graph , as in a parse tree , each node corresponds to a rule application or to a terminal symbol in the derived sentence , and the edges leaving a node correspond to the symbols in the right hand side of that node's rule .", "In a derivation graph , however , a node can have more than one incoming edge in fact , one such edge for each of the symbols on the lefthand side of the rule corresponding to that node .", "Of these edges , only the one corresponding to the leading symbol is used to define the left to right order of the symbols in the sentence whose derivation is represented by the graph .", "If one deletes from a derivation graph all except the first of the incoming edges to every node , the result is a tree analogous to a parse tree .", "For example , Figure 7 . 1 shows the derivation graph for the string quot ; aabbcc quot ; according to the XG This XG defines the language formed by the set of all strings anbnen for n 0 .", "The example shows , incidentally , that XGs , even without arguments , are strictly more powerful than CFGs , since the language described is not context free .", "The topology of derivation graphs reflects clearly the bracketing constraint .", "Assume the following two conventions for the drawing of a derivation graph , which are followed in all the graphs shown here Then the derivation graph obeys the bracketing constraint if and only if it can be drawn , following the conventions , without any edges crossing . 1 The example of Figure 7 . 2 shows this clearly .", "In this figure , the closed path formed by edges 1 , 2 , 3 , and 4 has the same effect as a matching pair of brackets in a bracketed string .", "It is also worth noting that nested rule applications appear in a derivation graph as a configuration like the one depicted in Figure 7 . 3 .", "XGs and Left Extraposition We saw in Figure 4 . 2 a DCG for some relative clauses .", "The XG of Figure 8 . 1 describes essentially the same language fragment , showing how easy it is to describe left extraposition in an XG .", "In that grammar , the sentence The mouse that the cat chased squeaks . has the derivation graph shown in Figure 8 . 2 .", "The left extraposition implicit in the structure of the sentence is represented in the derivation graph by the application of the rule for 'rel marker' , at the node marked in the figure .", "One can say that the left extraposition has been quot ; reversed quot ; in the derivation by the use of this rule , which may be looked at as repositioning 'trace' to the right , thus quot ; reversing quot ; the extraposition of the original sentence .", "In the rest of this paper , I often refer to a constituent being repositioned into a bracketed string or into a fragment of derivation graph , to mean that a rule having that constituent as a non leading symbol in the left hand side has been applied , and the symbol matches some symbol in the string or corresponds to some edge in the fragment .", "For example , in Figure 8 . 2 the trace T is repositioned into the subgraph with root In the example of Figure 8 . 2 , there is only one application of a non DCG rule , at the place marked .", "However , we have seen that when a derivation contains several applications of such rules , the applications must obey the bracketing constraint .", "The use of the constraint in a grammar is better explained with an example .", "From the sentences The mouse squeaks .", "The cat likes fish .", "The cat chased the mouse . the grammar of Figure 8 . 1 can derive the following string , which violates the complex NP constraint The mouse that the cat that chased likes fish squeaks .", "The derivation of this ungrammatical string can be better understood if we compare it with a sentence outside the fragment The mouse , that the cat which chased it likes fish , squeaks . where the pronoun 'it' takes the place of the incorrect trace .", "The derivation graph for that un English string is shown in Figure 9 . 1 .", "In the graph , and mark two nested applications of the rule for rel marker' .", "The string is un English because the higher 'relative' marked in the graph binds a trace occurring inside a sentence which is part of the subordinated 'noun phrase' .", "Now , using the bracketing constraint one can neatly express the complex NP constraint .", "It is only necessary to change the second rule for 'relative' in Figure 8 . 1 to relative open , rel marker , sentence , close .", "5 and add the rule With this modified grammar , it is no longer possible to violate the complex NP constraint , because no constituent can be repositioned from outside into the gap created by the application of rule 6 to the result of applying the rule for relatives 5 .", "The non terminals 'open' and 'close' bracket a subderivation . . . open x close . . . x preventing any constituent from being repositioned from outside that subderivation into it .", "Figure 9 . 2 shows the use of rule 6 in the derivation of the sentence The mouse that the cat that likes fish chased squeaks .", "This is based on the same three simple sentences as the ungrammatical string of Figure 9 . 1 , which the reader can now try to derive in the modified grammar , to see how the bracketing constraint prevents the derivation .", "In the previous sections , I avoided the complication of non terminal arguments .", "Although it would be possible to describe fully the operation of XGs in terms of derivations on bracketed strings , it is much simpler to complete the explanation of XGs using the translation of XG rules into definite clauses .", "In fact , a rigorous definition of XGs independently of definite clauses would require a formal apparatus very similar to the one needed to formalise definite clause programs in the first place , and so it would fall outside the scope of the present paper .", "The interested reader will find a full discussion of those issues in two articles by Colmerauer 2 , 3 .", "Like a DCG , a general XG is no more than a convenient notation for a set of definite clauses .", "An XG non terminal of arity n corresponds to an n 4 place predicate with the same name .", "Of the extra four arguments , two are used to represent string positions as in DCGs , and the other two are used to represent positions in an extraposition list , which carries symbols to be repositioned .", "Each element of the extraposition list represents a symbol being repositioned as a 4 tuple x context , type , symbol , xlist where context is either 'gap' , if the symbol was preceded by . . . ' in the rule where it originated , or nogap' , if the symbol was preceded by , ' ; type may be 'terminal' or nonterminal' , with the obvious meaning ; symbol is the symbol proper ; x ist is the remainder of the extraposition list an empty list being represented by' ' .", "An XG rule is translated into a clause for the predicate corresponding to the leading symbol of the rule .", "In the case where the XG rule has just a single symbol on the left hand side , the translation is very similar to that of DCG rules .", "For example , the rule A terminal t in the right hand side of a rule translates into a call to the predicate 'terminal' , defined below , whose role is analogous to that of 'connects' in DCGs .", "For example , the rule The translation of a rule with more than one symbol in the left hand side is a bit more complicated .", "Informally , each symbol after the first is made into a 4 tuple as described above , and fronted to the extraposition list .", "Thus , for example , the rule rel marker . . . trace rel pronoun .", "Furthermore , for each distinct non leading nonterminal nt with arity n in the left hand side of a rule of the XG , the translation includes the clause where virtual C , X0 , X ' , defined later , can be read as quot ; C is the constituent between XO and X in the extraposition list quot ; , and the variables Vi transfer the arguments of the symbol in the extraposition list to the predicate which translates that symbol .", "For example , the rule marker Var , the . . . of . whom , trace Var whose . which can be used in a more complex grammar of relative clauses to transform quot ; whose X quot ; into quot ; the X of whom quot ; , corresponds to the clauses Finally , the two auxiliary predicates 'virtual' and 'terminal' are defined as follows gap x gap , T , S , X . gap . where 'connects' is as for DCGs .", "These definitions need some comment .", "The first clause for 'terminal' says that , provided the current extraposition list allows a gap to appear in the derivation , terminal symbol T may be taken from the position SO in the source string , where T connects SO to some new position S . The second clause for 'terminal' says that if the next symbol in the current extraposition list is a terminal T , then this symbol can be taken as if it occurred at S in the source string .", "The clause for 'virtual' allows a non terminal to be quot ; read off from quot ; the extraposition list .", "relative 6 , 9 , X , X open 6 , 6 , x gap , nt , trace , x gap , nt , close , , x gap , nt , close , x gap , nt , trace , x gap , nt , close , rel_marker 6 , 7 , x gap , nt , close , x gap , nt , trace , x gap , nt , close , , x gap , nt , trace , x gap , nt , close , x gap , nt , trace , x gap , nt , close , The nodes of the analysis fragment , for the relative clause quot ; that likes fish quot ; , are represented by the corresponding goals , indented in proportion to their distance from the root of the graph .", "The following conventions are used to simplify the figure The definite clause program corresponding to the grammar for this example is listed in Appendix II .", "The example shows clearly how the bracketing constraint works .", "Symbols are placed in the extraposition list by rules with more than one symbol in the left hand side , and removed by calls to 'virtual' , on a first in last out basis ; that is , the extraposition list is a stack .", "But this property of the extraposition list is exactly what is needed to balance quot ; on the fly quot ; the auxiliary brackets in the intermediate steps of a derivation .", "Being no more than a logic program , an XG can be used for analysis and for synthesis in the same way as a DCG .", "For instance , to determine whether a string s with initial point initial and final point final is in the language defined by the XG of Figure 8 . 1 , one tries to prove the goal statement As for DCGs , the string s can be represented in several ways .", "If it is represented as a list , the above goal would be written ? sentence sj LE LE 1 .", "The last two arguments of the goal are 1 ' to mean that the overall extraposition list goes from ' ' to ' ' ; i . e . , it is the empty list .", "Thus , no constituent can be repositioned into or out of the top level 'sentence' .", "In this paper I have proposed an extension of DCGs .", "The motivation for this extension was to provide a simple formal device to describe the structure of such important natural language constructions as relative clauses and interrogative sentences .", "In transformational grammar , these constructions have usually been analysed in terms of left extraposition , together with global constraints , such as the complex NP constraint , which restrict the range of the extraposition .", "Global constraints are not explicit in the grammar rules , but are given externally to be enforced across rule applications .", "These external global constraints cause theoretical difficulties , because the formal properties of the resulting systems are far from evident , and practical difficulties , because they lead to obscure grammars and prevent the use of any reasonable parsing algorithm .", "DCGs , although they provide the basic machinery for a clear description of languages and their structures , lack a mechanism to describe simply left extraposition and the associated restrictions .", "MGs can express the rewrite of several symbols in a single rule , but the symbols must be contiguous , as in a type 0 grammar rule .", "This is still not enough to describe left extraposition without complicating the rest of the grammar .", "XGs are an answer to those limitations .", "An XG has the same fundamental property as a DCG , that it is no more than a convenient notation for the clauses of an ordinary logic program .", "XGs and their translation into definite clauses have been designed to meet three requirements i to be a principled extension of DCGs , which can be interpreted as a grammar formalism independently of its translation into definite clauses ; ii to provide for simple description of left extraposition and related restrictions ; iii to be comparable in efficiency with DCGs when executed by PROLOG .", "It turns out that these requirements are not contradictory , and that the resulting design is extremely simple .", "The restrictions on extraposition are naturally expressed in terms of scope , and scope is expressed in the formalism by quot ; bracketing out quot ; subderivations corresponding to balanced strings .", "The notion of bracketed string derivation is introduced in order to describe extraposition and bracketing independently of the translation of XGs into logic programs .", "Some questions about XGs have not been tackled in this paper .", "First , from a theoretical point of view it would be necessary to complete the independent characterisation of XGs in terms of bracketed strings , and show rigorously that the translation of XGs into logic programs correctly renders this independent characterisation of the semantics of XGs .", "As pointed out before , this formalisation does not offer any substantial problems .", "Next , it is not clear whether XGs are as general as they could be .", "For instance , it might be possible to extend them to handle right extraposition of constituents , which , although less common than left extraposition , can be used to describe quite frequent English constructions , such as the gap between head noun and relative clause in What files are there that were created today ?", "It may however be possible to describe such situations in terms of left extraposition of some other constituent e . g . the verb phrase quot ; are there quot ; in the example above .", "Finally , I have been looking at what transformations should be applied to an XG developed as a clear description of a language , so that the resulting grammar could be used more efficiently in parsing .", "In particular , I have been trying to generalise results on deterministic parsing of context free languages into appropriate principles of transformation .", "David Warren and Michael McCord read drafts of this paper , and their comments led to many improvements , both in content and in form .", "The comments of the referees were also very useful .", "A British Council Fellowship partly supported my work in this subject .", "The computing facilities I used to experiment with XGs and to prepare this paper were made available by British Science Research Council grants ."], "summary_lines": ["Extraposition Grammars\n", "Extraposition grammars are an extension of definite clause grammars, and are similarly defined in terms of logic clauses.\n", "The extended formalism makes it easy to describe left extraposition of constituents, an important feature of natural language syntax.\n", "Whereas head grammars provide for an account of verb fronting and cross-serial dependencies, we, introducing extraposition grammars, is focused on displacement of noun phrases in English.\n"]}
{"article_lines": ["PARADISE A Framework For Evaluating Spoken Dialogue Agents", "This paper presents PARADISE PARAdigm for DIalogue System Evaluation , a general framework for evaluating spoken rlialogue agents .", "The framework decouples task requirements from an agent's dialogue behaviors , supports comparisons among dialogue strategies , enables the calculation of performance over subdialogues and whole dialogues , specifies the relative contribution of various factors to performance , and makes it possible to compare agents performing different tasks by normalizing for task complexity .", "Recent advances in dialogue modeling , speech recognition , and natural language processing have made it possible to build spoken dialogue agents for a wide variety of applications . '", "Potential benefits of such agents include remote or hands free access , ease of use , naturalness , and greater efficiency of interaction .", "However , a critical obstacle to progress in this area is the lack of a general framework for evaluating and comparing the performance of different dialogue agents .", "One widely used approach to evaluation is based on the notion of a reference answer Hirschman et al . , 1990 .", "An agent's responses to a query are compared with a predefined key of minimum and maximum reference answers ; performance is the proportion of responses that match the key .", "This approach has many widely acknowledged limitations Hirschman and Pao , 1993 ; Danieli et al . , 1992 ; Bates and Ayuso , 1993 , e . g . , although there may be many potential dialogue strategies for carrying out a task , the key is tied to one particular dialogue strategy .", "In contrast , agents using different dialogue strategies can be compared with measures such as inappropriate utterance ratio , turn correction ratio , concept accuracy , implicit recovery and transaction success Danieli 'We use the term agent to emphasize the fact that we are evaluating a speaking entity that may have a personality .", "Readers who wish to may substitute the word quot ; system quot ; wherever quot ; agent quot ; is used . and Gerbino , 1995 ; Hirschman and Pao , 1993 ; Polifroni et al . , 1992 ; Simpson and Fraser , 1993 ; Shriberg , Wade , and Price , 1992 .", "Consider a comparison of two train timetable information agents Danieli and Gerbino , 1995 , where Agent A in Dialogue 1 uses an explicit confirmation strategy , while Agent B in Dialogue 2 uses an implicit confirmation strategy Danieli and Gerbino found that Agent A had a higher transaction success rate and produced less inappropriate and repair utterances than Agent B , and thus concluded that Agent A was more robust than Agent B .", "However , one limitation of both this approach and the reference answer approach is the inability to generalize results to other tasks and environments Fraser , 1995 .", "Such generalization requires the identification of factors that affect performance Cohen , 1995 ; Sparck Jones and Galliers , 1996 .", "For example , while Danieli and Gerbino found that Agent A's dialogue strategy produced dialogues that were approximately twice as long as Agent B's , they had no way of determining whether Agent A's higher transaction success or Agent B's efficiency was more critical to performance .", "In addition to agent factors such as dialogue strategy , task factors such as database size and environmental factors such as background noise may also be relevant predictors of performance .", "These approaches are also limited in that they currently do not calculate performance over subdialogues as well as whole dialogues , correlate performance with an external validation criterion , or normalize performance for task complexity .", "This paper describes PARADISE , a general framework for evaluating spoken dialogue agents that addresses these limitations .", "PARADISE supports comparisons among dialogue strategies by providing a task representation that decouples what an agent needs to achieve in terms of the task requirements from how the agent carries out the task via dialogue .", "PARADISE uses a decision theoretic framework to specify the relative contribution of various factors to an agent's overall performance .", "Performance is modeled as a weighted function of a task based success measure and dialogue based cost measures , where weights are computed by correlating user satisfaction with performance .", "Also , performance can be calculated for subdialogues as well as whole dialogues .", "Since the goal of this paper is to explain and illustrate the application of the PARADISE framework , for expository purposes , the paper uses simplified domains with hypothetical data throughout .", "Section 2 describes PARADISE's performance model , and Section 3 discusses its generality , before concluding in Section 4 .", "PARADISE uses methods from decision theory Keeney and Raiffa , 1976 ; Doyle , 1992 to combine a disparate set of performance measures i . e . , user satisfaction , task success , and dialogue cost , all of which have been previously noted in the literature into a single performance evaluation function .", "The use of decision theory requires a specification of both the objectives of the decision problem and a set of measures known as attributes in decision theory for operationalizing the objectives .", "The PARADISE model is based on the structure of objectives rectangles shown in Figure 1 .", "The PARADISE model posits that performance can be correlated with a meaningful external criterion such as usability , and thus that the overall goal of a spoken dialogue agent is to maximize an objective related to usability .", "User satisfaction ratings Kamm , 1995 ; Shriberg , Wade , and Price , 1992 ; Polifroni et al . , 1992 have been frequently used in the literature as an external indicator of the usability of a dialogue agent .", "The model further posits that two types of factors are potential relevant contributors to user satisfaction namely task success and dialogue costs , and that two types of factors are potential relevant contributors to costs Walker , 1996 .", "In addition to the use of decision theory to create this objective structure , other novel aspects of PARADISE include the use of the Kappa coefficient Carletta , 1996 ; Siegel and Castellan , 1988 to operationalize task success , and the use of linear regression to quantify the relative contribution of the success and cost factors to user satisfaction .", "The remainder of this section explains the measures ovals in Figure 1 used to operationalize the set of objectives , and the methodology for estimating a quantitative performance function that reflects the objective structure .", "Section 2 . 1 describes PARADISE's task representation , which is needed to calculate the task based success measure described in Section 2 . 2 .", "Section 2 . 3 describes the cost measures considered in PARADISE , which reflect both the efficiency and the naturalness of an agent's dialogue behaviors .", "Section 2 . 4 describes the use of linear regression and user satisfaction to estimate the relative contribution of the success and cost measures in a single performance function .", "Finally , Section 2 . 5 explains how performance can be calculated for subdialogues as well as whole dialogues , while Section 2 . 6 summarizes the method .", "A general evaluation framework requires a task representation that decouples what an agent and user accomplish from how the task is accomplished using dialogue strategies .", "We propose that an attribute value matrix AVM can represent many dialogue tasks .", "This consists of the information that must be exchanged between the agent and the user during the dialogue , represented as a set of ordered pairs of attributes and their possible values . 2 As a first illustrative example , consider a simplification of the train timetable domain of Dialogues 1 and 2 , where the timetable only contains information about rush hour trains between four cities , as shown in Table 1 .", "This AVM consists of four attributes abbreviations for each attribute name are also shown . 3 In Table 1 , these attribute value pairs are annotated with the direction of information flow to represent who acquires the information , although this information is not used for evaluation .", "During the dialogue the agent must acquire from the user the values of DC , AC , and DR , while the user must acquire DT .", "Performance evaluation for an agent requires a corpus of dialogues between users and the agent , in which users execute a set of scenarios .", "Each scenario execution has a corresponding AVM instantiation indicating the task information requirements for the scenario , where each attribute is paired with the attribute value obtained via the dialogue .", "For example , assume that a scenario requires the user to find a train from Torino to Milano that leaves in the evening , as in the longer versions of Dialogues 1 and 2 in Figures 2 and 3 . 4 Table 2 contains an AVM corresponding to a quot ; key quot ; for this scenario .", "All dialogues resulting from execution of this scenario in which the agent and the user correctly convey all attribute values as in Figures 2 and 3 would have the same AVM as the scenario key in Table 2 .", "The AVMs of the remaining dialogues would differ from the key by at least one value .", "Thus , even though the dialogue strategies in Figures 2 and 3 are radically different , the AVM task representation for these dialogues is identical and the performance of the system for the same task can thus be assessed on the basis of the AVM representation .", "Success at the task for a whole dialogue or subdialogue is measured by how well the agent and user achieve the information requirements of the task by the end of the dialogue or subdialogue .", "This section explains how PARADISE uses the Kappa coefficient Carletta , 1996 ; Siegel and Castellan , 1988 to operationalize the taskbased success measure in Figure 1 .", "The Kappa coefficient , K , is calculated from a confusion matrix that summarizes how well an agent achieves the information requirements of a particular task for a set of dialogues instantiating a set of scenarios . 5 For example , Tables 3 and 4 show two hypothetical confusion matrices that could have been generated in an evaluation of 100 complete dialogues with each of two train timetable agents A and B perhaps using the confirmation strategies illustrated in Figures 2 and 3 , respectively . 6 The values in the matrix cells are based on comparisons between the dialogue and scenario key AVMs .", "Whenever an attribute value in a dialogue i . e . , data AVM matches the value in its scenario key , the number in the appropriate diagonal cell of the matrix boldface for clarity is incremented by 1 .", "The off diagonal cells represent misunderstandings that are not corrected in the dialogue .", "Note that depending on the strategy that a spoken dialogue agent uses , confusions across attributes are possible , e . g . , quot ; Milano quot ; could be confused with quot ; morning . quot ; The effect of misunderstandings that are corrected during the course of the dialogue are reflected in the costs associated with the dialogue , as will be discussed below .", "The first matrix summarizes how the 100 AVMs representing each dialogue with Agent A compare with the AVMs representing the relevant scenario keys , while the second matrix summarizes the information exchange with Agent B . Labels vi to v4 in each matrix represent the possible values of depart city shown in Table 1 ; v5 to v8 are for arrival city , etc .", "Columns represent the key , specifying which information values the agent and user were supposed to communicate to one another given a particular scenario .", "The equivalent column sums in both tables reflects that users of both agents were assumed to have performed the same scenarios .", "Rows represent the data collected from the dialogue corpus , reflecting what attribute values were actually communicated between the agent and the user .", "Given a confusion matrix M , success at achieving the information requirements of the task is measured with the Kappa coefficient Carletta , 1996 ; Siegel and Castellan , 1988 by chance . 7 When there is no agreement other than that which would be expected by chance , n 0 .", "When there is total agreement , K 1 . ic is superior to other measures of success such as transaction success Danieli and Gerbino , 1995 , concept accuracy Simpson and Fraser , 1993 , and percent agreement Gale , Church , and Yarowsky , 1992 because n takes into account the inherent complexity of the task by correcting for chance expected agreement .", "Thus rc provides a basis for comparisons across agents that are performing different tasks .", "When the prior distribution of the categories is unknown , P E , the expected chance agreement between the data and the key , can be estimated from the distribution of the values in the keys .", "This can be calculated from confusion matrix M , since the columns represent the values in the keys .", "In particular P A is the proportion of times that the AVMs for the actual set of dialogues agree with the AVMs for the scenario keys , and P E is the proportion of times that the AVMs for the dialogues and the keys are expected to agree 7K has been used to measure pairwise agreement among coders making category judgments Carletta , 1996 ; Krippendorf , 1980 ; Siegel and Castellan , 1988 .", "Thus , the observed user agent interactions are modeled as a coder , and the ideal interactions as an expert coder . where ti is the sum of the frequencies in column i of M , and T is the sum of the frequencies in M ti .", "tn .", "P A , the actual agreement between the data and the key , is always computed from the confusion matrix M Given the confusion matrices in Tables 3 and 4 , P E 0 . 079 for both agents . 8 For Agent A , P A 0 . 795 and frc 0 . 777 , while for Agent B , P A 0 . 59 and c 0 . 555 , suggesting that Agent A is more successful than B in achieving the task goals .", "As shown in Figure 1 , performance is also a function of a combination of cost measures .", "Intuitively , cost measures should be calculated on the basis of any user or agent dialogue behaviors that should be minimized .", "A wide range of cost measures have been used in previous work ; these include pure efficiency measures such as the number of turns or elapsed time to complete the task Abella , Brown , and Buntschuh , 1996 ; Hirschman et al . , 1990 ; Smith and Gordon , 1997 ; Walker , 1996 , as well as measures of qualitative phenomena such as inappropriate or repair utterances Danieli and Gerbino , 1995 ; Hirschman and Pao , 1993 ; Simpson and Fraser , 1993 .", "PARADISE represents each cost measure as a function ci that can be applied to any sub dialogue .", "First , consider the simplest case of calculating efficiency measures over a whole dialogue .", "For example , let cl be the total number of utterances .", "For the whole dialogue D1 in Figure 2 , c1 D1 is 23 utterances .", "For the whole dialogue D2 in Figure 3 , ci D2 is 10 utterances .", "To calculate costs over subdialogues and for some of the qualitative measures , it is necessary to be able to specify which information goals each utterance contributes to .", "PARADISE uses its AVM representation to link the information goals of the task to any arbitrary dialogue behavior , by tagging the dialogue with the attributes for the task . 9 This makes it possible to evaluate any potential dialogue strategies for achieving the task , as well as to evaluate dialogue strategies that operate at the level of dialogue subtasks subdialogues .", "Consider the longer versions of Dialogues 1 and 2 in Figures 2 and 3 .", "Each utterance in Figures 2 and 3 has been tagged using one or more of the attribute abbreviations in Table 1 , according to the subtask s the utterance contributes to .", "As a convention of this type of tagging , 'Using a single confusion matrix for all attributes as in Tables 3 and 4 inflates 1G when there are few cross attribute confusions by making P E smaller .", "In some cases it might be desirable to calculate ic first for identification of attributes and then for values within attributes , or to average ic for each attribute to produce an overall ic for the task .", "9This tagging can be hand generated , or system generated and hand corrected .", "Preliminary studies indicate that reliability for human tagging is higher for AVM attribute tagging than for other types of discourse segment tagging Passonneau and Litman , 1997 ; Hirschberg and Nakatani , 1996 . utterances that contribute to the success of the whole dialogue , such as greetings , are tagged with all the attributes .", "Since the structure of a dialogue reflects the structure of the task Carberry , 1989 ; Grosz and Sidner , 1986 ; Litman and Allen , 1990 , the tagging of a dialogue by the AVM attributes can be used to generate a hierarchical discourse structure such as that shown in Figure 4 for Dialogue 1 Figure 2 .", "For example , segment subdialogue S2 in Figure 4 is about both depart city DC and arrivalcity AC .", "It contains segments S3 and S4 within it , and consists of utterances UI . . . U6 .", "Tagging by AVM attributes is required to calculate costs over subdialogues , since for any subdialogue , task attributes define the subdialogue .", "For subdialogue S4 in Figure 4 , which is about the attribute arrival city and consists of utterances A6 and U6 , c S4 is 2 .", "Tagging by AVM attributes is also required to calculate the cost of some of the qualitative measures , such as number of repair utterances .", "Note that to calculate such costs , each utterance in the corpus of dialogues must also be tagged with respect to the qualitative phenomenon in question , e . g . whether the utterance is a repair For example , let c2 be the number of repair utterances .", "The repair utterances in Figure 2 are A3 through U6 , thus c2 D1 is 10 utterances and c2 S4 is 2 utterances .", "The repair utterance in Figure 3 is U2 , but note that according to the AVM task tagging , U2 simultaneously addresses the information goals for depart range .", "In general , if an utterance U contributes to the information goals of N different attributes , each attribute accounts for UN of any costs derivable from U .", "Thus , c2 D2 is . 5 .", "Given a set of ci , it is necessary to combine the difwPrevious work has shown that this can be done with high reliability Hirschman and Pao , 1993 . ferent cost measures in order to determine their relative contribution to performance .", "The next section explains how to combine is with a set of ci to yield an overall performance measure .", "Given the definition of success and costs above and the model in Figure 1 , performance for any sub dialogue D is defined as follows quot ; Here a is a weight on is , the cost functions ci are weighted by wi , and H is a Z score normalization function Cohen , 1995 .", "The normalization function is used to overcome the problem that the values of ci are not on the same scale as K , and that the cost measures ci may also be calculated over widely varying scales e . g . response delay could be measured using seconds while , in the example , costs were calculated in terms of number of utterances .", "This problem is easily solved by normalizing each factor x to its Z score cr , where is the standard deviation for x .", "Agents A and B To illustrate the method for estimating . a performance function , we will use a subset of the data from Tables 3 and 4 , shown in Table 5 .", "Table 5 represents the results quot ; We assume an additive performance utility function because it appears that K and the various cost factors ci are utility independent and additive independent Keeney and Raiffa , 1976 .", "It is possible however that user satisfaction data collected in future experiments or other data such as willingness to pay or use would indicate otherwise .", "If so , continuing use of an additive function might require a transformation of the data , a reworking of the model shown in Figure 1 , or the inclusion of interaction terms in the model Cohen , 1995 . from a hypothetical experiment in which eight users were randomly assigned to communicate with Agent A and eight users were randomly assigned to communicate with Agent B .", "Table 5 shows user satisfaction US ratings discussed below , is , number of utterances utt and number of repair utterances rep for each of these users .", "Users 5 and 11 correspond to the dialogues in Figures 2 and 3 respectively .", "To normalize ci for user 5 , we determine that FT is 38 . 6 and cc , is 18 . 9 .", "Thus , H ci is 0 . 83 .", "Similarly . 1V ci for user 11 is 1 . 51 .", "To estimate the performance function , the weights a and wi must be solved for .", "Recall that the claim implicit in Figure 1 was that the relative contribution of task success and dialogue costs to performance should be calculated by considering their contribution to user satisfaction .", "User satisfaction is typically calculated with surveys that ask users to specify the degree to which they agree with one or more statements about the behavior or the performance of the system .", "A single user satisfaction measure can be calculated from a single question , or as the mean of a set of ratings .", "The hypothetical user satisfaction ratings shown in Table 5 range from a high of 6 to a low of 1 .", "Given a set of dialogues for which user satisfaction US , is and the set of ci have been collected experimentally , the weights a and wi can be solved for using multiple linear regression .", "Multiple linear regression produces a set of coefficients weights describing the relative contribution of each predictor factor in accounting for the variance in a predicted factor .", "In this case , on the basis of the model in Figure 1 , US is treated as the predicted factor .", "Normalization of the predictor factors n and ci to their Z scores guarantees that the relative magnitude of the coefficients directly indicates the relative contribution of each factor .", "Regression on the Table 5 data for both sets of users tests which factors K , kat , rep most strongly predicts US .", "In this illustrative example , the results of the regression with all factors included shows that only K and rep are significant p . 02 .", "In order to develop a performance function estimate that includes only significant factors and eliminates redundancies , a second regression including only significant factors must then be done .", "In this case , a second regression yields the predictive equation i . e . , a is . 40 and w2 is . 78 .", "The results also show rc is significant at p . 0003 , rep significant at p . 0001 , and the combination of is and rep account for 92 of the variance in US , the external validation criterion .", "The factor utt was not a significant predictor of performance , in part because utt and rep are highly redundant .", "The correlation between utt and rep is 0 . 91 .", "Given these predictions about the relative contribution of different factors to performance , it is then possible to return to the problem first introduced in Section 1 given potentially conflicting performance criteria such as robustness and efficiency , how can the performance of Agent A and Agent B be compared ?", "Given values for a and wi , performance can be calculated for both agents using the equation above .", "The mean performance of A is . 44 and the mean performance of B is . 44 , suggesting that Agent B may perform better than Agent A overall .", "The evaluator must then however test these performance differences for statistical significance .", "In this case , a t test shows that differences are only significant at the p . 07 level , indicating a trend only .", "In this case , an evaluation over a larger subset of the user population would probably show significant differences .", "Since both tc and ci can be calculated over subdialogues , performance can also be calculated at the subdialogue level by using the values for a and wi as solved for above .", "This assumes that the factors that are predictive of global performance , based on US , generalize as predictors of local performance , i . e . within subdialogues defined by subtasks , as defined by the attribute tagging . I2 Consider calculating the performance of the dialogue strategies used by train timetable Agents A and B , over the subdialogues that repair the value of depart city .", "Segment S3 Figure 4 is an example of such a subdialogue with Agent A .", "As in the initial estimation of a performance function , our analysis requires experimental data , namely a set of values for and c , and the application of the Z score normalization function to this data .", "However , the values for rc and ci are now calculated at the subdialogue rather than the whole dialogue level .", "In addition , only data from comparable strategies can be used to calculate the mean and standard deviation for normalization .", "Informally , a comparable strategy is one which applies in the same state and has the same effects .", "For example , to calculate for Agent A over the subdialogues that repair depart city , P A and P E are computed using only the subpart of Table 3 concerned with depart city .", "For Agent A , P A . 78 , P E . 265 , and frc . 70 .", "Then , this value of is is normalized using data from comparable subdialogues with both Agent A and Agent B .", "Based on the data in Tables 3 and 4 , the mean is . 515 and a is . 261 , so that H x for Agent A is . 71 .", "To calculate c2 for Agent A , assume that the average number of repair utterances for Agent A's subdialogues that repair depart city is 6 , that the mean over all comparable repair subdialogues is 4 , and the standard deviation is 2 . 79 .", "Then H c2 is . 72 .", "Let Agent A's repair dialogue strategy for subdialogues repairing depart city be RA and Agent B's repair strategy for depart city be Rg .", "Then using the performance equation above , predicted performance for RA is For Agent B , using the appropriate subpart of Table 4 to calculate lc , assuming that the average number of depart city repair utterances is 1 . 38 , and using similar I2This assumption has a sound basis in theories of dialogue structure Carberry , 1989 ; Grosz and Sidner , 1986 ; Litman and Allen , 1990 , but should be tested empirically . calculations , yields Performance RB . 40 . 71 . 78 . 94 0 . 45 Thus the results of these experiments predict that when an agent needs to choose between the repair strategy that Agent B uses and the repair strategy that Agent A uses for repairing depart city , it should use Agent B's strategy RB , since the performance RB is predicted to be greater than the performance RA .", "Note that the ability to calculate performance over subdialogues allows us to conduct experiments that simultaneously test multiple dialogue strategies .", "For example , suppose Agents A and B had different strategies for presenting the value of depart time in addition to different confirmation strategies .", "Without the ability to calculate performance over subdialogues , it would be impossible to test the effect of the different presentation strategies independently of the different confirmation strategies .", "We have presented the PARADISE framework , and have used it to evaluate two hypothetical dialogue agents in a simplified train timetable task domain .", "We used PARADISE to derive a performance function for this task , by estimating the relative contribution of a set of potential predictors to user satisfaction .", "The PARADISE methodology consists of the following steps Note that all of these steps are required to develop the performance function .", "However once the weights in the performance function have been solved for , user satisfaction ratings no longer need to be collected .", "Instead , predictions about user satisfaction can be made on the basis of the predictor variables , as illustrated in the application of PARADISE to subdialogues .", "Given the current state of knowledge , it is important to emphasize that researchers should be cautious about generalizing a derived performance function to other agents or tasks .", "Performance function estimation should be done iteratively over many different tasks and dialogue strategies to see which factors generalize .", "In this way , the field can make progress on identifying the relationship between various factors and can move towards more predictive models of spoken dialogue agent performance .", "In the previous section we used PARADISE to evaluate two confirmation strategies , using as examples fairly simple information access dialogues in the train timetable domain .", "In this section we demonstrate that PARADISE is applicable to a range of tasks , domains , and dialogues , by presenting AVMs for two tasks involving more than information access , and showing how additional dialogue phenomena can be tagged using AVM attributes .", "First , consider an extension of the train timetable task , where an agent can handle requests to reserve a seat or purchase a ticket .", "This task could be represented using the AVM in Table 6 an extension of Table 1 , where the agent must now acquire the value of the attribute request type , in order to know what to do with the other information it has acquired .", "Figure 5 presents a hypothetical dialogue in this extended task domain , and illustrates user utterance types and an agent dialogue strategy that are very different from those in Figures 2 and 3 .", "First , Agent C in Figure 5 uses a quot ; no confirmation quot ; dialogue strategy , in contrast to the explicit and implicit confirmation strategies used in Figures 2 and 3 .", "Second , Figure 5 illustrates new types of user utterances that do not directly further the informational goals of the task .", "In U2 , the user asks the agent a wh question about the DR attribute itself , rather than providing information about that attribute's value .", "Since U2 satisfies a knowledge precondition related to answering Cl , U2 contributes to the DR goal and is tagged as such .", "In U3 , the user similarly asks a yes no question that addresses a subgoal related to answering Cl .", "Finally , U5 illustrates a user request for an agent action , and is tagged with the RT attribute .", "The value of RT in the AVM instantiation for the dialogue would be quot ; reserve ? '", "Second , consider the very different domain and task of diagnosing a fault and repairing a circuit Smith and Gordon , 1997 .", "Figure 6 presents one dialogue from this domain .", "Smith and Gordon collected 144 dialogues for this task , in which agent initiative was varied by using different dialogue strategies , and tagged each dialogue according to the following subtask structure 13 Our informational analysis of this task results in the AVM shown in Table 7 .", "Note that the attributes are almost identical to Smith and Gordon's list of subtasks .", "CircuitID corresponds to Introduction , Correct Circuit Behavior and Current Circuit Behavior correspond to Assessment , Fault Type corresponds to Diagnosis , Fault Correction corresponds to Repair , and Test corresponds to Test .", "The attribute names emphasize information exchange , while the subtask names emphasize function .", "Figure 6 is tagged with the attributes from Table 7 .", "Smith and Gordon's tagging of this dialogue according to their subtask representation was as follows turns 14 were I , turns 5 14 were A , turns 15 16 were D , turns 17 18 were R , and turns 19 35 were T . Note that there are only two differences between the dialogue structures yielded by the two tagging schemes .", "First , in our scheme Figure 6 , the greetings turns 1 and 2 are tagged with all the attributes .", "Second , Smith and Gordon's single tag A corresponds to two attribute tags in Table 7 , which in our scheme defines an extra level of structure within assessment subdialogues .", "This paper presented the PARADISE framework for evaluating spoken dialogue agents .", "PARADISE is a general framework for evaluating spoken dialogue agents that integrates and enhances previous work .", "PARADISE supports comparisons among dialogue strategies with a task representation that decouples what an agent needs to achieve in terms of the task requirements from how the agent carries out the task via dialogue .", "Furthermore , this task representation supports the calculation of performance over subdialogues as well as whole dialogues .", "In addition , because PARADISE's success measure normalizes for task complexity , it provides a basis for comparing agents performing different tasks .", "The PARADISE performance measure is a function of both task success K and dialogue costs ci , and has a number of advantages .", "First , it allows us to evaluate performance at any level of a dialogue , since K and ci can be calculated for any dialogue subtask .", "Since performance can be measured over any subtask , and since dialogue strategies can range over subdialogues or the whole dialogue , we can associate performance with individual dialogue strategies .", "Second , because our success measure K takes into account the complexity of the task , comparisons can be made across dialogue tasks .", "Third , K allows us to measure partial success at achieving the task .", "Fourth , performance can combine both objective and subjective cost measures , and specifies how to evaluate the relative contributions of those costs factors to overall performance .", "Finally , to our knowledge , we are the first to propose using user satisfaction to determine weights on factors related to performance .", "In addition , this approach is broadly integrative , incorporating aspects of transaction success , concept accuracy , multiple cost measures , and user satisfaction .", "In our framework , transaction success is reflected in K , corresponding to dialogues with a P A of 1 .", "Our performance measure also captures information similar to concept accuracy , where low concept accuracy scores translate into either higher costs for acquiring information from the user , or lower K scores .", "One limitation of the PARADISE approach is that the task based success measure does not reflect that some solutions might be better than others .", "For example , in the train timetable domain , we might like our task based success measure to give higher ratings to agents that suggest express over local trains , or that provide helpful information that was not explicitly requested , especially since the better solutions might occur in dialogues with higher costs .", "It might be possible to address this limitation by using the interval scaled data version of K Krippendorf , 1980 .", "Another possibility is to simply substitute a domain specific task based success measure in the performance model for K . The evaluation model presented here has many applications in apoken dialogue processing .", "We believe that the framework is also applicable to other dialogue modalities , and to human human task oriented dialogues .", "In addition , while there are many proposals in the literature for algorithms for dialogue strategies that are cooperative , collaborative or helpful to the user Webber and Joshi , 1982 ; Pollack , Hirschberg , and Webber , 1982 ; Joshi , Webber , and Weischedel , 1984 ; Chu Carrol and Carberry , 1995 , very few of these strategies have been evaluated as to whether they improve any measurable aspect of a dialogue interaction .", "As we have demonstrated here , any dialogue strategy can be evaluated , so it should be possible to show that a cooperative response , or other cooperative strategy , actually improves task performance by reducing costs or increasing task success .", "We hope that this framework will be broadly applied in future dialogue research .", "We would like to thank James Allen , Jennifer ChuCarroll , Morena Danieli , Wieland Eckert , Giuseppe Di Fabbrizio , Don Hindle , Julia Hirschberg , Shri Narayanan , Jay Wilpon , Steve Whittaker and three anonymous reviews for helpful discussion and comments on earlier versions of this paper ."], "summary_lines": ["PARADISE: A Framework For Evaluating Spoken Dialogue Agents\n", "This paper presents PARADISE (PARAdigm for Dialogue System Evaluation), a general framework for evaluating spoken dialogue agents.\n", "The framework decouples task requirements from an agent's dialogue behaviors, supports comparisons among dialogue strategies, enables the calculation of performance over subdialogues and whole dialogues, specifies the relative contribution of various factors to performance, and makes it possible to compare agents performing different tasks by normalizing for task complexity.\n", "We identify three factors which carry an influence on the performance of SDSs: agent factors (mainly related to the dialogue and the system itself), task factors (related to how the SDS captures the task it has been developed for) and environmental factor (e.g. factors related to the acoustic environment and the transmission channel.\n", "We aim to evaluate diaglogue agent strategies by relating overall user satisfaction to other metrics such as task success, efficiency measure and qualitative measures.\n"]}
{"article_lines": ["Mot ivat ions and Methods tbr Text Simpli f icat ion R . Chandrasekar Chr ist ine Doran B . Srinivas Institute for Research in l el artm ; nt of Deparl ; mcnt of Cognitive , Science cntcr for , inguistics Oml uter ?", "the Advanced Study of hldia InlbrHlatioll Scienc ; University ot7 Pcnnsylwmia , lqfiladclphia , PA 19104 ra ? ckeyc , doran , sr in ?", "upenn , edu Abst ract Lottg alld eolni licated seltteltces prov to b a . stumbling block for current systems rely ing on N , input .", "These systenls stand to gaill frolil ntethods that syntacti aHy sim plily su h sentences .", "b simplify a sen tence , we nee t an idea of tit .", "structure of the sentence , to identify the omponents o be separated out .", "Obviously a parser couhl be used to obtain the complete structure of the sentence .", "owever , hill parsing is slow a nd i rone to fa . ilure , especially on omph ! x sentences .", "In this l aper , we consider two alternatives to fu l parsing which could be use l for simplification .", "The tirst al l roach uses a Finite State Grammar FSG to pro dn e noun and verb groups while the second uses a Superta . gging model to i roduce de pendency linkages .", "We discuss the impact of these two input representations on the sim plification pro ess .", "1 Reasons fo r Text S impl i f i ca t ion l , ong and oml licatcd sentences prove to be a s tuml J ing block for urrent systems which rely on natural language input .", "llmsc systems stand to gain from metho ls that preprocess uch sentences so as to make them simpler .", "Consider , for exam ph ; , the following sentence l 7he embattled Major government survived a crucial vole on coal pits closure as its last minute concessions curbed the extent of lbry revolt over an issue that generated uausual heat in the l ousc of Commons and brought the miners to London streets .", "Such sentences are not uncommon in newswire texts .", "ompare this with the mult i sentence ver sion which has been manual ly simplif ied 2 The embatlled Major governmcnl survived a crucial vote ou coal pits closure .", "Its last minute conccssious curbed the cxlenl o On leave flom the National Centre for Soft , ware Techno ogy , lulmohar ? ross Road No .", "9 , Juhu , Bombay 4 0 149 , India Tory revolt over the coal miue issue .", "Th . is issue generaled unusual heat in the ltousc of Commons .", "II also brought the miners to London streels .", "If coml lex text can be made simphx , sen ten es beconae easier to process , both for In O grams and humans .", "Wc discuss a simplif ica tion process which identifies components of a sen tence that may be separated out , and transforms each of these into f rec sta , d ing s impler sentences .", "learly , some mmnees of meaning from the origi nal text may be lost in the simpli f ication process .", "Simplit ication is theretbre inappropr iate for texts such as legal docunlents where it is importa . nt not to lose any nuance .", "I owew ; r , one c . tl COil ceive of several areas of natura l language process ing where such simplit ication would be of great use .", "This is especially true in dolnains uch as Ina chine translat ion , which commonly have a manual post processing stage , where semantic and prag mat ic repairs may be arried out if ne ; essary .", "Parsing Syntact ical ly omplex sentences arc likely to generate a large number of parses , and may cause parsers to fail altogether .", "Re solving ambiguit ies in a t tachment of con st i tuents is non tr ivial .", "This ambiguii , y is re duced for simpler sentences in e they involve fewer constituents .", "Fhus s impler sentences lead to faster parsing and less parse aml igu ity .", "Once the i arses for the s impler sentences are obtained , the subparses can be assembled to form a full parse , or left as is , depending on the appl icat ion .", "Machine Translat ion MT As in the pars ing case , s impli f ication results in s impler scn tential structures and reduced ambiguity .", "As argued in Chandrasekar , 1994 , this conld lead to improvements in the qual ity of ma chine translat ion .", "In format ion Retrieval IR systems usual ly re trieve large segments of texts of which only a part n ay bc reh wml , .", "Wit , simplif ied texts , it is possible to extract Sl eCific phrases or simple sentences of relevance in response to queries .", "Summarization With the overload of infor mation that people face today , it would be very helpful to have text summarization tools that ; reduce large bodies of text to the salient minimum .", "Simplification can be used to weed out irrelevant ext with greater precision , and thus aid in summarization .", "Clarity of Text Assembly use maintenance manuals must be clear and simple to follow .", "Aircraft companies use a Simplified English for maintenance manuals precisely for this reason Wojcik et M . , 1993 .", "However , it is not easy to create text in such an artifi cially constrained language .", "Automatic or semi automatic simplification could be used to ensure that texts adhere to standards .", "We view simplification as a two stage process .", "The first stage provides a structural representa tion for a sentence on which the second stage ap plies a sequence of rules to identify and extract he components that can be simplified .", "One could use a parser to obtain the complete structure of the sentence .", "If all the constituents of the sentence along with the dependency relations are given , simplification is straightforward , ttowever , full parsing is slow and prone to failure , especially on complex sentences .", "To overcome the limitations of full parsers , researchers have adopted FSG based approaches to parsing Abney , 1994 ; Hobbs et al . , 1992 ; Grishman , 1995 .", "These parsers are fast and reasonably robust ; they produce sequences of noun and verb groups without any hierarchical structure .", "Section 3 discusses an FSG based ap proach to simplification .", "An alternative approach which is both fast and yields hierarchical struc ture is discussed in Section 4 .", "In Section 5 we compare the two approaches , and address some general concerns for the simplification task in Sec tion 6 .", "2 The Basics of Simplification Text simplification uses the f ct that complex texts typically contains complex syntax , some of which may be particular to specific domain of dis course , such as newswire texts .", "We assume that the simplification system will process one sentence at a time .", "Interactions across sentences will not bc considered .", "Wc also assume that sentences have to be maximally simplified .", "2o simplify sentences , we nced to know where we can split them .", "We define articulation points to be those points at which sentences may be log ically split .", "Possible articulation points include the beginnings and ends of phrases , punctuation marks , subordinating and coordinating conjunc tions , and relative pronouns .", "These articulation points are gcneral , and should apply to arbitrary English texts .", "These may , however , be augmented with domain specific articulation points .", "We can use these articulation points to define a set of rules which map froln given sentence patterns to sim pler sentences patterns .", "These rules are repeat edly applied on each sentence until they do not apply any more .", "For example , the sentence 3 with a relative clause can be simplified into two sentences 4 .", "3 Talwinder Singh , who masterminded the Kanishka crash in 198 , was killed in a fierce lwo honr e . connter . . . 4 Talwindcr Singh was killed in a . fierce two hoar cncounler . . . Talwinder Siugh masterminded the Kanishka crash in 198 .", "3 FSG based Simplification Chandrasekar , 1994 discusses an approach that uses a FSG for text simplification as part of a machine aided translation prototype named Vaakya .", "In this approach , we consider sentences to be composed of sequence of word groups , or chunks .", "Chunk boundaries are regarded as poten tial articulation points .", "Chunking allows us to de fine the syntax of a sentence and the structure of simplification rules at a coarser granularity , since we need no longer be concerned with the internal structure of the chunks .", "In this approach , we first tag each word with its part of speech .", "Chunks are then identified nsing a FSG .", "Each chunk is a word group consisting of a verb phrase or a noun phrase , with some attached modifiers .", "The noun phrase recognizer also marks the number singular plural of the phrase .", "The verb phrase recognizer provides some information on tense , voice and aspect .", "Chunks identified by this mechanism include phrases uch as the extent of Tory evolt and have recently bcen finalizcd .", "The chunked sentences are then simplified using a set of ordered simplification rules .", "The orderi g of the rules is decided manually , to take care of more frequent ransformations first , and to avoid unproductive rule interaction .", "An example rule that simplifies sentences with a relative pronoun is shown in 5 .", "5 X tiP , ReXPron Y , Z X tiP Z . X tiP Y .", "The rule is interpreted as follows .", "If a sentence starts with a noun phrase X tiP , and is followed by a phrase with a relative pronoun , of the orm , l elPron Y , followed by soIne Z , where Y and Z are arbitrary sequences of words , then the sentence may be simplified into two sentences , namely the sequence X followed by Z , and X followed by Y .", "The resulting sen ; ences are then recursively simplified , to the extent possible .", "The system has been tested on news text , and performs well on certain classes of sentences .", "See Chandrasekar and R , amani , 1996 ibr details of quantitative valuation of the system , including an evaluation of the acceptability of the resulting 1042 sentences .", "A set of news stories , consist , ing of 224 sentences , was simplitied by the prototype system , resulting in 369 simplified sentences .", "Ilowever , there are certain weMenesses in this system , caused mostly by the relatively simple mechanisms used to detect phrases and attach meats .", "Sentences which include long distance or crossed del enden ies , and sentences which have malt ply stacked appositives are not handled llrOl erly ; nor are sentences with atnbiguous or un ch . ar attachnwnts .", "Some of these prol oms can be handhd I y augmenting the ruh set but what is ieally require I is ntorc syntactic firel ower .", "4 A Dependency based model A second a . I l roaeh to simplification is to use ri her syntactic in brmation , in terms of both con stituency inlbrmation and dependency informa tion .", "We use partial parsing and simple depen .", "dency attachment techniques as an alternative to the FSG I ased simpliiication .", "This no M the I SM is based on a sinq le dependency tel r sentation provided l y I , exicalized Tree .", "Adjoining Ira . tmnar I FAG and uses the SUl ertaggiug l ; echniques described in Josh and Srinivas , 1994 .", "4 . 1 Br ie f Ovt ; rvlt ; w of LTAGs The primitive elements el LTA formalism are . l lnentary trees .", "Elementary trees are of two types initial frees and au , iliary trees .", "Initial ; rees are minimal linguistic structures that con tain no recurs on , such as sitnph ; sentences , N Ps , l Ps etc .", "Auxiliary trees are recursive stru turcs which represent constituents that arc adjuncts to basic structure e . g .", "relative clauses , sentential adjuncts , a Iw . rbials .", "For a more R rmal and le taile I lescription of l , lA s see Schabes et M . , J988 .", "4 . 2 SuI xl ; agging Tlte elemmttary trees of LTAG localize dependen ies , including hmg distance dependencies , by re quiring that all and only the dependent elements be present within the same tree .", "As a result of this localization , a lexical item may be and al most alwws is associated with more than one eL ementary tree , We call these elementary trees su pcrlags , since they conttdn more information such as sul categorization a d agreement information than standard part of speech tags .", "Henc . e , each word is associated with more than one supertag .", "At the end of a complete l arse , each word is asso ciated with just one supertag assuming there is no global ambiguity , and the supertags of all the words in a sentence are combined by sul stitution and adjunct on .", "As in standard part of speech disambiguation , we can use local statistical information in the form of N gram models based on the distribution of sn l ertags ill a LTAG parsed corl us for disamhigua tion .", "use a trigram model to disambiguate ile supcrtags o as to assign one SUl ertag tbr each word , in a process termed supertagging .", "he tri gram model of supcrtagging is very efficient in linear time and robust Josh and Srinivas , 994 .", "1o establish the dependency links among the words of the sentence , we xph it the dei endency information present in the supertags .", "Each su perl ; ag associated with a word allocates lots for the arguments o1 the word .", "These slots have a polarity value re lecting their orientation wii ; h re Sl ect to the anchor o the SUl ertag .", "Also asso iated with a supertag is a list of internal nodes hmluding the root node thai , appear in the su pertag .", "Using I ; his information , a simple algo rithnt may be used to annotate the sentence with d , pe . ndency links .", "4 . 3 Simpl i f i cat ion w i th DeI mden y l inks Tlte output provide by t , he dellendency analyzer not only contains depen hmcy links annmg words but also in lical , cs the constituent strncture as cn code I by snpertags .", "The constituent information is used to identify whether a supertag contains a clausal constituent and the dependency links are used to identify the span of the clause .", "Thus , embedded clauses can easily be identified and ex tracte t , akmg with their arguments .", "nnctuation can be used to identify constituents such as appos itives which can also 1 e sel arate I ont .", "As with the finite state al l roach , the resulting segments may 1 e incomplete as indelltndetlt clauses .", "I the segments are to I e reassembhd , no further pro cessing need be done on them .", "l ? igme 1 shows a rule br extracting relative lauses , in dependency notation .", "We tits iden tify the relative clause tree Z , and then extract the verb which anchors it along with all of its te pendents .", "The right hand side shows the two re suiting trees .", "The gap in the relative clause Y need only be tilled if the clauses are not going to bc reconlbined .", "Examples 6 and 7 show a sen tence belbre and after this rule has applied .", "X S Y NP W Z RelClause yZ S NP Y NP W Figure 1 R , ule for extracting relative clauses 6 .", "an issue that generated unnsnal heat in the IIouse of Commons .", "7 An issne generated unusnal heat in the Ilouse of Commons .", "1043 5 Evaluat ion The objective of the evaluation is to examine the advantages of the DSM over the FSG based model for simplification .", "In the FSG approach since the input to the simplifier is a set of noun and verb groups , the rules for the simplifier have to identify basic predicate argument relations to ensure that the right chunks remain together in the output .", "The simplifier in the DSM has access to infor mation about argument structure , which makes it much easier to specify simplification patterns involving complete constituents .", "Consider exam pie 8 , 8 Th . e creator of Air India , Mr . JRD 7hta , believes that the airline , which celebrated 60 years today , could return to its old days of glory .", "qhe FSG based model fails to recognize the rel ative clause on the embedded subject the airline in example 8 , because Rule 5 looks for matrix subject NPs .", "On the other hand , the DSM cor rectly identifies the relative clause using the rule shown in Figure 1 , which holds for relative clauses in all positions .", "Other differences are in the areas of modifier at tachment and rule generality .", "In contrast o the SM approach , the FSG output does not have all modifiers attached , so the bulk of attachment de cisions must be made by the simplification rules .", "The FSG approach is forced to enumerate all pos sible variants of the LHS of each simplification rule eg .", "Subject versus Object relatives , singular versus plural NPs whereas in the DSM approach , the rules , encoded in supertags and the associated constituent types , are more general .", "Preliminary results using the DSM model are very promising .", "Using a corpus of newswire data , and only considering relative clause and apposi tive simplification , we correctly recovered 25 out of 28 relative clauses and i4 of 14 appositives .", "We generated 1 spurious relative clause and 2 spuri ous appositives .", "A version of the FSG model on the same data recovered 17 relative clauses and 3 appositives .", "6 Discuss ion Simplification can be used for two general lasses of tasks .", "The first is as a preprocessor to a flfll parser so as to reduce ; he parse ambiguity for the parser .", "Tile second class of tasks demands that the output of the simplifier be free standing sen tences .", "Maintaining the coherence of the simpli fied text raises the fbllowing problems ?", "Determining the relative order of the simpli fied sentences , which impacts the choice of referring expressions to be used and the over all coherence of the text .", "Choosing referring expressions For instance , when separating relative clauses fiom the nouns they modify , copying the head noun into the relative clause is simple , but leads to quite awkward sounding texts .", "IIowever , choosing an appropriate pronoun or choosing between definite and indefinite NPs involves knowledge of complex discourse information .", "Selecting the right tense when creating new sentences presents imilar problems .", "No matter how sophisticated the simplifica tion heuristics , the subtleties of meaning in tended by the author may be diluted , if not lost altogether .", "For many computer appli cations , this disadvantage is outweighed by the advantages of simplification i . e .", "gains of speed and or accuracy , or may be corrected with the use of human l ost processiug .", "Acknowledgements This work is partially supported by NSF grant NSF STC SBR 8920230 , ARPA grant N00014 94 and All .", "grant DAAH04 94 G0426 .", "References Steven Abney .", "I epcndency Grammars and Context Free Grammars .", "Manuscript , University of Tubingen , March .", "R . Chandrasekar and S . Ramani .", "Auto matic Simplifica . tion of Natural Language Text .", "M muscript , National Centre for So tware Technol ogy , Bombay .", "R . Chandrasekar .", "A Hybrid Approach to Ma chine Translation using Man Machine Communica tion .", "Ph . D . thesis , Pata Institute of I undamcnta Research University of Bombay , Bombay .", "Ralph Grishman .", "Wheres the Syntax ?", "The New York University M UC 6 System .", "n P occe g ings of the Sixth Message Understanding Confer ence , Columbia , Maryland .", "Jerry Hobbs , Doug Appelt , John Bear , avid Israel , and W . Mary Tyson .", "FAST IS a system , for extracting information from natural anguage text .", "Technical Report 519 , SRI .", "Aravind K . Joshi and B . Srinivas .", "Disam biguation of Super Parts of Speech or Supertags Almost Parsing .", "In Proceedings of the 17 th Inter national Conference on Computational Linguistics COLING 94 , Kyoto , Japan , August .", "Yves Schabes , Anne Abeilld , and Aravind K . Joshi .", "Parsing strategies with lexicMized gram mars Application to Tree Adjoining Grammars .", "In Proceedings of the 12 th International Co rencc on Computational Linguistics COL1NG88 , Bu dapest , Ilungm y , August .", "Wojcik , Philip tIarrison , rod John Bremer .", "Using bracketed parses to evMuate a grain mar checking ; rpplication .", "In Proceedings of the 31 t Conference of Association of Computational Lin guistics , Ohio State University , Columbus , Ohio ."], "summary_lines": ["Motivations And Methods For Text Simplification\n", "Long and complicated sentences prove to be a stumbling block for current systems relying on NL input.\n", "These systems stand to gain from methods that syntactically simplify such sentences.\n", "To simplify a sentence, we need an idea of the structure of the sentence, to identify the components to be separated out.\n", "Obviously a parser could be used to obtain the complete structure of the sentence.\n", "However, full parsing is slow prone to failure, especially on complex sentences.\n", "In this paper, we consider two alternatives to full parsing which could be used for simplification.\n", "The first approach uses a Finite State Grammar (FSG) to produce noun and verb groups while the second uses a Supertagging model to produce dependency linkages.\n", "We discuss the impact of these two input representations on the sim-plification process.\n", "We introduce a two stage process, first transforming from sentence to syntactic tree, then from syntactic tree to new sentence.\n", "Our text simplification techniques deal not only with helping readers with reading disabilities, but also to help NLP systems as a preprocessing tool.\n"]}
{"article_lines": ["Semantic Roles for SMT A Hybrid Two Pass Model", "We present results on a novel hybrid semantic SMT model that incorporates the strengths of both semantic role labeling and phrase based statistical machine translation .", "The approach avoids major complexity limitations via a two pass architecture .", "The first pass is performed using a conventional phrase based SMT model .", "The second pass is performed by a re ordering strategy guided by shallow semantic parsers that produce both semantic frame and role labels .", "Evaluation on a Wall Street Journal newswire genre test set showed the hybrid model to yield an improvement of roughly half a point in BLEU score over a strong pure phrase based SMT baseline to our knowledge , the first successful application of semantic role labeling to SMT .", "Many of the most glaring errors made by today s statistical machine translation systems are those resulting from confusion of semantic roles .", "Translation errors of this type frequently result in critical misunderstandings of the essential meaning of the original input language sentences who did what to whom , for whom or what , how , where , when , and why .", "Semantic role confusions are errors of adequacy rather than fluency .", "It has often been noted that the dominance of lexically oriented , precisionbased metrics such as BLEU Papineni et al . 2002 tend to reward fluency more than adequacy .", "The length penalty in the BLEU metric , in particular , is only an indirect and weak indicator of adequacy .", "As a result , SMT work has been driven to optimize systems such that they often produce translations that contain significant role confusion errors despite reading fluently .", "The present work is inspired by the question of whether we can improve translation utility via a strategy of favoring semantic adequacy slightly more possibly at the expense of slight degradations in lexical fluency .", "Shallow semantic parsing models have attained increasing levels of accuracy in recent years Gildea and Jurafsky 2000 ; Sun and Jurafsky 2004 ; Pradhan et al . 2004 , 2005 ; Pradhan 2005 ; Fung et al .", "2006 , 2007 ; Gim6nez and M\u02c6rquez 2007a , 2008 .", "Such models , which identify semantic frames within input sentences by marking its predicates , and labeling their arguments with the semantic roles that they fill .", "Evidence has begun to accumulate that semantic frames predicates and semantic roles tend to preserve consistency across translations better than syntactic roles do .", "This is , of course , by design ; it follows from the definition of semantic roles , which are less language dependent than syntactic roles .", "Across Chinese and English , for example , it has been reported that approximately 84 of semantic roles are preserved consistently Fung et al . 2006 .", "Of these , roughly 15 do not preserve syntactic roles consistently .", "Since this directly targets the task of determining semantic correctness , we believe that the adequacy of MT output could be improved by leveraging the predictions of semantic parsers .", "We would like to exploit automatic semantic parsers to identify inconsistent semantic frame and role mappings between the input source sentences and their output translations .", "However , we take note of the difficult experience in making syntactic and semantic models conBoulder , Colorado , June 2009 . c 2009 Association for Computational Linguistics tribute to improving SMT accuracy .", "On the one hand , there is reason to be optimistic .", "Over the past decade , we have seen an accumulation of evidence that SMT accuracy can be improved via tree structured and syntactic models e . g . , Wu 1997 ; Wu and Chiang 2009 , and more recently , work from lexical semantics has also at long last been successfully applied to increasing SMT accuracy , in the form of techniques adapted from word sense disambiguation models Chan et al . 2007 ; Gimenez and M\u02c6rquez 2007b ; Carpuat and Wu 2007 .", "On the other hand , both directions saw unexpected disappointments along the way e . g . , Och et al . 2003 ; Carpuat and Wu 2005 .", "We are therefore forewarned that it is likely to be at least as difficult to successfully adapt the even more complex types of lexical semantics modeling from semantic parsing and role labeling to the translation task .", "In this paper , we present a novel hybrid model that , for the first time to our knowledge , successfully applies semantic parsing technology to the challenge of improving the quality of ChineseEnglish statistical machine translation .", "The model makes use of a typical representative SMT system based on Moses , plus shallow semantic parsers for both English and Chinese .", "While the accuracy of shallow semantic parsers has been approaching reasonably high levels in recent years for well studied languages like English , and to a lesser extent , Chinese , the problem of excessive computational complexity is one of the primary challenges in adapting semantic parsing technology to the translation task .", "Semantic parses , by definition , are less likely than syntactic parses to obey clearly nested hierarchical composition rules .", "Moreover , the semantic parses are less likely to share an exactly isomorphic structure across the input and output languages , since the raison d \u00eatre of semantic parsing is to capture semantic frame and role regularities independent of syntactic variation monolingually and cross lingually .", "This makes it difficult to incorporate semantic parsing into SMT merely by applying the sort of dynamic programming techniques found in current syntactic and tree structured SMT models , most of which rely on being able to factor the computation into independent computations on the subtrees .", "In other words , the key computational obstacle is that the semantic parse of a larger string or string pair , in the case of translation is not in general strictly mechanically composable from the semantic parses of its smaller substrings or substring pairs .", "In fact , the lack of easy compositionality is the reason that today s most accurate shallow semantic parsers rely not primarily on compositional parsing techniques , but rather on ensembles of predictors that independently rate rank a wide variety of factors supporting the role assignments given a broad sentence wide range of context features .", "But while this improves semantic parsing accuracy , it poses a major obstacle for efficient tight integration into the sub hypothesis construction and maintenance loops within SMT decoders .", "To circumvent this computational obstacle , the hybrid two pass model defers application of the non compositional semantic parsing information until a second error correcting pass .", "This imposes a division of labor between the two passes .", "The first pass is performed using a conventional phrase based SMT model .", "The phrase based SMT model is assigned to the tasks of a providing an initial baseline hypothesis translation , and b fixing the lexical choice decisions .", "Note that the lexical choice decisions are not only at the single word level , but are in general at the phrasal level .", "The second pass takes the output of the first pass , and re orders constituent phrases corresponding to semantic predicates and arguments , seeking to maximize the cross lingual match of the semantic parse of the re ordered translation to that of the original input sentence .", "The second pass algorithm performs the error correction shown in Figure 1 .", "The design decision to allow the first pass to fix all lexical choices follows an insight inspired by an empirical observation from our error analyses the lexical choice decisions being made by today s SMT models have attained fairly reasonable levels , and are not where the major problems of adequacy lie .", "Rather , the ordering of arguments in relation to their predicates is often where the main failures of adequacy occur .", "By avoiding lexical choice variations while considering re ordering hypotheses , a significantly larger amount of re ordering can be done without further increasing computational complexity .", "So we sacrifice a small amount of fluency by allowing re ordering without compensating lexical choice in exchange for gaining potentially a larger amount of fluency by getting the predicate argument structure right .", "The model has a similar rationale for employing a re ordering pass instead of re ranking n best lists or lattices .", "Oracle analysis of n best lists and lattices show that they often focus on lexical choice alternatives rather than re ordering role variations which are more important to semantic adequacy .", "A Chinese English experiment was conducted on the two pass hybrid model .", "A phrase based SMT baseline model was built by augmenting the open source statistical machine translation decoder Moses Koehn et al . 2007 with additional preprocessors .", "English and Chinese shallow semantic parsers followed those discussed in Section 1 .", "The model was trained on LDC newswire parallel text consisting of 3 . 42 million sentence pairs , containing 64 . 1 million English words and 56 . 9 million Chinese words .", "The English was tokenized and case normalized ; the Chinese was tokenized via a maximum entropy model Fung et al . 2004 .", "Phrase translations were extracted via the growdiag final heuristic .", "The language model is a 6 gram model trained with Kneser Ney smoothing using the SRI language modeling toolkit Stolcke 2002 .", "The test set of Wall Street Journal newswire sentences was randomly extracted from the Chinese English Bilingual Propbank .", "Although we did not make use of the Propbank annotations , this would potentially allow other types of analyses in the future .", "The phrase based SMT model used for the first pass achieves a BLEU score of 42 . 99 , establishing a fairly strong baseline to begin with .", "In comparison , the automatically errorcorrected translations that are output by the second pass achieve a BLEU score of 43 . 51 .", "This represents approximately half a point improvement over the strong baseline .", "An example is seen in Figure 2 .", "The SMT first pass translation has an ARG0 National Development Bank of Japan in the capital market which is badly mismatched to both the input sentence s \u8d44 r J \u573a .", "The second pass ends up re ordering the constituent phrase corresponding to the mismatched ARGM LOC , of Japan in the capital market , to follow the PRED issued , where the new English semantic parse now assigns most of its words the correctly matched ARGM LOC semantic role label .", "Similarly , samurai bonds 30 billion yen is re ordered to 30 billion yen samurai bonds .", "To our knowledge , this is a first result demonstrating that shallow semantic parsing can improve translation accuracy of SMT models .", "We note that accuracy here was measured via BLEU , and it has been widely observed that the negative impacts of semantic predicate argument errors on the utility of the translation are underestimated by evaluation metrics based on lexical criteria such as BLEU .", "We conjecture that more expensive manual evaluation techniques which directly measure translation utility could even more strongly reveal improvement in role confusion errors .", "The hybrid two pass approach can be compared with the greedy re ordering based strategy of the ReWrite decoder Germann et al . 2001 , although our search is breadth first rather than purely greedy .", "Whereas ReWrite was based on wordlevel re ordering , however , our approach is based on constituent phrase re ordering , and the phrases to be re ordered are more selectively chosen via the semantic parse labels .", "Moreover , the objective function being maximized by ReWrite is still the SMT model score ; whereas in our case the new objective function is cross lingual semantic predicate argument match plus an implicit search bias toward fewer re orderings .", "The hybrid two pass approach can also be compared with serial combination architectures for hybrid MT e . g . , Ueffing et al . 2008 .", "But whereas Ueffing et al . take the output from a first pass rulebased MT system , and then correct it using a second pass SMT system , our two pass semantic SMT model does the reverse it takes the output from a first pass SMT system , and then corrects it with the aid of semantic analyzers ."], "summary_lines": ["Semantic Roles for SMT: A Hybrid Two-Pass Model\n", "We present results on a novel hybrid semantic SMT model that incorporates the strengths of both semantic role labeling and phrase-based statistical machine translation.\n", "The approach avoids major complexity limitations via a two-pass architecture.\n", "The first pass is performed using a conventional phrase-based SMT model.\n", "The second pass is performed by a re-ordering strategy guided by shallow semantic parsers that produce both semantic frame and role labels.\n", "Evaluation on a Wall Street Journal newswire genre test set showed the hybrid model to yield an improvement of roughly half a point in BLEU score over a strong pure phrase-based SMT baseline \u2013 to our knowledge, the first successful application of semantic role labeling to SMT.\n", "We perform semantic role labeling on translation output and reordered arguments to maximize the cross-lingual match of the semantic frames between the source sentence and the target translation.\n"]}
{"article_lines": ["Semi Supervised Recursive Autoencoders for Predicting Sentiment Distributions", "We introduce a novel machine learning framework based on recursive autoencoders for sentence level prediction of sentiment label distributions .", "Our method learns vector space representations for multi word phrases .", "In sentiment prediction tasks these representations outperform other state of the art approaches on commonly used datasets , such as movie reviews , without using any pre defined sentiment lexica or polarity shifting rules .", "We also evaluate the model s ability to predict sentiment distributions on a new dataset based on confessions from the experience project .", "The dataset consists of personal user stories annotated with multiple labels which , when aggregated , form a multinomial distribution that captures emotional reactions .", "Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines .", "The ability to identify sentiments about personal experiences , products , movies etc . is crucial to understand user generated content in social networks , blogs or product reviews .", "Detecting sentiment in these data is a challenging task which has recently spawned a lot of interest Pang and Lee , 2008 .", "Current baseline methods often use bag of words representations which cannot properly capture more complex linguistic phenomena in sentiment analysis Pang et al . , 2002 .", "For instance , while the two phrases white blood cells destroying an infection and an infection destroying white blood cells have the same bag of words representation , the former is a positive reaction while the later is very negative .", "More advanced methods such as Nakagawa et al . , tecture which learns semantic vector representations of phrases .", "Word indices orange are first mapped into a semantic vector space blue .", "Then they are recursively merged by the same autoencoder network into a fixed length sentence representation .", "The vectors at each node are used as features to predict a distribution over sentiment labels .", "2010 that can capture such phenomena use many manually constructed resources sentiment lexica , parsers , polarity shifting rules .", "This limits the applicability of these methods to a broader range of tasks and languages .", "Lastly , almost all previous work is based on single , positive negative categories or scales such as star ratings .", "Examples are movie reviews Pang and Lee , 2005 , opinions Wiebe et al . , 2005 , customer reviews Ding et al . , 2008 or multiple aspects of restaurants Snyder and Barzilay , 2007 .", "Such a one dimensional scale does not accurately reflect the complexity of human emotions and sentiments .", "In this work , we seek to address three issues .", "i Instead of using a bag of words representation , our model exploits hierarchical structure and uses compositional semantics to understand sentiment .", "ii Our system can be trained both on unlabeled domain data and on supervised sentiment data and does not require any language specific sentiment lexica , Sorry , Hugs You Rock Teehee I Understand Wow , Just Wow i walked into a parked car parsers , etc .", "iii Rather than limiting sentiment to a positive negative scale , we predict a multidimensional distribution over several complex , interconnected sentiments .", "We introduce an approach based on semisupervised , recursive autoencoders RAE which use as input continuous word vectors .", "Fig .", "1 shows an illustration of the model which learns vector representations of phrases and full sentences as well as their hierarchical structure from unsupervised text .", "We extend our model to also learn a distribution over sentiment labels at each node of the hierarchy .", "We evaluate our approach on several standard datasets where we achieve state of the art performance .", "Furthermore , we show results on the recently introduced experience project EP dataset Potts , 2010 that captures a broader spectrum of human sentiments and emotions .", "The dataset consists of very personal confessions anonymously made by people on the experience project website www . experienceproject . com .", "Confessions are labeled with a set of five reactions by other users .", "Reaction labels are you rock expressing approvement , tehee amusement , I understand , Sorry , hugs and Wow , just wow displaying shock .", "For evaluation on this dataset we predict both the label with the most votes as well as the full distribution over the sentiment categories .", "On both tasks our model outperforms competitive baselines .", "A set of over 31 , 000 confessions as well as the code of our model are available at www . socher . org .", "After describing the model in detail , we evaluate it qualitatively by analyzing the learned n gram vector representations and compare quantitatively against other methods on standard datasets and the EP dataset .", "Our model aims to find vector representations for variable sized phrases in either unsupervised or semi supervised training regimes .", "These representations can then be used for subsequent tasks .", "We first describe neural word representations and then proceed to review a related recursive model based on autoencoders , introduce our recursive autoencoder RAE and describe how it can be modified to jointly learn phrase representations , phrase structure and sentiment distributions .", "We represent words as continuous vectors of parameters .", "We explore two settings .", "In the first setting we simply initialize each word vector x E Rn by sampling it from a zero mean Gaussian distribution x N 0 , U2 .", "These word vectors are then stacked into a word embedding matrix L E Rn V , where V is the size of the vocabulary .", "This initialization works well in supervised settings where a network can subsequently modify these vectors to capture certain label distributions .", "In the second setting , we pre train the word vectors with an unsupervised neural language model Bengio et al . , 2003 ; Collobert and Weston , 2008 .", "These models jointly learn an embedding of words into a vector space and use these vectors to predict how likely a word occurs given its context .", "After learning via gradient ascent the word vectors capture syntactic and semantic information from their co occurrence statistics .", "In both cases we can use the resulting matrix of word vectors L for subsequent tasks as follows .", "Assume we are given a sentence as an ordered list of m words .", "Each word has an associated vocabulary index k into the embedding matrix which we use to retrieve the word s vector representation .", "Mathematically , this look up operation can be seen as a simple projection layer where we use a binary vector b which is zero in all positions except at the kth index , In the remainder of this paper , we represent a sentence or any n gram as an ordered list of these vectors x1 , . . . , xm .", "This word representation is better suited to autoencoders than the binary number representations used in previous related autoencoder models such as the recursive autoassociative memory RAAM model Pollack , 1990 ; Voegtlin and Dominey , 2005 or recurrent neural networks Elman , 1991 since sigmoid units are inherently continuous .", "Pollack circumvented this problem by having vocabularies with only a handful of words and by manually defining a threshold to binarize the resulting vectors .", "The goal of autoencoders is to learn a representation of their inputs .", "In this section we describe how to obtain a reduced dimensional vector representation for sentences .", "In the past autoencoders have only been used in setting where the tree structure was given a priori .", "We review this setting before continuing with our model which does not require a given tree structure .", "Fig .", "2 shows an instance of a recursive autoencoder RAE applied to a given tree .", "Assume we are given a list of word vectors x x1 , . . . , xm as described in the previous section as well as a binary tree structure for this input in the form of branching triplets of parents with children p c1c2 .", "Each child can be either an input word vector xi or a nonterminal node in the tree .", "For the example in Fig .", "2 , we have the following triplets y1 x3x4 , y2 x2y1 , y1 x1y2 .", "In order to be able to apply the same neural network to each pair of children , the hidden representations yi have to have the same dimensionality as the xi s .", "Given this tree structure , we can now compute the parent representations .", "The first parent vector y1 is computed from the children c1 , c2 x3 , x4 where we multiplied a matrix of parameters W 1 E Rnx2n by the concatenation of the two children .", "After adding a bias term we applied an elementwise activation function such as tanh to the resulting vector .", "One way of assessing how well this ndimensional vector represents its children is to try to reconstruct the children in a reconstruction layer During training , the goal is to minimize the reconstruction errors of this input pair .", "For each pair , we compute the Euclidean distance between the original input and its reconstruction This model of a standard autoencoder is boxed in Fig .", "Now that we have defined how an autoencoder can be used to compute an n dimensional vector representation p of two n dimensional children c1 , c2 , we can describe how such a network can be used for the rest of the tree .", "Essentially , the same steps repeat .", "Now that y1 is given , we can use Eq .", "2 to compute y2 by setting the children to be c1 , c2 x2 , y1 .", "Again , after computing the intermediate parent vector y2 , we can assess how well this vector capture the content of the children by computing the reconstruction error as in Eq .", "The process repeat until the full tree is constructed and we have a reconstruction error at each nonterminal node .", "This model is similar to the RAAM model Pollack , 1990 which also requires a fixed tree structure .", "Now , assume there is no tree structure given for the input vectors in x .", "The goal of our structureprediction RAE is to minimize the reconstruction error of all vector pairs of children in a tree .", "We define A x as the set of all possible trees that can be built from an input sentence x .", "Further , let T y be a function that returns the triplets of a tree indexed by s of all the non terminal nodes in a tree .", "Using the reconstruction error of Eq .", "4 , we compute We now describe a greedy approximation that constructs such a tree .", "Greedy Unsupervised RAE .", "For a sentence with m words , we apply the autoencoder recursively .", "It takes the first pair of neighboring vectors , defines them as potential children of a phrase c1 ; c2 x1 ; x2 , concatenates them and gives them as input to the autoencoder .", "For each word pair , we save the potential parent node p and the resulting reconstruction error .", "After computing the score for the first pair , the network is shifted by one position and takes as input vectors c1 , c2 x2 , x3 and again computes a potential parent node and a score .", "This process repeats until it hits the last pair of words in the sentence c1 , c2 xm 1 , xm .", "Next , it selects the pair which had the lowest reconstruction error ETec and its parent representation p will represent this phrase and replace both children in the sentence word list .", "For instance , consider the sequence x1 , x2 , x3 , x4 and assume the lowest ETec was obtained by the pair x3 , x4 .", "After the first pass , the new sequence then consists of x1 , x2 , p 3 , 4 .", "The process repeats and treats the new vector p 3 , 4 like any other input vector .", "For instance , subsequent states could be either x1 , p 2 , 3 , 4 or p 1 , 2 , p 3 , 4 .", "Both states would then finish with a deterministic choice of collapsing the remaining two states into one parent to obtain p 1 , 2 , 3 , 4 or p 1 , 2 , 3 , 4 respectively .", "The tree is then recovered by unfolding the collapsing decisions .", "The resulting tree structure captures as much of the single word information as possible in order to allow reconstructing the word vectors but does not necessarily follow standard syntactic constraints .", "We also experimented with a method that finds better solutions to Eq .", "5 based on CKY like beam search algorithms Socher et al . , 2010 ; Socher et al . , 2011 but the performance is similar and the greedy version is much faster .", "Weighted Reconstruction .", "One problem with simply using the reconstruction error of both children equally as describe in Eq .", "4 is that each child could represent a different number of previously collapsed words and is hence of bigger importance for the overall meaning reconstruction of the sentence .", "For instance in the case of x1 , p 2 , 3 , 4 one would like to give more importance to reconstructing p than x1 .", "We capture this desideratum by adjusting the reconstruction error .", "Let n1 , n2 be the number of words underneath a current potential child , we re define the reconstruction error to be Length Normalization .", "One of the goals of RAEs is to induce semantic vector representations that allow us to compare n grams of different lengths .", "The RAE tries to lower reconstruction error of not only the bigrams but also of nodes higher in the tree .", "Unfortunately , since the RAE computes the hidden representations it then tries to reconstruct , it can just lower reconstruction error by making the hidden layer very small in magnitude .", "To prevent such undesirable behavior , we modify the hidden layer such that the resulting parent representation always has length one , after computing p as in Eq .", "2 , we simply set p p p .", "So far , the RAE was completely unsupervised and induced general representations that capture the semantics of multi word phrases . In this section , we extend RAEs to a semi supervised setting in order to predict a sentence or phrase level target distribution t . 1 One of the main advantages of the RAE is that each node of the tree built by the RAE has associated with it a distributed vector representation the parent vector p which could also be seen as features describing that phrase .", "We can leverage this representation by adding on top of each parent node a simple softmax layer to predict class distributions Assuming there are K labels , d E RK is a K dimensional multinomial distribution and P k 1 dk 1 .", "Fig .", "3 shows such a semi supervised RAE unit .", "Let tk be the kth element of the multinomial target label distribution t for one entry .", "The softmax layer s outputs are interpreted as conditional probabilities dk p kJ c1 ; c2 , hence the cross entropy error is 1For the binary label classification case , the distribution is of the form 1 , 0 for class 1 and 0 , 1 for class 2 .", "Using this cross entropy error for the label and the reconstruction error from Eq .", "6 , the final semisupervised RAE objective over sentences , label pairs x , t in a corpus becomes where we have an error for each entry in the training set that is the sum over the error at the nodes of the tree that is constructed by the greedy RAE Let \u03b8 W 1 , b 1 , W 2 , b 1 , Wlabel , L be the set of our model parameters , then the gradient becomes To compute this gradient , we first greedily construct all trees and then derivatives for these trees are computed efficiently via backpropagation through structure Goller and K uchler , 1996 .", "Because the algorithm is greedy and the derivatives of the supervised cross entropy error also modify the matrix W 1 , this objective is not necessarily continuous and a step in the gradient descent direction may not necessarily decrease the objective .", "However , we found that L BFGS run over the complete training data batch mode to minimize the objective works well in practice , and that convergence is smooth , with the algorithm typically finding a good solution quickly .", "The error at each nonterminal node is the weighted sum of reconstruction and cross entropy errors , The hyperparameter \u03b1 weighs reconstruction and cross entropy error .", "When minimizing the crossentropy error of this softmax layer , the error will backpropagate and influence both the RAE parameters and the word representations .", "Initially , words such as good and bad have very similar representations .", "This is also the case for Brown clusters and other methods that use only cooccurrence statistics in a small window around each word .", "When learning with positive negative sentiment , the word embeddings get modified and capture less syntactic and more sentiment information .", "In order to predict the sentiment distribution of a sentence with this model , we use the learned vector representation of the top tree node and train a simple logistic regression classifier .", "We first describe the new experience project EP dataset , results of standard classification tasks on this dataset and how to predict its sentiment label distributions .", "We then show results on other commonly used datasets and conclude with an analysis of the important parameters of the model .", "In all experiments involving our model , we represent words using 100 dimensional word vectors .", "We explore the two settings mentioned in Sec .", "We compare performance on standard datasets when using randomly initialized word vectors random word init .", "or word vectors trained by the model of Collobert and Weston 2008 and provided by Turian et al . 2010 . 2 These vectors were trained on an unlabeled corpus of the English Wikipedia .", "Note that alternatives such as Brown clusters are not suitable since they do not capture sentiment information good and bad are usually in the same cluster and cannot be modified via backpropagation .", "The confessions section of the experience project website3 lets people anonymously write short personal stories or confessions .", "Once a story is on the site , each user can give a single vote to one of five label categories with our interpretation The EP dataset has 31 , 676 confession entries , a total number of 74 , 859 votes for the 5 labels above , the average number of votes per entry is 2 . 4 with a variance of 33 .", "For the five categories , the numbers of votes are 14 , 816 ; 13 , 325 ; 10 , 073 ; 30 , 844 ; 5 , 801 .", "Since an entry with less than 4 votes is not very well identified , we train and test only on entries with at least 4 total votes .", "There are 6 , 129 total such entries .", "The distribution over total votes in the 5 classes is similar 0 . 22 ; 0 . 2 ; 0 . 11 ; 0 . 37 ; 0 . 1 .", "The average length of entries is 129 words .", "Some entries contain multiple sentences .", "In these cases , we average the predicted label distributions from the sentences .", "Table 1 shows statistics of this and other commonly used sentiment datasets which we compare on in later experiments .", "Table 2 shows example entries as well as gold and predicted label distributions as described in the next sections .", "Compared to other datasets , the EP dataset contains a wider range of human emotions that goes far beyond positive negative product or movie reviews .", "Each item is labeled with a multinomial distribution over interconnected response categories .", "This is in contrast to most other datasets including multiaspect rating where several distinct aspects are rated independently but on the same scale .", "The topics range from generic happy statements , daily clumsiness reports , love , loneliness , to relationship abuse and suicidal notes .", "As is evident from the total number of label votes , the most common user reaction is one of empathy and an ability to relate to the authors experience .", "However , some stories describe horrible scenarios that are not common and hence receive more offers of condolence .", "In the following sections we show some examples of stories with predicted and true distributions but refrain from listing the most horrible experiences .", "For all experiments on the EP dataset , we split the data into train 49 , development 21 and test data 30 .", "The first task for our evaluation on the EP dataset is to simply predict the single class that receives the most votes .", "In order to compare our novel joint phrase representation and classifier learning framework to traditional methods , we use the following baselines Random Since there are five classes , this gives 20 accuracy .", "Most Frequent Selecting the class which most frequently has the most votes the class I understand .", "Baseline 1 Binary BoW This baseline uses logistic regression on binary bag of word representations that are 1 if a word is present and 0 otherwise .", "Baseline 2 Features This model is similar to traditional approaches to sentiment classification in that it uses many hand engineered resources .", "We first used a spell checker and Wordnet to map words and their misspellings to synsets to reduce the total number of words .", "We then replaced sentiment words with a sentiment category identifier using the sentiment lexica of the Harvard Inquirer Stone , 1966 and LIWC Pennebaker et al . , 2007 .", "Lastly , we used tf idf weighting on the bag of word representations and trained an SVM .", "KL Predicted Gold V . Entry Shortened if it ends with . . . . 03 . 16 . 16 . 16 . 33 . 16 6 I reguarly shoplift .", "I got caught once and went to jail , but I ve found that this was not a deterrent .", "I don t buy groceries , I don t buy school supplies for my kids , I don t buy gifts for my kids , we don t pay for movies , and I dont buy most incidentals for the house cleaning supplies , toothpaste , etc . . . .", ". 03 . 38 . 04 . 06 . 35 . 14 165 i am a very succesfull buissnes man . i make good money but i have been addicted to crack for 13 years . i moved 1 hour away from my dealers 10 years ago to stop using now i dont use daily but once a week usally friday nights . i used to use 1 or 2 hundred a day now i use 4 or 5 hundred on a friday . my problem is i am a funcational addict . . . . 05 . 14 . 28 . 14 . 28 . 14 7 Hi there , Im a guy that loves a girl , the same old bloody story . . .", "I met her a while ago , while studying , she Is so perfect , so mature and yet so lonely , I get to know her and she get ahold of me , by opening her life to me and so did I with her , she has been the first person , male or female that has ever made that bond with me , . . . . 07 . 27 . 18 . 00 . 45 . 09 11 be kissing you right now . i should be wrapped in your arms in the dark , but instead i ve ruined everything . i ve piled bricks to make a wall where there never should have been one . i feel an ache that i shouldn t feel because i ve never had you close enough . we ve never touched , but i still feel as though a part of me is missing . . . . . 05 23 Dear Love , I just want to say that I am looking for you .", "Tonight I felt the urge to write , and I am becoming more and more frustrated that I have not found you yet .", "I m also tired of spending so much heart on an old dream . . . . . 05 5 I wish I knew somone to talk to here .", ". 06 24 I loved her but I screwed it up .", "Now she s moved on .", "I ll never have her again .", "I don t know if I ll ever stop thinking about her .", ". 06 5 i am 13 years old and i hate my father he is alwas geting drunk and do s not care about how it affects me or my sisters i want to care but the truthis i dont care if he dies . 13 6 well i think hairy women are attractive . 35 5 As soon as I put clothings on I will go down to DQ and get a thin mint blizzard .", "I need it .", "It ll make my soul feel a bit better . 36 6 I am a 45 year old divoced woman , and I havent been on a date or had any significant relationship in 12 years . . . yes , 12 yrs . the sad thing is , Im not some dried up old granny who is no longer interested in men , I just can t meet men .", "before you judge , no Im not terribly picky !", "What is wrong with me ?", ". 63 6 When i was in kindergarden i used to lock myself in the closet and eat all the candy .", "Then the teacher found out it was one of us and made us go two days without freetime .", "It might be a little late now , but sorry guys it was me haha . 92 4 My paper is due in less than 24 hours and I m still dancing round my room !", "Baseline 3 Word Vectors We can ignore the RAE tree structure and only train softmax layers directly on the pre trained words in order to influence the word vectors .", "This is followed by an SVM trained on the average of the word vectors .", "We also experimented with latent Dirichlet allocation Blei et al . , 2003 but performance was very low .", "Table 3 shows the results for predicting the class with the most votes .", "Even the approach that is based on sentiment lexica and other resources is outperformed by our model by almost 3 , showing that for tasks involving complex broad range human sentiment , the often used sentiment lexica lack in coverage and traditional bag of words representations are not powerful enough .", "We now turn to evaluating our distributionprediction approach .", "In both this and the previous maximum label task , we backprop using the gold multinomial distribution as a target .", "Since we maximize likelihood and because we want to predict a distribution that is closest to the distribution of labels that people would assign to a story , we evaluate using KL divergence KL g p Ei gi log gi pi , where g is the gold distribution and p is the predicted one .", "We report the average KL divergence , where a smaller value indicates better predictive power .", "To get an idea of the values of KL divergence , predicting random distributions gives a an average of 1 . 2 in KL divergence , predicting simply the average distribution in the training data give 0 . 83 .", "Fig .", "4 shows that our RAE based model outperforms the other baselines .", "Table 2 shows EP example entries with predicted and gold distributions , as well as numbers of votes .", "In order to compare our approach to other methods we also show results on commonly used sentiment datasets movie reviews4 MR Pang and Lee , 2005 and opinions5 MPQA Wiebe et al . , 2005 . We give statistical information on these and the EP corpus in Table 1 .", "We compare to the state of the art system of Nakagawa et al . , 2010 , a dependency tree based classification method that uses CRFs with hidden variables .", "We use the same training and testing regimen 10 fold cross validation as well as their baselines majority phrase voting using sentiment and reversal lexica ; rule based reversal using a dependency tree ; Bag of Features and their full Tree CRF model .", "As shown in Table 4 , our algorithm outperforms their approach on both datasets .", "For the movie review MR data set , we do not use any handdesigned lexica .", "An error analysis on the MPQA dataset showed several cases of single words which never occurred in the training set .", "Correctly classifying these instances can only be the result of having them in the original sentiment lexicon .", "Hence , for the experiment on MPQA we added the same sentiment lexicon that Nakagawa et al . , 2010 used in their system to our training set .", "This improved accuracy from 86 . 0 to 86 . 4 . Using the pre trained word vectors boosts performance by less than 1 compared to randomly initialized word vectors setting random word init .", "This shows that our method can work well even in settings with little training data .", "We visualize the semantic vectors that the recursive autoencoder learns by listing n grams that give the highest probability for each polarity .", "Table 5 shows such n grams for different lengths when the RAE is trained on the movie review polarity dataset .", "On a 4 core machine , training time for the smaller corpora such as the movie reviews takes around 3 hours and for the larger EP corpus around 12 hours until convergence .", "Testing of hundreds of movie reviews takes only a few seconds .", "In this experiment , we show how the hyperparameter \u03b1 influences accuracy on the development set of one of the cross validation splits of the MR dataset .", "This parameter essentially trade off the supervised and unsupervised parts of the objective .", "Fig .", "5 shows that a larger focus on the supervised objective is important but that a weight of \u03b1 0 . 2 for the reconstruction error prevents overfitting and achieves the highest performance .", "Autoencoders are neural networks that learn a reduced dimensional representation of fixed size inputs such as image patches or bag of word representations of text documents .", "They can be used to efficiently learn feature encodings which are useful for classification .", "Recently , Mirowski et al . 2010 learn dynamic autoencoders for documents in a bagof words format which , like ours , combine supervised and reconstruction objectives .", "The idea of applying an autoencoder in a recursive setting was introduced by Pollack 1990 .", "Pollack s recursive auto associative memories RAAMs are similar to ours in that they are a connectionst , feedforward model .", "However , RAAMs learn vector representations only for fixed recursive data structures , whereas our RAE builds this recursive data structure .", "More recently , Voegtlin and Dominey , 2005 introduced a linear modification to RAAMs that is able to better generalize to novel combinations of previously seen constituents .", "One of the major shortcomings of previous applications of recursive autoencoders to natural language sentences was their binary word representation as discussed in Sec .", "Recently , Socher et al . , 2010 ; Socher et al . , 2011 introduced a max margin framework based on recursive neural networks RNNs for labeled structure prediction .", "Their models are applicable to natural language and computer vision tasks such as parsing or object detection .", "The current work is related in that it uses a recursive deep learning model .", "However , RNNs require labeled tree structures and use a supervised score at each node .", "Instead , RAEs learn hierarchical structures that are trying to capture as much of the the original word vectors as possible .", "The learned structures are not necessarily syntactically plausible but can capture more of the semantic content of the word vectors .", "Other recent deep learning methods for sentiment analysis include Maas et al . , 2011 .", "Pang et al . 2002 were one of the first to experiment with sentiment classification .", "They show that simple bag of words approaches based on Naive Bayes , MaxEnt models or SVMs are often insufficient for predicting sentiment of documents even though they work well for general topic based document classification .", "Even adding specific negation words , bigrams or part of speech information to these models did not add significant improvements .", "Other document level sentiment work includes Turney , 2002 ; Dave et al . , 2003 ; Beineke et al . , 2004 ; Pang and Lee , 2004 .", "For further references , see Pang and Lee , 2008 .", "Instead of document level sentiment classification , Wilson et al . , 2005 analyze the contextual polarity of phrases and incorporate many well designed features including dependency trees .", "They also show improvements by first distinguishing between neutral and polar sentences .", "Our model naturally incorporates the recursive interaction between context and polarity words in sentences in a unified framework while simultaneously learning the necessary features to make accurate predictions .", "Other approaches for sentence level sentiment detection include Yu and Hatzivassiloglou , 2003 ; Grefenstette et al . , 2004 ; Ikeda et al . , 2008 .", "Most previous work is centered around a given sentiment lexicon or building one via heuristics Kim and Hovy , 2007 ; Esuli and Sebastiani , 2007 , manual annotation Das and Chen , 2001 or machine learning techniques Turney , 2002 .", "In contrast , we do not require an initial or constructed sentiment lexicon of positive and negative words .", "In fact , when training our approach on documents or sentences , it jointly learns such lexica for both single words and n grams see Table 5 .", "Mao and Lebanon , 2007 propose isotonic conditional random fields and differentiate between local , sentence level and global , document level sentiment .", "The work of Polanyi and Zaenen , 2006 ; Choi and Cardie , 2008 focuses on manually constructing several lexica and rules for both polar words and related content word negators , such as prevent cancer , where prevent reverses the negative polarity of cancer .", "Like our approach they capture compositional semantics .", "However , our model does so without manually constructing any rules or lexica .", "Recently , Velikovich et al . , 2010 showed how to use a seed lexicon and a graph propagation framework to learn a larger sentiment lexicon that also includes polar multi word phrases such as once in a life time .", "While our method can also learn multiword phrases it does not require a seed set or a large web graph .", "Nakagawa et al . , 2010 introduced an approach based on CRFs with hidden variables with very good performance .", "We compare to their stateof the art system .", "We outperform them on the standard corpora that we tested on without requiring external systems such as POS taggers , dependency parsers and sentiment lexica .", "Our approach jointly learns the necessary features and tree structure .", "In multi aspect rating Snyder and Barzilay , 2007 one finds several distinct aspects such as food or service in a restaurant and then rates them on a fixed linear scale such as 1 5 stars , where all aspects could obtain just 1 star or all aspects could obtain 5 stars independently .", "In contrast , in our method a single aspect a complex reaction to a human experience is predicted not in terms of a fixed scale but in terms of a multinomial distribution over several interconnected , sometimes mutually exclusive emotions .", "A single story cannot simultaneously obtain a strong reaction in different emotional responses by virtue of having to sum to one .", "We presented a novel algorithm that can accurately predict sentence level sentiment distributions .", "Without using any hand engineered resources such as sentiment lexica , parsers or sentiment shifting rules , our model achieves state of the art performance on commonly used sentiment datasets .", "Furthermore , we introduce a new dataset that contains distributions over a broad range of human emotions .", "Our evaluation shows that our model can more accurately predict these distributions than other models .", "We gratefully acknowledge the support of the Defense Advanced Research Projects Agency DARPA Machine Reading Program under Air Force Research Laboratory AFRL prime contract no .", "FA8750 09 C 0181 .", "Any opinions , findings , and conclusion or recommendations expressed in this material are those of the author s and do not necessarily reflect the view of DARPA , AFRL , or the US government .", "This work was also supported in part by the DARPA Deep Learning program under contract number FA8650 10 C 7020 .", "We thank Chris Potts for help with the EP data set , Raymond Hsu , Bozhi See , and Alan Wu for letting us use their system as a baseline and Jiquan Ngiam , Quoc Le , Gabor Angeli and Andrew Maas for their feedback ."], "summary_lines": ["Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions\n", "We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions.\n", "Our method learns vector space representations for multi-word phrases.\n", "In sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-defined sentiment lexica or polarity shifting rules.\n", "We also evaluate the model\u2019s ability to predict sentiment distributions on a new dataset based on confessions from the experience project.\n", "The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions.\n", "Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines.\n", "we introduce a semi-supervised approach that uses recursive autoencoders to learn the hierarchical structure and sentiment distribution of a sentence.\n"]}
{"article_lines": ["Inference In DATR", "a declarative language for representing a restricted class of inheritance networks , permitting both multiple and default inheritance .", "The principal intended area of application is the representation of lexical entries for natural language processing , and we use examples from this domain throughout .", "In this paper we present the syntax and inference mechanisms for language .", "The goal of the is the design of a simple language that i has the necessary expressive power to encode the lexical entries presupposed by contemporary work in the unification grammar tradition , ii can express all the evident generalizations about such entries , iii has an explicit theory of inference , iv is computationally tractable , and v has an explicit declarative semantics .", "The present paper is primarily concerned with iii , though the examples used may hint at our strategy in respect of i and ii .", "Inheritance networks quot ; semantic nets quot ; provide an intuitively appealing way of thinking about the representation of various kinds of knowledge .", "This fact has not gone unnoticed by a number of researchers working on lexical knowledge representation , e . g .", "de Smedt 1984 , Flickinger et al . 1985 , Calder te Linden 1987 , Daelemans 1987a , 1987b , Gazdar 1987 and Calder 1989 .", "However , many such networks have been realized in the context of programming systems or programming languages that leave their precise meaning unclear .", "In the light of Braclunan 1985 , Ether . ington 1988 and much other recent work , it ha become apparent that the formal properties oi notations intended to represent inheritance arc highly problematic .", "Although not discussec here , DATR has a formal semantics Evans Gazdar 1989 for which some completeness anc soundness results have been derived .", "These results , and others on complexity , for example ; will be provided in a subsequent paper .", "There are several prototype computational implementa . tions of the language , and non trivial lexicor fragments for English , German and Latin have been developed and tested .", "The syntax of DATR , especially the use of value terminated attribute trees to encode information derives from PATR Shieber 1986 .", "The language consists of strings of symbols drawn from the se S'YM , quot ; , , , , , , and the set ATOM and NODE , all of which are disjoint .", "A string is in DATR , with respect to given set ATOM of atom s and NODE of node s if it is sentence as defined by the following set 01 rules There are two kinds of sentence , those containing ' and those containing ' .", "Both kinds have on their left hand side a node path specification , where a path is a sequence of atoms enclosed in . . . .", "Pragmatically , the 3 sentences are intended for defining the network , whilst the ' ' statements express the values at individual nodes .", "Put another way , the former provide the database definition language whilst the latter provide the query language the useful premises will standardly all be statements , whilst the interesting theorems will standardly all be statements though the language itself also allows the former to be derived as theorems and the latter to be used as premises .", "In view of this distinction , we shall sometimes refer to ' sentences as definitional and 4 ' sentences as extensional .", "Throughout the examples in this paper , we shall use bold for nodes and roman for atoms .", "Bold italic and italic will be used for corresponding meta notational variables .", "Variables such as N , P , L , G and V will be assumed to be typed as nodes , paths , lvalues , gvalues and values respectively .", "We shall sometimes refer to atoms occurring in paths as attributes .", "The right hand sides of extensional sentences are values , that is , simple atoms or lists of atoms nested lists enclosed in . . . .", "Lists are provided to allow the components of complex values to be specified independently inherited from different places , for example .", "As an example , the following sentences might be derivable from a lexical entry for English 'be' Likewise , the following for German Buch' Values are the principal 'results' of a DATR description the most typical operation is to determine the value associated by an extensional sentence with some node path pair .", "The right hand sides of definitional sentences are lvalues , which can be simple atoms , inheritance descriptors quoted or unquoted , or lists of lvalues .", "An atom is primitive , an inheritance descriptor specifies where the required value can be inherited from , and lists allow arbitrary structures to be built as values .", "Inheritance descriptors come in several forms with two dimensions of variation .", "The unquoted quoted distinction specifies whether the inheritance context is local the most recent context employed or global the initial context employed .", "Once the context is established , the descriptor specifies a new node , a new lpath , or both to be used to determine the inherited value .", "For example , the following sentences might be found in a description of a lexicon for English Finally an lpath is a path made up of lvalues , that is , elements which themselves may need evaluation , as in this example", "form quot ; gen quot ; quot ; num quot ; quot ; case quot ; .", "We adopt the following abbreviation convention for sets of sentences about a single node", "DATR has seven syntactic rules of inference falling into three groups .", "The first rule just provides us with a trivial route from definitional to extensional sentences Note that V must be a value not an lvalue here , otherwise the consequent would not be wellformed .", "The next three rules implement local inheritance of values , and use the following additional meta notational device the expression E0 E21E1 is well formed iff EO , El and E2 are lvalues and El occurs as a subexpression of EO .", "In that case , the expression denotes the result of substituting E2 for all occurrences of El in EO .", "Rule II says that if we have a theorem Nl P1 L . where L contains N2 P2 as a subexpression , and we also have a theorem N2 P2 G . , then we can derive a theorem in which all occurrences of N2 P2 in L are replaced by G . In the simplest case , this means that we can interpret a sentence of the form N1 P1 N2 P2 . as an inheritance specification meaning quot ; the value of P1 at Ni is inherited from P2 at N2 quot ; .", "So for example , from Rules III and IV are similar , but specify only a new node or path not both to inherit from .", "The other component path or node is unchanged , that is , it is the same as the corresponding component on the left hand side of the rule specifying the inheritance .", "In fact , the following two sentence schemas are entirely equivalent Rules II , III , and IV implement a local notion of inheritance in the sense that the new node or path specifications are interpreted in the current local context .", "The three remaining inference rules implement a non local notion of inheritance quoted descriptors specify values to be 68 interpreted in the context in which the original query was made the global context , rather than the current context .", "To see how the operation of these rules differs from the earlier unquoted cases , consider the following theory The intention here is that the CAT node expresses the generalisation that by default plural is the same as singular , v and Al inherit this , but A2 , while inheriting its plural form from Al , has an exceptional singular form , overriding inheritance from CAT via Al .", "Now from this theory we can derive all the following theorems concerning plural and the following theorem concerning singular A2 sing en .", "But we cannot derive a theorem for V sing , for example .", "This is because v sing inherits from cAT sing , which inherits locally from cAT plur , which is not defined .", "What we wanted was for cAT sing to inherit from v plur , that is , from the global initial context .", "To achieve this we change the CAT definition to be CAT sing quot ; plur quot ; .", "Now we find that we can still derive the same plural theorems , but now in addition we get all these theorems concerning singular For example , the derivation for the first of these is as follows Finally , given a set of sentences T , we define the rule closure of 7 , rc1 T to be the closure of T under finite application of the above inference rules in the conventional fashion .", "In addition to the conventional inference defined above , DATR has a nonmonotonic notion of inference by default each definitional sentence about some node path combination implicitly determines additional sentences about all the extensions to the path at that node for which no more specific definitional sentence exists in the theory .", "Our overall approach follows Moore 1983 , 1985 , whose treatment of inferences from sets of beliefs can be viewed more generally as a technique for providing a semantics for a declarative notion of inference by default cf .", "Touretzky 1986 , p34 ; Evans 1987 .", "We begin with some auxiliary definitions .", "The expression PAQ , where P and Q are paths , denotes the path formed by concatenating components of P and Q .", "A path P2 is an extension of a path P1 iff there is a path Q such that P2 P1AQ .", "P2 is a strict extension if Q is nonempty .", "We also use the A operator to denote extension of all the paths in a DATR sentence , as in the following examples Given a sentence S , we define the root of S to be the node path expression appearing to the left of the equality ' or ' in S for example the root of 'N P V . ' is 'N P ' .", "The root does not correspond to any syntactic category defined above it is simply a substring of the sentence .", "Given a set of sentences in DATR , T , a node N and a path P . we say N P is specified in q if 'T contains a definitional sentence S whose root is N P . Let Ni P1 , Ni P2 be such that Ni P1 is specified in T . We say Nl P2 is connected to Ni P1 relative to , r if there is no strict extension P3 of P1 of which P2 is an extension such that N1 P3 is specified in T . So Ni P2 is connected to Ni P1 if PI is the maximal subpath of P2 that is specified with Ni in T . Now given a set of sentences T , define the path closure pcl T of T to be pcl T S S is an extensional sentence in S Q S is a definitional sentence in T , with root N P , and N PAQ is connected to N P It is clear from these definitions that any N P is connected to itself and thus that T is always a subset of pd T .", "The path closure contains all those theorems which can be inferred by default from T . To illustrate path closure , consider the following example theory The situation is slightly more complicated with sentences that have paths on their right hand sides .", "Such paths are also extended by the subpath used to extend the left hand side .", "So the sentence might give rise by default to sentences such as A2 sing fern nom quot ; Alxplur fern nom quot ; .", "Using default inference , the example theory we used to illustrate global inference can be phrased more succinctly In this version , we state that anything not specifically mentioned for V is inherited by default from CAT , whereas before we had to list cases only 'sing' in the example explicitly .", "Similarly Al inherits by default from CAT , and A2 from Al .", "The operation of path closure is non monotonic if we add more sentences to our original theory , some of our derived sentences may cease to be true .", "The two forms of inference in DATR are combined by taking the path closure of a theory first , and then applying the inference rules to the result .", "In other words , given a theory qc and a sentence S , S is provable from T if rd pc1 2 .", "Evans's work was supported by a grant from the SERC .", "Gazdar's work was supported by grants from the ESRC and SERC .", "We are grateful to our referees and to Jon Cunningham , Walter Daelemans , David Israel , Bill Keller , Tom Khabaza , Ewan Klein , Bob Moore , Fernando Pereira , Allan Ramsay and Chris Thornton for clarifying our thinking about aspects of DATR ."], "summary_lines": ["Inference In DATR\n", "DATR is a declarative language for representing a restricted class of inheritance networks, permitting both multiple and default inheritance.\n", "The principal intended area of application is the representation of lexical entries for natural language processing, and we use examples from this domain throughout.\n", "In this paper we present the syntax and inference mechanisms for the language.\n", "The goal of the DATR enterprise is the design of a simple language that (i) has the necessary expressive power to encode the lexical entries presupposed by contemporary work in the unification grammar tradition, (ii) can express all the evident generalizations about such entries, (iii) has an explicit theory of inference, (iv) is computationally tractable, and (v) has an explicit declarative semantics.\n", "The present paper is primarily concerned with (iii), though the examples used may hint at our strategy in respect of (i) and (ii).\n", "we introduce DATR, a formal language for representing lexical knowledge.\n"]}
{"article_lines": ["A Boosting Algorithm For Classification Of Semi Structured Text", "The focus of research in text classification has expanded from simple topic identification to more challenging tasks such as opinion modality identification .", "Unfortunately , the latter goals exceed the ability of the traditional bag of word representation approach , and a richer , more structural representation is required .", "Accordingly , learning algorithms must be created that can handle the structures observed in texts .", "In this paper , we propose a Boosting algorithm that captures sub structures embedded in texts .", "The proposal consists of i decision stumps that use subtrees as features and ii the Boosting algorithm which employs the subtree based decision stumps as weak learners .", "We also discuss the relation between our algorithm and SVMs with tree kernel .", "Two experiments on opinion modality classification confirm that subtree features are important .", "Text classification plays an important role in organizing the online texts available on the World Wide Web , Internet news , and E mails .", "Until recently , a number of machine learning algorithms have been applied to this problem and have been proven successful in many domains Sebastiani , 2002 .", "In the traditional text classification tasks , one has to identify predefined text topics , such as politics , finance , sports or entertainment .", "For learning algorithms to identify these topics , a text is usually represented as a bag of words , where a text is regarded as a multi set i . e . , a bag of words and the word order or syntactic relations appearing in the original text is ignored .", "Even though the bag of words representation is naive and does not convey the meaning of the original text , reasonable accuracy can be obtained .", "This is because each word occurring in the text is highly relevant to the predefined topics to be identified .", "At present , NTT Communication Science Laboratories , 2 4 , Hikaridai , Seika cho , Soraku , Kyoto , 619 0237 Japan taku cslab . kecl . ntt . co . jp Given that a number of successes have been reported in the field of traditional text classification , the focus of recent research has expanded from simple topic identification to more challenging tasks such as opinion modality identification .", "Example includes categorization of customer E mails and reviews by types of claims , modalities or subjectivities Turney , 2002 ; Wiebe , 2000 .", "For the latter , the traditional bag of words representation is not sufficient , and a richer , structural representation is required .", "A straightforward way to extend the traditional bag of words representation is to heuristically add new types of features to the original bag of words features , such as fixed length n grams e . g . , word bi gram or tri gram or fixedlength syntactic relations e . g . , modifier head relations .", "These ad hoc solutions might give us reasonable performance , however , they are highly taskdependent and require careful design to create the optimal feature set for each task .", "Generally speaking , by using text processing systems , a text can be converted into a semi structured text annotated with parts of speech , base phrase information or syntactic relations .", "This information is useful in identifying opinions or modalities contained in the text .", "We think that it is more useful to propose a learning algorithm that can automatically capture relevant structural information observed in text , rather than to heuristically add this information as new features .", "From these points of view , this paper proposes a classification algorithm that captures sub structures embedded in text .", "To simplify the problem , we first assume that a text to be classified is represented as a labeled ordered tree , which is a general data structure and a simple abstraction of text .", "Note that word sequence , base phrase annotation , dependency tree and an XML document can be modeled as a labeled ordered tree .", "The algorithm proposed here has the following characteristics i It performs learning and classification using structural information of text . ii It uses a set of all subtrees bag of subtrees for the feature set without any constraints . iii Even though the size of the candidate feature set becomes quite large , it automatically selects a compact and relevant feature set based on Boosting .", "This paper is organized as follows .", "First , we describe the details of our Boosting algorithm in which the subtree based decision stumps are applied as weak learners .", "Second , we show an implementation issue related to constructing an efficient learning algorithm .", "We also discuss the relation between our algorithm and SVMs Boser et al . , 1992 with tree kernel Collins and Duffy , 2002 ; Kashima and Koyanagi , 2002 .", "Two experiments on the opinion and modality classification tasks are employed to confirm that subtree features are important .", "We first assume that a text to be classified is represented as a labeled ordered tree .", "The focused problem can be formalized as a general problem , called the tree classification problem .", "The tree classification problem is to induce a mapping f x X 1 , from given training examples T hxi , yii Li 1 , where xi X is a labeled ordered tree and yi 1 is a class label associated with each training data we focus here on the problem of binary classification . .", "The important characteristic is that the input example xi is represented not as a numerical feature vector bagof words but a labeled ordered tree .", "Let us introduce a labeled ordered tree or simply tree , its definition and notations , first .", "We denote the number of nodes in t as t .", "Figure 1 shows an example of a labeled ordered tree and its subtree and non subtree .", "Decision stumps are simple classifiers , where the final decision is made by only a single hypothesis or feature .", "Boostexter Schapire and Singer , 2000 uses word based decision stumps for topic based text classification .", "To classify trees , we here extend the decision stump definition as follows .", "Definition 3 Decision Stumps for Trees Let t and x be labeled ordered trees , and y be a class label y 1 , a decision stump classifier for trees is given by The parameter for classification is the tuple ht , yi , hereafter referred to as the rule of the decision stumps .", "The decision stumps are trained to find rule h\u02c6t , \u02c6yi that minimizes the error rate for the given training data T hxi , yii Li 1 In this paper , we will use gain instead of error rate for clarity .", "The decision stumps classifiers for trees are too inaccurate to be applied to real applications , since the final decision relies on the existence of a single tree .", "However , accuracies can be boosted by the Boosting algorithm Freund and Schapire , 1996 ; Schapire and Singer , 2000 .", "Boosting repeatedly calls a given weak learner to finally produce hypothesis f , which is a linear combination of K hypotheses produced by the prior weak learners , i , e .", "A weak learner is built at each iteration k with different distributions or weights d k d k The weights are calculated in such a way that hard examples are focused on more than easier examples .", "To use the decision stumps as the weak learner of Boosting , we redefine the gain function 2 as follows There exist many Boosting algorithm variants , however , the original and the best known algorithm is AdaBoost Freund and Schapire , 1996 .", "We here use Arc GV Breiman , 1999 instead of AdaBoost , since Arc GV asymptotically maximizes the margin and shows faster convergence to the optimal solution than AdaBoost .", "In this section , we introduce an efficient and practical algorithm to find the optimal rule \u02c6t , \u02c6y from given training data .", "This problem is formally defined as follows .", "Problem 1 Find Optimal Rule Let T x1 , y1 , d1 , . . . , xL , yL , dL be training data , where , xi is a labeled ordered tree , yi E 1 is a class label associated with xi and di EL i 1 di 1 , di 0 is a normalized weight assigned to xi .", "Given T , find the optimal rule \u02c6t , \u02c6y that maximizes the gain , i . e . , The most naive and exhaustive method , in which we first enumerate all subtrees F and then calculate the gains for all subtrees , is usually impractical , since the number of subtrees is exponential to its size .", "We thus adopt an alternative strategy to avoid such exhaustive enumeration .", "The method to find the optimal rule is modeled as a variant of the branch and bound algorithm , and is summarized in the following strategies We will describe these steps more precisely in the following subsections .", "Abe and Zaki independently proposed an efficient method , rightmost extension , to enumerate all subtrees from a given tree Abe et al . , 2002 ; Zaki , 2002 .", "First , the algorithm starts with a set of trees consisting of single nodes , and then expands a given tree of size k 1 by attaching a new node to this tree to obtain trees of size k . However , it would be inefficient to expand nodes at arbitrary positions of the tree , as duplicated enumeration is inevitable .", "The algorithm , rightmost extension , avoids such duplicated enumerations by restricting the position of attachment .", "We here give the definition of rightmost extension to describe this restriction in detail .", "Definition 4 Rightmost Extension Abe et al . , 2002 ; Zaki , 2002 Let t and t' be labeled ordered trees .", "We say t' is a rightmost extension of t , if and only if t and t' satisfy the following three conditions Consider Figure 2 , which illustrates example tree t with the labels drawn from the set G a , b , c .", "For the sake of convenience , each node in this figure has its original number depth first enumeration .", "The rightmost path of the tree t is a c b , and occurs at positions 1 , 4 and 6 respectively .", "The set of rightmost extended trees is then enumerated by simply adding a single node to a node on the rightmost path .", "Since there are three nodes on the rightmost path and the size of the label set is 3 G , a total of 9 trees are enumerated from the original tree t . Note that rightmost extension preserves the prefix ordering of nodes in t i . e . , nodes at positions 1 . . t are preserved .", "By repeating the process of rightmost extension recursively , we can create a search space in which all trees drawn from the set G are enumerated .", "Figure 3 shows a snapshot of such a search space .", "Rightmost extension defines a canonical search space in which one can enumerate all subtrees from a given set of trees .", "We here consider an upper bound of the gain that allows subspace pruning in this canonical search space .", "The following theorem , an extension of Morhishita Morhishita , 2002 , gives a convenient way of computing a tight upper bound on gain ht' , yi for any super tree t' of t . We can efficiently prune the search space spanned by right most extension using the upper bound of gain u t .", "During the traverse of the subtree lattice built by the recursive process of rightmost extension , we always maintain the temporally suboptimal gain \u03c4 among all gains calculated previously .", "If \u00b5 t \u03c4 , the gain of any super tree t' t is no greater than \u03c4 , and therefore we can safely prune the search space spanned from the subtree t . If \u00b5 t \u03c4 , in contrast , we cannot prune this space , since there might exist a super tree t' t such that gain t' \u03c4 .", "We can also prune the space with respect to the expanded single node s . Even if \u00b5 t \u03c4 and a node s is attached to the tree t , we can ignore the space spanned from the tree t' if \u00b5 s \u03c4 , since no super tree of s can yield optimal gain .", "Figure 4 presents a pseudo code of the algorithm Find Optimal Rule .", "The two pruning are marked with 1 and 2 respectively .", "Recent studies Breiman , 1999 ; Schapire et al . , 1997 ; R atsch et al . , 2001 have shown that both Boosting and SVMs Boser et al . , 1992 have a similar strategy ; constructing an optimal hypothesis that maximizes the smallest margin between the positive and negative examples .", "We here describe a connection between our Boosting algorithm and SVMs with tree kernel Collins and Duffy , 2002 ; Kashima and Koyanagi , 2002 .", "Tree kernel is one of the convolution kernels , and implicitly maps the example represented in a labeled ordered tree into all subtree spaces .", "The implicit mapping defined by tree kernel is given as \u03a6 x I t1 x , . . . , I t F x , where tj F , x X and I is the indicator function 1 .", "The final hypothesis of SVMs with tree kernel can be given by Similarly , the final hypothesis of our boosting algorithm can be reformulated as a linear classifier 1Strictly speaking , tree kernel uses the cardinality of each substructure .", "However , it makes little difference since a given tree is often sparse in NLP and the cardinality of substructures will be approximated by their existence .", "argument T hx1 , y1 , d1i . . . , hxL , yL , dLi xi a tree , yi 1 is a class , and di PLi 1 di 1 , di 0 is a weight returns Optimal rule h\u02c6t , \u02c6yi begin We can thus see that both algorithms are essentially the same in terms of their feature space .", "The difference between them is the metric of margin ; the margin of Boosting is measured in l1 norm , while , that of SVMs is measured in l2 norm .", "The question one might ask is how the difference is expressed in practice .", "The difference between them can be explained by sparseness .", "It is well known that the solution or separating hyperplane of SVMs is expressed as a linear combination of the training examples using some coefficients A , i . e . , w PL i 1 Ai\u03a6 xi .", "Maximizing l2norm margin gives a sparse solution in the example space , i . e . , most of Ai becomes 0 .", "Examples that have non zero coefficient are called support vectors that form the final solution .", "Boosting , in contrast , performs the computation explicitly in the feature space .", "The concept behind Boosting is that only a few hypotheses are needed to express the final solution .", "The l1 norm margin allows us to realize this property .", "Boosting thus finds a sparse solution in the feature space .", "The accuracies of these two methods depends on the given training data .", "However , we argue that Boosting has the following practical advantages .", "First , sparse hypotheses allow us to build an efficient classification algorithm .", "The complexity of SVMs with tree kernel is O L' N1 N2 , where N1 and N2 are trees , and L' is the number of support vectors , which is too heavy to realize real applications .", "Boosting , in contrast , runs faster , since the complexity depends only on the small number of decision stumps .", "Second , sparse hypotheses are useful in practice as they provide transparent models with which we can analyze how the model performs or what kind of features are useful .", "It is difficult to give such analysis with kernel methods , since they define the feature space implicitly .", "We conducted two experiments in sentence classification .", "The goal of this task is to classify reviews in Japanese for PHS2 as positive reviews or negative reviews .", "A total of 5 , 741 sentences were collected from a Web based discussion BBS on PHS , in which users are directed to submit positive reviews separately from negative reviews .", "The unit of classification is a sentence .", "The categories to be identified are positive or negative with the numbers 2 , 679 and 3 , 062 respectively .", "This task is to classify sentences in Japanese by modality .", "A total of 1 , 710 sentences from a Japanese newspaper were manually annotated according to Tamura s taxonomy Tamura and Wada , 1996 .", "The unit of classification is a sentence .", "The categories to be identified are opinion , assertion or description with the numbers 159 , 540 , and 1 , 011 respectively .", "To employ learning and classification , we have to represent a given sentence as a labeled ordered tree .", "In this paper , we use the following three representation forms .", "Ignoring structural information embedded in text , we simply represent a text as a set of words .", "This is exactly the same setting as Boostexter .", "Word boundaries are identified using a Japanese morphological analyzer , ChaSen3 .", "We represent a text in a word based dependency tree .", "We first use CaboCha4 to obtain a chunk based dependency tree of the text .", "The chunk approximately corresponds to the basephrase in English .", "By identifying the head word in the chunk , a chunk based dependency tree is converted into a word based dependency tree .", "It is the word based dependency tree that assumes that each word simply modifies the next word .", "Any subtree of this structure becomes a word n gram .", "We compared the performance of our Boosting algorithm and support vector machines SVMs with bag of words kernel and tree kernel according to their F measure in 5 fold cross validation .", "Although there exist some extensions for tree kernel Kashima and Koyanagi , 2002 , we use the original tree kernel by Collins Collins and Duffy , 2002 , where all subtrees of a tree are used as distinct features .", "This setting yields a fair comparison in terms of feature space .", "To extend a binary classifier to a multi class classifier , we use the one vs rest method .", "Hyperparameters , such as number of iterations K in Boosting and soft margin parameter C in SVMs were selected by using cross validation .", "We implemented SVMs with tree kernel based on TinySVM5 with custom kernels incorporated therein .", "Table 1 summarizes the results of PHS and MOD tasks .", "To examine the statistical significance of the results , we employed a McNemar s paired test , a variant of the sign test , on the labeling disagreements .", "This table also includes the results of significance tests .", "In all tasks and categories , our subtree based Boosting algorithm dep ngram performs better than the baseline method bow .", "This result supports our first intuition that structural information within texts is important when classifying a text by opinions or modalities , not by topics .", "We also find that there are no significant differences in accuracy between dependency and n gram in all cases , p 0 . 2 .", "When using the bag of words feature , no significant differences in accuracy are observed between Boosting and SVMs .", "When structural information is used in training and classification , Boosting performs slightly better than SVMs with tree kernel .", "The differences are significant when we use dependency features in the MOD task .", "SVMs show worse performance depending on tasks and categories , e . g . , 24 . 2 F measure in the smallest category opinion in the MOD task .", "When a convolution kernel is applied to sparse data , kernel dot products between almost the same instances become much larger than those between different instances .", "This is because the number of common features between similar instances exponentially increases with size .", "This sometimes leads to overfitting in training , where a test instance very close to an instance in training data is correctly classified , and other instances are classified as a default class .", "This problem can be tackled by several heuristic approaches i employing a decay factor to reduce the weights of large sub structures Collins and Duffy , 2002 ; Kashima and Koyanagi , 2002 . ii substituting kernel dot products for the Gaussian function to smooth the original kernel dot products Haussler , 1999 .", "These approaches may achieve better accuracy , however , they yield neither the fast classification nor the interpretable feature space targeted by this paper .", "Moreover , we cannot give a fair comparison in terms of the same feature space .", "The selection of optimal hyperparameters , such as decay factors in the first approach and smoothing parameters in the second approach , is also left to as an open question .", "We employed a McNemar s paired test on the labeling disagreements .", "Underlined results indicate that there is a significant difference p 0 . 01 against the baseline bow .", "If there is a statistical difference p 0 . 01 between Boosting and SVMs with the same feature representation bow dep n gram , better results are asterisked .", "In the previous section , we described the merits of our Boosting algorithm .", "We experimentally verified these merits from the results of the PHS task .", "As illustrated in section 4 , our method can automatically select relevant and compact features from a number of feature candidates .", "In the PHS task , a total 1 , 793 features rules were selected , while the set sizes of distinct uni gram , bi gram and trigram appearing in the data were 4 , 211 , 24 , 206 , and 43 , 658 respectively .", "Even though all subtrees are used as feature candidates , Boosting selects a small and highly relevant subset of features .", "When we explicitly enumerate the subtrees used in tree kernel , the number of active non zero features might amount to ten thousand or more .", "Table 2 shows examples of extracted support features pairs of feature tree t and weight wt in Eq .", "5 in the PHS task .", "A .", "Features including the word \u306b\u304f\u3044 hard , difficult In general , \u306b\u304f\u3044 hard , difficult is an adjective expressing negative opinions .", "Most of features including \u306b\u304f\u3044 are assigned a negative weight negative opinion .", "However , only one feature \u5207\u308c\u306b \u304f\u3044 hard to cut off has a positive weight .", "This result strongly reflects the domain knowledge , PHS cell phone reviews .", "B .", "Features including the word \u4f7f\u3046 use \u4f7f\u3046 use is a neutral expression for opinion classifications .", "However , the weight varies according to the surrounding context 1 \u4f7f\u3044 \u305f\u3044 want to use positive , 2 \u4f7f\u3044 \u3084\u3059 \u3044 be easy to use positive , 3 \u4f7f\u3044 \u3084\u3059 \u304b \u3063\u305f was easy to use past form negative , 4 \u306e \u307b\u3046\u304c \u4f7f\u3044 \u3084\u3059\u3044 . . . is easier to use than . . comparative negative .", "C . Features including the word \u5145\u96fb recharge Features reflecting the domain knowledge are extracted 1 \u5145\u96fb \u6642\u9593 \u304c \u77ed\u3044 recharging time is short positive , 2 \u5145\u96fb \u6642\u9593 \u9577\u3044 recharging time is long negative .", "These features are interesting , since we cannot determine the correct label positive negative by using just the bag of words features , such as recharge , short or long alone .", "Table 3 illustrates an example of actual classification .", "For the input sentence \u6db2\u6676\u304c\u5927\u304d\u304f\u3066 , \u7dba\u9e97 , \u898b\u3084\u3059\u3044 The LCD is large , beautiful , and easy to see .", ", the system outputs the features applied to this classification along with their weights wt .", "This information allows us to analyze how the system classifies the input sentence in a category and what kind of features are used in the classification .", "We cannot perform these analyses with tree kernel , since it defines their feature space implicitly .", "The testing speed of our Boosting algorithm is much higher than that of SVMs with tree kernel .", "In the PHS task , the speeds of Boosting and SVMs are 0 . 531 sec . 5 , 741 instances and 255 . 42 sec . 5 , 741 instances respectively 6 .", "We can say that Boosting is about 480 times faster than SVMs with tree kernel .", "Even though the potential size of search space is huge , the pruning criterion proposed in this paper effectively prunes the search space .", "The pruning conditions in Fig . 4 are fulfilled with more than 90 probabitity .", "The training speed of our method is 1 , 384 sec . 5 , 741 instances when we set K 60 , 000 of iterations for Boosting .", "It takes only 0 . 023 1 , 384 60 , 000 sec . to invoke the weak learner , Find Optimal Rule .", "In this paper , we focused on an algorithm for the classification of semi structured text in which a sentence is represented as a labeled ordered tree7 .", "Our proposal consists of i decision stumps that use subtrees as features and ii Boosting algorithm in which the subtree based decision stumps are applied as weak learners .", "Two experiments on opinion modality classification tasks confirmed that subtree features are important .", "One natural extension is to adopt confidence rated predictions to the subtree based weak learners .", "This extension is also found in BoosTexter and shows better performance than binary valued learners .", "In our experiments , n gram features showed comparable performance to dependency features .", "We would like to apply our method to other applications where instances are represented in a tree and their subtrees play an important role in classifications e . g . , parse re ranking Collins and Duffy , 2002 and information extraction ."], "summary_lines": ["A Boosting Algorithm For Classification Of Semi-Structured Text\n", "The focus of research in text classification has expanded from simple topic identification to more challenging tasks such as opinion/modality identification.\n", "Unfortunately, the latter goals exceed the ability of the traditional bag-of-word representation approach, and a richer, more structural representation is required.\n", "Accordingly, learning algorithms must be created that can handle the structures observed in texts.\n", "In this paper, we propose a Boosting algorithm that captures sub-structures embedded in texts.\n", "The proposal consists of i) decision stumps that use subtrees as features and ii) the Boosting algorithm which employs the subtree-based decision stumps as weak learners.\n", "We also discuss the relation between our algorithm and SVMs with tree kernel.\n", "Two experiments on opinion/modality classification confirm that subtree features are important.\n", "We adopt the BACT learning algorithm to effectively learn subtrees useful for both antecedent identification and zero pronoun detection.\n"]}
{"article_lines": ["Efficient Support Vector Classifiers For Named Entity Recognition", "Named Entity NE recognition is a task in whichproper nouns and numerical information are extracted from documents and are classified into cat egories such as person , organization , and date .", "It is a key technology of Information Extraction and Open Domain Question Answering .", "First , we showthat an NE recognizer based on Support Vector Ma chines SVMs gives better scores than conventional systems .", "However , off the shelf SVM classifiers are too inefficient for this task .", "Therefore , we present a method that makes the system substantially faster . This approach can also be applied to other similar tasks such as chunking and part of speech tagging .", "We also present an SVM based feature selec tion method and an efficient training method .", "Named Entity NE recognition is a task in whichproper nouns and numerical information in a docu ment are detected and classified into categories suchas person , organization , and date .", "It is a key technol ogy of Information Extraction and Open Domain Question Answering Voorhees and Harman , 2000 .", "We are building a trainable Open Domain Question Answering System called SAIQA II .", "In this paper , we show that an NE recognizer based on Support Vector Machines SVMs gives better scores thanconventional systems .", "SVMs have given high per formance in various classification tasks Joachims , 1998 ; Kudo and Matsumoto , 2001 .", "However , it turned out that off the shelf SVM classifiers are too inefficient for NE recognition .", "The recognizer runs at a rate of only 85 bytes sec on an Athlon 1 . 3 GHz Linux PC , while rule based systems e . g . , Isozaki , 2001 can process several kilobytes in a second .", "The major reason is the inefficiency of SVM classifiers .", "There are otherreports on the slowness of SVM classifiers .", "Another SVM based NE recognizer Yamada and Mat sumoto , 2001 is 0 . 8 sentences sec on a Pentium III 933 MHz PC .", "An SVM based part of speech POS .", "tagger Nakagawa et al , 2001 is 20 tokens sec on an Alpha 21164A 500 MHz processor .", "It is difficult to use such slow systems in practical applications .", "In this paper , we present a method that makes the NE system substantially faster .", "This method can also be applied to other tasks in natural languageprocessing such as chunking and POS tagging .", "Another problem with SVMs is its incomprehensibil ity .", "It is not clear which features are important or how they work .", "The above method is also useful for finding useless features .", "We also mention a method to reduce training time .", "1 . 1 Support Vector Machines .", "Suppose we have a set of training data for a two class problem , where ffflfi is a feature vector of the ffi th sample in the training data and ! is the label forthe sample .", "The goal is to find a decision func tion that accurately predicts for unseen . A non linear SVM classifier gives a decision function sign , for an input vector where . 0 21 3 546879 ! 6 ; Here , ! means is a member of a cer tain class and means is not a mem ber .", "7 s are called support vectors and are repre sentatives of training examples .", "is the numberof support vectors .", "Therefore , computational com plexity of ? is proportional to . Support vectorsand other constants are determined by solving a cer tain quadratic programming problem .", "4687 is akernel that implicitly maps vectors into a higher di mensional space .", "Typical kernels use dot products 4687 A CBED7 . A polynomial kernel of degree Fis given by BG ? HI J ! KG L . We can use vari MM M M N M M M M M M M M M N M O O O O O N O O O O O O O O O O O O M positive example , O negative example N M , N O support vectors Figure 1 Support Vector Machine ous kernels , and the design of an appropriate kernel for a particular application is an important research issue . Figure 1 shows a linearly separable case .", "The de cision hyperplane defined by P RQ separatespositive and negative examples by the largest mar gin .", "The solid line indicates the decision hyperplaneand two parallel dotted lines indicate the margin be tween positive and negative examples .", "Since such aseparating hyperplane may not exist , a positive pa rameter S is introduced to allow misclassifications .", "See Vapnik 1995 .", "1 . 2 SVM based NE recognition .", "As far as we know , the first SVM based NE system was proposed by Yamada et al 2001 for Japanese . His system is an extension of Kudo ? s chunking sys tem Kudo and Matsumoto , 2001 that gave the best performance at CoNLL 2000 shared tasks .", "In theirsystem , every word in a sentence is classified sequentially from the beginning or the end of a sen tence .", "However , since Yamada has not compared it with other methods under the same conditions , it is not clear whether his NE system is better or not .", "Here , we show that our SVM based NE system ismore accurate than conventional systems .", "Our sys tem uses the Viterbi search Allen , 1995 instead of sequential determination . For training , we use ? CRL data ? , which was prepared for IREX Information Retrieval and Extrac tion Exercise1 , Sekine and Eriguchi 2000 .", "It has about 19 , 000 NEs in 1 , 174 articles .", "We also use additional data by Isozaki 2001 .", "Both datasets are based on Mainichi Newspaper ? s 1994 and 1995 CD ROMs .", "We use IREX ? s formal test data calledGENERAL that has 1 , 510 named entities in 71 ar ticles from Mainichi Newspaper of 1999 .", "Systems are compared in terms of GENERAL ? s F measure 1http cs . nyu . edu cs projects proteus irexwhich is the harmonic mean of ? recall ?", "and ? preci sion ?", "and is defined as follows .", "Recall M the number of correct NEs , Precision M the number of NEs extracted by a system , where M is the number of NEs correctly extracted and classified by the system . We developed an SVM based NE system by following our NE system based on maximum entropy ME modeling Isozaki , 2001 .", "We sim ply replaced the ME model with SVM classifiers . The above datasets are processed by a morphological analyzer ChaSen 2 . 2 . 12 .", "It tokenizes a sen tence into words and adds POS tags .", "ChaSen uses about 90 POS tags such as common noun and location name .", "Since most unknown words are proper nouns , ChaSen ? s parameters for unknownwords are modified for better results .", "Then , a char acter type tag is added to each word .", "It uses 17character types such as all kanji and small integer .", "See Isozaki 2001 for details .", "Now , Japanese NE recognition is solved by theclassification of words Sekine et al , 1998 ; Borth wick , 1999 ; Uchimoto et al , 2000 .", "For instance , the words in ? President George Herbert Bush saidClinton is . . .", "are classified as follows ? President ?", "OTHER , ? George ?", "PERSON BEGIN , ? Her bert ?", "PERSON MIDDLE , ? Bush ?", "PERSON END , ? said ?", "OTHER , ? Clinton ?", "PERSON SINGLE , ? is ? OTHER .", "In this way , the first word of a person ? s name is labeled as PERSON BEGIN .", "The last word is labeled as PERSON END .", "Other words in the nameare PERSON MIDDLE .", "If a person ? s name is expressed by a single word , it is labeled as PERSON SINGLE .", "If a word does not belong to any namedentities , it is labeled as OTHER .", "Since IREX de fines eight NE classes , words are classified into 33 UTWVEX ! K categories . Each sample is represented by 15 features be cause each word has three features part of speech tag , character type , and the word itself , and two preceding words and two succeeding words are also used for context dependence .", "Although infrequent features are usually removed to prevent overfitting , we use all features because SVMs are robust .", "Each sample is represented by a long binary vector , i . e . , a sequence of 0 false and 1 true .", "For instance , ? Bush ?", "in the above example is represented by a 2http chasen . aist nara . ac . jp vector P YG Z _ G Z a described below .", "Only 15 elements are 1 .", "bdcfe8ghji Current word is not ? Alice ?", "bdc klghme Current word is ? Bush ?", "bdc nghji Current word is not ? Charlie ?", "bdcfe opikpqpghme Current POS is a proper noun bdcfe opinipghji Current POS is not a verb bdc nqre sre ghji Previous word is not ? Henry ?", "bdc nqre skghme Previous word is ? Herbert ?", "Here , we have to consider the following problems .", "First , SVMs can solve only a two class problem .", "Therefore , we have to reduce the above multi class problem to a group of two class problems .", "Second , we have to consider consistency among word classes in a sentence .", "For instance , a word classified as PERSON BEGIN should be followed by PERSON MIDDLE or PERSON END .", "It impliesthat the system has to determine the best combina tions of word classes from numerous possibilities . Here , we solve these problems by combining exist ing methods .", "There are a few approaches to extend SVMs to cover t class problems .", "Here , we employ the ? oneclass versus all others ?", "approach .", "That is , each clas sifier u is trained to distinguish members of a class v from non members .", "In this method , two or more classifiers may give ! to an unseen vector or no classifier may give ! . One common way to avoid such situations is to compare u values and to choose the class index v of the largest u . The consistency problem is solved by the Viterbi search .", "Since SVMs do not output probabilities , we use the SVM sigmoid method Platt , 2000 .", "That is , we use a sigmoid function wxG ? J y zI !", "l G to map u to a probability like value .", "The output of the Viterbi search is adjusted by a postprocessor for wrong word boundaries .", "The adjustment rules are also statistically determined Isozaki , 2001 .", "1 . 3 Comparison of NE recognizers .", "We use a fixed value ? Q9Q . F measures are not very sensitive to unless is too small .", "Whenwe used 1 , 038 , 986 training vectors , GENERAL ? s F measure was 89 . 64 for ? ? Q ? and 90 . 03 for 6 ? Q9Q . We employ the quadratic kernel F Y ? because it gives the best results .", "Polynomial kernels of degree 1 , 2 , and 3 resulted in 83 . 03 , 88 . 31 , F measure ? ?", "RG DT ? ?", "ME ? ?", "SVM 0 20 40 60 80 100 120 CRL data ? ? ? E ? ? ? ? ? ? ?", "76 78 80 82 84 86 88 90 Number of NEs in training data ? ?", "Figure 2 F measures of NE systems and 87 . 04 respectively when we used 569 , 994 training vectors .", "Figure 2 compares NE recognizers in terms ofGENERAL ? s F measures .", "? SVM ?", "in the figure in dicates F measures of our system trained by Kudo ? s TinySVM 0 . 073 with S ? ? Q ? . It attained 85 . 04 when we used only CRL data .", "? ME ? indicates our ME system and ? RG DT ?", "indicates a rule basedmachine learning system Isozaki , 2001 .", "According to this graph , ? SVM ?", "is better than the other sys tems . However , SVM classifiers are too slow .", "Fa mous SVM Light 3 . 50 Joachims , 1999 took 1 . 2 days to classify 569 , 994 vectors derived from 2 MB documents .", "That is , it runs at only 19 bytes sec .", "TinySVM ? s classifier seems best optimized among publicly available SVM toolkits , but it still works at only 92 bytes sec .", "In this section , we investigate the cause of this in efficiency and propose a solution .", "All experiments are conducted for training data of 569 , 994 vectors .", "The total size of the original news articles was 2 MB and the number of NEs was 39 , 022 .", "According to the definition of , a classifier has to process support vectors for each . Table 1 shows s for different word classes .", "According to this table , classi fication of one word requires ? s dot products with 228 , 306 support vectors in 33 classifiers .", "Therefore , the classifiers are very slow .", "We have never seensuch large s in SVM literature on pattern recogni tion .", "The reason for the large s is word features .", "Inother domains such as character recognition , dimen 3http cl . aist nara . ac . jp ? taku ku software TinySVM sion is usually fixed .", "However , in the NE task , increases monotonically with respect to the size of the training data .", "Since SVMs learn combinations of features , tends to be very large .", "This tendencywill hold for other tasks of natural language pro cessing , too .", "Here , we focus on the quadratic kernel BG I ! ? G ?", "that yielded the best score in the above experiments .", "Suppose ? G Z _ G Z a hasonly ?", "15 non zero elements .", "The dot prod uct of and 7 5 ?", "Z _ ? Z is given by ? fi ? 1 G Z ? ?", "Z ? ?", ". Hence , I ! ? ? D ? 7 ? ? ! W ? fi 0 ? 1 G ? Z ? ?", "Z ? ? ? ! ? fi 0 ? 1 G ? Z ? ?", "Z ? ? ? ? We can rewrite as follows .", "fi 0 ? 1 _ ? Z ? ?", "? G Z ? ? ? ? ! m ? ? Z ? ? ? ? G Z ? ? ?", "fi . ?", "0 ? 1 fi 0 ? 1 ? ?", "? ? ? rZ ? ? ? B ? G Z ? ?", "? G ? Z ? B _ where ? ?", "? 1 3 ? ? ? Z ? ?", "? 1 3 5 ? Z ? ?", "Z ? ?", "? ? ? 1 3 ? ? p8Z ? ?", "? ? ? P ? rZ ? ? B ? ? ?", "? 1 3 ? Z ? ?", "Z ? B _ For binary vectors , it can be simplified as . ? ?", "? l ? 1 _ ? C ? Z ? ? ?", "Z ? ? B where ? ?", "Z ? ? ? ? ? Z ? ? ?", "! m ? ? Z ? ? ? Y ? 0 ? ? ? 5 ?", "? l ? 1 3 ? ? ? 9Z ? ? ? B ? ? 0 ? , ? ? ? _ ? ? l ? 1 ? ? ? ? ?", "1 3 Now , ? can be given by summing up ? ?", "Z ? ? ?", "for every non zero element G ? Z ? ?", "and ? ?", "Z ? ? B for every non zero pair G ? Z ? ?", "? G Z ? B . Accordingly , we only need to add W ! ? ? ? ! ? ? j ? R ? z ?", "121 con stants to get . Therefore , we can expect thismethod to be much faster than a na ? ? ve implementa tion that computes tens of thousands of dot products at run time .", "We call this method ? XQK ?", "eXpand the Quadratic Kernel .", "Table 1 compares TinySVM and XQK in terms of CPU time taken to apply 33 classifiers to process the training data .", "Classes are sorted by . Small numbers in parentheses indicate the initializationtime for reading support vectors 7 and allocat ing memory .", "XQK requires a longer initialization time in order to prepare ? ?", "and ? ? ?", "For instance , TinySVM took 11 , 490 . 26 seconds 3 . 2 hours in to tal for applying OTHER ? s classifier to all vectors in the training data .", "Its initialization phase took 2 . 13 seconds and all vectors in the training data were classified in 11 , 488 . 13 9 X ? Q ? ? 9 ? ? ? ? x ? p ? sec onds .", "On the other hand , XQK took 225 . 28 secondsin total and its initialization phase took 174 . 17 sec onds .", "Therefore , 569 , 994 vectors were classified in51 . 11 seconds .", "The initialization time can be disre garded because we can reuse the above coefficents .", "Consequently , XQK is 224 . 8 11 , 488 . 13 51 . 11 times faster than TinySVM for OTHER .", "TinySVM took 6 hours to process all the word classes , whereas XQK took only 17 minutes .", "XQK is 102 times faster than SVM Light 3 . 50 which took 1 . 2 days .", "XQK makes the classifiers faster , but mem ory requirement increases from ? ? ? 1 ? to ? ? ? 1 ? ?", "! fl z ? r where ? 15 is the num ber of non zero elements in 7 . Therefore , removal .", "of useless features would be beneficial .", "Conven tional SVMs do not tell us how an individual feature works because weights are given not to features but to 4687 . However , the above weights ? ?", "and ? ? ?", "clarify how a feature or a feature pair works .", "We can use this fact for feature selection after the training .", "We simplify by removing all features ? that satisfy ? ?", "Z ? ?", "? f ? ? ?", "? ? ? ? ? ? rZ ? ? ? B ? f ? ?", "? ? ? ? P ? rZ ? B ? ? ?", "K ? ? ?", "The largest ? that does not change the number of misclassifications for the training data is found by using the binary searchfor each word class .", "We call this method ? XQKFS ?", "XQK with Feature Selection .", "This approx imation slightly degraded GENERAL ? s F measure from 88 . 31 to 88 . 03 . Table 2 shows the reduction of features that ap pear in support vectors .", "Classes are sorted by the numbers of original features .", "For instance , OTHERhas 56 , 220 features in its support vectors .", "Accord ing to the binary search , its performance did notchange even when the number of features was re duced to 21 , 852 at ? KQ ? Qr ? 9 ? r ? ?", "Table 1 Reduction of CPU time in seconds by XQK word class TinySVM init XQK init speed up SVM Light OTHER 64 , 970 11 , 488 . 13 2 . 13 51 . 11 174 . 17 224 . 8 29 , 986 . 52 ARTIFACT MIDDLE 14 , 171 1 , 372 . 85 0 . 51 41 . 32 14 . 98 33 . 2 6 , 666 . 26 LOCATION SINGLE 13 , 019 1 , 209 . 29 0 . 47 38 . 24 11 . 41 31 . 6 6 , 100 . 54 ORGANIZ . . MIDDLE 12 , 050 987 . 39 0 . 44 37 . 93 11 . 70 26 . 0 5 , 570 . 82 TOTAL 228 , 306 21 , 754 . 23 9 . 83 1 , 019 . 20 281 . 28 21 . 3 104 , 466 . 31 Table 2 Reduction of features by XQK FS word class number of features number of non zero weights seconds OTHER 56 , 220 ? 21 , 852 38 . 9 1 , 512 , 827 ? 892 , 228 59 . 0 42 . 31 ARTIFIFACT MIDDLE 22 , 090 ? 4 , 410 20 . 0 473 , 923 ? 164 , 632 34 . 7 30 . 47 LOCATION SINGLE 17 , 169 ? 3 , 382 19 . 7 366 , 961 ? 123 , 808 33 . 7 27 . 72 ORGANIZ . . MIDDLE 17 , 123 ? 9 , 959 58 . 2 372 , 784 ? 263 , 695 70 . 7 31 . 02 ORGANIZ . . END 15 , 214 ? 3 , 073 20 . 2 324 , 514 ? 112 , 307 34 . 6 26 . 87 TOTAL 307 , 721 ? 75 , 455 24 . 5 6 , 669 , 664 ? 2 , 650 , 681 39 . 7 763 . 10 The total number of features was reduced by 75 and that of weights was reduced by 60 .", "The ta ble also shows CPU time for classification by the selected features .", "XQK FS is 28 . 5 21754 . 23 763 . 10 times faster than TinySVM .", "Although the reduction of features is significant , the reduction of CPU time is moderate , because most of the reducedfeatures are infrequent ones .", "However , simple re duction of infrequent features without consideringweights damages the system ? s performance .", "For instance , when we removed 5 , 066 features that ap peared four times or less in the training data , themodified classifier for ORGANIZATION END misclassified 103 training examples , whereas the origi nal classifier misclassified only 19 examples .", "On theother hand , XQK FS removed 12 , 141 features with out an increase in misclassifications for the training data .", "XQK can be easily extended to a more generalquadratic kernel BG ? ? ? ? vl ? ? ! ? v G ?", "and to nonbinary sparse vectors .", "XQK FS can be used to se lect useful features before training by other kernels .", "As mentioned above , we conducted an experiment for the cubic kernel F ? ?", "by using all features . When we trained the cubic kernel classifiers by us ing only features selected by XQK FS , TinySVM ? s classification time was reduced by 40 because was reduced by 38 .", "GENERAL ? s F measure was slightly improved from 87 . 04 to 87 . 10 .", "Onthe other hand , when we trained the cubic ker nel classifiers by using only features that appeared three times or more without considering weights , TinySVM ? s classification time was reduced by only 14 and the F measure was slightly degraded to86 . 85 .", "Therefore , we expect XQK FS to be use ful as a feature selection method for other kernels when such kernels give much better results than the quadratic kernel .", "Since training of 33 classifiers also takes a longtime , it is difficult to try various combinations of pa rameters and features .", "Here , we present a solution for this problem .", "In the training time , calculation of B ? ? ? Dr B ? ? Dr ? B ? ? D for various ? s is dominant .", "Conventional systems save time by caching the results .", "By analyzing TinySVM ? s classifier , we found that they can be calculated more efficiently .", "For sparse vectors , most SVM classifiers e . g . , SVM Light use a sparse dot product algorithm Platt , 1999 that compares non zero elements of and those of 7 to get BED7 in . However , is common to all dot products in B ? D7 BD 7 . Therefore , we can implement a faster classifierthat calculates them concurrently .", "TinySVM ? s clas sifier prepares a list fi2si Z ? ?", "that contains all 7 s whose ? th coordinates are not zero .", "In addition , counters for ? D 7 p ? D 7 are prepared because dot products of binary vectors are integers .", "Then , for each non zero G Z ? ?", ", the counters are incremented for all 7 fi2si Z ? ? ?", "By checking only members of fi2si Z ? ?", "for non zero G Z ? ?", ", the classifier is not bothered by fruitless cases G ? Z ? ?", "? ? Q ? 8Z ? ? ? ? ? YQ orG Z ? ? ? W ? ? Q ? ? Z ? ? ? ? yQ . Therefore , TinySVM ? s clas sifier is faster than other classifiers .", "This method is applicable to any kernels based on dot products .", "For the training phase , we can build fi2si ? Z ? ? ?", "that contains all s whose ? th coordinates are notzero .", "Then , B ? ? D B ? ? ? D can be efficiently calculated because ? ?", "is common .", "This im provement is effective especially when the cache is small and or the training data is large .", "When we used a 200 MB cache , the improved system took only 13 hours for training by the CRL data , while TinySVM and SVM Light took 30 hours and 46hours respectively for the same cache size .", "Al though we have examined other SVM toolkits , we could not find any system that uses this approach in the training phase .", "The above methods can also be applied to othertasks in natural language processing such as chunk ing and POS tagging because the quadratic kernels give good results .", "Utsuro et al 2001 report that a combination of two NE recognizers attained F 84 . 07 , butwrong word boundary cases are excluded .", "Our system attained 85 . 04 and word boundaries are auto matically adjusted .", "Yamada Yamada et al , 2001 also reports that F ? ?", "is best .", "Although his sys tem attained F 83 . 7 for 5 fold cross validation of the CRL data Yamada and Matsumoto , 2001 , our system attained 86 . 8 .", "Since we followedIsozaki ? s implementation Isozaki , 2001 , our system is different from Yamada ? s system in the fol lowing points 1 adjustment of word boundaries , 2 ChaSen ? s parameters for unknown words , 3 char acter types , 4 use of the Viterbi search .", "For efficient classification , Burges and Scho ? lkopf 1997 propose an approximation method that uses ? reduced set vectors ?", "instead of support vectors .", "Since the size of the reduced set vectors is smaller than , classifiers become more efficient , but the computational cost to determine the vectors is verylarge .", "Osuna and Girosi 1999 propose two meth ods .", "The first method approximates by support vector regression , but this method is applicable onlywhen S is large enough .", "The second method reformulates the training phase .", "Our approach is sim pler than these methods .", "Downs et al Downs et al , 2001 try to reduce the number of support vectors by using linear dependence .", "We can also reduce the run time complexity of a multi class problem by cascading SVMs in the form of a binary tree Schwenker , 2001 or a directacyclic graph Platt et al , 2000 .", "Yamada and Mat sumoto 2001 applied such a method to their NEsystem and reduced its CPU time by 39 .", "This ap proach can be combined with our SVM classifers . NE recognition can be regarded as a variablelength multi class problem .", "For this kind of prob lem , probability based kernels are studied for more theoretically well founded methods Jaakkola and Haussler , 1998 ; Tsuda et al , 2001 ; Shimodaira et al . , 2001 .", "Our SVM based NE recognizer attained F 90 . 03 .", "This is the best score , as far as we know .", "Since it was too slow , we made SVMs faster .", "The improved classifier is 21 times faster than TinySVMand 102 times faster than SVM Light .", "The im proved training program is 2 . 3 times faster than TinySVM and 3 . 5 times faster than SVM Light .", "We also presented an SVM based feature selectionmethod that removed 75 of features .", "These methods can also be applied to other tasks such as chunk ing and POS tagging .", "AcknowledgmentWe would like to thank Yutaka Sasaki for the training data .", "We thank members of Knowledge Pro cessing Research Group for valuable comments and discussion .", "We also thank Shigeru Katagiri and Ken ichiro Ishii for their support ."], "summary_lines": ["Efficient Support Vector Classifiers For Named Entity Recognition\n", "Named Entity (NE) recognition is a task in which proper nouns and numerical information are extracted from documents and are classified into categories such as person, organization, and date.\n", "It is a key technology of Information Extraction and Open-Domain Question Answering.\n", "First, we show that an NE recognizer based on Support Vector Machines (SVMs) gives better scores than conventional systems.\n", "However, off-the-shelf SVM classifiers are too inefficient for this task.\n", "Therefore, we present a method that makes the system substantially faster.\n", "This approach can also be applied to other similar tasks such as chunking and part-of-speech tagging.\n", "We also present an SVM-based feature selection method and an efficient training method.\n", "We propose Kernel Expansion that is used to transform the d-degree polynomial kernel based classifier into a linear one, with a modified decision function.\n", "We propose an XQK (eXpand the Quadratic Kernel) which can make their Named-Entity recognizer drastically fast.\n"]}
{"article_lines": ["Evaluating Discourse Processing Algorithms", "In order to take steps towards establishing a methodology for evaluating Natural Language systems , we conducted a case study .", "We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co specifiers of pronouns in naturally occurring texts and dialogues .", "We present the quantitative results of handsimulating these algorithms , but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general .", "We illustrate the general difficulties encountered with quantitative evaluation .", "These are problems with a allowing for underlying assumptions , b determining how to handle underspecifications , and c evaluating the contribution of false positives and error chaining .", "In the course of developing natural language interfaces , computational linguists are often in the position of evaluating different theoretical approaches to the analysis of natural language NL .", "They might want to a evaluate and improve on a current system , b add a capability to a system that it didn't previously have , c combine modules from different systems .", "Consider the goal of adding a discourse component to a system , or evaluating and improving one that is already in place .", "A discourse module might combine theories on , e . g . , centering or local focusing GJW83 , Sid79 , global focus Gro77 , coherence relations Hob85 , event' reference Web86 , intonational structure PH87 , system vs . user beliefs Po186 , plan or intent recognition or production Coh78 , AP86 , SI81 , control WS88 , or complex syntactic structures Pri85 .", "How might one evaluate the relative contributions of each of these factors or compare two approaches to the same problem ?", "In order to take steps towards establishing a methodology for doing this type of comparison , we conducted a case study .", "We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the cospecifiers of pronouns in naturally occurring texts and dialogues Hob76b , BFP87 .", "Thus there are two parts to this paper we present the quantitative results of hand simulating these algorithms henceforth Hobbs algorithm and BFP algorithm , but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general .", "We illustrate the general difficulties encountered with quantitative evaluation .", "These are problems with a allowing for underlying assumptions , b determining how to handle underspecifications , and c evaluating the contribution of false positives and error chaining .", "Although both algorithms are part of theories of discourse that posit the interaction of the algorithm with an inference or intentional component , we will not use reasoning in tandem with the algorithm's operation .", "We have made this choice because we want to be able to analyse the performance of the algorithms across different domains .", "We focus on the linguistic basis of these approaches , using only selectional restrictions , so that our analysis is independent of the vagaries of a particular knowledge representation .", "Thus what we are evaluating is the extent to which these algorithms suffice to narrow the search of an inference component' .", "This analysis gives us 'But note the definition of success in section 2 . 1 . some indication of the contribution of syntactic constraints , task structure and global focus to anaphoric processing .", "The data on which we compare the algorithms are important if we are to evaluate claims of generality .", "If we look at types of NL input , one clear division is between textual and interactive input .", "A related , though not identical factor is whether the language being analysed is produced by more than one person , although this distinction may be conflated in textual material such as novels that contain reported conversations .", "Within two person interactive dialogues , there are the task oriented masterslave type , where all the expertise and hence much of the initiative , rests with one person .", "In other twoperson dialogues , both parties may contribute discourse entities to the conversation on a more equal basis .", "Other factors of interest are whether the dialogues are human to human or human to computer , as well as the modality of communication , e . g . spoken or typed , since some researchers have indicated that dialogues , and particularly uses of reference within them , vary along these dimensions Coh84 , Tho80 , GSBC86 , D389 , WS89 .", "We analyse the performance of the algorithms on three types of data .", "Two of the samples are those that Hobbs used when developing his algorithm .", "One is an excerpt from a novel and the other a sample of journalistic writing .", "The remaining sample is a set of 5 human human , keyboard mediated , task oriented dialogues about the assembly of a plastic water pump Coh84 .", "This covers only a subset of the above types .", "Obviously it would be instructive to conduct a similar analysis on other textual types .", "When embarking on such a comparison , it would be convenient to assume that the inputs to the algorithms are identical and compare their outputs .", "Unfortunately since researchers do not even agree on which phenomena can be explained syntactically and which semantically , the boundaries between two modules are rarely the same in NL systems .", "In this case the BFP centering algorithm and Hobbs algorithm both make ASSUMPTIONS about other system components .", "These are , in some sense , a further specification of the operation of the algorithms that must be made in order to hand simulate the algorithms .", "There are two major sets of assumptions , based on discourse segmentation and syntactic representation .", "We attempt to make these explicit for each algorithm and pinpoint where the algorithms might behave differently were these assumptions not well founded .", "In addition , there may be a number of UNDERSPECIFICATIONS in the descriptions of the algorithms .", "These often arise because theories that attempt to categorize naturally occurring data and algorithms based on them will always be prey to previously unencountered examples .", "For example , since the BFP salience hierarchy for discourse entities is based on grammatical relation , an implicit assumption is that an utterance only has one subject .", "However the novel Wheels has many examples of reported dialogue such as She continued , unperturbed , quot ; Mr . Vale quotes the Bible about air pollution . quot ; One might wonder whether the subject is She or Mr . Vale .", "In some cases , the algorithm might need to be further specificied in order to be able to process any of the data , whereas in others they may just highlight where the algorithm needs to be modified see section 3 . 2 .", "In general we count underspecifications as failures .", "Finally , it may not be clear what the DEFINITION OF SUCCESS is .", "In particular it is not clear what to do in those cases where an algorithm produces multiple or partial interpretations .", "In this situation a system might flag the utterance as ambiguous and draw in support from other discourse components .", "This arises in the present analysis for two reasons 1 the constraints given by GJW86 do not always allow one to choose a preferred interpretation , 2 the BFP algorithm proposes equally ranked interpretations in parallel .", "This doesn't happen with the Hobbs algorithm because it proposes interpretations in a sequential manner , one at a time .", "We chose to count as a failure those situations in which the BFP algorithm only reduces the number of possible interpretations , but Hobbs algorithm stops with a correct interpretation .", "This ignores the fact that Hobbs may have rejected a number of interpretations before stopping .", "We also have not needed to make a decision on how to score an algorithm that only finds one interpretation for an utterance that humans find ambiguous .", "The centering algorithm as defined by Brennan , Friedman and Pollard , BFP algorithm , is derived from a set of rules and constraints put forth by Grosz , Josh i and Weinstein GJW83 , 0JW86 .", "We shall not reproduce this algorithm here See BFP87 .", "There are two main structures in the centering algorithm , the CB , the BACKWARD LOOKING CENTER , which is what the discourse is 'about' , and an ordered list , CF , of FORWARD LOOKING CENTERS , which are the discourse entities available to the next utterance for pronominalization .", "The centering framework predicts that in a local coherent stretch of dialogue , speakers will prefer to CONTINUE talking about the same discourse entity , that the CB will be the highest ranked entity of the previous utterance's forward centers that is realized in the current utterance , and that if anything is pronominalized the CB must be .", "In the centering framework , the order of the forward centers list is intended to reflect the salience of discourse entities .", "The BFP algorithm orders this list by grammatical relation of the complements of the main verb , i . e . first the subject , then object , then indirect object , then other subcategorized for complements , then noun phrases found in adjunct clauses .", "This captures the intuition that subjects are more salient than other discourse entities .", "The BFP algorithm added linguistic constraints on CONTRA INDEXING to the centering framework .", "These constraints are exemplified by the fact that , in the sentence he likes him , the entity cospecified by he cannot be the same as that cospecified by him .", "We say that he and him are CONTRA INDEXED .", "The BFP algorithm depends on semantic processing to precompute these constraints , since they are derived from the syntactic structure , and depend on some notion of c command Rei76 .", "The other assumption that is dependent on syntax is that the the representations of discourse entities can be marked with the grammatical function through which they were realized , e . g . subject .", "The BFP algorithm assumes that some other mechanism can structure both written texts and taskoriented dialogues into hierarchical segments .", "The present concern is not with whether there might be a grammar of discourse that determines this structure , or whether it is derived from the cues that cooperative speakers give hearers to aid in processing .", "Since centering is a local phenomenon and is intended to operate within a segment , we needed to deduce a segmental structure in order to analyse the data .", "Speaker's intentions , task structure , cue words like O . K . now . . , intonational properties of utterances , coherence relations , the scoping of modal operators , and mechanisms for shifting control between discourse participants have all been proposed as ways of determining discourse segmentation Gro77 , GSSG , Rei85 , PH87 , HL87 , Hob78 , Hob85 , Rob88 , WS88 .", "Here , we use a combination of orthography , anaphora distribution , cue words and task structure .", "The rules are BFP never state that cospecifiers for pronouns within the same segment are preferred over those in previous segments , but this is an implicit assumption , since this line of research is derived from Sidner's work on local focusing .", "Segment initial utterances therefore are the only situation where the BFP algorithm will prefer a within sentence noun phrase as the cospecifier of a pronoun .", "The Hobbs algorithm is based on searching for a pronoun's co specifier in the syntactic parse tree of input sentences Hob76b .", "We reproduce this algorithm in full in the appendix along with an example .", "Hobbs algorithm operates on one sentence at a time , but the structure of previous sentences in the discourse is available .", "It is stated in terms of searches on parse trees .", "When looking for an intrasentential antecedent , these searches are conducted in a left toright , breadth first manner .", "However , when looking for a pronoun's antecedent within a sentence , it will go sequentially further and further up the tree to the left of the pronoun , and that failing will look in the previous sentence .", "Hobbs does not assume a segmentation of discourse structure in this algorithm ; the algorithm will go back arbitrarily far in the text to find an antecedent .", "In more recent work , Hobbs uses the notion of COHERENCE RELATIONS to structure the discourse HM87 .", "The order by which Hobbs' algorithm traverses the parse tree is the closest thing in his framework to predictions about which discourse entities are salient .", "In the main it prefers co specifiers for pronouns that are within the same sentence , and also ones that are closer to the pronoun in the sentence .", "This amounts to a claim that different discourse entities are salient , depending on the position of a pronoun in a sentence .", "When seeking an intersentential cospecification , Hobbs algorithm searches the parse tree of the previous utterance breadth first , from left to right .", "This predicts that entities realized in subject position are more salient , since even if an adjunct clause linearly precedes the main subject , any noun phrases within it will be deeper in the parse tree .", "This also means that objects and indirect objects will be among the first possible antecedents found , and in general that the depth of syntactic embedding is an important determiner of discourse prominence .", "Miming to the assumptions about syntax , we note that Hobbs assumes that one can produce the correct syntactic structure for an utterance , with all adjunct phrases attached at the proper point of the parse tree .", "In addition , in order to obey linguistic constraints on coreference , the algorithm depends on the existence of a N parse tree node , which denotes a noun phrase without its determiner See the example in the Appendix .", "Hobbs algorithm procedurally encodes contra indexing constraints by skipping over NP nodes whose N node dominates the part of the parse tree in which the pronoun is found , which means that he cannot guarantee that two contraindexed pronouns will not choose the same NP as a co specifier .", "Hobbs also assumes that his algorithm can somehow collect discourse entities mentioned alone into sets as co specifiers of plural anaphors .", "Hobbs discusses at length other assumptions that he makes about the capabilities of an interpretive process that operates before the algorithm Hob761 .", "This includes such things as being able to recover syntactically recoverable omitted text , such as elided verb phrases , and the identities of the speakers and hearers in a dialogue .", "A major component of any discourse algorithm is the prediction of which entities are salient , even though all the factors that contribute to the salience of a discourse entity have not been identified Pri81 , Pri85 , BF83 , HTD86 .", "So an obvious question is when the two algorithms actually make different predictions .", "The main difference is that the choice of a co specifier for a pronoun in the Hobbs algorithm depends in part on the position of that pronoun in the sentence .", "In the centering framework , no matter what criteria one uses to order the forward centers list , pronouns take the most salient entities as antecedents , irrespective of that pronoun's position .", "Hobbs ordering of entities from a previous utterance varies from BFP in that possessors come before case marked objects and indirect objects , and there may be some other differences as well but none of them were relevant to the analysis that follows .", "The effects ot some of the assumptions are measurable and we will attempt to specify exactly what these effects are , however some are not , e . g . we cannot measure the effect of Hobbs' syntax assumption since it is difficult to say how likely one is to get the wrong parse .", "We adopt the set collection assumption for both algorithms as well as the ability to recover the identity of speakers and hearers in dialogue .", "The texts on which the algorithms are analysed are the first chapter of Arthur Hailey's novel Wheels , and the July 7 , 1975 edition of Newsweek .", "The sentences in Wheels are short and simple with long sequences consisting of reported conversation , so it is similar to a conversational text .", "The articles from Newsweek are typical of journalistic writing .", "For each text , the first 100 occurrences of singular and plural thirdperson pronouns were used to test the performance of the algorithms .", "The task dialogues contain a total of 81 uses of it and no other pronouns except for I and you .", "In the figures below note that possessives like his are counted along with he and that accusatives like him and her are counted as he and she2 .", "We performed three analyses on the quantitative results .", "A comparison of the two algorithms on each data set individually and an overall analysis on the three data sets combined revealed no significant differences in the performance of the two algorithms x2 3 . 25 , not significant .", "In addition for each algorithm alone we tested whether there were significant differences in performance for different textual types .", "Both of the algorithms performed significantly worse on the task dialogues x2 22 . 05 for Hobbs , x2 21 . 55 for BFP , p 0 . 05 .", "We might wonder with what confidence we should view these numbers .", "A significant factor that must be considered is the contribution of FALSE POSITIVES and ERROR CHAINING .", "A FALSE POSITIVE is when an algorithm gets the right answer for the wrong reason .", "A very simple example of this phenomena is illustrated by this sequence from one of the task dialogues .", "The first it in Expi refers to the pump .", "Hobbs algorithm gets the right antecedent for it in Exp3 , which is the little handle , but then fails on it in Exp4 , whereas the BFP algorithm has the pump centered at Expi and continues to select that as the antecedent for it throughout the text .", "This means BFP gets the wrong co specifier in Exp3 but this error allows it to get the correct co specifier in Exp4 .", "Another type of false positive example is quot ; Everybody and HIS brother suddenly wants to be the President's friend , quot ; said one aide .", "Hobbs gets this correct as long as one is willing to accept that Everybody is really the antecedent of his .", "It seems to me that this might be an idiomatic use .", "ERROR CHAINING refers to the fact that once an algorithm makes an error , other errors can result .", "Consider Sorry no luck .", "Expi I bet IT's the stupid red thing .", "Exp2 Take IT out .", "Cli2 Ok .", "IT is stuck .", "In this example once an algorithm fails at Expi it will fail on Exp2 and Cli2 as well since the choices of a cospecifier in the following examples are dependent on the choice in Expi .", "It isn't possible to measure the effect of false positives , since in some sense they are subjective judgements .", "However one can and should measure the effects of error chaining , since reporting numbers that correct for error chaining is misleading , but if the error that produced the error chain can be corrected then the algorithm might show a significant improvement .", "In this analysis , error chains contributed 22 failures to Hobbs' algorithm and 19 failures to BFP .", "The numbers presented in the previous section are intuitively unsatisfying .", "They tell us nothing about what makes the algorithms more or less general , or how they might be improved .", "In addition , given the assumptions that we needed to make in order to produce them , one might wonder to what extent the data is a result of these assumptions .", "Figure 1 also fails to indicate whether the two algorithms missed the same examples or are covering a different set of phenomena , i . e . what the relative distribution of the successes and failures are .", "But having done the hand simulation in order to produce such numbers , all of this information is available .", "In this section we will first discuss the relative importance of various factors that go into producing the numbers above , then discuss if the algorithms can be modified since the flexibility of a framework in allowing one to make modifications is an important dimension of evaluation .", "The figures 2 , 3 and 4 show for each pronominal category , the distribution of successes and failures for both algorithms .", "Since the main purpose of evaluation must be to improve the theory that we are evaluating , the most interesting cases are the ones on which the algorithms' performance varies and those that neither algorithm gets correct .", "We discuss these below .", "In the Wheels data , 4 examples rest on the assumption that the identities of speakers and hearers is recoverable .", "For example in The GM president smiled .", "quot ; Except Henry will be damned forceful and the papers won't print all HIS language . quot ; , getting the his correct here depends on knowing that it is the GM president speaking .", "Only 4 examples rest on being able to produce collections or discourse entities , and 2 of these occurred with an explicit instruction to the hearer to produce such a collection by using the phrase them both .", "There are 21 cases that Hobbs gets that BFP don't , and of these these a few classes stand out .", "In every case the relevant factor is Hobbs' preference for intrasentential co specifiers .", "One class , n 3 , is exemplified by Put the little black ring into the the large blue CAP with the hole in IT .", "All three involved using the preposition with in a descriptive adjunct on a noun phrase .", "It may be that with adjuncts are common in visual descriptions , since they were only found in our data in the task dialogues , and a quick inspection of Gross's task oriented dialogues revealed some as well Deu74 .", "Another class , n 7 , are possessives .", "In some cases the possessive co specified with the subject of the sentence , e . g .", "The SENATE took time from ITS paralyzing New Hampshire election debate to vote agreement , and in others it was within a relative clause and co specified with the subject of that clause , e . g .", "The auto industry should be able to produce a totally safe , defect free CAR that doesn't pollute ITS environment .", "Other cases seem to be syntactically marked subject matching with constructions that link two S clauses n 8 .", "These are uses of more than in e . g . but Chamberlain grossed about 8 . 3 million morethan HE could have made by selling on the home front .", "There also are S if S cases , as in Mondale said quot ; I think THE MAFIA would be broke if IT conducted all its business that way . quot ; We also have subject matching in AS AS examples as in . . . and the resulting EXPOSURE to daylight has become as uncomfortable as IT was unaccustomed , as well as in sentential complements , such as But another liberal , Minnesota's Walter MONDALE , said HE had found a lot of incompetence in the agency's operations .", "The fact that quite a few of these are also marked with But may be significant .", "In terms of the possible effects that we noted earlier , the DEFINITION OF SUCCESS see section 2 . 1 favors Hobbs n 2 .", "Consider K Next take the red piece that is the smallest and insert it into the hole in the side of the large plastic tube .", "IT goes in the hole nearest the end with the engravings on IT .", "The Hobbs algorithm will correctly choose the end as the antecedent for the second it .", "The BFP algorithm on the other hand will get two interpretations , one in which the second it co specifies the red piece and one in which it co specifies the end .", "They are both CONTINUING interpretations since the first it co specifies the CB , but the constraints don't make a choice .", "All of the examples on which BFP succeed and Hobbs fails have to do with extended discussion of one discourse entity .", "For instance Expi Now take the blue cap with the two prongs sticking out Cu blue cap Exp2 and fit the little piece of pink plastic on IT .", "Ok ?", "Cs blue cap Clii ok . Exp3 Insert the rubber ring into that blue cap .", "Ca blue cap Exp4 Now screw IT onto the cylinder .", "On this example , Hobbs fails by choosing the cospecifier of it in Exp4 to be the rubber ring , even 2 5 6 though the whole segment has been about the blue cap .", "Another example from the novel WHEELS is given below .", "On this one Hobbs gets the first use of he but then misses the next four , as a result of missing the second one by choosing a housekeeper as the cospecifier for HIS .", ". . An executive vice president of Ford was preparing to leave for Detroit Metropolitan Airport .", "HE had already breakfasted , alone .", "A housekeeper had brought a tray to HIS desk in the softly lighted study where , since 5 a . m . , HE had been alternately reading memoranda mostly on special blue stationery which Ford vice presidents used in implementing policy and dictating crisp instructions into a recording machine .", "HE had scarcely looked up , either as the mail arrived , or while eating , as HE accomplished in an hour what would have taken . . .", "Since an executive vice president is centered in the first sentence , and continued in each following sentence , the BFP algorithm will correctly choose the cospecifier .", "Among the examples that neither algorithm gets correctly are 20 examples from the task dialogues of it referring to the global focus , the pump .", "In 15 cases , these shifts to global focus are marked syntactically with a cue word such as Now , and are not marked in 5 cases .", "Presumably they are felicitous since the pump is visually salient .", "Besides the global focus cases , pronominal references to entities that were not linguistically introduced are rare .", "The only other example is an implicit reference to 'the problem' of the pump not working Clii Sorry no luck .", "Expi I bet IT's the stupid red thing .", "We have only two examples of sentential or VP anaphora altogether , such as Madam Chairwoman , said Colby at last , I am trying to run a secret intelligence service .", "IT was a forlorn hope .", "Neither Hobbs algorithm nor BFP attempt to cover these examples .", "Three of the examples are uses of it that seem to be lexicalized with certain verbs , e . g .", "They hit IT off real well .", "One can imagine these being treated as phrasal lexical items , and therefore not handled by an anaphoric processing component AS89 .", "Most of the interchanges in the task dialogues consist of the client responding to commands with cues such as O . K . or Ready to let the expert know when they have completed a task .", "When both parties contribute discourse entities to the common ground , both algorithms may fail n 4 .", "Consider Expi Now we have a little red piece left Exp2 and I don't know what to do with IT .", "CHI Well , there is a hole in the green plunger inside the cylinder .", "Exp3 I don't think IT goes in THERE .", "Exp4 I think IT may belong in the blue cap onto which you put the pink piece of plastic .", "In Exp3 , one might claim that it and there are contraindexed , and that there can be properly resolved to a hole , so that it cannot be any of the noun phrases in the prepositional phrases that modify a hole , but whether any theory of contra indexing actually give us this is questionable .", "The main factor seems to be that even though Expi is not syntactically a question , the little red piece is the focus of a question , and as such is in focus despite the fact that the syntactic construction there is supposedly focuses a hole in the green plunger . . . Sid79 .", "These examples suggest that a questioned entity is left focused until the point in the dialogue at which the question is resolved .", "The fact that well has been noted as a marker of response to questions supports this analysis Sch87 .", "Thus the relevant factor here may be the switching of control among discourse participants WS88 .", "These mixed initiative features make these sequences inherently different than text .", "Task structure in the pump dialogues is an important factor especially as it relates to the use of global focus .", "Twenty of the cases on which both algorithms fail are references to the pump , which is the global focus .", "We can include a global focus in the centering framework , as a separate notion from the current CB .", "This means that in the 15 out of 20 cases where the shift to global focus is identifiably marked with a cue word such as now , the segment rules will allow BFP to get the global focus examples . forward centers list , as Sidner does in her algorithm for local focusing Sid79 .", "This lets BFP get the two examples of event anaphora .", "Hobbs discusses the fact that his algorithm cannot be modified to get event anaphora in Hob76b .", "Another interesting fact is that in every case in which Hobbs' algorithm gets the correct co specifier and BFP didn't , the relevant factor is Hobbs' preference for intrasentential co specifiers .", "One view on these cases may be that these are not discourse anaphora , but there seems to be no principled way to make this distinction .", "However , Carter has proposed some extensions to Sidner's algorithm for local focusing that seem to be relevant here chap .", "6 , Car87 .", "He argues that intra sentential candidates ISCs should be preferred over candidates from the previous utterance , ONLY in the cases where no discourse center has been established or the discourse center is rejected for syntactic or selectional reasons .", "He then uses Hobbs algorithm to produce an ordering of these ISCs .", "This is compatible with the centering framework since it is underspecified as to whether one should always choose to establish a discourse center with a co specifier from a previous utterance .", "If we adopt Carter's rule into the centering framework , we find that of the 21 cases that Hobbs gets that BFP don't , in 7 cases there is no discourse center established , and in another 4 the current center can be rejected on the basis of syntactic or sortal information .", "Of these Carter's rule clearly gets 5 , and another 3 seem to rest on whether one might want to establish a discourse entity from a previous utterance .", "Since the addition of this constraint does not allow BFP to get any examples that neither algorithm got , it seems that this combination is a way of making the best out of both algorithms .", "The addition of these modifications changes the quantitative results .", "See the Figure 5 .", "However , the statistical analyses still show that there is no significant difference in the performance of the algorithms in general .", "It is also still the case that the performance of each algorithm significantly varies depending on the data .", "The only significant difference as a result of the modifications is that the BFP algorithm now performs significantly better on the pump dialogues alone x2 4 . 31 , p . 05 .", "We can benefit in two ways from performing such evaluations a we get general results on a methodology for doing evaluation , b we discover ways we can improve current theories .", "A split of evaluation efforts into quantitative versus qualitative is incoherent .", "We cannot trust the results of a quantitative evaluation without doing a considerable amount of qualitative analyses and we should perform our qualitative analyses on those components that make a significant contribution to the quantitative results ; we need to be able to measure the effect of various factors .", "These measurements must be made by doing comparisons at the data level .", "In terms of general results , we have identified some factors that make evaluations of this type more complicated and which might lead us to evaluate solely quantitative results with care .", "These are a To decide how to evaluate UNDERSPECIFICATIONS and the contribution of ASSUMPTIONS , and b To determine the effects of FALSE POSITIVES and ERROR CHAINING .", "We advocate an approach in which the contribution of each underspecification and assumption is tabulated as well as the effect of error chains .", "If a principled way could be found to identify false positives , their effect should be reported as well as part of any quantitative evaluation .", "In addition , we have taken a few steps towards determining the relative importance of different factors to the successful operation of discourse modules .", "The percent of successes that both algorithms get indicates that syntax has a strong influence , and that at the very least we can reduce the amount of inference required .", "In 59 to 82 of the cases both algorithms get the correct result .", "This probably means that in a large number of cases there was no potential conflict of co specifiers .", "In addition , this analysis has shown , that at least for task oriented dialogues global focus is a significant factor , and in general discourse structure is more important in the task dialogues .", "However simple devices such as cue words may go a long way toward determining this structure .", "Finally , we should note that doing evaluations such as this allows us to determine the GENERALITY of our approaches .", "Since the performance of both Hobbs and BFP varies according to the type of the text , and in fact was significantly worse on the task dialogues than on the texts , we might question how their performance would vary on other inputs .", "An annotated corpus comprising some of the various NL input types such as those I discussed in the introduction would go a long way towards giving us a basis against which we could evaluate the generality of our theories .", "David Carter , Phil Cohen , Nick Haddock , Jerry Hobbs , Aravind Joshi , Don Knuth , Candy Sidner , Phil Stenton , Bonnie Webber , and Steve Whittaker have provided valuable insights toward this endeavor and critical comments on a multiplicity of earlier versions of this paper .", "Steve Whittaker advised me on the statistical analyses .", "I would like to thank Jerry Hobbs for encouraging me to do this in the first place .", "ers plans .", "In Proc .", "International Joint Conference on Artificial Intelligence , pages 203 208 , Vancouver , BC , Canada , 1981 .", "Sid79 Candace L . Sidner .", "Toward a computational theory of definite anaphora comprehension in English .", "Technical Report AITR 537 , MIT , 1979 ."], "summary_lines": ["Evaluating Discourse Processing Algorithms\n", "In order to take steps towards establishing a methodology for evaluating Natural Language systems, we conducted a case study.\n", "We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues.\n", "We present the quantitative results of hand-simulating these algorithms, but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general.\n", "We illustrate the general difficulties encoun- tered with quantitative evaluation.\n", "These are problems with: (a) allowing for underlying assumptions, (b) determining how to handle underspecifications, and (c) evaluating the contribution of false positives and error chaining.\n", "We refer to error chaining as the case in which a pronoun x correctly says that it is coreferent with another pronoun y while the program misidentifies the antecedent of y.\n"]}
{"article_lines": ["What is the Jeopardy Model ? A Quasi Synchronous Grammar for QA", "This paper presents a syntax driven ap proach to question answering , specifically the answer sentence selection problem forshort answer questions .", "Rather than using syntactic features to augment exist ing statistical classifiers as in previouswork , we build on the idea that ques tions and their correct answers relate toeach other via loose but predictable syntactic transformations .", "We propose a probabilistic quasi synchronous grammar , inspired by one proposed for machine translation D . Smith and Eisner , 2006 , and parameterized by mixtures of a robust non lexical syntax alignment model with a n optional lexical semantics driven log linear model .", "Our model learns soft alignments as a hidden variable in discriminative training .", "Experimental results using the TREC dataset are shown to significantly outperform strong state of the art baselines .", "Open domain question answering QA is a widelystudied and fast growing research problem .", "State of the art QA systems are extremely complex .", "They usually take the form of a pipeline architecture , chaining together modules that perform tasks such as answer type analysis identifying whether the correct answer will be a person , location , date , etc . , document retrieval , answer candidate extrac tion , and answer reranking .", "This architecture is so predominant that each task listed above has evolvedinto its own sub field and is often studied and evalu ated independently Shima et al , 2006 .", "At a high level , the QA task boils down to only two essential steps Echihabi andMarcu , 2003 .", "The first step , retrieval , narrows down the search spacefrom a corpus of millions of documents to a focused set of maybe a few hundred using an IR engine , where efficiency and recall are the main focus .", "The second step , selection , assesses each can didate answer string proposed by the first step , and finds the one that is most likely to be an answerto the given question .", "The granularity of the tar get answer string varies depending on the type ofthe question .", "For example , answers to factoid ques tions e . g . , Who , When , Where are usually single words or short phrases , while definitional questions and other more complex question types e . g . , How , Why look for sentences or short passages .", "In this work , we fix the granularity of an answer to a single sentence .", "Earlier work on answer selection relies only on the surface level text information .", "Two approaches are most common surface pattern matching , andsimilarity measures on the question and answer , represented as bags of words .", "In the former , pat terns for a certain answer type are either craftedmanually Soubbotin and Soubbotin , 2001 or acquired from training examples automatically Itty cheriah et al , 2001 ; Ravichandran et al , 2003 ; Licuanan and Weischedel , 2003 .", "In the latter , measures like cosine similarity are applied to usu ally bag of words representations of the question and answer .", "Although many of these systems haveachieved very good results in TREC style evaluations , shallow methods using the bag of word repre sentation clearly have their limitations .", "Examples of 22 cases where the bag of words approach fails abound in QA literature ; here we borrow an example used by Echihabi and Marcu 2003 .", "The question is ? Who is the leader of France ? ? , and the sentence ? Henri Hadjenberg , who is the leader of France ? s Jewish community , endorsed . . . ?", "note tokenization , which is not the correct answer , matches all keywords inthe question in exactly the same order .", "The cor rect answer is found in ? Bush later met with French President Jacques Chirac . ?", "This example illustrates two types of variation that need to be recognized in order to connect this question answer pair .", "The first variation is thechange of the word ? leader ?", "to its semantically re lated term ? president ? .", "The second variation is the syntactic shift from ? leader of France ?", "to ? French president . ?", "It is also important to recognize that ? France ?", "in the first sentence is modifying ? com munity ? , and therefore ? Henri Hadjenberg ?", "is the ? leader of . . . community ?", "rather than the ? leader ofFrance . ?", "These syntactic and semantic variations occur in almost every question answer pair , and typi cally they cannot be easily captured using shallow representations .", "It is also worth noting that such syntactic and semantic variations are not unique to QA ; they can be found in many other closely related NLP tasks , motivating extensive community efforts in syntactic and semantic processing .", "Indeed , in this work , we imagine a generative story for QA in which the question is generatedfrom the answer sentence through a series of syn tactic and semantic transformations .", "The same story has been told for machine translation Yamada and Knight , 2001 , inter alia , in which a target languagesentence the desired output has undergone seman tic transformation word to word translation and syntactic transformation syntax divergence acrosslanguages to generate the source language sen tence noisy channel model .", "Similar stories can also be found in paraphrasing Quirk et al , 2004 ; Wu , 2005 and textual entailment Harabagiu and Hickl , 2006 ; Wu , 2005 .", "Our story makes use of a weighted formalism known as quasi synchronous grammar hereafter , QG , originally developed by D . Smith and Eisner 2006 for machine translation .", "Unlike most synchronous formalisms , QG does not posit a strict iso morphism between the two trees , and it providesan elegant description for the set of local configura tions .", "In Section 2 we situate our contribution in thecontext of earlier work , and we give a brief discus sion of quasi synchronous grammars in Section 3 .", "Our version of QG , called the Jeopardy model , and our parameter estimation method are described inSection 4 .", "Experimental results comparing our ap proach to two state of the art baselines are presented in Section 5 .", "We discuss portability to cross lingual QA and other applied semantic processing tasks in Section 6 .", "To model the syntactic transformation process , re searchers in these fields ? especially in machine translation ? have developed powerful grammatical formalisms and statistical models for representing and learning these tree to tree relations Wu and Wong , 1998 ; Eisner , 2003 ; Gildea , 2003 ; Melamed , 2004 ; Ding and Palmer , 2005 ; Quirk et al , 2005 ; Galley et al , 2006 ; Smith and Eisner , 2006 , in ter alia .", "We can also observe a trend in recent work in textual entailment that more emphasis is put onexplicit learning of the syntactic graph mapping between the entailed and entailed by sentences Mac Cartney et al , 2006 .", "However , relatively fewer attempts have been made in the QA community .", "As pointed out by Katz and Lin 2003 , most early experiments in QA that tried to bring in syntactic or semantic features showed little or no improvement , and itwas often the case that performance actually de graded Litkowski , 1999 ; Attardi et al , 2001 .", "Morerecent attempts have tried to augment the bag of words representation ? which , after all , is simply a real valued feature vector ? with syntactic features .", "The usual similarity measures can then be used onthe new feature representation .", "For example , Punyakanok et al 2004 used approximate tree match ing and tree edit distance to compute a similarity score between the question and answer parse trees . Similarly , Shen et al 2005 experimented with de pendency tree kernels to compute similarity between parse trees .", "Cui et al 2005 measured sentencesimilarity based on similarity measures between de pendency paths among aligned words .", "They used heuristic functions similar to mutual information to 23 assign scores to matched pairs of dependency links .", "Shen and Klakow 2006 extend the idea furtherthrough the use of log linear models to learn a scor ing function for relation pairs . Echihabi and Marcu 2003 presented a noisy channel approach in which they adapted the IBM model 4 from statistical machine translation Brown et al , 1990 ; Brown et al , 1993 and applied it to QA . Similarly , Murdock and Croft 2005 adopted a sim ple translation model from IBM model 1 Brown et al . , 1990 ; Brown et al , 1993 and applied it to QA . Porting the translation model to QA is not straight forward ; it involves parse tree pruning heuristics the first two deterministic steps in Echihabi andMarcu , 2003 and also replacing the lexical trans lation table with a monolingual ? dictionary ?", "which simply encodes the identity relation .", "This brings usto the question that drives this work is there a statistical translation like model that is natural and accu rate for question answering ?", "We propose Smith andEisner ? s 2006 quasi synchronous grammar Sec tion 3 as a general solution and the Jeopardy model Section 4 as a specific instance .", "For a formal description of QG , we recommendSmith and Eisner 2006 .", "We briefly review the central idea here .", "QG arose out of the empirical observation that translated sentences often have some isomorphic syntactic structure , but not usually in en tirety , and the strictness of the isomorphism may vary across words or syntactic rules .", "The idea is that , rather than a synchronous structure over the source and target sentences , a tree over the target sentence is modeled by a source sentence specific grammar that is inspired by the source sentence ? s tree . 1 This is implemented by a ? sense ? ? really just a subsetof nodes in the source tree ? attached to each gram mar node in the target tree .", "The senses define an alignment between the trees .", "Because it only loosely links the two sentences ?", "syntactic structure , QG is particularly well suited for QA insofar as QA is like ? free ?", "translation .", "A concrete example that is easy to understand is a binary quasi synchronous context free grammar 1Smith and Eisner also show how QG formalisms generalize synchronous grammar formalisms . denoted QCFG .", "Let VS be the set of constituent to kens in the source tree .", "QCFG rules would take the augmented form ? X , S1 ?", "? Y , S2 ? ? Z , S3 ?", "? X , S1 ?", "w where X , Y , and Z are ordinary CFG nonterminals , each Si ? 2VS subsets of nodes in the source treeto which the nonterminals align , and w is a targetlanguage word .", "QG can be made more or less ? lib eral ?", "by constraining the cardinality of the Si weforce all Si 1 , and by constraining the relation ships among the Si mentioned in a single rule .", "These are called permissible ? configurations . ?", "An example of a strict configuration is that a target parent childpair must align respectively to a source parent child pair .", "Configurations are shown in Table 1 .", "Here , following Smith and Eisner 2006 , we usea weighted , quasi synchronous dependency grammar .", "Apart from the obvious difference in appli cation task , there are a few important differences with their model .", "First , we are not interested in thealignments per se ; we will sum them out as a hid den variable when scoring a question answer pair .", "Second , our probability model includes an optional mixture component that permits arbitrary features ? we experiment with a small set of WordNet lexicalsemantics features see Section 4 . 4 .", "Third , we apply a more discriminative training method condi tional maximum likelihood estimation , Section 4 . 5 .", "Our model , informally speaking , aims to follow theprocess a player of the television game show Jeop ardy !", "might follow .", "The player knows the answer or at least thinks he knows the answer and must quickly turn it into a question . 2 The question answer pairs used on Jeopardy !", "are not precisely what we have in mind for the real task the questions are notspecific enough , but the syntactic transformation in spires our model .", "In this section we formally define 2A round of Jeopardy !", "involves a somewhat involved and specific ? answer ?", "presented to the competitors , and the first competitor to hit a buzzer proposes the ? question ?", "that leads tothe answer .", "For example , an answer might be , This Eastern Eu ropean capital is famous for defenestrations .", "In Jeopardy !", "the players must respond with a queston What is Prague ?", "24this probability model and present the necessary al gorithms for parameter estimation .", "4 . 1 Probabilistic Model .", "The Jeopardy model is a QG designed for QA .", "Let q ? q1 , . . . , qn ?", "be a question sentence each qi is aword , and let a ? a1 , . . . , am ?", "be a candidate an swer sentence .", "We will use w to denote an abstract sequence that could be a question or an answer .", "Inpractice , these sequences may include other infor mation , such as POS , but for clarity we assume justwords in the exposition .", "Let A be the set of can didate answers under consideration .", "Our aim is to choose a ?", "argmax a ? A p a q 1 At a high level , we make three adjustments .", "The first is to apply Bayes ?", "rule , p a q ? p q a ? p a .", "Because A is known and is assumed to be generated by an external extraction system , we could use that extraction system to assign scores and hence , probabilities p a to the candidate an swers .", "Other scores could also be used , such as reputability of the document the answer came from , grammaticality , etc . Here , aiming for simplicity , we do not aim to use such information .", "Hence we treat p a as uniform over A . 3 The second adjustment adds a labeled , directed dependency tree to the question and the answer . The tree is produced by a state of the art depen dency parser McDonald et al , 2005 trained on the Wall Street Journal Penn Treebank Marcus et al . , 1993 .", "A dependency tree on a sequence w ? w1 , . . . , wk ?", "is a mapping of indices of words to in dices of their syntactic parents and a label for the syntactic relation , ? 1 , . . . , k ? 0 , . . . , k ? L . Each word wi has a single parent , denoted w ? i . par . Cycles are not permitted .", "w0 is taken to be the invis ible ? wall ?", "symbol at the left edge of the sentence ; it has a single child i ? i 0 1 .", "The label for wi is denoted ? i . lab .", "The third adjustment involves a hidden variable X , the alignment between question and answer3The main motivation for modeling p q a is that it is easier to model deletion of information such as the part of the sen tence that answers the question than insertion .", "Our QG doesnot model the real world knowledge required to fill in an an swer ; its job is to know what answers are likely to look like , syntactically .", "words .", "In our model , each question word maps to exactly one answer word .", "Let x 1 , . . . , n ? 1 , . . . , m be a mapping from indices of words in qto indices of words in a .", "It is for computational rea sons that we assume x i 1 ; in general x couldrange over subsets of 1 , . . . , m .", "Because we de fine the correspondence in this direction , note that it is possible for multple question words to map to the same answer word . Why do we treat the alignmentX as a hidden vari able ?", "In prior work , the alignment is assumed to be known given the sentences , but we aim to discoverit from data .", "Our guide in this learning is the struc ture inherent in the QG the configurations betweenparent child pairs in the question and their corre sponding , aligned words in the answer .", "The hidden variable treatment lets us avoid commitment to any one x mapping , making the method more robust tonoisy parses after all , the parser is not 100 ac curate and any wrong assumptions imposed by the model that x i 1 , for example , or that syntactic transformations can explain the connection between q and a at all . 4 Our model , then , defines p q , ? q a , ? a ? x p q , ? q , x a , ? a 2 where ? q and ? a are the question tree and answer tree , respectively .", "The stochastic process defined by our model factors cleanly into recursive steps thatderive the question from the top down .", "The QG defines a grammar for this derivation ; the grammar de pends on the specific answer .", "Let ? iw refer to the subtree of ? w rooted at wi .", "The model is defined by p ?", "iq qi , ? q i , x i , ? a 3 p kids j ? q j i , j i qi , left ? p kids j ? q j i , j i qi , right ? ?", "j ? q j i m ?", "x j 0 pkid qj , ? q j . lab qi , ? q i , x i , x j , ? a ? p ?", "jq qj , ? q j , x j , ? a 4If parsing performance is a concern , we might also treat the question and or answer parse trees as hidden variables , thoughthat makes training and testing more computationally expen sive .", "25 Note the recursion in the last line .", "While the above may be daunting , in practice it boils down only to defining the conditional distribution pkid , since the number of left and right children of each node need not be modeled the trees are assumed known ?", "p kids is included above for completeness , but in the model applied here we do not condition it on qi and therefore do not need to estimate it since the trees are fixed .", "pkid defines a distribution over syntactic children of qi and their labels , given 1 the word qi , 2 the parent of qi , 3 the dependency relation between qi and its parent , 4 the answer word qi is aligned to , 5 the answer word the child being predicted is aligned to , and 6 the remainder of the answer tree .", "4 . 2 Dynamic Programming .", "Given q , the score for an answer is simply p q , ? q a , ? a .", "Computing the score requires summing over alignments and can be done efficiently by bottom up dynamic programming .", "Let S j , refer to the score of ? jq , assuming that the parent of qj , ? q j . par , is aligned to a .", "The base case , for leaves of ? q , is S j , 4 p kids 0 qj , left ? p kids 0 qj , right ? m ? k 0 pkid qj , ? q j . lab q ? q j , , k , ? a Note that k ranges over indices of answer words to be aligned to qj . The recursive case is S i , 5 p kids j ? q j i , j i qj , left ? p kids j ? q j i , j i qj , right ? m ? k 0 pkid qi , ? q i . lab q ? q i , , k , ? a ? ?", "j ? q j i S j , k Solving these equations bottom up can be done in O nm2 time and O nm space ; in practice this is very efficient .", "In our experiments , computing the value of a question answer pair took two seconds on average . 5 We turn next to the details of pkid , the core of the model .", "4 . 3 Base Model .", "Our base model factors pkid into three conditional multinomial distributions .", "pbasekid qi , ? q i . lab q ? q i , , k , ? a p qi . pos ak . pos ? p qi . ne ak . ne ? p ? q i . lab config ? q , ? a , i 6 where qi . pos is question word i ? s POS label and qi . ne is its named entity label .", "config maps question word i , its parent , and their alignees to a QG configuration as described in Table 1 ; notethat some configurations are extended with addi tional tree information .", "The base model does not directly predict the specific words in the question ?", "only their parts of speech , named entity labels , anddependency relation labels .", "This model is very sim ilar to Smith and Eisner 2006 .", "Because we are interested in augmenting the QG with additional lexical semantic knowledge , we also estimate pkid by mixing the base model with a model that exploits WordNet Miller et al , 1990 lexical semantic relations .", "The mixture is given by pkid ?", "? ? p base kid ?", "? 1 ? ? p ls kid ?", "7 4 . 4 Lexical Semantics Log Linear Model .", "The lexical semantics model plskid is defined by pre dicting a nonempty subset of the thirteen classes for the question side word given the identity ofits aligned answer side word .", "These classes in clude WordNet relations identical word , synonym , antonym also extended and indirect antonym , hy pernym , hyponym , derived form , morphological variation e . g . , plural form , verb group , entailment , entailed by , see also , and causal relation .", "In addition , to capture the special importance of Whwords in questions , we add a special semantic re lation called ? q word ?", "between any word and any Wh word .", "This is done through a log linear model with one feature per relation .", "Multiple relations may fire , motivating the log linear model , which permits ? overlapping ?", "features , and , therefore prediction of 5Experiments were run on a 64 bit machine with 2 ?", "2 . 2GHz dual core CPUs and 4GB of memory .", "26 any of the possible 213 ? 1 nonempty subsets .", "It is important to note that this model assigns zero probability to alignment of an answer word with any question word that is not directly related to it through any relation .", "Such words may be linked in the mixture model , however , via pbasekid . 6 It is worth pointing out that log linear models provide great flexibility in defining new features .", "It is straightforward to extend the feature set to include more domain specific knowledge or other kinds of morphological , syntactic , or semantic information . Indeed , we explored some additional syntactic fea tures , fleshing out the configurations in Table 1 inmore detail , but did not see any interesting improve ments . parent child Question parent child pair align respectively to answer parent child pair .", "Augmented with the q . side dependency la bel . child parent Question parent child pair align respectively to answer child parent pair .", "Augmented with the q . side dependency la bel . grandparent child Question parent child pair align respec tively to answer grandparent child pair .", "Augmented with the q . side dependency label .", "same node Question parent child pair align to the same answer word . siblings Question parent child pair align to sib lings in the answer .", "Augmented withthe tree distance between the a . side sib lings .", "c command The parent of one answer side word is an ancestor of the other answer side word . other A catch all for all other types of config urations , which are permitted . Table 1 Syntactic alignment configurations are partitioned into these sets for prediction under the Jeop ardy model .", "4 . 5 Parameter Estimation .", "The parameters to be estimated for the Jeopardy model boil down to the conditional multinomialdistributions in pbasekid , the log linear weights in side of plskid , and the mixture coefficient ? .", "7 Stan .", "6It is to preserve that robustness property that the models are mixed , and not combined some other way .", "7In our experiments , all log linear weights are initialized tobe 1 ; all multinomial distributions are initialized as uniform disdard applications of log linear models apply con ditional maximum likelihood estimation , which for our case involves using an empirical distribution p ? over question answer pairs and their trees to opti mize as follows max ? ?", "q , ? q , a , ? a p ? q , ? q , a , ? a log p ? q , ? q a , ? a ? ? ?", "P x p ? q , ? q , x a , ? a 8 Note the hidden variable x being summed out ; that makes the optimization problem non convex .", "Thissort of problem can be solved in principle by conditional variants of the Expectation Maximization al gorithm Baum et al , 1970 ; Dempster et al , 1977 ; Meng and Rubin , 1993 ; Jebara and Pentland , 1999 .", "We use a quasi Newton method known as L BFGS Liu and Nocedal , 1989 that makes use of the gradient of the above function straightforward to com pute , but omitted for space .", "To evaluate our model , we conducted experiments using Text REtrieval Conference TREC 8 ? 13 QA dataset . 8 5 . 1 Experimental Setup .", "The TREC dataset contains questions and answer patterns , as well as a pool of documents returned byparticipating teams .", "Our task is the same as Pun yakanok et al 2004 and Cui et al 2005 , where we search for single sentence answers to factoid questions .", "We follow a similar setup to Shen and Klakow 2006 by automatically selecting answer candidate sentences and then comparing against a human judged gold standard .", "We used the questions in TREC 8 ? 12 for training and set aside TREC 13 questions for development 84 questions and testing 100 questions .", "To gen erate the candidate answer set for development and testing , we automatically selected sentences from each question ? s document pool that contains one ormore non stopwords from the question .", "For gen erating the training candidate set , in addtion to thesentences that contain non stopwords from the ques tion , we also added sentences that contain correct tributions ; ? is initialized to be 0 . 1 .", "8We thank the organizers and NIST for making the dataset publicly available .", "27 answer pattern .", "Manual judgement was produced for the entire TREC 13 set , and also for the first 100questions from the training set TREC 8 ? 12 . 9 On av erage , each question in the development set has 3 . 1 positive and 17 . 1 negative answers .", "There are 3 . 6 positive and 20 . 0 negative answers per question in the test set . We tokenized sentences using the standard tree bank tokenization script , and then we performedpart of speech tagging using MXPOST tagger Ratnaparkhi , 1996 .", "The resulting POS tagged sentences were then parsed using MSTParser McDon ald et al , 2005 , trained on the entire Penn Treebank to produce labeled dependency parse trees we used a coarse dependency label set that includes twelve label types .", "We used BBN Identifinder Bikel et al , 1999 for named entity tagging . As answers in our task are considered to be sin gle sentences , our evaluation differs slightly from TREC , where an answer string a word or phrase like 1977 or George Bush has to be accompaniedby a supporting document ID . As discussed by Punyakanok et al 2004 , the single sentence assump tion does not simplify the task , since the hardest part of answer finding is to locate the correct sentence .", "From an end user ? s point of view , presenting thesentence that contains the answer is often more in formative and evidential .", "Furthermore , although the judgement data in our case are more labor intensiveto obtain , we believe our evaluation method is a better indicator than the TREC evaluation for the qual ity of an answer selection algorithm . To illustrate the point , consider the example question , ? When did James Dean die ? ?", "The correct an9More human judged data are desirable , though we will address training from noisy , automatically judged data in Section 5 . 4 .", "It is important to note that human judgement of answer sentence correctness was carried out prior to any experi ments , and therefore is unbiased .", "The total number of questions in TREC 13 is 230 .", "We exclude from the TREC 13 set questions that either have no correct answer candidates 27 questions , or no incorrect answer candidates 19 questions .", "Any algorithm will get the same performance on these questions , and thereforeobscures the evaluation results .", "6 such questions were also excluded from the 100 manually judged training questions , result ing in 94 questions for training .", "For computational reasons the cost of parsing , we also eliminated answer candidate sentences that are longer than 40 words from the training and evaluation set .", "After these data preparation steps , we have 348 positive Q A pairs for training , 1 , 415 Q A pairs in the development set , and 1 , 703 Q A pairs in the test set .", "swer as appeared in the sentence ? In 1955 , actor James Dean was killed in a two car collision nearCholame , Calif . ?", "is 1955 .", "But from the same docu ment , there is another sentence which also contains 1955 ? In 1955 , the studio asked him to become a technical adviser on Elia Kazan ? s ? East of Eden , ?", "starring James Dean . ?", "If a system missed the first sentence but happened to have extracted 1955 fromthe second one , the TREC evaluation grants it a ? cor rect and well supported ?", "point , since the document ID matches the correct document ID ? even though the latter answer does not entail the true answer .", "Our evaluation does not suffer from this problem . We report two standard evaluation measures commonly used in IR and QA research mean av erage precision MAP and mean reciprocal rank MRR .", "All results are produced using the standard trec eval program .", "5 . 2 Baseline Systems .", "We implemented two state of the art answer finding algorithms Cui et al , 2005 ; Punyakanok et al , 2004 as strong baselines for comparison .", "Cui et al .", "2005 is the answer finding algorithm behindone of the best performing systems in TREC eval uations .", "It uses a mutual information inspired scorecomputed over dependency trees and a single alignment between them .", "We found the method to be brit tle , often not finding a score for a testing instance because alignment was not possible .", "We extendedthe original algorithm , allowing fuzzy word align ments through WordNet expansion ; both results are reported . The second baseline is the approximate tree matching work by Punyakanok et al 2004 .", "Their algorithm measures the similarity between ? q and ? a by computing tree edit distance .", "Our replication is close to the algorithm they describe , with one subtle difference .", "Punyakanok et al used answer typing in computing edit distance ; this is not available in our dataset and our method does not explicitly carry out answer typing .", "Their heuristics for reformulating questions into statements were not replicated .", "Wedid , however , apply WordNet type checking and ap proximate , penalized lexical matching .", "Both results are reported .", "28 development set test set training dataset model MAP MRR MAP MRR 100 manually judged TreeMatch 0 . 4074 0 . 4458 0 . 3814 0 . 4462 WN 0 . 4328 0 . 4961 0 . 4189 0 . 4939 Cui et al 0 . 4715 0 . 6059 0 . 4350 0 . 5569 WN 0 . 5311 0 . 6162 0 . 4271 0 . 5259 Jeopardy base only 0 . 5189 0 . 5788 0 . 4828 0 . 5571 Jeopardy 0 . 6812 0 . 7636 0 . 6029 0 . 6852 2 , 293 noisy Cui et al 0 . 2165 0 . 3690 0 . 2833 0 . 4248 WN 0 . 4333 0 . 5363 0 . 3811 0 . 4964 Jeopardy base only 0 . 5174 0 . 5570 0 . 4922 0 . 5732 Jeopardy 0 . 6683 0 . 7443 0 . 5655 0 . 6687 Table 2 Results on development and test sets .", "TreeMatch is our implementation of Punyakanok et al 2004 ; WN modifies their edit distance function using WordNet .", "We also report our implementation of Cui et al 2005 , along with our WordNet expansion WN .", "The Jeopardy base model and mixture with the lexical semantics log linear model perform best ; both are trained using conditional maximum likelihood estimation .", "The top part of the table shows performance using 100 manually annotated question examples questions 1 ? 100 in TREC 8 ? 12 , and the bottom part adds noisily , automatically annotated questions 101 ?", "Boldface marks the best score in a column and any scores in that column not significantly worse under a a two tailed paired t test p 0 . 03 .", "5 . 3 Results .", "Evaluation results on the development and test setsof our model in comparison with the baseline algo rithms are shown in Table 2 .", "Both our model and the model in Cui et al 2005 are trained on the manually judged training set questions 1 100 fromTREC 8 ? 12 .", "The approximate tree matching algorithm in Punyakanok et al 2004 uses fixed edit distance functions and therefore does not require training .", "From the table we can see that our model signif icantly outperforms the two baseline algorithms ?", "even when they are given the benefit of WordNet ?", "on both development and test set , and on both MRR and MAP .", "5 . 4 Experiments with Noisy Training Data .", "Although manual annotation of the remaining 2 , 293 training sentences ?", "answers in TREC 8 ? 12 was too labor intensive , we did experiment with a simple , noisy automatic labeling technique .", "Any answer that had at least three non stop word types seen in the question and contains the answer pattern defined in the dataset was labeled as ? correct ?", "and used intraining .", "The bottom part of Table 2 shows the re sults .", "Adding the noisy data hurts all methods , butthe Jeopardy model maintains its lead and consis tently suffers less damage than Cui et al 2005 .", "The TreeMatch method of Punyakanok et al 2004 does not use training examples .", "5 . 5 Summing vs . Maximizing .", "Unlike most previous work , our model does not try to find a single correspondence between words in the question and words in the answer , during training or during testing .", "An alternative method might choose the best most probable alignment , rather than the sum of all alignment scores .", "This involves a slight change to Equation 3 , replacing the summation with a maximization .", "The change could be made during training , during testing , or both .", "Table 3 shows that summing is preferable , especially during training .", "The key experimental result of this work is that loose syntactic transformations are an effective way to carry out statistical question answering . One unique advantage of our model is the mix ture of a factored , multinomial based base model and a potentially very rich log linear model .", "Thebase model gives our model robustness , and the log 29 test set training decoding MAP MRR ? ?", "0 . 6029 0 . 6852 ? max 0 . 5822 0 . 6489 max ? 0 . 5559 0 . 6250 max max 0 . 5571 0 . 6365Table 3 Experimental results on comparing sum ming over alignments ?", "with maximizing max over alignments on the test set .", "Boldface marks the best score in a column and any scores in that columnnot significantly worse under a a two tailed paired t test p 0 . 03 . linear model allows us to throw in task or domainspecific features .", "Using a mixture gives the advantage of smoothing in the base model without hav ing to normalize the log linear model by summing over large sets .", "This powerful combination leads us to believe that our model can be easily ported to other semantic processing tasks where modeling syntactic and semantic transformations is the key , such as textual entailment , paraphrasing , and cross lingual QA .", "The traditional approach to cross lingual QA isthat translation is either a pre processing or post processing step done independently from the main QA task .", "Notice that the QG formalism that we have employed in this work was originally proposed formachine translation .", "We might envision transformations that are performed together to form ques tions from answers or vice versa and to translate ?", "a Jeopardy !", "game in which bilingual players must ask a question in a different language than that in which the answer is posed .", "We described a statistical syntax based model that softly aligns a question sentence with a candidateanswer sentence and returns a score .", "Discriminative training and a relatively straightforward , barelyengineered feature set were used in the implementation .", "Our scoring model was found to greatly out perform two state of the art baselines on an answer selection task using the TREC dataset .", "Acknowledgments The authors acknowledge helpful input from three anonymous reviewers , Kevin Gimpel , and David Smith .", "This work is supported in part by ARDA DTO Advanced Question Answering for Intelligence AQUAINT program award number NBCHC040164 ."], "summary_lines": ["What is the Jeopardy Model? A Quasi-Synchronous Grammar for QA\n", "This paper presents a syntax-driven approach to question answering, specifically the answer-sentence selection problem for short-answer questions.\n", "Rather than using syntactic features to augment existing statistical classifiers (as in previous work), we build on the idea that questions and their (correct) answers relate to each other via loose but predictable syntactic transformations.\n", "We propose a probabilistic quasi-synchronous grammar, inspired by one proposed for machine translation (D. Smith and Eisner, 2006), and parameterized by mixtures of a robust nonlexical syntax/alignment model with a(n optional) lexical-semantics-driven log-linear model.\n", "Our model learns soft alignments as a hidden variable in discriminative training.\n", "Experimental results using the TREC dataset are shown to significantly outperform strong state-of-the-art baselines.\n", "We explore the use a formalism called quasi synchronous grammar (Smith and Eisner, 2006) in order to find a more explicit model for matching the set of dependencies, and yet still allow for looseness in the matching.\n", "We use quasi-synchronous translation to map all parent-child paths in a question to any path in an answer.\n"]}
{"article_lines": ["Structural Ambiguity And Lexical Relations", "We propose that many ambiguous prepositional phrase attachments can be resolved on the basis of the relative strength of association of the preposition with verbal and nominal heads , estimated on the basis of distribution in an automatically parsed corpus .", "This suggests that a distributional approach can provide an approximate solution to parsing problems that , in the worst case , call for complex reasoning .", "We propose that many ambiguous prepositional phrase attachments can be resolved on the basis of the relative strength of association of the preposition with verbal and nominal heads , estimated on the basis of distribution in an automatically parsed corpus .", "This suggests that a distributional approach can provide an approximate solution to parsing problems that , in the worst case , call for complex reasoning .", "Prepositional phrase attachment is the canonical case of structural ambiguity , as in the timeworn example Example 1 I saw the man with the telescope .", "An analysis where the prepositional phrase pp with the telescope is part of the object noun phrase has the semantics quot ; the man who had the telescope quot ; ; an analysis where the PP has a higher attachment perhaps as daughter of VP is associated with a semantics where the seeing is achieved by means of a telescope .", "The existence of such ambiguity raises problems for language models .", "It looks like it might require extremely complex computation to determine what attaches to what .", "Indeed , one recent proposal suggests that resolving attachment ambiguity requires the construction of a discourse model in which the entities referred to in a text are represented and reasoned about Altmann and Steedman 1988 .", "We take this argument to show that reasoning essentially involving reference in a discourse model is implicated in resolving attachment ambiguities in a certain class of cases .", "If this phenomenon is typical , there is little hope in the near term for building computational models capable of resolving such ambiguities in unrestricted text .", "There have been several structure based proposals about ambiguity resolution in the literature ; they are particularly attractive because they are simple and don't demand calculations in the semantic or discourse domains .", "The two main ones are as follows .", "For the particular case we are concerned with , attachment of a prepositional phrase in a verb object context as in Example 1 , these two principles at least given the version of syntax that Frazier assumes make opposite predictions Right Association predicts noun attachment , while Minimal Attachment predicts verb attachment .", "Psycholinguistic work on structure based strategies is primarily concerned with modeling the time course of parsing and disambiguation , and acknowledges that other information enters into determining a final parse .", "Still , one can ask what information is relevant to determining a final parse , and it seems that in this domain structurebased disambiguation is not a very good predictor .", "A recent study of attachment of prepositional phrases in a sample of written responses to a quot ; Wizard of Oz quot ; travel information experiment shows that neither Right Association nor Minimal Attachment accounts for more than 55 of the cases Whittemore , Ferrara , and Brunner 1990 .", "And experiments by Taraban and McClelland 1988 show that the structural models are not in fact good predictors of people's behavior in resolving ambiguity .", "Whittemore , Ferrara , and Brunner 1990 found lexical preferences to be the key to resolving attachment ambiguity .", "Similarly , Taraban and McClelland found that lexical content was key in explaining people's behavior .", "Various previous proposals for guiding attachment disambiguation by the lexical content of specific words have appeared e . g .", "Ford , Bresnan , and Kaplan 1982 ; Marcus 1980 .", "Unfortunately , it is not clear where the necessary information about lexical preferences is to be found .", "Jenson and Binot 1987 describe the use of dictionary definitions for disambiguation , but dictionaries are typically rather uneven in their coverage .", "In the Whittemore , Ferrara , and Brunner study 1990 , the judgment of attachment preferences had to be made by hand for the cases that their study covered ; no precompiled list of lexical preferences was available .", "Thus , we are posed with the problem of how we can get a good list of lexical preferences .", "Our proposal is to use co occurrence of verbs and nouns with prepositions in a large body of text as an indicator of lexical preference .", "Thus , for example , the preposition to occurs frequently in the context send NP_ , that is , after the object of the verb send .", "This is evidence of a lexical association of the verb send with to .", "Similarly , from occurs frequently in the context withdrawal_ , and this is evidence of a lexical association of the noun withdrawal with the preposition from .", "This kind of association is a symmetric notion it provides no indication of whether the preposition is selecting the verbal or nominal head , or vice versa .", "We will treat the association as a property of the pair of words .", "It is a separate issue , which we will not be concerned with in the initial part of this paper , to assign the association to a particular linguistic licensing relation .", "The suggestion that we want to explore is that the association revealed by textual distribution whether its source is a complementation relation , a modification relation , or something else gives us information needed to resolve prepositional attachment in the majority of cases .", "A 13 million word sample of Associated Press news stories from 1989 were automatically parsed by the Fidditch parser Hindle 1983 and in press , using Church's A sample of NP heads , preceding verbs , and following prepositions derived from the parsed corpus . part of speech analyzer as a preprocessor Church 1988 , a combination that we will call simply quot ; the parser . quot ; The parser produces a single partial syntactic description of a sentence .", "Consider Example 2 , and its parsed representation in Example 3 .", "The information in the tree representation is partial in the sense that some attachment information is missing the nodes dominated by quot ; ? quot ; have not been integrated into the syntactic representation .", "Note in particular that many PPs have not been attached .", "This is a symptom of the fact that the parser does not in many cases have the kind of lexical information that we have just claimed is required in resolving PP attachment .", "Example 2 The radical changes in export and customs regulations evidently are aimed at remedying an extreme shortage of consumer goods in the Soviet Union and assuaging citizens angry over the scarcity of such basic items as soap and windshield wipers .", "From the syntactic analysis provided by the parser , we extracted a table containing the heads of all noun phrases .", "For each noun phrase head , we recorded the following preposition if any occurred ignoring whether or not the parser had attached the preposition to the noun phrase , and the preceding verb if the noun phrase was the object of that verb .", "The entries in Table 1 are those generated from the text above .", "Each noun phrase in Example 3 is associated with an entry in the Noun column of the table .", "Usually this is simply the root of the head of the noun phrase good is the root of the head of consumer goods .", "Noun phrases with no head , or where the head is not a common noun , are coded in a special way DART PNP represents a noun phrase beginning with a definite article and headed by a proper noun , and VING represents a gerundive noun phrase .", "PRO represents the empty category which , in the syntactic theory underlying the parser , is assumed to be the object of the passive verb aimed .", "In cases where a prepositional phrase follows the noun phrase , the head preposition appears in the Prep column ; attached and unattached prepositional phrases generate the same kinds of entries .", "If the noun phrase is an object , the root of the governing verb appears in the Verb column aim is the root of aimed , the verb governing the empty category , , .", "The last column in the table , labeled Syntax , marks with the symbol V all cases where there is no preceding verb that might license the preposition the initial subject of Example 2 is such a case .", "In the 13 million word sample , 2 , 661 , 872 noun phrases were identified .", "Of these , 467 , 920 were recognized as the object of a verb , and 753 , 843 were followed by a preposition .", "Of the object noun phrases identified , 223 , 666 were ambiguous verb noun preposition triples .", "The table of verbs , nouns , and prepositions is in several respects an imperfect source of information about lexical associations .", "First , the parser gives us incorrect analyses in some cases .", "For instance , in the analysis partially described in Example 4a , the parser incorrectly classified probes as a verb , resulting in a table entry probe lightning in .", "Similarly , in Example 4b , the infinitival marker to has been misidentified as a preposition . a .", "The space v , probes detected lightning in Jupiter's upper atmosphere and observed auroral emissions like Earth's northern lights in the Jovian polar regions . b .", "The Bush administration told Congress on Tuesday it wants to v preserve the right to control entry to the United States of anyone who was ever a Communist .", "Second , a preposition in an entry might be structurally related to neither the noun of the entry nor the verb if there is one , even if the entry is derived from a correct parse .", "For instance , the phrase headed by the preposition might have a higher locus of attachment a .", "The Supreme Court today agreed to consider reinstating the murder conviction of a New York City man who confessed to VING killing , his former girlfriend , after police illegally arrested him at his home .", "The temporal phrase headed by after modifies confess , but given the procedure described above , Example 5a results in a tuple kill girlfriend after .", "In the second example , a tuple legalize abortion under is extracted , although the PP headed by under modifies the higher verb shot .", "Finally , entries of the form verb noun preposition do not tell us whether to induce a lexical association between verb and preposition or between noun and preposition .", "We will view the first two problems as noise that we do not have the means to eliminate , 1 For present purposes , we can consider a parse correct if it contains no incorrect information in the relevant area .", "Provided the PPs in Example 5 are unattached , the parses would be correct in this sense .", "The incorrect information is added by our table construction step , which given our interpretation of the table assumes that a preposition following an object NP modifies either the NP or its governing verb . and partially address the third problem in a procedure we will now describe .", "We want to use the verb noun preposition table to derive a table of bigrams counts , where a bigram is a pair consisting of a noun or verb and an associated preposition or no preposition .", "To do this we need to try to assign each preposition that occurs either to the noun or to the verb that it occurs with .", "In some cases it is fairly certain whether the preposition attaches to the noun or the verb ; in other cases , this is far less certain .", "Our approach is to assign the clear cases first , then to use these to decide the unclear cases that can be decided , and finally to divide the data in the remaining unresolved cases between the two hypotheses verb and noun attachment .", "The procedure for assigning prepositions is as follows This procedure gives us bigram counts representing the frequency with which a given noun occurs associated with an immediately following preposition or no preposition , or a given verb occurs in a transitive use and is associated with a preposition immediately following the object of the verb .", "We use the following notation f w , p is the frequency count for the pair consisting of the verb or noun w and the preposition p . The unigram frequency count for the word w either a verb , noun , or preposition can be viewed as a sum of bigram frequencies , and is written f w .", "For instance , if p is a preposition , f p Ew f w , p .", "Our object is to develop a procedure to guess whether a preposition is attached to the verb or its object when a verb and its object are followed by a preposition .", "We assume that in each case of attachment ambiguity , there is a forced choice between two outcomes the preposition attaches either to the verb or to the noun . '", "For example , in Example 6 , we want to choose between two possibilities either into is attached to the verb send or it is attached to the noun soldier .", "Moscow sent more than 100 , 000 soldiers into Afghanistan . . .", "In particular , we want to choose between two structures For the verb_attach case , we require not only that the preposition attach to the verb send but also that the noun soldier have no following prepositional phrase attached since into directly follows the head of the object noun phrase , there is no room for any post modifier of the noun soldier .", "We use the notation NULL to emphasize that in order for a preposition licensed by the verb to be in the immediately postnominal position , the noun must have no following complements or adjuncts .", "For the case of noun attachment , the verb may or may not have additional prepositional complements following the prepositional phrase associated with the noun .", "Since we have a forced choice between two outcomes , it is appropriate to use a likelihood ratio to compare the attachment probabilities cf .", "Mosteller and Wallace 1964 . 3 In particular , we look at the log of the ratio of the probability of verb_attach to the probability of noun_attach .", "We will call this log likelihood ratio the LA lexical association score . and Again , the probability of noun attachment does not involve a term indicating that the verb sponsors no additional complement ; when we observe a prepositional phrase that is in fact attached to the object NP , the verb might or might not have a complement or adjunct following the object phrase .", "2 Thus we are ignoring the fact that the preposition may in fact be licensed by neither the verb nor the noun , as in Example 5 .", "3 In earlier versions of this paper we used a t test for deciding attachment and a different procedure for estimating the probabilities .", "The current procedure has several advantages .", "Unlike the t test used previously , it is sensitive to the magnitude of the difference between the two probabilities , not to our confidence in our ability to estimate those probabilities accurately .", "And our estimation procedure has the property that it defaults in case of novel words to the average behavior for nouns or verbs , for instance , reflecting a default preference with of for noun attachment .", "We can estimate these probabilities from the table of co occurrence counts as 4 The LA score has several useful properties .", "The sign indicates which possibility , verb attachment or noun attachment , is more likely ; an LA score of zero means they are equally likely .", "The magnitude of the score indicates how much more probable one outcome is than the other .", "For example , if the LA score is 2 . 0 , then the probability of verb attachment is four times greater than noun attachment .", "Depending on the task , we can require a certain threshold of LA score magnitude before making a decision . '", "As usual , in dealing with counts from corpora we must confront the problem of how to estimate probabilities when counts are small .", "The maximum likelihood estimate described above is not very good when frequencies are small , and when frequencies are zero , the formula will not work at all .", "We use a crude adjustment to observed frequencies that has the right general properties , though it is not likely to be a very good estimate when frequencies are small .", "For our purposes , however exploring in general the relation of distribution in a corpus to attachment disambiguation we believe it is sufficient .", "Other approaches to adjusting small frequencies are discussed in Church et al . 1991 and Gale , Church , Yarowsky in press .", "The idea is to use the typical association rates of nouns and verbs to interpolate our probabilities .", "Where f N , p En f n , p , f V , p E , f v , p , f N En f n and 4 The nonintegral count for send is a consequence of the data splitting step Ambiguous Attach 2 , and the definition of unigram frequencies as a sum of bigram frequencies .", "5 An advantage of the likelihood ratio approach is that we can use it in a Bayesian discrimination framework to take into account other factors that might influence our decision about attachment see Gale , Church , and Yarowsky in press for a discussion of this approach .", "We know of course that other information has a bearing on the attachment decision .", "For example , we have observed that if the noun phrase object includes a superlative adjective as a premodifier , then noun attachment is certain for a small sample of 16 cases .", "We could easily take this into account by setting the prior odds ratio to heavily favor noun attachment let's suppose that if there is a superlative in the object noun phrase , then noun attachment is say 1000 times more probable than verb attachment ; otherwise , they are equally probable .", "Then following Mosteller and Wallace 1964 , we assume that Final attachment odds log , initial odds LA .", "In case there is no superlative in the object , the initial log odds will be zero verb and noun attachment are equally probable , and the final odds will equal our LA score .", "If there is a superlative , Final attachment odds log 2 LA v , , n , p . and similarly for verbs .", "When f n , p is zero , the estimate used is proportional to this average .", "If we have seen only one case of a noun and it occurred with a preposition p that is f n , p 1 and f n 1 , then our estimate is nearly cut in half .", "This is the kind of effect we want , since under these circumstances we are not very confident in 1 as an estimate of P p I n .", "When f n , p is large , the adjustment factor does not make much difference .", "In general ; this interpolation procedure adjusts small counts in the right direction and has little effect when counts are large .", "For our current example , this estimation procedure changes the LA score little The LA score of 5 . 87 for this example is positive and therefore indicates verb attachment ; the magnitude is large enough to suggest a strong preference for verb attachment .", "This method of calculating the LA score was used both to decide unsure cases in building the bigram tables as described in Ambiguous Attach 1 , and to make the attachment decisions in novel ambiguous cases , as discussed in the sections following .", "To evaluate the performance of the procedure , 1000 test sentences in which the parser identified an ambiguous verb noun preposition triple were randomly selected from AP news stories .", "These sentences were selected from stories included in the 13 million word sample , but the particular sentences were excluded from the calculation of lexical associations .", "The two authors first guessed attachments on the verb noun preposition triples , making a judgment on the basis of the three headwords alone .", "The judges were required to make a choice in each instance .", "This task is in essence the one that we will give the computer to judge the attachment without any more information than the preposition and the heads of the two possible attachment sites .", "This initial step provides a rough indication of what we might expect to be achievable based on the information our procedure is using .", "We also wanted a standard of correctness for the test sentences .", "We again judged the attachment for the 1000 triples , this time using the full sentence context , first grading the test sentences separately , and then discussing examples on which there was disagreement .", "Disambiguating the test sample turned out to be a surprisingly difficult task .", "While many decisions were straightforward , more than 10 of the sentences seemed problematic to at least one author .", "There are several kinds of constructions where the attachment decision is not clear theoretically .", "These include idioms as in Examples 8 and 9 , light verb constructions Example 10 , and small clauses Example 11 .", "Example 8 But over time , misery has given way to mending .", "Example 9 The meeting will take place in Quantico .", "Example 10 Bush has said he would not make cuts in Social Security .", "Example 11 Sides said Francke kept a . 38 caliber revolver in his car's glove compartment .", "In the case of idioms , we made the assignment on the basis of a guess about the syntactic structure of the idiom , though this was sometimes difficult to judge .", "We chose always to assign light verb constructions to noun attachment , based on the fact that the noun supplies the lexical information about what prepositions are possible , and small clauses to verb attachment , based on the fact that this is a predicative construction lexically licensed by the verb .", "Another difficulty arose with cases where there seemed to be a systematic semantically based indeterminacy about the attachment .", "In the situation described by Example 12a , the bar and the described event or events are presumably in the same location , and so there is no semantic reason to decide on one attachment .", "Example 12b shows a systematic benefactive indeterminacy if you arrange something for someone , then the thing arranged is also for them .", "The problem in Example 12c is that signing an agreement usually involves two participants who are also parties to the agreement .", "Example 13 gives some further examples drawn from another test sample .", "Example 12 a .", ". . . known to frequent the same bars in one neighborhood .", "In general , we can say that an attachment is semantically indeterminate if situations that verify the meaning associated with one attachment also make the meaning associated with the other attachment true .", "Even a substantial overlap as opposed to identity between the classes of situations verifying the two meanings makes an attachment choice difficult .", "The problems in determining attachments are heterogeneous .", "The idiom , light verb , and small clause constructions represent cases where the simple distinction between noun attachment and verb attachment perhaps does not make sense , or is very theory dependent .", "It seems to us that the phenomenon of semantically based indeterminacy deserves further exploration .", "If it is often difficult to decide what licenses a prepositional phrase , we need to develop language models that appropriately capture this .", "For our present purpose , we decided to make an attachment choice in all cases , in some cases relying on controversial theoretical considerations , or relatively unanalyzed intuitions .", "In addition to the problematic cases , 120 of the 1000 triples identified automatically as instances of the verb object preposition configuration turned out in fact to be other constructions , often as the result of parsing errors .", "Examples of this kind were given above , in the context of our description of the construction of the verb noun preposition table .", "Some further misidentifications that showed up in the test sample are identifying the subject of the complement clause of say as its object , as in Example 10 , which was identified as say ministers from , and misparsing two constituents as a single object noun phrase , as in Example 11 , which was identified as make subject to .", "First , consider how the simple structural attachment preference schemas perform at predicting the outcome in our test set .", "Right Association predicts noun attachment and does better , since in our sample there are more noun attachments , but it still has an error rate of 33 .", "Minimal Attachment , interpreted as entailing verb attachment , has the complementary error rate of 67 .", "Obviously , neither of these procedures is particularly impressive .", "Performance on the test sentences for two human judges and the lexical association procedure LA .", "LA actual N actual V precision recall N guess 496 89 N . 848 . 846 V guess 90 205 V . 695 . 697 neither 0 0 combined . 797 . 797 Judge 1 actual N actual V precision recall N guess 527 48 N . 917 . 899 V guess 59 246 V . 807 . 837 neither 0 0 combined . 878 . 878 Judge 2 actual N actual V precision recall N guess 482 29 N . 943 . 823 V guess 104 265 V . 718 . 901 neither 0 0 combined . 849 . 849 Now consider the performance of our lexical association LA procedure for the 880 standard test sentences .", "Table 2 shows the performance for the two human judges and for the lexical association attachment procedure .", "First , we note that the task of judging attachment on the basis of verb , noun , and preposition alone is not easy .", "The figures in the entry labeled quot ; combined precision quot ; indicate that the human judges had overall error rates of 12 15 . 6 The lexical association procedure is somewhat worse than the human judges , with an error rate of 20 , but this is an improvement over the structural strategies .", "The table also gives results broken down according to N vs . V attachment .", "The precision figures indicate the proportion of test items assigned to a given category that actually belong to the category .", "For instance , N precision is the fraction of cases that the procedure identified as N attachments that actually were N attachments .", "The recall figures indicate the proportion of test items actually belonging to a given category that were assigned to that category N precision is the fraction of actual N attachments that were identified as N attachments .", "The LA procedure recognized about 85 of the 586 actual noun attachment examples as noun attachments , and about 70 of the actual verb attachments as verb attachments .", "If we restrict the lexical association procedure to choose attachment only in cases where the absolute value of the LA score is greater than 2 . 0 an arbitrary threshold indicating that the probability of one attachment is four times greater than the other , we get attachment judgments on 621 of the 880 test sentences , with overall precision of about 89 .", "On these same examples , the judges also showed improvement , as evident in Table 3 . 7 The fact that an LA score threshold improves precision indicates that the LA score gives information about how confident we can be about an attachment choice .", "In some applications , this information is useful .", "For instance , suppose that we wanted to incorporate the PP attachment procedure in a parser such as Fidditch .", "It might be preferable to achieve increased precision in PP attachment , in return for leaving some PPs unattached .", "For this purpose , a threshold could be used .", "Table 4 shows the combined precision and recall levels at various LA thresholds .", "It is clear that the LA score can be used effectively to trade off precision and recall , with a floor for the forced choice at about 80 .", "A comparison of Table 3 with Table 2 indicates , however , that the decline in recall is severe for V attachment .", "And in general , the performance of the LA procedure is worse on V attachment examples than on N attachments , according to both precision and recall criteria .", "The next section is concerned with a classification of the test examples , which gives insight into why performance on V attachments is worse .", "Our model takes frequency of co occurrence as evidence of an underlying relationship but makes no attempt to determine what sort of relationship is involved .", "It is interesting to see what kinds of relationships are responsible for the associations the model is identifying .", "To investigate this we categorized the 880 triples according to the nature of the relationship underlying the attachment .", "In many cases , the decision was difficult .", "The argument adjunct distinction showed many gray cases between clear participants in an action and clear adjuncts , such as temporal modifiers .", "We made rough best guesses to partition the cases into the following categories argument , adjunct , idiom , small clause , systematic locative indeterminacy , other systematic indeterminacy , and light verb .", "With this set of categories , 78 of the 880 cases remained so problematic that we assigned them to the category other .", "Table 5 shows the proportion of items in a given category that were assigned the correct attachment by the lexical association procedure .", "Even granting the roughness of the categorization , some clear patterns emerge .", "Our approach is most successful at attaching arguments correctly .", "Notice that the 378 noun arguments constitute 65 of the total 586 noun attachments , while the 104 verb arguments amount to only 35 of the 294 verb attachments .", "Furthermore , performance with verb adjuncts is worse than with noun adjuncts .", "Thus much of the problem with V attachments noted in the previous section appears to be attributable to a problem with adjuncts , particularly verbal ones .", "Performance on verbal arguments remains worse than performance on nominal ones , however .", "The remaining cases are all complex in some way , and the performance is poor on these classes , showing clearly the need for a more elaborated model of the syntactic structure that is being identified .", "The idea that lexical preference is a key factor in resolving structural ambiguity leads us naturally to ask whether existing dictionaries can provide information relevant to disambiguation .", "The Collins COBUILD English Language Dictionary Sinclair et al . 1987 is useful for a comparison with the AP sample for several reasons it was compiled on the basis of a large text corpus , and thus may be less subject to idiosyncrasy than other works , and it provides , in a separate field , a direct indication of prepositions typically associated with many nouns and verbs .", "From a machine readable version of the dictionary , we extracted a list of 1 , 942 nouns associated with a particular preposition , and of 2 , 291 verbs associated with a particular preposition after an object noun phrase . '", "These 4 , 233 pairs are many fewer than the number of associations in the AP sample see Table 6 , even if we ignore the most infrequent pairs .", "Of the total 76 , 597 pairs , 20 , 005 have a frequency greater than 3 , and 7 , 822 have a frequency that is greater than 3 and more than 4 times what one would predict on the basis of the unigram frequencies of the noun or verb and the preposition . '", "We can use the fixed lexicon of noun preposition and verb preposition associations derived from COBUILD to choose attachment in our test set .", "The COBUILD dictionary has information on 257 of the 880 test verb noun preposition triples .", "In 241 of those cases , there is information only on noun or only on verb association .", "In these cases , we can use the dictionary to choose the attachment according to the association indicated .", "In the remaining 16 cases , associations between the preposition and both the noun and the verb are recorded in the dictionary .", "For these , we select noun attachment , since it is the more probable outcome in general .", "For the remaining cases , we assume that the dictionary makes no decision .", "Table 7 gives the results obtained where U is E f w , p , the total number of token bigrams .", "It is equivalent tow and p having a w , p mutual information defined as greater than 2 .", "This threshold of 2 , of course , is an arbitrary cutoff . by this attachment procedure .", "The precision figure is similar to that obtained by the lexical association procedure with a threshold of zero , but the recall is far lower the dictionary provides insufficient information in most cases .", "Like the lexicon derived from the COBUILD dictionary , the fixed lexicon of 7 , 822 corpus derived associations derived from our bigram table as described above that is , all bigrams where f w , p 3 and I w , p 2 contains categorical information about associations .", "Using it for disambiguation in the way the COBUILD dictionary was used gives the results indicated in Table 7 .", "The precision is similar to that which was achieved with the LA procedure with a threshold of 2 , although the recall is lower .", "This suggests that while overall coverage of association pairs is important , the information about the relative strengths of associations contributing to the LA score is also significant .", "It must be noted that the dictionary information we derived from COBUILD was composed for people to use in printed form .", "It seems likely that associations were left out because they did not serve this purpose in one way or another .", "For instance , listing many infrequent or semantically predictable associations might be confusing .", "Furthermore , our procedure undoubtedly gained advantage from the fact that the test items are drawn from the same body of text as the training corpus .", "Nevertheless , the results of this comparison suggest that for the purpose of this paper , a partially parsed corpus is a better source of information than a dictionary .", "This conclusion should not be overstated , however .", "Table 6 showed that most of the associations in each lexicon are not found in the others .", "Table 8 is a sample of a verb preposition association dictionary obtained by merging information from the AP sample and from COBUILD , illustrating both the common ground and the differences between the two lexicons .", "Each source of information provides intuitively important associations that are missing from the other .", "In our judgment , the results of the lexical association procedure are good enough to make it useful for some purposes , in particular for inclusion in a parser such as Fidditch .", "The fact that the LA score provides a measure of confidence increases this usefulness , since in some applications such as exploratory linguistic analysis of text Verb NP Preposition associations in the COBUILD dictionary and in the AP sample with f v , p 3 and I v , p 2 . 0 .", "AP sample COBUILD approach about as at with corpora it is advantageous to be able to achieve increased precision in exchange for discarding a proportion of the data .", "From another perspective , our results are less good than what might be demanded .", "The performance of the human judges with access just to the verb noun preposition triple is a standard of what is possible based on this information , and the lexical association procedure falls somewhat short of this standard .", "The analysis of underlying relations indicated some particular areas in which the procedure did not do well , and where there is therefore room for improvement .", "In particular , performance on adjuncts was poor .", "A number of classes of adjuncts , such as temporal ones , are fairly easy to identify once information about the object of the preposition is taken into account .", "Beginning with such an identification step which could be conceived of as adding a feature such as temporal to individual prepositions , or replacing individual token prepositions with an abstract temporal preposition might yield a lexical association procedure that would do better with adjuncts .", "But it is also possible that a procedure that evaluates associations with individual nouns and verbs is simply inappropriate for adjuncts .", "This is an area for further investigation .", "This experiment was deliberately limited to one kind of attachment ambiguity .", "However , we expect that the method will be extendable to other instances of PP attachment ambiguity , such as the ambiguity that arises when several prepositional phrases follow a subject NP , and to ambiguities involving other phrases , especially phrases such as infinitives that have syntactic markers analogous to a preposition .", "We began this paper by alluding to several approaches to PP attachment , specifically work assuming the construction of discourse models , approaches based on structural attachment preferences , and work indicating a dominant role for lexical preference .", "Our results tend to confirm the importance of lexical preference .", "However , we can draw no firm conclusions about the other approaches .", "Since our method yielded incorrect results on roughly 20 of the cases , its coverage is far from complete .", "This leaves a lot of work to be done , within both psycholinguistic and computational approaches .", "Furthermore , as we noted above , contemporary psycholinguistic work is concerned with modeling the time course of parsing .", "Our experiment gives no information about how lexical preference information is exploited at this level of detail , or the importance of such information compared with other factors such as structural preferences at a given temporal stage of the human parsing process .", "However , the numerical estimates of lexical association we have obtained may be relevant to a psycholinguistic investigation of this issue .", "We thank Bill Gale , Ken Church , and David Yarowsky for many helpful discussions of this work and are grateful to four reviewers and Christian Rohrer for their comments on an earlier version ."], "summary_lines": ["Structural Ambiguity And Lexical Relations\n", "We propose that many ambiguous prepositional phrase attachments can be resolved on the basis of the relative strength of association of the preposition with verbal and nominal heads, estimated on the basis of distribution in an automatically parsed corpus.\n", "This suggests that a distributional approach can provide an approximate solution to parsing problems that, in the worst case, call for complex reasoning.\n", "We are the first to show that a corpus-based approach to PP attachment ambiguity resolution can lead to good results.\n", "We propose one of the earliest corpus-based approaches to prepositional phrase attachment used lexical preference by computing co-occurrence frequencies (lexical associations) of verbs and nouns with prepositions.\n", "We used a partial parser to extract (v, n, p) tuples from a corpus, where p is the preposition whose attachment is ambiguous between the verb v and the noun n.\n"]}
{"article_lines": ["Wide Coverage Semantic Representations From A CCG Parser", "This paper shows how to construct semantic representations from the derivations producedby a wide coverage CCG parser .", "Unlike the dependency structures returned by the parser itself , these can be used directly for semantic in terpretation .", "We demonstrate that well formed semantic representations can be produced for over 97 of the sentences in unseen WSJ text . We believe this is a major step towards wide coverage semantic interpretation , one of the key objectives of the field of NLP .", "The levels of accuracy and robustness recently achieved by statistical parsers e . g . Collins 1999 , Charniak 2000 have led to their use in a num ber of NLP applications , such as question answering Pasca and Harabagiu , 2001 , machine translation Charniak et al , 2003 , sentence simplifica tion Carroll et al , 1999 , and a linguist ? s search engine Resnik and Elkiss , 2003 .", "Such parsers typically return phrase structure trees in the styleof the Penn Treebank , but without traces and co indexation .", "However , the usefulness of this outputis limited , since the underlying meaning as repre sented in a predicate argument structure or logical form is difficult to reconstruct from such skeletal parse trees . In this paper we demonstrate how a widecoverage statistical parser using Combinatory Categorial Grammar CCG can be used to generate semantic representations .", "There are a number of ad vantages to using CCG for this task .", "First , CCG provides ? surface compositional ?", "analysis of certainsyntactic phenomena such as coordination and ex traction , allowing the logical form to be obtained for such cases in a straightforward way .", "Second , CCG isa lexicalised grammar , and only uses a small num ber of semantically transparent combinatory rules tocombine CCG categories .", "Hence providing a compositional semantics for CCG simply amounts to assigning semantic representations to the lexical en tries and interpreting the combinatory rules .", "Andthird , there exist highly accurate , efficient and ro bust CCG parsers which can be used directly for this task Clark and Curran , 2004b ; Hockenmaier , 2003 . The existing CCG parsers deliver predicate argu ment structures , but not semantic representations that can be used for inference .", "The present paper seeks to extend one of these wide coverage parsers by using it to build logical forms suitable for use invarious NLP applications that require semantic in terpretation . We show how to construct first order represen tations from CCG derivations using the ? calculus , and demonstrate that semantic representations can be produced for over 97 of the sentences in unseen WSJ text .", "The only other deep parser we are aware of to achieve such levels of robustness for the WSJ is Kaplan et al 2004 .", "The use of the ? calculusis integral to our method .", "However , first order rep resentations are simply used as a proof of concept ; we could have used DRSs Kamp and Reyle , 1993 or some other representation more tailored to the ap plication in hand . There is some existing work with a similar motivation to ours .", "Briscoe and Carroll 2002 gen erate underspecified semantic representations fromtheir robust parser .", "Toutanova et al 2002 and Ka plan et al 2004 combine statistical methods with a linguistically motivated grammar formalism HPSG and LFG respectively in an attempt to achieve levels of robustness and accuracy comparable to the Penn Treebank parsers which Kaplan et al do achieve .", "However , there is a key difference between these approaches and ours .", "In our approach the creation of the semantic representations forms a completely It could cost taxpayers 15 million to install and residents 1 million a year to maintain NP"], "summary_lines": ["Wide-Coverage Semantic Representations From A CCG Parser\n", "This paper shows how to construct semantic representations from the derivations produced by a wide-coverage CCG parser.\n", "Unlike the dependency structures returned by the parser itself, these can be used directly for semantic interpretation.\n", "We demonstrate that well-formed semantic representations can be produced for over 97% of the sentences in unseen WSJ text.\n", "We believe this is a major step towards wide-coverage semantic interpretation, one of the key objectives of the field of NLP.\n", "We present an algorithm that learns CCG lexicons with semantics but requires fully specified CCG derivations in the training data.\n", "We consider the challenging problem of constructing broad-coverage semantic representations with CCG, but do not learn the lexicon.\n"]}
{"article_lines": ["Enforcing Transitivity in Coreference Resolution", "A desirable quality of a coreference resolution system is the ability to handle transitivity constraints , such that even if it places high likelihood on a particular mention being coreferent with each of two other mentions , it will also consider the likelihood of those two mentions being coreferent when making a final assignment .", "This is exactly the kind of constraint that integer linear programming ILP is ideal for , but , surprisingly , previous work applying ILP to coreference resolution has not encoded this type of constraint .", "We train a coreference classifier over pairs of mentions , and show how to encode this type of constraint on top of the probabilities output from our pairwise classifier to extract the most probable legal entity assignments .", "We present results on two commonly used datasets which show that enforcement of transitive closure consistently improves performance , including imof up to 3 . 6 using the and up to 16 . 5 using cluster f measure .", "Much recent work on coreference resolution , which is the task of deciding which noun phrases , or mentions , in a document refer to the same real world entity , builds on Soon et al . 2001 .", "They built a decision tree classifier to label pairs of mentions as coreferent or not .", "Using their classifier , they would build up coreference chains , where each mention was linked up with the most recent previous mention that the classifier labeled as coreferent , if such a mention existed .", "Transitive closure in this model was done implicitly .", "If John Smith was labeled coreferent with Smith , and Smith with Jane Smith , then John Smith and Jane Smith were also coreferent regardless of the classifier s evaluation of that pair .", "Much work that followed improved upon this strategy , by improving the features Ng and Cardie , 2002b , the type of classifier Denis and Baldridge , 2007 , and changing mention links to be to the most likely antecedent rather than the most recent positively labeled antecedent Ng and Cardie , 2002b .", "This line of work has largely ignored the implicit transitivity of the decisions made , and can result in unintuitive chains such as the Smith chain just described , where each pairwise decision is sensible , but the final result is not .", "Ng and Cardie 2002a and Ng 2004 highlight the problem of determining whether or not common noun phrases are anaphoric .", "They use two classifiers , an anaphoricity classifier , which decides if a mention should have an antecedent and a pairwise classifier similar those just discussed , which are combined in a cascaded manner .", "More recently , Denis and Baldridge 2007 utilized an integer linear programming ILP solver to better combine the decisions made by these two complementary classifiers , by finding the globally optimal solution according to both classifiers .", "However , when encoding constraints into their ILP solver , they did not enforce transitivity .", "The goal of the present work is simply to show that transitivity constraints are a useful source of information , which can and should be incorporated into an ILP based coreference system .", "For this goal , we put aside the anaphoricity classifier and focus on the pairwise classifier and transitivity constraints .", "We build a pairwise logistic classifier , trained on all pairs of mentions , and then at test time we use an ILP solver equipped with transitivity constraints to find the most likely legal assignment to the variables which represent the pairwise decisions . 1 Our results show a significant improvement compared to the naive use of the pairwise classifier .", "Other work on global models of coreference as opposed to pairwise models has included Luo et al . 2004 who used a Bell tree whose leaves represent possible partitionings of the mentions into entities and then trained a model for searching the tree ; McCallum and Wellner 2004 who defined several conditional random field based models ; Ng 2005 who took a reranking approach ; and Culotta et al .", "2006 who use a probabilistic first order logic model .", "For this task we are given a document which is annotated with a set of mentions , and the goal is to cluster the mentions which refer to the same entity .", "When describing our model , we build upon the notation used by Denis and Baldridge 2007 .", "Our baseline systems are based on a logistic classifier over pairs of mentions .", "The probability of a pair of mentions takes the standard logistic form where mi and mj correspond to mentions i and 3 respectively ; f mi , mj is a feature function over a pair of mentions ; 0 are the feature weights we wish to learn ; and x i j is a boolean variable which takes value 1 if mi and mj are coreferent , and 0 if they are not .", "The log likelihood of a document is the sum of the log likelihoods of all pairs of mentions 2 where m is the set of mentions in the document , and x is the set of variables representing each pairwise coreference decision x i , j .", "Note that this model is degenerate , because it assigns probability mass to nonsensical clusterings .", "Specifically , it will allow Prior work Soon et al . , 2001 ; Denis and Baldridge , 2007 has generated training data for pairwise classifiers in the following manner .", "For each mention , work backwards through the preceding mentions in the document until you come to a true coreferent mention .", "Create negative examples for all intermediate mentions , and a positive example for the mention and its correct antecedent .", "This approach made sense for Soon et al . 2001 because testing proceeded in a similar manner for each mention , work backwards until you find a previous mention which the classifier thinks is coreferent , add a link , and terminate the search .", "The COREF ILP model of Denis and Baldridge 2007 took a different approach at test time for each mention they would work backwards and add a link for all previous mentions which the classifier deemed coreferent .", "This is equivalent to finding the most likely assignment to each x i , j in Equation 2 .", "As noted , these assignments may not be a legal clustering because there is no guarantee of transitivity .", "The transitive closure happens in an ad hoc manner after this assignment is found any two mentions linked through other mentions are determined to be coreferent .", "Our SOON STYLE baseline used the same training and testing regimen as Soon et al . 2001 .", "Our D B STYLE baseline used the same test time method as Denis and Baldridge 2007 , however at training time we created data for all mention pairs .", "Because of the ad hoc manner in which transitivity is enforced in our baseline systems , we do not necessarily find the most probable legal clustering .", "This is exactly the kind of task at which integer linear programming excels .", "We need to first formulate the objective function which we wish the ILP solver to maximize at test time . 2 Let p i j log P x i , j mi , mj ; 0 , which is the log probability that mi and mj are coreferent according to the pairwise logistic classifier discussed in the previous section , and let p i , j log 1 p i , j , be the log probability that they are not coreferent .", "Our objective function is then the log probability of a particular possibly illegal variable assignment We add binary constraints on each of the variables x i , j E 10 , 11 .", "We also add constraints , over each triple of mentions , to enforce transitivity This constraint ensures that whenever x zj x j k 1 it must also be the case that x z k 1 .", "We used lp solve3 to solve our ILP optimization problems .", "We ran experiments on two datasets .", "We used the MUC 6 formal training and test data , as well as the NWIRE and BNEWS portions of the ACE Phase 2 corpus .", "This corpus had a third portion , NPAPER , but we found that several documents where too long for lp solve to find a solution . 4 We added named entity NE tags to the data using the tagger of Finkel et al . 2005 .", "The ACE data is already annotated with NE tags , so when they conflicted they overrode the tags output by the tagger .", "We also added part of speech POS tags to the data using the tagger of Toutanova et al . 2003 , and used the tags to decide if mentions were plural or singular .", "The ACE data is labeled with mention type pronominal , nominal , and name , but the MUC6 data is not , so the POS and NE tags were used to infer this information .", "Our feature set was simple , and included many features from Soon et al . , 2001 , including the pronoun , string match , definite and demonstrative NP , number and gender agreement , proper name and appositive features .", "We had additional features for NE tags , head matching and head substring matching .", "The MUC scorer Vilain et al . , 1995 is a popular coreference evaluation metric , but we found it to be fatally flawed .", "As observed by Luo et al . 2004 , if all mentions in each document are placed into a single entity , the results on the MUC 6 formal test set are 100 recall , 78 . 9 precision , and 88 . 2 F1 score significantly higher than any published system .", "The V scorer Amit and Baldwin , 1998 was proposed to overcome several shortcomings of the MUC scorer .", "However , coreference resolution is a clustering task , and many cluster scorers already exist .", "In addition to the MUC and V scorers , we also evaluate using cluster f measure Ghosh , 2003 , which is the standard f measure computed over true false coreference decisions for pairs of mentions ; the Rand index Rand , 1971 , which is pairwise accuracy of the clustering ; and variation of information Meila , 2003 , which utilizes the entropy of the clusterings and their mutual information and for which lower values are better .", "Our results are summarized in Table 1 .", "We show performance for both baseline classifiers , as well as our ILP based classifier , which finds the most probable legal assignment to the variables representing coreference decisions over pairs of mentions .", "For comparison , we also give the results of the COREFILP system of Denis and Baldridge 2007 , which was also based on a naive pairwise classifier .", "They used an ILP solver to find an assignment for the variables , but as they note at the end of Section 5 . 1 , it is equivalent to taking all links for which the classifier returns a probability 0 . 5 , and so the ILP solver is not really necessary .", "We also include their JOINTILP numbers , however that system makes use of an additional anaphoricity classifier .", "For all three corpora , the ILP model beat both baselines for the cluster f score , Rand index , and variation of information metrics .", "Using the V metric , the ILP system and the D B STYLE baseline performed about the same on the MUC 6 corpus , though for both ACE corpora , the ILP system was the clear winner .", "When using the MUC scorer , the ILP system always did worse than the D B STYLE baseline .", "However , this is precisely because the transitivity constraints tend to yield smaller clusters which increase precision while decreasing recall .", "Remember that going in the opposite direction and simply putting all mentions in one cluster produces a MUC score which is higher than any in the table , even though this clustering is clearly not useful in applications .", "Hence , we are skeptical of this measure s utility and provide it primarily for comparison with previous work .", "The improvements from the ILP system are most clearly shown on the ACE NWIRE corpus , where the V f score improved 3 . 6 , and the cluster f score improved 16 . 5 .", "We showed how to use integer linear programming to encode transitivity constraints in a coreference classifier which models pairwise decisions over mentions .", "We also demonstrated that enforcing such constraints at test time can significantly improve performance , using a variety of evaluation metrics .", "Thanks to the following members of the Stanford NLP reading group for helpful discussion Sharon Goldwater , Michel Galley , Anna Rafferty .", "This paper is based on work funded by the Disruptive Technology Office DTO Phase III Program for Advanced Question Answering for Intelligence AQUAINT ."], "summary_lines": ["Enforcing Transitivity in Coreference Resolution\n", "A desirable quality of a coreference resolution system is the ability to handle transitivity constraints, such that even if it places high likelihood on a particular mention being coreferent with each of two other mentions, it will also consider the likelihood of those two mentions being coreferent when making a final assignment.\n", "This is exactly the kind of constraint that integer linear programming (ILP) is ideal for, but, surprisingly, previous work applying ILP to coreference resolution has not encoded this type of constraint.\n", "We train a coreference classifier over pairs of mentions, and show how to encode this type of constraint on top of the probabilities output from our pairwise classifier to extract the most probable legal entity assignments.\n", "We present results on two commonly used datasets which show that enforcement of transitive closure consistently improves performance, including improvements of up to 3.6% using the b3 scorer, and up to 16.5% using cluster f-measure.\n", "We present a supervised system which uses ILP inference to reconcile the predictions of a pairwise classifier.\n"]}

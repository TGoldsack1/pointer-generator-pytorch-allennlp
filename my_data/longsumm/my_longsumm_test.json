{
    "99341732": {
        "X": {
            "sections": [
                {
                    "text": "CCS CONCEPTS \u2022 Security and privacy \u2192 Systems security; Distributed systems security; \u2022 Computer systems organization\u2192 Reliability; Availability;\nKEYWORDS Byzantine fault tolerance, BFT, asynchronous BFT, blockchain, robustness, threshold cryptography"
                },
                {
                    "heading": "1 INTRODUCTION",
                    "text": "State machine replication (SMR) [64, 81] is a fundamental software approach to enabling highly available services in practical distributed systems and cloud computing platforms (e.g., Google\u2019s Chubby [20] and Spanner [29], Apache ZooKeeper [53]). Its Byzantine failure counterpart, Byzantine fault-tolerant SMR (BFT), has recently regained its prominence, as BFT has been regarded as the model for building permissioned blockchains where the distributed ledgers know each other\u2019s identities but may not trust one another. As an emerging technology transforming businessmodels, there has been a large number of industry implementations of permissioned blockchains, including Hyperledger Fabric [7, 87], Hyperledger Iroha [56], R3 Corda [30], Tendermint [88], and many more. The Hyperledger umbrella [5], for instance, has become a global collaborative open-source project under the Linux Foundation, now with more than 250 members.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CCS \u201918, October 15\u201319, 2018, Toronto, ON, Canada \u00a9 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5693-0/18/10. . . $15.00 https://doi.org/10.1145/3243734.3243812\nAsynchronous BFT protocols [14, 21, 23, 70] are arguably themost appropriate solutions for building high-assurance and intrusiontolerant permissioned blockchains in wide-area (WAN) environments, as these asynchronous protocols are inherently more robust against timing and denial-of-service (DoS) attacks that can be mounted over an unprotected network such as the Internet. Asynchronous BFT ensures liveness of the protocol without depending on any timing assumptions, which is prudent when the network is controlled by an adversary. In contrast, partially synchronous BFT (e.g., PBFT [27]) guarantees liveness only when the network becomes synchronous (i.e., satisfies timing assumptions). For instance, it was shown in [70] that PBFT would achieve zero throughput against an adversarial asynchronous scheduler. Challenges andopportunities in adopting asynchronous permissioned blockchains.While a recent asynchronous BFT protocol, HoneyBadgerBFT [70], significantly improves prior asynchronous BFT protocols [14, 21, 23, 70], there are still significant pain points and challenges that prevent it from being used in practice. Meanwhile, there are also new opportunities for asynchronous BFT with the rise of blockchains. Performance (latency, throughput) issues. Compared to partially synchronous BFT protocols (e.g., PBFT [27]), HoneyBadgerBFT has significantly higher latency and lower throughput, in part due to its use of expensive threshold cryptography (specifically, threshold encryption [10] and threshold signatures [17]). This is particularly visible in cases where each replica has limited computation power.\nThese limitations are further exacerbated by various engineering issues. For example, HoneyBadgerBFT was evaluated at only 80- bit security and it will be even slower if implemented with nowstandard 128-bit security. Moreover, due to its use of an erasurecoding library zfec [93], HoneyBadgerBFT can only support ReedSoloman codes (for which better alternatives exist) and at most 28 servers. No one-size-fits-all BFT. In partially synchronous environments, onesize-fits-all BFT protocols have been hard to achieve (as has been argued in various works, e.g., [8, 31, 59]). Indeed, a variety of partially synchronous BFT protocols [1, 8, 16, 27, 28, 31, 33, 59] have been proposed to meet different needs. For instance, chain-based BFT protocols, such as Aliph-Chain [8], BChain [33], and Shuttle [90], favor throughput over latency. Q/U [1] achieves fault-scalability that tolerates increasing numbers of faults without significantly decreasing performance. Zyzzyva [59] and Aliph [8] are hybrid protocols that have high performance in failure-free cases. Moreover, a large number of robust BFT protocols [4, 9, 16, 28, 91] aim to provide a trade-off between performance and liveness during attacks that affect the timing behavior of the network.\nWhile robustness is natively achieved in asynchronous BFT, we still require different designs and trade-offs for different performance metrics. Unlike HoneyBadgerBFT, which was designed to\noptimize throughput only, BEAT aims to be flexible and versatile, providing protocol instances optimized for latency, throughput, bandwidth, or scalability (in terms of the number of servers). Append-only ledger vs. smart contracts. We advocate distinguishing two different classes of blockchain applications: append-only ledgers and on-chain smart contracts. The former corresponds to append-only, linearizable storage systems (hereinafter, BFT storage), and the latter corresponds to general SMR. While they share security requirements (agreement, total order of updates, liveness), general SMR requires each replica to maintain a copy of all service state to support contracts that operate on that state. In contrast, BFT storage may leverage erasure coding to reduce overall storage by allowing servers to keep only fragments. (See Sec. 3 for formal definitions.) Both of the applications are rather popular. Applications such as food safety [92] and exchange of healthcare data [54] are examples of append-only ledgers, while AI blockchain [86] and financial payments [55] fall into the category of requiring smart contracts. Internet of things (IoT) with blockchains may be of either type, depending on the applications: if one just uses blockchains to store and distribute IoT data to avoid the single point of failure that the clouds may have, then we just need the distributed ledger functionality; if one additionally uses blockchains to consume and analyze the data, then we will additionally need smart contracts.\nBFT storage may be extended to support off-chain smart contracts run among clients (e.g., Hyperledger Fabric [7]). While offchain smart contracts have many benefits (e.g., achieving some level of confidentiality, as argued in [7]), they also have limitations: 1) they are less suited to running complex smart contract applications with power- and computation-restricted clients (e.g., IoT devices); 2) they require communication channels among clients; and 3) they do not support efficient cross-contract state update.\nSome blockchain systems use BFT for building consensus ordering services (e.g., Hyperledger Fabric). We find that BFT storage may be used to model the consensus ordering service, and a more efficient BFT storage can lead to a more efficient ordering service.\nWhen designing BEAT, we aimed to answer the following major question: Can we have asynchronous BFT storage that significantly outperforms asynchronous general SMR? Flexible read. Some applications benefit from flexible reading, i.e., reading just a portion of a data block as needed (instead of the whole block). For example, in a blockchain that stores video, a user may only want to read the first portion of the stored video. This can be challenging when we use erasure-coding as the underlying storage mechanism. BEAT aims to achieve flexible read with significantly reduced bandwidth. BEAT in a nutshell.We design, implement, and evaluate BEAT \u2014 a set of practical asynchronous BFT protocols that resolve the above challenges. First, BEAT leverages more secure and efficient cryptography support and more flexible and efficient erasure-coding support. Second, BEAT is flexible, versatile, and extensible; the BEAT family includes asynchronous BFT protocols that are designed to meet different needs. BEAT\u2019s design is modular, and it can be extended to provide many more meaningful trade-offs among functionality and performance. Third, BEAT is efficient. Roughly, all our BEAT instances significantly outperform, in terms of both latency and throughput, HoneyBadgerBFT.\nTheBEATprotocols.BEAT includes five BEAT instances (BEAT0\u2013 BEAT4). BEAT0, BEAT1, are BEAT2 are general SMR that can support both off-chain and on-chain smart contracts, while BEAT3 and BEAT4 are BFT storage that can support off-chain smart contracts only. We summarize the characteristics of the BEAT protocols in Table 1 as a series of improvements to HoneyBadgerBFT. \u2022 BEAT0, our baseline protocol, incorporates a more secure and efficient threshold encryption [85], a direct instantiation of threshold coin-flipping [22] (instead of using threshold signatures [17]), and more flexible and efficient erasure-coding support. \u2022 BEAT1 additionally replaces an erasure-coded broadcast (AVID broadcast) [24] used in HoneyBadgerBFT with a replicationbased broadcast (Bracha\u2019s broadcast [19]). This helps reduce latency when there is low contention and the batch size is small. \u2022 BEAT2 opportunisticallymoves the encryption part of the threshold encryption to the client, further reducing latency. BEAT2 does so at the price of achieving a weaker liveness notion, but can be combined with anonymous communication networks to achieve full liveness. Asynchronous BFT with Tor networks has been demonstrated in HoneyBadgerBFT. BEAT2 additionally achieves causal order [21, 35, 79], a rather useful property for many blockchain applications that process transactions in a \u201cfirst come, first served\u201d manner, such as stock trading and financial payments. \u2022 BEAT3 is a BFT storage system.While HoneyBadgerBFT, BEAT0, BEAT1, and BEAT2 use Byzantine reliable broadcast [19, 24, 67], we find that replacing Byzantine reliable broadcast with a different and more efficient primitive \u2014 bandwidth-efficient asynchronous verifiable information dispersal (AVID-FP) [45] (using fingerprinted cross-checksum) suffices to build a BFT storage. The bandwidth consumption in BEAT3 is information-theoretically optimal. To order transactions of size B, the communication complexity of BEAT3 is O(B), while the complexity for HoneyBadger and PBFT is O(nB) (where n is the total number of replicas). This improvement is significant, as it allows running BEAT in bandwidth-restricted environments, allows more aggressive batching, and significantly improves scalability. \u2022 BEAT4 further reduces read bandwidth. BEAT4 is particularly useful when it is common that clients frequently read only a fraction of stored transactions. We provide a generic framework to enable this optimization, and BEAT4 is a specific instantiation of the framework. Roughly, BEAT4 reduces the access overhead by 50% with around 10% additional storage overhead. To achieve this, we extend fingerprinted cross-checksums [45] to handle partial read and to the case of pyramid codes [51], and we design a novel erasure-coded asynchronous verifiable information dispersal protocol with reduced read bandwidth (AVID-FP-Pyramid). Both techniques may be of independent interest. To our knowledge, all the erasure-coded systems against arbitrary failures in reliable distributed systems community [6, 25, 32, 41, 46] use conventional MDS (maximum distance separable) codes [69] such as Reed-Solomon codes [78] and they inherit the large bandwidth features of MDS codes. On the other hand, a large number of works aim to reduce the read bandwidth by designing new erasure coding schemes [42\u201344, 50\u201352, 58]. The\nsystems using these codes work in synchronous environments only, and do not achieve any strong consistency goals even in the crash failure model (let alone Byzantine failures). It is our goal to blend these two disjoint communities and offer new insights to both, by designing novel Byzantine reliable broadcast and BFT protocols with reduced bandwidth. \u2022 BEAT\u2019s design is modular, and features of these protocols can be mixed to achieve even more meaningful trade-offs among functionalities, performance metrics, and concrete applications."
                },
                {
                    "heading": "2 RELATEDWORK",
                    "text": "The (subtle) differences between (BFT) SMRand (BFT) atomic registers. State machine replication [81] is a general technique to provide a fault-tolerant services using a number of server replicas. It can support arbitrary operations, not just read and write. In SMR, the servers need to communicate with each other and run an interactive consensus protocol to keep the servers in the same state.\nRegister specifications were introduced by Lamport in a series of papers [62, 65, 66], with atomic register as the strongest one. The notions of linearizability andwait-freedom for atomic registerswere introduced by Herlihy andWing [48] and Herlihy [47], respectively. Atomic registers can only support reads and writes.\nAtomic registers can be realized in asynchronous distributed systems with failures. However, state machine replication cannot be achieved in asynchronous environments [38], unless it uses randomization to circumvent this impossibility result. HoneyBadgerBFT and BEAT fall into this category.\nBFT SMR is suitable for a number of permissioned blockchain applications (e.g., on-chain smart contracts), while atomic registers are more suitable to model data-centric and cloud storage applications. Comparison with erasure-coded Byzantine atomic registers. An active line of research studies erasure-coded Byzantine atomic registers, as erasure coding can be used to provide storage reduction and/or reduce bandwidth. Notable systems include Pasis [41], CT [25], M-PoWerStore [32], Loft [46], and AWE [6]. These systems have rather different properties from BEAT storage (i.e., BEAT3 and BEAT4). Loft has the same communication complexity as BEAT storage, but it only achieves obstruction-freedom, vs. BEAT\u2019s (randomized) wait-freedom. AWE, Pasis, CT, and M-PoWerStore have larger communication complexity. Additionally, while AWE achieves waitfreedom, it relies on an architecture that separates storage from metadata and therefore may rely on more servers.\nErasure-code choice in BEAT4 (Or:Why pyramid codes?). As discussed in Section 1, an ingredient in BEAT is a novel adaptation of fingerprinted cross-checksums [45] to accommodate pyramid codes. Pyramid codes and their derivatives have already been used in practice, although in a very different setting (data centers), and offer a significant performance boost [26, 51, 52]. We leverage them here to reduce bandwidth costs for fragments that contain real data. Its close competitor, Xorbas codes [80], reduces bandwidth cost for both data and redundant fragments, though we do not leverage them here. We also do not choose the (more complex) derivatives of basic pyramid codes such as generalized pyramid codes [51] and local reconstruction codes [52] that offer maximal recoverability and improve the fault tolerance of basic pyramid codes. For our designed protocol, these codes offer even greater recoverability than we need and hence would be overkill. Weaver codes [43], HoVer codes [44], and Stepped Combination codes [42] (belonging to LDPC codes) do not provide the bandwidth savings and flexibility that we need.\nAnother direction of research in code design is to read instead from more fragments (see [50, 58] and references therein), but less data from each. However, the bandwidth savings are only around 20%\u223c30%, much less than pyramid codes and its derivatives. In addition, these codes do not fit our setting where we assume a fixed number of servers may behave maliciously and we attempt to mask as many Byzantine servers as possible."
                },
                {
                    "heading": "3 SYSTEM AND THREAT MODEL",
                    "text": "Timing assumptions.Distributed systems can be roughly divided into three categories according to their timing assumption: asynchronous, synchronous, or partially synchronous. An asynchronous system makes no timing assumptions on message processing or transmission delays. If there is a known bound on message processing delays and transmission delays, then the corresponding system is synchronous. The partial synchrony model [37] lies in-between: messages are guaranteed to be delivered within a time bound, but the bound may be unknown to participants of the system.\nIn protocols for asynchronous systems, neither safety nor liveness can rely on timing assumptions. In contrast, a protocol built for a synchronous or partially synchronous system risks having its safety or liveness properties violated if the synchrony assumption on which it depends is violated. For this reason, protocols built for asynchronous systems are inherently more robust to timing and denial-of-service (DoS) attacks [70, 94].\nBFT SMR. We consider asynchronous Byzantine fault-tolerant state machine replication (BFT SMR) protocols, where f out of n replicas can fail arbitrarily (Byzantine failures) and a computationally bounded adversary can coordinate faulty replicas.\nThe replicas collectively implement the abstraction of a keyvalue store. A replica delivers operations, each submitted by some client. All operations must be deterministic functions of the keyvalue store contents. The client should be able to compute a final response to its submitted operation from the responses it receives from replicas. Correctness for a secure BFT SMR protocol is specified as follows. \u2022 Agreement: If any correct replica delivers an operationm, then every correct replica deliversm. \u2022 Total order: If a correct replica has deliveredm1,m2, \u00b7 \u00b7 \u00b7 ,ms and another correct replica has deliveredm\u20321,m \u2032 2, \u00b7 \u00b7 \u00b7 ,m\u2032s \u2032 , then\nmi =m \u2032 i for 1 \u2264 i \u2264 min(s, s \u2032).\n\u2022 Liveness: If an operationm is submitted ton\u2212 f correct replicas, then all correct replicas will eventually deliverm. The liveness property has been referred to by other names, e.g., \u201cfairness\u201d in CKPS [21] and SINTRA [23], and \u201ccensorship resilience\u201d in HoneyBadgerBFT [70]. We use them interchangeably. We consider two types of BFT SMR services. BFT storage. A BFT storage service implements only read(key) and write(key, val) operations. The former should return to the client the current value for key in the key-value store, and the latter should update the value of key in the key-value store to val. General SMR. A general SMR service\u2014which is our default concern, unless specified otherwise\u2014supports operations that consist of arbitrary deterministic programs, or transactions, that operate on the key-value store.\nTo support operations that are arbitrary transactions, each replica will typically maintain the contents of the key-value store in its entirety. Then, total order and the determinism of transactions ensures that the key-value store contents remain synchronized at correct replicas (assuming they begin in the same state). BFT storage can be implemented in more space-efficient ways, e.g., with each replica storing only an erasure-coded fragment for the value of each key (e.g., [24, 41, 45, 46]). Secure causal BFTprotocols.One of the BEAT instances achieves causality, whichwe briefly recall as follows. Input causality prevents the faulty replicas from creating an operation derived from a correct client\u2019s but that is delivered (and so executed) before the operation from which it is derived. The problem of preserving input causality in BFT atomic broadcast protocols was first introduced by Reiter and Birman [79]. The notion was later refined by Cachin et al. [21] and recently generalized by Duan et al. [35]."
                },
                {
                    "heading": "4 BUILDING BLOCKS",
                    "text": "This section reviews the cryptographic and distributed systems building blocks for BEAT. Labeled threshold cryptosystems. We review robust labeled threshold cryptosystem (i.e., threshold encryption) [85] where a public key is associated with the system and a decryption key is shared among all the servers. Syntactically, a (t ,n) threshold encryption scheme ThreshEnc consists of the following algorithms.\nA probabilistic key generation algorithm TGen takes as input a security parameter l , the number n of total servers, and threshold parameter t , and outputs (pk, vk, sk), where pk is the public key, vk is the verification key, and sk = (sk1, \u00b7 \u00b7 \u00b7 , skn ) is a list of private keys. A probabilistic encryption algorithm TEnc takes as input a public key pk, a messagem, and a label lb, and outputs a ciphertext c . A probabilistic decryption share generation algorithm ShareDec takes as input a private key ski , a ciphertext c , and a label lb, and outputs a decryption share \u03c3 . A deterministic share verification algorithm Vrf takes as input the verification key vk, a ciphertext c , a label lb, and a decryption share \u03c3 , and outputs b \u2208 {0, 1}. A deterministic combining algorithm Comb takes as input the verification key vk, a ciphertext c , a label lb, a set of t decryption shares, and outputs a messagem, or \u22a5 (a distinguished symbol).\nWe require the threshold encryption scheme to be chosen ciphertext attack (CCA) secure against an adversary that controls up to t \u2212 1 servers. We also require consistency of decryptions, i.e., no adversary that controls up to t \u2212 1 servers can produce a ciphertext and two t-size sets of valid decryption shares (i.e., where Vrf returns b = 1 for each share) such that they yield different plaintexts.\nFor our purpose, we require a labeled threshold encryption scheme [85]; threshold cryptosystems that do not support labels [10, 18] are not suitable. Threshold PRF. We review threshold PRF (e.g., [22]), where a public key is associated with the system and a PRF key is shared among all the servers. A (t ,n) threshold PRF scheme for a function F consists of the following algorithms. A probabilistic key algorithm FGen takes as input a security parameter l , the number n of total servers, and threshold parameter t , and outputs (pk,vk, sk), where pk is the public key, vk is the verification key, and sk = (sk1, \u00b7 \u00b7 \u00b7 , skn ) is a list of private keys. A PRF share evaluation algorithm Eva takes a PRF input c , pk , and a private key ski , and outputs a PRF share yi . A deterministic share verification algorithm Vrf takes as input the verification key vk, a PRF input c , and a PRF share yi , and outputs b \u2208 {0, 1}. A deterministic combining algorithm FCom takes as input the verification key vk, x , and a set of t valid PRF shares, and outputs a PRF value y.1\nWe require the threshold PRF value to be unpredictable against an adversary that controls up to t \u2212 1 servers. We also require the threshold PRF to be robust in the sense the combined PRF value for c is equal to F(c).\nWe can use a direct implementation of threshold PRF [22] or can use build a threshold PRF using threshold signatures [17, 83]. Erasure coding scheme. An (n,m) erasure coding scheme takes as inputm data fragments and outputs n (n \u2265 m) same-size coded fragments. This essentially captures the encode algorithm of an erasure code, but we (intentionally) leave the decode algorithm undefined.\nAn (n, m) erasure coding scheme is systematic if the n coded fragments contain the originalm data fragments and \u0434 = n \u2212m redundant fragments. Let di (i \u2208 [1..m]) denote the data fragments, and di (i \u2208 [m + 1..n]) denote the redundant fragments. We have di (i \u2208 [1..n]) to denote all the coded fragments.\n1Our syntactic description of threshold encryption and threshold PRF can be made more general and the algorithms are not necessarily non-interactive.\nAn (n,m) erasure coding scheme is maximum distance separable (MDS) [69] if and only if all the data fragments can be recovered from anym-size subset of coded fragments.\nA systematic (n,m) erasure coding scheme is linear if each redundant fragment di (i \u2208 [m + 1..n]) is a linear combination of the data fragments, i.e., di = \u2211m j=1 bi jdj , where bi j \u2019s are coding coefficients. Basic pyramid codes.Huang et al. [51] introduced (basic) pyramid codes to slightly trade space for access efficiency in erasure-coded storage systems. For practical parameters, the pyramid codes can reduce the access overhead by 50% with around 10% additional storage overhead, compared to MDS erasure codes.\nPyramid codes can be efficiently built from any (n,m) systematic and MDS code that tolerates arbitrary \u0434 = n \u2212m erasures as defined above. Specifically, we divide them data fragments into L equal-size2 disjoint groups Sl (l \u2208 [1..L]), each of which contains m/L data fragments. Next, we keep \u04341 out of \u0434 redundant fragments unchanged. These fragments are global redundant fragments. Then, for each group Sl , we compute \u04340 = \u0434 \u2212 \u04341 group redundant fragments and add them to each group, where the j-th group redundant fragment, denoted dj,l , is a projection of the j-th redundant block dm+j in the original MDS code onto that group, i.e., dj,l is computed as dm+j except setting all the coding coefficients that do not correspond to Sl to 0.\nThis yields an (m + \u04340L + \u04341,m) systematic and non-MDS code, which has m data fragments and \u04340L + \u04341 redundant fragments, where each of group Sl has \u04340 group redundant fragments. It is important to note that new code is also linear.\nWe briefly describe two useful properties of the basic pyramid codes [51]: (1) An (m + \u04340L + \u04341,m) pyramid code can tolerate arbitrary \u0434 = \u04340 + \u04341 erasures; (2) Each equal-size group Sl is an (m/L + \u04340,m/L) MDS code. To decode a data fragment, first try from the group level. For each group Sl , if the number of the unavailable fragments is less than \u04340, any unavailable fragments can be recovered. Otherwise, first recover all the unavailable fragments from the group level, and then move to global level. One may needs to compute the fragments in the original MDS code that correspond to group redundant fragments, and then uses the conventional decoding algorithm of an MDS code to recover unavailable fragments.\nFigure 1 uses an example to describe pyramid codes and shows how they can reduce read bandwidth and system I/O. The example builds a (10, 6) pyramid code from a (9, 6) linear, MDS code. In 2While there is no need to requirem divides L for pyramid codes, in practice one almost always uses equal-size sets. Typically, L can be set to 2.\nthe (9, 6) MDS code, there are 6 data fragments and 3 redundant fragments. The redundant fragments (d7, d8, and d9) are linear combinations of the data fragments. For instance, d7 can be written as d7 = \u22116 j=1 b7jdj , where b7j \u2019s are coding coefficients.\nWe then divide the data fragments equally (setting L = 2), and compute one redundant fragment for each group: d7\u22121 =\u22113 j=1 b7jdj and d7\u22122 = \u22116 j=4 b7jdj , respectively. Clearly, we have d7\u22121 + d7\u22122 = d7. They are local (group) redundant fragments, and d8 and d9 are global redundant fragments. If the fragment d1 is not available, one can just use 3 fragments (d2, d3, and d7\u22121) to recover d1. If there is more than one failure in the local group, one would need to use the traditional MDS decoding algorithm to recover the faulty local fragment. One may need to compute the sum of d7\u22121 and d7\u22122 to recover d7 if necessary. Fingerprinted cross-checksum.Afingerprinted cross-checksum [45] is data structure used by a server to verify that its fragment corresponds to a unique original data block. An (n,m) fingerprinted cross-checksum fpcc consists of an array fpcc.cc[ ] of n values and an array fpcc.fp[ ] ofm values. The first array is a cross-checksum [40, 60] that contains the n hashes of the n coded fragments. The second array holds homomorphic fingerprints of data fragments that preserve the property of linear codes. Let h be a collision-resistant hash function and H be a hash function modeled as a random oracle. A homomorphic fingerprinting function fingerprint takes as input a random key and a data fragment and outputs a small field element. A fragment d is consistent with fpcc for index i \u2208 [1..n], if fpcc.cc[i] = h(d) and fingerprint(r ,d) = encode(fpcc.fp[1], \u00b7 \u00b7 \u00b7 , fpcc.fp[m]), where r = H(fpcc.cc[1], \u00b7 \u00b7 \u00b7 , fpcc.cc[n]).\nA central theorem in [45, 46] is that for an (n,m) systematic, MDS, and linear erasure coding scheme, no adversaryA can produce two different sets ofm fragments such that each fragment is consistent with fpcc for its index and they can be decoded into two different data blocks with non-negligible probability. Asynchronous verifiable information dispersal using fingerprinted cross-checksum. In an asynchronous verifiable information dispersal (AVID) protocol [24], a client disperses a blockM to n servers (where at most f of them might be faulty). The clients can later retrieve the full blockM through the servers. The verifiability of the protocol ensures that any two clients retrieve the same block.\nAn (n,m)-asynchronous verifiable information dispersal scheme is a pair of protocols (disperse, retrieve) that satisfy the following with high probability: \u2022 Termination: If a correct client initializes disperse(M), then all correct servers will eventually complete dispersal disperse(M). \u2022 Agreement: If some correct server completes disperse(M), all correct servers eventually complete disperse(M). \u2022 Availability: If f + 1 correct servers complete disperse(M), a correct client can run retrieve( ) to eventually reconstruct some blockM \u2032. \u2022 Correctness: If f + 1 correct servers complete disperse(M), all correct clients that run retrieve( ) eventually retrieve the same block M \u2032. If the client that initiated disperse(M) was correct, thenM \u2032 = M .\nCachin and Tessaro [24] proposed an erasure-coded AVID, which we call AVID-CT, To broadcast a messageM , the communication complexity of AVID-CT is O(n |M |).\nAVID-FP [45] is a bandwidth-efficient AVID using fingerprinted cross-checksum. In AVID-FP, given a block B to be dispersed, the dealer applies an (m,n) erasure coding scheme, wherem \u2265 f + 1 and n = m + 2f . Here f is the maximum number of Byzantine faulty servers that system can tolerate, and n is the total number of servers. Then it generates the corresponding fingerprinted crosschecksum for B with respect to the erasure coding scheme. Next, the client distributes the erasure-coded fragments and the same fingerprinted cross-checksum to the servers. Each server verifies the correctness of the fragment that it receives according to the fingerprinted cross-checksum and then, roughly speaking, leverages the (much smaller) fingerprinted cross-checksum in place of the fragment in the original AVID protocol. Different from AVID-CT, to disperse a messageM , the communication complexity of AVID-FP is O(|M |). Byzantine reliable broadcast.Byzantine reliable broadcast (RBC), also known as the \u201cByzantine generals\u2019 problem,\u201d was first introduced by Lamport et al. [67]. An asynchronous reliable broadcast protocol satisfies the following properties: \u2022 Agreement: If two correct servers deliver two messagesM and M \u2032 thenM = M \u2032. \u2022 Totality: If some correct server delivers a messageM , all correct servers deliverM . \u2022 Validity: If a correct sender broadcasts a messageM , all correct servers deliverM . Bracha\u2019s broadcast [19], one that assumes only authenticated channels, is a well-known implementation of Byzantine reliable broadcast. To broadcast amessageM , its communication complexity is O(n2 |M |). Cachin and Tessaro [24] proposed both an erasurecoded AVID (AVID-CT, mentioned above) and an erasure-coded variant of Bracha\u2019s broadcast \u2014 AVID broadcast, which reduces the cost to O(n |M |) compared to that of Bracha\u2019s broadcast. Note that we explicitly distinguish among AVID-CT and AVID-FP (both of which are verifiable information dispersal protocols) and AVID broadcast (a RBC protocol)."
                },
                {
                    "heading": "5 REVIEWING HONEYBADGERBFT",
                    "text": "This section provides an overview of HoneyBadgerBFT and related primitives. We begin by introducing asynchronous common subset (ACS). Asynchronous common subset.HoneyBadgerBFT uses ACS [14, 21]. Formally, an ACS protocol satisfies the following properties: \u2022 Validity: If a correct server delivers a set V , then |V | \u2265 n \u2212 f and V contains the inputs of at least n \u2212 2f correct servers. \u2022 Agreement: If a correct server delivers a set V , then all correct servers deliver V . \u2022 Totality: Ifn\u2212 f correct servers submit an input, then all correct servers deliver an output. ACS can trivially lead to asynchronous BFT: each server can propose a subset of transactions, and deliver the union of the transactions in the agreed-upon vector; sequence numbers can be then\nassigned to the agreed transactions using any predefined but fixed order. HoneyBadgerBFT in a nutshell. HoneyBadgerBFT essentially follows Ben-Or et al. [14], which uses reliable broadcast (RBC) and asynchronous binary Byzantine agreement (ABA) to achieve ACS. HoneyBadgerBFT cherry-picks a bandwidth-efficient, erasurecoded RBC (AVID broadcast) [24] and the most efficient ABA [72] to realize ACS. Specifically, HoneyBadgerBFT uses Boldyreva\u2019s threshold signature [17] to provide common coins for the randomized ABA protocol [72]. HoneyBadgerBFT favors throughput over latency by aggressively batching client transactions. It was shown that HoneyBadgerBFT can outperform PBFT when the number of servers exceeds 16 in terms of throughput in WANs, primarily because HoneyBadgerBFT distributes the network load more evenly than PBFT [27].\nAs illustrated in Figure 2, the HoneyBadgerBFT protocol is composed of two subprotocols/phases: RBC and ABA. In the RBC phase, each replica first proposes a set of transactions and uses reliable broadcast to disseminate its proposal to all other replicas. In the second phase, n concurrent ABA instances are used to agree on an n-bit vector bi for i \u2208 [1..n], where bi indicates that if replica i\u2019s proposed transactions are included.\nHoneyBadgerBFT proceeds in epochs. Let B be a batch size of client transactions. In each epoch, each replica will propose B/n transactions. Each epoch will commit \u2126(B) transactions. To improve efficiency, HoneyBadgerBFT ensures that each replica proposes mostly disjoint sets of transactions. For this reason, it asks replicas to propose randomly selected transactions. To prevent adversary from censoring some particular transaction by excluding whichever replicas propose it, HoneyBadgerBFT requires replicas to use threshold encryption to encrypt transactions proposed to avoid censorship.\nHoneyBadgerBFT contains four distributed algorithms: a threshold signature [17] that provides common coins for ABA, an ABA protocol [72] that has expected running time O(1) (completing within O(k) rounds with probability 1\u2212 2\u2212k ), a bandwidth-efficient reliable broadcast [24], and a threshold encryption [10] to avoid censorship and achieve liveness.\nRoughly, the reliable broadcast dominates the bandwidth and guides the selection of batch size. The threshold encryption scheme and the threshold signature scheme use expensive cryptographic operations, and they and the ABA dominate the latency of HoneyBadgerBFT.\nWhile HoneyBadgerBFT is the most efficient asynchronous BFT protocol known, HoneyBadgerBFT favors throughput over other performance metrics (latency, bandwidth, scalability). For instance, HoneyBadgerBFT has rather high latency, which is particularly visible in local area networks (LANs) [89]. This makes it difficult to work in latency-critical applications. Indeed, it is desirable to have asynchronous BFT protocols that are designed for different goals (different performance metrics, different application scenarios)."
                },
                {
                    "heading": "6 BEAT0",
                    "text": "This section describes BEAT0, our baseline protocol, that uses a set of generic techniques to improve HoneyBadgerBFT. Specifically, BEAT0 incorporates a more secure and efficient threshold encryption, a direct implementation of threshold coin flipping, and more flexible and efficient erasure-coding support. BEAT0 specification. Instead of using CPA/CCA-secure threshold encryption that does not support labels, BEAT0 leverages a CCAsecure, labeled threshold encryption [85] to encrypt transactions while making the ciphertexts uniquely identifiable.\nBEAT0 proceeds in epochs (i.e., rounds). Let r the current epoch number. Let n be the total number of replicas. Let ThreshEnc = (TGen, TEnc, ShareDec,Vrf,Comb) be a (f +1,n) labeled threshold encryption scheme. Let pk and vk be threshold encryption public key and verification key, respectively. Let ski be the private key for replica i \u2208 [1..n]. Let B be the batch size of BEAT0.\nEach replica i \u2208 [1..n] randomly selects a setT of transactions of size B/n. It then computes a labeled threshold encryption ciphertext (lb, c) $\u2190 TEncpk(lb,T ) where lb = (r , i). Next, each replica submits the labeled ciphertexts to ACS as input. Each replica i , upon receiving some labeled threshold ciphertexts (r , j \u2032, c) from some other replica j, does a sanity check to see if j = j \u2032 and if there is already a different triple for the same r and j before proceeding. Namely, each replica i only stores and processes one ciphertext from the same j and the same r , and will discard ciphertexts subsequently received for the same j and r .\nAfter getting output from ACS, a replica i can run ShareDec to decrypt the ciphertexts using its secret key ski , and broadcasts its decryption shares. When receiving f + 1 valid shares (that pass the verification of Vrf), a replica can use Comb to combine the transactions. Efficiently instantiating CCA secure labeled threshold encryption.Weobserve that much of the latency inHoneyBadgerBFT is due to usage of pairing-based cryptography, which is much slower than elliptic curve cryptography (cf. [71]). We thus implement our threshold encryption using the TDH2 scheme by Shoup and Gennaro [85] using the P-256 curve which provides standard 128-bit security. TDH2 is secure against chosen-ciphertext attacks, under the Decisional Diffie-Hellman (DDH) assumption in the random oracle model [13].\nJumping ahead, while we use a stronger and functionally more complex cryptographic scheme, our experiments show that doing so actually improves the latency of HoneyBadgerBFT greatly. Directly instantiating common coin protocol. Instead of using a threshold signature to derive the common coins as in HoneyBadgerBFT and other multi-party computation protocols, we choose to directly use threshold coin flipping. Specifically, we use the scheme\ndue to Cachin, Kursawe, and Shoup (CKS) [22] and implement it again using the P-256 curve that provides 128 bits of security. The threshold PRF scheme is proven secure under the Computational Diffie-Hellman (CDH) assumption in the random oracle model. Enabling more efficient and more flexible erasure coding. HoneyBadgerBFT uses an erasure-coding library zfec [93] that supports Reed-Soloman codes only and supports at most 128 servers.\nWe integrate the C erasure coding library Jerasure 2.0 [73] with our BEAT framework. This allows us to remove the restriction that HoneyBadgerBFT can only support at most 128 replicas, use more efficient erasure-coding schemes (e.g., Cauchy Reed-Soloman codes [75]), and flexibly choose between erasure-coding scheme parameters to improve performance. Distributedkey generation.Our threshold encryption and threshold PRF are discrete-log based, and BEAT0 and all subsequent BEAT instances allow efficient distributed key generation [39, 57], which should be run during setup. The implementation of distributed key generation, however, is outside the scope of the present paper."
                },
                {
                    "heading": "7 BEAT1 AND BEAT2 \u2014 LATENCY OPTIMIZED",
                    "text": "This section presents two latency-optimized protocols in BEAT: BEAT1 and BEAT2. BEAT1. Via a careful study of latency for each HoneyBadgerBFT subprotocol, we find that 1) most of latency comes from threshold encryption and threshold signatures, and 2) somewhat surprisingly, when the load is small and there is low contention, erasurecoded reliable broadcast (AVID broadcast) [24] causes significant latency. To test the actual latency overhead incurred by erasure-coded broadcast, we implement a variant of HoneyBadgerBFT, HB-Bracha, which replaces erasure-coded broadcast with a popular, replicationbased reliable broadcast protocol \u2014 Bracha\u2019s broadcast [19]. We find that when the client load is small, HB-Bracha outperforms HoneyBadgerBFT in terms of latency by 20%\u223c60%. This motivates us to devise BEAT1.\nBEAT1 replaces the AVID broadcast protocol in BEAT0 with Bracha\u2019s broadcast. It turns out that when the load is small, BEAT1 is consistently faster than BEAT0, though the difference by percentage is not as significant as that between HB-Bracha and HoneyBadgerBFT. However, when the load becomes larger, BEAT1 has significantly higher throughput, just as the case betweenHB-Bracha and HoneyBadgerBFT. BEAT2. In BEAT0, our use of CCA-secure, labeled threshold encryption is at the server side, to prevent the adversary from choosing which servers\u2019 proposals to include. BEAT2 opportunistically moves the use of threshold encryption to the client side, while still using Bracha\u2019s broadcast as in BEAT1.\nIn BEAT2, when the ciphertexts are delivered, it is too late for the adversary to censor transactions. Thus, the adversary does not know what transactions to delay, and can only delay transactions from specific clients. BEAT2 can be combined with anonymous communication networks to achieve full liveness. BEAT2 additionally achieves causal order [21, 35, 79], which prevents the adversary from inserting derived transactions before the original, causally prior transactions. Causal order is a rather useful property for blockchain applications that process client transactions in a \u201cfirst\ncome, first served\u201d manner, such as trading services, financial payments, and supply chain management."
                },
                {
                    "heading": "8 BEAT3 \u2014 BANDWIDTH OPTIMIZED BFT STORAGE",
                    "text": "This section presents BEAT3, an asynchronous BFT storage system. BEAT3 significantly improves all performancemetrics that we know of \u2014 latency (compared to HoneyBadgerBFT), bandwidth, storage overhead, throughput, and scalability. Deployment scenarios. Recall that the safety and liveness properties of BFT storage remain the same as those of general SMR, with the only exception that the state may not be replicated at each server (but instead may be erasure-coded). BEAT3 can be used for blockchain applications that need append-only ledgers, and specific blockchains where the consensus protocol serves as an ordering service, such as Hyperledger Fabric [7, 87]. BEAT3. BEAT3 achieves better performance by using a novel combination of a bandwidth-efficient information dispersal scheme (AVID-FP [45]) and an ABA protocol [72]. In comparison, HoneyBadgerBFT, BEAT0, BEAT1, and BEAT2 use a combination of reliable broadcast and an ABA protocol.\nAVID-FP has optimal bandwidth consumption which does not depend on the number of replicas. The bandwidth required to disperse a blockM in AVID-FP is only O(|M |), while the bandwidth in AVID broadcast (used in HoneyBadgerBFT) is O(n |M |). Technically speaking, AVID-FP has a much smaller communication complexity than AVID-CT because replicas in AVID-FP agree upon a small constant-size fingerprinted cross-checksum instead of on the block itself (i.e., the bulk data).\nOur basic idea is to replace AVID broadcast used in HoneyBadgerBFT with an (n,m) AVID-FP protocol, where n = m + 2f and m \u2265 f + 1. Accordingly, at the end of the AVID-FP protocol, each replica now stores some fingerprinted cross-checksum and the corresponding erasure-coded fragment. There is, however, a challenge to use the approach. In AVID-FP, a correct replica cannot reconstruct its fragment if it is not provided by the AVID-FP client who proposes some transaction (here, some other replica in our protocol). Namely, as mentioned by Hendricks et al. [45], even with a successful dispersal, only f + 1 correct replicas, instead of all correct replicas, may have the corresponding fragments. However, ABA expects all correct replicas to deliver the transaction during the broadcast/dispersal stage (to correctly proceed). Note that we cannot trivially ask replicas in AVID-FP to reconstruct their individual fragment or reconstruct the whole transaction, which would nullify the bandwidth benefit of using AVID-FP.\nWe observe that AVID-FP actually agrees on the fingerprinted cross-checksum of the transaction. It is good enough for us to proceed to the ABA protocol once each replica delivers the fingerprinted cross-checksum. The consequence for BEAT3 is just as in AVID-FP: at least f + 1 correct replicas have their fragments, and some correct replicas may not have their fragments. This causes no problem, as the data is retrievable using f + 1 =m correct fragments. Each replica just needs to send the client the fingerprinted cross-checksum and its fragment. The client can then reconstruct the transaction.\nMore formally, validity, agreement, and totality of the ACS using AVID-FP follow directly from the properties of asynchronous verifiable information dispersal, just as the case of using reliable broadcast. The only difference is that the ACS using AVID-FP now delivers a fingerprinted cross-checksum. We just need to prove that our ACS is functionally correct. This follows easily from correctness of asynchronous verifiable information dispersal: if a fingerprinted cross-checksum is delivered, then the corresponding data (i.e., transaction) is retrievable, and all clients are able to retrieve the data and the data was previously proposed by some server. Bandwidth comparison. To order transactions of size B, the communication complexity of BEAT1, BEAT2, andHB-Bracha isO(n2B), the complexity of HoneyBadgerBFT and BEAT0 is O(nB), while the communication complexity of BEAT3 is only O(B). This improvement is significant, as it allows running BEAT in bandwidthrestricted environments, allows more aggressive batching, and greatly improves scalability."
                },
                {
                    "heading": "9 BEAT4 \u2014 FLEXIBLE READ",
                    "text": "This section presents a general optimization for erasure-coded BEAT instances that significantly reduce read bandwidth. For many blockchain applications, particularly data-intensive ones, it is common for clients to read only a fraction of the data block. Additionally, for many applications using smart contracts, clients may be interested in seeing the first few key terms of a large contract instead of the lengthy, detailed, and explanatory terms.\nOur technique relies on a novel erasure-coded reliable broadcast protocol, AVID-FP-Pyramid, that reduces read bandwidth. AVIDFP-Pyramid uses pyramid codes [51]. As reviewed in Sec. 4, a (m+\u04340L+\u04341,m) pyramid code can tolerate arbitrary \u0434 = \u04340+\u04341 erasures. Let n =m+\u04340L+\u04341. We define for a (m+\u04340L+\u04341,m) pyramid code a tailored fingerprinted cross-checksum. Our (m+\u04340L+\u04341,m) fingerprinted cross-checksum fpcc consists of an array fpcc.cc[ ] that holds the hashes of all n coded fragments. The second array fpcc.fp[ ] still containsm values that are fingerprints of the firstm data fragments, and because pyramid codes are linear, all the fingerprints of coded fragments can be derived by thesem fingerprints, just as all the coded fragments can be derived by the originalm fragments.\nWe say a fragment d is consistent with fpcc for index i \u2208 [1..n], if fpcc.cc[i] = h(d) and fingerprint(r ,d)= encode (fpcc.fp[1], \u00b7 \u00b7 \u00b7 , fpcc.fp[m]), where r = H(fpcc.cc[1], \u00b7 \u00b7 \u00b7 , fpcc.cc[n]).\nWe extend the central theorem used in [45, 46] to the case of pyramid codes and to the case for fragments. We derive the following new lemma.\nLemma 9.1. For an (m+\u04340L+\u04341,m) fingerprinted cross-checksum fpcc, any probabilistic adversaryA can produce with negligible probability a target data fragment index (resp., data fragment indexes) and two sets of fragments (that may have different sizes) such that each fragment is consistent with fpcc for its index and they can be decoded into two different data fragments for the target index (resp., different sets of fragments for the target indexes).\nThe target data fragment index(es) may be an index of one of data fragment, indexes of all data fragments, or any number of indexes in between. The two set of fragments that A provides can\nbe of different sizes, and the decoding approaches for two sets may differ (may it be a group level or global level decoding).\nThe proof the lemma is an adaptation to the one due to Hendricks et al. [45, Theorem 3.4]. In proving Theorem 3.4 [45], the key claim is that two different sets ofm fragments for the same fragment indexes and the same consistent fingerprinted cross-checksum imply that at least one fragment from the two sets is different, which is the starting point of their proof. Following the same argument, we can show that the probability that two fragments with the same index are different is bounded by \u03f5 \u2032 + q \u00b7 \u03f5 , where \u03f5 \u2032 is the advantage of attacking the hash function, q is the total number random oracle queries, and \u03f5 is the probability of the collisions in the fingerprinting function. The proof applies to any linear erasure-coding schemes, including pyramid codes. AVID-FP-Pyramid. Now we describe AVID-FP-Pyramid, an asynchronous verifiable information dispersal protocol that compared to AVID-FP, further reduces read bandwidth. Instead of using a conventional MDS erasure code, AVID-FP-Pyramid uses a pyramid code. In an MDS code,m valid fragments can be used to reconstruct the original block. In a pyramid code, we need in generalm+\u04340(L\u22121) valid fragments to reconstruct the block. Therefore, we have to make sure that in our newAVID protocol at leastm+\u04340(L\u22121)+f servers receive consistent fragments, of which f servers might be faulty. Moreover, one needs to make sure thatm +\u04340L +\u04341 \u2265 m +\u04340(L \u2212 1)+ 2f , i.e., f \u2264 (\u04340 +\u04341)/2, which ensures that the total number of replicas do not overflow.\nGiven a pyramid code (n,m) where n = m + \u04340L + \u04341 that can tolerate arbitrary \u0434 = \u04340 + \u04341 erasures, we construct AVID-FPPyramid where f \u2264 m and f \u2264 (\u04340 + \u04341)/2. Specifically, AVID-FPPyramid consists of a triple of protocols (disperse, retrieve, read) which are described as follows. Dispersal. To disperse a block B, a client applies the (n,m) pyramid code to generate n fragments {di }ni=1 and the fingerprinted crosschecksum fpcc. The server then sends each server i its fragment di and fpcc.\nUpon receiving a disperse message, a server i verifies that the fragment di is consistent with fpcc. (Concretely, server i checks if fpcc.cc[i] = h(d) and fingerprint(r ,d)= encode (fpcc.fp[1], \u00b7 \u00b7 \u00b7 , fpcc.fp[m]), where r = H(fpcc.cc[1], \u00b7 \u00b7 \u00b7 , fpcc.cc[n]).) If this is true, the server stores the fragment and sends an echo message containing fpcc (and only fpcc) to all servers.\nUpon receivingm + \u04340(L \u2212 1) + f echo messages with matching fingerprinted cross-checksum fpcc, a server sends a ready message containing fpcc to all servers.\nIf receiving f + 1 ready with matching fingerprinted crosschecksum fpcc, and if a server does not yet send a ready message, it sends a ready message to all other servers.\nUpon receiving 2f + 1 ready messages with matching fpcc, it stores and delivers fpcc. Retrieval. The retrieval protocol is almost the same as that in AVIDFP, with only a parameter difference. To retrieve a block, a client retrieves a fragment and fingerprinted cross-checksum from each server, waiting for matching fingerprinted cross-checksums from f + 1 servers and consistent fragments fromm + \u04340(L \u2212 1) servers. These fragments are then decoded and the resulting block is returned.\nRead. To read a single fragment di , one could choose one of the following two options. In the first option, which we term as the optimistic mode, a client requests from all servers the fingerprinted cross-checksum and only the target server i for the fragment. If it does not receive the fragment in time (set arbitrarily by the client), it queries the servers at the group level that contains the server i , and all servers in the local group should send their fragments. The client will repeat the procedure from the group level until it receives m + \u04340(L \u2212 1) fragments with matching fpcc and then recovers the fragment. In the second, which we term as the balanced mode, a client directly queries all servers at the group level, expecting the fragments from these group level servers. Definition and security.While we could be more general, we provide a definition for AVID-FP-Pyramid that is specifically tailored for our purpose.\nAn (n,m)-asynchronous verifiable information dispersal scheme is a triple of protocols (disperse, retrieve, read) that satisfy the following with high probability: \u2022 Termination: If a correct client initializes disperse(M) then all correct servers will eventually complete dispersal disperse(M). \u2022 Agreement: If some correct server completes disperse(M), all correct servers eventually complete disperse(M). \u2022 Availability: If f + 1 correct servers complete disperse(M), a correct client can run retrieve( ) to eventually reconstruct some blockM \u2032. Additionally, if f + 1 correct servers complete disperse(M), a correct client can run read(i) where i \u2208 [1..m] to eventually obtain a fragment di . \u2022 Correctness: If f + 1 correct servers complete disperse(M), all correct clients that run retrieve( ) eventually retrieve the same block M \u2032. If the client that initiated disperse(M) was correct, then M \u2032 = M . Additionally, if f + 1 correct servers complete disperse(M), all correct clients that run read(i) for i \u2208 [1..m] eventually obtain the same fragmentd \u2032i . If the client that initiated disperse(M) was correct, then d \u2032i = di , where di is the i-th data fragment ofM .\nTheorem 9.2. AVID-FP-Pyramid is an asynchronous verifiable information dispersal protocol as defined above.\nBEAT-FR. Replacing the AVID-FP protocol in BEAT3 with our AVID-FP-Pyramid protocol, we obtain a new BFT storage protocol \u2014 BEAT-FR which has reduced read bandwidth.\nCorollary 9.3. BEAT-FR is a BFT storage.\nInstantiating BEAT-FR: BEAT4. BEAT-FR is a generic asynchronous BFT framework that reduces read bandwidth. BEAT4 is an instantiation to BEAT-FR for concrete parameters. In BEAT4, we set L = 2,m is even, and \u04340 = 1, which allows us to tolerate one failure within the local group, and reduces the read bandwidth by 50%. In BEAT4, we have n =m + 2f + 1,m = f + 1, and n = 3m \u2212 1. Note that the number of echo messages which a replica has to wait before it can send ready message in BEAT4 ism + f . Technique applicability. We comment that our technique presented in the section is general. While it is described for the setting of AVID-FP, it can be applied to all erasure-coded asynchronous verifiable information dispersal and erasure-coded reliable broadcast protocols, including AVID-CT [24] and AVID broadcast [24].\nTherefore, the technique can be used to improve both erasure-coded BFT storage (BEAT3) and general SMR (BEAT0)."
                },
                {
                    "heading": "10 IMPLEMENTATION AND EVALUATION",
                    "text": ""
                },
                {
                    "heading": "10.1 Implementation",
                    "text": "We utilize the HoneyBadgerBFT prototype as the baseline to implement six asynchronous BFT protocols, including five BEAT protocols (BEAT0 to BEAT4) and HB-Bracha. HB-Bracha is implemented to understand the latency overhead caused by erasure coding. HBBracha replaces the underlying erasure-coded reliable broadcast (AVID broadcast) with Bracha\u2019s Broadcast [19], with the rest of the components intact.\nEach of the six protocols involves 6,000 to 8,000 lines of code in Python. The underlying erasure-coding schemes (Reed-Soloman codes and pyramid codes) and fingerprinted cross-checksum, however, are implemented in C. The design and implementation of BEAT is modular, and we have implemented the following building blocks for the protocols. Erasure coding support. HoneyBadgerBFT is 100% Python, and uses the zfec library to implement the Reed-Soloman code, an MDS erasure code. The zfec library, while popular in Python projects, suffers from both efficiency and usability issues: it supports only the traditional Reed-Soloman code implementation and supports only a word size (finite field size, a key tunable parameter in erasure coding for efficiency) of 8. Moreover, due to the usage of an erasure-coding library zfec [93], HoneyBadgerBFT supports at most 28 replicas.\nIn BEAT, we instead use Jerasure 2.0 [73], a C library for erasurecoding, to implement the underlying erasure-coding schemes (including Reed-Soloman codes and pyramid codes). Jerasure 2.0 supports a variety of other coding schemes (including Cauchy ReedSoloman codes [75]), and allows fine-grained parameter tuning. Fingerprinted cross-checksum. We observe that for efficiency reasons one cannot separate the implementation of fingerprinting functions from the underlying erasure-coding support. The only implementation of fingerprinting is due to Hendricks et al. [45, 46]. They implemented their own erasure coding scheme using Rabin\u2019s information dispersal scheme [77] and the corresponding fingerprinted cross-checksum using Shoup\u2019s NTL [84]. While their fingerprinted cross-checksum is efficient, the erasure coding scheme is rather slow.\nIn contrast, we use GF-Complete [74], the Jerasure\u2019s underlying Galois Field library using Intel SSID, to implement the fingerprinted cross-checksum primitive. Erasure coding schemes have three parameters n, m, and w , where n is the number of fragments (also the number of replicas),m is the number of data fragments (where m = f +1 in our protocols), andw is the word size (the index size of the Galois Field GF(2w )). It is required that n +m < 2w and therefore n < 2w . The word sizew is typically set to be between 4 and 16 for efficiency, and indeedw = 32 is the largest value supported by Jerasure. However, for our applications, we need to use larger w = 64 or 128 for the security of fingerprinted cross-checksum. We therefore extend Jerasure to include these largew\u2019s.\nThe specific fingerprinting function we implemented is the evaluation fingerprinting [82]. Currently, we apply Horner\u2019s rule to evaluate the polynomial directly, without leveraging faster lookup tables. While the implementation can be further improved, we find\nthat the implementation can already improve all performance metrics significantly. We implement fingerprinted cross-checksum in C, with 3,500 lines of code.\nFinally, we use Cython [11] to wrap the C code in Python and support functions including Reed-Solomon codes, pyramid codes, matrix generation, coding padding, and fingerprinted cross-checksum. The implementation involves around 1,000 lines of code in Python.3\nThreshold cryptography. We use the TDH2 scheme [85] for CCA-secure labeled threshold encryption and the threshold PRF scheme [22] for distributed coin flipping.We implement both schemes using the Charm [2] Python library. We use NIST recommended P-256 curve to implement both schemes to provide standard 128-bit security."
                },
                {
                    "heading": "10.2 Evaluation",
                    "text": "Overview.We deploy and test our protocols on Amazon EC2 utilizing up to 92 nodes from ten different regions in five different continents. Each node is a general purposed t2.medium type with two virtual CPUs and 4GB memory. We evaluate our protocols in both LAN and WAN settings, where the LAN nodes are selected from the same Amazon EC2 region, and the WAN nodes are uniformly selected from different regions. We evaluate the protocols under different network sizes (number of replicas) and contention levels (batch sizes). For each experiment, we use f to represent the network size, where 3f + 1 nodes are launched in total for BEAT0 to BEAT3, HB-Bracha, and HoneyBadgerBFT (abbreviated as HB in the figures), and 3f + 2 nodes are used for BEAT4. We vary the batch size where nodes propose 1 to 20,000 transactions at a time. Bandwidth. The protocols mentioned above have rather different communication complexity. To order transactions of size B, the communication complexity of BEAT1, BEAT2, and HB-Bracha is O(n2B), the communication complexity of HoneyBadgerBFT and BEAT0 is O(nB), while the communication complexity of BEAT3 is only O(B). The consequence for throughput is significant: with the same bandwidth, BEAT3 and BEAT4 can process an order of magnitude more batched transactions, leading to significantly higher throughput. Our evaluation, however, does not set the bandwidth this way, but rather assumes the bandwidth is ample and assumes all protocols use the same batch size. The readers should be aware that BEAT3 and BEAT4 have much higher throughput if using a larger batch size. Latency. We first evaluate the latency in the LAN setting with f = 1, 2, 5, 10, and 15, respectively. We examine and compare the average latency under no contention where each node proposes a single transaction (with variable size) at a time and no concurrent requests are sent by the clients. In the LAN setting, network latency is relatively small, so the overhead is mainly caused by the protocols themselves. We report our result for f = 1, 2 in Figure 3.\nWhen f = 1, BEAT0, BEAT1, BEAT2, and BEAT3 are around 2\u00d7 faster than HoneyBadger, and when f becomes larger, they are even faster than HoneyBadger. When f = 1, BEAT4 is about as fast as 3PyECLib [76] is popular python library for erasure-coding: it has a Python interface but implements C based library, Liberasurecode [61], which allows us to use existing erasure-coding library such as Jerasure[73] and Intel(R) ISA-L. We choose not to use PyECLib, primarily because the underlying Liberasurecode has implemented data structures that are not necessary for our purpose. We therefore (have to) write our own wrapper for Jerasure and fingerprinted cross-checksum using Cython [11].\nHoneyBadger. This is primarily because BEAT4 has one more node, and the added overhead for the underlying consensus protocols and threshold cryptography is particularly visible when f is small. As f increases, HoneyBadger is much slower than BEAT4. Meanwhile, the difference between BEAT3 and BEAT4 becomes smaller; when f is 15, we barely notice the difference between them (not shown).\nThe differences among BEAT0, BEAT1, and BEAT2 are rather small when the batch size is 1, but becomes much more visible when the batch size becomes larger. However, the difference between BEAT1 and BEAT2 is not as large as the difference between HoneyBadger and HB-Bracha. Meanwhile, when the batch size exceeds 1,000, BEAT0 becomes faster than BEAT1 (not shown).\nWe further assess the latency breakdown for HoneyBadgerBFT, BEAT0, BEAT1, and HB-Bracha in order to better understand why we have these results. As illustrated in Figure 4, we evaluate the time for encrypting transactions, consensus protocols, and decrypting and combining transactions. We find the encryption and decryption for BEAT0 and BEAT1 are about three times faster than those in HoneyBadger and HB-Bracha. In addition, BEAT0 and BEAT1 use threshold PRF to produce the common coins for the consensus, and the latency of the consensus is also reduced by about 50%. HBBracha also achieves lower latency than HoneyBadgerBFT due to the use of latency-optimized Bracha\u2019s broadcast. This also explains why BEAT1 has lower latency than BEAT0 when the batch size is small. Throughput.We evaluate the throughput of the protocols under different contention levels.We present the results in the LAN setting in Figure 5(a) and the the result in the WAN setting in Figure 5(b). Both cases set f = 1. We also show latency vs. throughput in Figure 5(c).\nWe first notice that BEAT0 slightly outperforms HoneyBadgerBFT in both settings. This is expected since BEAT0 employs optimized threshold cryptography. This also matches the result for the latency under no contention. In comparison, while BEAT1, BEAT2, and HB-Bracha are latency optimized, they do not outperform HoneyBadgerBFT in terms of throughput. We observe that in both the LAN setting and WAN setting, BEAT1, BEAT2, and HBBracha achieve higher throughput than HoneyBadgerBFT when the batch size is small. However, when batch size is higher than 5000, all the three protocols have 20% to 30% lower throughput than HoneyBadgerBFT. This is mainly because HB-Bracha consumes higher network bandwidth, which causes degradation when the batch size is large. This underscores the wisdom in designing HoneyBadgerBFT.\nBEAT3 and BEAT4 outperform HoneyBadgerBFT consistently. They also outperform BEAT0, BEAT1, and BEAT2 consistently, though under low contention in the LAN setting, BEAT1 has larger throughput than the other protocols. These results also meet our expectation since BEAT3 and BEAT4 are bandwidth optimized. Again, we stress that we compare the performance of the protocols under the same batch size. BEAT3 and BEAT4 actually use much lower network bandwidth than the other protocols, and so for the same bandwidth budget, BEAT3 and BEAT4 (with more aggressive batching) will achievemuch better throughput compared with other protocols. Scalability.We evaluate the scalability of BEAT0, BEAT3, and HoneyBadger by varying f from 1 to 30. We report our comparison between BEAT3 and HoneyBadger in Figure 5(d) (without BEAT0, for ease of illustration). We observe that the throughput for both protocols is in general higher when the number of replicas is smaller. Peak throughput for BEAT3 is reached in all the cases when the batch size is greater than 15,000. In the most extreme case for our experiment, where f = 30 and batch size is 20,000, the average latency is about 1.5 minutes. As we can see in the figure, BEAT3 outperforms HoneyBadgerBFT in all the cases. However, the difference between BEAT3 and HoneyBadgerBFT becomes smaller as the number of replicas grows. This is in part due to the fact that in large-scale networks, network latency may dominate the overhead of the protocol. BEAT0 has performance between BEAT3 and HoneyBadger, and again when f increases their difference becomes smaller."
                },
                {
                    "heading": "11 LESSONS LEARNED",
                    "text": "We implemented six new protocols (BEAT instances andHB-Bracha). Whilemany of these protocols use similar components, maintaining, deploying, and comparing different BEAT instances takes tremendous effort. While one of our goals is to make BEAT modular and extensible, in practice it is still challenging to develop all the variants of the protocols. This is in part because even for the same function (e.g., threshold encryption), different APIs need to maintained. In fact, changing a small function in a BEAT instance may need to touch a large number of related functions accordingly.\nOn the other hand, we find that perhaps surprisingly, it may be easier to develop and deploy asynchronous BFT than partially synchronous BFT, for at least two reasons. First, protocols assuming partial synchrony rely on view change subprotocols, which are very\ndifficult to implement well from our own experience and from the fact that a significant number of academic papers choose not to implement the view change protocols. Second, because of native robustness against timing and liveness attacks for asynchronous BFT, we simply do not need to take further measures to ensure robustness."
                },
                {
                    "heading": "12 CONCLUSION",
                    "text": "We describe the design and implementation of BEAT, a family of practical asynchronous BFT protocols that are efficient, flexible, versatile, and extensible. We deploy and evaluate the five BEAT protocols using 92 instances on Amazon EC2, and we show BEAT protocols are significantly more efficient than HoneyBadgerBFT, the most efficient asynchronous BFT known. We also develop new distributed system ingredients, including generalized fingerprinted cross-checksum and new asynchronous verifiable information dispersal, which might be of independent interest."
                },
                {
                    "heading": "ACKNOWLEDGMENT",
                    "text": "The authors are indebted to our shepherd Haibo Chen and the CCS reviewers for their helpful comments that greatly improve our paper."
                },
                {
                    "heading": "A CORRECTNESS PROOF",
                    "text": "Proof of Theorem 9.2. Termination is simple, as in AVID-FP. If a correct server initiates disperse, the server erasures codes the transaction, and sends fragments and the fingerprinted cross-checksum to each server. As the server initiating disperse is correct, at least n\u2212 f \u2265 m+\u04340(L\u2212 1)+ f correct servers receive dispersemessages, and send echo messages to all servers. Each server will eventually receivem + \u04340(L \u2212 1) + f echo messages, and then sends a ready message, if it has not done so. Each correct server will eventually receive at lest 2f + 1 ready messages, and will then store the fingerprinted cross-checksum and complete.\nAgreement follows exactly as in AVID-FP. If some correct server completes disperse(M), then the server must have received 2f + 1 ready messages and at least f + 1 ready messages much have come from correct servers. This means that all correct servers will eventually receive ready messages from these correct servers. As our protocol implements the amplification step as in all other Bracha\u2019s broadcast like broadcast, all correct servers will send ready messages, and all of them will eventually receive at least 2f + 1 ready messages. Agreement thus follows.\nWe first prove the first part of availability. In our protocol, if a correct server completes disperse, it must have received 2f +1 ready messages, and at least one correct server receivedm +\u04340(L\u2212 1)+ f echo messages. Therefore, at least m + \u04340(L \u2212 1) correct servers stored consistent fragments. According to the property of pyramid codes, these fragments can be used to reconstruct the original block. Accordingly, if f + 1 correct servers complete disperse, any client that initiates retrievewill receivem+\u04340(L\u22121) consistent fragments and f + 1 matching fingerprinted cross-checksums. The client can then decode the fragments to generate some block.\nWe now prove the second part of availability. Following an analogous line of the above argument, if a correct server completes disperse, at least m + \u04340(L \u2212 1) correct servers stored consistent fragments. If f + 1 correct servers complete disperse, any client that initiates read(i) for i \u2208 [1..m] will receive f + 1 matching fingerprinted cross-checksums. If the fragment i happens to be available or there is less than \u04340 failures in the local group, the fragment will be available for the client. Otherwise, another round of interaction is needed, and the client will obtainm + \u04340(L \u2212 1) consistent fragments and reconstruct the fragment needed.\nWe now prove correctness. We first claim that if some correct server delivers fpcc1 and some correct server delivers fpcc2, then\nfpcc1 = fpcc2. The proof is quite \u201cstandard\u201d for a quorum based protocol: if fpcc1 is delivered thenm +\u04340(L\u2212 1)+ f servers echoed fpcc1, of which at leastm+\u04340(L\u2212 1) is correct. The same applied to fpcc2. As a correct server will only echo once, there are at least 2m+ 2\u04340(L \u2212 1) + f servers echoed, which is larger than the total server (note that L \u2265 2 and 2f \u2264 (\u04340 + \u04341)). This leads to a contradiction. Thus, any block decoded during retrieve or any fragment during read is consistent with the same fpcc. By Theorem 3.4 in [45] and by Lemma 9.1, the probability that clients do not obtain the same block or fragment(s) is negligible."
                }
            ],
            "year": 2018,
            "references": [
                {
                    "title": "Fault-scalable Byzantine fault-tolerant services",
                    "authors": [
                        "M. Abd-El-Malek",
                        "G. Ganger",
                        "G. Goodson",
                        "M.K. Reiter",
                        "J. Wylie"
                    ],
                    "venue": "SOSP",
                    "year": 2005
                },
                {
                    "title": "Charm: a framework for rapidly prototyping cryptosystems",
                    "authors": [
                        "J.A. Akinyele"
                    ],
                    "venue": "Journal of Cryptographic Engineering,",
                    "year": 2013
                },
                {
                    "title": "Prime: Byzantine replication under attack",
                    "authors": [
                        "Y. Amir",
                        "B. Coan",
                        "J. Kirsch",
                        "J. Lane"
                    ],
                    "venue": "IEEE TDSC, 8(4):564\u2013577,",
                    "year": 2011
                },
                {
                    "title": "Erasure-coded Byzantine storage with separate metadata",
                    "authors": [
                        "E. Androulaki",
                        "C. Cachin",
                        "D. Dobre",
                        "M. Vukolic"
                    ],
                    "venue": "OPODIS 2014, pp. 76\u201390,",
                    "year": 2014
                },
                {
                    "title": "Hyperledger Fabric: a distributed operating system for permissioned blockchains",
                    "authors": [
                        "E. Androulaki"
                    ],
                    "year": 2018
                },
                {
                    "title": "The next 700 BFT protocols",
                    "authors": [
                        "P-L. Aublin",
                        "R. Guerraoui",
                        "N. Knezevic",
                        "V. Quema",
                        "M. Vukolic"
                    ],
                    "venue": "TOCS, vol. 32, issue 4, January",
                    "year": 2015
                },
                {
                    "title": "RBFT: Redundant Byzantine fault tolerance",
                    "authors": [
                        "P-L. Aublin",
                        "S. Mokhtar",
                        "V. Quema"
                    ],
                    "venue": "ICDCS",
                    "year": 2013
                },
                {
                    "title": "Simple and efficient threshold cryptosystem from the gap Diffie-Hellman group",
                    "authors": [
                        "J. Baek",
                        "Y. Zheng"
                    ],
                    "venue": "GLOBECOM \u201903, pp. 1491\u20131495,",
                    "year": 2003
                },
                {
                    "title": "Cython: The best of both worlds",
                    "authors": [
                        "S. Behnel"
                    ],
                    "venue": "Computing in Science & Engineering,",
                    "year": 2011
                },
                {
                    "title": "Keying hash functions for message authentication",
                    "authors": [
                        "M. Bellare",
                        "R. Canetti",
                        "H. Krawczyk"
                    ],
                    "venue": "CRYPTO",
                    "year": 1996
                },
                {
                    "title": "Random oracles are practical: A paradigm for designing efficient protocols",
                    "authors": [
                        "M. Bellare",
                        "P. Rogaway"
                    ],
                    "venue": "CCS 93,",
                    "year": 1993
                },
                {
                    "title": "Efficient threshold signature, multisignature and blind signature schemes based on the gap-Diffie-Hellman-group signature scheme",
                    "authors": [
                        "A. Boldyreva"
                    ],
                    "venue": "PKC",
                    "year": 2003
                },
                {
                    "title": "Chosen ciphertext secure public key threshold encryption without random oracles",
                    "authors": [
                        "D. Boneh",
                        "X. Boyen",
                        "S. Halevi"
                    ],
                    "venue": "CT-RSA,",
                    "year": 2006
                },
                {
                    "title": "Asynchronous Byzantine agreement protocols",
                    "authors": [
                        "G. Bracha"
                    ],
                    "venue": "Information and Computation 75, pp. 130\u2013143,",
                    "year": 1987
                },
                {
                    "title": "The Chubby lock service for loosely-coupled distributed systems",
                    "authors": [
                        "M. Burrows"
                    ],
                    "venue": "OSDI,",
                    "year": 2006
                },
                {
                    "title": "Secure and efficient asynchronous broadcast protocols (extended abstract)",
                    "authors": [
                        "C. Cachin",
                        "K. Kursawe",
                        "F. Petzold",
                        "V. Shoup"
                    ],
                    "venue": "CRYPTO",
                    "year": 2001
                },
                {
                    "title": "Secure Intrusion-tolerant replication on the Internet",
                    "authors": [
                        "C. Cachin",
                        "J. Poritz"
                    ],
                    "venue": "DSN 2002, pp. 167\u2013176,",
                    "year": 2002
                },
                {
                    "title": "Asynchronous verifiable information dispersal",
                    "authors": [
                        "C. Cachin",
                        "S. Tessaro"
                    ],
                    "venue": "SRDS",
                    "year": 2005
                },
                {
                    "title": "Optimal resilience for erasure-coded Byzantine distributed storage",
                    "authors": [
                        "C. Cachin",
                        "S. Tessaro"
                    ],
                    "venue": "DSN-DCCS 2006, pp. 115\u2013124,",
                    "year": 2006
                },
                {
                    "title": "Windows Azure Storage: A highly available cloud storage service with strong consistency",
                    "authors": [
                        "B. Calder"
                    ],
                    "venue": "ACM SOSP,",
                    "year": 2011
                },
                {
                    "title": "Practical Byzantine fault tolerance and proactive recovery",
                    "authors": [
                        "M. Castro",
                        "B. Liskov"
                    ],
                    "venue": "ACM Trans. Comput. Syst, 20(4): 398\u2013461,",
                    "year": 2002
                },
                {
                    "title": "Making Byzantine fault tolerant systems tolerate Byzantine faults",
                    "authors": [
                        "A. Clement",
                        "E. Wong",
                        "L. Alvisi",
                        "M. Dahlin",
                        "M. Marchetti"
                    ],
                    "venue": "NSDI",
                    "year": 2009
                },
                {
                    "title": "HQ replication: A hybrid quorum protocol for Byzantine fault tolerance",
                    "authors": [
                        "J. Cowling"
                    ],
                    "year": 2006
                },
                {
                    "title": "PoWerStore: Proofs of writing for efficient and robust storage",
                    "authors": [
                        "D. Dobre",
                        "G. Karame",
                        "W. Li",
                        "M. Majuntke",
                        "N. Suri",
                        "M. Vukolic"
                    ],
                    "venue": "ACM CCS,",
                    "year": 2013
                },
                {
                    "title": "BChain: Byzantine replication with high throughput and embedded reconfiguration",
                    "authors": [
                        "S. Duan",
                        "H. Meling",
                        "S. Peisert",
                        "H. Zhang"
                    ],
                    "venue": "OPODIS",
                    "year": 2014
                },
                {
                    "title": "hBFT: Speculative Byzantine fault tolerance with minimum cost",
                    "authors": [
                        "Sisi Duan",
                        "Sean Peisert",
                        "Karl Levitt"
                    ],
                    "venue": "IEEE Transaction on Dependable and Secure Computing,",
                    "year": 2015
                },
                {
                    "title": "Secure causal atomic broadcast, revisited",
                    "authors": [
                        "S. Duan",
                        "M.K. Reiter",
                        "H. Zhang"
                    ],
                    "venue": "DSN",
                    "year": 2017
                },
                {
                    "title": "Practical state machine replication with confidentiality",
                    "authors": [
                        "S. Duan",
                        "H. Zhang"
                    ],
                    "venue": "SRDS,",
                    "year": 2016
                },
                {
                    "title": "Consensus in the presence of partial synchrony",
                    "authors": [
                        "C. Dwork",
                        "N. Lynch",
                        "L. Stockmeyer"
                    ],
                    "venue": "J. ACM 35(2): 288\u2013323,",
                    "year": 1988
                },
                {
                    "title": "Impossibility of distributed consensus with one faulty process",
                    "authors": [
                        "M. Fischer",
                        "N. Lynch",
                        "M. Paterson"
                    ],
                    "venue": "J. ACM 32(2): 374\u2013382,",
                    "year": 1985
                },
                {
                    "title": "Secure distributed key generation for discrete-log based cryptosystems",
                    "authors": [
                        "R. Gennaro",
                        "S. Jarecki",
                        "H. Krawczyk",
                        "T. Rabin"
                    ],
                    "venue": "J. Cryptology",
                    "year": 2007
                },
                {
                    "title": "Securely replicating authentication services",
                    "authors": [
                        "L. Gong"
                    ],
                    "venue": "ICDCS, pp. 85\u201391, IEEE Computer Society,",
                    "year": 1989
                },
                {
                    "title": "Efficient Byzantinetolerant erasure-coded storage",
                    "authors": [
                        "G.R. Goodson",
                        "J.J. Wylie",
                        "G.R. Ganger",
                        "M.K. Reiter"
                    ],
                    "venue": "DSN-DCCS 2004, pp. 135\u2013144,",
                    "year": 2004
                },
                {
                    "title": "Flat XOR-based erasure codes in storage systems: Constructions, efficient recovery, and tradeoffs",
                    "authors": [
                        "K.M. Greenan",
                        "X. Li",
                        "J.J. Wylie"
                    ],
                    "venue": "IEEE Mass Storage Systems and Technologies,",
                    "year": 2010
                },
                {
                    "title": "Weaver codes: Highly fault tolerant erasure codes for storage systems",
                    "authors": [
                        "J.L. Hafner"
                    ],
                    "venue": "USENIX FAST,",
                    "year": 2005
                },
                {
                    "title": "HoVer erasure codes for disk arrays",
                    "authors": [
                        "J.L. Hafner"
                    ],
                    "venue": "DSN,",
                    "year": 2006
                },
                {
                    "title": "Verifying distributed erasure-coded data",
                    "authors": [
                        "J. Hendricks",
                        "G.R. Ganger",
                        "M.K. Reiter"
                    ],
                    "venue": "PODC 2007, pp. 139\u2013146,",
                    "year": 2007
                },
                {
                    "title": "Low-overhead Byzantine faulttolerant storage",
                    "authors": [
                        "J. Hendricks",
                        "G.R. Ganger",
                        "M.K. Reiter"
                    ],
                    "venue": "SOSP 2007,",
                    "year": 2007
                },
                {
                    "title": "Wait-free synchronization",
                    "authors": [
                        "M. Herlihy"
                    ],
                    "venue": "ACM Transactions on Programming Languages and Systems, 13(1):124\u2013149,",
                    "year": 1991
                },
                {
                    "title": "Linearizability: a correctness condition for concurrent objects",
                    "authors": [
                        "M. Herlihy",
                        "J. Wing"
                    ],
                    "venue": "ACM Transactions on Programming Languages and Systems, 12(3):463\u2013 492,",
                    "year": 1990
                },
                {
                    "title": "Obstruction-free synchronization: Doubleended queues as an example",
                    "authors": [
                        "M. Herlihy",
                        "V. Luchangco",
                        "andM.Moir"
                    ],
                    "venue": "Proceedings of the 23rd International Conference on Distributed Computing Systems,",
                    "year": 2003
                },
                {
                    "title": "NCCloud: Applying network coding for the storage repair in a Cloud-of-Clouds",
                    "authors": [
                        "Y. Hu",
                        "H. Chen",
                        "P. Lee",
                        "Y. Tang"
                    ],
                    "venue": "USENIX FAST,",
                    "year": 2012
                },
                {
                    "title": "Pyramid codes: Flexible schemes to trade space for access efficiency in reliable data storage systems",
                    "authors": [
                        "C. Huang",
                        "M. Chen",
                        "J. Li"
                    ],
                    "venue": "ACM Transactions on Storage (TOS), Volume 9 Issue 1, March 2013. Earlier version in NCA",
                    "year": 2007
                },
                {
                    "title": "Erasure coding in Windows Azure Storage",
                    "authors": [
                        "C. Huang",
                        "H. Simitci",
                        "Y. Xu",
                        "A. Ogus",
                        "B. Calder",
                        "P. Gopalan",
                        "J. Li",
                        "S. Yekhanin"
                    ],
                    "venue": "USENIX ATC\u201912,",
                    "year": 2012
                },
                {
                    "title": "ZooKeeper:Wait-free coordination for Internet-scale systems",
                    "authors": [
                        "P. Hunt",
                        "M. Konar",
                        "F.P. Junqueira",
                        "B. Reed"
                    ],
                    "venue": "USENIX ATC",
                    "year": 2010
                },
                {
                    "title": "Distributed key generation in the wild",
                    "authors": [
                        "A. Kate",
                        "Y. Huang",
                        "I. Goldberg"
                    ],
                    "venue": "IACR Cryptology ePrint Archive 2012:",
                    "year": 2012
                },
                {
                    "title": "Rethinking erasure codes for cloud file systems: Minimizing I/O for recovery and degraded reads",
                    "authors": [
                        "O. Khan",
                        "R. Burns",
                        "J. Plank",
                        "W. Pierce",
                        "C. Huang"
                    ],
                    "venue": "USENIX FAST,",
                    "year": 2012
                },
                {
                    "title": "Zyzzyva: Speculative Byzantine fault tolerance",
                    "authors": [
                        "R. Kotla",
                        "L. Alvisi",
                        "M. Dahlin",
                        "A. Clement",
                        "E. Wong"
                    ],
                    "venue": "SOSP",
                    "year": 2007
                },
                {
                    "title": "Distributed fingerprints and secure information dispersal",
                    "authors": [
                        "H. Krawczyk"
                    ],
                    "venue": "Proceedings of the 12th ACM Symposium on Principles of Distributed Computing, pp. 207\u2013 218, ACM Press,",
                    "year": 1993
                },
                {
                    "title": "Concurrent reading and writing",
                    "authors": [
                        "L. Lamport"
                    ],
                    "venue": "Communications of the ACM 11(20), 806\u2013811,",
                    "year": 1977
                },
                {
                    "title": "Time, clocks, and the ordering of events in a distributed system",
                    "authors": [
                        "L. Lamport"
                    ],
                    "venue": "Comm. ACM 21, 7 (July), 558\u2013565,",
                    "year": 1978
                },
                {
                    "title": "Using time instead of timeout for fault-tolerant distributed systems",
                    "authors": [
                        "L. Lamport"
                    ],
                    "venue": "Trans. Prog. Lang. and Systems 6(2):254\u2013280,",
                    "year": 1984
                },
                {
                    "title": "On interprocess communication",
                    "authors": [
                        "L. Lamport"
                    ],
                    "venue": "Part I: Basic formalism. Distrib. Comput. 1, 2, 77\u201385,",
                    "year": 1986
                },
                {
                    "title": "On interprocess communication",
                    "authors": [
                        "L. Lamport"
                    ],
                    "venue": "Part II: Algorithms.Distrib. Comput. 1, 2, 86\u2013101,",
                    "year": 1986
                },
                {
                    "title": "The Byzantine generals problem",
                    "authors": [
                        "L. Lamport",
                        "R. Shostak",
                        "M. Pease"
                    ],
                    "venue": "ACM Trans. on Programming Languages and Systems 4(3): 382\u2013401,",
                    "year": 1982
                },
                {
                    "title": "On the impact of replica placement to the reliability of distributed brick storage systems",
                    "authors": [
                        "Q. Lian",
                        "W. Chen",
                        "Z. Zhang"
                    ],
                    "venue": "ICDCS 2005, pp. 187\u2013196,",
                    "year": 2005
                },
                {
                    "title": "The Theory of Error Correcting Codes",
                    "authors": [
                        "F.J. MacWilliams",
                        "N.J.A. Sloane"
                    ],
                    "venue": "Amsterdam, North-Holland,",
                    "year": 1977
                },
                {
                    "title": "The honey badger of BFT protocols",
                    "authors": [
                        "A. Miller",
                        "Y. Xia",
                        "K. Croman",
                        "E. Shi",
                        "D. Song"
                    ],
                    "venue": "ACM CCS 16,",
                    "year": 2016
                },
                {
                    "title": "Report on pairing-based cryptography",
                    "authors": [
                        "D. Moody",
                        "R. Peralta",
                        "R. Perlner",
                        "A. Regenscheid",
                        "A. Roginsky",
                        "L. Chen"
                    ],
                    "venue": "Journal of Research of the National Institute of Standards and Technology,",
                    "year": 2015
                },
                {
                    "title": "Signature-free asynchronous Byzantine consensus with t < n/3 and O(n2) messages",
                    "authors": [
                        "A. Mostefaoui",
                        "H. Moumen",
                        "M. Raynal"
                    ],
                    "venue": "PODC",
                    "year": 2014
                },
                {
                    "title": "Screaming fast Galois field arithmetic using Intel SIMD instructions",
                    "authors": [
                        "J. Plank",
                        "K. Greenan",
                        "E. Miller"
                    ],
                    "venue": "FAST 2013,",
                    "year": 2013
                },
                {
                    "title": "Optimizing Cauchy Reed-Solomon codes for fault-tolerant network storage applications",
                    "authors": [
                        "J. Plank",
                        "L. Xu"
                    ],
                    "venue": "NCA",
                    "year": 2006
                },
                {
                    "title": "Efficient dispersal of information for security, load balancing, and fault tolerance",
                    "authors": [
                        "M.O. Rabin"
                    ],
                    "venue": "Journal of the ACM, 36(2):335\u2013348,",
                    "year": 1989
                },
                {
                    "title": "Polynomial codes over certain finite fields",
                    "authors": [
                        "I.S. Reed",
                        "G. Solomon"
                    ],
                    "venue": "J. Soc. Industrial Appl. Math,",
                    "year": 1960
                },
                {
                    "title": "How to securely replicate services",
                    "authors": [
                        "M.K. Reiter",
                        "K. Birman"
                    ],
                    "venue": "ACM TOPLAS, vol. 16 issue 3, pp. 986\u20131009, ACM,",
                    "year": 1994
                },
                {
                    "title": "et al",
                    "authors": [
                        "M. Sathiamoorthy"
                    ],
                    "venue": "XORing elephants: novel erasure codes for big data. Journal Proceedings of the VLDB Endowment volume 6, issue 5, pp. 325\u2013336,",
                    "year": 2013
                },
                {
                    "title": "Implementing fault-tolerant services using the state machine approach: A tutorial",
                    "authors": [
                        "F. Schneider"
                    ],
                    "venue": "ACM Comput. Surveys 22(4): 299\u2013319,",
                    "year": 1990
                },
                {
                    "title": "On fast and provably secure message authentication based on universal hashing",
                    "authors": [
                        "V. Shoup"
                    ],
                    "venue": "CRYPTO \u201996, pages 313\u2013328,",
                    "year": 1996
                },
                {
                    "title": "Practical threshold signatures",
                    "authors": [
                        "V. Shoup"
                    ],
                    "venue": "EUROCRYPT",
                    "year": 2000
                },
                {
                    "title": "NTL: A library for doing number theory. http://shoup.net/ntl 13  CCS \u201918",
                    "authors": [
                        "V. Shoup"
                    ],
                    "venue": "October 15\u201319,",
                    "year": 2018
                },
                {
                    "title": "A Byzantine fault-tolerant ordering service for the Hyperledger Fabric blockchain platform",
                    "authors": [
                        "J. Sousa",
                        "A. Bessani",
                        "M. Vukolic"
                    ],
                    "venue": "DSN",
                    "year": 2018
                },
                {
                    "title": "Byzantine chain replication",
                    "authors": [
                        "R. van Renesse",
                        "C. Ho",
                        "N. Schiper"
                    ],
                    "venue": "OPODIS",
                    "year": 2012
                },
                {
                    "title": "Spin one\u2019s wheels? Byzantine fault tolerance with a spinning primary",
                    "authors": [
                        "G.S. Veronese",
                        "M. Correia",
                        "A.N. Bessani",
                        "L.C. Lung"
                    ],
                    "venue": "SRDS",
                    "year": 2009
                }
            ],
            "id": "SP:8e89bbb3278a047e6e51cf2a1e2ed68dbf84cca5",
            "authors": [
                {
                    "name": "Sisi Duan",
                    "affiliations": []
                },
                {
                    "name": "Michael K. Reiter",
                    "affiliations": []
                },
                {
                    "name": "Haibin Zhang",
                    "affiliations": []
                }
            ],
            "abstractText": "We present BEAT, a set of practical Byzantine fault-tolerant (BFT) protocols for completely asynchronous environments. BEAT is flexible, versatile, and extensible, consisting of five asynchronous BFT protocols that are designed to meet different goals (e.g., different performance metrics, different application scenarios). Due to modularity in its design, features of these protocols can be mixed to achieve even more meaningful trade-offs between functionality and performance for various applications. Through a 92-instance, five-continent deployment of BEAT on Amazon EC2, we show that BEAT is efficient: roughly, all our BEAT instances significantly outperform, in terms of both latency and throughput, HoneyBadgerBFT, the most efficient asynchronous BFT known.",
            "title": "BEAT: Asynchronous BFT Made Practical"
        },
        "Y": {
            "blog_id": "beat-asynchronous-bft-made-practical",
            "summary": [
                "BEAT: asynchronous BFT made practical Duan et al., CCS\u201918  Reaching agreement (consensus) is hard enough, doing it in the presence of active adversaries who can tamper with or destroy your communications is much harder still.",
                "That\u2019s the world of Byzantine fault tolerance (BFT).",
                "We\u2019ve looked at Practical BFT (PBFT) and HoneyBadger on previous editions of The Morning Paper.",
                "Today\u2019s paper, BEAT, builds on top of HoneyBadger to offer BFT with even better latency and throughput.",
                "Asynchronous BFT protocols are arguably the most appropriate solutions for building high-assurance and intrusion-tolerant permissioned blockchains in wide-are (WAN) environments, as these asynchronous protocols are inherently more robust against timing and denial-of-service (DoS) attacks that can be mounted over an unprotected network such as the Internet.",
                "The best performing asynchronous BFT protocol, HoneyBadger , still lags behind the partially synchronous PBFT protocol in terms of throughput and latency.",
                "BEAT is actually a family of five different asynchronous BFT protocols that start from the HoneyBadger baseline and make improvements targeted at different application scenarios.",
                "Unlike HoneyBadgerBFT, which was designed to optimize throughput only, BEAT aims to be flexible and versatile, providing protocol instances optimized for latency, throughput, bandwidth, or scalability (in terms of the number of servers).",
                "The BEAT protocols divide into two groups: those supporting full (general) state-machine replication (SMR), as required e.g. for smart contract use cases (BEAT0, BEAT1, BEAT2); and those that support BFT storage (append-only ledger) use cases only (BEAT3, BEAT4).",
                "The following table summarises the BEAT family and the key distinguishing features of each member.",
                "( Enlarge )  There\u2019s a lot of ground to cover here, but I\u2019ll do my best to give you an overview.",
                "Alongside the BEAT protocols themselves, the paper also includes two new building blocks: the generalized fingerprinted cross-checksum and an asynchronous verifiable information dispersal (AVID) algorithm.",
                "The HoneyBadger baseline  HoneyBadger supports ACS (the asynchronous common subset) meaning that it provides these guarantees:  Validity: if a correct server delivers a set  , then  and  contains the inputs of at least  correct servers.",
                "Agreement: if a correct server delivers a set  , then all correct servers deliver  .",
                "Totality: if  correct servers submit an input, then all correct servers deliver an output.",
                "HoneyBadger uses reliable broadcast (RBC) and asynchronous Byzantine binary agreement (ABA) protocols to achieve its aims.",
                "Threshold signatures are used to provide common coins for ABA, and threshold encryption is used to avoid censorship and achieve liveness.",
                "In a threshold scheme the partial outputs (e.g. decryption shares) of at least t participants need to be combined in order to recover (decrypt) the intended value.",
                "BEAT0: improved security and performance  BEAT0, our baseline protocol, incorporates a more secure and efficient threshold encryption, a direct instantiation of threshold coin-flipping (instead of using threshold signatures), and more flexible and efficient erasure-coding support.",
                "BEAT0\u2019s threshold encryption uses the TDH2 scheme by Shoup and , providing 128-bit security under elliptic curve cryptography.",
                "This gives stronger security and better performance than the scheme used in HoneyBadger.",
                "In place of the zfec erasure coding library used by HoneyBadger, which supports only Reed-Solomon codes and at most 128 servers, BEAT uses the Jerasure library giving access to more efficient erasure coding schemes and lifting the replica restriction.",
                "BEAT1: lower latency  Via a careful study of latency for each HoneyBadgerBFT subprotocol, we find that (1) most of the latency comes from threshold encryption and threshold signatures, and (2) somewhat surprisingly, when the load is small and there is low contention, erasure-coded reliable broadcast (AVID broadcast) causes significant latency.",
                "BEAT1 swaps out the AVID broadcast protocol of BEAT0 for a replication-based reliable broadcast protocol, Bracha\u2019s broadcast .",
                "Under small loads BEAT1 has lower latency.",
                "With small batch sizes BEAT1\u2019s throughput is higher than HoneyBadger / BEAT0, but with larger batch sizes throughput is down by 20-30%.",
                "BEAT2: causal ordering  BEAT2 builds on BEAT1 and also opportunistically moves the use of threshold encryption to the client side.",
                "In BEAT2, when the ciphertexts are delivered, it is too late for the adversary to censor transactions.",
                "Thus, the adversary does not know what transactions to delay, and can only delay transactions from specific clients.",
                "BEAT2 can be combined with anonymous communication networks to achieve full liveness.",
                "BEAT2 additionally achieves causal order, which prevents the adversary from inserting derived transactions before the original, causally prior transactions.",
                "BEAT3: higher throughput for storage use cases  BEAT3 is the first member of the BEAT family targeted for BFT-storage use cases (as opposed to general SMR).",
                "Recall that the safety and liveness properties of BFT storage remain the same as those of general SMR, with the only exception that the state may not be replicated at each server (but instead may be erasure-coded).",
                "BEAT3 can be used for blockchain applications that need append-only ledgers, and specific blockchains where the consensus protocol serves as an ordering service, such as Hyperledger Fabric.",
                "Whereas so far we\u2019ve been using a reliable broadcast protocol (AVID), BEAT3 replaces this with a bandwidth-efficient information dispersal scheme called AVID-FP.",
                "To disperse a block  , AVID requires bandwidth  , whereas AVID-FP can do it in  .",
                "To order transactions of size  , the communication complexity of BEAT0 is  , of BEAT1 and BEAT2 is  , and of BEAT3 is  .",
                "AVID-FP is a bandwidth-efficient AVID (asynchronous verifiable information dispersal) protocol using fingerprinted cross-checksum.",
                "In AVID-FP, given a block B to be dispersed, the dealer applies an (m,n) erasure coding scheme, where  and  \u2026 then it generates the corresponding fingerprinted cross-checksum for B with respect to the erasure coding scheme.",
                "Each server verifies the correctness of its fragment with respect to the fingerprint cross-checksum, \u201cand then, roughly speaking, leverages the (much smaller) fingerprinted cross-checksum in place of the fragment in the original AVID protocol.\u201d  An (n,m) fingerprinted cross-checksum contains a cross-checksum array of n values, and a fingerprint array of m values.",
                "The ith entry in the checksum array contains the hash of the ith coded fragment.",
                "See section 4 in the paper for details of the fingerprint array usage.",
                "BEAT4: partial reads  BEAT4 further reduces read bandwidth using a novel erasure-coded reliable broadcast protocol called AVID-FP-Pyramid.",
                "This supports use cases where clients only need to read a fraction of a data block.",
                "AVID-FD-Pyramid is based on pyramid codes, which trade space for access efficiency in erasure-coded storage systems (about 10% extra space requirement for a 50% drop in access overhead).",
                "Pyramid codes can be efficiently built from any (n, m) systematic and MDS (maximum distance separable) code.",
                "See section 4 in the paper for brief details, or Huang et al. for an in-depth treatment.",
                "BEAT4 uses a 2-level pyramid scheme which can tolerate one failure in each level, and is able to reduce read bandwidth by 50%.",
                "Full details are in section 9 of the paper.",
                "Evaluation  The evaluation is conducted on EC2 with up to 92 nodes from ten different regions in five different continents, using a variety of network sizes and batch sizes.",
                "In the figures that follow,  represents the network size such that BEAT0,1,2 & 3 require  nodes and BEAT4 requires  nodes.",
                "When f=1, BEAT0, BEAT1, BEAT2, and BEAT3 are around 2x faster than HoneyBadger, and when f becomes larger, they are even faster than HoneyBadger.",
                "When f = 1, BEAT4 is about as fast as HoneyBadger\u2026 As f increases, HoneyBadger is much slower than BEAT4.",
                "For throughput, BEAT0 slightly outperforms HoneyBadger.",
                "BEAT1 and BEAT2 achieve higher throughput than HoneyBadger with small batch sizes, but have 20-30% lower throughput at larger batch sizes.",
                "BEAT3 and BEAT4 outperform all the other protocols consistently.",
                "If this write-up has captured your interest, I highly encourage you to go an and read the full paper which contains significantly more detail than I was able to convey here."
            ],
            "author_id": "ACOLYER",
            "pdf_url": "https://www.csee.umbc.edu/~hbzhang/files/beat.pdf",
            "author_full_name": "Adrian Colyer",
            "source_website": "https://blog.acolyer.org/about/",
            "id": 99341732
        }
    },
    "93980002": {
        "X": {
            "sections": [
                {
                    "text": "I. INTRODUCTION\nDecision-making for autonomous driving is hard due to uncertainty on the continuous state of nearby vehicles and, in particular, due to uncertainty over their discrete potential intentions (such as turning at an intersection or changing lanes).\nPrevious approaches have employed hand-tuned heuristics [28, 29, 41] and numerical optimization [17, 21, 42], but these methods fail to capture the coupled dynamic effects of interacting traffic agents. Partially observable Markov decision process (POMDP) solvers [2, 26, 35] offer a theoreticallygrounded framework to capture these interactions, but have difficulty scaling up to real-world scenarios. In addition, current approaches for anticipating future intentions of other traffic agents [1, 22, 24, 25] either consider only the current state of the target vehicle, ignoring the history of its past actions, or rather require expensive collection of training data.\nIn this paper, we present an integrated behavioral anticipation and decision-making system that models behavior for both our vehicle and nearby vehicles as the result of closed-loop\npolicies. This approach is made tractable by considering only a finite set of a priori known policies. Each policy is designed to capture a different high-level behavior, such as following a lane, changing lanes, or turning at an intersection. Our system proceeds in a sequence of two interleaved stages of behavioral prediction and decision-making. In the first stage, we estimate the probability distribution over the potential policies other traffic agents may be executing. To this aim, we leverage Bayesian changepoint detection to estimate which policy a given vehicle was executing at each point in its history of actions, and then infer the likelihood of each potential intention of the vehicle. Furthermore, we propose a statistical test based on changepoint detection to identify anomalous behavior of other vehicles, such as driving in the wrong direction or swerving out of lanes. Individual policies can therefore adjust their behavior to react to anomalous cars.\nIn the second stage, we use this distribution to sample over permutations of other vehicle policies and the policies available for our car, with forward-simulation of these sampled intentions to evaluate their outcomes via a user-defined\nreward function. Our vehicle finally executes the policy that maximizes the expected reward given the sampled outcomes. Thus, our system is able to make decisions based on closedloop interactions between cars in a tractable manner.\nWe evaluate our behavioral prediction system using a realworld autonomous vehicle, and present decision-making results in simulation involving highway traffic scenarios.\nThe central contributions of this paper are: \u2022 A changepoint-based behavioral prediction approach that\nleverages the history of actions of a target vehicle to infer the likelihood of its possible future actions and detect anomalous behavior online. \u2022 A decision-making algorithm that evaluates the outcomes of modeled interactions between vehicles, being able to account for the effect of its actions on the future reactions of other participants. \u2022 An evaluation of the proposed system using both traffic data obtained from a real-world autonomous vehicle and simulated traffic scenarios.\nThis work extends our earlier work [11], where we proposed the strategy of selecting between multiple policies for our car by evaluating them via forward simulation, and demonstrated passing maneuvers using a real-world autonomous vehicle. However, that work did not address anticipation of policies for other cars. In contrast, this paper presents a fully integrated behavioral anticipation and decision-making approach."
                },
                {
                    "heading": "II. RELATED WORK",
                    "text": ""
                },
                {
                    "heading": "A. Related Work on Behavioral Prediction",
                    "text": "Despite the probabilistic nature of the anticipation problem, some approaches in the literature assume no uncertainty on the future states of other participants [10, 31, 33]. Such an approach could be justified in a scenario where vehicles broadcast their intentions over some communications channel, but it is an unrealistic assumption otherwise.\nSome approaches assume a dynamic model of the obstacle and propagate its state using standard filtering techniques such as the extended Kalman filter [13, 18]. Despite providing rigorous probabilistic estimates over an obstacle\u2019s future states, these methods often perform poorly when dealing with nonlinearities in the assumed dynamics model and the multimodalities induced by discrete decisions (e.g. continuing straight, merging, or passing). Some researchers have explored using Gaussian mixture models (GMMs) [14, 22] and contextsensitive models [19, 20] to account for nonlinearities and multiple discrete decisions. However, this approach does not consider the history of previous states of the target object, assigning an equal likelihood to each discrete hypothesis and leading to a conservative estimate.\nA common anticipation strategy in autonomous driving [7, 16, 21] consists in computing the possible goals of a target vehicle by planning from its standpoint, accounting for its current state. This strategy is similar to our factorization of potential driving behavior into a set of policies, but lacks our closed-loop simulation of vehicle interactions.\nRecent work uses Gaussian process (GP) regression to learn typical motion patterns for classification and prediction of agent trajectories [24, 25, 40], particularly in autonomous driving [1, 38, 39]. Nonetheless, these methods require collecting training data to reflect all possible motion patterns the system may encounter, which can be time consuming. For instance, a lane change motion pattern learned in urban roads will not be representative of the same maneuver performed at higher speeds on the highway."
                },
                {
                    "heading": "B. Related Work on Decision Making",
                    "text": "The first instances of decision making systems for autonomous vehicles capable of handling urban traffic situations stem from the 2007 DARPA Urban Challenge [12]. In that event, participants tackled decision making using a variety of solutions ranging from finite state machines (FSMs) [29] and decision trees [28] to several heuristics [41]. However, these approaches were tailored for very specific and simplified situations and were, even according to their authors, \u201cnot robust to a varied world\u201d [41].\nMore recent approaches have addressed the decision making problem for autonomous driving through the lens of trajectory optimization [17, 21, 42]. However, these methods do not model the closed-loop interactions between vehicles, failing to reason about their potential outcomes.\nThe POMDP model provides a mathematically rigorous formulation of the decision making problem in dynamic, uncertain scenarios such as autonomous driving. Unfortunately, finding an optimal solution to most POMDPs is intractable [27, 32]. A variety of general [2, 5, 26, 35, 37] and domainspecific [8] POMDP solvers exist in the literature that seek to approximate the solution. Nonetheless, online application of POMDP solvers [6] remains challenging because they often explore unlikely regions of the belief space.\nThe idea of assuming finite sets of policies to speed up planning has appeared before in the POMDP literature [3, 23, 36]. However, these approaches dedicate significant resources to compute their sets of policies, and as a result they are limited to short planning horizons and relatively small state, observation, and action spaces. In contrast, we propose to exploit domain knowledge to design a set of policies that are readily available at planning time."
                },
                {
                    "heading": "III. PROBLEM FORMULATION",
                    "text": "We first formulate the problem of decision making in dynamic, uncertain environments with tightly coupled interactions between multiple agents as a multiagent POMDP. We then show how we exploit autonomous driving domain knowledge to make approximations to the POMDP formulation, thus enabling principled decisions in a tractable manner."
                },
                {
                    "heading": "A. General Decision Process",
                    "text": "Let V denote the set of vehicles interacting in a local neighborhood of our vehicle, including our controlled vehicle. At time t, a vehicle v \u2208 V can take an action avt \u2208 Av to transition from state xvt \u2208 X v to xvt+1. In our system, a state\nxvt is a tuple of the pose, velocity, and acceleration and an action avt is a tuple of controls for steering, throttle, brake, shifter, and directionals. As a notational convenience, let xt include all state variables xvt for all vehicles at time t, and similarly let at \u2208 A be the actions of all vehicles.\nWe model the vehicle dynamics with a conditional probability function T (xt, at, xt+1) = p(xt+1|xt, at). Similarly, we model observation uncertainty as Z(xt, zvt ) = p(z v t |xt), where zvt \u2208 Zv is the observation made by vehicle v at time t, and zt \u2208 Z is the vector of all sensor observations made by all vehicles. In our system, an observation zvt is a tuple including the estimated poses and velocities of nearby vehicles and an occupancy grid of static obstacles. Further, we model uncertainty on the behavior of other agents with the following driver model: D(xt, zvt , a v t ) = p(a v t |xt, zvt ), where avt \u2208 A is a latent variable that must be inferred from sensor observations. Our vehicle\u2019s goal is to find an optimal policy \u03c0\u2217 that maximizes the expected reward over a given decision horizon H , where a policy is a mapping \u03c0 : X \u00d7 Zv \u2192 Av that yields an action from the current maximum a posteriori (MAP) estimate of the state and an observation:\n\u03c0\u2217 = argmax \u03c0 E [ H\u2211 t=t0 \u222b X R(xt)p(xt) dxt ] , (1)\nwhere R(xt) is a real-valued reward function R : X \u2192 R. The evolution of p(xt) over time is governed by\np(xt+1) = \u222b\u222b\u222b X Z A p(xt+1|xt, at)p(zt|xt)\np(at|xt, zt)p(xt) dat dzt dxt. (2)\nThe driver model D(xt, zvt , a v t ) implicitly assumes that the instantaneous actions of each vehicle are independent of each other, since avt is conditioned only on xt and z v t . However, modeled agents can still react to the observed states of nearby vehicles via zvt . That is to say that vehicles do not collaborate with each other, as would be implied by an action avt dependent on at. Thus, the joint density for a single vehicle v can be written as pv(xvt , x v t+1, z v t , a v t ) = p(x v t+1|xvt , avt )p(zvt |xvt )\np(avt |xvt , zvt )p(xvt ), (3)\nand the independence assumption finally leads to p(xt+1) = \u220f v\u2208V \u222b\u222b\u222b Xv Zv Av pv(xvt , x v t+1, z v t , a v t ) da v t dz v t dx v t .\n(4) Despite assuming independent vehicle actions, marginalizing over the large state, observation and action spaces in Eq. 4 is too expensive to find an optimal policy online in a timely manner. A possible approximation to speed up the process, commonly used by general POMDP solvers [2, 37] is to solve Eq. 1 by drawing samples from p(xt). However, sampling over the full probability space with random walks will yield a large number of low probability samples (see Fig. 1). This paper presents an approach designed to sample from high likelihood scenarios such that the decision-making process is tractable."
                },
                {
                    "heading": "B. Multipolicy Approach",
                    "text": "We make the following approximations to sample from the likely interactions of traffic agents:\n1) At any given time, both our vehicle and other vehicles are executing a policy from a discrete set of policies. 2) We approximate the vehicle dynamics and observation models through deterministic, closed-loop forward simulation of all vehicles with assigned policies.\nThese approximations allow us to evaluate the consequences of our decisions over a limited set of high-level behaviors determined by the available policies (for both our vehicle and other agents), rather than performing the evaluation for every possible control input of every vehicle.\nLet \u03a0 be a discrete set of policies, where each policy captures a specific high-level driving behavior. Let each policy \u03c0 \u2208 \u03a0 be parameterized by a parameter vector \u03b8 capturing variations of the given policy. For example, for a lanefollowing policy, \u03b8 can capture the \u201cdriving style\u201d of the policy by regulating its acceleration profile to be more or less aggressive. We thus reduce the search in Eq. 1 to a limited set of policies. By assuming each vehicle v \u2208 V is executing a policy \u03c0vt \u2208 \u03a0 at time t, the driver model for other agents can be now expressed as:\nD(xt, z v t , a v t , \u03c0 v t ) = p(a v t |xt, zvt , \u03c0vt )p(\u03c0vt |xt, z0:t), (5)\nwhere p(\u03c0vt |xt, z0:t) is the probability that vehicle v is executing the policy \u03c0vt (we describe how we infer this probability in \u00a7IV). Thus, the per-vehicle joint density from Eq. 3 can now be approximated in terms of \u03c0vt :\npv(xvt , x v t+1, z v t , a v t , \u03c0 v t ) = p(x v t+1|xvt , avt )p(zvt |xvt )\np(avt |xvt , zvt , \u03c0vt )p(\u03c0vt |xt, z0:t)p(xvt ). (6)\nFinally, since we have full authority over the policy executed by our controlled car q \u2208 V , we can separate our vehicle from the other agents in p(xt+1) as follows:\np(xt+1) \u2248 \u222b\u222b X q Zq pq(xqt , x q t+1, z q t , a q t , \u03c0 q t ) dz q t dx q t\n\u220f v\u2208V |v 6=q \u2211 \u03a0 \u222b\u222b Xv Zv pv(xvt , x v t+1, z v t , a v t , \u03c0 v t ) dz v t dx v t  . (7) We have thus far factored out the action space from p(xt+1) by assuming actions are given by the available policies. However, Eq. 7 still requires integration over the state and observation spaces. Our second approximation addresses this issue. Given samples from p(\u03c0vt |xt, z0:t) that assign a policy to each vehicle, we simulate forward in time the interactions of our vehicle and other vehicles under their assigned policies, and obtain a corresponding sequence of future states and observations. We are thereby able to evaluate the reward function over the entire decision horizon."
                },
                {
                    "heading": "IV. BEHAVIORAL ANALYSIS AND PREDICTION VIA CHANGEPOINT DETECTION",
                    "text": "In this section, we describe how we infer the probability of the policies executed by other cars and their parameters. Our behavioral anticipation method is based on a segmentation of the history of observed states of each vehicle, where each segment is associated with the policy most likely to have generated the observations in the segment. We obtain this segmentation using Bayesian changepoint detection, which infers the points in the history of observations where the underlying policy generating the observations changes. Thereby, we can compute the likelihood of all available policies for the target car given the observations in the most recent segment, capturing the distribution p(\u03c0vt |xt, z0:t) over the car\u2019s potential policies at the current timestep. Further, full history segmentation allows us to detect anomalous behavior that is not explained by the set of policies in our system. The changepoint-detection procedure is illustrated by the simulation in Fig. 2. We next describe the anticipation method for a single vehicle, which we then apply successively to all nearby vehicles."
                },
                {
                    "heading": "A. Changepoint Detection",
                    "text": "To segment a target car\u2019s history of observed states, we adopt the recently proposed CHAMP algorithm by Niekum et al. [30], which builds upon the work of Fearnhead and Liu [15]. Given the set of available policies \u03a0 and a time series of the observed states of a given vehicle z1:n = (z1, z2, . . . , zn), CHAMP infers the MAP set of times \u03c41, \u03c42, . . . , \u03c4m, at which changepoints between policies have occurred, yielding m+ 1 segments. Thus, the ith segment consists of observations z\u03c4i+1:\u03c4i+1 and has an associated policy \u03c0i \u2208 \u03a0 with parameters \u03b8i.\nThe changepoint positions are modeled as a Markov chain where the transition probabilites are a function of the time since the last changepoint:\np(\u03c4i+1 = t|\u03c4i = s) = g(t\u2212 s), (8)\nwhere g(\u00b7) is a pdf over time, and G(\u00b7) denotes its cdf. Given a segment from time s to t and a policy \u03c0, CHAMP approximates the logarithm of the policy evidence for that segment via the Bayesian information criterion (BIC) [4] as:\nlogL(s, t, \u03c0) \u2248 log p(zs+1:t|\u03c0, \u03b8\u0302)\u2212 1\n2 k\u03c0 log(t\u2212 s), (9)\nwhere k\u03c0 is the number of parameters of policy \u03c0 and \u03b8\u0302 are estimated parameters for policy \u03c0. The BIC is a well-known approximation that avoids marginalizing over the policy parameters and provides a principled penalty against complex policies by assuming a Gaussian posterior around the estimated parameters \u03b8\u0302. Thus, only the ability to fit policies to the observed data is required, which can be achieved via a maximum likelihood estimation (MLE) method of choice (we elaborate on this in \u00a7IV-B).\nAs shown by Fearnhead and Liu [15], the distribution Ct over the position of the first changepoint before time t can be\nestimated efficiently using standard Bayesian filtering and an online Viterbi algorithm. Defining\nPt(j, q) = p(Ct = j, q, Ej , z1:t) (10)\nPMAPt = p(Changepoint at t, Et, z1:t), (11)\nwhere Ej is the event that the MAP choice of changepoints has occurred prior to a given changepoint at time j, results in:\nPt(j, q) = (1\u2212G(t\u2212 j \u2212 1))L(j, t, q)p(q)PMAPj (12)\nPMAPt = max j,q\n[ g(t\u2212 j)\n1\u2212G(t\u2212 j \u2212 1) Pt(j, q)\n] . (13)\nAt any time, the most likely sequence of latent policies (called the Viterbi path) that results in the sequence of observations can be recovered by finding (j, q) that maximize PMAPt , and then repeating the maximization for PMAPj , successively until time zero is reached. Further details on this changepoint detection method are provided by Niekum et al. [30]."
                },
                {
                    "heading": "B. Behavioral Prediction",
                    "text": "In contrast with other anticipation approaches in the literature which consider only the current state of the target vehicle and assign equal likelihood to all its potential intentions [16, 21, 22], here we compute the likelihood of each latent policy by leveraging changepoint detection on the history of observed vehicle states.\nConsider the (m + 1)th segment (the most recent), obtained via changepoint detection and consisting of observations z\u03c4m+1:n. The likelihood and parameters of each latent policy \u03c0 \u2208 \u03a0 for the target vehicle given the present segment can be computed by solving the following MLE problem:\n\u2200\u03c0 \u2208 \u03a0, L(\u03c0) = argmax \u03b8 log p(z\u03c4m+1:n|\u03c0, \u03b8). (14)\nSpecifically, we assume p(z\u03c4m+1:n|\u03c0, \u03b8) to be a multivariate Gaussian with mean at the trajectory \u03c8\u03c0,\u03b8 obtained by simulating forward in time the execution of policy \u03c0 under parameters \u03b8 from timestep \u03c4m + 1:\np(z\u03c4m+1:n|\u03c0, \u03b8) = N (z\u03c4m+1:n;\u03c8\u03c0,\u03b8, \u03c3I), (15)\nwhere \u03c3 is a nuisance parameter capturing modeling error and I is a suitable identity matrix (we discuss our forward simulation of policies further in \u00a7V-B). That is, Eq. 15 essentially measures the deviation of the observed states from those prescribed by the given policy. The policy likelihoods obtained via Eq. 14 capture the probability distribution over the possible policies that the observed vehicle might be executing at the current timestep, which can be represented, using delta functions, as a mixture distribution:\np(\u03c0vt |xt, z0:t) = \u03b7 |\u03a0|\u2211 i=1 \u03b4(\u03b1i) \u00b7 L(\u03c0i), (16)\nwhere \u03b1i is the hypothesis over policy \u03c0i and \u03b7 is a normalizing constant. We can therefore compute the approximated posterior of Eq. 7 by sampling from this distribution for each vehicle, obtaining high-likelihood samples from the coupled interactions of traffic agents."
                },
                {
                    "heading": "C. Anomaly Detection",
                    "text": "The time-series segmentation obtained via changepoint detection allows us to perform online detection of anomalous behavior not modeled by our policies. Inspired by prior work on anomaly detection [9, 25, 34], we first define the properties of anomalous behavior in terms of policy likelihoods, and then compare the observed data against labeled normal patterns in previously-recorded vehicle trajectories. Thus, we define the following two criteria for anomalous behavior:\n1) Unlikelihood against available policies. Anomalous behavior is not likely to be explained by any of the available policies, since they are designed to abide by traffic rules and provide a smooth riding experience. Therefore, behaviors like driving in the wrong direction or crossing a solid line on the highway will not be captured by the available policies. We thus measure the average likelihood among all segments in the vehicle\u2019s history as the global similarity of the observed history to all available policies:\nS = 1 m+ 1 m+1\u2211 i=1 L(\u03c0i), (17)\nwhere \u03c0i is the policy associated with the ith segment. 2) Ambiguity among policies. A history segmentation that\nfluctuates frequently among different policies might be a sign of ambiguity on the segmentation. To express this criterion formally, we first construct a histogram capturing the occurrences of each policy in the vehicle\u2019s segmented history. A histogram with a broad spread indicates frequent fluctuation, whereas one with a single mode is more likely to correspond to normal behavior. We measure this characteristic as the excess kurtosis of the histogram, \u03ba = \u00b54\u03c34 \u2212 3, where \u00b54 is the fourth moment of the mean and \u03c3 is the standard deviation. The excess kurtosis satisfies \u22122 < \u03ba < \u221e. If \u03ba = 0, the histogram resembles a normal distribution, whereas if \u03ba < 0, the histogram presents a broader spread. That is, we seek to identify changepoint sequences where there is no dominant policy.\nUsing these criteria, we define the following normality measure given a vehicle\u2019s MAP choice of changepoints:\nN = 1\n2 [(\u03ba+ 2)S] . (18)\nThis normality measure on the target car\u2019s history can then be compared to that of a set of previously recorded trajectories of other vehicles. We thus define the normality test for the\ncurrent vehicle\u2019s history as N < 0.5\u03b3, where \u03b3 is the minimum normality measure evaluated on the prior time-series."
                },
                {
                    "heading": "V. MULTIPOLICY DECISION-MAKING",
                    "text": "We now present the policy selection procedure for our car (Algorithm 1), which implements the formulation and approximations given in \u00a7III by leveraging the anticipation scheme from \u00a7IV. The algorithm begins by drawing a set of samples s \u2208 S from the distribution over policies of other cars via Eq. 16, where each sample assigns a policy \u03c0v \u2208 \u03a0 to each nearby vehicle v, excluding our car. For each policy \u03c0 available to our car and for each sample s, we roll out forward in time until the decision horizon H all vehicles under the policy assignments (\u03c0, s) with closed loop simulation to yield a set \u03a8 of simulated trajectories \u03c8. We then evaluate the reward r\u03c0,s for each rollout \u03a8, and finally select the policy \u03c0\u2217 maximizing the expected reward. The process continuously repeats in a receding horizon manner. Note that policies that are not applicable given the current state x0, such as an intersection handling policy when driving on the highway, are not considered for selection (line 5). We next discuss three key points of our decision-making procedure: the design of the set of available policies, using forward simulation to roll out potential interactions, and the reward function.\nAlgorithm 1: Policy selection procedure. Input: \u2022 Current MAP estimate of the state, x0. \u2022 Set of available policies \u03a0. \u2022 Policy assignment probabilities (Eq. 16). \u2022 Planning horizon H .\n1 Draw a set of samples s \u2208 S via Eq. 16, where each sample assigns a policy to each nearby vehicle. 2 R \u2190 \u2205 // Rewards for each rollout 3 foreach \u03c0 \u2208 \u03a0 do // Policies for our car 4 foreach s \u2208 S do // Policies for other cars 5 if APPLICABLE(\u03c0, x0) then 6 \u03a8\u03c0,s \u2190 SIMULATEFORWARD(x0, \u03c0, s,H) // \u03a8\u03c0,s captures all vehicles 7 R \u2190 R\u222a{(\u03c0, s, COMPUTEREWARD(\u03a8\u03c0,s))} 8 return \u03c0\u2217 \u2190 SELECTBEST(R)"
                },
                {
                    "heading": "A. Policy Design",
                    "text": "There are many possible design choices for engineering the set of available policies in our approach, which we wish to explore in future work. However, in this work we use a set\nof policies that covers many in-lane and intersection driving situations, comprising the following policies: lane-nominal, drive in the current lane and maintain distance to the car directly in front; lane-change-right/lane-change-left, separate policies for a single lane change in each direction; and turnright, turn-left, go-straight, or yield at an intersection."
                },
                {
                    "heading": "B. Sample Rollout via Forward Simulation",
                    "text": "While it is possible to perform high-fidelity simulation for rolling out sampled policy assignments, a lower-fidelity simulation can capture the necessary interactions between vehicles to make reasonable choices for our vehicle behavior, while providing faster performance. In practice, we use a simplified simulation model for each vehicle that assumes an idealized steering controller. Nonetheless, this simplification still faithfully describes the high-level behavior of the between-vehicle interactions our method reasons about. For vehicles classified as anomalous, we simulate them using a single policy accounting only for their current state and map of the environment, since they are not likely to be modeled by the set of behaviors in our system."
                },
                {
                    "heading": "C. Reward Function",
                    "text": "The reward function for evaluating the outcome of a rollout \u03a8 involving all vehicles is a weighted combination of metrics mq(\u00b7) \u2208 M, with weights wq that express user importance. The construction of a reward function based on a flexible set of metrics derives from our previous work [11], which we extend here to handle multiple potential policies for other vehicles. In our system, typical metrics include the distance to the goal at the end of the evaluation horizon as a measure of accomplishment, minimum distance to obstacles to evaluate safety, a lane choice bias to add a preference for the right lane, and the maximum yaw rate and longitudinal jerk to measure passenger comfort. For a full policy assignment (\u03c0, s) with rollout \u03a8\u03c0,s, we compute the rollout reward r\u03c0,s as the weighted sum r\u03c0,s = \u2211|M| q=1 wqmq(\u03a8\n\u03c0,s). We normalize each mq(\u03a8\u03c0,s) across all rollouts to ensure comparability between metrics. To avoid biasing decisions, we set the weight wq to zero when the range of mq(\u00b7) across all samples is too small to be informative.\nWe finally evaluate each policy reward r\u03c0 for our vehicle as the expected reward over all rollout rewards r\u03c0,s, computed as r\u03c0 = \u2211|S| k=1 r\u03c0,skp(sk), where p(sk) is the joint probability of the policy assignments in sample sk, computed as a product of the per-vehicle assignment probabilities (Eq. 16). We use expected reward to target better average-case performance, as it is easy to become overly conservative when negotiating traffic if one only accounts for worst-case behavior. By weighting by the probability of each sample, we can avoid overcorrecting for low-probability events."
                },
                {
                    "heading": "VI. RESULTS",
                    "text": "To evaluate our behavioral anticipation method and our multipolicy sampling strategy, we use traffic-tracking data collected using our autonomous vehicle platform. We first\nintroduce the traffic-tracking dataset and the vehicle used to collect it. Next, we use this dataset to evaluate our prediction and anomaly detection method and the performance of our multipolicy sampling strategy. Finally, we evaluate our multipolicy approach performing integrated behavioral analysis and decision-making on highway traffic scenarios using our multivehicle simulation engine."
                },
                {
                    "heading": "A. Autonomous Vehicle Platform, Dataset, and Setup",
                    "text": "To collect the traffic-tracking dataset we use in this work, we have used our autonomous vehicle platform (shown in Fig. 3), a 2013 Ford Fusion equipped with a sensor suite including four Velodyne HDL-32E 3D LIDAR scanners, an Applanix POSLV 420 inertial navigation system (INS), GPS, and several other sensors.\nThe vehicle uses prior maps of the area it operates on that capture information about the environment such as LIDAR reflectivity and road height, and are used for localization and tracking of other agents. The road network is encoded as a metric-topological map that provides information about the location and connectivity of road segments, and lanes therein.\nEstimates over the states of other traffic participants are provided by a dynamic object tracker running on the vehicle, which uses LIDAR range measurements. The geometry and location of static obstacles are also inferred onboard using LIDAR measurements.\nThe traffic-tracking dataset consists of 67 dynamic object trajectories recorded in an urban area. Of these 67 trajectories (shown in Fig. 4), 18 correspond to \u201cfollow the lane\u201d maneuvers and 20 to lane change maneuvers, recorded on a divided highway. The remaining 29 trajectories correspond to maneuvers observed at a four-way intersection regulated by stop signs. All trajectories were recorded by the dynamic object tracker onboard the vehicle and extracted from approximately 3.5 h of total tracking data.\nIn all experiments we use a C implementation of our system running on a single 2.8GHz Intel i7 laptop computer."
                },
                {
                    "heading": "B. Behavioral Prediction",
                    "text": "For our system, we are interested in correctly identifying the behavior of target vehicles by associating it to the most likely policy according to the observations. Thus, we evaluate\nour behavioral analysis method in the context of a classification problem, where we want to map each trajectory to the underlying policy (class) that is generating it at the current timestep. The available policies used in this evaluation are:\n\u03a0 = {lane-nominal, lane-change-left, lane-change-right} \u222a\n{turn-right, turn-left, go-straight, yield}, (19)\nwhere the first subset applies to in-lane maneuvers and the second subset applies to intersection maneuvers. For all policies we use a fixed set of parameters tuned empirically to control our autonomous vehicle platform, including maximum longitudinal and lateral accelerations, and allowed distances to nearby cars, among other parameters.\nTo assess each classification as correct or incorrect, we leverage the road network map and compare the final lane where the trajectory actually ends to that predicted by the declared policy. In addition, we assess behavioral prediction performance on subsequences of incremental duration of the input trajectory, measuring classification performance on increasingly longer observation sequences.\nFig. 5 shows the accuracy and precision curves for policy classification over the entire dataset. The ambiguity among hypotheses results in poor performance when only an early stage of the trajectories is used, especially under 30% completion. However, we are able to classify the trajectories with over 85% accuracy and precision after only 50% of the trajectory has\nbeen completed. Note, however, that the closed-loop nature of our policies allows us to maintain safety at all times regardless of anticipation performance."
                },
                {
                    "heading": "C. Anomaly Detection",
                    "text": "We now qualitatively explore the performance of our anomaly detection test. We recorded three additional trajectories corresponding to two bikes and a bus. The bikes crossed the intersection from the sidewalk, while the bus made a significantly wide turn. We run the test on these trajectories and on three additional intersection trajectories using the minimum normality value on the intersection portion of the dataset, \u03b3 = 0.1233. As shown by the results in Fig. 6, our test is able to correctly detect the anomalous behaviors not modeled in our system."
                },
                {
                    "heading": "D. Multipolicy Sampling Performance",
                    "text": "To show that our approach makes decision-making tractable, we assess the sampling performance in terms of the likelihood of the samples using the recorded intersection trajectories. We compare our multipolicy sampling strategy to an uninformed sampling strategy such as those used by general decisionmaking algorithms that do not account for domain knowledge to focus sampling (e.g., Silver and Veness [35], Thrun [37]).\nWe take groups of coupled trajectories from the dataset involving from one to four vehicles negotiating the intersection simultaneously. For each vehicle in each group, we compute, via Eq. 15, the likelihood of the most likely policy \u03c0ML in {turn-right, turn-left, go-straight, yield} according to the corresponding trajectory in the group. We then evaluate the computation time required by each of the two sampling strategies to find a sampled trajectory with a likelihood equal or greater than L(\u03c0ML).\nThe uninformed strategy generates, for each vehicle involved, a trajectory that either remains static for the duration of the trajectory to yield or crosses the intersection at constant speed. This decision is made at random. If the decision is to cross, the direction of the vehicle is determined via random steering wheel angle rates in a simple car kinematic model. Conversely, the multipolicy sampling strategy consists of randomly selecting policies for each vehicle and obtaining their rollouts. The computation times for each strategy are shown in Table I. Times are computed out of 100 simulations for each case (from one to four cars). Although the time required grows dramatically fast for both strategies due to the combinatorial explosion of vehicle intentions, these results show that our multipolicy sampling strategy is able to find high-likelihood samples orders of magnitude faster than an uninformed sampling strategy. A visualization of a sample simulation of this experiment is shown in Fig. 1."
                },
                {
                    "heading": "E. Decision-Making Results",
                    "text": "We tested the full decision-making algorithm with behavioral prediction in a simulated environment with a multi-lane highway scenario involving two nearby cars. Fig. 7(a) shows the scenario used for testing at an illustrative point at half way through the scenario. This simulation uses the same policy models we have developed and tested on our real-world test car [11]. Fig. 7(b) shows the policy reward function, in which the chosen policy is the maximum of the available policies. Note that this decision process is instantaneous, which explains the oscillations when policies are near decision surfaces. We prevent the executed policy from oscillating with a simple pre-emption model that ensures we only switch policies when distinct maneuvers (such as lane-changes) are complete.\nWe collected timing information on different operations in the experiment to evaluate runtime performance. The main expense is forward simulation and metric evaluation for each\nrollout, however, these tasks are easily parallelizable. In the test scenario in which we rollout all sample permutations, the theoretical maximum number of rollouts is 27 given 3 policy options per vehicle, but in practice the maximum number of rollouts was 12, with a mean of 8.6. This smaller number of rollouts is because not all policies are applicable at once. Parallel evaluation performance is bounded by the maximum time for a single rollout, for which the mean worst time was 84ms, and the worst time over the whole experiment was 106ms. Even in the worst case, our real-time decision-making target of 1 Hz is acheiveable."
                },
                {
                    "heading": "VII. CONCLUSION",
                    "text": "We introduced a principled framework for integrated behavioral anticipation and decision-making in environments with extensively coupled interactions between agents. By explicitly modeling reasonable behaviors of both our vehicle and other vehicles as policies, we make informed high-level behavioral decisions that account for the consequences of our actions.\nWe presented a behavior analysis and anticipation system based on Bayesian changepoint detection that infers the likelihood of policies of other vehicles. Furthermore, we provided a normality test to detect unexpected behavior of other traffic participants. We have shown that our behavioral anticipation approach can identify the most-likely underlying policies that explain the observed behavior of other cars, and to detect anomalous behavior not modeled by the policies in our system.\nIn future work we will explicitly model unexpected behavior, such as the appearance of a pedestrian or vehicles occluded by large objects. We can also extend the system to scale to larger environments by strategically sampling policies to focus on those outcomes that most affect our choices. Exploring principled methods for reacting to detected anomalous behavior is also an avenue for future work."
                },
                {
                    "heading": "ACKNOWLEDGMENTS",
                    "text": "This work was supported in part by a grant from Ford Motor Company via the Ford-UM Alliance under award N015392 and in part by DARPA under award D13AP00059.\nThe authors are sincerely grateful to Patrick Carmody for his help in collecting the traffic-tracking data used in this work and to Ryan Wolcott for his helpful comments."
                }
            ],
            "year": 2015,
            "references": [
                {
                    "title": "Probabilistically safe motion planning to avoid dynamic obstacles with uncertain motion patterns",
                    "authors": [
                        "G.S. Aoude",
                        "B.D. Luders",
                        "J.M. Joseph",
                        "N. Roy",
                        "J.P. How"
                    ],
                    "venue": "Auton. Robot.,",
                    "year": 2013
                },
                {
                    "title": "Integrated perception and planning in the continuous space: A POMDP approach",
                    "authors": [
                        "H. Bai",
                        "D. Hsu",
                        "W.S. Lee"
                    ],
                    "venue": "Int. J. Robot. Res.,",
                    "year": 2014
                },
                {
                    "title": "Intention-aware motion planning",
                    "authors": [
                        "T. Bandyopadhyay",
                        "K. Won",
                        "E. Frazzoli",
                        "D. Hsu",
                        "W. Lee",
                        "D. Rus"
                    ],
                    "venue": "Proc. Int. Work. Alg. Foundation of Robotics,",
                    "year": 2013
                },
                {
                    "title": "Pattern Recognition and Machine Learning",
                    "authors": [
                        "C.M. Bishop"
                    ],
                    "venue": "Information Science and Statistics. Springer,",
                    "year": 2007
                },
                {
                    "title": "Solving continuous pomdps: Value iteration with incremental learning of an efficient space representation",
                    "authors": [
                        "S. Brechtel",
                        "T. Gindele",
                        "R. Dillmann"
                    ],
                    "venue": "Proc. Int. Conf. Machine Learning,",
                    "year": 2013
                },
                {
                    "title": "Probabilistic decision-making under uncertainty for autonomous driving using continuous POMDPs",
                    "authors": [
                        "S. Brechtel",
                        "T. Gindele",
                        "R. Dillmann"
                    ],
                    "venue": "In Proc. IEEE Int. Conf. Intell. Transp. Syst.,",
                    "year": 2014
                },
                {
                    "title": "Monte carlo road safety reasoning",
                    "authors": [
                        "A. Broadhurst",
                        "S. Baker",
                        "T. Kanade"
                    ],
                    "venue": "In Proc. IEEE Intell. Veh. Symp.,",
                    "year": 2005
                },
                {
                    "title": "Exploiting domain knowledge in planning for uncertain robot systems modeled as pomdps",
                    "authors": [
                        "S. Candido",
                        "J. Davidson",
                        "S. Hutchinson"
                    ],
                    "venue": "In Proc. IEEE Int. Conf. Robot. and Automation,",
                    "year": 2010
                },
                {
                    "title": "Anomaly detection: A survey",
                    "authors": [
                        "V. Chandola",
                        "A. Banerjee",
                        "V. Kumar"
                    ],
                    "venue": "ACM Computing Surveys,",
                    "year": 2009
                },
                {
                    "title": "Analytic collision anticipation technology considering agents\u2019 future behavior",
                    "authors": [
                        "J. Choi",
                        "G. Eoh",
                        "J. Kim",
                        "Y. Yoon",
                        "J. Park",
                        "B.-H. Lee"
                    ],
                    "venue": "In Proc. IEEE/RSJ Int. Conf. Intell. Robots and Syst.,",
                    "year": 2010
                },
                {
                    "title": "MPDM: Multipolicy decision-making in dynamic, uncertain environments for autonomous driving",
                    "authors": [
                        "A.G. Cunningham",
                        "E. Galceran",
                        "R.M. Eustice",
                        "E. Olson"
                    ],
                    "venue": "In Proc. IEEE Int. Conf. Robot. and Automation,",
                    "year": 2015
                },
                {
                    "title": "Robotic motion planning in dynamic, cluttered, uncertain environments",
                    "authors": [
                        "N. Du Toit",
                        "J. Burdick"
                    ],
                    "venue": "In Proc. IEEE Int. Conf. Robot. and Automation,",
                    "year": 2010
                },
                {
                    "title": "Robot motion planning in dynamic, uncertain environments",
                    "authors": [
                        "N.E. Du Toit",
                        "J.W. Burdick"
                    ],
                    "venue": "IEEE Trans. Robot.,",
                    "year": 2012
                },
                {
                    "title": "On-line inference for multiple changepoint problems",
                    "authors": [
                        "P. Fearnhead",
                        "Z. Liu"
                    ],
                    "venue": "J. Royal Statistical Society: Series B (Statistical Methodology),",
                    "year": 2007
                },
                {
                    "title": "Detection, prediction, and avoidance of dynamic obstacles in urban environments",
                    "authors": [
                        "D. Ferguson",
                        "M. Darms",
                        "C. Urmson",
                        "S. Kolski"
                    ],
                    "venue": "In Proc. IEEE Intell. Veh. Symp.,",
                    "year": 2008
                },
                {
                    "title": "Motion planning in urban environments",
                    "authors": [
                        "D. Ferguson",
                        "T.M. Howard",
                        "M. Likhachev"
                    ],
                    "venue": "J. Field Robot.,",
                    "year": 2008
                },
                {
                    "title": "Probabilistic navigation in dynamic environment using rapidly-exploring random trees and gaussian processes",
                    "authors": [
                        "C. Fulgenzi",
                        "C. Tay",
                        "A. Spalanzani",
                        "C. Laugier"
                    ],
                    "venue": "In Proc. IEEE/RSJ Int. Conf. Intell. Robots and Syst.,",
                    "year": 2008
                },
                {
                    "title": "A probabilistic model for estimating driver behaviors and vehicle trajectories in traffic environments",
                    "authors": [
                        "T. Gindele",
                        "S. Brechtel",
                        "R. Dillmann"
                    ],
                    "venue": "In Proc. IEEE Int. Conf. Intell. Transp. Syst.,",
                    "year": 2010
                },
                {
                    "title": "Learning context sensitive behavior models from observations for predicting traffic situations",
                    "authors": [
                        "T. Gindele",
                        "S. Brechtel",
                        "R. Dillmann"
                    ],
                    "venue": "In Proc. IEEE Int. Conf. Intell. Transp. Syst.,",
                    "year": 2013
                },
                {
                    "title": "Contingency planning over probabilistic obstacle predictions for autonomous road vehicles",
                    "authors": [
                        "J. Hardy",
                        "M. Campbell"
                    ],
                    "venue": "IEEE Trans. Robot.,",
                    "year": 2013
                },
                {
                    "title": "Discrete and continuous, probabilistic anticipation for autonomous robots in urban environments",
                    "authors": [
                        "F. Havlak",
                        "M. Campbell"
                    ],
                    "venue": "IEEE Trans. Robot.,",
                    "year": 2014
                },
                {
                    "title": "Efficient planning under uncertainty with macro-actions",
                    "authors": [
                        "R. He",
                        "E. Brunskill",
                        "N. Roy"
                    ],
                    "venue": "J. Artif. Intell. Res.,",
                    "year": 2011
                },
                {
                    "title": "A Bayesian nonparametric approach to modeling motion patterns",
                    "authors": [
                        "J. Joseph",
                        "F. Doshi-Velez",
                        "A.S. Huang",
                        "N. Roy"
                    ],
                    "venue": "Auton. Robot.,",
                    "year": 2011
                },
                {
                    "title": "Gaussian process regression flow for analysis of motion trajectories",
                    "authors": [
                        "K. Kim",
                        "D. Lee",
                        "I. Essa"
                    ],
                    "venue": "In Proc. IEEE Int. Conf. Comput. Vis.,",
                    "year": 2011
                },
                {
                    "title": "SARSOP: Efficient point-based POMDP planning by approximating optimally reachable belief spaces",
                    "authors": [
                        "H. Kurniawati",
                        "D. Hsu",
                        "W. Lee"
                    ],
                    "venue": "In Proc. Robot.: Sci. & Syst. Conf.,",
                    "year": 2008
                },
                {
                    "title": "On the undecidability of probabilistic planning and related stochastic  optimization problems",
                    "authors": [
                        "O. Madani",
                        "S. Hanks",
                        "A. Condon"
                    ],
                    "venue": "Artificial Intelligence,",
                    "year": 2003
                },
                {
                    "title": "Team Cornell\u2019s Skynet: Robust perception and planning in an urban environment",
                    "authors": [
                        "I. Miller"
                    ],
                    "venue": "J. Field Robot.,",
                    "year": 2008
                },
                {
                    "title": "Junior: The Stanford entry in the Urban Challenge",
                    "authors": [
                        "M. Montemerlo"
                    ],
                    "venue": "J. Field Robot.,",
                    "year": 2008
                },
                {
                    "title": "CHAMP: Changepoint detection using approximate model parameters",
                    "authors": [
                        "S. Niekum",
                        "S. Osentoski",
                        "C.G. Atkeson",
                        "A.G. Barto"
                    ],
                    "venue": "Technical Report CMU-RI-TR- 14-10,",
                    "year": 2014
                },
                {
                    "title": "Collision avoidance method for mobile robot considering motion and personal spaces of evacuees",
                    "authors": [
                        "T. Ohki",
                        "K. Nagatani",
                        "K. Yoshida"
                    ],
                    "venue": "In Proc. IEEE/RSJ Int. Conf. Intell. Robots and Syst.,",
                    "year": 2010
                },
                {
                    "title": "The complexity of Markov decision processes",
                    "authors": [
                        "C.H. Papadimitriou",
                        "J.N. Tsitsiklis"
                    ],
                    "venue": "Mathematics of Operations Research,",
                    "year": 1987
                },
                {
                    "title": "Safe motion planning in dynamic environments",
                    "authors": [
                        "S. Petti",
                        "T. Fraichard"
                    ],
                    "venue": "In Proc. IEEE/RSJ Int. Conf. Intell. Robots and Syst.,",
                    "year": 2005
                },
                {
                    "title": "On-line trajectory clustering for anomalous events detection",
                    "authors": [
                        "C. Piciarelli",
                        "G. Foresti"
                    ],
                    "venue": "Pattern Recognition Letters,",
                    "year": 2006
                },
                {
                    "title": "Monte-carlo planning in large POMDPs",
                    "authors": [
                        "D. Silver",
                        "J. Veness"
                    ],
                    "venue": "Advances in Neural Information Processing Systems",
                    "year": 2010
                },
                {
                    "title": "DESPOT: Online POMDP planning with regularization",
                    "authors": [
                        "A. Somani",
                        "N. Ye",
                        "D. Hsu",
                        "W.S. Lee"
                    ],
                    "venue": "Advances in Neural Information Processing Systems",
                    "year": 2013
                },
                {
                    "title": "Monte Carlo POMDPs",
                    "authors": [
                        "S. Thrun"
                    ],
                    "venue": "Proc. Advances Neural Inform. Process. Syst. Conf., pages 1064\u20131070,",
                    "year": 2000
                },
                {
                    "title": "Modelling of traffic situations at urban intersections with probabilistic non-parametric regression",
                    "authors": [
                        "Q. Tran",
                        "J. Firl"
                    ],
                    "venue": "In Proc. IEEE Intell. Veh. Symp.,",
                    "year": 2013
                },
                {
                    "title": "Online maneuver recognition and multimodal trajectory prediction for intersection assistance using non-parametric regression",
                    "authors": [
                        "Q. Tran",
                        "J. Firl"
                    ],
                    "venue": "In Proc. IEEE Intell. Veh. Symp.,",
                    "year": 2014
                },
                {
                    "title": "Unfreezing the robot: Navigation in dense, interacting crowds",
                    "authors": [
                        "P. Trautman",
                        "A. Krause"
                    ],
                    "venue": "In Proc. IEEE/RSJ Int. Conf. Intell. Robots and Syst.,",
                    "year": 2010
                },
                {
                    "title": "Autonomous driving in urban environments: Boss and the Urban Challenge",
                    "authors": [
                        "C. Urmson"
                    ],
                    "venue": "J. Field Robot.,",
                    "year": 2008
                },
                {
                    "title": "A real-time motion planner with trajectory optimization for autonomous vehicles",
                    "authors": [
                        "W. Xu",
                        "J. Wei",
                        "J. Dolan",
                        "H. Zhao",
                        "H. Zha"
                    ],
                    "venue": "In Proc. IEEE Int. Conf. Robot. and Automation,",
                    "year": 2012
                }
            ],
            "id": "SP:8c05cdd39c8e56780f4a7d321794b6bc7cac2b72",
            "authors": [
                {
                    "name": "Enric Galceran",
                    "affiliations": []
                },
                {
                    "name": "Alexander G. Cunningham",
                    "affiliations": []
                },
                {
                    "name": "Ryan M. Eustice",
                    "affiliations": []
                },
                {
                    "name": "Edwin Olson",
                    "affiliations": []
                }
            ],
            "abstractText": "To operate reliably in real-world traffic, an autonomous car must evaluate the consequences of its potential actions by anticipating the uncertain intentions of other traffic participants. This paper presents an integrated behavioral inference and decision-making approach that models vehicle behavior for both our vehicle and nearby vehicles as a discrete set of closedloop policies that react to the actions of other agents. Each policy captures a distinct high-level behavior and intention, such as driving along a lane or turning at an intersection. We first employ Bayesian changepoint detection on the observed history of states of nearby cars to estimate the distribution over potential policies that each nearby car might be executing. We then sample policies from these distributions to obtain high-likelihood actions for each participating vehicle. Through closed-loop forward simulation of these samples, we can evaluate the outcomes of the interaction of our vehicle with other participants (e.g., a merging vehicle accelerates and we slow down to make room for it, or the vehicle in front of ours suddenly slows down and we decide to pass it). Based on those samples, our vehicle then executes the policy with the maximum expected reward value. Thus, our system is able to make decisions based on coupled interactions between cars in a tractable manner. This work extends our previous multipolicy system [11] by incorporating behavioral anticipation into decision-making to evaluate sampled potential vehicle interactions. We evaluate our approach using real-world traffic-tracking data from our autonomous vehicle platform, and present decision-making results in simulation involving highway traffic scenarios.",
            "title": "Multipolicy Decision-Making for Autonomous Driving via Changepoint-based Behavior Prediction"
        },
        "Y": {
            "blog_id": "multipolicy-decision-making",
            "summary": [
                "This paper presents an integrated behavioral anticipation and decision-making system that models behavior for both our vehicle and nearby vehicles as the result of closed-loop policies.",
                "Only a finite set of a priori known policies are considered.",
                "Bayesian changepoint detection is used to estimate which policy a given vehicle was executing at each point in its history of actions, then infer the likelihood of each potential intention of the vehicle.",
                "A statistical test is proposed based on changepoint detection to identify anomalous behavior of other vehicles, such as driving in the wrong direction or swerving out of lanes.",
                "Evidence  Anomaly detection was explored by recording three trajectories corresponding to two bikes and a bus.",
                "The bikes crossed an intersection from a sidewalk, while the bus made a significantly wide turn.",
                "System was able to detect these trajectories as anomalous (Not within the set of known policies)  Evaluated in simulated driving environment  Notes  Bayesian Changepoint detection infers the points in the history of observations where the underlying policy that generated the observations changed.",
                "Then, the likelihood of all available policies for the target car given the distribution over the car\u2019s potential policies at the current timestep can be computed (sounds like HMM).",
                "The CHAMP algorithm infers the maximum a posteriori set of times at which changepoints between policies have occurred, yielding a set of segments.",
                "Given a segment from time s to t and a policy pi, CHAMP approx the log of the policy-evidence for that segment via the (Bae)yesian information criterion (BIC)  Viterbi path is found for the most likely sequence of latent policies  For decision-making, a set of samples are drawn from the distribution over policies of other cars where each sample assigns a policy to each nearby vehicle, excluding the ego car.",
                "For each policy available to the ego car (not all policies are available in every scenario e.g. intersection handling policy is not applicable when driving on a highway), and for each sample s, the process is rolled out forward in time until the decision horizon.",
                "This yields a set of simulated trajectories.",
                "The reward is evaluated for each element of the set of simulated trajectories and the maximal policy for the ego vehicle is chosen.",
                "This repeats continuously in a receding horizon manner.",
                "Reward function  distance to the goal at the end of the evaluation horizon  minimum distance to obstacles to evaluate safety  lane choice bias to add a preference for the right lane  maximum yaw rate and longitudinal jerk to measure passenger comfort"
            ],
            "author_id": "pemami",
            "pdf_url": "http://www.roboticsproceedings.org/rss11/p43.pdf",
            "author_full_name": "Patrick Emami",
            "source_website": "https://pemami4911.github.io/index.html",
            "id": 93980002
        }
    },
    "67930407": {
        "X": {
            "sections": [
                {
                    "heading": "1 INTRODUCTION",
                    "text": "The brain is an abduction machine, continuously trying to prove abductively that the observables in its environment constitute a coherent situation.\n\u2013 Jerry Hobbs, ACL 2013 Lifetime Achievement Award1\nAbductive reasoning is inference to the most plausible explanation for incomplete observations (Peirce, 1965a). Figure 1 illustrates an example. Given the incomplete observations about the world that O1: \u201cJenny cleaned her house and went to work, leaving the window just a crack open.\u201d and sometime later O2: \u201cWhen Jenny returned home, she saw her house was a mess.\u201d, we can hypothesize different potential explanations and reason about which is the most likely. We can readily rule out H3 since it fails to justify the observation O2. While H1 and H2 are both plausible, the most likely explanation based on commonsense is H1 as H2 is somewhat implausible given O1.\nOne crucial observation Peirce makes about abductive reasoning is that abduction is \u201cthe only logical operation which introduces any new ideas\u201d, which contrasts with other types of inference such as entailment, that focuses on inferring only such information that is already provided in the premise.\n\u2217Work done while at AI2 1The full transcript of his award speech is available at https://www.mitpressjournals.org/\ndoi/full/10.1162/COLI_a_00171\nar X\niv :1\n90 8.\n05 73\n9v 2\n[ cs\n.C L\n] 1\n4 Fe\nb 20\nAbductive reasoning has long been considered to be at the core of understanding narratives (Hobbs et al., 1988), reading between the lines (Norvig, 1987; Charniak & Shimony, 1990), reasoning about everyday situations (Peirce, 1965b; Andersen, 1973), and counterfactual reasoning (Pearl, 2002; Pearl & Mackenzie, 2018). Despite the broad recognition of its importance, however, the study of abductive reasoning in narrative text has very rarely appeared in the NLP literature, in large part because most previous work on abductive reasoning has focused on formal logic, which has proven to be too rigid to generalize to the full complexity of natural language.\nIn this paper, we present the first study to investigate the viability of language-based abductive reasoning. This shift from logic-based to language-based reasoning draws inspirations from a significant body of work on language-based entailment (Bowman et al., 2015; Williams et al., 2018b), language-based logic (Lakoff, 1970; MacCartney & Manning, 2007), and language-based commonsense reasoning (Mostafazadeh et al., 2016; Zellers et al., 2018). In particular, we investigate the use of natural language as the representation medium, and probe deep neural models on language-based abductive reasoning.\nMore concretely, we propose Abductive Natural Language Inference (\u03b1NLI) and Abductive Natural Language Generation (\u03b1NLG) as two novel reasoning tasks in narrative contexts.2 We formulate \u03b1NLI as a multiple-choice task to support easy and reliable automatic evaluation: given a context, the task is to choose the more likely explanation from a given pair of hypotheses choices. We also introduce a new challenge dataset, ART, that consists of 20K narratives accompanied by over 200K explanatory hypothesis.34 We then establish comprehensive baseline performance based on state-of-the-art NLI and language models. The best baseline for \u03b1NLI based on BERT achieves 68.9% accuracy, with a considerable gap compared to human performance of 91.4%(\u00a75.2). The best generative model, based on GPT2, performs well below human performance on the \u03b1NLG task (\u00a75.2). Our analysis leads to insights into the types of reasoning that deep pre-trained language models fail to perform \u2014 despite their strong performance on the closely related but different task of entailment NLI \u2014 pointing to future research directions."
                },
                {
                    "heading": "2 TASK DEFINITION",
                    "text": "Abductive Natural Language Inference We formulate \u03b1NLI as multiple choice problems consisting of a pair of observations as context and a pair of hypothesis choices. Each instance in ART is defined as follows:\n\u2022 O1: The observation at time t1. 2\u03b1NLI and \u03b1NLG are pronounced as alpha-NLI and alpha-NLG, respectively 3ART: Abductive Reasoning in narrative Text. 4Data available to download at http://abductivecommonsense.xyz\n\u2022 O2: The observation at time t2 > t1. \u2022 h+: A plausible hypothesis that explains the two observations O1 and O2. \u2022 h\u2212: An implausible (or less plausible) hypothesis for observations O1 and O2.\nGiven the observations and a pair of hypotheses, the \u03b1NLI task is to select the most plausible explanation (hypothesis).\nAbductive Natural Language Generation \u03b1NLG is the task of generating a valid hypothesis h+ given the two observations O1 and O2. Formally, the task requires to maximize P (h+|O1, O2)."
                },
                {
                    "heading": "3 MODELS FOR ABDUCTIVE COMMONSENSE REASONING",
                    "text": ""
                },
                {
                    "heading": "3.1 ABDUCTIVE NATURAL LANGUAGE INFERENCE",
                    "text": "A Probabilistic Framework for \u03b1NLI: A distinct feature of the \u03b1NLI task is that it requires jointly considering all available observations and their commonsense implications, to identify the correct hypothesis. Formally, the \u03b1NLI task is to select the hypothesis h\u2217 that is most probable given the observations.\nh\u2217 = arg max hi P (H = hi|O1, O2) (1)\nRewriting the objective using Bayes Rule conditioned on O1, we have:\nP (hi|O1, O2) \u221d P (O2|hi, O1)P (hi|O1) (2)\nWe formulate a set of probabilistic models for \u03b1NLI that make various independence assumptions on Equation 2 \u2013 starting from a simple baseline that ignores the observations entirely, and building up to a fully joint model. These models are depicted as Bayesian Networks in Figure 2.\nHypothesis Only: Our simplest model makes the strong assumption that the hypothesis is entirely independent of both observations, i.e. (H \u22a5 O1, O2), in which case we simply aim to maximize the marginal P (H).\nFirst (or Second) Observation Only: Our next two models make weaker assumptions: that the hypothesis depends on only one of the first O1 or second O2 observation.\nLinear Chain: Our next model uses both observations, but considers each observation\u2019s influence on the hypothesis independently, i.e. it does not combine information across the observations. Formally, the model assumes that the three variables \u3008O1, H,O2\u3009 form a linear Markov chain, where the second observation is conditionally independent of the first, given the hypothesis (i.e. (O1 \u22a5 O2|H)). Under this assumption, we aim to maximize a somewhat simpler objective than Equation 2:\nh\u2217 = arg max hi P (O2|hi)P (hi|O1) where (O1 \u22a5 O2|H) (3)\nFully Connected: Finally, our most sophisticated model jointly models all three random variables as in Equation 2, and can in principle combine information across both observations to choose the correct hypothesis.\nTo help illustrate the subtle distinction between how the Linear Chain and Fully Connected models consider both observations, consider the following example. Let observation O1: \u201cCarl went to the store desperately searching for flour tortillas for a recipe.\u201d and O2: \u201cCarl left the store very frustrated.\u201d. Then consider two distinct hypotheses, an incorrect h1: \u201cThe cashier was rude\u201d and the correct h2: \u201cThe store had corn tortillas, but not flour ones.\u201d. For this example, a Linear Chain model could arrive at the wrong answer, because it reasons about the observations separately\u2014taking O1 in isolation, both h1 and h2 seem plausible next events, albeit each a priori unlikely. And for O2 in isolation\u2014i.e. in the absence of O1, as for a randomly drawn shopper\u2014the h1 explanation of a rude cashier seems a much more plausible explanation of Carl\u2019s frustration than are the details of the store\u2019s tortilla selection. Combining these two separate factors leads the Linear Chain to select h1 as the more plausible explanation. It is only by reasoning about Carl\u2019s goal in O1 jointly with his frustration in O2, as in the Fully Connected model, that we arrive at the correct answer h2 as the more plausible explanation.\nIn our experiments, we encode the different independence assumptions in the best performing neural network model. For the hypothesis-only and single observation models, we can enforce the independencies by simply restricting the inputs of the model to only the relevant variables. On the other hand, the Linear Chain model takes all three variables as input, but we restrict the form of the model to enforce the conditional independence. Specifically, we learn a discriminative classifier:\nPLinear Chain(h|O1, O2) \u221d e\u03c6(O1,h)+\u03c6 \u2032(h,O2)\nwhere \u03c6 and \u03c6\u2032 are neural networks that produce scalar values."
                },
                {
                    "heading": "3.2 ABDUCTIVE NATURAL LANGUAGE GENERATION",
                    "text": "Given h+= {wh1 . . . whl }, O1={wo11 . . . wo1m } and O2={wo21 . . . wo2n } as sequences of tokens, the \u03b1NLG task can be modeled as P (h+|O1, O2) = \u220f P (whi |wh<i, wo11 . . . wo1m , wo21 . . . wo2n ) Optionally, the model can also be conditioned on background knowledge K. Parameterized models can then be trained to minimize the negative log-likelihood over instances in ART:\nL = \u2212 N\u2211 i=1 logP (whi |wh<i, wo11 . . . wo1m , wo21 . . . wo2n ,K) (4)\n4 ART DATASET: ABDUCTIVE REASONING IN NARRATIVE TEXT\nART is the first large-scale benchmark dataset for studying abductive reasoning in narrative texts. It consists of \u223c20K narrative contexts (pairs of observations \u3008O1, O2\u3009) with over 200K explanatory hypotheses. Table 6 in the Appendix summarizes corpus-level statistics of the ART dataset.5 Figure 4 shows some illustrative examples from ART (dev split). The best model based on BERT fails to correctly predict the first two dev examples.\n5We will publicly release the ART dataset upon acceptance.\nCollecting Observations: The pairs O1, O2 in ART are drawn from the ROCStories dataset (Mostafazadeh et al., 2016). ROCStories is a large collection of short, manually curated fivesentence stories. It was designed to have a clear beginning and ending for each story, which naturally map to the first (O1) and second (O2) observations in ART.\nCollecting Hypotheses Options: We crowdsourced the plausible and implausible hypotheses options on Amazon Mechanical Turk (AMT) in two separate tasks6:\n1. Plausible Hypothesis Options: We presented O1 and O2 as narrative context to crowdworkers who were prompted to fill in \u201cWhat happened in-between?\u201d in natural language. The design of the task motivates the use of abductive reasoning to hypothesize likely explanations for the two given observations.\n2. Implausible Hypothesis Options: In this task, we presented workers with observationsO1,O2 and one plausible hypothesis option h+ \u2208 H+ collected from the previous task. Crowdworkers were instructed to make minimal edits (up to 5 words) to a given h+ to create implausible hypothesis variations for each plausible hypothesis.\nA significant challenge in creating datasets is avoiding annotation artifacts \u2013 unintentional patterns in the data that leak information about the target label \u2013 that several recent studies (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018) have reported on crowdsourced datasets . To tackle this challenge, we collect multiple plausible and implausible hypotheses for each \u3008O1, O2\u3009 pair (as described above) and then apply an adversarial filtering algorithm to retain one challenging pair of hypotheses that are hard to distinguish between. We describe our algorithm in detail in Appendix A.5. While our final dataset uses BERT as the adversary, preliminary experiments that used GPT as an adversary resulted in similar drops in performance of all models, including all BERT variants. We compare the results of the two adversaries in Table 1."
                },
                {
                    "heading": "5 EXPERIMENTS AND RESULTS",
                    "text": "We now present our evaluation of finetuned state-of-the-art pre-trained language models on the ART dataset, and several other baseline systems for both \u03b1NLI and \u03b1NLG. Since \u03b1NLI is framed as a binary classification problem, we choose accuracy as our primary metric. For \u03b1NLG, we report performance on automated metrics such as BLEU (Papineni et al., 2002), CIDEr (Vedantam et al., 2015), METEOR (Banerjee & Lavie, 2005) and also report human evaluation results."
                },
                {
                    "heading": "5.1 ABDUCTIVE NATURAL LANGUAGE INFERENCE",
                    "text": "6Both crowdsourcing tasks are complex and require creative writing. Along with the ART dataset, we will publicly release templates and the full set of instructions for all crowdsourcing tasks to facilitate future data collection and research in this direction.\nModel GPT AFAcc. (%) ART Acc. (%)\nRandom (2-way choice) 50.1 50.4 Majority (from dev set) 50.1 50.8 Infersent (Conneau et al., 2017) 50.9 50.8 ESIM+ELMo (Chen et al., 2017) 58.2 58.8 Finetuning Pre-trained LMs\nGPT-ft 52.6 (0.9) 63.1 (0.5) BERT-ft [hi Only] 55.9 (0.7) 59.5 (0.2) BERT-ft [O1 Only] 63.9 (0.8) 63.5 (0.7) BERT-ft [O2 Only] 68.1 (0.6) 66.6 (0.2) BERT-ft [Linear Chain] 65.3 (1.4) 68.9 (0.5) BERT-ft [Fully Connected] 72.0 (0.5) 68.6 (0.5)\nHuman Performance - 91.4\nTable 1: Performance of baselines and finetuned-LM approaches on the test set of ART. Test accuracy is reported as the mean of five models trained with random seeds, with the standard deviation in parenthesis.\nDespite strong performance on several other NLP benchmark datasets, the best baseline model based on BERT achieves an accuracy of just 68.9% on ART compared to human performance of 91.4%. The large gap between human performance and that of the best system provides significant scope for development of more sophisticated abductive reasoning models. Our experiments show that introducing the additional independence assumptions described in Section 3.1 over the fully connected model tends to degrade system performance (see Table 1) in general.\nHuman Performance We compute human performance using AMT. Each instance (two observations and two hypothesis choices) is shown to three workers who were prompted to choose the more plausible hypothesis choice.7 We compute majority vote on the labels assigned which\nleads to a human accuracy of 91.4% on the ART test set.\nBaselines We include baselines that rely on simple features to verify that ART is not trivially solvable due to noticeable annotation artifacts, observed in several crowdsourced datasets. The accuracies of all simple baselines are close to chance-performance on the task \u2013 indicating that the dataset is free of simple annotation artifacts.\nA model for the related but distinct task of entailment NLI (e.g. SNLI) forms a natural baseline for \u03b1NLI. We re-train the ESIM+ELMo (Chen et al., 2017; Peters et al., 2018) model as its performance on entailment NLI (88.9%) is close to state-of-the-art models (excluding pre-trained language models). This model only achieves an accuracy of 58.8% highlighting that performing well on ART requires models to go far beyond the linguistic notion of entailment.\nPre-trained Language Models BERT (Devlin et al., 2018) and GPT (Radford, 2018) have recently been shown to achieve state-of-the-art results on several NLP benchmarks (Wang et al., 2018). We finetune both BERT-Large and GPT as suggested in previous work and we present each instance in their natural narrative order. BERT-ft (fully connected) is the best performing model achieving 68.9% accuracy, compared to GPT\u2019s 63.1%.8 Our AF approach was able to reduce BERT performance from over 88% by 20 points."
                },
                {
                    "heading": "Learning Curve and Dataset Size",
                    "text": "While there is enough scope for considerably scaling up the dataset based on ROCStories, the learning curve in Figure 5 shows that the performance of the best\nmodel plateaus after \u223c10, 000 instances. In addition, there is still a wide gap (\u223c23%) between the performance of the best model and human performance.\n7Additional crowdsourcing details in the Appendix A.1 8The input format for the GPT model and BERT variants is described in the Appendix A.4.\nGPT Adversary Table 1 also includes results of our experiments where GPT was used as the adversary. Notably, in this case, adversarially filtering the dataset brings down GPT performance under 53%. On the other hand, the best BERT model, that encodes the fully connected bayesian network performs significantly better than the BERT model that encodes the linear chain assumptions \u2013 72% compared to 65%. Therefore, we use the BERT fully connected model as the adversary in ART. The gap between the linear chain and fully connected BERT models diminishes when BERT is used as an adversary \u2013 in spite of being a more powerful model \u2013 which indicates that adversarial filtering disproportionately impacts the model used as the adversary. However, the dataset also becomes more difficult for the other models that were not used as adversaries. For example, before any filtering, BERT scores 88% and OpenGPT gets 80%, which is much higher than either model achieves in Table 1 when the other model is used for filtering. This result is a reasonable indicator, albeit not a guarantee, that ART will remain challenging for new models released in the future."
                },
                {
                    "heading": "5.2 ABDUCTIVE NATURAL LANGUAGE GENERATION",
                    "text": "Generative Language Models As described in Equation 4, we train GPT2 conditioned on the tokens of the two observations O1 and O2. Both observations are enclosed with field-specific tags. ATOMIC (Sap et al., 2019), a repository of inferential if-then knowledge is a natural source of background commonsense required to reason about narrative contexts in ART. Yet, there is no straightforward way to include such knowledge into a neural model as ATOMIC\u2019s nodes are not canonicalized and are represented as short phrases of text. Thus, we rely on COMeT \u2013 a transformer model trained on ATOMIC that generates nine commonsense inferences of events in natural language.9 Specifically, we experiment with two ways of integrating information from COMeT in GPT2: (i) as textual phrases, and (ii) as embeddings.\nFigure 3 shows how we integrate COMeT representations. Concretely, after the input tokens are embedded by the word-embedding layer, we append eighteen (corresponding to nine relations for each observation) embeddings to the sequence before passing through the layers of the Transformer architecture. This allows the model to learn each token\u2019s representation while attending to the COMeT embeddings \u2013 effectively integrating background commonsense knowledge into a language model.10\nDiscussion Table 2 reports results on the \u03b1NLG task. Among automatic metrics, we report BLEU4 (Papineni et al., 2002), METEOR (Banerjee & Lavie, 2005), ROUGE (Lin, 2004), CIDEr (Vedantam et al., 2015) and BERT-Score (Zhang et al., 2019) (with the bert-base-uncased model). We establish human performance through crowdsourcing on AMT. Crowdworkers are shown pairs of observations and a generated hypothesis and asked to label whether the hypothesis explains the given observations. The last column reports the human evaluation score. The last row reports the score of a held-out human-written hypothesis and serves as a ceiling for model performance. Human-written hypotheses are found to be correct for 96% of instances, while our best generative models, even when enhanced with background commonsense knowledge, only achieve 45% \u2013 indicating that the \u03b1NLG generation task is especially challenging for current state-of-the-art text generators.\n9Please see Appendix A.6 for a full list of the nine relations. 10We describe the format of input for each model in Appendix A.7.\n6 ANALYSIS\n6.1 \u03b1NLI\nCategory Human Accuracy BERT Accuracy \u2206\nAll (1, 000) 91.4 68.8 22.6 Numerical (44) 88.6 56.8 21.8 Spatial (130) 91.5 65.4 26.1 Emotional (84) 86.9 72.6 14.3\nTable 3: BERT\u2019s performance and human evaluation on categories for 1,000 instances from the test set, based on commonsense reasoning domains (Numerical, Spatial, Emotional). The number in parenthesis indicates the size of the category.\nCommonsense reasoning categories We investigate the categories of commonsense-based abductive reasoning that are challenging for current systems and the ones where the best model over-performs. While there have been previous attempts to categorize commonsense knowledge required for entailment (LoBue & Yates, 2011; Clark et al., 2007), crowdsourcing this task at scale with high fidelity and high agreement across annotators remains challenging. Instead, we aim to probe the model with soft categories identified by matching lists of category-specific keywords to the hypothesis choices.\nTable 3 shows the accuracy of the best model (BERT-ft) across various categories of commonsense knowledge. BERT-ft significantly underperforms on instances involving Numerical (56.8%) and Spatial (65.4%) commonsense. These two categories include reasoning about numerical quantities and the spatial location of agents and objects, and highlight some of the limitations of the language models. In contrast, it significantly overperforms on the Emotional category (72.6%) where the hypotheses exhibit strong textual cues about emotions and sentiments.\nImplausible transitions A model for an instance of the ART dataset should discard implausible hypotheses in the context of the two given observations. In narrative contexts, there are three main reasons for an implausible hypothesis to be labeled as such:\n1. O1 6\u2192h\u2212: h\u2212 is unlikely to follow after the first observation O1. 2. h\u2212 6\u2192O2: h\u2212 is plausible after O1 but unlikely to precede the second observation O2. 3. Plausible: \u3008O1, h\u2212, O2\u3009 is a coherent narrative and forms a plausible alternative, but it is less plausible than \u3008O1, h+, O2\u3009.\nWe analyze the prevalence of each of these reasons in ART. We design a crowdsourcing task in which we show the implausible option along with the narrative context \u3008O1, O2\u3009 and get labels for which transition (O1 6\u2192h\u2212, h\u2212 6\u2192O2 or neither) in the narrative chain is broken. Table 4 shows the proportion of each category from a subset of 1, 000 instances from the test set. While h\u2212 6\u2192O2 accounts for almost half of the implausible transitions in ART, all three categories are substantially present in the dataset. BERT performance on each of these categories indicates that the model finds it particularly hard when the narrative created by the incorrect hypothesis is plausible, but less plausible than the correct hypothesis. On that subset of the test set, the fully connected model performs better than the linear chain model where it is important to consider both observations jointly to arrive at the more likely hypothesis.\n6.2 \u03b1NLG\nFigure 6 shows some examples of generations from the trained models compared to human-written generations. The example on the left is an example of an instance that only humans could get correct, while for the one on the right, COMeT-Emb+GPT2also generates the correct explanation for the observations.\n7 TRANSFER LEARNING FROM ART\nART contains a large number of questions for the novel abductive reasoning task. In addition to serving as a benchmark, we investigate if ART can be used as a resource to boost performance on other commonsense tasks. We apply transfer learning by first training a model on ART, and subsequently training on four target datasets \u2013 WinoGrande Sakaguchi et al. (2020), WSC Levesque et al. (2011), DPR Rahman & Ng (2012) and HellaSwag Zellers et al. (2019). We show that compared to a model that is only trained on the target dataset, a model that is sequentially trained on ART first and then on the target dataset can perform better. In particular, pre-training on ART consistently improves performance on related datasets when they have relatively few training examples.\nOn the other hand, for target datasets with large amounts of training data, pre-training on ART does not provide a significant improvement."
                },
                {
                    "heading": "8 RELATED WORK",
                    "text": ""
                },
                {
                    "heading": "Cloze-Style Task vs. Abductive Reasoning",
                    "text": "Since abduction is fundamentally concerned with plausible chains of cause-and-effect, our work draws inspiration from previous works that deal with narratives such as script learning\n(Schank & Abelson, 1975) and the narrative cloze test (Chambers & Jurafsky, 2009; Jans et al., 2012; Pichotta & Mooney, 2014; Rudinger et al., 2015). Rather than learning prototypical scripts or narrative chains, we instead reason about the most plausible events conditioned on observations. We make use of the ROCStories dataset (Mostafazadeh et al., 2016), which was specifically designed for the narrative cloze task. But, instead of reasoning about plausible event sequences, our task requires reasoning about plausible explanations for narrative omissions.\nEntailment vs. Abductive Reasoning The formulation of \u03b1NLI is closely related to entailment NLI, but there are two critical distinctions that make abductive reasoning uniquely challenging. First, abduction requires reasoning about commonsense implications of observations (e.g., if we observe that the \u201cgrass is wet\u201d, a likely hypothesis is that \u201cit rained earlier\u201d) which go beyond the linguistic notion of entailment (also noted by Josephson (2000)). Second, abduction requires non-monotonic reasoning about a set of commonsense implications collectively, to check the potential contradictions against multiple observations and to compare the level of plausibility of different hypotheses. This makes abductive reasoning distinctly challenging compared to other forms of reasoning such as induction and deduction (Shank, 1998). Perhaps more importantly, abduction is closely related to the kind of reasoning humans perform in everyday situations, where information is incomplete and definite inferences cannot be made.\nGenerative Language Modeling Recent advancements in the development of large-scale pretrained language models (Radford, 2018; Devlin et al., 2018; Radford et al., 2019) have improved the quality and coherence of generated language. Although these models have shown to generate reasonably coherent text when condition on a sequence of text, our experiments highlight the limitations of these models to 1) generate language non-monotonically and 2) adhere to commonsense knowledge. We attempt to overcome these limitations with the incorporation of a generative commonsense model during hypothesis generation.\nRelated Datasets Our new resource ART complements ongoing efforts in building resources for natural language inference (Dagan et al., 2006; MacCartney & Manning, 2009; Bowman et al., 2015; Williams et al., 2018a; Camburu et al., 2018). Existing datasets have mostly focused on textual entailment in a deductive reasoning set-up (Bowman et al., 2015; Williams et al., 2018a) and making inferences about plausible events (Maslan et al., 2015; Zhang et al., 2017). In their typical setting, these datasets require a system to deduce the logically entailed consequences of a given premise. In contrast, the nature of abduction requires the use of commonsense reasoning capabilities, with less focus on lexical entailment. While abductive reasoning has been applied to entailment datasets (Raina et al., 2005), they have been applied in a logical theorem-proving framework as an intermediate step to perform textual entailment \u2013 a fundamentally different task than \u03b1NLI."
                },
                {
                    "heading": "9 CONCLUSION",
                    "text": "We present the first study that investigates the viability of language-based abductive reasoning. We conceptualize and introduce Abductive Natural Language Inference (\u03b1NLI) \u2013 a novel task focused on abductive reasoning in narrative contexts. The task is formulated as a multiple-choice questionanswering problem. We also introduce Abductive Natural Language Generation (\u03b1NLG) \u2013 a novel task that requires machines to generate plausible hypotheses for given observations. To support these tasks, we create and introduce a new challenge dataset, ART, which consists of 20,000 commonsense narratives accompanied with over 200,000 explanatory hypotheses. In our experiments, we establish comprehensive baseline performance on this new task based on state-of-the-art NLI and language models, which leads to 68.9% accuracy with a considerable gap with human performance (91.4%). The \u03b1NLG task is significantly harder \u2013 while humans can write a valid explanation 96% of times, the best generator models can only achieve 45%. Our analysis leads to new insights into the types of reasoning that deep pre-trained language models fail to perform \u2013 despite their strong performance on the closely related but different task of entailment NLI \u2013 pointing to interesting avenues for future research. We hope that ART will serve as a challenging benchmark for future research in languagebased abductive reasoning and the \u03b1NLI and \u03b1NLG tasks will encourage representation learning that enables complex reasoning capabilities in AI systems."
                },
                {
                    "heading": "ACKNOWLEDGMENTS",
                    "text": "We thank the anonymous reviewers for their insightful feedback. This research was supported in part by NSF (IIS-1524371), the National Science Foundation Graduate Research Fellowship under Grant No. DGE 1256082, DARPA CwC through ARO (W911NF15-1- 0543), DARPA MCS program through NIWC Pacific (N66001-19-2-4031), and the Allen Institute for AI. Computations on beaker.org were supported in part by credits from Google Cloud."
                },
                {
                    "heading": "A APPENDICES",
                    "text": ""
                },
                {
                    "heading": "A.1 DATA COLLECTION DETAILS",
                    "text": "We describe the crowdsourcing details of our data collection method.\nTask 1 - Plausible Hypothesis Options In this task, participants were presented an incomplete three-part story, which consisted of the first observation (O1) and the second observation (O2) of the story. They were then asked to complete the story by writing a probable middle sentence that explains why the second observation should follow after the first one. We instructed participants to make sure that the plausible middle sentence (1) is short (fewer than 10 words) and (2) simple as if narrating to a child, (3) avoids introducing any extraneous information, and (4) uses names instead of pronouns (e.g., he/she) wherever possible.\nAll participants were required to meet the following qualification requirements: (1) their location is in the US, (2) HIT approval rate is greater than 95(%), and (3) Number of HITs approved is greater than 5,000. The reward of this task was set to be $0.07 per question ($14/hour in average), and each HIT was assigned to five different workers (i.e., 5-way redundancy).\nTask 2 - Implausible Hypothesis Options In this task, participants were presented a three-part story, which consisted of the first observation (O1), a middle sentence (h+) collected in Task 1, and the second observation (O2) of the story. They were then asked to rewrite the middle sentence (h+) with minimal changes, so that the story becomes unlikely, implausible or inconsistent (h\u2212). We asked participants to add or remove at most four words to h+, while ensuring that the new middle sentence is grammatical. In addition, we asked them to stick to the context in the given story. For example, if the story talks about \u201cdoctors\u201d, they are welcome to talk about \u201chealth\u201d or \u201cdiagnosis\u201d, but not mention \u201caliens\u201d. Finally, we also asked workers to verify if the given middle (h+) makes a plausible story, in order to confirm the plausibility of h+collected in Task 1.\nWith respect to this task\u2019s qualification, participants were required to fulfill the following requirements: (1) their location is the US or Canada, (2) HIT approval rate is greater than or equal to 99(%), and (3) number of HITs approved is greater than or equal to 10, 000. Participants were paid $0.1 per question ($14/hour in average), and each HIT was assigned to three different participants (i.e., 3-way redundancy).\nTask 3 - \u03b1NLI Human Performance Human performance was evaluated by asking participants to answer the \u03b1NLI questions. Given a narrative context \u3008O1, O2\u3009 and two hypotheses, they were asked to choose the more plausible hypothesis. They were also allowed to choose \u201cNone of the above\u201c when neither hypothesis was deemed plausible.\nWe asked each question to seven participants with the following qualification requirements: (1) their location is either in the US, UK, or Canada, (2) HIT approval rate is greater than 98(%), (3) Number of HITs approved is greater than 10, 000. The reward was set to $0.05 per HIT. We took the majority vote among the seven participants for every question to compute human performance.\nA.2 ART DATA STATISTICS\nTable 6 shows some statistics of the ART dataset."
                },
                {
                    "heading": "A.3 FINE-TUNING BERT",
                    "text": "We fine-tuned the BERT model using a grid search with the following set of hyper-parameters:\n\u2022 batch size: {3, 4, 8} \u2022 number of epochs: {3, 4, 10} \u2022 learning rate: {1e-5, 2e-5, 3e-5, 5e-5}\nThe warmup proportion was set to 0.2, and cross-entropy was used for computing the loss. The best performance was obtained with a batch size of 4, learning rate of 5e-5, and number of epochs equal to 10. Table 7 describes the input format for GPT and BERT (and its variants)."
                },
                {
                    "heading": "A.4 BASELINES",
                    "text": "The SVM classifier is trained on simple features like word length, overlap and sentiment features to select one of the two hypothesis choices. The bag-of-words baseline computes the average of GloVe (Pennington et al., 2014) embeddings for words in each sentence to form sentence embeddings. The sentence embeddings in a story (two observations and a hypothesis option) are concatenated and passed through fully-connected layers to produce a score for each hypothesis. The accuracies of both baselines are close to 50% (SVM: 50.6; BOW: 50.5).\nSpecifically, we train an SVM classifier and a bag-of-words model using GLoVE embeddings. Both models achieve accuracies close to 50%. An Infersent (Conneau et al., 2017) baseline that uses sentences embedded by max-pooling over Bi-LSTM token representations achieves only 50.8% accuracy."
                },
                {
                    "heading": "A.5 ADVERSARIAL FILTERING OF HYPOTHESES CHOICES",
                    "text": "Given an observation pair and sets of plausible and implausible hypotheses \u3008O1, O2,H+,H\u2212\u3009, our adversarial filtering algorithm selects one plausible and one implausible hypothesis \u3008O1, O2, h+, h\u2212 \u3009 such that h+ and h\u2212 are hard to distinguish between. We make three key improvements over the previously proposed Adversarial Filtering (AF) approach in Zellers et al. (2018). First, Instead of a single positive sample, we exploit a poolH+ of positive samples to choose from (i.e. plausible hypotheses). Second, Instead of machine generated distractors, the pool H\u2212 of negative samples (i.e. implausible hypotheses) is human-generated. Thus, the distractors share stylistic features of the positive samples as well as that of the context (i.e. observations O1 and O2) \u2013 making the negative samples harder to distinguish from positive samples. Finally, We use BERT (Devlin et al., 2018) as\nthe adversary and introduce a temperature parameter that controls the maximum number of instances that can be modified in each iteration of AF. In later iterations, fewer instances get modified resulting in a smoother convergence of the AF algorithm (described in more detail below).\nAlgorithm 1 provides a formal description of our approach. In each iteration i, we train an adversarial model Mi on a random subset Ti of the data and update the validation set Vi to make it more challenging for Mi. For a pair (h+k , h \u2212 k ) of plausible and implausible hypotheses for an instance k, we denote \u03b4 = \u2206Mi(h + k , h \u2212 k ) the difference in the model evaluation of h + k and h \u2212 k . A positive value of \u03b4 indicates that the model Mi favors the plausible hypothesis h+k over the implausible one h \u2212 k . With probability ti, we update instance k that Mi gets correct with a pair (h+, h\u2212) \u2208 H+k \u00d7 H \u2212 k of hypotheses that reduces the value of \u03b4, where H+k (resp. H \u2212 k ) is the pool of plausible (resp. implausible) hypotheses for instance k .\nWe ran AF for 50 iterations and the temperature ti follows a sigmoid function, parameterized by the iteration number, between ts = 1.0 and te = 0.2. Our final dataset, ART, is generated using BERT as the adversary in Algorithm 1.\nAlgorithm 1: Dual Adversarial Filtering input : dataset D0, plausible & implausible hypothesis sets (H+,H\u2212), number of iterations n, initial & final temperatures (ts, te) output: dataset Dn\n1 for iteration i : 0..n\u2212 1 do 2 ti = te +\nts\u2212te 1+e0.3(i\u2212 3n 4 )\n3 Randomly partition Di into (Ti,Vi). 4 Train model Mi on Ti. 5 Si = \u2205, the selected hypotheses for Vi. 6 for (h+k , h \u2212 k ) \u2208 Vi do 7 Pick r uniformly at random in [0, 1]. 8 if r > ti or \u2206Mi(h + k , h \u2212 k ) < 0 then 9 Add (h+k , h \u2212 k ) to Si.\n10 else 11 Pick (h+, h\u2212) \u2208 H+k \u00d7H \u2212 k s.t. \u2206Mi(h +, h\u2212) < \u2206Mi(h + k , h \u2212 k ) 12 Add (h+, h\u2212) to Si. 13 end 14 end 15 Di+1 = Ti \u222a Si 16 end"
                },
                {
                    "heading": "A.6 ATOMIC RELATIONS",
                    "text": "ATOMIC (Sap et al., 2019) represents commonsense knowledge as a graph with events are nodes and the following nine relations as edges:\n1. xIntent: Why does X cause an event?\n2. xNeed: What does X need to do before the event?\n3. xAttr: How would X be described?\n4. xEffect: What effects does the event have on X?\n5. xWant: What would X likely want to do after the event?\n6. xReaction: How does X feel after the event?\n7. oReact: How do others\u2019 feel after the event?\n8. oWant: What would others likely want to do after the event?\n9. oEffect: What effects does the event have on others?"
                },
                {
                    "heading": "A.7 GENERATION MODELS INPUT FORMAT",
                    "text": "Table 8 describes the format of input to each variation of the generative model evaluated."
                }
            ],
            "year": 2020,
            "references": [
                {
                    "title": "Abductive and deductive change",
                    "authors": [
                        "Henning Andersen"
                    ],
                    "venue": "Language, pp. 765\u2013793,",
                    "year": 1973
                },
                {
                    "title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization",
                    "authors": [
                        "Satanjeev Banerjee",
                        "Alon Lavie"
                    ],
                    "year": 2005
                },
                {
                    "title": "Comet: Commonsense transformers for automatic knowledge graph construction",
                    "authors": [
                        "Antoine Bosselut",
                        "Hannah Rashkin",
                        "Maarten Sap",
                        "Chaitanya Malaviya",
                        "Asli Celikyilmaz",
                        "Yejin Choi"
                    ],
                    "year": 1906
                },
                {
                    "title": "A large annotated corpus for learning natural language inference",
                    "authors": [
                        "Samuel R. Bowman",
                        "Gabor Angeli",
                        "Christopher Potts",
                        "Christopher D. Manning"
                    ],
                    "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics,",
                    "year": 2015
                },
                {
                    "title": "e-snli: Natural language inference with natural language explanations",
                    "authors": [
                        "Oana-Maria Camburu",
                        "Tim Rockt\u00e4schel",
                        "Thomas Lukasiewicz",
                        "Phil Blunsom"
                    ],
                    "venue": "In Advances in Neural Information Processing Systems,",
                    "year": 2018
                },
                {
                    "title": "Unsupervised learning of narrative schemas and their participants",
                    "authors": [
                        "Nathanael Chambers",
                        "Dan Jurafsky"
                    ],
                    "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,",
                    "year": 2009
                },
                {
                    "title": "Probabilistic semantics for cost based abduction",
                    "authors": [
                        "Eugene Charniak",
                        "Solomon Eyal Shimony"
                    ],
                    "venue": "Brown University, Department of Computer Science,",
                    "year": 1990
                },
                {
                    "title": "Enhanced lstm for natural language inference",
                    "authors": [
                        "Qian Chen",
                        "Xiao-Dan Zhu",
                        "Zhen-Hua Ling",
                        "Si Wei",
                        "Hui Jiang",
                        "Diana Inkpen"
                    ],
                    "venue": "In ACL,",
                    "year": 2017
                },
                {
                    "title": "On the role of lexical and world knowledge in rte3",
                    "authors": [
                        "Peter E. Clark",
                        "Philip Harrison",
                        "John A. Thompson",
                        "William R. Murray",
                        "Jerry R. Hobbs",
                        "Christiane Fellbaum"
                    ],
                    "venue": "In ACL-PASCAL@ACL,",
                    "year": 2007
                },
                {
                    "title": "Supervised learning of universal sentence representations from natural language inference data",
                    "authors": [
                        "Alexis Conneau",
                        "Douwe Kiela",
                        "Holger Schwenk",
                        "Lo\u0131\u0308c Barrault",
                        "Antoine Bordes"
                    ],
                    "venue": "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,",
                    "year": 2017
                },
                {
                    "title": "The pascal recognising textual entailment challenge. In Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment, pp. 177\u2013190",
                    "authors": [
                        "Ido Dagan",
                        "Oren Glickman",
                        "Bernardo Magnini"
                    ],
                    "venue": "URL http: //u.cs.biu.ac.il/",
                    "year": 2006
                },
                {
                    "title": "Annotation artifacts in natural language inference data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
                    "authors": [
                        "Suchin Gururangan",
                        "Swabha Swayamdipta",
                        "Omer Levy",
                        "Roy Schwartz",
                        "Samuel Bowman",
                        "Noah A. Smith"
                    ],
                    "year": 2018
                },
                {
                    "title": "Interpretation as abduction",
                    "authors": [
                        "Jerry R. Hobbs",
                        "Mark Stickel",
                        "Paul Martin",
                        "Douglas Edwards"
                    ],
                    "venue": "In Proceedings of the 26th Annual Meeting of the Association for Computational Linguistics,",
                    "year": 1988
                },
                {
                    "title": "Abductive inference: Computation",
                    "authors": [
                        "Susan G. Josephson"
                    ],
                    "year": 2000
                },
                {
                    "title": "The winograd schema challenge",
                    "authors": [
                        "Hector J. Levesque",
                        "Ernest Davis",
                        "Leora Morgenstern"
                    ],
                    "venue": "In KR,",
                    "year": 2011
                },
                {
                    "title": "Rouge: A package for automatic evaluation of summaries",
                    "authors": [
                        "Chin-Yew Lin"
                    ],
                    "venue": "Text Summarization Branches Out,",
                    "year": 2004
                },
                {
                    "title": "Types of common-sense knowledge needed for recognizing textual entailment",
                    "authors": [
                        "Peter LoBue",
                        "Alexander Yates"
                    ],
                    "venue": "In ACL,",
                    "year": 2011
                },
                {
                    "title": "Natural logic for textual inference",
                    "authors": [
                        "Bill MacCartney",
                        "Christopher D. Manning"
                    ],
                    "venue": "In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing,",
                    "year": 2007
                },
                {
                    "title": "An extended model of natural logic",
                    "authors": [
                        "Bill MacCartney",
                        "Christopher D. Manning"
                    ],
                    "venue": "In Proceedings of the Eight International Conference on Computational Semantics,",
                    "year": 2009
                },
                {
                    "title": "One hundred challenge problems for logical formalizations of commonsense psychology",
                    "authors": [
                        "Nicole Maslan",
                        "Melissa Roemmele",
                        "Andrew S. Gordon"
                    ],
                    "venue": "In AAAI Spring Symposia,",
                    "year": 2015
                },
                {
                    "title": "Inference in text understanding",
                    "authors": [
                        "Peter Norvig"
                    ],
                    "venue": "In AAAI, pp. 561\u2013565,",
                    "year": 1987
                },
                {
                    "title": "Bleu: a method for automatic evaluation of machine translation",
                    "authors": [
                        "Kishore Papineni",
                        "Salim Roukos",
                        "Todd Ward",
                        "Wei-Jing Zhu"
                    ],
                    "venue": "In ACL,",
                    "year": 2002
                },
                {
                    "title": "Reasoning with cause and effect",
                    "authors": [
                        "Judea Pearl"
                    ],
                    "venue": "AI Magazine,",
                    "year": 2002
                },
                {
                    "title": "The Book of Why: The New Science of Cause and Effect. Basic Books, Inc., New York, NY, USA, 1st edition, 2018",
                    "authors": [
                        "Judea Pearl",
                        "Dana Mackenzie"
                    ],
                    "venue": "ISBN 046509760X,",
                    "year": 2018
                },
                {
                    "title": "Collected papers of Charles Sanders Peirce, volume 5. Harvard University Press, 1965a",
                    "authors": [
                        "Charles Sanders Peirce"
                    ],
                    "venue": "URL http://www.hup.harvard.edu/catalog.php?isbn=",
                    "year": 1965
                },
                {
                    "title": "Pragmatism and pragmaticism, volume 5",
                    "authors": [
                        "Charles Sanders Peirce"
                    ],
                    "year": 1965
                },
                {
                    "title": "Glove: Global vectors for word representation",
                    "authors": [
                        "Jeffrey Pennington",
                        "Richard Socher",
                        "Christopher Manning"
                    ],
                    "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
                    "year": 2014
                },
                {
                    "title": "Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
                    "authors": [
                        "Matthew Peters",
                        "Mark Neumann",
                        "Mohit Iyyer",
                        "Matt Gardner",
                        "Christopher Clark",
                        "Kenton Lee",
                        "Luke Zettlemoyer"
                    ],
                    "year": 2018
                },
                {
                    "title": "Statistical script learning with multi-argument events",
                    "authors": [
                        "Karl Pichotta",
                        "Raymond Mooney"
                    ],
                    "venue": "In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,",
                    "year": 2014
                },
                {
                    "title": "Hypothesis only baselines in natural language inference",
                    "authors": [
                        "Adam Poliak",
                        "Jason Naradowsky",
                        "Aparajita Haldar",
                        "Rachel Rudinger",
                        "Benjamin Van Durme"
                    ],
                    "venue": "In Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics,",
                    "year": 2018
                },
                {
                    "title": "Improving language understanding by generative pre-training",
                    "authors": [
                        "Alec Radford"
                    ],
                    "year": 2018
                },
                {
                    "title": "Language models are unsupervised multitask learners",
                    "authors": [
                        "Alec Radford",
                        "Jeffrey Wu",
                        "Rewon Child",
                        "David Luan",
                        "Dario Amodei",
                        "Ilya Sutskever"
                    ],
                    "venue": "OpenAI Blog,",
                    "year": 2019
                },
                {
                    "title": "Resolving complex cases of definite pronouns: The winograd schema challenge",
                    "authors": [
                        "Altaf Rahman",
                        "Vincent Ng"
                    ],
                    "venue": "In EMNLP-CoNLL,",
                    "year": 2012
                },
                {
                    "title": "Robust textual inference via learning and abductive reasoning",
                    "authors": [
                        "Rajat Raina",
                        "Andrew Y Ng",
                        "Christopher D Manning"
                    ],
                    "venue": "In AAAI, pp. 1099\u20131105,",
                    "year": 2005
                },
                {
                    "title": "Winogrande: An adversarial winograd schema challenge at scale",
                    "authors": [
                        "Keisuke Sakaguchi",
                        "Ronan Le Bras",
                        "Chandra Bhagavatula",
                        "Yejin Choi"
                    ],
                    "venue": "In AAAI,",
                    "year": 2020
                },
                {
                    "title": "Atomic: an atlas of machine commonsense for if-then reasoning",
                    "authors": [
                        "Maarten Sap",
                        "Ronan Le Bras",
                        "Emily Allaway",
                        "Chandra Bhagavatula",
                        "Nicholas Lourie",
                        "Hannah Rashkin",
                        "Brendan Roof",
                        "Noah A Smith",
                        "Yejin Choi"
                    ],
                    "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
                    "year": 2019
                },
                {
                    "title": "Scripts, plans, and knowledge",
                    "authors": [
                        "Roger C. Schank",
                        "Robert P. Abelson"
                    ],
                    "venue": "In Proceedings of the 4th International Joint Conference on Artificial Intelligence - Volume 1,",
                    "year": 1975
                },
                {
                    "title": "The extraordinary ordinary powers of abductive reasoning",
                    "authors": [
                        "Gary Shank"
                    ],
                    "venue": "Theory & Psychology,",
                    "year": 1998
                },
                {
                    "title": "Performance impact caused by hidden bias of training data for recognizing textual entailment",
                    "authors": [
                        "Masatoshi Tsuchiya"
                    ],
                    "venue": "CoRR, abs/1804.08117,",
                    "year": 2018
                },
                {
                    "title": "Cider: Consensus-based image description evaluation",
                    "authors": [
                        "Ramakrishna Vedantam",
                        "C Lawrence Zitnick",
                        "Devi Parikh"
                    ],
                    "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
                    "year": 2015
                },
                {
                    "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
                    "authors": [
                        "Alex Wang",
                        "Amanpreet Singh",
                        "Julian Michael",
                        "Felix Hill",
                        "Omer Levy",
                        "Samuel Bowman"
                    ],
                    "venue": "In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,",
                    "year": 2018
                },
                {
                    "title": "A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 1112\u20131122",
                    "authors": [
                        "Adina Williams",
                        "Nikita Nangia",
                        "Samuel Bowman"
                    ],
                    "year": 2018
                },
                {
                    "title": "A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
                    "authors": [
                        "Adina Williams",
                        "Nikita Nangia",
                        "Samuel Bowman"
                    ],
                    "year": 2018
                },
                {
                    "title": "Swag: A large-scale adversarial dataset for grounded commonsense inference",
                    "authors": [
                        "Rowan Zellers",
                        "Yonatan Bisk",
                        "Roy Schwartz",
                        "Yejin Choi"
                    ],
                    "venue": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
                    "year": 2018
                },
                {
                    "title": "Hellaswag: Can a machine really finish your sentence",
                    "authors": [
                        "Rowan Zellers",
                        "Ari Holtzman",
                        "Yonatan Bisk",
                        "Ali Farhadi",
                        "Yejin Choi"
                    ],
                    "year": 2019
                },
                {
                    "title": "Ordinal common-sense inference",
                    "authors": [
                        "Sheng Zhang",
                        "Rachel Rudinger",
                        "Kevin Duh",
                        "Benjamin Van Durme"
                    ],
                    "venue": "Transactions of the Association for Computational Linguistics,",
                    "year": 2017
                },
                {
                    "title": "Bertscore: Evaluating text generation with bert",
                    "authors": [
                        "Tianyi Zhang",
                        "Varsha Kishore",
                        "Felix Wu",
                        "Kilian Q Weinberger",
                        "Yoav Artzi"
                    ],
                    "year": 1904
                },
                {
                    "title": "2019) represents commonsense knowledge as a graph with events are nodes and the following nine relations",
                    "authors": [
                        "ATOMIC (Sap"
                    ],
                    "year": 2019
                }
            ],
            "id": "SP:766d1f94f9fe05c786066ff62cb7b34d3742cec9",
            "authors": [
                {
                    "name": "Chandra Bhagavatula",
                    "affiliations": []
                },
                {
                    "name": "Ronan Le Bras",
                    "affiliations": []
                },
                {
                    "name": "Chaitanya Malaviya",
                    "affiliations": []
                },
                {
                    "name": "Keisuke Sakaguchi",
                    "affiliations": []
                },
                {
                    "name": "Ari Holtzman",
                    "affiliations": []
                },
                {
                    "name": "Hannah Rashkin",
                    "affiliations": []
                },
                {
                    "name": "Doug Downey",
                    "affiliations": []
                },
                {
                    "name": "Scott Wen-tau Yih",
                    "affiliations": []
                },
                {
                    "name": "Yejin Choi",
                    "affiliations": []
                }
            ],
            "abstractText": "Abductive reasoning is inference to the most plausible explanation. For example, if Jenny finds her house in a mess when she returns from work, and remembers that she left a window open, she can hypothesize that a thief broke into her house and caused the mess, as the most plausible explanation. While abduction has long been considered to be at the core of how people interpret and read between the lines in natural language (Hobbs et al., 1988), there has been relatively little research in support of abductive natural language inference and generation. We present the first study that investigates the viability of language-based abductive reasoning. We introduce a challenge dataset, ART, that consists of over 20k commonsense narrative contexts and 200k explanations. Based on this dataset, we conceptualize two new tasks \u2013 (i) Abductive NLI: a multiple-choice question answering task for choosing the more likely explanation, and (ii) Abductive NLG: a conditional generation task for explaining given observations in natural language. On Abductive NLI, the best model achieves 68.9% accuracy, well below human performance of 91.4%. On Abductive NLG, the current best language generators struggle even more, as they lack reasoning capabilities that are trivial for humans. Our analysis leads to new insights into the types of reasoning that deep pre-trained language models fail to perform\u2014despite their strong performance on the related but more narrowly defined task of entailment NLI\u2014pointing to interesting avenues for future research.",
            "title": "ABDUCTIVE COMMONSENSE REASONING"
        },
        "Y": {
            "blog_id": "abductive-commonsense-reasoning",
            "summary": [
                "The paper presents the task of abductive NLP (pronounced as alpha NLP) where the model needs to perform abductive reasoning.",
                "Abductive reasoning is the inference to the most plausible explanation.",
                "Even though it is considered to be an important component for understanding narratives, the work in this domain is sparse.",
                "A new dataset called as Abstractive Reasoning in narrative Text (ART) consisting of 20K narrative contexts and 200k explanations is also provided.",
                "The dataset models the task as multiple-choice questions to make the evaluation process easy.",
                "Task Setup  Given a pair of observations O1 and O2 and two hypothesis h1 and h2, the task is to select the most plausible hypothesis.",
                "In general, P(h | O1, O2) is propotional to P(h |O1)P(O2|h, O1).",
                "Different independence assumptions can be imposed on the structure of the problem eg one assumption could be that the hypothesis is independent of the observations or the \u201cfully connected\u201d assumption would jointly model both the observations and the hypothesis.",
                "Dataset  Along with crowdsourcing several plausible hypotheses for each observation instance pair, an adversarial filtering algorithm (AF) is used to remove weak pairs of hypothesis.",
                "Observation pairs are created using the ROCStories dataset which is a collection of short, manually crafted stories of 5 sentences.",
                "The average word length for both the content and the hypothesis is between 8 to 9.",
                "To collect plausible hypothesis, the crowd workers were asked to fill in a plausible \u201cin-between\u201d sentence in natural language.",
                "Given the plausible hypothesis, the crowd workers were asked to create an implausible hypothesis by editing fewer than 6 words.",
                "Adversarial filtering approach from Zellers et al. is used with BERT as the adversary.",
                "A temperature parameter is introduced to control the maximum number of instances that can be changed in each adversarial filtering iteration.",
                "Key Observations  Human performance: 91.4%  Baselines like SVM classifier, the bag-of-words classifier (using Glove) and max-pooling overt BiLSTM representation: approx 50%  Entailment NLI baseline: 59%.",
                "This highlights the additional complexity of abductive NLI as compared to entailment NLI.",
                "BERT: 68.9%  GPT: 63.1%  Numerical and spatial knowledge-based data points are particularly hard.",
                "The model is more likely to fail when the narrative created by the incorrect hypothesis is plausible"
            ],
            "author_id": "shugan",
            "pdf_url": "https://arxiv.org/pdf/1908.05739",
            "author_full_name": "Shagun Sodhani",
            "source_website": "https://github.com/shagunsodhani/papers-I-read",
            "id": 67930407
        }
    },
    "63531440": {
        "X": {
            "sections": [
                {
                    "heading": "1 INTRODUCTION",
                    "text": "The Maximum Inner Product Search (MIPS) problem has recently received increased attention, as it arises naturally in many large scale tasks. In recommendation systems (Koenigstein et al., 2012; Bachrach et al., 2014), users and items to be recommended are represented as vectors that are learnt at training time based on the user-item rating matrix. At test time, when the model is deployed for suggesting recommendations, given a user vector, the model will perform a dot product of the user vector with all the item vectors and pick top K items with maximum dot product to recommend. With millions of candidate items to recommend, it is usually not possible to do a full linear search within the available time frame of only few milliseconds. This problem amounts to solving a KMIPS problem.\nAnother common instance where the K-MIPS problem arises is in extreme classification tasks (Vijayanarasimhan et al., 2014), with a huge number of classes. At inference time, predicting the top-K most likely class labels for a given data point can be cast as a K-MIPS problem. Such extreme (probabilistic) classification problems occur often in Natural Language Processing (NLP) tasks where the classes are words in a predetermined vocabulary. For example in neural probabilistic language models (Bengio et al., 2003) the probabilities of a next word given the context of the few previous words is computed, in the last layer of the network, as a multiplication of the last hidden layer representation with a very large matrix (an embedding dictionary) that has as many columns as there are words in the vocabulary. Each such column can be seen as corresponding to the embedding of a vocabulary word in the hidden layer space. Thus an inner product is taken between each of these and the hidden representation, to yield an inner product \u201cscore\u201d for each vocabulary word. Passed through a softmax nonlinearity, these yield the predicted probabilities for all possible words. The ranking of these probability values is unaffected by the softmax layer, so finding the k most probable words is\n\u2217Equal contribution \u2020and CIFAR\nar X\niv :1\n50 7.\n05 91\n0v 3\n[ cs\n.L G\n] 3\n0 N\nov 2\nexactly equivalent to finding the ones with the largest inner product scores, i.e. solving a K-MIPS problem.\nIn many cases the retrieved result need not be exact: it may be sufficient to obtain a subset of k vectors whose inner product with the query is very high, and thus highly likely (though not guaranteed) to contain some of the exact K-MIPS vectors. These examples motivate research on approximate K-MIPS algorithms. If we can obtain large speedups over a full linear search without sacrificing too much on precision, it will have a direct impact on such large-scale applications.\nFormally the K-MIPS problem is stated as follows: given a set X = {x1, . . . , xn} of points and a query vector q, find\nargmax (K) i\u2208X q >xi (1)\nwhere the argmax(K) notation corresponds to the set of the indices providing the K maximum values. Such a problem can be solved exactly in linear time by calculating all the q>xi and selecting the K maximum items, but such a method is too costly to be used on large applications where we typically have hundreds of thousands of entries in the set.\nAll the methods discussed in this article are based on the notion of a candidate set, i.e. a subset of the dataset that they return, and on which we will do an exact K-MIPS, making its computation much faster. There is no guarantee that the candidate set contains the target elements, therefore these methods solve approximate K-MIPS. Better algorithms will provide us with candidate sets that are both smaller and have larger intersections with the actual K maximum inner product vectors.\nMIPS is related to nearest neighbor search (NNS), and to maximum similarity search. But it is considered a harder problem because the inner product neither satisfies the triangular inequality as distances usually do, nor does it satisfy a basic property of similarity functions, namely that the similarity of an entry with itself is at least as large as its similarity with anything else: for a vector x, there is no guarantee that xTx \u2265 xT y for all y. Thus we cannot directly apply efficient nearest neighbor search or maximum similarity search algorithms to the MIPS problem.\nGiven a set X = {x1, . . . , xn} of points and a query vector q, the K-NNS problem with Euclidean distance is defined as:\nargmin (K) i\u2208X ||q \u2212 xi|| 2 2 = argmax (K) i\u2208X q Txi \u2212 ||xi||22\n2 (2)\nand the maximum cosine similarity problem (K-MCSS) is defined as:\nargmax (K) i\u2208X qTxi ||q|| ||xi|| = argmax (K) i\u2208X qTxi ||xi||\n(3)\nK-NNS and K-MCSS are different problems than K-MIPS, but it is easy to see that all three become equivalent provided all data vectors xi have the same Euclidean norm. Several approaches to MIPS make use of this observation and first transform a MIPS problem into a NNS or MCSS problem.\nIn this paper, we propose and empirically investigate a very simple approach for the approximate K-MIPS problem. It consists in first reducing the problem to an approximate K-MCSS problem (as has been previously done in (Shrivastava and Li, 2015) ) on top of which we perform a spherical k-means clustering. The few clusters whose centers best match the query yield the candidate set.\nThe rest of the paper is organized as follows: In section 2, we review previously proposed approaches for MIPS. Section 3 describes our proposed simple solution k-means MIPS in more details and section 4 discusses ways to further improve the performance by using a hierarchical k-means version. In section 5, we empirically compare our methods to the state-of-the-art in tree-based and hashing-based approaches, on two standard collaborative filtering benchmarks and on a larger word embedding datasets. Section 6 concludes the paper with discussion on future work."
                },
                {
                    "heading": "2 RELATED WORK",
                    "text": "There are two common types of solution for MIPS in the literature: tree-based methods and hashingbased methods. Tree-based methods are data dependent (i.e. first trained to adapt to the specific data set) while hash-based methods are mostly data independent.\nTree-based approaches: The Maximum Inner Product Search problem was first formalized in (Ram and Gray, 2012). Ram and Gray (2012) provided a tree-based solution for the problem. Specifically, they constructed a ball tree with vectors in the database and bounded the maximum inner product with a ball. Their novel analytical upper bound for maximum inner product of a given point with points in a ball made it possible to design a branch and bound algorithm to solve MIPS using the constructed ball tree. Ram and Gray (2012) also proposes a dual-tree based search using cone trees when you have a batch of queries. One issue with this ball-tree based approach (IP-Tree) is that it partitions the set of data points based on the Euclidean distance, while the problem hasn\u2019t effectively been converted to NNS. In contrast, PCA-Tree (Bachrach et al., 2014), the current state-of-the-art tree-based approach to MIPS, first converts MIPS to NNS by appending an additional component to the vector that ensures that all vectors are of constant norm. This is followed by PCA and by a balanced kd-tree style tree construction.\nHashing based approaches: Shrivastava and Li (2014) is the first work to propose an explicit Asymmetric Locality Sensitive Hashing (ALSH) construction to perform MIPS. They converted MIPS to NNS and used the L2-LSH algorithm (Datar et al., 2004). Subsequently, Shrivastava and Li (2015) proposed another construction to convert MIPS to MCSS and used the Signed Random Projection (SRP) hashing method. Both works were based on the assumption that a symmetricLSH family does not exist for MIPS problem. Later, Neyshabur and Srebro (2015) showed an explicit construction of a symmetric-LSH algorithm for MIPS which had better performance than the previous ALSH algorithms. Finally, Vijayanarasimhan et al. (2014) propose to use Winner-TakeAll hashing to pick top-K classes to consider during training and inference in large classification problems.\nHierarchical softmax: A notable approach to address the problem of scaling classifiers to a huge number of classes is the hierarchical softmax (Morin and Bengio, 2005). It is based on prior clustering of the words into a binary, or more generally n-ary tree that serves as a fixed structure for the learning process of the model. The complexity of training is reduced from O(n) to O(log n). Due to its clustering and tree structure, it resembles the MIPS techniques we explore in this paper. However, the approaches differ at a fundamental level. Hierarchical softmax defines the probability of a leaf node as the product of all the probabilities computed by all the intermediate softmaxes on the way to that leaf node. By contrast, an approximate MIPS search imposes no such constraining structure on the probabilistic model, and is better though as efficiently searching for top winners of what amounts to a large ordinary flat softmax.\n3 k-MEANS CLUSTERING FOR APPROXIMATE MIPS\nIn this section, we propose a simple k-means clustering based solution for approximate MIPS."
                },
                {
                    "heading": "3.1 MIPS TO MCSS",
                    "text": "We follow the previous work by Shrivastava and Li (2015) for reducing the MIPS problem to the MCSS problem by ingeniously rescaling the vectors and adding new components, making the norms of all the vectors approximately the same. Let X = {x1, . . . , xn} be our dataset. Let U < 1 and m \u2208 N\u2217 be parameters of the algorithm. The first step is to scale all the vectors in our dataset by the same factor such that maxi ||xi||2 = U . We then apply two mappings P and Q, one on the data points and another on the query vector. These two mappings simply concatenate m new components to the vectors making the norms of the data points all roughly the same. The mappings are defined as follows:\nP (x) = [x, 1/2\u2212 ||x||22, 1/2\u2212 ||x||42, . . . , 1/2\u2212 ||x||2 m\n2 ] (4) Q(x) = [x, 0, 0, . . . , 0] (5)\nAs shown in Shrivastava and Li (2015), mapping P brings all the vectors to roughly the same norm: we have ||P (xi)||22 = m/4 + ||xi||2 m+1\n2 , with the last term vanishing as m \u2192 +\u221e, since ||xi||2 \u2264 U < 1. We thus have the following approximation of MIPS by MCSS for any query vector q,\nargmax (K) i q >xi ' argmax(K)i Q(q)>P (xi)\n||Q(q)||2 \u00b7 ||P (xi)||2 (6)\n3.2 MCSS USING SPHERICAL k-MEANS\nAssuming all data points x1, . . . , xn have been transformed as xj \u2190 P (xj) so as to be scaled to a norm of approximately 1, then the spherical k-means1 algorithm (Zhong, 2005) can efficiently be used to do approximate MCSS. Algorithm 1 is a formal specification of the spherical k-means algorithm, where we denote by ci the centroid of cluster i (i \u2208 {1, . . . ,K}) and aj the index of the cluster assigned to each point xj .\nAlgorithm 1 Spherical k-means aj \u2190 rand(k) while ci or aj changed at previous step do\nci \u2190 \u2211 j|aj=i xj || \u2211\nj|aj=i xj ||\naj \u2190 argmaxi\u2208{1,...,k}x>j ci end while\nThe difference between standard k-means clustering and spherical k-means is that in the spherical variant, the data points are clustered not according to their position in the Euclidian space, but according to their direction.\nTo find the one vector that has maximum cosine similarity to query point q in a dataset clustered by this method, we first find the cluster whose centroid has the best cosine similarity with the query vector \u2013 i.e. the i such that q>ci is maximal \u2013 and consider all the points belonging to that cluster as the candidate set. We then simply take argmaxj|aj=i q\n>xj as an approximation for our maximum cosine similarity vector. This method can be extended for finding the k maximum cosine similarity vectors: we compute the cosine similarity between the query and all the vectors of the candidate set and take the k best matches.\nOne issue with constructing a candidate set from a single cluster is that the quality of the set will be poor for points close to the boundaries between clusters. To alleviate this problem, we can increase the size of candidate sets by constructing them instead from the top-p best matching clusters to construct our candidate set.\nWe note that other approximate search methods exploit similar ideas. For example, Bachrach et al. (2014) proposes a so-called neighborhood boosting method for PCA-Tree, by considering the path to each leaf as a binary vector (based on decision to go left or right) and given a target leaf, consider all other leaves which are one hamming distance away.\n4 HIERARCHICAL k-MEANS FOR FASTER AND MORE PRECISE SEARCH\nWhile using a single-level clustering of the data points might yield a sufficiently fast search procedure for moderately large databases, it can be insufficient for much larger collections. Indeed, if we have n points, by clustering our dataset into \u221a n clusters so that each cluster contains approximately \u221a n points, we reduce the complexity of the search from O(n) to roughly O ( \u221a n). If we use the single closest cluster as a candidate set, then the candidate set size is of the order of\u221a n. But as mentioned earlier, we will typically want to consider the two or three closest clusters as a candidate set, in order to limit problems arising from the query points close to the boundary between clusters or when doing approximate K-MIPS with K fairly big (for example 100). A consequence of increasing candidate sets this way is that they can quickly grow wastefully big, containing many unwanted items. To restrict the candidate sets to a smaller count of better targeted items, we would need to have smaller clusters, but then the search for the best matching clusters becomes the most expensive part. To address this situation, we propose an approach where we cluster our dataset into many small clusters, and then cluster the small clusters into bigger clusters, and so on any number of times. Our approach is thus a bottom-up clustering approach.\n1Note that we use K to refer to the number of top-K items to retrieve in search and k for the number of clusters in k-means. These two quantities are otherwise not the same.\nFor example, we can cluster our datasets in n2/3 first-level, small clusters, and then cluster the centroids of the first-level clusters into n1/3 second-level clusters, making our data structure a twolayer hierarchical clustering. This approach can be generalized to as many levels of clustering as necessary.\nTo search for the small clusters that best match the query point and will constitute a good candidate set, we go down the hierarchy keeping at each level only the p best matching clusters. This process is illustrated in Figure 1. Since at all levels the clusters are of much smaller size, we can take much larger values for p, for example p = 8 or p = 16.\nFormally, if we have L levels of clustering, let Il be a set of indices for the clusters at level l \u2208 {0, . . . , L}. Let c(l)i , i \u2208 Il be the centroids of the clusters at level l, with {c (L) i } conveniently defined as being the data points themselves, and let a(l)i \u2208 Il\u22121, i \u2208 Il be the assignment of the centroids c(l)i to the clusters of layer l\u2212 1. The candidate set is found using the method described in Algorithm 2. Our candidate set is the set CL obtained at the end of the algorithm. In our approach,\nAlgorithm 2 Search in hierarchical spherical k-means C0 = I0 for l = 0, . . . , L\u2212 1 do Al = argmax (p) i\u2208Clq >c (l) i\nCl+1 = { i|a(l+1)i \u2208 Al } end for return CL\nwe do a bottom-up clustering, i.e. we first cluster the dataset into small clusters, then we cluster the small cluster into bigger clusters, and so on until we get to the top level which is only one cluster. Other approaches have been suggested such as in (Mnih and Hinton, 2009), where the method employed is a top-down clustering strategy where at each level the points assigned to the current cluster are divided in smaller clusters. The approach of (Mnih and Hinton, 2009) also addresses the problem that using a single lowest-level cluster as a candidate set is an inaccurate solution by having the data points be in multiple clusters. We use an alternative solution that consists in exploring several branches of the clustering hierarchy in parallel."
                },
                {
                    "heading": "5 EXPERIMENTS",
                    "text": "In this section, we will evaluate the proposed algorithm for approximate MIPS. Specifically, we analyze the following characteristics: speedup, compared to the exact full linear search, of retrieving top-K items with largest inner product, and robustness of retrieved results to noise in the query."
                },
                {
                    "heading": "5.1 DATASETS",
                    "text": "We have used 2 collaborative filtering datasets and 1 word embedding dataset, which are descibed below:\nMovielens-10M: A collaborative filtering dataset with 10,677 movies (items) and 69,888 users. Given the user-item matrix Z, we follow the pureSVD procedure described in (Cremonesi et al., 2010) to generate user and movie vectors. Specifically, we subtracted the average rating of each user from his individual ratings and considered unobserved entries as zeros. Then we compute an SVD approximation of Z with its top 150 singular components, Z 'W\u03a3RT . Each row in W\u03a3 is used as the vector representation of the user and each row in R is the vector representation of the movie. We construct a database of all 10,677 movies and consider 60,000 randomly selected users as queries.\nNetflix: Another standard collaborative filtering dataset with 17,770 movies (items) and 480,189 users. We follow the same procedure as described for movielens but construct 300 dimensional vector representations, as is standard in the literature (Neyshabur and Srebro, 2015). We consider 60,000 randomly selected users as queries.\nWord2vec embeddings: We use the 300-dimensional word2vec embeddings released by Mikolov et al. (2013). We construct a database composed of the first 100,000 word embedding vectors. We consider two types of queries: 2,000 randomly selected word vectors from that database, and 2,000 randomly selected word vectors from the database corrupted with Gaussian noise. This acts as a test bench to evaluate the performance of different algorithms based on the characteristics of the queries."
                },
                {
                    "heading": "5.2 BASELINES",
                    "text": "We consider the following baselines to compare with.\nPCA-Tree: PCA-Tree (Bachrach et al., 2014) is the state-of-the-art tree-based method which was shown to be superior to IP-Tree (Koenigstein et al., 2012). This method first converts MIPS to NNS by appending an additional component to the vectors to make them of constant norm. Then the principal directions are learnt and the data is projected using these principal directions. Finally, a balanced tree is constructed using as splitting criteria at each level the median of component values along the corresponding principal direction. Each level uses a different principal direction, in decreasing order of variance.\nSRP-Hash: This is the signed random projection hashing method for MIPS proposed in Shrivastava and Li (2015). SRP-Hash converts MIPS to MCSS by vector augmentation. We consider n hash functions and each hash function considers p random projections of the vector to compute the hash.\nWTA-Hash: Winner Takes All hashing (Vijayanarasimhan et al., 2014) is another hashing-based baseline which also converts MIPS to MCSS by vector augmentation. We consider n hash functions and each hash function does p different random permutations of the vector. Then the prefix constituted by the first k elements of each permuted vector is used to construct the hash for the vector."
                },
                {
                    "heading": "5.3 SPEEDUP RESULTS",
                    "text": "In these first experiments, we consider the two collaborative filtering tasks and evaluate the speedup provided by the different approximate K-MIPS algorithms (for K \u2208 {1, 10, 100}) compared to the exact full search. Note that this section does not include the hierarchical version of k-means in the experiments, as the databases were small enough (less than 20,000) for flat k-means to perform well.\nSpecifically, speedup is defined as\nspeedupA0(A) = Time taken by Algorithm A0 Time taken by Algorithm A\n(7)\nwhere A0 is the exact linear search algorithm that consists in computing the inner product with all training items. Because we want to compare the preformance of algorithms, rather than of specifically optimized implementations, we approximate the time with the number of dot product operations computed by the algorithm2. In other words, our unit of time is the time taken by a dot product.\n2For example, k-means algorithm was run using GPU while PCA-Tree was run using CPU.\nAll algorithms return a set of candidates for which we do exact linear seacrh. This induces a number of dot products at least as large as the size of the identified candidate set. In addition to the candidate set size, the following operations count towards the count of dot products:\nk-means: dot products done with all cluster centroids involved in finding the top-p clusters of the (hierarchical) search.\nPCA-Tree: dot product done to project the query to the PCA space. Note that if the tree is of depth d, then we need to do d dot products to project the query.\nSRP-Hash: total number of random projections of the data (each random projection is considered a single dot product). If we have n hashes with p random projections each, then the cost is p \u2217 n. WTA-Hash: a full random permutation of the vector involves the same number of query element access operations as a single dot product. However, we consider only k prefixes in the permutations, which means we only need to do a fraction of dot product. While a dot product involves accessing all d components of the vector, each permutation in WTA-Hash only needs to access k elements of the vector. So we consider its cost to be a fraction k/d of the cost of a dot product. Specifically, if we have n hash functions each with p random permutations and consider prefixes of length k, then the total cost would be n \u2217 p \u2217 k/d where d is the dimension of the vector. Let us call true top-K the actual K elements from the database that have the largest inner products with the query. Let us call retrieved top-K the K elements, among the candidate set retrieved by a specific approximate MIPS, that have the largest inner products with the query. We define precision for K-MIPS as the number of elements in the intersection of true top-K and retrived top-K vectors, divided by K.\nprecision at K = |retrieved top K \u2229 true top K|\nK (8)\nWe varied hyper-parameters of each algorithm (k in k-means, depth in PCA-Tree, number of hash functions in SRP-Hash and WTA-Hash), and computed the precision and speedup in each case. Resulting precision v.s. speedup curves obtained for the Movielens-10M and Netflix datasets are reported in Figure 2. We make the following observations from these results:\n\u2022 Hashing-based methods perform better with lower speedups. But their performance decrease rapidly after 10x speedup.\n\u2022 PCA-Tree performs better than SRP-Hash. \u2022 WTA-Hash performs better than PCA-Tree with lower speedups. However, their perfor-\nmance degrades faster as the speedup increases and PCA-Tree outperforms WTA-Hash with higer speedups.\n\u2022 k-means is a clear winner as the speed up increases. Also, performance of k-means degrades very slowly with increase in speedup as compared to rapid decrease in performance of other algorithms."
                },
                {
                    "heading": "5.4 NEIGHBORHOOD PRESERVING AND ROBUSTNESS RESULTS",
                    "text": "In this experiment, we consider a word embedding retrieval task. As a first experiment, we consider using a query set of 2,000 embeddings, corresponding to a subset of a large database of pretrained embeddings. Note that while a query is thus present in the database, it is not guaranteed to correspond to the top-1 MIPS result. Also, we\u2019ll be interested in the top-10 and top-100 MIPS performance. Algorithms which perform better in top-10 and top-100 MIPS for queries which already belong to the database preserve the neighborhood of data points better. Figure 3 shows the precision vs. speedup curve for top-1, top-10 and top-100 MIPS. From the results, we can see that data dependent algorithms (k-means and PCA-Tree) better preserve the neighborhood, compared to data independent algorithms (SRP-Hash, WTA-Hash), which is not surprising. However, k-means and hierarchical k-means performs significantly better than PCA-Tree in top-10 and top-100 MIPS suggesting that it is better than PCA-Tree in capturing the neighborhood. One reason might be that k-means has the global view of the vector at every step while PCA-Tree considers one dimension at a time.\nAs the next experiment, we would like to study how different algorithms behave with respect to the noise in the query. For a fair comparison, we chose hyper-parameters for each model such that the\nspeedup is the same (we set it to 30x) for all algorithms. We take 2,000 random word embeddings from the database and corrupt them random Gaussian noise. We vary the scale of the noise from 0 to 0.4 and plot the performance. Figure 4 shows the performance of various algorithms on the top-1, top-10, top-100 MIPS problems, as the noise increases. We can see that k-means always performs better than other algorithms, even with increase in noise. Also, the performance of kmeans remains reasonable, compared to other algorithms. These results suggest that our approach might be particularly appropriate in a scenario where word embeddings are simultaneously being trained, and are thus not fixed. In such a scenario, having a robust MIPS method would allow us to update the MIPS model less frequently."
                },
                {
                    "heading": "6 CONCLUSION AND FUTURE WORK",
                    "text": "In this paper, we have proposed a new and efficient way of solving approximate K-MIPS based on a simple clustering strategy, and showed it can be a good alternative to the more popular LSH or tree-based techniques. We regard the simplicity of this approach as one of its strengths. Empirical results on three real-world datasets show that this simple approach clearly outperforms the other families of techniques. It achieves a larger speedup while maintaining precision, and is more robust to input corruption, an important property for generalization, as query test points are expected to not be exactly equal to training data points. Clustering MIPS generalizes better to related, but unseen data than the hashing approaches we evaluated.\nIn future work, we plan to research ways to adapt on-the-fly the clustering for our approximate KMIPS as its input representation evolves during the learning of a model, leverage efficient K-MIPS to speed up extreme classifier training and improve precision and speedup by combining multiple clusterings.\nFinally, we mention that, while putting the final touches to this paper, another very recent and different MIPS approach, based on vector quantization, came to our knowledge (Guo et al., 2015). We highlight that the first arXiv post of our work predates their work. Nevertheless, while we did not have time to empirically compare to this approach here, we hope to do so in future work."
                },
                {
                    "heading": "ACKNOWLEDGEMENTS",
                    "text": "The authors would like to thank the developers of Theano (Bergstra et al., 2010) for developing such a powerful tool. We acknowledge the support of the following organizations for research funding and computing support: Samsung, NSERC, Calcul Quebec, Compute Canada, the Canada Research Chairs and CIFAR."
                }
            ],
            "year": 2015,
            "references": [
                {
                    "title": "Speeding up the xbox recommender system using a euclidean transformation for inner-product spaces",
                    "authors": [
                        "Yoram Bachrach",
                        "Yehuda Finkelstein",
                        "Ran Gilad-Bachrach",
                        "Liran Katzir",
                        "Noam Koenigstein",
                        "Nir Nice",
                        "Ulrich Paquet"
                    ],
                    "venue": "In Proceedings of the 8th ACM Conference on Recommender Systems,",
                    "year": 2014
                },
                {
                    "title": "A neural probabilistic language model",
                    "authors": [
                        "Yoshua Bengio",
                        "R\u00e9jean Ducharme",
                        "Pascal Vincent",
                        "Christian Janvin"
                    ],
                    "venue": "J. Mach. Learn. Res.,",
                    "year": 2003
                },
                {
                    "title": "Theano: a CPU and GPU math expression compiler",
                    "authors": [
                        "James Bergstra",
                        "Olivier Breuleux",
                        "Fr\u00e9d\u00e9ric Bastien",
                        "Pascal Lamblin",
                        "Razvan Pascanu",
                        "Guillaume Desjardins",
                        "Joseph Turian",
                        "David Warde-Farley",
                        "Yoshua Bengio"
                    ],
                    "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy),",
                    "year": 2010
                },
                {
                    "title": "Performance of recommender algorithms on top-n recommendation tasks",
                    "authors": [
                        "Paolo Cremonesi",
                        "Yehuda Koren",
                        "Roberto Turrin"
                    ],
                    "venue": "In Proceedings of the Fourth ACM Conference on Recommender Systems,",
                    "year": 2010
                },
                {
                    "title": "Locality-sensitive hashing scheme based on p-stable distributions",
                    "authors": [
                        "Mayur Datar",
                        "Nicole Immorlica",
                        "Piotr Indyk",
                        "Vahab S. Mirrokni"
                    ],
                    "venue": "In Proceedings of the Twentieth Annual Symposium on Computational Geometry,",
                    "year": 2004
                },
                {
                    "title": "Quantization based fast inner product search",
                    "authors": [
                        "Ruiqi Guo",
                        "Sanjiv Kumar",
                        "Krzysztof Choromanski",
                        "David Simcha"
                    ],
                    "venue": "CoRR, abs/1509.01469,",
                    "year": 2015
                },
                {
                    "title": "Efficient retrieval of recommendations in a matrix factorization framework",
                    "authors": [
                        "Noam Koenigstein",
                        "Parikshit Ram",
                        "Yuval Shavitt"
                    ],
                    "venue": "In Proceedings of the 21st ACM International Conference on Information and Knowledge Management,",
                    "year": 2012
                },
                {
                    "title": "Distributed representations of words and phrases and their compositionality",
                    "authors": [
                        "Tomas Mikolov",
                        "Ilya Sutskever",
                        "Kai Chen",
                        "Greg S Corrado",
                        "Jeff Dean"
                    ],
                    "venue": "Advances in Neural Information Processing Systems",
                    "year": 2013
                },
                {
                    "title": "A scalable hierarchical distributed language model",
                    "authors": [
                        "Andriy Mnih",
                        "Geoffrey Hinton"
                    ],
                    "venue": "In Advances in Neural Information Processing Systems,",
                    "year": 2009
                },
                {
                    "title": "Hierarchical probabilistic neural network language model",
                    "authors": [
                        "Frederic Morin",
                        "Yoshua Bengio"
                    ],
                    "venue": "Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics,",
                    "year": 2005
                },
                {
                    "title": "On symmetric and asymmetric lshs for inner product search",
                    "authors": [
                        "Behnam Neyshabur",
                        "Nathan Srebro"
                    ],
                    "venue": "In Proceedings of the 31st International Conference on Machine Learning,",
                    "year": 2015
                },
                {
                    "title": "Maximum inner-product search using cone trees",
                    "authors": [
                        "Parikshit Ram",
                        "Alexander G. Gray"
                    ],
                    "venue": "In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD",
                    "year": 2012
                },
                {
                    "title": "Asymmetric LSH (ALSH) for sublinear time maximum inner product search (MIPS)",
                    "authors": [
                        "Anshumali Shrivastava",
                        "Ping Li"
                    ],
                    "venue": "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems",
                    "year": 2014
                },
                {
                    "title": "Improved asymmetric locality sensitive hashing (alsh) for maximum inner product search (mips)",
                    "authors": [
                        "Anshumali Shrivastava",
                        "Ping Li"
                    ],
                    "venue": "In Proceedings of Conference on Uncertainty in Artificial Intelligence (UAI),",
                    "year": 2015
                },
                {
                    "title": "Deep networks with large output spaces",
                    "authors": [
                        "Sudheendra Vijayanarasimhan",
                        "Jon Shlens",
                        "Rajat Monga",
                        "Jay Yagnik"
                    ],
                    "venue": "arXiv preprint arXiv:1412.7479,",
                    "year": 2014
                },
                {
                    "title": "Efficient online spherical k-means clustering",
                    "authors": [
                        "Shi Zhong"
                    ],
                    "venue": "In Neural Networks,",
                    "year": 2005
                }
            ],
            "id": "SP:a2e2970dd99b86fd6198c0b756c6cd4d52e34c3b",
            "authors": [
                {
                    "name": "Alex Auvolat",
                    "affiliations": []
                },
                {
                    "name": "Sarath Chandar",
                    "affiliations": []
                },
                {
                    "name": "Pascal Vincent",
                    "affiliations": []
                },
                {
                    "name": "Hugo Larochelle",
                    "affiliations": []
                },
                {
                    "name": "Yoshua Bengio",
                    "affiliations": []
                }
            ],
            "abstractText": "Efficient Maximum Inner Product Search (MIPS) is an important task that has a wide applicability in recommendation systems and classification with a large number of classes. Solutions based on locality-sensitive hashing (LSH) as well as tree-based solutions have been investigated in the recent literature, to perform approximate MIPS in sublinear time. In this paper, we compare these to another extremely simple approach for solving approximate MIPS, based on variants of the k-means clustering algorithm. Specifically, we propose to train a spherical kmeans, after having reduced the MIPS problem to a Maximum Cosine Similarity Search (MCSS). Experiments on two standard recommendation system benchmarks as well as on large vocabulary word embeddings, show that this simple approach yields much higher speedups, for the same retrieval precision, than current state-of-the-art hashing-based and tree-based methods. This simple method also yields more robust retrievals when the query is corrupted by noise.",
            "title": "MAXIMUM INNER PRODUCT SEARCH"
        },
        "Y": {
            "blog_id": "auvolatv15",
            "summary": [
                "`Update 2015/11/23: Since I first wrote this note, I became involved in the next iterations of this work, which became v2 of the arXiv manuscript.",
                "The notes below were made based on v1.`  This paper considers the problem of Maximum Inner Product Search (MIPS).",
                "In MIPS, given a query $q$ and a set of inputs $x_i$, we want to find the input (or the top n inputs) with highest inner product, i.e. $argmax_i q' x_i$.",
                "Recently, it was shown that a simple transformation to the query and input vectors made it possible to approximately solve MIPS using hashing methods for Maximum Cosine Similarity Search (MCSS), a problem for which solutions are readily available (see section 2.4 for a brief but very clear description of the transformation).",
                "In this paper, the authors combine this approach with clustering, in order to improve the quality of retrieved inputs.",
                "Specifically, they consider the spherical k-means algorithm, which is a variant of k-means in which data points are clustered based on cosine similarity instead of the euclidean similarity (in short, data points are first scaled to be of unit norm, then in the training inner loop points are assigned to the cluster centroid with highest dot product and cluster centroids are updated as usual, except that they are always rescaled to unit norm).",
                "Moreover, they consider a bottom-up application of the algorithm to yield a hierarchical clustering tree.",
                "They propose to use such a hierarchical clustering tree to find the top-n candidates for MIPS.",
                "The key insight here is that, since spherical k-means relies on cosine similarity for finding the best cluster, and since we have a transformation that allows the maximisation of inner product to be approximated by the maximisation of cosine similarity, then a tree to find MIPS candidates could be constructed by running spherical k-means on the inputs transformed by the same transformation used for hashing-based MIPS.",
                "In order to make the search more robust to border issues when a query is close to the frontier between clusters, at each level of the tree they consider more than one candidate cluster during top-down search, so as to merge the candidates in several leaves of the tree at the very end of a full top down query.",
                "Their experiments using search with word embeddings show that the quality of the top 1, 10 and 100 MIPS candidates using their spherical k-means approach is better than using two hashing-based search methods."
            ],
            "author_id": "hlarochelle",
            "pdf_url": "http://arxiv.org/pdf/1507.05910",
            "author_full_name": "Hugo Larochelle",
            "source_website": "https://www.shortscience.org/user?name=hlarochelle",
            "id": 63531440
        }
    },
    "99384474": {
        "X": {
            "sections": [
                {
                    "text": "performance properties such as latency and throughput. In this paper, we explore a new dimension, life cycle management complexity, which attempts to understand the complexity of deploying a topology and expanding it. By analyzing current practice in lifecycle management, we devise complexity metrics for lifecycle management, and show that existing topology classes have low lifecycle management complexity by some measures, but not by others. Motivated by this, we design a new class of topologies, FatClique, that, while being performance-equivalent to existing topologies, is comparable to, or better than them by all our lifecycle management complexity metrics."
                },
                {
                    "heading": "1 Introduction",
                    "text": "Over the past decade, there has been a long line of work on designing datacenter topologies [2, 35, 31, 32, 3, 4, 20, 1]. While most have focused on performance properties such as latency and throughput, and on resilience to link and switch failures, datacenter lifecycle management [30, 38] has largely been overlooked. Lifecycle management is the process of building a network, physically deploying it on a data-center floor, and expanding it over several years so that it is available for use by a constantly increasing set of services.\nWith datacenters living on for years, sometimes up to a decade [31, 12], their lifecycle costs can be high. A data center design that is hard to deploy can stall the rollout of services for months; this can be expensive considering the rate at which network demands have historically increased [31, 23]. A design that is hard to expand can leave the network functioning with degraded capacity impacting the large array of services that depend on it.\nIt is therefore desirable to commit to a data-center network design only after getting a sense of its lifecycle management cost and complexity over time. Unfortunately, the costs of the large array of components needed for deployment such as switches, transceivers, cables, racks, patch panels1, and cable trays, are proprietary and change over time, and so are hard to quantify. An alternative approach is to develop complexity measures (as opposed to dollar costs) for lifecycle management, but as far as we know, no prior work has addressed this. In part, this is due to the fact that intuitions about lifecycle management are developed over time and with operations experience, and these lessons are not made available universally.\n1A patch panel or a wiring aggregator is a device that simplifies cable re-wiring.\nUnfortunately, in our experience, this lack of a clear understanding of lifecycle management complexity often results in costly mistakes in the design of datacenters that are discovered during deployment and therefore cannot be rectified. Our paper is a first step towards useful characterizations of lifecycle management complexity. Contributions. To this end, our paper makes three contributions. First, we design several complexity metrics (\u00a73 and \u00a74) that can be indicative of lifecycle management costs (i.e., capital expenditure, time and manpower required). These metrics include the number of: switches, patch panels, bundle-types, expansion steps, and links to be re-wired at a patch panel rack during an expansion step.\nWe design these metrics by identifying structural elements of network deployments that make their deployment and expansion challenging. For instance, the number of switches in the topology determines how complex the network is in terms of packaging \u2013 laying out switches into homogeneous racks in a space efficient manner. Wiring complexity can be assessed by the number of cable bundles and the patch panels a design requires. As these increase, the complexity of manufacturing and packaging all the different cable bundles efficiently into cable trays, and then routing them from one patch panel to the next can be expected to increase. Finally, because expansion is carried out in steps [38], where the network operates at degraded capacity at each step, the number of expansion steps is a measure of the reduced availability in the network induced by lifecycle management. Wiring patterns also determine the number of links that need to be rewired at a patch panel during each step of expansion, a measure of step complexity [38].\nOur second contribution is to use these metrics to compare the lifecycle management costs of two main classes of datacenter topologies recently explored in the research literature (\u00a72), Clos [2] and expander graphs [32, 35]. We find that neither class dominates the other: Clos has relatively lower wiring complexity; its symmetric design leads to more uniform bundling (and fewer cable bundle types); but expander graphs at certain scales can have simpler packaging requirements due to their edge expansion property [32]; they end up using much fewer switches than Clos to achieve the same network capacity. Expander graphs also demonstrate better expansion properties because they have fat edges (\u00a74) which permit more links to be rewired in each step.\nFinally we design and synthesize a novel and practical class of topologies called FatClique (\u00a75), that has lower overall lifecycle management complexity compared to Clos and expander graphs. We do this by combining favorable design\nUSENIX Association 16th USENIX Symposium on Networked Systems Design and Implementation 235\nelements from these two topology classes. By design, FatClique incorporates 3 levels of hierarchy and uses a clique as a building block while ensuring edge expansion. At every level of its hierarchy, FatClique is designed to have fat edges, for easier expansion, while utilizing much fewer patch panels and therefore inter-rack cabling.\nEvaluations of these topology classes at three different scales, the largest of which is 16\u00d7 the size of Jupiter, shows that FatClique is the best at most scales by all our complexity metrics. It uses 50% fewer switches and 33% fewer patch panels than Clos at large scale, and has a 23% lower cabling cost (an estimate we are able to derive from published cable prices). Finally, FatClique can permit fast expansion while degrading network capacity by small amounts (2.5-10%): at these levels, Clos can take 5 \u00d7 longer to expand the topology."
                },
                {
                    "heading": "2 Background",
                    "text": "Data center topology families. Data centers are often designed for high throughput, low latency and resilience. Existing data center designs can be broadly classified into the following families: (a) Clos-like tree topologies, e.g., Google\u2019s Jupiter [31], Facebook\u2019s fbfabric [3], Microsoft\u2019s VL2 [13], F10 [22]; (b) Expander graph based topologies, e.g., Jellyfish [32], Xpander [35]; (c) \u2018Direct\u2019 topologies built from multi-port servers, e.g., BCube [14], DCell [15]. (d) Low diameter, strongly-connected topologies that rely on highradix switches, e.g., Slimfly [4], Dragonfly [20]; (e) Reconfigurable optical topologies like Rotornet and ProjectToR [24, 9, 11, 16, 39].\nOf these, Clos and Expander based topologies have been shown to scale using widely deployed merchant silicon. The ecosystem around the hardware used by these two classes, e.g., cabling, cable trays used, rack sizes, is mature and wellunderstood, allowing us to quantify some of the operational complexity of these topologies.\nDirect multi-port server topologies and some reconfigurable optical topologies [24, 11, 16, 39] rely on newer hardware technologies that are not mainstream yet. It is hard to quantify the operational costs of these classes without making significant assumptions about such hardware. Low diameter topologies like Slimfly [4] and Dragonfly [20], can be built with hardware that is available today, but they require strongly connected groups of switches. Their incremental expansion comes at high cost and complexity; high-radix switches either need to be deployed well in advance, or every switch in the topology needs to be upgraded during expansion, to preserve low diameter.\nTo avoid estimating operational complexity of topologies that rely on new hardware, or on topologies that unacceptably constrain expansion, we focus on the Clos and Expander families. Clos. A logical Clos topology with N servers can be constructed using switches with radix k connected in n = log k\n2 (N2 ) layers based on a canonical recursive algorithm\nin [36]2. Fattree [2] and Jupiter [31] are special cases of Clos topology with 3 and 5 layers respectively. Clos construction naturally allows switches to be packaged together to form a chassis [31]. Since there are no known generic Clos packaging algorithm that can help design such a chassis, for a Clos of any scale, we designed one to help our study of its operational complexity. We present this algorithm in \u00a7A.1. Expander graphs. Jellyfish and Xpander benefit from the high edge expansion property of expander graph to use a near optimal number of switches, while achieving the same bisection bandwidth as Clos based topologies [35]. Xpander splits N servers among switches by attaching s servers to each switch. With a k port switch, the remaining ports p= k\u2212 s are connected to other switches that are organized in p blocks called metanodes. Metanodes are a group of switches, containing l =N/(s \u00b7 (p+1)) switches, which increase as topology scale N increases. There are no connections between the switches of a metanode. Jellyfish is a degree bounded random graph (see [32] for more details). Takeaway. A topology with high edge expansion [35] can achieve a target capacity with fewer switches, leading to lower overall cost."
                },
                {
                    "heading": "3 Deployment Complexity",
                    "text": "Deployment is the process of realizing a physical topology in a data center space (e.g., a building), from a given logical topology. Deployment complexity can be reduced by careful packaging, placement and bundling strategies [31, 20, 1]."
                },
                {
                    "heading": "3.1 Packaging, Placement, and Bundling",
                    "text": "Packaging of a topology involves careful arrangement of switches into racks, while placement involves arranging these racks into rows on the data center floor. The spatial arrangement of the topology determines the type of cables needed between switches. For instance, if two connected switches are within the same rack, they can use short-range cheaper copper cables, while connections between racks require more expensive optical cables. Optical cable costs are determined by two factors: the cost of transceivers and the length of cables (\u00a73.2). Placement of switches on the datacenter floor can also determine costs: connecting two switches placed at two ends of the data center building might require long range cables and high-end transceivers. Chassis, racks, and blocks. Packaging connected switches into a single chassis using a backplane completely removes the need for physical connecting cables. At scale, the cost and complexity savings from using a chassis-backplane can be significant. One or more chassis that are interconnected can be packed into racks such that: (a) racks are as homogeneous as possible, i.e., a topology makes use of only a few types of racks to simplify manufacturing and (b) racks are packed as\n2This equation for n can be used to build a Clos with 1:1 oversubscription. For a Clos with an over-subscription x:y we would need n = log k\n2 ( y\u00b7N/x2 )\nlayers.\n236 16th USENIX Symposium on Networked Systems Design and Implementation USENIX Association\ndensely as possible to reduce space wastage. Some topologies define larger units of co-placement and packaging called blocks, which consist of groups of racks. Examples of blocks include pods in Fattree. External cabling from racks within a block are routed to wiring aggregators (i.e., patch panels [25]) to be routed to other blocks. For blocks to result in lower deployment complexity, three properties must be met: (a) the ports on the patch panel that it connects to are not wasted, when the topology is built out to full scale, (b) wiring out of the block should be as uniform as possible, and (c) racks in a block must be placed close to each other to reduce the length and complexity of wiring. Bundling and cable trays. When multiple fibers from the same set of physically adjoint (or neighboring) racks are destined to another set of neighboring racks, these fibers can be bundled together. A fiber bundle is a fixed number of identical-length fibers between two clusters of switches or racks. Manufacturing bundles is simpler than manufacturing individual fibers, and handling such bundles significantly simplifies operation complexity. Cable bundling reduces capex and opex by around 40% in Jupiter [31].\nPatch panels facilitate bundling since the patch panel represents a convenient aggregation point to create and route bundles from the set of fibers destined to the same patch panel (or the same set of physically proximate patch panels). Figure 1 shows a Clos topology instance (left) and its physical realization using patch panels (right). Each aggregation block in the Clos network connects with one link to each spine block. The figure on the right shows how these links are routed physically. Bundles with two fibers each from two aggregations are routed to two (lower) patch panels. At each patch panel, these fibers are rebundled, by grouping fibers that go to the same spine in new bundles, and routed to two other (upper) patch panels that connect to spines. The bundles from the upper patch panels are then routed to the spines. Figure 1 assumes that patch panels are used as follows: bundles are connected to both the front and back ports on patch panels. For example, bundles from the aggregation layer connect to front ports on patch panels and bundles from spines connect to the back ports of patch panels. This enables bundle aggregation and rebundling and simplifies topology expansion.3\nBundles and fibers are routed through the datacenter on cable trays. The cables that aggregate at a patch panel rack must be routed overhead by using over-row and cross-row trays [26]. Trays have capacity constraints [34], which can constrain rack placement, block sizes, and patch panel placement. Today, trays can support at most a few thousand fibers [34].\n3[38]\u2019s usage of patch panels is slightly different. All bundles are connected to front ports of patch panels and links are established using jumper cables between the back ports of patch panels. For patch panels of a given port count, both approaches require the same number of patch panels. Our approach enables bundling closer to the aggregation and spine layers; [38] does not describe how bundling is accomplished in their design."
                },
                {
                    "heading": "3.2 Deployment Complexity Metrics",
                    "text": "Based on the previous discussion, we identify several metrics that quantify the complexity of the two aspects of datacenter topology deployment: packaging and placement. In the next subsection, we use these metrics to identify differences between Clos and Expander graph topology classes. Number of Switches. The total number of switches in the topology determines the capital expenditure for the topology, but it also determines the packaging complexity (switches need to be packed to chassis and racks) and the placement complexity (racks need to be placed on the datacenter floor). Number of Patch panels. By acting as bundle waypoints, the number of patch panels captures one measure of wiring complexity. The more the number of patch panels, the shorter the cable lengths from switches to the nearest patch panel, but the fewer the bundling opportunities, and vice versa. The number of patch panels needed is a function of topological structure. For instance, in a Clos topology, if an aggregation layer fits into one rack or a neighboring set of racks, a patch panel is not needed between the ToR and the aggregation layer. However, for larger Clos topologies where an aggregation block can span multiple racks, ToR to aggregation links may need to be rebundled through a patch panel. We discuss this in detail in \u00a76.2. Number of Bundle Types. The number of patch panels alone does not capture wiring complexity. The other measure is the number of distinct bundle types. A bundle type is represented by a tuple of (a) the capacity of the number of fibers in the bundle, and (b) the length of the bundle. If a topology requires only a small number of bundle types, its bundling is more homogeneous; manufacturing and procuring such bundles is significantly simpler, and deploying the topology is also simplified since fewer bundling errors are likely with fewer types.\nThese complexity measures are complete. The number of cable trays, the design of the chassis, and the number of racks can be derived from the number of switches (and the number of servers and the datacenter floor dimensions, which\nUSENIX Association 16th USENIX Symposium on Networked Systems Design and Implementation 237\nare inputs to the topology design). The number of cables and transceivers can be derived from the number of patch panels.\nIn some cases, a metric is related to another metric, but not completely subsumed by it. For example, the number of switches determines rack packaging, which only partially determines the number of transceivers per switch. The other determinant of this quantity is the connectivity in the logical topology (which switch is connected to which other switch). Similarly, the number of patch panels can influence the number of bundle types, but these are also determined by logical connectivity."
                },
                {
                    "heading": "3.3 Comparing Topology Classes",
                    "text": "To understand how the two main classes of topologies compare by these metrics, we apply these to a Clos topology and to a Jellyfish topology that support the same number of servers (131,072) and the same bisection bandwidth. This topology corresponds to twice the size of Jupiter. In \u00a76, we perform a more thorough comparison at larger and smaller scales, and we describe the methodology by which these numbers were generated.\nTable 1 shows that the two topology classes are qualitatively different by these metrics. Consistent with the finding in [32], Jellyfish only needs a little over half the switches compared to Clos to achieve comparable capacity due to its high edge expansion property. But, by other measures, Clos performs better. It exposes far fewer ports outside the rack (a little over half that of Jellyfish); we say Clos has better port-hiding. A pod in this Clos contains 16 aggregation and 16 edge switches4. The aggregation switches can be can be packed into a single rack, so bundles from edge switches to aggregation switches do not need to be rebundled though patch panels, and we only need two layers of patch panels between aggregation and spine layer. However, in Jellyfish, almost all links are inter-rack links, so it requires more patch panels.\nMoreover, for Clos, since each pod has the same number of links to each spine, all bundles in Clos have the same capacity (number of fibers). However, the length of bundles can be different, depending on the relative placement of the patch panels between aggregation and spine layers, so Clos has 74 bundle types. However, since Jellyfish is a purely random graph without structure, to enable bundling, we group a fixed amount of neighbor racks as blocks to enable bundling. Since connectivity is random, the number of links between blocks are not uniform, Jellyfish needs almost 20\u00d7 the number of bundle types. In \u00a76, we show that Xpander also has\n4we follow the definition of pod in [2].\nqualitatively similar behavior in large scale. Takeaway. Relative to a structured hierarchical class of topologies like Clos, the expander graph topology has inherently higher deployment complexity in terms of the number of bundle types and cannot support port-hiding well."
                },
                {
                    "heading": "4 Topology Expansion",
                    "text": "The second important component of topology lifecycle management is expansion. Datacenters are rarely deployed to maximal capacity in one shot; rather, they are gradually expanded as network capacity demands increase."
                },
                {
                    "heading": "4.1 The Practice of Expansion",
                    "text": "In-place Expansion. At a high-level, expanding a topology involves two conceptual phases: (a) procuring new switches, servers, and cables and laying them on the datacenter floor, and (b) re-wiring (or adding) links between switches in the existing topology and the new switches. Phase (b), the re-wiring phase, can potentially disrupt traffic; as links are re-wired, network capacity can drop, leading to traffic loss. To avoid traffic loss, providers can either take the existing topology offline (migrate services away, for example, to another datacenter), or can carefully schedule link re-wiring while carrying live traffic, but schedule the re-wiring to maintain a desired target capacity. The first choice can impact service availability significantly.\nSo, today, datacenters are expanded while carrying live traffic [30, 12, 31, 38]. To do this, expansion is carried out in steps, where at each step, the capacity of the topology is guaranteed to be at least a percentage p of the capacity of the existing topology. This fraction is sometimes called the expansion SLO. Today, many providers operate at expansion SLOs of 75% [38]; higher SLOs of 85-90% can impact availability budgets less while allowing providers to carry more traffic during expansion. The unit of expansion. Since expansion involves procurement, topologies are usually expanded in discrete units called blocks to simplify the procurement and layout logistics. In a structured topology, there are natural candidates for blocks. For example, in a Clos, a pod can be block, while in an Xpander, the metanode can be a block. During expansion, a block is first fully assembled and placed, and links between switches within a block are connected (as an aside, an Xpander metanode has no such links). During the re-wiring phase, only links between existing blocks and new blocks are re-wired. (This phase does not re-wire links between switches within an existing block). Aside from simplifying logistics, expanding at the granularity of a block preserves structure in structured topologies."
                },
                {
                    "heading": "4.2 An Expansion Step",
                    "text": "What happens during a step. Figure 2 shows an example of Clos expansion. The upper left figure shows a partiallydeployed logical Clos, in which each spine and aggregation\n238 16th USENIX Symposium on Networked Systems Design and Implementation USENIX Association\nblock are connected by two links. The upper right is the target fully-deployed Clos, where each spine and aggregation block are connected by a single link. During expansion, we need to redistribute half of existing links (dashed) to the newly added spines without violating wiring and capacity constraints.\nSuppose we want to maintain 87.5% of the capacity of the topology (i.e., the expansion SLO is 0.875), this expansion will require 4 steps in total, where each patch panel is involved in 2 of these steps. In Figure 2, we only show the rewiring process on the second existing patch panels. To maintain 87.5% capacity at each pod, only one link is allowed to be drained. In the first step, the red link from the first existing aggregation block and the green link from the second existing aggregation block are rewired to the first new spine block. In the second step, the orange links from the first existing aggregation block and the purple link from the second existing aggregation block are rewired to the first new spine block. A similar process happens in the first patch panel.\nIn practice, each step of expansion involves four sub-steps. In the first sub-step, the existing links that are to be re-wired are drained. Draining a link involves programming switches at each end of the link to disable the corresponding ports, and may also require reprogramming other switches or ports to route traffic around the disabled link. Second, one or more human operators physically rewire the links at a patch panel (explained in more detail below). Third, the newly wired links are tested for bit errors by sending test traffic through them. Finally, the new links are undrained.\nBy far the most time consuming part of each step is the second sub-step, which requires human involvement. This sub-step is also the most important from an availability perspective; the longer this sub-step takes, the longer the datacenter operates at reduced capacity, which can impact availability targets [12].\nThe role of patch panels in re-wiring. The lower figure in Figure 2 depicts the physical realization of the (logical) rewiring shown in the upper figure. (For simplicity, the figure only shows the re-wiring of links on one patch panel to a new pod). Fibers and bundles originate and terminate at patch panels, so re-wiring requires reconnecting input and output\nports at each patch panel. One important constraint in this process is that re-wiring cannot remove fibers that are already part of an existing bundle.\nPatch panels help localize rewiring and reuse existing cable bundling during expansions. Figure 3 shows, in more detail the rewiring process at a single patch panel. The leftmost figure shows the original wiring with connections (a, A), (b, B), (c, C), (d, D). To enable expansion, a topology is always deployed such that some ports at the patch panel are reserved for expansion steps. In the figure, we use these reserved ports to connect new fibers e, f , E and F (Phase 1). To get to a target wiring in the expanded network with connections (a, A), (b, B), (e, C), (f , D), (c, E), (d, F ), the following steps are taken: (1) Traffic is drained from (c, C), (d, D), (2) Connections (c, C), (d, D) are rewired, with c being connected to E, d being connected to F and so on, and (3) The new links are undrained, allowing traffic to use new capacity."
                },
                {
                    "heading": "4.3 Expansion Complexity Metrics",
                    "text": "We identify two metrics that quantify expansion complexity and use these metrics to identify differences between Clos and Jellyfish in the next subsection. Number of Expansion Steps. As mentioned each expansion step requires a series of substeps which cannot be parallelized. Therefore the number of expansion steps determines the total time for expansion. Average number of rewired links in a patch panel rack per step. With patch panels, manual rewiring dominates the time taken within each expansion step. Within steps, it is possible to parallelize rewiring across racks of patch panels. With such parallelization, the time taken to rewire a single patch panel rack will dominate the time taken for each expansion step."
                },
                {
                    "heading": "4.4 Comparing Topology Classes",
                    "text": "Table 2 shows the value of these measures for a medium-sized Clos and a comparable Jellyfish topology, when the expansion\nUSENIX Association 16th USENIX Symposium on Networked Systems Design and Implementation 239\nSLO is 90%. (\u00a76 has more extensive comparisons for these metrics, and also describes the methodology more carefully). In this setting, the number of links rewired per patch panel can be a factor of two less than Clos. Moreover, Jellyfish requires 3 steps, while Clos twice the number of steps.\nTo understand why Jellyfish requires fewer steps, we define a metric called the north-to-south capacity ratio for a block. This is the ratio of the aggregate capacity of all \u201cnorthbound\u201d links exiting a block to the aggregate capacity of all \u201csouthbound\u201d links to/from the servers within the block. Figure 4 illustrates this ratio: a thin edge (left), has an equal number of southbound and northbound links while a fat edge (right), has more northbound links than southbound links. A Clos topology has a thin edge, i.e., this ratio is 1, since the block is a pod. Now, consider an expansion SLO of 75%. This means that the southbound aggregate capacity must be at least 75%. That implies that, for Clos, at most 25% of the links can be rewired in a single step. However, Jellyfish has a much higher ratio of 3, i.e., it has a fat edge. This means that many more links can be rewired in a single step in Jellyfish than in Clos. This property of Jellyfish is required for reducing the number of expansion steps. Takeaway. Clos topologies re-wire more links in each patch panel during an expansion step and require many steps because they have a low north-south capacity ratio."
                },
                {
                    "heading": "5 Towards Lower Lifecycle Complexity",
                    "text": "Our discussions in \u00a73 and \u00a74, together with preliminary results presented in those sections (\u00a76 has more extensive results) suggest the following qualitative comparison between Clos and the expander graph families with respect to lifecycle management costs (Table 3): \u2022 Clos uses fewer bundle types and patch panels. \u2022 Jellyfish has significantly lower switch counts, uses fewer\nexpansion steps, and touches fewer links per patch panel during an expansion step. In all of these comparisons, we compare topologies with the same number of servers and the same bisection bandwidth.\nThe question we ask in this paper is: Is there a family of topologies which are comparable to, or dominate, both Clos and expander graphs by all our lifecycle management metrics? In this section, we present the design of the FatClique class of topologies and validate in \u00a76 that FatClique answers this question affirmatively."
                },
                {
                    "heading": "5.1 FatClique Construction",
                    "text": "FatClique (Figure 5) combines the hierarchical structure in Clos with the edge expansion in expander graphs to achieve lower lifecycle management complexity. FatClique has three\nlevels of hierarchy: individual sub-block (top left), interconnected into a block (top right), which are in turn interconnected to form FatClique (bottom). The interconnection used at every level in the hierarchy is a clique, similar to Dragonfly [20]. Additionally, each level in the hierarchy is designed to have a fat edge (a north-south capacity ratio greater than 1). The cliques enable high edge expansion, while hierarchy enables lower wiring complexity than random-graph based expanders [32, 35].\nFatClique is a class of topologies. To obtain an instance of this class, a topology designer specifies two input parameters: N , the number of servers, and k the chip radix. A synthesis algorithm takes these as inputs, and attempts to instantiate four design variables that completely determine the FatClique instance Table 4. These four design variables are: \u2022 s, the number of ports in a switch that connect to servers \u2022 pc, the number of ports in each switch that connect to other\nsub-blocks inside a block \u2022 Sc, the number of switches in a sub-block \u2022 Sb, the number of sub-blocks in a block The synthesis algorithm searches for the best combination of values for design variables, guided by six constraints, C1 through C6, described below. The algorithm also defines auxiliary variables for convenience; these can be derived from the design variables (Table 4). We define these variables in the narrative below.\nSub-block connectivity. In FatClique, the sub-block forms the lowest level of the hierarchy, and contains switches and servers. All sub-blocks have the same structure. Servers are distributed uniformly among all switches of the topology, such that each sub-block has the same number of servers attached. However, because this number of servers may not be an exact multiple of the number of switches, we distribute the remainder across the switches, so that some switches may be connected to one more server than others. The alternative would have been to truncate or round up the number of servers per sub-block to be divisible by the number of switches in\n240 16th USENIX Symposium on Networked Systems Design and Implementation USENIX Association\nthe sub-block, which could lead to overprovisioning or underprovisioning. Within a sub-block, every switch has a link to every other switch within its sub-block, to form a clique (or complete graph). To ensure a fat edge at the sub-block level, each switch must connect to more switches than servers, captured by the constraint C1 : s < r\u2212s, where r is the switch radix and s is the number of ports on a switch connected to servers.\nBlock-level connectivity. The next level in the hierarchy is the block. Each sub-block is connected to other sub-blocks within a block using a clique (Figure 5, top-left). In this clique, each sub-block may have multiple links to another sub-block; these inter-sub-block links are evenly distributed among all switches in the sub-block such that every pair of switches from different sub-block has at most one link. Ensuring a fat edge at this level requires that a sub-block has more intersub-block and inter-block links egressing from the sub-block than the number of servers it connects to. Because sub-blocks contain switches which are homogeneous5, this constraint is ensured if the sum of (a) the number ports on each switch connected to other sub-block (pc) and (b) those connected to other blocks (pb, an auxiliary variable in Table 4, see also Figure 6) exceeds the number of servers connected to the switch (captured by C2 : pc +pb > s).\nInter-block connectivity. The top of the hierarchy is the overall network, in which each block is connected to every\n5They are nearly homogeneous, since a switch may differ from another by one in the number of servers connected\nother block, resulting in a clique. The inter-block links are evenly distributed among all sub-blocks, and, within a subblock, evenly among all switches. To ensure a fat edge at this level, the number of inter-block links at each switch should be larger than the number of servers it connects to, captured by C3 : pb > s. Note that C3 subsumes (is a stronger constraint than) C2. Moreover, the constraint that blocks are connected in a clique imposes a constraint on the block radix (Rb, a derived variable). The block radix is the total number of links in a block destined to other blocks. Rb should be large enough to reach all other blocks (captured by C4 :Rb \u2265Nb\u22121) such that the whole topology is a clique.\nIncorporating rack space constraints. Beyond connectivity constraints, we need to consider packaging constraints in sub-block design. Ideally, we need to ensure that a sub-block fits completely into one or more racks with no wasted rack space. For example, if we use 58RU racks, and each switch is to be connected to 8 1RU servers, we can accommodate 6 switches per sub-block, leaving 58\u2212 (6\u00d7 8 + 6) = 4U in the rack for power supply and other equipment. In contrast, choosing 8 switches per sub-block would be a bad choice because it would need 8\u00d78+8 = 72U rack space, overflowing into a second rack that would have 44RU un-utilized. We model this packaging fragmentation as a soft constraint: our synthesis algorithm generates multiple candidate assignments to the design variables that satisfy our constraints, and of these, we pick the alternative that has the lowest wasted rack space.\nEnsuring edge expansion. At each level of the hierarchy, edge expansion is ensured by using a clique. This is necessary for high edge expansion, but not sufficient, since it does not guarantee that every switch connects to as many other switches across the network as possible. One way to ensure this diversity is to make sure that each pair of switches is connected by at most one link. The constraints discussed so far do not ensure this. For instance, consider Figure 6, in which Lcc (another auxiliary variable in Table 4) is the number of links from one sub-block to another. If this number is greater than the number of switches Sc in the sub-block, then, some pair of switches might have more than one link to each other. Thus, C5 : Lcc \u2264 Sc is a condition to ensure that each pair of switches must be connected by a single link. Our topology synthesis algorithm generates assignments to design variables, and a topology generator then assigns links to ensure this property (\u00a75.2).\nIncorporating patch panel constraints. The size of the block is also limited by the number of ports in a single patch panel rack (denoted by PP \u2212Rackports). It is desirable to ensure that the inter-block links egressing each block connect to at most 12 the ports in a patch panel rack, so that the rest of the patch panel ports are available for external connections into the block (captured by C6 :Rb \u2264 12 \u00b7PP \u2212Rackports).\nUSENIX Association 16th USENIX Symposium on Networked Systems Design and Implementation 241\n5.2 FatClique Synthesis Algorithm Generating candidate assignments. The FatClique synthesis algorithm attempts to assign values to the design variables, subject to constraints C1 to C6. The algorithm enumerates all possible combinations of value assignments for these variables, and filters out each assignment that fails to satisfy all the constraints. For each remaining assignment, it generates the topology specified by the design variable, and determines if the topology satisfies a required capacity Cap\u2217, which is an input to the algorithm. Each assignment that fails the capacity test is also filtered out, leaving a candidate set of assignments. These steps are described in \u00a7A.6.\nFatClique placement. For each assignment in this candidate set, the synthesis algorithm generates a topology placement. Because FatClique\u2019s design is regular, its topology placement algorithm is conceptually simple. A sub-block may span one or more racks, and these racks are placed adjacent to each other. All sub-blocks within a block are arranged in a rectangular fashion on the datacenter floor. For example, if a block has 25 racks, it is arranged in a 5\u00d75 pattern of racks. Blocks are then arranged in a similar grid-like fashion.\nSelecting best candidate. For each placement, the synthesizer computes the cabling cost of the resulting placement (using [7]), and picks the candidate with the lowest cost. This step is not shown in Algorithm 3. This approach implicitly filters out candidates whose sub-block cannot be efficiently packed into racks (\u00a75.1)."
                },
                {
                    "heading": "5.3 FatClique Expansion",
                    "text": "Re-wiring during expansion. Consider a small FatClique topology, shown top left in Figure 7, that has 3 blocks and Lbb = 5, i.e., five inter-block links. To expand it to a clique with six blocks, we would need to rewire the topology to have L\u2032bb = 2 (top right in Figure 7). This means we need to redistribute more than half (6 out of 10) of existing links\n(red) at each block to new blocks without violating wiring and capacity constraints.\nThe expansion process with patch panels is shown in the bottom of Figure 7. Similar to the procedure for Clos described in \u00a74.1, all new blocks (shown in orange) are first deployed and interconnected and links from the new blocks are routed to reserved ports on patch panels associated with existing blocks (shown in blue), before re-wiring begins.\nFor FatClique, rewiring one existing link requires releasing one patch panel port so that a new link can be added. Since links are already parts of existing bundles and routed through cable trays, we can not rewire them directly, e.g., by rerouting it from one patch panel to another. For example, link 1 (lower half of Figure 7) is originally connected blocks 1 and 3 by connecting ports a and b on the patch panel. Suppose we want to remove that link, and add two links, one from block 1 to block 5 (labeled 3), and another from block 3 to block 5 (labeled 4). The part of the original link (labeled 1) between the two patch panels is already bundled, so we cannot physically reroute it from block 3 to block 5. Instead, we effect re-wiring by releasing port a, connecting link 3 to port a, connecting link 1 to port c. Logically, this is equivalent to connecting ports a and d and b and c on the patch panel shown in lower half of Figure 7. This preserves bundling, while permitting expansion.\nIf the original topology has Nb blocks, by comparing the old and target topology, the total number of rewired links is computed by Nb(Nb\u2212 1)(Lbb\u2212L\u2032bb)/2. For this example, the total number of links to be rewired is 9. Iterative Expansion Plan Generation. By design, FatClique has fat edges, which allows draining more and more links at each step of the expansion, as network capacity increases. At each step, we drain links across all blocks uniformly, so that each block loses the same aggregate capacity. However the relationship between overall network capacity, and the number of links drained at every block in FatClique is unclear, because traffic needs to be sent over non-shortest paths to fully utilize the fabric.\nTherefore, we use an iterative approach to expansion planning, where, at each step, we search for the maximal ratio of links to be drained that still preserves expansion SLO. (\u00a7A.4 discusses the algorithm in more detail). Our evaluation \u00a76 shows that the number of expansion steps computed by this algorithm is much smaller than that for expanding symmetric Clos."
                },
                {
                    "heading": "5.4 Discussion",
                    "text": "Achieving low complexity. By construction, FatClique achieves low lifecycle management complexity (Table 3), while ensuring full-bisection bandwidth. It ensures high edge expansion, resulting in fewer switches. By packaging clique connections into a sub-block, it exports fewer external ports, an idea we call port hiding. By employing hierarchy and a regular (non-random) structure, it permits bundling and re-\n242 16th USENIX Symposium on Networked Systems Design and Implementation USENIX Association\nquires fewer patch panels. By ensuring fat edges at each level of the hierarchy, it enables fewer re-wired links per patch panel, and fewer expansion steps. We quantify these in \u00a76.\nScalability. Since Xpander and Jellyfish do not incorporate hierarchy, they can be scaled to arbitrarily large sizes. However, because Clos and FatClique are hierarchical, they can only scale to a fixed size for a given chip radix. Table 5 shows the maximum scale of each topology as a function of switch radix k. FatClique scales to the same order of magnitude as a 5-layer Clos. As shown in \u00a76, both of them can scale to 64 times bisection bandwidth of Jupiter.\nFatClique and Dragonfly. FatClique is inspired by Dragonfly [20] and they are both hierarchical topologies that use cliques as building blocks, but differ in several respects. First, for a given switch radix, FatClique can scale to larger topologies than Dragonfly because it incorporates one additional layer of hierarchy. Second, the Dragonfly class of topologies is defined by many more degrees of freedom than FatClique, so instantiating an instance of Dragonfly can require an expensive search [33]. In contrast, FatClique\u2019s constraints enable more efficient search for candidate topologies. Finally, since Dragonfly does not explicitly incorporate constraints for expansion, a given instance of Dragonfly may not end up with fat edges.\nRouting and Load Balancing on FatClique. Unlike for Clos, ECMP-based forwarding cannot be used achieve high utilization in more recently proposed topologies [20, 35, 32, 19]. FatClique belongs to this latter class, for which a combination of ECMP and Valiant Load Balancing [37] has been shown to achieve performance comparable to Clos [19]."
                },
                {
                    "heading": "6 Evaluating Lifecycle Complexity",
                    "text": "In this section, we compare three classes of topologies, Clos, expander graphs and FatClique by our complexity metrics."
                },
                {
                    "heading": "6.1 Methodology",
                    "text": "Topology scales. Because the lifecycle complexity of topology classes can be a function of topology scale, we evaluate complexity across three different topology sizes based on the number of servers they support: small, medium, and large. Small topologies support as many servers as a 3-layer clos topology. Medium topologies support as many servers as 4-layer Clos. Large topologies support as many servers as 5-layer Clos topologies6. All our experiments in this section are based on comparing topologies at the same scale.\nAt each scale, we generate one topology for each of Clos, Xpander, Jellyfish, and FatClique. The characteristics of these topologies are listed in Table 6. All these topologies use 32-port switching chips, the most common switch radix available today for all port capacities [5]. To compare topologies\n6To achieve low wiring complexity, a full 5-layer Clos topology would require patch panel racks with four times as many ports as available today, so we restrict ourselves to the largest Clos that can be constructed with today\u2019s patch panel capacities\nfairly, we need to equalize them first. Specifically, at a given scale, each topology has approximately the same bisection bandwidth, computed (following prior work [32, 35]) using METIS [18]. All topologies at the same scale support roughly the same number of servers; small, medium and large scale topologies achieve, respectively, 14 , 4, and 16 times capacity of Jupiter. (In A.8, we also compare these topologies using two other metrics).\nTable 6 also shows the scale of individual building blocks of these topologies in terms of number of switches. For Clos, we use the algorithm in \u00a7A.1 to design building blocks (chassis) and then use them to compose Clos. One interesting aspect of this table is that, at the 3 scales we consider, a FatClique\u2019s sub-block and block designs are identical, suggesting lower manufacturing and assembly complexity. We plan to explore this dimension in future work.\nFor each topology we compute the metrics listed in Table 3: the number of switches, the number of bundle types, the number of patch panels, the average number of re-wired links at a patch panel during each expansion step, and the number of expansion steps. To compute these, we need component parameters, and placement and expansion algorithms for each topology class. Component Parameters. In keeping with [4, 40], we use optical links for all inter-rack links. We use 96 port 1RU patch panels [10] in our analysis. A 58RU [28] rack with patch panels can aggregate 2 \u2217 96 \u2217 58 = 11,136 fibers. We call this rack a patch-panel rack. Most datacenter settings, such as rack dimensions, aisle dimensions, cable routing and distance between cable trays follow practices in [26]. We list all parameters used in our paper in \u00a7A.7. Placement Algorithms. For Clos, following Facebook\u2019s fbfabric [3], spine blocks are placed at the center of the datacenter, which might take multiple rows of racks, and pods are placed at two sides of spine blocks. Each pod is organized into a rectangular area with aggregation blocks placed in the middle to reduce the cable length from ToR to aggregation. FatClique\u2019s placement algorithm is discussed in \u00a75.2. For Xpander, we use the placement algorithm proposed in [19]. We follow the practice that all switches in a metanode are placed closed to each other. However, instead of placing a metanode into a row of racks, we place a metanode into a rectangular area of racks, which reduces cable lengths when metanodes are large. For Jellyfish, we design a random search algorithm to aggressively reduce the cable length (\u00a7A.2). Expansion Algorithms. For Clos, as shown in [38], it is fairly complex to compute the optimal number of rewired links for asymmetric Clos during expansion. However, when the original and target topologies are both symmetric, this number is easy to compute. For this case, we design an optimal algorithm (\u00a7A.5) which rewires the maximum number of links at each step and therefore uses the smallest number of steps to finish expansion. For FatClique, we use the algorithm discussed in \u00a75.3. For Xpander and Jellyfish, we design an\nUSENIX Association 16th USENIX Symposium on Networked Systems Design and Implementation 243\nexpansion algorithm based on the intuition from [35, 32] that, to expand a topology by n ports requires breaking n2 existing links. Finally, we have found that for all topologies, the number of expansion steps at a given SLO is scale invariant: it does not depend on the size of the original topology as long as the expansion ratio (target-topology-size-to-originaltopology-size ratio) is fixed (\u00a7A.3). Presenting results. In order to bring out the relative merits of topologies, and trends of how cost and complexity increase with scale, we present values for metrics we measure for all topologies and scales in the same graph. In most cases, we present the absolute values of these metrics; in some cases though, because our three topologies span a large size range, for some metrics the results across topologies are so far apart that we are unable to do so without loss of information. In these cases, we normalize our results by the most expensive, or complex topology."
                },
                {
                    "heading": "6.2 Patch Panel Placement",
                    "text": "The placement of patch panels is determined both by the structure of the topology and its scale. Between edge and aggregation layers in Clos. For small and medium scale Clos, no patch panels are needed between edge and aggregation layers. Each pod at these scales contains 16 aggregation switches, which can be packed into a single rack (we call this an aggregation-rack). Given that a pod at this scale is small, all links from the edge can connect to this rack. Since all links connect to one physical location, bundles form naturally. In this case, each bundle from edge racks contains 3\u00d716 fibers7. Therefore, no patch panels are needed between edge and aggregation layers.\nHowever, a large Clos needs one layer of patch panels between edge and aggregation layers since a pod at this scale is large. An aggregation block consists of 16 middle blocks8, each with 32 switches. The aggregation block by itself occupies a single rack. Based on the logical connectivity, links from any edge need to connect to all middle blocks. Without using patch panels, each bundle could at most contain 3\u00d716/16 = 3 fibers. In our design, we use patch panels to aggregate local bundles from edges first and then rebundle them on patch panels to form new high capacity bundles from patch panels to aggregation racks. Based on the patch panel\n7In our setting, each rack with 58RU can accommodate at most 3 switches and 48 associated servers. The total number of links out of this rack is 3\u221716.\n8We follow the terminology in [31]. A middle block is a sub-block in an aggregation block.\nrack capacity constraint, two patch panel racks are enough to form high capacity bundles from edge to aggregation layers. Specifically, in our design 128 edge switches and 8 aggregation racks connect to a single patch panel. In this design, each edge-side bundle contains 48 fibers and each aggregation-side bundle contains 128 fibers. Between aggregation and spine layers. The topology between aggregation and spine layer in Clos is much larger than that inside a pod. For this reason, to form high capacity bundles, two layers of patch panels are needed. As shown in Figure 1, one layer of patch panels is placed near spine blocks at the center of the data center floor. Each patch panel rack aggregates local bundles from four spine racks in medium and large scale topologies. Similarly, another layer of patch panels are placed near aggregation rack, permitting long bundles between those patch panels. In expanders and FatClique. For Jellyfish, Xpander and FatClique, patch panels are deployed at the server block side and long bundles form between those patch panels. In FatClique, each block requires one patch panel rack (\u00a75.3). In a large Xpander, since a metanode is too big (Table 6), it is not possible to use one patch panel rack to aggregate all links from a metanode. Therefore, we divide a metanode into homogeneous sections, called sub-metanodes, such that links from a sub-metanode can be aggregated at one patch panel rack. For Jellyfish, we partition the topology into groups, each of which contains the same number of switches as in a block in FatClique, so each group needs one patch panel rack."
                },
                {
                    "heading": "6.3 Deployment Complexity",
                    "text": "In this section, we evaluate our different topologies by our three measures of deployment complexity (\u00a73.2).\nNumber of Switches. Figure 8 shows how the different topologies compare in terms of number of switches used at various topology scales. Figure 8(a) shows the total number of\n244 16th USENIX Symposium on Networked Systems Design and Implementation USENIX Association\nswitches for the small topologies, Figure 8(b) for the medium, and Figure 8(c) for the large. The y-axes increase in scale by about an order of magnitude from left to right. FatClique has 20% fewer switches than Clos for a small topology, and 50% fewer for the large. The results for Jellyfish and Xpander are similar, consistent with findings in [35, 32]. This benefit comes from the edge expansion property of the non-Clos topologies we consider. This implies that Clos topologies, at large scale, may require nearly twice the capital expenditures for switches, racks, and space as the other topologies.\nNumber of Patch panels. Figure 9 shows the number of patch panels at different scales. As before, across these graphs, the y-axis scale increases approximately by one order of magnitude from left to right. At small and medium scales, Clos relies on patch panels mainly for connections between aggregation and spine blocks. Of all topologies at these scales, Clos uses the fewest number of patch panels: FatClique uses about 11% more patch panels, and Jellyfish and Xpander use almost 44-50% more. Xpander and Jellyfish rely on patch panels for all northbound links, and therefore in general, as scale increases, the number of patch panels in these networks grows (as seen by the increase in the y-axis scale from left to right).\nAt large scale, however, Clos needs many more patch panels, comparable to Xpander and Jellyfish. At this scale, Clos aggregation blocks span multiple racks, and patch panels are also needed for connections between ToRs and aggregation blocks. Here, FatClique\u2019s careful packaging strategy becomes more evident, as it needs nearly 25% fewer patch panels than Clos. The majority of patch panels used in FatClique at all scales comes from inter-block links (which increase with scale).\nFor this metric, Clos and FatClique are comparable at small and medium scales, but FatClique dominates at large scale.\nNumber of Bundle Types. Table 7 shows the number of bundle types used by different topologies at different scales. A bundle type (\u00a73.1) is characterized by (a) the number of\nfibers in the bundle, and (b) the length of the bundle. The number of bundle types is a measure of wiring complexity. In this table, if bundles differ by more than 1m in length, they are designated as separate bundle types.\nTable 7 shows that Clos and FatClique use the fewest number of bundle types; this is due to the hierarchical structure of the topology, where links between different elements in the hierarchy can be bundled. As the topology size increases, the number of bundle types also increases in these topologies, by a factor of about 40 for Clos to 20 for FatClique when going from small to large topologies.\nOn the other hand, Xpander and Jellyfish use an order of magnitude more bundle types compared to Clos and FatClique at medium and large scales, but use a comparable number for small scale topologies. Even at the small scale, Jellyfish uses many more bundle types because it uses a random connectivity pattern. At small scales Xpander metanodes use a single patch panel rack and bundles from all metanodes are uniform. With larger scales, Xpander metanodes become too big to connect to a single patch panel rack. We have to divide a metanode into several homogeneous sub-metanodes such that all links from sub-metanodes connect to a patch panel rack. However, because of the randomness in connectivity, this subdivision cannot ensure uniformity of bundles egressing sub-metanode patch panel racks, so we find that Xpander has a large number of bundle types in medium and large topologies.\nThus, by this metric, Clos and FatClique have the lowest complexity across all three scales, while Xpander and Jellyfish have an order of magnitude more complexity. Moreover, across all metrics FatClique has lowest deployment complexity, especially at large scales.\nCase Study: Quantifying cabling costs. While not all aspects of lifecycle management complexity can be translated to actual dollar costs, it is possible to estimate one aspect, namely the cost of cables. Cabling cost includes the cost of transceivers and cables, and is reported to be the dominant component of overall datacenter network cost [31, 20]. We can estimate costs because our placement algorithms generate cable or bundle lengths, the topology packaging determines the number of transceivers, and estimates of cable and transceiver costs as a function of cable length are publicly available [7].\nFigure 10 quantifies the cabling cost of all topologies,\nUSENIX Association 16th USENIX Symposium on Networked Systems Design and Implementation 245\nacross different scales. Clos has higher cabling costs at small and medium scales compared to expander graphs, although the relative difference decreases at medium scale. At large scales, the reverse is true. Clos is around 12% cheaper than Xpander in terms of cabling cost since Xpander does not support port-hiding at all and uses more long inter-rack cables. Thus, given that cabling cost is the dominant component of overall cost, it is unclear whether the tradeoff Xpander and Jellyfish makes in terms of number of switches and cabling design pays off in terms of capital expenditure, especially at large scale.\nWe find that FatClique has the lowest cabling cost of the topologies we study with a cabling cost 23-36% less than Clos. This result came as a surprise to us, because intuitively topologies that require all-to-all clique like connections might use longer length cables (and therefore more expensive transceivers). However on deeper examination, we found that Clos uses a larger number of cables (especially inter-rack cables) compared to other topologies since it has a relatively higher number of switches (Figure 8) to achieve the same bisection bandwidth. Thus, more switches leads to more racks and datacenter floor area, which stretches the cable length. All those factors together explain why Clos cabling costs are higher than FatClique\u2019s.\nThus, from an equipment capital expenditure perspective, at large scale a FatClique can be at least 23% cheaper than a Clos, because it has at least 23% fewer switches, 33% fewer patch panel racks, and 23% lower cabling costs than Clos."
                },
                {
                    "heading": "6.4 Expansion Complexity",
                    "text": "In this section, we evaluate topologies by our two measures of expansion complexity (\u00a74.3): number of expansion steps required, and number of rewired-links per patch panel rack per step. Since the number of steps is scale-invariant (\u00a76.1), we only present the results from expanding medium size topologies for both metrics9. When evaluating Clos, we study the expansion of symmetric Clos topologies; generic Clos expansion is studied in [38]. As discussed in \u00a76.1, for symmetric Clos, we have developed an algorithm with optimal number of rewiring steps. Number of expansion steps. Figure 11 shows the number of steps (y-axis) required to expand topologies to twice their existing size (expansion ratio = 2) at different expansion SLOs (x-axis). We find that at 75% SLO, all topologies require the same number of expansion steps. But the number\n9We have verified that the relative trend in the number of re-wired links per patch panel holds for small and large topologies\nof steps required to expand Clos with tighter SLOs steeply increases. This is because the number of links that can be rewired per aggregation block in Clos per step, is limited (due to north-to-south capacity ratio \u00a74.3) by the SLO. The tighter the SLO, fewer the number of links rewired per aggregation block per step, and larger the number of steps required to complete expansion. FatClique, Xpander and Jellyfish require fewer and comparable number of expansion steps due to their fat edge property, allowing many more links to be rewired per block per step. Their curves largely overlap (with FatClique taking one more step as SLO increases beyond 95%) . Number of rewired links per patch panel rack per step. This metric is an indication of the time it takes to finish an expansion step because, today, rewiring each patch panel requires a human operator [38]. A datacenter operator can reduce re-wiring time by employing staff to rewire each patch panel rack in parallel, in which case, the number of links per patch panel rack per step is a good indicator of the complexity of an expansion step. Figure 12 shows the average of the maximum rewired links per patch panel rack, per step (y-axis), when expanding to twice the topology size size at different SLOs (y-axis). Even though the north-to-south capacity ratio restricts the number of links that can be rewired in Clos per step, the number of rewired links per patch panel rack per step in Clos remains consistently higher than other topologies, until we hit 97.5% SLO. The reason is that the links that need to be rewired in Clos are usually concentrated in few patch panel racks by design. As such, it is harder to parallelize rewiring in Clos, than it is in the other topologies. FatClique has the lowest rewiring step complexity across all topologies."
                },
                {
                    "heading": "6.5 FatClique Result Summary",
                    "text": "We find that FatClique is the best at most scales by all our complexity metrics. (The one exception is that at small and medium scales, Clos has slightly fewer patch panels). It uses 50% fewer switches and 33% fewer patch panels than Clos at large scale, and has a 23% lower cabling cost (an estimate we are able to derive from published cable prices). Finally, FatClique can permit fast expansion while degrading network capacity by small amounts (2.5-10%): at these levels, Clos can take 5 \u00d7 longer to expand the topology, and each step of Clos expansion can take longer than FatClique because the\n246 16th USENIX Symposium on Networked Systems Design and Implementation USENIX Association\nnumber of links to be rewired at each step per patch panel can be 30-50% higher."
                },
                {
                    "heading": "7 Related Work",
                    "text": "Topology Design. Previous topology designs have focused on cost effective, high capacity and low diameter datacenter topologies like [6, 35, 32, 4, 20]. Although they achieve good performance and cost properties, the lifecycle management complexity of these topologies have not been investigated either in the original papers or in subsequent work that has compared topologies [26, 27]. In contrast to these, we explore topology designs that have low lifecycle complexity. Recent work has explored datacenter topologies based on free space optics [24, 11, 9, 16, 39] but because we lack operational experience with them at scale, it is harder to design and evaluate lifecycle complexity metrics for them. Topology Expansion. Prior work has discussed several aspects of topology expansion [30, 32, 35, 8, 38]. Condor [30] permits synthesis of Clos-based datacenter topologies with declarative constraints some of which can be used to specify expansion properties. A more recent paper [38] attempts to develop a target topology for expansion, given an existing Clos topology, that would require the least number of link rewiring. REWIRE [8] finds target expansion topologies with highest capacity and smallest latency without preserving topological structure. Jellyfish [32] and Xpander [35] study expansion properties of their topology, but do not consider practical details in re-wiring. Unlike these, our work is examines lifecycle management as a whole, across different topology classes, and develops new performance-equivalent topologies with better lifecycle management properties."
                },
                {
                    "heading": "8 Conclusions and Future Work",
                    "text": "In this paper, we have attempted to characterize the complexity of lifecycle management of datacenter topologies, an unexplored but critically important area of research. Lifecycle management consists of network deployment and expansion, and we devise metrics that capture the complexity of each. We use these to compare topology classes explored in the research literature: Clos and expander graphs. We find that each class has low complexity by some metrics, but high by others. However, our evaluation suggests topological features important for low lifecycle complexity: hierarchy, edge expansion and fat edges. We design a family of topologies called FatClique that incorporates these features, and this class has low complexity by all our metrics at large scale.\nAs the management complexity of networks increases, the importance of designing for manageability will increase in the coming years. Our paper is only a first step in this direction; several future directions remain. Topology oversubscription. In our comparisons, we have only considered topologies with an over-subscription ratio of 1:1. Jupiter [31] permits over-subscription at the edge of the network, but there is anecdotal evidence that providers also\nover-subscribe at higher levels in Clos topologies. To explore the manageability of over-subscribed topologies it will be necessary to design over-subscription techniques in FatClique, Xpander and Jellyfish in a way in which all topologies can be compared on a equal footing. Topology heterogeneity. In practice, topologies have a long lifetime over which they accrue heterogeneity: new blocks with higher radix switches, patch panels with different port counts etc. These complicate lifecycle management. To evaluate these, we need to develop data-driven models for how heterogeneity accrues in topologies over time and adapt our metrics for lifecycle complexity to accommodate heterogeneity. Other management problems. Our paper focuses on topology lifecycle management, and explicitly does not consider other network management problems like fault isolation or control plane complexity. Designs for manageability must take these into account.\nUSENIX Association 16th USENIX Symposium on Networked Systems Design and Implementation 247"
                },
                {
                    "heading": "A Appendix",
                    "text": "A.1 Clos Generation Algorithm For Clos topologies, the canonical recursive algorithm in [36] can only generate non-modular topologies as shown in Figure 13. In practice, as shown in Jupiter [31], the topology is composed of heterogenous building blocks (chassis), which are packed into a single rack and therefore enforce port hiding (the idea that as few ports from a rack are exposed outside the rack). Although Jupiter is modular and supports port hiding, it is single instance of a Clos-like topology with a specific set of parameters. We seek an algorithm that can take any valid set of Clos parameters and produce chassis-based topologies automatically. Besides, it would be desirable for this algorithm to generate all possible feasible topologies satisfying the parameters, so we can select the one that is most compactly packed.\nOur logical Clos generation algorithm achieves these goals. Specifically, the algorithm uses the following steps:\n1. Compute the total number of layers of homogeneous switching chips needed. Namely, given N servers and radix k switches, we use n= log k\n2 (N2 ) to compute the\nnumber of layers of chips n needed.\n2. Determine the total number of layers of chips for edge, aggregation and core layers, which are represented by e, a and s respectively, such that e+a+s= n.\n3. Identify blocks for edge, aggregation and core layer. Clos networks rely on every edge being able to reach every spine through exactly one path, by fanning out via as many different aggregation blocks as possible (and vice versa). We find that the resulting interconnection is a derivative of the classical perfect shuffle Omega network ([21], e.g., aggregation blocks in Figure 14 and Figure 15). Therefore, we use Omega networks to build both the edge and aggregation blocks, and to define the connections between edge-aggregation and aggregationspines. The spine block on the other hand needs to be rearrangeably-nonblocking, so it can relay flows from any edge to any other edge with full capacity. Therefore it is built as a smaller Clos topology [6] (e.g., spine blocks in Figure 14).\n4. Compose the whole network using edge, aggregation and core blocks. The process to compose the whole topology is to link all these blocks and uses the same procedure as Jupiter[31].\nWe have verified that topologies generated by our construction algorithm, such as the ones in Figure 14 and Figure 15, are isomorphic to a topology generated using the canonical algorithm in Figure 13. By changing different combinations of e, a and s, we can obtain multiple candidate topologies, as shown in Figure 14 and Figure 15.\nA.2 Jellyfish Placement Algorithm For Jellyfish, we use a heuristic random search algorithm to place switches and servers. The algorithm works as follows. At each stage of the algorithm, a node can be in one of two states: placed, or un-placed. A placed node is one which has been positioned in a rack. Each step of the algorithm randomly selects an un-placed node. If the selected node has logical neighbor nodes that have already been placed, we place this node at the centroid of the area formed by its placed logical neighbors. If no placed neighbor exists, the algorithm randomly selects a rack to place the node. We have also tried other heuristics like neighbor-first, which tries to place a switch\u2019s logical neighbors as close as possible around it. However, this performs worse than our algorithm.\nA.3 Scale-invariance of Expansion\nScale-invariance of Expandability for Symmetric Clos. For a symmetric Clos network, the number of expansion steps is scale-invariant and independent of the degree to which the original topology is partially deployed. Consider a simplified Clos where the original topology has g aggregation blocks. Each aggregation block has p ports for spine-aggregation links, each of which has the unit capacity. Assume the worstcase traffic in which all sources are located in the left half of aggregation blocks and all destinations are in the right half. This network contains g \u00b7p/2 crossing links between left and right halves. If, during expansion, the network is expected to support a demand of d units capacity per aggregation block, the total demand traversing the cut between the left and right halves in one direction is d \u00b7g/2. Then, the maximum number of links that can be redistributed in an expansion step is k = g \u00b7 p/2\u2212 d \u00b7 g/2 = g(p\u2212 d)/2, which is linear in the number of aggregation blocks (network size). This linearity between k and g implies scale-invariant expandability, e.g., when an aggregation block is doubled to 2g, the maximum number of redistributed links per expansion step becomes 2k. Scale-invariance of Expandability for Jellyfish, Xpander, and FatClique. A random graph consists of s nodes, which is a first-order approximation for Jellyfish\u2019s switch, Xpander\u2019s metanode and FatClique\u2019s block. Each node has p internode ports, so there are s \u00b7 p/2 inter-node links. We can treat the network as a bipartite graph. We assume the worstcase traffic matrix, where all traffic is sent through one part of the bipartite graph to the other. Suppose an expansion SLO requires each source-destination node pair to support d unit demand. Then the total demands from all sources are d \u00b7 s/2. The probability of a link being a cross link is 1/2, and the expected number of cross links is s \u00b7p/4. These cross links are expected to be the bottleneck between the sourcedestinations pairs. Therefore, in the first expansion step, we can redistribute at most k = s \u00b7p/4\u2212d \u00b7s/2 = s(p/4\u2212d/2) links, and the maximum number of redistributed links is linear in the number of nodes (network size), e.g., if the number of\n250 16th USENIX Symposium on Networked Systems Design and Implementation USENIX Association\nnodes is doubled to 2s, we can redistribute 2k links in the first step. It is easy to see that, after each expansion step, the number of links added to the bottleneck is also linear with the number of nodes, so the expandability is scale-invariant.\nA.4 FatClique Expansion Algorithm Algorithm 1 shows the expansion algorithm for FatClique. The input to the algorithm includes original and target topologies T o and Tn, the link break ratio during an expansion step \u03b1, multipliers \u03b2 < 1 and \u03b3 > 1, which are used to adjust \u03b1 based on network capacity. \u03b1 specifies the fraction of existing links that must be broken for re-wiring. The output of the algorithm is the expansion plan Plan.\nOur expansion algorithm is an iterative trial-and-error approach (Line 4). Each iteration tries to find the right amount of links to break while satisfying the aggregate capacity constraint (Line 11) and the edge capacity constraint (Line 6), which guarantees that the north-to-south capacity ratio is always not smaller than 1 during any expansion step. If all constraints are satisfied, we accept this plan and tentatively increase the link break ratio \u03b1 (Line 16, by multiplying by \u03b3) due to capacity increase. Otherwise, the link break ratio \u03b1 (Line 12) is decreased (by multiplying by \u03b2 conservatively.)\ninput : T o, Tn, SLO output: Plan\n1 Initialize \u03b1 \u2208 (0,\u221e),\u03b2 \u2208 (0,1),\u03b3 \u2208 (1,\u221e) 2 Find the total set of links to break, L, based on T o and Tn 3 Compute original capacity c0 4 while |L|> 0 do 5 Select a subset of links Lb, from L uniformly across all blocks, where |Lb|= \u03b1|L|. 6 if Lb does not satisfy edge capacity constraint then 7 \u03b1= \u03b1 \u00b7\u03b2 8 end 9 Delete Lb from T o\n10 c = ComputeCapacity(T o) 11 if c < c0 \u00b7SLO then 12 \u03b1= \u03b1 \u00b7\u03b2 13 add Lb back to T o 14 else 15 T o = AddNewLinks(Lb, T\no, Tn) 16 \u03b1= \u03b1 \u00b7\u03b3 17 Plan.add(Lb) 18 end 19 end\nAlgorithm 1: FatClique Expansion Plan Generation\nA.5 Expansion for Clos Since the motivation of this work is to compare topologies, we only focus on developping optimal expansion solutions for symmetrical Clos. More general algorithms for Clos\u2019 expansion can be found in [38]. Also, similar to [38], we assume the worst case traffic matrices for Clos, i.e., servers under a pod will send traffic using full capacity to servers in other pods. Target Topology Generation. As mentioned in \u00a74.1, a pod is the unit of expansion in Clos. When we add new pods and associated spines to a Clos topology for expansion, the wiring pattern inside a pod remains unchanged. To make the target topology non-blocking and to ease expansion (i.e., number of to-be-redistributed links on each pod is the same), links from a pod should be distributed across all spines as evenly as possible. Expansion plan generation. Once a target Clos topology is generated, the next step is to redistribute links to convert the original topology into the target topology. By comparing the original and target topology, it is easy to figure out which new links should be routed to which patch panels to satisfy the wiring constraint. In this section, we mainly focus on how to drain links such that the capacity constraint is satisfied and the number of expansion steps is minimized.\nInsight 1: Maximum rewired links at each pod is bounded. At each expansion step, when links are drained, network capacity drops. At the same time, as expansion proceeds, new devices are added incrementally, the overall network capacity increases gradually during the whole expansion process. In general, during expansion, the incrementally added capacity should be leveraged to speed up the expansion process. Due to the thin edges in Clos, no matter what the overall network capacity is, the maximum number links to be drained at each pod is bounded by the number of links on each pod multiplied by (1\u2212SLO). Figure 16 shows an example. The leftmost figure is a folded Clos, where each pod has 16 links (4 trunks). If the SLO is 75%, the maximum number of links to be drained at a single step is 16\u00d7 (1\u22120.75) = 4. For our expansion plan generation algorithm, we try to achieve this bound at each pod at every single step.\nInsight 2: Drain links at spines uniformly across edges (pods). Given the number of links allowed to be drained at each pod, we need to carefully select which links are to be drained. Figure 16 shows two draining plans. Drain plan 1 will drain links from two spines uniformly across all pods. The residual capacity is 48, satisfying the requirement\nUSENIX Association 16th USENIX Symposium on Networked Systems Design and Implementation 251\nInsight 3: Create physical loops by selecting the right target spines. Ideally, drained links with the same index on a pod on the same original spine should be redistributed to the same spine because the traffic sent from the pod to the target spine has a return path to the pod. Otherwise, the traffic will be dropped. Figure 17 illustrates this insight. The right side of the figure shows the performance of two redistribution plans. The y axis shows the normalized capacity of the network at each expansion step. In the first plan, link 1 is first moved to spine s1 (1-s1),followed by link 3 to the same spine s1 (3-s1) which results in 75% capacity loss, since the two pods are connected by three paths instead of four. Once links 1 and 3 are undrained, s1 connects the two pods by a fourth path, and the normalized capacity is restored to 1. This redistribution step now provides leeway for supporting 25% capacity loss in the next step. In this next step, links 2 and 4 are rewired to connect to s2. During the rewiring, capacity again drops to 75%, with three paths between the pods. On undraining links 2 and 4, the capacity is once again restored to 1. In contrast, redistribution plan 2 violates SLO because it does not focus on restoring capacity by establishing paths via the new spine, as suggested by the insight (links 1 and 3 are moved to different spines).\nInspired by these insights, we designed Algorithm 2, which can achieve all our insights simultaneously when both original and target topologies are symmetric. The algorithm is optimal since at every expansion step, it achieves the upper bound of the links that could be drained. Therefore, our algorithm uses smallest steps to expand Clos.\nThe input to the algorithm is the original and new symmetric topology T o and Tn. We use T osp and T n sp to represent the number of links between spine s and pod p in the old and new topology respectively. Initially, T os\u2032p = 0, where s\n\u2032 is a new spine. The output of the algorithm is the draining plan, Subplani, for expansion step i. The final expansion plan Plan= {Subplani} and the number of Subplan, |Plan|, is the total expansion step.\nThe algorithm starts by indexing old spines, new spines and links on each pod from left to right respectively (Line 1-2), which are critical for the correctness of the algorithm since the algorithm relies on these indexes to break ties when selecting spines and links to redistribute. Then, based on our Insight 1, Line 3 computes the upper bound on the number of links to be redistributed on each pod, np. We show experimentally that our algorithm can always achieve this upper bound in each individual step as long as T o and Tn are symmetric. Next, the algorithm iterates over all indexed old spines (Line 4) and tries to drain np links uniformly across all pods (Line 5) such that Insight 2 is satisfied. Line 6 compares the number of remaining to-be-redistributed links \u03b4sp and np and is useful only at the last expansion step. For each pod, the algorithm needs to find spines to redistribute links to (Line 7-14) while satisfying the constraint in Insight 3, i.e., drained links with the same index on a pod on the same original spine are redistributed to the same spine. Due to indexing and symmetric structure of Clos, our algorithm can always satisfy Insight 3. Specifically, when selecting spines, the spine satisfying \u03b4s\u2032p = Tns\u2032p\u2212T o s\u2032p > 0 with the smallest index will be considered first (Line 8-Line 10). When selecting links from pod to redistribute, we always select the first na links to redistribute (Line 14).\nTheorem 1 Algorithm 2 produces the optimal expansion plan for Clos topology.\nThe proof is simple. Since at every expansion step, our algorithm achieves the upper bound of the links that could be drained, our algorithm uses smallest steps to finish the expansion.\nA.6 FatClique Topology Synthesis Algorithm The topology synthesis algorithm for FatClique is shown in Algorithm 3. Essentially, the algorithm is a search algorithm, and leverages the constraints C1 to C6 in \u00a75.1 to prune the search space. It works as follows. The outermost loop (Line 2) enumerates the number of racks used for a sub-block. Based on the rack space constraints, sub-block size Sc is determined Line 4. Next, the algorithm iterates over the number of subblocks in a block Sb Line 5, whose size is constrained by MaxBlockSize. Inside this loop, we leverage constraints C1 to C6 and derivations in \u00a75.1 to find the feasible set of pc, which is represented by Pc (Line 6). Then we construct FatClique based all design variables Line 8 and compute its capacity Line 9. If the capacity matches the target capacity,\n252 16th USENIX Symposium on Networked Systems Design and Implementation USENIX Association\ninput : T o, Tn, SLO output: Subplan\n1 Index original and new spines from left to right starting from 1 respectively 2 Index links at each pod from left to right starting from 1 3 \u2200 pod p, np = num_links_per_pod \u00b7 (1-SLO) // Insight 1 4 foreach Original Spine s do 5 foreach pod p do // Insight 2 6 \u03b4sp = T osp\u2212Tnsp, np = min(np, \u03b4sp) // Insight 2 7 while np > 0 do 8 foreach New Spine s\u2032 do // Insight 3 9 \u03b4s\u2032p = Tns\u2032p\u2212T o s\u2032p\n10 if \u03b4s\u2032p > 0 then break 11 12 end 13 na =min(\u03b4s\u2032p,np) 14 Find the first na to-be-distributed links, Lsp 15 np = np\u2212na, update(T o) 16 Subplan.add(Lsp) 17 end 18 end 19 end\nAlgorithm 2: Single Step Clos Expansion Plan Generation\nwe add this topology into candidate set (Line 15). If the capacity is larger than required, the algorithm will increase s by 1 which will decrease the number of switches used n= N/s (N is fixed) and therefore reduce the network capacity in next search step (Line 13). If the capacity is smaller than required, the algorithm will decrease s by 1 (Line 11) to increase the number of switches and capacity in next search step.\nA.7 Parameter Setting The cable price with transceivers used in our evaluation is listed in Table 9. We found that a simple linear model does not fit the data. The data is better approximated by a piecewise linear function: cables shorter than 100 meters are fit using one linear model and cables beyond 100 meters are fit using another linear model. The latter has a larger slope because beyond 100 meters, more advanced and expensive transceivers are necessary. In our experiment, since we only know the discrete price for cables and associated transceivers, we do the following: if the length of the cable is X, we use the exact price; if the length if larger than X, we use the first cable price larger than X.\nA.8 Other Metrics In our evaluations, we have tried to topologies with qualitatively similar properties 6. In this section, we quantify other properties of these topologies. Edge Expansion and Spectral Gap. Since computing edge expansion is computationally hard, we follow the method in [35] using spectral gap [17] to approximate edge expansion. A larger spectral gap implies larger edge expansion. To fairly compare topologies, we equalize their bisection bandwidth first. As shown before, to achieve the same bisection\nUSENIX Association 16th USENIX Symposium on Networked Systems Design and Implementation 253\n100 200 300 400 500\nSmall-scale Clos Jellyfish Xpander FatClique\n4 50 5\n10 15\nNu m\nbe r o\nf P at\nhs\nPath Length\nFigure 19: Path Diversity for Small-scale Topologies\nbandwidth, Clos uses many more switches. Also, Clos is not a d-regular graph and do not know of a way to compute the spectral graph for Clos-like topologies. Therefore, we compare the spectral gap only for d-regular graphs, Jellyfish, Xpander and FatClique at different scales (1k-4k nodes). The spectral gap is defined as follows [17]. Let G with node degree d and A(G) denote the d-regular topology and its adjacent matrix. The matrix A(G) has n real eigenvalues which we denote by \u03bb1 \u2265 \u03bb2 \u2265 \u00b7\u00b7 \u00b7 \u2265 \u03bbn. Spectral gap SG= d\u2212\u03bb2. In our experiments, chip radix is 32 and each node in those topologies connects to 8 servers, d= 24. The result is shown in Figure 18. First, we observe that spectral gap stays roughly the same under different scales. Also, the spectral gap of FatClique is slightly lower than that of other topologies, which implies that FatClique has slightly smaller edge expansion compared to Jellyfish and Xpander. This is to be expected, since FatClique adds some hierarchical structure to cliques. Path Diversity. We compute the path diversity for different topologies. For Clos, we only calculate the number of shortest paths between two ToR switches from different pods. For other topologies, we compute the number of paths which are no longer than the shortest paths in the same-scale Clos. For example, for small-scale Clos, the shortest path length is 5. We will only calculate paths whose length is no larger than 5 in other topologies. This is a rough metric for path diversity. The results are shown in Figure 19 and Figure 20. We found that Jellyfish, Xpander and FatClique have the same level of path diversity, which is higher than that of Clos. Also, those topologies have shorter paths than Clos.\n254 16th USENIX Symposium on Networked Systems Design and Implementation USENIX Association"
                }
            ],
            "year": 2019,
            "references": [
                {
                    "title": "Hyperx: Topology, routing, and packaging of efficient large-scale networks",
                    "authors": [
                        "J.H. Ahn",
                        "N. Binkert",
                        "A. Davis",
                        "M. McLaren",
                        "R.S. Schreiber"
                    ],
                    "venue": "Proc. SC9,",
                    "year": 2009
                },
                {
                    "title": "A scalable, commodity data center network architecture",
                    "authors": [
                        "M. Al-Fares",
                        "A. Loukissas",
                        "A. Vahdat"
                    ],
                    "venue": "Proc. ACM SIGCOMM,",
                    "year": 2008
                },
                {
                    "title": "Slim fly: A cost effective low-diameter network topology",
                    "authors": [
                        "M. Besta",
                        "T. Hoefler"
                    ],
                    "venue": "Proc. SC14,",
                    "year": 2014
                },
                {
                    "title": "A study of non-blocking switching networks",
                    "authors": [
                        "C. Clos"
                    ],
                    "venue": "The Bell System Technical Journal, 32(2):406\u2013424, March",
                    "year": 1953
                },
                {
                    "title": "Rewire: An optimization-based framework for unstructured data center network design",
                    "authors": [
                        "Andrew R. Curtis",
                        "Tommy Carpenter",
                        "Mustafa Elsheikh",
                        "Alejandro L\u00f3pez-Ortiz",
                        "Srinivasan Keshav"
                    ],
                    "venue": "In Proc. IEEE INFOCOMM,",
                    "year": 2012
                },
                {
                    "title": "Helios: A hybrid electrical/optical switch architecture for modular data centers",
                    "authors": [
                        "N. Farrington",
                        "G. Porter",
                        "S. Radhakrishnan",
                        "H.H. Bazzaz",
                        "V. Subramanya",
                        "Y. Fainman",
                        "G. Papen",
                        "A. Vahdat"
                    ],
                    "venue": "Proc. ACM SIGCOMM,",
                    "year": 2010
                },
                {
                    "title": "Projector: Agile reconfigurable data center interconnect",
                    "authors": [
                        "M. Ghobadi",
                        "R. Mahajan",
                        "A. Phanishayee",
                        "N. Devanur",
                        "J. Kulkarni",
                        "G. Ranade",
                        "P.-A. Blanche",
                        "H. Rastegarfar",
                        "M. Glick",
                        "D. Kilper"
                    ],
                    "venue": "Proc. ACM SIGCOMM,",
                    "year": 2016
                },
                {
                    "title": "Evolve or die: High-availability design principles drawn from googles network infrastructure",
                    "authors": [
                        "R. Govindan",
                        "I. Minei",
                        "M. Kallahalla",
                        "B. Koley",
                        "A. Vahdat"
                    ],
                    "venue": "Proc. ACM SIGCOMM,",
                    "year": 2016
                },
                {
                    "title": "Vl2: a scalable and flexible data center network",
                    "authors": [
                        "A. Greenberg",
                        "J.R. Hamilton",
                        "N. Jain",
                        "S. Kandula",
                        "C. Kim",
                        "P. Lahiri",
                        "D.A. Maltz",
                        "P. Patel",
                        "S. Sengupta"
                    ],
                    "venue": "Proc. ACM SIGCOMM,",
                    "year": 2009
                },
                {
                    "title": "BCube: A High Performance, Server-centric Network Architecture for Modular Data Centers",
                    "authors": [
                        "C. Guo",
                        "G. Lu",
                        "D. Li",
                        "H. Wu",
                        "X. Zhang",
                        "Y. Shi",
                        "C. Tian",
                        "Y. Zhang",
                        "S. Lu"
                    ],
                    "venue": "Proc. ACM SIGCOMM,",
                    "year": 2008
                },
                {
                    "title": "DCell: A Scalable and Fault-tolerant Network Structure for Data Centers",
                    "authors": [
                        "C. Guo",
                        "H. Wu",
                        "K. Tan",
                        "L. Shi",
                        "Y. Zhang",
                        "S. Lu"
                    ],
                    "venue": "Proc. ACM SIGCOMM,",
                    "year": 2008
                },
                {
                    "title": "Firefly: A reconfigurable wireless data center fabric using freespace optics",
                    "authors": [
                        "N. Hamedazimi",
                        "Z. Qazi",
                        "H. Gupta",
                        "V. Sekar",
                        "S.R. Das",
                        "J.P. Longtin",
                        "H. Shah",
                        "A. Tanwer"
                    ],
                    "venue": "Proc. ACM SIGCOMM,",
                    "year": 2014
                },
                {
                    "title": "Expander graphs and their applications",
                    "authors": [
                        "Shlomo Hoory",
                        "Nathan Linial",
                        "Avi Wigderson"
                    ],
                    "venue": "Bull. Amer. Math. Soc.,",
                    "year": 2006
                },
                {
                    "title": "A fast and high quality multilevel scheme for partitioning irregular graphs",
                    "authors": [
                        "G. Karypis",
                        "V. Kumar"
                    ],
                    "venue": "SIAM J. Sci. Comput.,",
                    "year": 1998
                },
                {
                    "title": "Beyond fat-trees without antennae, mirrors, and disco-balls",
                    "authors": [
                        "S. Kassing",
                        "A. Valadarsky",
                        "G. Shahaf",
                        "M. Schapira",
                        "A. Singla"
                    ],
                    "venue": "Proc. ACM SIGCOMM,",
                    "year": 2017
                },
                {
                    "title": "Technologydriven, highly-scalable dragonfly topology",
                    "authors": [
                        "J. Kim",
                        "W.J. Dally",
                        "S. Scott",
                        "D. Abts"
                    ],
                    "venue": "2008 International Symposium on Computer Architecture,",
                    "year": 2008
                },
                {
                    "title": "Access and alignment of data in an array processor",
                    "authors": [
                        "D.H. Lawrie"
                    ],
                    "venue": "IEEE Trans. Computers, C-24(12):1145\u2013 1155, Dec",
                    "year": 1975
                },
                {
                    "title": "F10: A fault-tolerant engineered network",
                    "authors": [
                        "V. Liu",
                        "D. Halperin",
                        "A. Krishnamurthy",
                        "T. Anderson"
                    ],
                    "venue": "Proc. USENIX NSDI,",
                    "year": 2013
                },
                {
                    "title": "Rotornet: A scalable, low-complexity, optical datacenter network",
                    "authors": [
                        "W.M. Mellette",
                        "R. McGuinness",
                        "A. Roy",
                        "A. Forencich",
                        "G. Papen",
                        "A.C. Snoeren",
                        "G. Porter"
                    ],
                    "venue": "Proc. ACM SIGCOMM,",
                    "year": 2017
                },
                {
                    "title": "What are Patch Panels & When to Use Them? https://www.lonestarracks.com/news/2016/10/ 28/patch-panels",
                    "authors": [
                        "J. Mitchell"
                    ],
                    "venue": "USENIX Symposium on Networked Systems Design and Implementation",
                    "year": 2016
                },
                {
                    "title": "Taming the flying cable monster: A topology design and optimization framework for data-center networks",
                    "authors": [
                        "J. Mudigonda",
                        "P. Yalagandula",
                        "J.C. Mogul"
                    ],
                    "venue": "Proc. USENIX ATC,",
                    "year": 2011
                },
                {
                    "title": "A cost comparison of datacenter network architectures",
                    "authors": [
                        "L. Popa",
                        "S. Ratnasamy",
                        "G. Iannaccone",
                        "A. Krishnamurthy",
                        "I. Stoica"
                    ],
                    "venue": "Proceedings of the 6th International COnference, Co-NEXT \u201910,",
                    "year": 2010
                },
                {
                    "title": "Condor: Better topologies through declarative design",
                    "authors": [
                        "B. Schlinker",
                        "R.N. Mysore",
                        "S. Smith",
                        "J.C. Mogul",
                        "A. Vahdat",
                        "M. Yu",
                        "E. Katz-Bassett",
                        "M. Rubin"
                    ],
                    "venue": "Proc. USENIX NSDI,",
                    "year": 2015
                },
                {
                    "title": "Jupiter rising: A decade of clos topologies and centralized control in google\u2019s datacenter network",
                    "authors": [
                        "A. Singh",
                        "J. Ong",
                        "A. Agarwal",
                        "G. Anderson",
                        "A. Armistead",
                        "R. Bannon",
                        "S. Boving",
                        "G. Desai",
                        "B. Felderman",
                        "P. Germano",
                        "A. Kanagala",
                        "J. Provost",
                        "J. Simmons",
                        "E. Tanda",
                        "J. Wanderer",
                        "U. H\u00f6lzle",
                        "S. Stuart",
                        "A. Vahdat"
                    ],
                    "venue": "Proc. ACM SIGCOMM,",
                    "year": 2015
                },
                {
                    "title": "Jellyfish: Networking data centers randomly",
                    "authors": [
                        "A. Singla",
                        "C.-Y. Hong",
                        "L. Popa",
                        "P.B. Godfrey"
                    ],
                    "venue": "Proc. USENIX NSDI,",
                    "year": 2012
                },
                {
                    "title": "Design space exploration of the dragonfly topology",
                    "authors": [
                        "M.Y. Teh",
                        "J.J. Wilke",
                        "K. Bergman",
                        "S. Rumley"
                    ],
                    "venue": "ISC Workshops,",
                    "year": 2017
                },
                {
                    "title": "Xpander: Towards optimal-performance datacenters",
                    "authors": [
                        "A. Valadarsky",
                        "G. Shahaf",
                        "M. Dinitz",
                        "M. Schapira"
                    ],
                    "venue": "Proc. ACM CoNEXT,",
                    "year": 2016
                },
                {
                    "title": "Computer Architecture: Single and Parallel Systems",
                    "authors": [
                        "M.R. Zargham"
                    ],
                    "venue": "Prentice Hall,",
                    "year": 1996
                },
                {
                    "title": "Designing a predictable internet backbone with valiant load-balancing",
                    "authors": [
                        "R. Zhang-Shen",
                        "N. McKeown"
                    ],
                    "venue": "Proc. IEEE IWQoS,",
                    "year": 2005
                },
                {
                    "title": "Minimal rewiring: Efficient live expansion for clos data center networks",
                    "authors": [
                        "S. Zhao",
                        "R. Wang",
                        "J. Zhou",
                        "J. Ong",
                        "J. Mogul",
                        "A. Vahdat"
                    ],
                    "venue": "Proc. USENIX NSDI,",
                    "year": 2019
                },
                {
                    "title": "Mirror mirror on the ceiling: Flexible wireless links for data centers",
                    "authors": [
                        "X. Zhou",
                        "Z. Zhang",
                        "Y. Zhu",
                        "Y. Li",
                        "S. Kumar",
                        "A. Vahdat",
                        "B.Y. Zhao",
                        "H. Zheng"
                    ],
                    "venue": "Proc. ACM SIGCOMM,",
                    "year": 2012
                },
                {
                    "title": "Understanding and mitigating packet corruption in data center networks",
                    "authors": [
                        "D. Zhuo",
                        "M. Ghobadi",
                        "R. Mahajan",
                        "K.-T. F\u00f6rster",
                        "A. Krishnamurthy",
                        "T. Anderson"
                    ],
                    "venue": "Proc. ACM SIGCOMM,",
                    "year": 2017
                }
            ],
            "id": "SP:e362c0802fd32dd21736d8e79941239022cacdc4",
            "authors": [
                {
                    "name": "Mingyang Zhang",
                    "affiliations": []
                },
                {
                    "name": "Radhika Niranjan Mysore",
                    "affiliations": []
                },
                {
                    "name": "Ramesh Govindan",
                    "affiliations": []
                }
            ],
            "abstractText": "Most recent datacenter topology designs have focused on performance properties such as latency and throughput. In this paper, we explore a new dimension, life cycle management complexity, which attempts to understand the complexity of deploying a topology and expanding it. By analyzing current practice in lifecycle management, we devise complexity metrics for lifecycle management, and show that existing topology classes have low lifecycle management complexity by some measures, but not by others. Motivated by this, we design a new class of topologies, FatClique, that, while being performance-equivalent to existing topologies, is comparable to, or better than them by all our lifecycle management complexity metrics.",
            "title": "Understanding Lifecycle Management Complexity of Datacenter Topologies"
        },
        "Y": {
            "blog_id": "understanding-lifecycle-management-complexity-of-datacenter-topologies",
            "summary": [
                "Understanding lifecycle management complexity of datacenter topologies Zhang et al., NSDI\u201919  There has been plenty of interesting research on network topologies for datacenters, with Clos-like tree topologies and Expander based graph topologies both shown to scale using widely deployed hardware.",
                "This research tends to focus on performance properties such as throughput and latency, together with resilience to failures.",
                "Important as these are, note that they\u2019re also what\u2019s right in front of you as a designer, and relatively easy to measure.",
                "The great thing about today\u2019s paper is that the authors look beneath the surface to consider the less visible but still very important \u201clifecycle management\u201d implications of topology design.",
                "In networking, this translates into how easy it is to physically deploy the network, and how easy it to subsequently expand.",
                "They find a way to quantify the associated lifecycle management costs, and then use this to help drive the design of a new class of topologies, called FatClique.",
                "\u2026 we show that existing topology classes have low lifecycle management complexity by some measures, but not by others.",
                "Motivated by this, we design a new class of topologies, FatClique, that, while being performance-equivalent to existing topologies, is comparable to, or better than them by all our lifecycle management complexity metrics.",
                "Now, there\u2019s probably only a relatively small subset of The Morning Paper readers involved in designing and deploying datacenter network topologies.",
                "So my challenge to you as you read through this paper, is to think about where the hidden complexity and costs are in your own systems.",
                "Would you do things differently if these were made more visible?",
                "It would be great to see more emphasis for example on things like developer experience (DX) and operational simplicity \u2013 in my experience these kinds of attributes can have an outsize impact on the long-term success of a system.",
                "Anyway, let\u2019s get back to cables and switches\u2026  Physically deploying network topologies  When it comes to laying out a network topology for real in a datacenter, you need to think about packaging, placement, and bundling.",
                "Packaging is how you group things together, e.g. the arrangement of switches in racks, and placement concerns how these racks are physically placed on the datacenter floor.",
                "Placement in turn determines the kinds of cabling you need, and for optical cables the power of the transceivers.",
                "Within a rack we might package several connected switches into a single chassis using a backplane.",
                "At the other end of the scale, blocks are larger units of co-placement and packaging that combine several racks.",
                "With all those connections, it makes things a lot easier to group together multiple fibres all connecting the same two endpoints (racks) into bundles, which contain a fixed number of identical length fibres.",
                "Manufacturing bundles is simpler than manufacturing individual fibres, and handling such bundles significantly simplifies operational complexity.",
                "Patch panels make bundling easier by providing a convenient aggregation point to create and route bundles.",
                "Bundles and fibres are physically routed through the datacenter on cable trays.",
                "The trays themselves have capacity constraints of course.",
                "Here\u2019s an example of a logical Clos topology and its physical instantiation:  The authors identify three key metrics that together capture much of the deployment complexity in a topology:  The number of switches.",
                "More switches equals more packaging complexity.",
                "The number of patch panels, which is a function of topological structure and a good proxy for wiring complexity.",
                "The number of bundle types.",
                "This metric captures the other important part of wiring complexity \u2013 how many distinct bundle types are needed.",
                "A bundle type is represented by its capacity (how how many fibres) and its length.",
                "These complexity measures are complete.",
                "The number of cable trays, the design of the chassis, and the number of racks can be derived from the number of switches (and the number of servers and the datacenter floor dimensions, which are inputs to the topology design).",
                "The number of cables and transceivers can be derived from the number of patch panels.",
                "Here\u2019s how Clos and Expander (Jellyfish) representative topologies for the same number of servers stack up against these metrics:  The expander graph topology shows much higher deployment complexity in terms of the number of bundle types.",
                "Clos also exposes far fewer ports outside of a rack (it has better port hiding).",
                "Expanding existing networks  When you want to expand an existing network first you need to buy all the new gear and lay it out on the datacenter floor, and then you can begin a re-wiring process.",
                "This is all going on with live traffic flowing, so expansion is carried out in steps.",
                "During each step the capacity of the topology is guaranteed to be at least some percentage of the existing topology capacity.",
                "The percentage is sometimes known as the expansion SLO.",
                "During a step existing links to be re-wired are drained, then human operators physical rewire links at patch panels.",
                "The new links are tested and then undrained (strange word!",
                "), i.e., brought into service.",
                "For example, here\u2019s a logical expansion (top row) and its physical realisation:  The most time-consuming part of all this is the physical rewiring.",
                "The two metrics that capture expansion complexity are therefore:  The number of expansion steps, and  The average number of rewired links in a patch panel rack.",
                "Here\u2019s how Clos and Expander stack up on those metrics for the same networks we saw earlier:  This time the victory goes to Expander (Jellyfish).",
                "Jellyfish has a much higher north-to-south capacity ratio.",
                "Northbound links exit a block, and southbound links are to/from servers within a block.",
                "\u201cFat edges\u201d have more northbound than southbound links, and the extra capacity means you can accomplish more movement in each step.",
                "Clos topologies re-wire more links in each patch panel during an expansion step and require many steps because they have a low north-south capacity ratio.",
                "Enter the FatClique  Inspired by these insights, the authors define a new class of topologies called FatClique, which combine the hierarchical structure of Clos with the edge expansion capabilities of expander graphs.",
                "There are three levels in the hierarchy.",
                "A clique of switches form a sub-block.",
                "Cliques of sub-blocks come together to form blocks.",
                "And cliques of blocks come together to from the full FatClique topology.",
                "Four key design variables determine the particular instantiation of a FatClique topology: the number of ports in a switch that connect to other servers; the number of ports in a switch that connect to other sub-blocks in a block; the number of switches in a sub-block; and the number of sub-blocks in a block.",
                "A synthesis algorithm  takes a set of six input constraints (see \u00a75.1) and determines the values for these four design variables.",
                "There is plenty more detail in section 5 of the paper which I don\u2019t have the space to do justice too here.",
                "FatClique vs Clos vs Expander  The evaluation compares FatClique to Clos, Xpander, and Jellyfish at different network topology sizes, as shown in the table below.",
                "( Enlarge )  Here\u2019s how they stack up against the complexity metrics:  Number of switches  Number of patch panels  Number of bundle types  and associated cabling costs:  Number of expansion steps  Average number of rewired links  We find that FatClique is the best at most scales by all our complexity metrics.",
                "(The one exception is that at small and medium scales, Clos has slightly fewer patch panels).",
                "It uses 50% fewer switches and 33% fewer patch panels than Clos at large scale, and has a 23% lower cabling cost (an estimate we were able to derive from published cable prices).",
                "Finally, FatClique can permit fast expansion while degrading network capacity by small amounts (2.5-10%): at these levels, Clos can take 5x longer to expand the topology, and each step of Clos expansion can take longer than FatClique because the number of links to be rewired at each step per patch panel can be 30-50% higher.",
                "The one thing I couldn\u2019t find in the evaluation is any data to back up the opening claim that FatClique achieves all of this \u201cwhile being performance-equivalent to existing topologies.\u201d  The last word  As the management complexity of networks increases, the importance of designing for manageability will increase in the coming years.",
                "Our paper is only a first step in this direction\u2026"
            ],
            "author_id": "ACOLYER",
            "pdf_url": "https://www.usenix.org/system/files/nsdi19-zhang.pdf",
            "author_full_name": "Adrian Colyer",
            "source_website": "https://blog.acolyer.org/about/",
            "id": 99384474
        }
    },
    "37479343": {
        "X": {
            "id": "empty"
        },
        "Y": {
            "blog_id": "time-adaptive-sketches-ada-sketches-for-summarizing-data-streams",
            "summary": [
                "Time-adaptive sketches (Ada Sketches) for Summarizing Data Streams Shrivastava et al. SIGMOD 2016  More algorithm fun today, and again in the context of data streams.",
                "It\u2019s the 3 V\u2019s of big data, but not as you know it: Volume, Velocity, and Var\u2026 Volatility.",
                "Volatility here refers to changing patterns in the data over time, and that can make life awkward if you\u2019re trying to extract information from a stream.",
                "In particular, the authors study the heavy hitters problem, but with a twist: we want to give more weight to recent trends.",
                "In most applications that involve temporal data, most recent trends tend to be most informative for predictive purposes\u2026.",
                "For instance, most recent variations in credit history are much stronger indicators of a person\u2019s ability to make loan payments compared to variations in credit history from the distant past.",
                "Time-adaptive sketches generalize sketching algorithms and have the property that they retain counts of heavy hitters with good accuracy, while also providing provable time-adaptive guarantees.",
                "Coming in at 16 pages, the essence of the paper, especially if you\u2019re familiar with count-min sketches is this: instead of increasing counters by 1 every time you see an item, increase them by f(t), where f(t) is a monotone function in time.",
                "When you want to extract count estimates for time t, divide by f(t).",
                "The authors experiment with a linear function f(t) = at, for fixed a (0.5), and also an exponential function f(t) = at for fixed a (1.0015).",
                "Both gave good results.",
                "Finishing the write-up here though would be to short-change you.",
                "We\u2019re interested in why this works, and what guarantees it gives.",
                "Plus the paper also gives an excellent tour through some of the prior approaches to solving the heavy hitters problem.",
                "Let\u2019s start there, with a very quick recap on the basic Count-Min Sketch (CMS) algorithm.",
                "Count-Min Sketch  Create an integer array initialised to zeros that is w wide and d deep.",
                "Take d pairwise independent hash functions, h1,\u2026,hd and associate one with each row of the table, these functions should produce a value in the range 1..w. When a new value is seen, for each row of the table, hash the value with the corresponding hash function, and increment the counter in the indicated array slot.",
                "If you want to know the estimate of how many instances of a given value have been seen, hash the value as previously and look up the counter values that gives you in each row.",
                "Take the smallest of these as your estimate.",
                "Hokusai \u2013 nearly but not quite  Hokusai-sketching (Matusevych et al. 2012) introduced an item aggregation algorithm for constructing time-adaptive sketches.",
                "Hokusai uses a set of Count-Min sketches for different time intervals, to estimate the counts of any item for a given time or interval.",
                "To adapt the error rate temporally in limited space, the algorithm uses larger sketches for recent intervals and sketches of smaller size for older intervals.",
                "At the end of a time interval (e.g T), a sketch needs to be moved into the next-sized-down sketch, (the one for T\u20131).",
                "Hokusai has a very elegant way of doing this: at each rung on the ladder, sketch widths are halved.",
                "You can therefore compress a larger sketch into a smaller one by simply adding one half of the sketch to the other, and also halving the hash function ranges using modulo 2 operations.",
                "Although this idea of having different-size sketches for different time intervals is reasonable and yields accuracies that are time-adaptive, it comes with several inherent shortingcomings.",
                "Inspiration \u2013 Dolby noise reduction!",
                "This might date some of The Morning Paper readers \u2013 do you remember Dolby B noise reduction?",
                "And then the exciting introduction of Dolby C?",
                "Some of us grew up with music on cassette tapes, and Dolby Noise Reduction was ever present.",
                "When recording, Dolby systems employ pre-emphasis \u2013 artificially boosting certain parts of the input signal.",
                "On playback, the reverse de-emphasis translation restores the original signal levels.",
                "This process helps to improve the signal-to-noise ratio and combat tape hiss.",
                "We exploit the fact that Count-Min Sketch (CMS)\u2026 has better accuracy for heavy-hitters as compared to the rest of the items.",
                "While updating the sketch we apply pre-emphasis and artificially inflate the counts of more recent items compared to older ones, i.e., we make them heavier with respect to the older items.",
                "This is done by multiplying updates cit with f(t), which is any monotonically increasing function of time t. Thus, instead of updating the sketch with cit we update the sketch with _f(t) x cit.",
                "The tendency of the sketch is to preserve large values.",
                "This inflation thus preserves the accuracy of recent items, after artificial inflation, compared to the older ones.",
                "On querying of course, the de-emphasis process must be applied, which means dividing the results by f(t) to obtain the estimate of item i at time t. In the absence of collisions, as with the base CMS, counts are estimated exactly.",
                "Consider a CMS with only one row, and the case when two independent items i and j collide.",
                "We see cit instances of i, and cjt\u2019 instances of j.",
                "With plain CMS, we would over-estimate the count for i by cjt\u2019, whereas with the pre-emphasis process we overestimate by (f(t) x cjt)/f(t\u2019)).",
                "Therefore it is easy to see that more recent items suffer less compared to older items.",
                "Adaptive CMS  The Adaptive Count-Min Sketch algorithm (Ada-CMS), is just CMS but with the update and query mechanisms adapted to use the pre-emphasis and de-emphasis mechanism just described.",
                "Note that when f(t) = 1 we obtain the original CMS algorithm.",
                "By choosing appropriate f(t) functions, we can tailor the behaviour for different situations.",
                "One major question we are interested in is \"Given a fixed space and current state of time T, what are the values of time t \u2264 T where Ada-CMS is more accurate than vanilla CMS?",
                "For a given w and d, we can see as a start that the expected error of Ada-CMS will be less than CMS if:  For t=T this will always be true (due to the monotonicity requirement on f(t)).",
                "The upper bound on the error with vanilla CMS is &sqrt;T, so Ada-CMS wins when its error is less than this.",
                "To illustrate a reasonable scenario, suppose we want the errors with Ada-CMS to be never off by a factor \u03b3 away from that of vanilla CMS \u2200 t. This ensures that we guarantee accuracy within a factor \u03b3 of what the original CMS would achieve to even very old heavy hitters.",
                "In addition, we want to be more accurate than CMS on all recent time t > K, for some desirable choice of K.  With a couple of simple manoeuvres (see section 5.2), this turns into solving the following pair of simultaneous equations:  Other applications  The pre-emphasis and de-emphasis technique can be used in a number of other scenarios.",
                "The authors show an example with the Lossy Counting algorithm, and also how it can be applied to range queries (see \u00a76).",
                "Evaluation  Experimental evaluation is undertaken with two real-world streaming datasets from AOL (36M search queries with 3.8M unique terms) and Criteo (150K unique categorical terms).",
                "Comparison is undertaken between vanilla CMS, the Hokusai algorithm, Ada-CMS with a linear function (f(t) = 0.5t), and Ada-CMS with an exponential function (f(t)=1.0015t).",
                "In all cases d = 4, and w was varied from 210 to 223 to see the impact of varying range sizes.",
                "Here are the results for the AOL dataset:  Here\u2019s the standard deviation of those errors with w=218:  The Last Word  The proposed integration of sketches with pre-emphasis and de-emphasis, as we demonstrate, posseses strong theoretical guarantees on errors over time.",
                "Experiments on real datasets support our theoretical findings and show significantly superior accuracy and runtime overhead compared to the recently proposed Hokusai algorithm.",
                "We hope that our proposal will be adopted in practice, and it will lead to further exploration of the pre-emphasis and de-emphasis idea for solving massive data stream problems."
            ],
            "author_id": "ACOLYER",
            "pdf_url": "http://research.microsoft.com/en-us/um/people/mbilenko/papers/16-ada-sketches.pdf",
            "author_full_name": "Adrian Colyer",
            "source_website": "https://blog.acolyer.org/about/",
            "id": 37479343
        }
    },
    "45795840": {
        "X": {
            "sections": [
                {
                    "text": "We develop several novel techniques to maximize the throughput and hide the latency of the PCIe connection between the NIC and the host memory, which becomes the new bottleneck. Combined, these mechanisms allow a single NIC KV-Direct to achieve up to 180 M key-value operations per second, equivalent to the throughput of tens of CPU cores. Compared with CPU based KVS implementation, KV-Direct improves power efficiency by 3x, while keeping tail latency below 10 \u00b5s. Moreover, KV-Direct can achieve near linear scalability with multiple NICs. With 10 programmable NIC cards in a commodity server, we achieve 1.22 billion KV operations per second, which is almost an order-of-magnitude improvement over existing systems, setting a new milestone for a general-purpose in-memory key-value store.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SOSP \u201917, Shanghai, China \u00a9 2017 ACM. 978-1-4503-5085-3/17/10. . . $15.00 DOI: 10.1145/3132747.3132756\nCCS CONCEPTS \u2022Information systems\u2192 Key-value stores; \u2022Hardware\u2192 Hardware-software codesign;\nKEYWORDS Key-Value Store, Programmable Hardware, Performance\nACM Reference format: Bojie Li*\u00a7\u2020 Zhenyuan Ruan[1]\u2021\u2020 Wencong Xiao\u2022\u2020 Yuanwei Lu\u00a7\u2020 and Yongqiang Xiong\u2020 Andrew Putnam\u2020 Enhong Chen\u00a7 Lintao Zhang\u2020 . 2017. KV-Direct: High-Performance In-Memory Key-Value Store with Programmable NIC. In Proceedings of SOSP\n\u201917, Shanghai, China, October 28, 2017, 16 pages. DOI: 10.1145/3132747.3132756"
                },
                {
                    "heading": "1 INTRODUCTION",
                    "text": "In-memory key-value store (KVS) is a key distributed system component in many data centers. KVS enables access to a shared key-value hash table among distributed clients. Historically, KVS such as Memcached [25] gained popularity as an object caching system for web services. Large web service providers such as Amazon [17] and Facebook [3, 57], have deployed distributed key-value stores at scale. More recently, as main-memory based computing becomes a major trend in the data centers [18, 58], KVS starts to go beyond caching and becomes an infrastructure to store shared data structure in a distributed system. Many data structures can be expressed in a key-value hash table, e.g., data indexes in NoSQL databases [12], model parameters in machine learning [46], nodes and edges in graph computing [67, 74] and sequencers in distributed synchronization [37]. For most of these applications, the performance of the KVS is the key factor that directly determines the system efficiency. Due to its importance, over the years significant amount of research effort has been invested on improving KVS performance.\nEarlier key-value systems [17, 25, 57] are built on top of traditional OS abstractions such as OS lock and TCP/IP stack. This puts considerable stress on the performance of\n*Bojie Li and Zhenyuan Ruan are co-first authors who finish this work during internship at Microsoft Research.\nthe OS, especially the networking stack. The bottleneck is exacerbated by the fact that physical network transport speed has seen huge improvements in the last decade due to heavy bandwidth demand from data center applications.\nMore recently, as both the single core frequency scaling and multi-core architecture scaling are slowing down [21, 69], a new research trend in distributed systems is to leverage Remote Direct Memory Access (RDMA) technology on NIC to reduce network processing cost. One line of research [36, 37] uses two-sided RDMA to accelerate communication (Figure 1a). KVS built with this approach are bounded by CPU performance of the KVS servers. Another line of research uses one-sided RDMA to bypass remote CPU and shift KV processing workload to clients [18, 55] (Figure 1b). This approach achieves better GET performance but degrades performance for PUT operations due to high communication and synchronization overhead. Due to lack of transactional support, the abstraction provided by RDMA is not a perfect fit for building efficient KVS.\nIn the meantime, another trend is emerging in data center hardware evolution. More and more servers in data centers are now equipped with programmable NICs [10, 27, 64]. At the heart of a programmable NIC is a field-programmable gate array (FPGA) with an embedded NIC chip to connect to the network and a PCIe connector to attach to the server. Programmable NIC is initially designed to enable network virtualization [24, 44]. However, many found that FPGA resources can be used to offload some workloads of CPU and significantly reduce CPU resource usage [14, 30, 52, 60]. Our work takes this general approach.\nWe present KV-Direct, a new in-memory key-value system that takes advantage of programmable NIC in data center. KVDirect, as its name implies, directly fetches data and applies updates in the host memory to serve KV requests, bypassing host CPU (Figure 1c). KV-Direct extends the RDMA primitives from memory operations (READ and WRITE) to key-value operations (GET, PUT, DELETE and ATOMIC ops). Compared with one-sided RDMA based systems, KVDirect deals with the consistency and synchronization issues at server-side, thus removes computation overhead in client\nand reduces network traffic. In addition, to support vectorbased operations and reduce network traffic, KV-Direct also provides new vector primitives UPDATE, REDUCE, and FILTER, allowing users to define active messages [19] and delegate certain computation to programmable NIC for efficiency.\nSince the key-value operations are offloaded to the programmable NIC, we focus our design on optimizing the PCIe traffic between the NIC and host memory. KV-Direct adopts a series of optimizations to fully utilize PCIe bandwidth and hide latency. Firstly, we design a new hash table and memory allocator to leverage parallelism available in FPGA and minimize the number of PCIe DMA requests. On average, KV-Direct achieves close to one PCIe DMA per READ operation and two PCIe DMAs per WRITE operation. Secondly, to guarantee consistency among dependent KV operations, KV-Direct includes an out-of-order execution engine to track operation dependencies while maximizing the throughput of independent requests. Thirdly, KV-Direct exploits on-board DRAM buffer available on programmable NIC by implementing a hardware-based load dispatcher and caching component in FPGA to fully utilize on-board DRAM bandwidth and capacity.\nA single NIC KV-Direct is able to achieve up to 180 M KV operations per second (Ops), equivalent to the throughput of 36 CPU cores [47]. Compared with state-of-art CPU KVS implementations, KV-Direct reduces tail latency to as low as 10 \u00b5s while achieving a 3x improvement on power efficiency. Moreover, KV-Direct can achieve near linear scalability with multiple NICs. With 10 programmable NIC cards in a server, we achieve 1.22 billion KV operations per second in a single commodity server, which is more than an order of magnitude improvement over existing systems.\nKV-Direct supports general atomic operations up to 180 Mops, equal to normal KV operation and significantly outperforms the number reported in state-of-art RDMA-based system: 2.24 Mops [36]. The atomic operation agnostic performance is mainly a result of our out-of-order execution engine that can efficiently track the dependency among KV operations without explicitly stalling the pipeline."
                },
                {
                    "heading": "2 BACKGROUND",
                    "text": ""
                },
                {
                    "heading": "2.1 Workload Shift in KVS",
                    "text": "Historically, KVS such as Memcached [25] gained popularity as an object caching system for web services. In the era of in-memory computation, KVS goes beyond caching and becomes an infrastructure service to store shared data structure in a distributed system. Many data structures can be expressed in a key-value hash table, e.g., data indexes in NoSQL databases [12], model parameters in machine learning [46], nodes and edges in graph computing [67, 74] and sequencers in distributed synchronization [37, 45].\nThe workload shifts from object cache to generic data structure store implies several design goals for KVS.\nHigh batch throughput for small KV. In-memory computations typically access small key-value pairs in large batches, e.g., sparse parameters in linear regression [48, 74] or all neighbor nodes in graph traversal [67], therefore a KVS should be able to benefit from batching and pipelining.\nPredictable low latency. For many data-parallel computation tasks, the latency of an iteration is determined by the slowest operations [59]. Therefore, it is important to control the tail latency of KVS. CPU based implementations often have large fluctuations under heavy load due to scheduling irregularities and inflated buffering.\nHigh efficiency under write-intensive workload. For cache workloads, KVS often has much more reads than writes [3], but it is no longer the case for distributed computation workloads such as graph computation [61], parameter servers [46]. These workloads favor hash table structures that can handle both read and write operations efficiently.\nFast atomic operations. Atomic operations on several extremely popular keys appear in applications such as centralized schedulers [63], sequencers [37, 45], counters [76] and short-term values in web applications [3]. This requires high throughput on single-key atomics.\nSupport vector-type operations. Machine learning and graph computing workloads [46, 67, 74] often require operating on every element in a vector, e.g., incrementing every element in a vector with a scalar or reducing a vector into the sum of its elements. KVS without vector support requires the client to either issue one KVS operation per element, or retrieve the vector back to the client and perform the operation. Supporting vector data type and operations in KVS can greatly reduce network communication and CPU computation overhead."
                },
                {
                    "heading": "2.2 Achilles\u2019 Heel of High-Performance KVS Systems",
                    "text": "Building a high performance KVS is a non-trivial exercise of optimizing various software and hardware components in a computer system. Characterized by where the KV processing\ntakes place, state-of-the-art high-performance KVS systems basically falls into three categories: on the CPU of KVS server (Figure 1a), on KVS clients (Figure 1b) or on a hardware accelerator (Figure 1c).\nWhen pushed to the limit, in high performance KVS systems the throughput bottleneck can be attributed to the computation in KV operation and the latency in random memory access. CPU-based KVS needs to spend CPU cycles for key comparison and hash slot computation. Moreover, KVS hash table is orders of magnitude larger than the CPU cache, therefore the memory access latency is dominated by cache miss latency for practical access patterns.\nBy our measurement, a 64-byte random read latency for a contemporary computer is \u223c110 ns. A CPU core can issue several memory access instructions concurrently when they fall in the instruction window, limited by the number of loadstore units in a core (measured to be 3\u223c4 in our CPU) [26, 28, 75]. In our CPU, we measure a max throughput of 29.3 M random 64B access per second per core. On the other hand, an operation to access 64-byte KV pair typically requires \u223c100 ns computation or \u223c500 instructions, which is too large to fit in the instruction window (measured to be 100\u223c200). When interleaved with computation, the performance of a CPU core degrades to only 5.5 M KV operations per second (Mops). An optimization is to batch memory accesses in a KV store by clustering the computation for several operations together before issuing the memory access all at once [47, 56]. This improves the per-core throughput to 7.9 MOps in our CPU, which is still far less than the random 64B throughput of host DRAM.\nObserving the limited capacity of CPU in KV processing, recent work [18, 55, 70] leverage one-sided RDMA to offload KV processing to clients and effectively using the KVS server as a shared memory pool. Despite the high message rate (8\u223c150 Mops [37]) provided by RDMA NICs, it is challenging to find an efficient match between RDMA primitives and key-value operations. For a write (PUT or atomic) operation, multiple network round-trips and multiple memory accesses may be required to query the hash index, handle hash collisions and allocate variable-size memory. RDMA does not support transactions. Clients must synchronize with each other to ensure consistency using RDMA atomics or distributed atomic broadcast [70], both incurring communication overhead and synchronization latency [18, 55]. Therefore, most RDMA-based KVS [18, 36, 55] recommend using onesided RDMA for GET operations only. For PUT operations, they fall back to the server CPU. The throughput of writeintensive workload is still bottlenecked by CPU cores."
                },
                {
                    "heading": "2.3 Programmable NIC with FPGA",
                    "text": "Ten years ago, processor frequency scaling slowed down and people turned to multi-core and concurrency [69]. Nowadays,\npower ceiling implies that multi-core scaling has also met difficulties [22]. People are now turning to domain-specific architectures (DSAs) for better performance.\nDue to the increasing mismatch of network speed and CPU network processing capacity, programmable NICs with FPGA [10, 24, 27, 44] now witness large-scale deployment in datacenters. As shown in Figure 2, the heart of the programmable NIC we use is an FPGA, with an embedded NIC chip to connect to the network. Programmable NICs typically come with on-board DRAM as packet buffers and runtime memory for NIC firmware [44], but the DRAM is typically not large enough to hold the entire key-value store."
                },
                {
                    "heading": "2.4 Challenges for Remote Direct Key-Value Access",
                    "text": "KV-Direct moves KV processing from the CPU to the programmable NIC in the server (Figure 1c). Same as RDMA, the KV-Direct NIC accesses host memory via PCIe. PCIe is a packet switched network with \u223c500 ns round-trip latency and 7.87 GB/s theoretical bandwidth per Gen3 x8 endpoint. On the latency side, for our programmable NIC, the cached PCIe DMA read latency is 800 ns due to additional processing delay in FPGA. For random non-cached DMA read, there is an additional 250 ns average latency (Figure 3b) due to DRAM access, DRAM refresh and PCIe response reordering in PCIe DMA engine. On the throughput side, each DMA read or write operation needs a PCIe transport-layer packet (TLP) with 26-byte header and padding for 64-bit addressing. For a PCIe Gen3 x8 NIC to access host memory in 64-byte granularity, the theoretical throughput is therefore 5.6 GB/s, or 87 Mops.\nTo saturate PCIe Gen3 x8 with 64-byte DMA requests, 92 concurrent DMA requests are needed considering our latency of 1050 ns. In practice, two factors further limit the concurrency of DMA requests. First, PCIe credit-based flow control constrains the number of in-flight requests for each DMA type. The PCIe root complex in our server advertises 88 TLP posted header credits for DMA write and 84 TLP non-posted header credits for DMA read. Second, DMA read requires assigning a unique PCIe tag to identify DMA responses which may come out of order. The DMA engine in our FPGA only support 64 PCIe tags, further limiting our DMA read concurrency to 64 requests, which renders a throughput of 60 Mops as shown in Figure 3a. On the other hand, with 40 Gbps network and 64-byte KV pairs, the throughput ceiling is 78 Mops with client-side batching. In order to saturate the network with GET operations, the KVS on NIC must make full use of PCIe bandwidth and achieve close to one average memory access per GET. This boils down to three challenges:\nMinimize DMA requests per KV operation. Hash table and memory allocation are two major components in KVS that require random memory access. Previous works propose hash tables [9, 18] with close to 1 memory access per GET operation even under high load factors. However, under higher than 50% load factor, these tables need multiple memory accesses per PUT operation on average with large variance. This not only consumes PCIe throughput, but also leads to latency variations for write-intensive workloads.\nIn addition to hash table lookup, dynamic memory allocation is required to store variable-length KVs that cannot be inlined in the hash table. Minimizing hash table lookups per KV operation and memory accesses per memory allocation is essential for matching PCIe and network throughput under write-intensive, small-KV workloads.\nHide PCIe latency while maintaining consistency. An efficient KVS on NIC must pipeline KV operations and DMA requests to hide the PCIe latency. However, KV operations may have dependencies. A GET following PUT on a same key needs to return the updated value. This requires tracking KV operations being processed and stall the pipeline on data hazard, or better, design an out-of-order executor to resolve data dependency without explicitly stalling the pipeline.\nDispatch load between NIC DRAM and host memory. An obvious idea is to use the DRAM on NIC as a cache for host memory, but in our NIC, the DRAM throughput (12.8 GB/s) is on par with the achievable throughput (13.2 GB/s) of two PCIe Gen3 x8 endpoints. It is more desirable to distribute memory access between DRAM and host memory in order to utilize both of their bandwidths. However, the onboard DRAM is small (4 GiB) compared to the host memory (64 GiB), calling for a hybrid caching and load-dispatching approach.\nIn the following, we will present KV-Direct, a novel FPGAbased key-value store that satisfies all aforementioned goals and describe how we address the challenges."
                },
                {
                    "heading": "3 DESIGN",
                    "text": ""
                },
                {
                    "heading": "3.1 System Architecture",
                    "text": "KV-Direct enables remote direct key-value access. Clients send KV-Direct operations (\u00a73.2) to KVS server while the programmable NIC processes the requests and sending back results, bypassing the CPU. The programmable NIC on KVS server is an FPGA reconfigured as a KV processor (\u00a73.3). Figure 2 shows the architecture of KV-Direct."
                },
                {
                    "heading": "3.2 KV-Direct Operations",
                    "text": "KV-Direct extends one-sided RDMA operations to key-value operations, as summarized in Table 1. In addition to standard KVS operations as shown in the top part of Table 1, KV-Direct supports two types of vector operations: Sending a scalar to the NIC on the server and the NIC applies the update to each element in the vector; or send a vector to the server and the NIC updates the original vector element-by-element. Furthermore, KV-Direct supports user-defined update functions as a generalization to atomic operations. The update function needs to be pre-registered and compiled to hardware logic before executing. KV operations with user-defined update functions are similar to active messages [19], saving communication and synchronization cost.\nWhen a vector operation update, reduce or filter is operated on a key, its value is treated as an array of fixed-bit-width elements. Each function \u03bb operates on one element in the vector, a client-specified parameter \u2206, and/or an initial value \u03a3 for reduction. The KV-Direct development toolchain duplicates the \u03bb several times to leverage parallelism in FPGA and match computation throughput with PCIe throughput, then compiles it into reconfigurable hardware logic using an high-level synthesis (HLS) tool [2]. The HLS tool automatically extracts data dependencies in the duplicated function and generates a fully pipelined programmable logic.\nUpdate operations with user-defined functions are capable of general stream processing on a vector value. For example, a network processing application may interpret the vector as a stream of packets for network functions [44] or a bunch of states for packet transactions [68]. Single-object transaction processing completely in the programmable NIC is also possible, e.g., wrapping around S QUANTITY in TPC-C benchmark [16]. Vector reduce operation supports neighbor weight accumulation in PageRank [61]. Non-zero values in a sparse vector can be fetched with vector filter operation."
                },
                {
                    "heading": "3.3 KV Processor",
                    "text": "As shown in Figure 4, the KV processor in FPGA receives packets from the network, decodes vector operations and buffers KV operations in the reservation station (\u00a73.3.3). Next, the out-of-order engine (\u00a73.3.3) issues independent KV operations from reservation station into the operation decoder. Depending on the operation type, the KV processor looks up the hash table (\u00a73.3.1) and executes the corresponding operations. To minimize the number of memory accesses, small KV pairs are stored inline in the hash table, others are stored in dynamically allocated memory from the slab memory allocator (\u00a73.3.2). Both the hash index and the slaballocated memory are managed by a unified memory access engine (\u00a73.3.4), which accesses the host memory via PCIe DMA and caches a portion of host memory in NIC DRAM. After the KV operation completes, the result is sent back to the out-of-order execution engine (\u00a73.3.3) to find and execute matching KV operations in reservation station.\nAs discussed in \u00a72.4, the scarcity of PCIe operation throughput requires the KV processor to be frugal on DMA accesses. For GET operation, at least one memory read is required. For PUT or DELETE operation, one read and one write are minimal for hash tables. Log-based data structures can achieve one write per PUT, but it sacrifices GET performance. KVDirect carefully designs the hash table to achieve close to ideal DMA accesses per lookup and insertion, as well as the memory allocator to achieve < 0.1 amortized DMA operations per dynamic memory allocation.\n3.3.1 Hash Table. To store variable-sized KVs, the KV storage is partitioned into two parts. The first part is a hash index (Figure 5), which consists a fixed number of hash buckets. Each hash bucket contains several hash slots and some metadata. The rest of the memory is dynamically allocated, and managed by a slab allocator (\u00a73.3.2). A hash index ratio configured at initialization time determines the percentage of the memory allocated for hash index. The choice of hash index ratio will be discussed in \u00a75.1.1.\nEach hash slot includes a pointer to the KV data in dynamically allocated memory and a secondary hash. Secondary hash is an optimization that enables parallel inline checking. The key is always checked to ensure correctness, at the cost of one additional memory access. Assuming a 64 GiB KV storage in host memory and 32-byte allocation granularity (a trade-off between internal fragmentation and allocation metadata overhead), the pointer requires 31 bits. A secondary hash of 9 bits gives a 1/512 false positive probability. Cumulatively, the hash slot size is 5 bytes. To determine the hash bucket size, we need to trade-off between the number of hash slots per bucket and the DMA throughput. Figure 3a shows that the DMA read throughput below 64B granularity is bound by PCIe latency and parallelism in the DMA engine. A bucket size less than 64B is suboptimal due to increased possibility of hash collision. On the other hand, increasing the bucket size above 64B would decrease hash lookup throughput. So we choose the bucket size to be 64 bytes.\nKV size is the combined size of key and value. KVs smaller than a threshold are stored inline in the hash index to save the additional memory access to fetch KV data. An inline KV may span multiple hash slots, whose pointer and secondary\nhash fields are re-purposed for storing KV data. It might not be optimal to inline all KVs that can fit in a bucket. To minimize average access time, assuming that smaller and larger keys are equally likely to be accessed, it is more desirable to inline KVs smaller than an inline threshold. To quantify the portion of used buckets in all buckets, we use memory utilization instead of load factor, because it relates more to the number of KVs that can fit in a fixed amount of memory. As shown in Figure 6, for a certain inline threshold, the average memory access count increases with memory utilization, due to more hash collisions. Higher inline threshold shows a more steep growth curve of memory access count, so an optimal inline threshold can be found to minimize memory accesses under a given memory utilization. As with hash index ratio, the inline threshold can also be configured at initialization time.\nWhen all slots in a bucket are filled up, there are several solutions to resolve hash collisions. Cuckoo hashing [62] and hopscotch hashing [29] guarantee constant-time lookup by moving occupied slots during insertion. However, in writeintensive workload, the memory access time under high load factor would experience large fluctuations. Linear probing may suffer from primary clustering, therefore its performance is sensitive to the uniformity of hash function. We choose chaining to resolve hash conflicts, which balances lookup and insertion, while being more robust to hash clustering.\n3.3.2 Slab Memory Allocator. Chained hash slots and non-inline KVs need dynamic memory allocation. We choose slab memory allocator [7] to achieve O(1) average memory access per allocation and deallocation. The main slab allocator logic runs on host CPU and communicates with the KV-processor through PCIe. Slab allocator rounds up allocation size to the nearest power of two, called slab size. It maintains a free slab pool for each possible slab size (32, 64, . . . , 512 bytes), and a global allocation bitmap to help to merge small free slabs back to larger slabs. Each free slab pool is an array of slab entries consisting of an address field and a slab type field indicating the size of the slab entry. The free slab pool can be cached on the NIC. The cache syncs\nwith the host memory in batches of slab entries. Amortized by batching, less than 0.07 DMA operation is needed per allocation or deallocation. When a small slab pool is almost empty, larger slabs need to be split. Because the slab type is already included in a slab entry, in slab splitting, slab entries are simply copied from the larger pool to the smaller pool, without the need for computation. Including slab type in the slab entry also saves communication cost because one slab entry may contain multiple slots.\nOn deallocation, the slab allocator needs to check whether the freed slab can be merged with its neighbor, requiring at least one read and write to the allocation bitmap. Inspired by garbage collection, we propose lazy slab merging to merge free slabs in batch when a slab pool is almost empty and no larger slab pools have enough slabs to split.\n3.3.3 Out-of-Order Execution Engine. Dependency between two KV operations with the same key in the KV processor will lead to data hazard and pipeline stall. This problem is magnified in single-key atomics where all operations are dependent, thus limiting the atomics throughput. We borrow the concept of dynamic scheduling from computer architecture and implement a reservation station to track all in-flight KV operations and their execution context. To saturate PCIe, DRAM and the processing pipeline, up to 256 in-flight KV operations are needed. However, comparing 256 16-byte keys in parallel would take 40% logic resource of our FPGA. Instead, we store the KV operations in a small hash table in on-chip BRAM, indexed by the hash of the key. To simplify hash collision resolution, we regard KV operations with the same hash as dependent, so there may be false positives, but it will never miss a dependency. Operations with the same hash are organized in a chain and examined sequentially. Hash collision would degrade the efficiency of chain examination, so the reservation station contains 1024 hash slots to make hash collision probability below 25%.\nThe reservation station not only holds pending operations, but also caches their latest values for data forwarding. When a KV operation is completed by the main processing pipeline, its result is returned to the client, and the latest value is forwarded to the reservation station. Pending operations in the same hash slot are checked one by one, and operations with matching key are executed immediately and removed from the reservation station. For atomic operations, the computation is performed in a dedicated execution engine. For write operations, the cached value is updated. The execution result is returned to the client directly. After scanning through the chain of dependent operations, if the cached value is updated, a PUT operation is issued to the main processing pipeline for cache write back. This data forwarding and fast execution path enable single-key atomics to be processed one operation per clock cycle (180 Mops), eliminate head-of-line blocking\nunder workload with popular keys, and ensure consistency because no two operations on the same key can be in the main processing pipeline simultaneously.\n3.3.4 DRAM Load Dispatcher. To further save the burden on PCIe, we dispatch memory accesses between PCIe and the NIC on-board DRAM. Our NIC DRAM has 4 GiB size and 12.8 GB/s throughput, which is an order of magnitude smaller than the KVS storage on host DRAM (64 GiB) and slightly slower than the PCIe link (14 GB/s). One approach is to put a fixed portion of the KVS in NIC DRAM. However, the NIC DRAM is too small to carry a significant portion of memory accesses. The other approach is to use the NIC DRAM as a cache for host memory, the throughput would degrade due to the limited throughput of our NIC DRAM.\nWe adopt a hybrid solution to use the DRAM as a cache for a fixed portion of the KVS in host memory, as shown in Figure 7. The cache-able part is determined by the hash of memory address, in granularity of 64 bytes. The hash function is selected so that a bucket in hash index and a dynamically allocated slab have an equal probability of being cache-able. The portion of cache-able part in entire host memory is called load dispatch ratio (l). Assume the cache hit probability is h(l). To balance load on PCIe and NIC DRAM, the load dispatch ratio l should be optimized so that:\nl tputDRAM = (1 \u2212 l) + l \u00b7 (1 \u2212 h(l)) tputPCIe\nlet k be the ratio of NIC memory size and host memory size. Under uniform workload, cache hit probability h(l) = NIC memory sizecache-able corpus size = k l when k \u2264 l . Caching under uniform workload is not efficient. Under long-tail workload with Zipf distribution, assume n is the total number of KVs, approximately h(l) = log(NIC memory size)log(cache-able corpus size) = log(kn) log(ln) when k \u2264 l . Under long-tail workload, the cache hit probability is as high as 0.7 with 1M cache in 1G corpus. An optimal l can be solved numerically, as discussed in \u00a76.3.1."
                },
                {
                    "heading": "4 IMPLEMENTATION",
                    "text": "Our hardware platform is built on an Intel Stratix V FPGA based programmable NIC (\u00a72.3). The programmable NIC is attached to the server through two PCIe Gen3 x8 links in\na bifurcated x16 physical connector, and contains 4 GiB of on-board DRAM with a single DDR3-1600 channel.\nFor development efficiency, we use Intel FPGA SDK for OpenCL [2] to synthesize hardware logic from OpenCL. Our KV processor is implemented in 11K lines of OpenCL code and all kernels are fully pipelined, i.e., the throughput is one operation per clock cycle. With 180 MHz clock frequency, our design can process KV operations at 180 M op/s if not bottlenecked by network, DRAM or PCIe.\nBelow highlights several implementation details. Slab Memory Allocator. As shown in Figure 8, for each slab size, the slab cache on the NIC is synchronized with host DRAM using two double-ended stacks. For the NICside double-ended stack (left side in Figure 8), the left end is popped and pushed by the allocator and deallocator, and the right end is synchronized with the left end of the corresponding host-side stack via DMA. The NIC monitors the size of NIC stack and synchronizes to or from the host stack according to high and low watermarks. Host daemon periodically checks the size of host-side double-ended stack. If it grows above a high watermark, slab merging is triggered; when it drops below a low watermark, slab splitting is triggered. Because each end of a stack is either accessed by the NIC or the host, and the data is accessed prior to moving pointers, race conditions would not occur.\nDRAM Load Dispatcher. One technical challenge is the storage of metadata in DRAM cache, which requires additional 4 address bits and one dirty flag per 64-byte cache line. Cache valid bit is not needed because all KVS storage is accessed exclusively by the NIC. To store the 5 metadata bits per cache line, extending the cache line to 65 bytes would reduce DRAM performance due to unaligned access; saving the metadata elsewhere will double memory accesses. Instead, we leverage spare bits in ECC DRAM for metadata storage. ECC DRAM typically has 8 ECC bits per 64 bits of data. For Hamming code to correct one bit of error in 64 bits of data, only 7 additional bits are required. The 8th ECC bit is a parity bit for detecting double-bit errors. As we access DRAM in 64-byte granularity and alignment, there are 8 parity bits per 64B data. We increase the parity checking granularity from 64 data bits to 256 data bits, so double-bit errors can still be detected. This allows us to have 6 extra bits which can save our address bits and dirty flag.\nVector Operation Decoder. Compared with PCIe, network is a more scarce resource with lower bandwidth (5 GB/s) and higher latency (2 \u00b5s). An RDMA write packet over Ethernet has 88 bytes of header and padding overhead, while a PCIe TLP packet has only 26 bytes of overhead. This is why previous FPGA-based key-value stores [5, 6] have not saturated the PCIe bandwidth, although their hash table designs are less efficient than KV-Direct. This calls for client-side batching in two aspects: batching multiple KV operations in one packet and supporting vector operations for a more compact representation. Towards this end, we implement a decoder in the KV-engine to unpack multiple KV operations from a single RDMA packet. Observing that many KVs have a same size or repetitive values, the KV format includes two flag bits to allow copying key and value size, or the value of the previous KV in the packet. Fortunately, many significant workloads (e.g. graph traversal, parameter server) can issue KV operations in batches. Looking forward, batching would be unnecessary if higher-bandwidth network is available."
                },
                {
                    "heading": "5 EVALUATION",
                    "text": "In this section, we first take a reductionist perspective to support our design choices with microbenchmarks of key components, then switch to a holistic approach to demonstrate the overall performance of KV-Direct in system benchmark.\nWe evaluate KV-Direct in a testbed of eight servers and one Arista DCS-7060CX-32S switch. Each server equips two 8 core Xeon E5-2650 v2 CPUs with hyper-threading disabled, forming two NUMA nodes connected through QPI Link. Each NUMA node is populated with 8 DIMMs of 8 GiB Samsung DDR3-1333 ECC RAM, resulting a total of 128 GiB of host memory on each server. A programmable NIC [10] is connected to the PCIe root complex of CPU 0, and its 40 Gbps Ethernet port is connected to the switch. The programmable NIC has two PCIe Gen3 x8 links in a bifurcated Gen3 x16 physical connector. The tested server equips SuperMicro X9DRG-QF motherboard and one 120 GB SATA SSD running Archlinux (kernel 4.11.9-1).\nFor system benchmark, we use YCSB workload [15]. For skewed Zipf workload, we choose skewness 0.99 and refer it as long-tail workload."
                },
                {
                    "heading": "5.1 Microbenchmarks",
                    "text": "5.1.1 Hash Table. There are two free parameters in our hash table design: (1) inline threshold, (2) the ratio of hash index in the entire memory space. As shown in Figure 9a, when hash index ratio grows, more KV pairs can be stored inline, yielding a lower average memory access time. Figure 9b shows the increase of memory accesses as more memory is utilized. As shown in Figure 10, the maximal achievable memory utilization drops under higher hash index ratio, because less memory is available for dynamic allocation. Consequently, aiming to accommodate the entire corpus in a given memory size, the hash index ratio has an upper bound. We choose this upper bound and get a minimal average memory access times, shown as the dashed line in Figure 10.\nIn Figure 11, we plot the number of memory accesses per GET and PUT operation for three possible hash table designs: chaining in KV-Direct, bucket cuckoo hashing in MemC3 [23] and chain-associative hopscotch hashing in FaRM [18]. For KV-Direct, we make the optimal choice of inline threshold and hash index ratio for the given KV size and memory utilization requirement. For cuckoo and hopscotch hashing, we assume that keys are inlined and can be compared in parallel, while the values are stored in dynamically allocated slabs.\nSince the hash table of MemC3 and FaRM cannot support more than 55% memory utilization for 10B KV size, the three rightmost bars in Figure 11a and Figure 11b only show the performance of KV-Direct.\nFor inline KVs, KV-Direct has close to 1 memory access per GET and close to 2 memory accesses per PUT under non-extreme memory utilizations. GET and PUT for noninline KVs have one additional memory access. Comparing KV-Direct and chained hopscotch hashing under high memory utilization, hopscotch hashing performs better in GET, but significantly worse in PUT. Although KV-Direct cannot guarantee worst case DMA accesses, we strike for a balance between GET and PUT. Cuckoo hashing needs to access up to two hash slots on GET, therefore has more memory accesses than KV-Direct under most memory utilizations. Under high memory utilization, cuckoo hashing incurs large fluctuations in memory access times per PUT.\n5.1.2 Slab Memory Allocator. The communication overhead of slab memory allocator comes from the NIC accessing available slab queues in host memory. To sustain the maximal throughput of 180M operations per second, in the worst case, 180M slab slots need to be transferred, consuming 720 MB/s PCIe throughput, i.e., 5% of total PCIe throughput of our NIC.\nThe computation overhead of slab memory allocator comes from slab splitting and merging on host CPU. Fortunately, they are not frequently invoked. For workloads with stable KV size distributions, newly freed slab slots are reused by subsequent allocations, therefore does not trigger splitting and merging.\nSlab splitting requires moving continuous slab entries from one slab queue to another. When the workload shifts from large KV to small KV, in the worst case the CPU needs to move 90M slab entries per second, which only utilizes 10% of a core because it is simply continuous memory copy.\nMerging free slab slots to larger slots is rather a timeconsuming task, because it involves filling the allocation bitmap with potentially random offsets and thus requiring random memory accesses. To sort the addresses of free slabs and merge continuous ones, radix sort [66] scales better to multiple cores than simple bitmap. As shown in Figure 12,\nmerging all 4 billion free slab slots in a 16 GiB vector requires 30 seconds on a single core, or only 1.8 seconds on 32 cores using radix sort [66]. Although garbage collecting free slab slots takes seconds, it runs in background without stalling the slab allocator, and practically only triggered when the workload shifts from small KV to large KV.\n5.1.3 Out-of-Order Execution Engine. We evaluate the effectiveness of out-of-order execution by comparing the throughput with the simple approach that stalls the pipeline on key conflict, under atomics and long-tail workload. Onesided RDMA and two-sided RDMA [37] throughputs are also shown as baselines.\nWithout this engine, an atomic operation needs to wait for PCIe latency and processing delay in the NIC, during which subsequent atomic operations on the same key cannot be executed. This renders a single-key atomics throughput of 0.94 Mops in Figure 13a, consistent with 2.24 Mops measured from an RDMA NIC [37]. The higher throughput of RDMA NIC can be attributed to its higher clock frequency and lower processing delay. With out-of-order execution, single-key atomic operations in KV-Direct can be processed at peak throughput, i.e., one operation per clock cycle. In MICA [51], single-key atomics throughput cannot scale beyond a single core. Atomic fetch-and-add can be spread to multiple cores in [37], but it relies on the commutativity among the atomics and therefore does not apply to non-commutative atomics such as compare-and-swap.\nWith out-of-order execution, single-key atomics throughput improves by 191x and reaches the clock frequency bound of 180 Mops. When the atomic operations spread uniformly among multiple keys, the throughput of one-sided RDMA, two-sided RDMA and KV-Direct without out-of-order execution grow linearly with the number of keys, but still far from the optimal throughput of KV-Direct.\nFigure 13b shows the throughput under the long-tail workload. Recall that the pipeline is stalled when a PUT operation finds any in-flight operation with the same key. The long-tail workload has multiple extremely popular keys, so it is likely that two operations with the same popular key arrive closely in time. With higher PUT ratio, it is more likely that at least one of the two operations is a PUT, therefore triggering a pipeline stall.\n5.1.4 DRAM Load Dispatcher. Figure 14 shows the throughput improvement of DRAM load dispatch over the baseline of using PCIe only. Under uniform workload, the caching effect of DRAM is negligible because its size is only 6% of host KVS memory. Under long-tail workload, \u223c30% of memory accesses are served by the DRAM cache. Overall, the memory access throughput for 95% and 100% GET achieves the 180 Mops clock frequency bound. However, if DRAM is simply used as a cache, the throughput would be adversely impacted because the DRAM throughput is lower than PCIe throughput.\n5.1.5 Vector Operation Decoder. To evaluate the efficiency of vector operations in KV-Direct, Table 2 compares the throughput of atomic vector increment with two alternative approaches: (1) If each element is stored as a unique key, the bottleneck is the network to transfer the KV operations. (2) If the vector is stored as a large opaque value, retrieving the vector to the client also overwhelms the network. Additionally, the two alternatives in Table 2 do not ensure consistency within the vector. Adding synchronization would incur further overhead.\nKV-Direct client packs KV operations in network packets to mitigate packet header overhead. Figure 15 shows that network batching increases network throughput by up to 4x, while keeping networking latency below 3.5 \u00b5s."
                },
                {
                    "heading": "5.2 System Benchmark",
                    "text": "5.2.1 Methodology. Before each benchmark, we tune hash index ratio, inline threshold and load dispatch ratio according to the KV size, access pattern and target memory utilization. Then we generate random KV pairs with a given size. The key size in a given inline KV size is irrelevant to the performance of KV-Direct, because the key is padded to the longest possible inline KV size during processing. To test inline case, we use KV size that is a multiple of slot size (when size \u2264 50, i.e. 10 slots). To test non-inline case, we use KV size that is a power of two minus 2 bytes (for metadata). As the last step of preparation, we issue PUT operations to insert the KV pairs into an idle KVS until 50% memory utilization. The performance under other memory utilizations can be derived from Figure 11.\nDuring benchmark, we use an FPGA-based packet generator [44] in the same ToR to generate batched KV operations, send them to the KV server, receive completions and measure sustainable throughput and latency. The processing delay of the packet generator is pre-calibrated via direct loop-back and removed from latency measurements. Error bars represent the 5th and 95th percentile.\n5.2.2 Throughput. Figure 16 shows the throughput of KV-Direct under YCSB uniform and long-tail (skewed Zipf) workload. Three factors may be the bottleneck of KV-Direct: clock frequency, network and PCIe/DRAM. For 5B\u223c15B KVs inlined in the hash index, most GETs require one PCIe/DRAM access and PUTs require two PCIe/DRAM accesses. Such tiny KVs are prevalent in many systems. In PageRank, the KV size for an edge is 8B. In sparse logistic regression, the KV size is typically 8B-16B. For sequencers and locks in distributed systems, the KV size is 8B.\nUnder same memory utilization, larger inline KVs have lower throughput, due to a higher probability of hash collision. 62B and larger KVs are not inlined, so they require an additional memory access. Long-tail workload has higher throughput than uniform workload and able to reach the clock frequency bound of 180 Mops under read-intensive workload, or reach the network throughput bound for \u226562B KV sizes.\nUnder long-tail workload, the out-of-order execution engine merges up to \u223c15% operations on the most popular keys, and the NIC DRAM has \u223c60% cache hit rate under 60% load dispatch ratio, which collectively lead to up to 2x throughput as uniform workload. As shown in Table 3, the throughput of a KV-Direct NIC is on-par with a state-of-the-art KVS server with tens of CPU cores.\n5.2.3 Power efficiency. When the KV-Direct server is at peak throughput, the system power is 121.4 watts (measured on the wall). Compared with state-of-the-art KVS systems in Table 3, KV-Direct is 3x more power efficient than other systems, being the first general-purpose KVS system to achieve 1 million KV operations per watt on commodity servers.\nWhen the KV-Direct NIC is unplugged, an idle server consumes 87.0 watts power, therefore the combined power consumption of programmable NIC, PCIe, host memory and the daemon process on CPU is only 34 watts. The measured power difference is justified since the CPU is almost idle and the server can run other workloads when KV-Direct is operating (we use the same criterion for one-sided RDMA, shown at parentheses of Table 3). In this regard, KV-Direct is 10x more power efficient than CPU-based systems.\n5.2.4 Latency. Figure 17 shows the latency of KV-Direct under the peak throughput of YCSB workload. Without network batching, the tail latency ranges from 3\u223c9 \u00b5s depending on KV size, operation type and key distribution. PUT has higher latency than GET due to additional memory access. Skewed workload has lower latency than uniform due to more likelihood of being cached in NIC DRAM. Larger KV has higher latency due to additional network and PCIe transmission delay. Network batching adds less than 1 \u00b5s latency than non-batched operations, but significantly improves throughput, which has been evaluated in Figure 15.\n5.2.5 Impact on CPU performance. KV-Direct is designed to bypass the server CPU and uses only a portion of host memory for KV storage. Therefore, the CPU can still run other applications. Our measurements find a minimal impact on other workloads on the server when a single NIC KV-Direct is at peak load. Table 4 quantifies this impact at the\npeak throughput of KV-Direct. Except for sequential throughput of CPU 0 to access its own NUMA memory (the line marked in bold), the latency and throughput of CPU memory accesses are mostly unaffected. This is because 8 channels of host memory can provide far higher random access throughput than all CPU cores could consume, while the CPU can indeed stress the sequential throughput of the DRAM channels. The impact of the host daemon process is minimal when the distribution of KV sizes is relatively stable, because the garbage collector is invoked only when the number of available slots for different slab sizes are imbalanced."
                },
                {
                    "heading": "6 EXTENSIONS",
                    "text": ""
                },
                {
                    "heading": "6.1 CPU-based Scatter-Gather DMA",
                    "text": "PCIe has 29% TLP header and padding overhead for 64B DMA operations (\u00a72.4) and the DMA engine may not have enough parallelism to saturate the PCIe bandwidth-delay product with small TLPs (\u00a74). Larger DMA operations with up to 256-byte TLP payload is supported by the PCIe root complex\nin our system. In this case, the TLP head and padding overhead is only 9%, and the DMA engine has enough parallelism (64) to saturate the PCIe link with 27 in-flight DMA reads. To batch the DMA operations on PCIe link, we can leverage the CPU to perform scatter-gather (Figure 19). First, the NIC DMAs addresses to a request queue in host memory. The host CPU polls the request queue, performs random memory access, put the data in response queue and writes MMIO doorbell to the NIC. The NIC then fetches the data from response queue via DMA.\nFigure 18 shows that CPU-based scatter-gather DMA has up to 79% throughput improvement compared to the CPUbypassing approach. In addition to the CPU overhead, the primary drawback of CPU-based scatter-gather is the additional latency. To save MMIOs from the CPU to the NIC, we batch 256 DMA operations per doorbell, which requires 10 \u00b5s to complete. The overall latency for the NIC to access host memory using CPU-based scatter-gather is \u223c20 \u00b5s, almost 20x higher than direct DMA."
                },
                {
                    "heading": "6.2 Multiple NICs per Server",
                    "text": "In some cases, it may be desirable to build a dedicated keyvalue store with maximal throughput per server. Through simulation, [47] showed the possibility of achieving a billion KV op/s in a single server with four (currently unavailable) 60-core CPUs. As shown in Table 3, with 10 KV-Direct NICs\non a server, the one billion KV op/s performance is readily achievable with a commodity server. The KV-Direct server consumes 357 watts power (measured on the wall) to achieve 1.22 Gop/s GET or 0.61 Gop/s PUT.\nIn order to saturate the 80 PCIe Gen3 lanes of two Xeon E5 CPUs, we replace the motherboard of the benchmarked server (Sec. 5) with a SuperMicro X9DRX+-F motherboard with 10 PCIe Gen3 x8 slots. We use PCIe x16 to x8 converters to connect 10 programmable NICs on each of the slots, and only one PCIe Gen3 x8 link is enabled on each NIC, so the throughput per NIC is lower than Figure 16. Each NIC owns an exclusive memory region in host memory and serves a disjoint partition of keys. Multiple NICs suffer the same load imbalance problem as a multi-core KVS implementation. Fortunately, for a small number of partitions (e.g. 10), the load imbalance is not significant [47, 51]. Under YCSB longtail workload, the highest-loaded NIC has 1.5x load of the average, and the added load from extremely popular keys is served by the out-of-order execution engine (Sec. 3.3.3). Figure 20 shows that KV-Direct throughput scales almost linearly with the number of NICs on a server."
                },
                {
                    "heading": "6.3 Discussion",
                    "text": "6.3.1 NIC hardware with different capacity. The goal of KV-Direct is to leverage existing hardware in data centers to offload an important workload (KV access), instead of designing a special hardware to achieve maximal KVS performance. We use programmable NICs, which usually contain limited amount of DRAM for buffering and connection-state tracking. Large DRAMs are expensive in both die size and power consumption.\nEven if future NICs have faster or larger on-board memory, under long-tail workload, our load dispatch design (Sec. 3.3.4)\nstill shows performance gain over the simple design of partitioning the keys uniformly according to NIC and host memory capacity. Table 5 shows the optimal load dispatch ratio for long-tail workload with a corpus of 1 billion keys, under different ratio of NIC DRAM and PCIe throughput and different ratio of NIC and host memory size. If a NIC has faster DRAM, more load will be dispatched to the NIC. A load dispatch ratio of 1 means the NIC memory behaves exactly like a cache of host memory. If a NIC has larger DRAM, a slightly less portion of load will be dispatched to the NIC. As shown in Table 6, even when the size of NIC DRAM is a tiny fraction of host memory, the throughput gain is significant.\nThe hash table and slab allocator design (Sec. 3.3.1) is generally applicable to hash-based storage systems that need to be frugal of random accesses for both lookup and insertion.\nThe out-of-order execution engine (Sec. 3.3.3) can be applied to all kinds of applications in need of latency hiding, and we hope future RDMA NICs to support that for atomics.\nIn 40 Gbps networks, network bandwidth bounds nonbatched KV throughput, so we use client-side batching (Sec.4). With higher network bandwidth, batch size can be reduced, thus reducing latency. In a 200 Gbps network, a KV-Direct NIC could achieve 180 Mop/s without batching.\nKV-Direct leverages widely-deployed programmable NICs with FPGAs [10, 64]. FlexNIC [40, 41] is another promising architecture for programmable NICs with Reconfigurable Match-action Tables (RMT) [8]. NetCache [35] implements a KV cache in RMT-based programmable switches, showing potential for building KV-Direct in an RMT-based NIC.\n6.3.2 Implications for real-world applications. Backof-the-envelope calculations show potential performance gains when KV-Direct is applied in end-to-end applications. In PageRank [61], because each edge traversal can be implemented with one KV operation, KV-Direct supports 1.22G TEPS on a server with 10 programmable NICs. In comparison, GRAM [73] supports 250M TEPS per server, bound by interleaved computation and random memory access.\nKV-Direct supports user-defined functions and vector operations (Table 1) that can further optimize PageRank by offloading client computation to hardware. Similar arguments hold for parameter server [46]. We expect future work to leverage hardware-accelerated key-value stores to improve distributed application performance."
                },
                {
                    "heading": "7 RELATED WORK",
                    "text": "As an important infrastructure, the research and development of distributed key-value store systems have been driven by performance. A large body of distributed KVS are based on CPU. To reduce the computation cost, Masstree [53], MemC3 [23] and libcuckoo [48] optimize locking, caching, hashing and memory allocation algorithms, while KV-Direct comes with a new hash table and memory management mechanism specially designed for FPGA to minimize the PCIe traffic. MICA [51] partitions the hash table to each core thus completely avoids synchronization. This approach, however, introduces core imbalance for skewed workloads.\nTo get rid of the OS kernel overhead, [31, 65] directly poll network packets from NIC and [34, 54] process them with the user space lightweight network stack. Key-value store systems [39, 47, 51, 58, 59] benefit from such optimizations for high performance. As a further step towards this direction, recent works [1, 36, 36, 37, 37] leverage the hardware-based network stack of RDMA NIC, using two-sided RDMA as an RPC mechanism between KVS client and server to further improve per-core throughput and reduce latency. Still, these systems are CPU bound (\u00a72.2).\nAnother different approach is to leverage one-sided RDMA. Pilaf [55] and FaRM [18] adopt one-sided RDMA read for GET operation and FaRM achieves throughput that saturates the network. Nessie [70], DrTM [72], DrTM+R [13] and FaSST [38] leverage distributed transactions to implement both GET and PUT with one-sided RDMA. However, the performance of PUT operation suffer from unavoidable synchronization overhead for consistency guarantee, limited by RDMA primitives [37]. Moreover, client-side CPU is involved in KV processing, limiting per-core throughput to \u223c10 Mops on the client side. In contrast, KV-Direct extends the RDMA primitives to key-value operations while guarantees the consistency in server side, leaving the KVS client totally transparent while achieving high throughput and low latency even for PUT operation.\nAs a flexible and customizable hardware, FPGA is now widely deployed in datacenter-scale [10, 64] and greatly improved for programmability [4, 44]. Several early works have explored building KVS on FPGA. But some of them are not practical by limiting the data storage in on-chip (about several MB memory) [50] or on-board DRAM (typically 8GB memory) [11, 32, 33]. [6] focuses on improving system capacity rather than throughput, and adopts SSD as the secondary storage out of on-board DRAM. [11, 50] limit their usage in fixed size key-value pairs, which can only work for the special purpose rather than a general key-value store. [5, 43] uses host DRAM to store the hash table, and [71] uses NIC DRAM as a cache of host DRAM, but they did not optimize for network and PCIe DMA bandwidth, resulting in poor performance. KV-Direct fully utilizes both NIC DRAM and host DRAM,\nmaking our FPGA-based key-value store system general and capable of large-scale deployment. Furthermore, our careful hardware and software co-design, together with optimizations for PCIe and networking push the performance to the physical limitation, advancing state-of-art solutions.\nSecondary index is an important feature to retrieve data by keys other than the primary key in data storage system [20, 42]. SLIK [42] supports multiple secondary keys using a B+ tree algorithm in key-value store system. It would be interesting to explore how to support secondary index to help KV-Direct step towards a general data storage system. SwitchKV [49] leverages content-based routing to route requests to backend nodes based on cached keys, and NetCache [35] takes a further step to cache KV in the switches. Such load balancing and caching will also benefit our system. Eris [45] leverages network sequencers to achieve efficient distributed transactions, which may give a new life to the one-sided RDMA approach with client synchronization."
                },
                {
                    "heading": "8 CONCLUSION",
                    "text": "In this paper, we describe the design and evaluation of KVDirect, a high performance in-memory key-value store. Following a long history in computer system design, KV-Direct is another exercise in leveraging reconfigurable hardware to accelerate an important workload. KV-Direct is able to obtain superior performance by carefully co-designing hardware and software in order to remove bottlenecks in the system and achieve performance that is close to the physical limits of the underlying hardware.\nAfter years of broken promises, FPGA-based reconfigurable hardware finally becomes widely available in main stream data centers. Many significant workloads will be scrutinized to see whether they can benefit from reconfigurable hardware, and we expect much more fruitful work in this general direction."
                },
                {
                    "heading": "ACKNOWLEDGEMENTS",
                    "text": "We would like to thank Kun Tan, Ningyi Xu, Ming Wu, Jiansong Zhang and Anuj Kalia for all technical discussions and valuable comments. We\u2019d also like to thank the whole Catapult v-team at Microsoft for support on the FPGA platform. We thank our shepherd, Simon Peter, and other anonymous reviewers for their valuable feedback and comments."
                }
            ],
            "year": 2017,
            "references": [
                {
                    "title": "Workload analysis of a large-scale key-value store",
                    "authors": [
                        "Berk Atikoglu",
                        "Yuehai Xu",
                        "Eitan Frachtenberg",
                        "Song Jiang",
                        "Mike Paleczny"
                    ],
                    "venue": "In ACM SIGMETRICS Performance Evaluation Review,",
                    "year": 2012
                },
                {
                    "title": "FPGA programming for the masses",
                    "authors": [
                        "David F Bacon",
                        "Rodric Rabbah",
                        "Sunil Shukla"
                    ],
                    "venue": "Commun. ACM 56,",
                    "year": 2013
                },
                {
                    "title": "Achieving 10Gbps Line-rate Key-value Stores with FPGAs",
                    "authors": [
                        "Michaela Blott",
                        "Kimon Karras",
                        "Ling Liu",
                        "Kees Vissers",
                        "Jeremia B\u00e4r",
                        "Zsolt Istv\u00e1n"
                    ],
                    "venue": "In The 5th USENIX Workshop on Hot Topics in Cloud Computing",
                    "year": 2013
                },
                {
                    "title": "Scaling Out to a Single-Node 80Gbps Memcached Server with 40Terabytes of Memory",
                    "authors": [
                        "Michaela Blott",
                        "Ling Liu",
                        "Kimon Karras",
                        "Kees A Vissers"
                    ],
                    "venue": "HotStorage",
                    "year": 2015
                },
                {
                    "title": "Forwarding metamorphosis: Fast programmable match-action processing in hardware for SDN",
                    "authors": [
                        "Pat Bosshart",
                        "Glen Gibb",
                        "Hun-Seok Kim",
                        "George Varghese",
                        "Nick McKeown",
                        "Martin Izzard",
                        "Fernando Mujica",
                        "Mark Horowitz"
                    ],
                    "venue": "In ACM SIGCOMM Computer Communication Review,",
                    "year": 2013
                },
                {
                    "title": "2016. Horton tables: fast hash tables for in-memory data-intensive computing",
                    "authors": [
                        "Alex D Breslow",
                        "Dong Ping Zhang",
                        "Joseph L Greathouse",
                        "Nuwan Jayasena",
                        "Dean M Tullsen"
                    ],
                    "venue": "ATC",
                    "year": 2016
                },
                {
                    "title": "An FPGA memcached appliance",
                    "authors": [
                        "Sai Rahul Chalamalasetti",
                        "Kevin Lim",
                        "Mitch Wright",
                        "Alvin AuYoung",
                        "Parthasarathy Ranganathan",
                        "Martin Margala"
                    ],
                    "venue": "In Proceedings of the ACM/SIGDA international symposium on Field programmable gate arrays (FPGA)",
                    "year": 2013
                },
                {
                    "title": "Bigtable: A distributed storage system for structured data",
                    "authors": [
                        "Fay Chang",
                        "Jeffrey Dean",
                        "Sanjay Ghemawat",
                        "Wilson C Hsieh",
                        "Deborah A Wallach",
                        "Mike Burrows",
                        "Tushar Chandra",
                        "Andrew Fikes",
                        "Robert E Gruber"
                    ],
                    "venue": "ACM Transactions on Computer Systems (TOCS) 26,",
                    "year": 2008
                },
                {
                    "title": "Fast and general distributed transactions using RDMA and HTM",
                    "authors": [
                        "Yanzhe Chen",
                        "Xingda Wei",
                        "Jiaxin Shi",
                        "Rong Chen",
                        "Haibo Chen"
                    ],
                    "venue": "In Eurosys \u201916",
                    "year": 2016
                },
                {
                    "title": "Invited - Heterogeneous Datacenters: Options and Opportunities",
                    "authors": [
                        "Jason Cong",
                        "Muhuan Huang",
                        "Di Wu",
                        "Cody Hao Yu"
                    ],
                    "venue": "In Proceedings of the 53rd Annual Design Automation Conference (DAC \u201916)",
                    "year": 2016
                },
                {
                    "title": "Benchmarking cloud serving systems with YCSB",
                    "authors": [
                        "Brian F Cooper",
                        "Adam Silberstein",
                        "Erwin Tam",
                        "Raghu Ramakrishnan",
                        "Russell Sears"
                    ],
                    "venue": "In Proceedings of the 1st ACM symposium on Cloud computing",
                    "year": 2010
                },
                {
                    "title": "2010",
                    "authors": [
                        "TPC Council"
                    ],
                    "venue": "tpc-c benchmark, revision 5.11. ",
                    "year": 2010
                },
                {
                    "title": "Dynamo: amazon\u2019s highly available key-value store",
                    "authors": [
                        "Giuseppe DeCandia",
                        "Deniz Hastorun",
                        "Madan Jampani",
                        "Gunavardhan Kakulapati",
                        "Avinash Lakshman",
                        "Alex Pilchin",
                        "Swaminathan Sivasubramanian",
                        "Peter Vosshall",
                        "Werner Vogels"
                    ],
                    "venue": "ACM SIGOPS Operating Systems Review 41,",
                    "year": 2007
                },
                {
                    "title": "FaRM: fast remote memory",
                    "authors": [
                        "Aleksandar Dragojevi\u0107",
                        "Dushyanth Narayanan",
                        "Miguel Castro",
                        "Orion Hodson"
                    ],
                    "venue": "NSDI",
                    "year": 2014
                },
                {
                    "title": "David E Culler",
                    "authors": [
                        "TV Eicken"
                    ],
                    "venue": "Seth Copen Goldstein, and Klaus Erik Schauser. 1992. Active messages: a mechanism for integrated communication and computation. In Computer Architecture",
                    "year": 1992
                },
                {
                    "title": "HyperDex: A distributed, searchable key-value store",
                    "authors": [
                        "Robert Escriva",
                        "Bernard Wong",
                        "Emin G\u00fcn Sirer"
                    ],
                    "venue": "ACM SIGCOMM Computer Communication Review 42,",
                    "year": 2012
                },
                {
                    "title": "Dark silicon and the end of multicore  scaling",
                    "authors": [
                        "Hadi Esmaeilzadeh",
                        "Emily Blem",
                        "Renee St Amant",
                        "Karthikeyan Sankaralingam",
                        "Doug Burger"
                    ],
                    "venue": "In Computer Architecture (ISCA),",
                    "year": 2011
                },
                {
                    "title": "Power challenges may end the multicore era",
                    "authors": [
                        "Hadi Esmaeilzadeh",
                        "Emily Blem",
                        "Ren\u00e9e St Amant",
                        "Karthikeyan Sankaralingam",
                        "Doug Burger"
                    ],
                    "venue": "Commun. ACM 56,",
                    "year": 2013
                },
                {
                    "title": "MemC3: Compact and concurrent memcache with dumber caching and smarter hashing",
                    "authors": [
                        "Bin Fan",
                        "David G Andersen",
                        "Michael Kaminsky"
                    ],
                    "venue": "In NSDI",
                    "year": 2013
                },
                {
                    "title": "VFP: A Virtual Switch Platform for Host SDN in the Public Cloud",
                    "authors": [
                        "Daniel Firestone"
                    ],
                    "venue": "In NSDI \u201917",
                    "year": 2017
                },
                {
                    "title": "Distributed caching with memcached",
                    "authors": [
                        "Brad Fitzpatrick"
                    ],
                    "venue": "Linux journal 2004,",
                    "year": 2004
                },
                {
                    "title": "Hiding memory latency using dynamic scheduling in shared-memory multiprocessors",
                    "authors": [
                        "Kourosh Gharachorloo",
                        "Anoop Gupta",
                        "John Hennessy"
                    ],
                    "year": 1992
                },
                {
                    "title": "SDN for the Cloud",
                    "authors": [
                        "Albert Greenberg"
                    ],
                    "venue": "In Keynote in the 2015 ACM Conference on Special Interest Group on Data Communication",
                    "year": 2015
                },
                {
                    "title": "PacketShader: a GPU-accelerated software router",
                    "authors": [
                        "Sangjin Han",
                        "Keon Jang",
                        "KyoungSoo Park",
                        "Sue Moon"
                    ],
                    "venue": "In ACM SIGCOMM Computer Communication Review,",
                    "year": 2010
                },
                {
                    "title": "Programming and Runtime Support to Blaze FPGA Accelerator Deployment at Datacenter Scale",
                    "authors": [
                        "Muhuan Huang",
                        "Di Wu",
                        "Cody Hao Yu",
                        "Zhenman Fang",
                        "Matteo Interlandi",
                        "Tyson Condie",
                        "Jason Cong"
                    ],
                    "venue": "In Proceedings of the Seventh ACM Symposium on Cloud Computing (SoCC \u201916)",
                    "year": 2016
                },
                {
                    "title": "2014",
                    "authors": [
                        "DPDK Intel"
                    ],
                    "venue": "Data plane development kit. ",
                    "year": 2014
                },
                {
                    "title": "A flexible hash table design for 10gbps key-value stores on fpgas",
                    "authors": [
                        "Zsolt Istv\u00e1n",
                        "Gustavo Alonso",
                        "Michaela Blott",
                        "Kees Vissers"
                    ],
                    "venue": "In 23rd International Conference on Field programmable Logic and Applications",
                    "year": 2013
                },
                {
                    "title": "A hash table for line-rate data processing",
                    "authors": [
                        "Zsolt Istv\u00e1n",
                        "Gustavo Alonso",
                        "Michaela Blott",
                        "Kees Vissers"
                    ],
                    "venue": "ACM Transactions on Reconfigurable Technology and Systems (TRETS)",
                    "year": 2015
                },
                {
                    "title": "mTCP: a Highly Scalable User-level TCP Stack for Multicore Systems",
                    "authors": [
                        "EunYoung Jeong",
                        "Shinae Woo",
                        "Muhammad Asim Jamshed",
                        "Haewon Jeong",
                        "Sunghwan Ihm",
                        "Dongsu Han",
                        "KyoungSoo Park"
                    ],
                    "venue": "In NSDI",
                    "year": 2014
                },
                {
                    "title": "NetCache: Balancing Key-Value Stores with Fast In-Network Caching",
                    "authors": [
                        "Xin Jin",
                        "Xiaozhou Li",
                        "Haoyu Zhang",
                        "Robert Soule",
                        "Jeongkeun Lee",
                        "Nate Foster",
                        "Changhoon Kim",
                        "Ion Stoica"
                    ],
                    "venue": "SOSP",
                    "year": 2017
                },
                {
                    "title": "Using RDMA efficiently for key-value services",
                    "authors": [
                        "Anuj Kalia",
                        "Michael Kaminsky",
                        "David G Andersen"
                    ],
                    "venue": "In ACM SIGCOMM Computer Communication Review,",
                    "year": 2014
                },
                {
                    "title": "Design Guidelines for High Performance RDMA Systems",
                    "authors": [
                        "Anuj Kalia",
                        "Michael Kaminsky",
                        "David G Andersen"
                    ],
                    "venue": "ATC",
                    "year": 2016
                },
                {
                    "title": "FaSST: fast, scalable and simple distributed transactions with two-sided RDMA datagram RPCs",
                    "authors": [
                        "Anuj Kalia",
                        "Michael Kaminsky",
                        "David G Andersen"
                    ],
                    "venue": "In OSDI",
                    "year": 2016
                },
                {
                    "title": "Chronos: predictable low latency for data center applications",
                    "authors": [
                        "Rishi Kapoor",
                        "George Porter",
                        "Malveeka Tewari",
                        "Geoffrey M Voelker",
                        "Amin Vahdat"
                    ],
                    "venue": "In Proceedings of the Third ACM Symposium on Cloud Computing. ACM,",
                    "year": 2012
                },
                {
                    "title": "FlexNIC: Rethinking Network DMA",
                    "authors": [
                        "Antoine Kaufmann",
                        "Simon Peter",
                        "Thomas E Anderson",
                        "Arvind Krishnamurthy"
                    ],
                    "venue": "HotOS",
                    "year": 2015
                },
                {
                    "title": "High Performance Packet Processing with FlexNIC",
                    "authors": [
                        "Antoine Kaufmann",
                        "Simon Peter",
                        "Navven Kumar Sharma",
                        "Thomas Anderson"
                    ],
                    "venue": "In Proceedings of the 21th International Conference on Architectural",
                    "year": 2016
                },
                {
                    "title": "SLIK: Scalable low-latency indexes for a key-value store",
                    "authors": [
                        "Ankita Kejriwal",
                        "Arjun Gopalan",
                        "Ashish Gupta",
                        "Zhihao Jia",
                        "Stephen Yang",
                        "John Ousterhout"
                    ],
                    "venue": "ATC",
                    "year": 2016
                },
                {
                    "title": "An FPGAbased in-line accelerator for Memcached",
                    "authors": [
                        "Maysam Lavasani",
                        "Hari Angepat",
                        "Derek Chiou"
                    ],
                    "venue": "IEEE Computer Architecture Letters 13,",
                    "year": 2014
                },
                {
                    "title": "ClickNP: Highly flexible and High-performance Network Processing with Reconfigurable Hardware",
                    "authors": [
                        "Bojie Li",
                        "Kun Tan",
                        "Layong Larry Luo",
                        "Yanqing Peng",
                        "Renqian Luo",
                        "Ningyi Xu",
                        "Yongqiang Xiong",
                        "Peng Cheng",
                        "Enhong Chen"
                    ],
                    "venue": "In SIGCOMM \u201916",
                    "year": 2016
                },
                {
                    "title": "Eris: Coordination- Free Consistent Transactions Using In-Network Concurrency Control",
                    "authors": [
                        "Jialin Li",
                        "Ellis Michael",
                        "Dan R.K. Ports"
                    ],
                    "venue": "SOSP",
                    "year": 2017
                },
                {
                    "title": "Scaling Distributed Machine Learning with the Parameter Server",
                    "authors": [
                        "Mu Li",
                        "David G Andersen",
                        "Jun Woo Park"
                    ],
                    "year": 2014
                },
                {
                    "title": "Algorithmic improvements for fast concurrent cuckoo hashing",
                    "authors": [
                        "Xiaozhou Li",
                        "David G Andersen",
                        "Michael Kaminsky",
                        "Michael J Freedman"
                    ],
                    "venue": "In Eurosys \u201914",
                    "year": 2014
                },
                {
                    "title": "Be fast, cheap and in control with SwitchKV",
                    "authors": [
                        "Xiaozhou Li",
                        "Raghav Sethi",
                        "Michael Kaminsky",
                        "David G Andersen",
                        "Michael J Freedman"
                    ],
                    "venue": "NSDI",
                    "year": 2016
                },
                {
                    "title": "Memory efficient and high performance key-value store on FPGA using Cuckoo hashing",
                    "authors": [
                        "Wei Liang",
                        "Wenbo Yin",
                        "Ping Kang",
                        "Lingli Wang"
                    ],
                    "venue": "In 2016 26th International Conference on Field Programmable Logic and Applications (FPL)",
                    "year": 2016
                },
                {
                    "title": "MICA: a holistic approach to fast in-memory key-value storage",
                    "authors": [
                        "Hyeontaek Lim",
                        "Dongsu Han",
                        "David G Andersen",
                        "Michael Kaminsky"
                    ],
                    "venue": "In NSDI",
                    "year": 2014
                },
                {
                    "title": "FPGA-Accelerated Transactional Execution of Graph Workloads",
                    "authors": [
                        "Xiaoyu Ma",
                        "Dan Zhang",
                        "Derek Chiou"
                    ],
                    "venue": "In Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, FPGA 2017,",
                    "year": 2017
                },
                {
                    "title": "Cache craftiness for fast multicore key-value storage",
                    "authors": [
                        "Yandong Mao",
                        "Eddie Kohler",
                        "Robert Tappan Morris"
                    ],
                    "venue": "In Proceedings of the 7th ACM european conference on Computer Systems",
                    "year": 2012
                },
                {
                    "title": "Network stack specialization for performance",
                    "authors": [
                        "Ilias Marinos",
                        "Robert NM Watson",
                        "Mark Handley"
                    ],
                    "venue": "In ACM SIGCOMM Computer Communication Review,",
                    "year": 2014
                },
                {
                    "title": "Using One- Sided RDMA Reads to Build a Fast, CPU-Efficient Key-Value Store",
                    "authors": [
                        "Christopher Mitchell",
                        "Yifeng Geng",
                        "Jinyang Li"
                    ],
                    "venue": "In USENIX ATC",
                    "year": 2013
                },
                {
                    "title": "Phase Reconciliation for Contended In-Memory Transactions",
                    "authors": [
                        "Neha Narula",
                        "Cody Cutler",
                        "Eddie Kohler",
                        "Robert Morris"
                    ],
                    "venue": "In OSDI \u201914,",
                    "year": 2014
                },
                {
                    "title": "SDA: Software-defined accelerator for large-scale DNN systems",
                    "authors": [
                        "Jian Ouyang",
                        "Shiding Lin",
                        "Wei Qi",
                        "Yong Wang",
                        "Bo Yu",
                        "Song Jiang"
                    ],
                    "venue": "IEEE Hot Chips 26 Symposium (HCS)",
                    "year": 2014
                },
                {
                    "title": "The PageRank citation ranking: Bringing order to the web",
                    "authors": [
                        "Lawrence Page",
                        "Sergey Brin",
                        "Rajeev Motwani",
                        "Terry Winograd"
                    ],
                    "venue": "Technical Report. Stanford InfoLab",
                    "year": 1999
                },
                {
                    "title": "Fastpass: A centralized zero-queue datacenter network",
                    "authors": [
                        "Jonathan Perry",
                        "Amy Ousterhout",
                        "Hari Balakrishnan",
                        "Devavrat Shah",
                        "Hans Fugal"
                    ],
                    "venue": "In ACM SIGCOMM Computer Communication Review,",
                    "year": 2014
                },
                {
                    "title": "Netmap: a novel framework for fast packet I/O",
                    "authors": [
                        "Luigi Rizzo"
                    ],
                    "venue": "In 21st USENIX Security Symposium (USENIX Security",
                    "year": 2012
                },
                {
                    "title": "Fast sort on CPUs and GPUs: a case for bandwidth oblivious SIMD sort",
                    "authors": [
                        "Nadathur Satish",
                        "Changkyu Kim",
                        "Jatin Chhugani",
                        "Anthony D Nguyen",
                        "Victor W Lee",
                        "Daehyun Kim",
                        "Pradeep Dubey"
                    ],
                    "venue": "In Proceedings of the 2010 ACM SIGMOD International Conference on Management of data",
                    "year": 2010
                },
                {
                    "title": "Trinity: A distributed graph engine on a memory cloud",
                    "authors": [
                        "Bin Shao",
                        "Haixun Wang",
                        "Yatao Li"
                    ],
                    "venue": "In Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data",
                    "year": 2013
                },
                {
                    "title": "Packet transactions: High-level programming for line-rate switches",
                    "authors": [
                        "Anirudh Sivaraman",
                        "Alvin Cheung",
                        "Mihai Budiu",
                        "Changhoon Kim",
                        "Mohammad Alizadeh",
                        "Hari Balakrishnan",
                        "George Varghese",
                        "Nick McKeown",
                        "Steve Licking"
                    ],
                    "venue": "In Proceedings of the ACM SIGCOMM 2016 Conference",
                    "year": 2016
                },
                {
                    "title": "The free lunch is over: A fundamental turn toward concurrency in software",
                    "authors": [
                        "Herb Sutter"
                    ],
                    "venue": "Dr. Dobbs journal 30,",
                    "year": 2005
                },
                {
                    "title": "Designing a low-latency cuckoo hash table for write-intensive workloads using RDMA",
                    "authors": [
                        "Tyler Szepesi",
                        "Bernard Wong",
                        "Ben Cassell",
                        "Tim Brecht"
                    ],
                    "venue": "In First International Workshop on Rack-scale Computing",
                    "year": 2014
                },
                {
                    "title": "A multilevel NOSQL cache design combining In-NIC and In-Kernel caches. In High- Performance Interconnects (HOTI \u201916)",
                    "authors": [
                        "Yuta Tokusashi",
                        "Hiroki Matsutani"
                    ],
                    "year": 2016
                },
                {
                    "title": "Fast in-memory transaction processing using RDMA and HTM",
                    "authors": [
                        "Xingda Wei",
                        "Jiaxin Shi",
                        "Yanzhe Chen",
                        "Rong Chen",
                        "Haibo Chen"
                    ],
                    "venue": "In SOSP \u201915",
                    "year": 2015
                },
                {
                    "title": "GraM: scaling graph computation to the trillions",
                    "authors": [
                        "Ming Wu",
                        "Fan Yang",
                        "Jilong Xue",
                        "Wencong Xiao",
                        "Youshan Miao",
                        "Lan Wei",
                        "Haoxiang Lin",
                        "Yafei Dai",
                        "Lidong Zhou"
                    ],
                    "venue": "In Proceedings of the Sixth ACM Symposium on Cloud Computing",
                    "year": 2015
                },
                {
                    "title": "TuX2: Distributed Graph Computation for Machine Learning",
                    "authors": [
                        "Wencong Xiao",
                        "Jilong Xue",
                        "Youshan Miao",
                        "Zhen Li",
                        "Cheng Chen",
                        "Ming Wu",
                        "Wei Li",
                        "Lidong Zhou"
                    ],
                    "venue": "NSDI",
                    "year": 2017
                },
                {
                    "title": "Mega-KV: a case for GPUs to maximize the throughput of in-memory key-value stores",
                    "authors": [
                        "Kai Zhang",
                        "Kaibo Wang",
                        "Yuan Yuan",
                        "Lei Guo",
                        "Rubao Lee",
                        "Xiaodong Zhang"
                    ],
                    "venue": "Proceedings of the VLDB Endowment 8,",
                    "year": 2015
                }
            ],
            "id": "SP:a6472fe7fbc978de8597c2f783891aa1eb1f87a5",
            "authors": [
                {
                    "name": "Bojie Li",
                    "affiliations": []
                },
                {
                    "name": "Zhenyuan Ruan",
                    "affiliations": []
                },
                {
                    "name": "Wencong Xiao",
                    "affiliations": []
                },
                {
                    "name": "Yuanwei Lu",
                    "affiliations": []
                },
                {
                    "name": "Yongqiang Xiong",
                    "affiliations": []
                },
                {
                    "name": "Andrew Putnam",
                    "affiliations": []
                },
                {
                    "name": "Enhong Chen",
                    "affiliations": []
                },
                {
                    "name": "Lintao Zhang",
                    "affiliations": []
                }
            ],
            "abstractText": "Performance of in-memory key-value store (KVS) continues to be of great importance as modern KVS goes beyond the traditional object-caching workload and becomes a key infrastructure to support distributed main-memory computation in data centers. Recent years have witnessed a rapid increase of network bandwidth in data centers, shifting the bottleneck of most KVS from the network to the CPU. RDMA-capable NIC partly alleviates the problem, but the primitives provided by RDMA abstraction are rather limited. Meanwhile, programmable NICs become available in data centers, enabling in-network processing. In this paper, we present KV-Direct, a high performance KVS that leverages programmable NIC to extend RDMA primitives and enable remote direct key-value access to the main host memory. We develop several novel techniques to maximize the throughput and hide the latency of the PCIe connection between the NIC and the host memory, which becomes the new bottleneck. Combined, these mechanisms allow a single NIC KV-Direct to achieve up to 180 M key-value operations per second, equivalent to the throughput of tens of CPU cores. Compared with CPU based KVS implementation, KV-Direct improves power efficiency by 3x, while keeping tail latency below 10 \u03bcs. Moreover, KV-Direct can achieve near linear scalability with multiple NICs. With 10 programmable NIC cards in a commodity server, we achieve 1.22 billion KV operations per second, which is almost an order-of-magnitude improvement over existing systems, setting a new milestone for a general-purpose in-memory key-value store. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SOSP \u201917, Shanghai, China \u00a9 2017 ACM. 978-1-4503-5085-3/17/10. . . $15.00 DOI: 10.1145/3132747.3132756 CCS CONCEPTS \u2022Information systems\u2192 Key-value stores; \u2022Hardware\u2192 Hardware-software codesign;",
            "title": "KV-Direct: High-Performance In-Memory Key-Value Store with Programmable NIC"
        },
        "Y": {
            "blog_id": "kv-direct-high-performance-in-memory-key-value-store-with-programmable-nic",
            "summary": [
                "KV-Direct: High-performance in-memory key-value store with programmable NIC Li et al., SOSP\u201917  We\u2019ve seen some pretty impressive in-memory datastores in past editions of The Morning Paper, including FaRM , RAMCloud , and DrTM .",
                "But nothing that compares with KV-Direct:  With 10 programmable NIC cards in a commodity server, we achieve 1.22 billion KV operations per second, which is almost an order-of-magnitude improvement over existing systems, setting a new milestone for a general-purpose in-memory key-value store.",
                "Check out the bottom line in this comparison table from the evaluation:  ( Enlarge )  In addition to sheer speed, you might also notice that KV-Direct is 3x more power efficient than other systems, and the first general purpose KVS system to achieve 1 million KV operations per watt on commodity servers.",
                "Since the server CPU can also be used to run other workloads at the same time, you can make a case for KV-Direct being as much as 10x more power efficient than CPU-based systems.",
                "What we\u2019re seeing here is a glimpse of how large-scale systems software of the future may well be constructed.",
                "As the power ceiling puts a limit on multi-core scaling, people are now turning to domain-specific architectures for better performance.",
                "A first generation of key-value stores were built in a straightforward manner on top of traditional operating systems and TCP/IP stacks.",
                "More recently, as both the single core frequency scaling and multi-core architecture scaling are slowing down, a new research trend in distributed systems is to leverage Remote Direct Memory Access (RDMA) technology on NIC to reduce network processing cost.",
                "KV-Direct however, goes one step beyond.",
                "To support network virtualisation, more and more servers in data centers are now equipped with programmable NICS containing field-programmable gate arrays (FPGA).",
                "An embedded NIC chip connects to the network, and a PCIe connector attaches to the server.",
                "KV-Direct uses the FPGA in the NIC to implement key-value primitives directly.",
                "Like one-sided RDMA (Fig 1b below), KV-Direct bypasses the remote CPU.",
                "But it also extends the RDMA primitives from simple memory operations (READ and WRITE) to key-value operations (GET, PUT, DELETE and ATOMIC ops) \u2014 Fig 1c below.",
                "Compared with one-sided RDMA based systems, KV-Direct deals with the consistency and synchronization issues on the server-side, thus removing computation overhead in the client, and reducing network traffic.",
                "In addition, to support vector-based operations and reduce network traffic, KV-Direct also provides new vector primitives UPDATE, REDUCE, and FILTER, allowing users to define active messages and delegate certain computations to programmable NIC for efficiency.",
                "Design goals and challenges  Use cases for in-memory key-value stores have evolved beyond caching to things such as storing data indices, machine learning model parameters, nodes and edges in graph computing, and sequencers in distributed synchronisation.",
                "The role of the store shifts from object caching to a generic data structure store (c.f.",
                "Redis).",
                "This leads to the following design goals:  High batch throughput for small key-value pairs (e.g., model parameters, graph node neighbours).",
                "Predictable low-latency (e.g., for data-parallel computation,where tail latency matters)  High efficiency under write-intensive workloads (e.g., graph computations, and parameter servers)  Fast atomic operations  (e.g., for centralized schedulers, sequencers , counters and so on).",
                "Vector-type operations (for machine learning and graph computing workloads that often require operating on every element in a vector).",
                "The throughput constraint ends up being PCIe bandwidth:  In order to saturate the network with GET operations, the KVS on NIC must make full use of PCIe bandwidth and achieve close to one average memory access per GET.",
                "Getting to this level involves work on three fronts:  Minimising DMA (direct memory access) requests per KV operation.",
                "The two major components that drive random memory access are hash tables and memory allocation.",
                "Hiding PCIe latency while maintaining consistency, which entails pipelining requests.",
                "Care must be taken to respect causal dependencies here though.",
                "Balancing load between NIC DRAM and host memory.",
                "The NIC itself has a small amount of DRAM available, but it turns out not to be much faster than going over PCIe.",
                "So the trick turns out to be to use both in order to utilise the joint bandwidth.",
                "KV-Direct  KV-Direct enables remote direct key-value access.",
                "Clients send operation requests to the KVS server, and the programmable NIC processes requests and sends back results, bypassing the CPU.",
                "The following table shows the supported operations.",
                "The most interesting of course are the vector operations.",
                "KV-Direct supports two types of vector operations: sending a scalar to the NIC on the server, where the NIC applies the update to each element in the vector; and sending a vector to the server, where the NIC updates the original vector element-by-element.",
                "Furthermore, KV-Direct supports user-defined update functions as a generalisation to atomic operations.",
                "The update functions needs to be pre-registered and compiled to hardware logic before executing.",
                "When the user supplies an update function, the KV-Direct toolchain duplicates it several times to leverage FPGA parallelism and match computation with PCIe throughput, and then compiles it into reconfigurable hardware logic using a high-level synthesis (HLS) tool.",
                "These functions can be used for general stream processing on a vector value.",
                "The programmable NIC on the KVS server is reconfigured as a KV processor, which receives packets from the network, decodes vector operations, and buffers KV operations in a reservation station.",
                "The out-of-order engine then issues independent KV operations from the reservation station into the decoder.",
                "To minimise memory accesses, small KV pairs are stored inline in the hash table, while others are stored in dynamically allocated memory from a slab memory allocator.",
                "After a KV operation completes, the result is sent back to the out-of-order execution engine to find and execute matching KV operations in the reservation station.",
                "The reservation station is used to avoid dependencies between two KV operations leading to data hazards and a stalled pipeline.",
                "We borrow the concept of dynamic scheduling from computer architecture and implement a reservation station to track all in-flight KV operations and their execution context.",
                "To saturate PCIe, DRAM and the processing pipeline, up to 256 in-flight KV operations are needed.",
                "However, comparing 256 16-byte keys in parallel would take 40% of the logic resource of our FPGA.",
                "Instead, we store the KV operations in a small hash table in on-chip BRAM, indexed by the hash of the key.",
                "When a KV operation completes, the latest value is forwarded to the reservation station, where pending operations in the same hash slot are checked.",
                "Those with a matching key are executed immediately and removed from the station.",
                "Further design and implementation details can be found in sections 3 and 4 of the paper.",
                "Evaluation  The evaluation section contains a suite of microbenchmarks, followed by a system benchmark based on the YCSB workload.",
                "To simulate a skewed Zipf workload, skewness 0.99 was chosen.",
                "This is referred to as the long-tail workload in the figures.",
                "The testbed comprises eight servers with two 8-core CPUS per server,and one Arista switch.",
                "There is a total of 128 GiB of host memory per server.",
                "A programmable NIC is connected to the PCIe root complex of CPU 0, and its 40 Gbps Ethernet port is connected to the switch.",
                "The NIC has two PCIe Gen3 x8 links in a bifurcated Gen3 x16 physical connector.",
                "Here\u2019s the overall throughput achieved by the system.",
                "The throughput of a KV-Direct NIC is on-par with a state-of-the-art KVS server with tens of CPU cores.",
                "Without network batching, the tail latency ranges from 3-9  s depending on KV size, operation type, and key distribution.",
                "Network batching adds less than 1  s latency, but significantly improves performance.",
                "It is possible to attach multiple NICs per server.",
                "With 10 KV-Direct NICs on a server, one billion KV ops/s is readily achievable on a commodity server.",
                "Each NIC owns a disjoin partition of the keys.",
                "Multiple NICs suffer the same load imbalance problem as a multi-core KVS implementation, but for a relatively small number of partitions (e.g. 10) the load imbalance is not too great \u2013 1.5x of the average in the highest loaded NIC even for the long-tail highly skewed workload.",
                "KV-Direct throughput scales almost linearly with the number of NICS on a server.",
                "The last word:  After years of broken promises, FPGA-based reconfigurable hardware finally becomes widely available in main stream data centers.",
                "Many significant workloads will be scrutinized to see whether they can benefit from reconfigurable hardware, and we expect much more fruitful work in this general direction."
            ],
            "author_id": "ACOLYER",
            "pdf_url": "https://lrita.github.io/images/blog/kv-direct.pdf",
            "author_full_name": "Adrian Colyer",
            "source_website": "https://blog.acolyer.org/about/",
            "id": 45795840
        }
    },
    "78860785": {
        "X": {
            "sections": [
                {
                    "text": "Higher-order organization of complex networks\nAustin R. Benson,1 David F. Gleich,2 Jure Leskovec3\u2217\n1Institute for Computational and Mathematical Engineering, Stanford University 2Department of Computer Science, Purdue University 3Computer Science Department, Stanford University\n\u2217To whom correspondence should be addressed; E-mail: jure@cs.stanford.edu\nNetworks are a fundamental tool for understanding and modeling complex systems in physics, biology, neuroscience, engineering, and social science. Many networks are known to exhibit rich, lower-order connectivity patterns that can be captured at the level of individual nodes and edges. However, higher-order organization of complex networks\u2014at the level of small network subgraphs\u2014 remains largely unknown. Here we develop a generalized framework for clustering networks based on higher-order connectivity patterns. This framework provides mathematical guarantees on the optimality of obtained clusters and scales to networks with billions of edges. The framework reveals higher-order organization in a number of networks including information propagation units in neuronal networks and hub structure in transportation networks. Results show that networks exhibit rich higher-order organizational structures that are exposed by clustering based on higher-order connectivity patterns.\n1\nar X\niv :1\n61 2.\n08 44\n7v 1\n[ cs\n.S I]\n2 6\nD ec\n2 01\n6\nNetworks are a standard representation of data throughout the sciences, and higher-order connectivity patterns are essential to understanding the fundamental structures that control and mediate the behavior of many complex systems (1\u20137). The most common higher-order structures are small network subgraphs, which we refer to as network motifs (Figure 1A). Network motifs are considered building blocks for complex networks (1, 8). For example, feedforward loops (Figure 1A M5) have proven fundamental to understanding transcriptional regulation networks (9), triangular motifs (Figure 1A M1\u2013M7) are crucial for social networks (4), open bidirectional wedges (Figure 1A M13) are key to structural hubs in the brain (10), and two-hop paths (Figure 1A M8\u2013M13) are essential to understanding air traffic patterns (5). While network motifs have been recognized as fundamental units of networks, the higher-order organization of networks at the level of network motifs largely remains an open question.\nHere we use higher-order network structures to gain new insights into the organization of complex systems. We develop a framework that identifies clusters of network motifs. For each network motif (Figure 1A), a different higher-order clustering may be revealed (Figure 1B), which means that different organizational patterns are exposed depending on the chosen motif.\nConceptually, given a network motif M , our framework searches for a cluster of nodes S with two goals. First, the nodes in S should participate in many instances of M . Second, the set S should avoid cutting instances of M , which occurs when only a subset of the nodes from a motif are in the set S (Figure 1B). More precisely, given a motif M , the higher-order clustering framework aims to find a cluster (defined by a set of nodes S) that minimizes the following ratio:\n\u03c6M(S) = cutM(S, S\u0304)/min(volM(S), volM(S\u0304)), (1)\nwhere S\u0304 denotes the remainder of the nodes (the complement of S), cutM(S, S\u0304) is the number of instances of motif M with at least one node in S and one in S\u0304, and volM(S) is the number of nodes in instances of M that reside in S. Equation 1 is a generalization of the conductance\n2\nmetric in spectral graph theory, one of the most useful graph partitioning scores (11). We refer to \u03c6M(S) as the motif conductance of S with respect to M .\nFinding the exact set of nodes S that minimizes the motif conductance is computationally infeasible (12). To approximately minimize Equation 1 and hence identify higher-order clusters, we develop an optimization framework that provably finds near-optimal clusters (Supplementary Materials (13)). We extend the spectral graph clustering methodology, which is based on the eigenvalues and eigenvectors of matrices associated with the graph (11), to account for higher-order structures in networks. The resulting method maintains the properties of traditional spectral graph clustering: computational efficiency, ease of implementation, and mathematical guarantees on the near-optimality of obtained clusters. Specifically, the clusters identified by our higher-order clustering framework satisfy the motif Cheeger inequality (14), which means that our optimization framework finds clusters that are at most a quadratic factor away from optimal.\nThe algorithm (illustrated in Figure 1C) efficiently identifies a cluster of nodes S as follows:\n\u2022 Step 1: Given a network and a motif M of interest, form the motif adjacency matrix WM\nwhose entries (i, j) are the co-occurrence counts of nodes i and j in the motif M :\n(WM)ij = number of instances of M that contain nodes i and j. (2)\n\u2022 Step 2: Compute the spectral ordering \u03c3 of the nodes from the normalized motif Laplacian\nmatrix constructed via WM (15).\n\u2022 Step 3: Find the prefix set of \u03c3 with the smallest motif conductance, formally: S :=\narg minr \u03c6M(Sr), where Sr = {\u03c31, . . . , \u03c3r}.\nFor triangular motifs, the algorithm scales to networks with billions of edges and typically only takes several hours to process graphs of such size. On smaller networks with hundreds\n3\nof thousands of edges, the algorithm can process motifs up to size 9 (13). While the worstcase computational complexity of the algorithm for triangular motifs is \u0398(m1.5) , where m is the number of edges in the network, in practice the algorithm is much faster. By analyzing 16 real-world networks where the number of edges m ranges from 159,000 to 2 billion we found the computational complexity to scale as \u0398(m1.2). Moreover, the algorithm can easily be parallelized and sampling techniques can be used to further improve performance (16).\nThe framework can be applied to directed, undirected, and weighted networks as well as motifs (13). Moreover, it can also be applied to networks with positive and negative signs on the edges, which are common in social networks (friend vs. foe or trust vs. distrust edges) and metabolic networks (edges signifying activation vs. inhibition) (13). The framework can be used to identify higher-order structure in networks where domain knowledge suggests the motif of interest. In the Supplementary Material (13) we also show that when domain-specific higherorder pattern is not known in advance, the framework can also serve to identify which motifs are important for the modular organization of a given network (13). Such a general framework allows for a study of complex higher-order organizational structures in a number of different networks using individual motifs and sets of motifs. The framework and mathematical theory immediately extend to other spectral methods such as localized algorithms that find clusters around a seed node (17) and algorithms for finding overlapping clusters (18). To find several clusters, one can use embeddings from multiple eigenvectors and k-means clustering (13,19) or apply recursive bi-partitioning (13, 20).\nThe framework can serve to identify higher-order modular organization of networks. We apply the higher-order clustering framework to the C. elegans neuronal network, where the fournode \u201cbi-fan\u201d motif (Figure 2A) is over-expressed (1). The higher-order clustering framework then reveals the organization of the motif within the C. elegans neuronal network. We find a cluster of 20 neurons in the frontal section with low bi-fan motif conductance (Figure 2B).\n4\nThe cluster shows a way that nictation is controlled. Within the cluster, ring motor neurons (RMEL/V/R), proposed pioneers of the nerve ring (21), propagate information to IL2 neurons, regulators of nictation (22), through the neuron RIH and several inner labial sensory neurons (Figure 2C). Our framework contextualizes the sifnifance of the bi-fan motif in this control mechanism.\nThe framework also provides new insights into network organization beyond the clustering of nodes based only on edges. Results on a transportation reachability network (23) demonstrate how it finds the essential hub interconnection airports (Figure 3). These appear as extrema on the primary spectral direction (Figure 3C) when two-hop motifs (Figure 3A) are used to capture highly connected nodes and non-hubs. (The first spectral coordinate of the normalized motif Laplacian embedding was positively correlated with the airport city\u2019s metropolitan population with Pearson correlation 99% confidence interval [0.33, 0.53]). The secondary spectral direction identified the West-East geography in the North American flight network (it was negatively correlated with the airport city\u2019s longitude with Pearson correlation 99% confidence interval [-0.66, -0.50]). On the other hand, edge-based methods conflate geography and hub structure. For example, Atlanta, a large hub, is embedded next to Salina, a non-hub, with an edge-based method (Figure 3D).\nOur higher-order network clustering framework unifies motif analysis and network partitioning\u2014\ntwo fundamental tools in network science\u2014and reveals new organizational patterns and modules in complex systems. Prior efforts along these lines do not provide worst-case performance guarantees on the obtained clustering (24), do not reveal which motifs organize the network (25), or rely on expanding the size of the network (26, 27). Theoretical results in the Supplementary Material (13) also explain why classes of hypergraph partitioning methods are more general than previously assumed and how motif-based clustering provides a rigorous framework for the special case of partitioning directed graphs. Finally, the higher-order net-\n5\nwork clustering framework is generally applicable to a wide range of networks types, including directed, undirected, weighted, and signed networks.\n6\n7\n8\n9"
                },
                {
                    "heading": "S1 Derivation and analysis of the motif-based spectral clus-",
                    "text": "tering method\nWe now cover the background and theory for deriving and understanding the method presented in the main text. We will start by reviewing the graph Laplacian and cut and volume measures for sets of vertices in a graph. We then define network motifs in Section S1.2 and generalizes the notions of cut and volume to motifs. Our new theory is presented in Section S1.6 and then we summarize some extensions of the method. Finally, we relate our method to existing methods for directed graph clustering and hypergraph partitioning.\nS1.1 Review of the graph Laplacian for weighted, undirected graphs\nConsider a weighted, undirected graphG = (V,E), with |V | = n. Further assume thatG has no isolated nodes. LetW encode the weights, of the graph, i.e., Wij = Wji = weight of edge (i, j).\nThe diagonal degree matrixD is defined asDii = \u2211n j=1Wij , and the graph Laplacian is defined as L = D \u2212W . We now relate these matrices to the conductance of a set S, \u03c6(G)(S):\n\u03c6(G)(S) = cut(G)(S, S\u0304)/min(vol(G)(S), vol(G)(S\u0304)), (S3)\ncut(G)(S, S\u0304) = \u2211\ni\u2208S, j\u2208S\u0304\nWij, (S4)\nvol(G)(S) = \u2211 i\u2208S Dii (S5)\nHere, S\u0304 = V \\S. (Note that conductance is a symmetric measure in S and S\u0304, i.e., \u03c6(G)(S) = \u03c6(G)(S\u0304).) Conceptually, the cut and volume measures are defined as follows:\ncut(G)(S, S\u0304) = weighted sum of weights of edges that are cut (S6)\nvol(G)(S) = weighted number of edge end points in S (S7)\nSince we have assumed G has no isolated nodes, vol(G)(S) > 0. If G is disconnected, then for any connected component C, \u03c6(G)(C) = 0. Thus, we usually consider breaking G into\nS10\nconnected components as a pre-processing step for algorithms that try to find low-conductance sets.\nWe now relate the cut metric to a quadratic form on L. Later, we will derive a similar form\nfor a motif cut measure. Note that for any vector y \u2208 Rn,\nyTLy = \u2211\n(i,j)\u2208E\nwij(yi \u2212 yj)2. (S8)\nNow, define x to be an indicator vector for a set of nodes S i.e., xi = 1 if node i is in S and xi = 0 if node i is in S\u0304. Note that if an edge (i, j) is cut, then xi and xj take different values and (xi \u2212 xj)2 = 1; otherwise, (xi \u2212 xj)2 = 0. Thus,\nxTLx = cut(G)(S, S\u0304). (S9)\nS1.2 Definition of network motifs\nWe now define network motifs as used in our work. We note that there are alternative definitions in the literature (1). We consider motifs to be a pattern of edges on a small number of nodes (see Figure S4). Formally, we define a motif on k nodes by a tuple (B,A), whereB is a k\u00d7k binary matrix and A \u2282 {1, 2, . . . , k} is a set of anchor nodes. The matrix B encodes the edge pattern between the k nodes, and A labels a relevant subset of nodes for defining motif conductance. In many cases, A is the entire set of nodes. Let \u03c7A be a selection function that takes the subset of a k-tuple indexed by A, and let set(\u00b7) be the operator that takes an (ordered) tuple to an (unordered) set. Specifically,\nset((v1, v2, . . . , vk)) = {v1, v2, . . . , vk}.\nThe set of motifs in an unweighted (possibly directed) graph with adjacency matrix A, denoted M(B,A), is defined by\nM(B,A) = {(set(v), set(\u03c7A(v))) | v \u2208 V k, v1, . . . , vk distinct, Av = B}, (S10)\nS11\nM3 M5M4M1 M2\nM8M7M6 M9 M10\nM11 M12 M13 Mbifan Medge\n+\ne\nd\nc\nb\na\nM2\nA\nB\nFigure S4: A: Illustration of network motifs used throughout the main text and supplementary material. The motif Medge is used to represent equivalence to undirected the graph. B: Diagram of motif definitions. The motif is defined by a binary matrix B and an anchor set of nodes. The figure shows an anchored version of motif M2 with anchors on the nodes that form the bi-directional edge. There are two instances of the motif in the graph on the right. Note that ({a, b, d}, {a, b}) is not included in the set of motif instances because the induced subgraph on the nodes a, b, and d is not isomorphic to the graph defined by B.\nwhere Av is the k \u00d7 k adjacency matrix on the subgraph induced by the k nodes of the ordered vector v. Figure S4 illustrates these definitions. The set operator is a convenient way to avoid duplicates when defining M(B,A) for motifs exhibiting symmetries. Henceforth, we will just use (v, \u03c7A(v)) to denote (set(v), set(\u03c7A(v))) when discussing elements of M(B,A). Furthermore, we call any (v, \u03c7A(v)) \u2208 M(B,A) a motif instance. When B and A are arbitrary or clear from context, we will simply denote the motif set by M .\nWe call motifs where \u03c7A(v) = v simple motifs and motifs where \u03c7A(v) 6= v anchored motifs. Motif analysis in the literature has mostly analyzed simple motifs (29). However, the anchored motif provides us with a more general framework, and we use an anchored motif for\nS12\nthe analysis of the transportation reachability network.\nOften, a distinction is made between a functional and a structural motif (30) (or a subgraph and an induced subgraph (31)) to distinguish whether a motif specifies simply the existence of a set of edges (functional motif or subgraph) or the existence and non-existence of edges (structural motif or induced subgraph). By the definition in Equation S10, we refer to structural motifs in this work. Note that functional motifs consist of a set of structural motifs. Our clustering framework allows for the simultaneous consideration of several motifs (see Section S1.9), so we have not lost any generality in our definitions.\nS1.3 Definition of motif conductance\nRecall that the key definitions for defining conductance are the notions of cut and volume. For an unweighted graph, these are\n\u03c6(G)(S, S\u0304) = cut(G)(S, S\u0304)/min(vol(G)(S), vol(G)(S\u0304)), (S11)\ncut(G)(S, S\u0304) = number of edges cut, (S12)\nvol(G)(S) = number of edge end points in S. (S13)\nOur conceptual definition of motif conductance simply replaces an edge with a motif instance of type M :\n\u03c6 (G) M (S) = cut (G) M (S, S\u0304)/min(vol (G) M (S), vol (G) M (S\u0304)), (S14)\ncut(G)M (S, S\u0304) = number of motif instances cut, (S15)\nvol(G)M (S) = number of motif instance end points in S. (S16)\nWe say that a motif instance is cut if there is at least one anchor node in S and at least one\nS13\nanchor node in S\u0304. We can formalize this when given a motif set M as in Equation S10:\ncut(G)M (S, S\u0304) = \u2211\n(v,\u03c7A(v))\u2208M\n1(\u2203 i, j \u2208 \u03c7A(v) | i \u2208 S, j \u2208 S\u0304), (S17)\nvol(G)M (S) = \u2211\n(v,\u03c7A(v))\u2208M \u2211 i\u2208\u03c7A(v) 1(i \u2208 S), (S18)\nwhere 1(s) is the truth-value indicator function on s, i.e., 1(s) takes the value 1 if the statement s is true and 0 otherwise. Note that Equation S17 makes explicit use of the anchor set A. The motif cut measure only counts an instance of a motif as cut if the anchor nodes are separated, and the motif volume counts the number of anchored nodes in the set. However, two nodes in an achor set may a part of several motif instances. Specifically, following the definition in Equation S10, there may be many different v with the same \u03c7A(v), and the nodes in \u03c7A(v) still get counted proportional to the number of motif instances.\nS1.4 Definition of the motif adjacency matrix and motif Laplacian\nGiven an unweighted, directed graph and a motif set M , we conceptually define the motif adjacency matrix by\n(WM)ij = number of motif instances in M where i and j participate in the motif. (S19)\nOr, formally,\n(WM)ij = \u2211\n(v,\u03c7A(v))\u2208M\n1({i, j} \u2282 \u03c7A(v)), (S20)\nfor i 6= j. Note that weight is added to (WM)ij only if i and j appear in the anchor set. This is important for the transportation reachability network analyzed in the main text and in Section S6, where weight is added between cities i and j based on the number of intermediary cities that can be traversed between them.\nNext, we define the motif diagonal degree matrix by (DM)ii = \u2211n j=1(WM)ij and the motif Laplacian as LM = DM \u2212 WM . Finally, the normalized motif Laplacian is LM =\nS14\nD \u22121/2 M LMD \u22121/2 M = I \u2212D \u22121/2 M WMD \u22121/2 M . The theory in the next section will examine quadratic forms LM and derive the main clustering method that uses an eigenvector of LM .\nS1.5 Algorithm for finding a single cluster\nWe are now ready to describe the algorithm for finding a single cluster in a graph. The algorithm finds a partition of the nodes into S and S\u0304. The motif conductance is symmetric in the sense that \u03c6(G)M (S) = \u03c6 (G) M (S\u0304), so either set of nodes (S or S\u0304) could be interpreted as a cluster. However, in practice, it is common that one set is substantially smaller than the other. We consider this smaller set to represent a module in the network. The algorithm is based on the Fiedler partition (32) of the motif weighted adjacency matrix and is presented below in Algorithm 1.1\nAlgorithm 1: Motif-based clustering algorithm for finding a single cluster. Input: Directed, unweighted graph G and motif M Output: Motif-based cluster (subset of nodes in G) (WM)ij \u2190 number of instances of M that contain nodes i and j. GM \u2190 weighted graph induced WM DM \u2190 diagonal matrix with (DM)ii = \u2211 j(WM)ij\nz \u2190 eigenvector of second smallest eigenvalue for LM = I \u2212D\u22121/2M WMD \u22121/2 M \u03c3i \u2190 to be index of D\u22121/2M z with ith smallest value /* Sweep over all prefixes of \u03c3 */ S \u2190 arg minl \u03c6(GM )(Sl), where Sl = {\u03c31, . . . , \u03c3l} if |S| < |S\u0304| then\nreturn S else\nreturn S\u0304\nIt is often informative to look at all conductance values found from the sweep procedure. We refer to a plot of \u03c6(GM )(Sl) versus l as a sweep profile plot. In the following subection, we show that when the motif has three nodes, \u03c6(GM )(Sl) = \u03c6 (G) M (Sl). In this case, the sweep profile shows how motif conductance varies with the size of the sets in Algorithm 1.\nIn the following subsection, we show that when the motif M has three nodes, the cluster 1An implementation of Algorithm 1 is available in SNAP. See http://snap.stanford.edu/ higher-order/.\nS15\nsatisfies \u03c6(G)M (S) \u2264 4 \u221a \u03c6\u2217, where \u03c6\u2217 is the smallest motif conductance over all sets of nodes. In other words, the cluster is nearly optimal. Later, we extend this algorithm to allow for signed, colored, and weighted motifs and to simultaneously finding multiple clusters.\nS1.6 Motif Cheeger inequality for network motifs with three nodes\nWe now derive the motif Cheeger inequality for simple three-node motifs, or, in general, motifs with three anchor nodes. The crux of this result is deriving a relationship between the motif conductance function and the weighted motif adjacency matrix, from which the Cheeger inequality is essentially a corollary. For the rest of this section, we will use the following notation. Given an unweighted, directed G and a motif M , the corresponding weighted graph defined by Equation S20 is denoted by GM .\nThe following Lemma relates the motif volume to the volume in the weighted graph. This lemma applies to any anchor setA consisting of at least two nodes. For our main result, we will apply the lemma assuming |A| = 3. However, we will apply the lemma more generally when discussing four node motifs in Section S1.7.\nLemma 1. Let G = (V,E) be a directed, unweighted graph and let GM be the weighted graph for a motif on k nodes and |A| \u2265 2 anchor nodes. Then for any S \u2282 V ,\nvol(G)M (S) = 1\n|A| \u2212 1 vol(GM )(S)\nProof. Consider an instance (v, \u03c7A(v)) of a motif. Let (u1, . . . , u|A|) = \u03c7A(v). By Equation S20, (WM)u1,j is incremented by one for j = u2, . . . , u|A|. Since (DM)u1,u1 = \u2211 j (WM)u1,j , the motif end point u1 is counted |A| \u2212 1 times.\nS16\nxi 2 + xj 2 + xk 2 - xixj - xjxk -xkxi =\nxi = 1\nxj = 1 xk = 1\ni\nj k\nxi = -1\nxj = -1 xk = -1\ni\nj k\nxi = 1\nxj = 1 xk = -1\ni\nj k\nxi = 1\nxj = -1 xk = -1\ni\nj k\n0\n0\n4\n4\nii\nkj\nl\nxj = 1 xk = 1\nxi = 1 xl = 1\nii\nkj\nl\nxj = -1 xk = -1\nxi = -1 xl = -1\nii\nkj\nl\nxj = 1 xk = -1\nxi = 1 xl = 1\nii\nkj\nl\nxj = -1 xk = -1\nxi = 1 xl = -1\nii\nkj\nl\nxj = 1 xk = -1\nxi = 1 xl = -1\n0\n0\n6\n6\n8\n6 - xixj - xixk - xixl - xjxk - xjxl - xkxl =BA\nFigure S5: Illustrations of the quadratic forms on indicator functions for set assignment. Here, the blue nodes have assignment to set S and the green nodes have assignment to set S\u0304. The quadratic function gives the penalty for cutting that motif. A: Illustration of Equation S21. The quadratic form is proportional to the indicator on whether or not the motif is cut. B: Illustration of Equation S22. The quadratic form is equal to zero when all nodes are in the same set. However, the form penalizes 2/2 splits more than 3/1 splits.\nThe following lemma states that the truth value for determining whether three binary variables in {\u22121, 1} are not all equal is a quadratic function of the variables (see Figure S5). Because this function is quadratic, we will be able to relate motif cuts on three nodes to a quadratic form on the motif Laplacian.\nS17\nLemma 2. Let xi, xj, xk \u2208 {\u22121, 1}. Then\n4 \u00b7 1(xi, xj, xk not all the same) = x2i + x2j + x2k \u2212 xixj \u2212 xjxk \u2212 xkxi.\nIt will be easier to derive our results with binary indicator variables taking values in {\u22121, 1}. However, in terms of the quadratic form on the Laplacian, we have already seen how indicator vectors taking values in {0, 1} relate to the cut value (Equation S9). The following lemma shows that the {0, 1} and {\u22121, 1} indicator vectors are equivalent, up to a constant, for defining the cut measure in terms of the Laplacian.\nLemma 3. Let z \u2208 {0, 1}n and define x by xi = 1 if zi = 1 and xi = \u22121 if zi = 0. Then for any graph Laplacian L = D \u2212W , 4zTLz = xTLx.\nProof.\nxTLx = \u2211\n(i,j)\u2208E\nWij(xi \u2212 xj)2 = \u2211\n(i,j)\u2208E\nWij4(zi \u2212 zj)2 = 4zTLz.\nThe next lemma contains the essential result that relates motif cuts in the original graph G to weighted edge cuts in GM . In particular, the lemma shows that the motif cut measure is proportional to the cut on the weighted graph defined in Equation S19 when there are three anchor nodes.\nLemma 4. Let G = (V,E) be a directed, unweighted graph and let GM be the weighted graph for a motif with |A| = 3. Then for any S \u2282 V ,\ncut(G)M (S, S\u0304) = 1\n2 cut(GM )(S, S\u0304)\nS18\nProof. Let x \u2208 {\u22121, 1}n be an indicator vector of the node set S.\n4 \u00b7 cut(G)M (S, S\u0304) = \u2211\n(v,{i,j,k})\u2208M\n4 \u00b7 1(xi, xj, xk not all the same)\n= \u2211\n(v,{i,j,k})\u2208M\n( x2i + x 2 j + x 2 k ) \u2212 (xixj + xjxk + xkxi)\n= 1\n2 xTDMx\u2212\n1 2 xTWMx\n= 1\n2 xTLMx\n= 2 \u00b7 cut(GM )(S, S\u0304).\nThe first equality follows from the definition of cut motifs (Equation S17). The second equality follows from Lemma 2. The third equality follows from Lemma 1 and Equation S20. The fourth equality follows from the definition of LM . The fifth equality follows from Lemma 3.\nWe are now ready to prove our main result, namely that motif conductance on the original graph G is equivalent to conductance on the weighted graph GM when there are three anchor nodes. The result is a consequence of the volume and cut relationships provided by Lemmas 1 and 4.\nTheorem 5. Let G = (V,E) be a directed, unweighted graph and let WM be the weighted adjacency matrix for any motif with |A| = 3. Then for any S \u2282 V ,\n\u03c6 (G) M (S) = \u03c6 (GM )(S)"
                },
                {
                    "heading": "In other words, when the number of anchor nodes is 3, the motif conductance is equal to the",
                    "text": "conductance on the weighted graph defined by Equation S19.\nProof. When |A| = 3, the motif cut and motif volume are both equal to half the motif cut and motif volume measures by Lemmas 1 and 4.\nS19\nFor any motif with three anchor nodes, conductance on the weighted graph is equal to the motif conductance. Because of this, we can use results from spectral graph theory for weighted graphs (32) and re-interpret the results in terms of motif conductance. In particular, we get the following \u201cmotif Cheeger inequality\u201d.\nTheorem 6. Motif Cheeger Inequality. Suppose we use Algorithm 1 to find a low-motif conductance set S. Let \u03c6\u2217 = minS\u2032 \u03c6 (G) M (S \u2032) be the optimal motif conductance over any set of nodes"
                },
                {
                    "heading": "S \u2032. Then",
                    "text": "1. \u03c6(G)M (S) \u2264 4 \u221a \u03c6\u2217 and\n2. \u03c6\u2217 \u2265 \u03bb2/2\nProof. The result follows from Theorem 5 and the standard Cheeger ineqaulity (32).\nThe first part of the result says that the set of nodes S is within a quadratic factor of optimal. This provides the mathematical guarantees that our procedure finds a good cluster in a graph, if one exists. The second result provides a lower bound on the optimal motif conductance in terms of the eigenvalue. We use this bound in our analysis of a food web (see Section S7.1) to show that certain motifs do not provide good clusters, regardless of the procedure to select S.\nS1.7 Discussion of motif Cheeger inequality for network motifs with four or more nodes\nAnalogs of the indicator function in Lemma 2 for four or more variables are not quadratic. Subsequently, for motifs with |A| > 3, we no longer get the motif Cheeger inequalities guaranteed by Theorem 6. That being said, solutions found by motif-based partitioning approximate a related value of conductance. We now provide the details.\nWe begin with a lemma that shows a functional form for four binary variables taking values\nin {\u22121, 1} to not all be equal. We see that it is quartic, not quadratic.\nS20\nLemma 7. Let xi, xj, xk, xl \u2208 {\u22121, 1}. Then the indicator function on all four elements not being equal is\n8 \u00b7 1(xi, xj, xk, xl not all the same) (S21)\n= (7\u2212 xixj \u2212 xixk \u2212 xixl \u2212 xjxk \u2212 xjxl \u2212 xkxl \u2212 xixjxkxl) .\nWe almost have a quadratic form, if not for the quartic term xixjxkxl. However, we could\nuse the following related quadratic form:\n6\u2212 xixj \u2212 xixk \u2212 xixl \u2212 xjxk \u2212 xjxl \u2212 xkxl\n=  0 xi, xj, xk, xl are all the same 6 exactly three of xi, xj, xk, xl are the same 8 exactly two of xi, xj, xk, xl are \u22121.\n(S22)\nThe quadratic still takes value 0 if all four entries are the same, and takes a non-zero value otherwise. However, the quadratic takes a larger value if exactly two of the entries are \u22121. Figure S5 illustrates this idea. From this, we can provide an analogous statement to Lemma 4 for motifs with |A| = 4.\nLemma 8. Let G = (V,E) be a directed, unweighted graph and let GM be the weighted graph for a motif with |A| = 4. Then for any S \u2282 V ,\ncut(G)M (S, S\u0304) = 1\n3 cut(GM )(S, S\u0304)\u2212 \u2211 (v,{i,j,k,l})\u2208M 1 3 \u00b7 1(exactly two of i, j, k, l in S)\nS21\nProof. Let x \u2208 {\u22121, 1}n be an indicator vector of the node set S.\n6 \u00b7 cut(G)M (S, S\u0304) + \u2211\n(v,{i,j,k,l})\u2208M\n2 \u00b7 1(exactly two of i, j, k, l in S)\n= \u2211\n(v,{i,j,k,l})\u2208M\n6\u2212 xixj \u2212 xixk \u2212 xixl \u2212 xjxk \u2212 xjxl \u2212 xkxl\n= \u2211\n(v,{i,j,k,l})\u2208M\n3 2\n( x2i + x 2 j + x 2 k + x 2 l ) \u2212 (xixj + xixk + xixl + xjxk + xjxl + xkxl)\n= 1\n2 xTDMx\u2212\n1 2 xTWMx\n= 1\n2 xTLMx\n= 2 \u00b7 cut(GM )(S, S\u0304).\nThe first equality follows from Equations S17 and S22. The third equality follows from Lemma 1. The fourth equality follows from the definition ofLM . The fifth equality follows from Lemma 3.\nWith four anchor nodes, the motif cut in G is slightly different than the weighted cut in the weighted graph GM . However, Lemma 1 says that the motif volume in G is still the same as the weighted volume in GM . We use this to derive the following result.\nTheorem 9. Let G = (V,E) be a directed, unweighted graph and let WM be the weighted adjacency matrix for any motif with |A| = 4. Then for any S \u2282 V ,\n\u03c6 (G) M (S) = \u03c6\n(GM )(S)\u2212 \u2211 (v,{i,j,k,l})\u2208M 1(exactly two of i, j, k, l in S)\nvol(GM )(S)"
                },
                {
                    "heading": "In other words, when there are four anchor nodes, the weighting scheme in Equation S19 models",
                    "text": "the exact conductance with an additional penalty for splitting the four anchor nodes into two groups of two.\nProof. This follows from Lemmas 1 and 8.\nS22\nTo summarize, we still get a Cheeger inequality from the weighted graph, but it is in terms of a penalized version of the motif conductance \u03c6(G)M (S). However, the penalty makes sense\u2014if the group of four nodes is \u201cmore split\u201d (2 and 2 as opposed to 3 and 1), the penalty is larger. When |A| > 4, we can derive similar penalized approximations to \u03c6(G)M (S).\nS1.8 Methods for simultaneously finding multiple clusters\nFor clustering a network into k > 2 clusters based on motifs, we could recursively cut the graph using the sweep procedure with some stopping criterion (20). For example, we could continue to cut the largest remaining cluster until the graph is partitioned into some pre-specified number of clusters. We refer to this method as recursive bi-partitioning.\nIn addition, we can use the following method of Ng et al. (19). Algorithm 2: Motif-based clustering algorithm for finding several clusters.\nInput: Directed, unweighted graph G, motif M , number of clusters k Output: k disjoint motif-based clusters (WM)ij \u2190 number of instances of M that contain nodes i and j. DM \u2190 diagonal matrix with (DM)ii = \u2211 j(WM)ij z1, . . . , zk \u2190 eigenvectors of k smallest eigenvalues for LM = I \u2212D\u22121/2M WMD \u22121/2 M\nYij \u2190 zij/ \u221a\u2211k j=1 z 2 ij Embed node i into Rk by taking the ith row of the matrix Y Run k-means clustering on the embedded nodes This method does not have the same Cheeger-like guarantee on quality. However, recent theory shows that by replacing k-means with a different clustering algorithm, there is a performance guarantee (33). While this provides motivation, we use k-means for its simplicity and empirical success.\nS1.9 Extensions of the method for simultaneously analyzing several network motifs\nAll of our results carry through when considering several motifs simultaneously. In particular, suppose we are interested in clustering based on motif sets M1, . . . ,Mq for q different motifs.\nS23\nFurther suppose that we want to weight the impact of some motifs more than other motifs. Let WMj be the weighted adjacency matrix for motifMj , j = 1, . . . , q, and let \u03b1j \u2265 0 be the weight of motif Mj , then we can form the weighted adjacency matrix\nWM = q\u2211 j=1 \u03b1jWMj . (S23)\nNow, the cut and volume measures are simply weighted sums by linearity. Suppose that the Mj all have three anchor nodes and let GM be the weighted graph corresponding to WM . Then\ncut(GM )(S, S\u0304) = q\u2211 j=1 \u03b1jcut (G) Mj (S, S\u0304), vol(GM )(S) = q\u2211 j=1 \u03b1jvol (G) Mj (S),\nand Theorem 6 applies to a weighted motif conductance equal to\u2211q j=1 \u03b1jcut (G) Mj (S, S\u0304)\nmin (\u2211q\nj=1 \u03b1jvol (G) Mj\n(S), \u2211q\nj=1 \u03b1jvol (G) Mj\n(S\u0304) ) .\nS1.10 Extensions of the method to signed, colored, and weighted motifs\nOur results easily generalize for signed networks. We only have to generalize Equation S10 by allowing the adjacency matrix B to be signed. Extending the method for motifs where the edges or nodes are \u201ccolored\u201d or \u201clabeled\u201d is similar. If the edges are colored, then we again just allow the adjacency matrix B to capture this information. If the nodes in the motif are colored, we only count motif instances with the specified pattern.\nWe can also generalize the notions of motif cut and motif volume for \u201cweighted motifs\u201d, i.e., each motif has an associated nonnegative weight. Let \u03c9(v,\u03c7A(v)) be the weight of a motif instance. Our cut and volume metrics are then\ncut(G)M (S, S\u0304) = \u2211\n(v,\u03c7A(v))\u2208M\n\u03c9(v,\u03c7A(v)) 1(\u2203 i, j \u2208 \u03c7A(v) | i \u2208 S, j \u2208 S\u0304),\nvol(G)M (S) = \u2211\n(v,\u03c7A(v))\u2208M\n\u03c9(v,\u03c7A(v)) \u2211\ni\u2208\u03c7A(v)\n1(i \u2208 S).\nS24\nSubsequently, we adjust the motif adjacency matrix as follows:\n(WM)ij = \u2211\n(v,\u03c7A(v))\u2208M\n\u03c9(v,\u03c7A(v)) 1({i, j} \u2282 \u03c7A(v)) (S24)\nS1.11 Connections to directed graph partitioning\nOur framework also provides a way to analyze methods for clustering directed graphs. Existing principled generalizations of undirected graph partitioning to directed graph partitioning proceed from graph circulations (34) or random walks (35) and are difficult to interpret. Our motif-based clustering framework provides a simple, rigorous framework for directed graph partitioning. For example, consider the common heuristic of clustering the symmetrized graph W = A + AT , where A is the (directed) adjacency matrix (36). Following Theorem 5, conductance-minimizing methods for partitioningW are actually trying to minimize a weighted sum of motif-based conductances for the directed edge motif and the bi-directional edge motif:\nB1 = [ 0 1 0 0 ] , B2 = [ 0 1 1 0 ] ,\nwhere both motifs are simple (A = {1, 2}). If W1 and W2 are the motif adjacency matrices for B1 and B2, then A + AT = W = W1 + 2W2. This weighting scheme gives a weight of two to bi-directional edges in the original graph and a weight of one to uni-directional edges.\nAn alternative strategy for clustering a directed graph is to simply remove the direction on all edges, treating bi-directional and uni-directional edges the same. The resulting adjacency matrix is equivalent to the motif adjacency matrix for the bi-directional and uni-directional edges (without any relative weighting). Formally, W = W1 + W2. We refer to this \u201cmotif\u201d as Medge (Figure S4), which will later provide a convenient notation when discussing both motifbased clustering and edge-based clustering.\nS25\nS1.12 Connections to hypergraph partitioning\nFinally, we contextualize our method in the context of existing literature on hypergraph partitioning. The problem of partitioning a graph based on relationships between more than two nodes has been studied in hypergraph partitioning (37), and we can interpret motifs as hyperedges in a graph. In contrast to existing hypergraph partitioning problems, we induce the hyperedges from motifs rather than take the hyperedges as given a priori. The goal with our analysis of the Florida Bay food web, for example, was to find which hyperedge sets (induced by a motif) provide a good clustering of the network (see Section S7.1).\nIn general, our motif-based spectral clustering methodology falls into the area of encoding a hypergraph partitioning problem by a graph partitioning problem (38, 39). With simple motifs on k nodes, the motif Laplacian LM formed from WM (Equation S20) is a special case of the Rodr\u0131\u0301guez Laplacian (38, 40) for k-regular hypergraphs. The motif Cheeger inequality we proved (Theorem 6) explains why this Laplacian is appropriate for 3-regular hypergraphs. Specifically, it respects the standard cut and volume metrics for graph partitioning."
                },
                {
                    "heading": "S2 Computational complexity and scalability of the method",
                    "text": "We now analyze the computation of the higher-order clustering method. We first provide a theoretical analysis of the computational complexity, which depends on motif. After, we empirically analyze the time to find clusters for triangular motifs on a variety of real-world networks, ranging in size from a few hundred thousand edges to nearly two billion edges. Finally, we show that we can practically compute the motif adjacency matrix for motifs up to size 9 on a number of real-world networks.\nS26\nS2.1 Analysis of computational complexity\nWe now analyze the computational complexity of the algorithm presented in Theorem 6. Overall, the complexity of the algorithm is governed by the computations of the motif adjacency matrix WM , an eigenvector, and the sweep cut procedure. For simplicity, we assume that we can access edges in a graph in O(1) time and access and modify matrix entries in O(1) time. Let m and n denote the number of edges in the graph. Theoretically, the eigenvector can be computed in O((m + n)(log n)O(1)) time using fast Laplacian solvers (41). For the sweep cut, it takes O(n log n) to sort the indices given the eigenvector using a standard sorting algorithm such is merge sort. Computing motif conductance for each set Sr in the sweep also takes linear term. In pratice, the sweep cut step takes a small fraction of the total running time of the algorithm. For the remainder of the analysis, we consider the more nuanced issue of the time to compute WM .\nThe computational time to formWM is bounded by the time to find all instances of the motif in the graph. Naively, for a motif on k nodes, we can compute WM in \u0398(nk) time by checking each k-tuple of nodes. Furthermore, there are cases where there are \u0398(nk) motif instances in the graph, e.g., there are \u0398(n3) triangles in a complete graph. However, since most real-world networks are sparse, we instead focus on the complexity of algorithms in terms of the number of edges and the maximum degree in the graph. For this case, there are several efficient practical algorithms for real networks with available software (42\u201346).\nTheoretically, motif counting is efficient. Here we consider four classes of motifs: (1) triangles, (2) wedges (connected, non-triangle three-node motifs), (3) four-node motifs, and (4) k-cliques. Let m be the number of edges in a graph. Latapy analyzed a number of algorithms for listing all triangles in an undirected network, including an algorithm that has computational complexity \u0398(m1.5) (47). For a directed graph G, we can use the following algorithm: (1) form a new graph Gundir by removing the direction from all edges in G (2) find all triangles in Gundir,\nS27\n(3) for every triangle in Gundir, check which directed triangle motif it is in G. Since step 1 is linear and we can perform the check in step 3 in O(1) time, the same \u0398(m1.5) complexity holds for directed networks. This analysis holds regardless of the structure of the networks. However, additional properties of the network can lead to improved algorithms. For example, in networks with a power law degree sequence with exponent greater than 7/2, Berry et al. provide a randomized algorithm with expected running time \u0398(m) (48). In the case of a bounded degree graph, enumerating over all nodes and checking all pairs of neighbors takes time \u0398(nd2max), where dmax is the maximum degree in the graph. We note that with triangular motifs, the number of non-zeros in WM is less than the number of non-zeros in the original adjacency matrix. Thus, we do not have to worry about additional storage requirements.\nNext, we consider wedges (open triangles). We can list all wedges by looking at every pair of neighbors of every node. This algorithm has \u0398(nd2max) computational complexity, where n is the number of nodes and dmax is again the maximum degree in the graph (a more precise bound\nis \u0398( \u2211\nj d 2 j), where dj is the degree of node j.) If the graph is sparse, the motif adjacency\nmatrix will have more non-zeros than the original adjacency matrix, so additional storage is required. Specifically, there is fill-in for all two-hop neighbors, so the motif adjacency matrix\nhas O( \u2211\nj d 2 j) non-zeros. This is impractical for large real-world networks but manageable for\nmodestly sized networks.\nMarcus and Shavitt present an algorithm for listing all four-node motifs in an undirected graph in O(m2) time (49). We can employ the same edge direction check as for triangles to extend this result to directed graphs. Chiba and Nishizeki develop an algorithm for finding a representation of all quadrangles (motif on four nodes that contains a four-node cycle as a subgraph) in O(am) time and O(m) space, where a is the arboricity of the graph (50). The arboricity of any connected graph is bounded byO(m1/2), so this algorithm runs in timeO(m3/2).\nChiba and Nishizeki present an algorithms for k-clique enumeration that also depends on\nS28\nthe arboricity of the graph. Specifically, they provide an algorithm for enumerating all k-cliques inO(kak\u22122m) time, where a is the arboricity of the graph. This algorithm achieves the \u0398(m3/2) bound for arbitrary graphs. (We note that the triangle listing sub-case is similar in spirit to the algorithm proposed by Schank and Wagner (51)). For four-node cliques, the algorithm runs in time O(m2) time, which matches the complexity of Marcus and Shavitt (49).\nWe note that we could also employ approximation algorithms to estimate the weights in the motif adjacency matrix (52). Such methods balance computation time and accuracy. Finally, we note that the computation of WM and the computation of the eigenvector are suitable for parallel computation. There are already distributed algorithms for triangle enumeration (53), and the (parallel) eigenvector computation of a sparse matrix is a classical problem in scientific computing (54, 55).\nS2.2 Experimental results on triangular motifs\nIn this section, we demonstrate that our method scales to real-world networks with billions of edges. We tested the scalability of our method on 16 large directed graphs from a variety of real-world applications. These networks range from a couple hundred thousand to two billion edges and from 10 thousand to over 50 million nodes. Table S1 lists short descriptions of these networks. The wiki-RfA, email-EuAll, cit-HepPh, web-NotreDame, amazon0601, wiki-Talk, ego-Gplus, soc-Pokec, and soc-LiveJournal1 networks were downloaded from the SNAP collection at http://snap.stanford.edu/data/ (56). The uk-2014tpd, uk-2014-host, enwiki-2013, uk-2002, arabic-2005, twitter-2010, and sk-2005 networks were downloaded from the Laboratory for Web Algorithmics collection at http://law.di. unimi.it/datasets.php (57\u201360). Links to all datasets are available on our project website: http://snap.stanford.edu/higher-order/.\nRecall that Algorithm 1 consists of two major computational components:\nS29\n1. Form the weighted graph WM .\n2. Compute the eigenvector z of second smallest eigenvalue of the matrix LM .\nAfter computing the eigenvector, we sort the vertices and loop over prefix sets to find the lowest motif conductance set. We consider these final steps as part of the eigenvector computation for our performance experiments.\nFor each network in Table S1, we ran the method for all directed triangular motifs (M1\u2013 M7). To compute WM , we used a standard algorithm that meets the O(m3/2) bound (47, 51) with some additional pre-processing based on the motif. Specifically, the algorithm is:\n1. Take motif type M and graph G as input.\n2. (Pre-processing.) If M is M1 or M5, remove all bi-directional edges in G since these\nmotifs only contain uni-directional edges. If M is M4, remove all uni-directional edges in G as this motif only contains bi-directional edges.\n3. Form the undirected graph Gundir by removing the direction of all edges in G.\n4. Let du be the degree of node u in Gundir. Order the nodes in Gundir by increasing degree,\nbreaking ties arbitrarily. Denote this ordering by \u03c8.\n5. For every edge undirected edge {u, v} in Gundir, if \u03c8u < \u03c8v, add directed edge (u, v) to\nGdir; otherwise, add directed edge (v, u) to Gdir.\n6. For every node in u in Gdir and every pair of directed edges (u, v) and (u,w), check to\nsee if u, v, and w form motif M in G. If they do, check if the triangle forms motif M in G and update WM accordingly.\nThe algorithm runs in time \u0398(m3/2) time in the worst case, and is also known as an effective heuristic for real-world networks (48). After, we find the largest connected component of the\nS30\ngraph corresponding to the motif adjacency matrix WM , form the motif normalized Laplacian LM of the largest component, and compute the eigenvector of second smallest eigenvalue of LM . To compute the eigenvector, we use MATLAB\u2019s eigs routine with tolerance 1e-4 and the \u201csmallest algebraic\u201d option for the eigenvalue type.\nTable S2 lists the time to compute WM and the time to compute the eigenvector for each network. We omitted the time to read the graph from disk because this time strongly depends on how the graph is compressed. All experiments ran on a 40-core server with four 2.4 GHz Intel Xeon E7-4870 processors. All computations of WM were in serial and the computations of the eigenvectors were in parallel.\nOver all networks and all motifs, the longest computation of WM (including pre-processing time) was for M2 on the sk-2005 network and took roughly 52.8 hours. The longest eigenvector computation was for M6 on the sk-2005 network, and took about 1.62 hours. We note that WM only needs to be computed once per network, regardless of the eventual number of clusters that are extracted. Also, the computation ofWM can easily be accelerated by parallel computing (the enumeration of motifs can be done in parallel over nodes, for example) or by more sophisticated algorithms (48). In this work, we perform the computation of WM in serial in order to better understand the scalability.\nIn theory, the triangle enumeration time is O(m1.5). We fit a linear regression of the log of the computation time of the last step of the enumeration algorithm to the regressor log(m) and a constant term:\nlog(time) \u223c a log(m) + b (S25)\nIf the computations truly took cm1.5 for some constant c, then the regression coefficient for log(m) would be 1.5. Because of the pre-processing of the algorithm, the number of edges m depends on the motif. For example, with motifs M1 and M5, we only count the number of uni-directional edges. The pre-processing time, which is linear in the total number of edges,\nS31\nis not included in the time. The regression coefficient for log(m) (a in Equation S25) was found to be smaller 1.5 for each motif (Table S3). The largest regression coefficient was 1.31 for M3 (with 95% confidence interval 1.31 \u00b1 0.19). We also performed a regression over the aggregate times of the motifs, and the regression coefficient was 1.17 (with 95% confidence interval 1.17 \u00b1 0.09). We conclude that on real-world datasets, the algorithm for computing WM performs much better than the worst-case guarantees.\nTable S1: Summary of networks used in scalability experiments with triangular motifs. The total number of edges is the sum of the number of unidirectional edges and twice the number of bidirectional edges.\nName description # nodes # edges total unidir. bidir. wiki-RfA Adminship voting on Wikipedia 10.8K 189K 175K 7.00K email-EuAll Emails in a research institution 265K 419K 310K 54.5K cit-HepPh Citations for papers on arXiv HEP-PH 34.5K 422K 420K 657 web-NotreDame Hyperlinks on nd.edu domain 326K 1.47M 711K 380K amazon0601 Product co-purchasing on Amazon 403K 3.39M 1.50M 944K wiki-Talk Wikipedia users interactions 2.39M 5.02M 4.30M 362K ego-Gplus Circles on Google+ 108K 13.7M 10.8M 1.44M uk-2014-tpd top private domain links on .uk web 1.77M 16.9M 13.7M 1.58M soc-Pokec Pokec friendships 1.63M 30.6M 14.0M 8.32M uk-2014-host Host links on .uk web 4.77M 46.8M 33.7M 6.55M soc-LiveJournal1 LiveJournal friendships 4.85M 68.5M 17.2M 25.6M enwiki-2013 Hyperlinks on English Wikipedia 4.21M 101M 82.6M 9.37M uk-2002 Hyperlinks on .uk web 18.5M 292M 231M 30.5M arabic-2005 Hyperlinks on arabic-language web pages 22.7M 631M 477M 77.3M twitter-2010 Twitter followers 41.7M 1.47B 937M 266M sk-2005 Hyperlinks on .sk web 50.6M 1.93B 1.69B 120M\nS32\nTable S2: Time to compute the motif adjacency matrix WM and the second eigenvector of the motif normalized Laplacian LM in seconds for each directed triangular motif.\nMotif adjacency matrix WM Second eigenvector of LM Network M1 M2 M3 M4 M5 M6 M7 M1 M2 M3 M4 M5 M6 M7 wiki-RfA 1.19e+00 2.67e+00 1.71e+00 2.06e-02 1.79e+00 2.42e+00 2.35e+00 1.14e-01 2.12e-01 1.22e-01 2.12e-01 2.12e-01 2.94e-01 2.93e-01 email-EuAll 4.74e-01 8.29e-01 6.26e-01 2.46e-01 5.02e-01 5.40e-01 5.41e-01 2.29e-01 1.62e-01 2.43e-01 1.62e-01 1.62e-01 2.35e-01 1.92e-01 cit-HepPh 7.65e+00 3.36e+00 2.73e+00 6.22e+00 8.20e+00 3.29e+00 3.35e+00 2.11e+00 2.10e+00 2.11e+00 2.10e+00 2.10e+00 2.24e+00 2.30e+00 web-NotreDame 9.42e-01 2.39e+01 2.33e+01 2.30e+00 1.17e+00 8.29e+00 8.40e+00 1.86e-01 3.62e-01 5.97e-01 3.62e-01 3.62e-01 9.61e-01 2.06e+00 amazon0601 2.35e+00 8.66e+00 6.91e+00 1.82e+00 2.94e+00 5.47e+00 5.73e+00 1.23e-01 6.96e-01 4.62e+00 6.96e-01 6.96e-01 4.97e+00 4.53e+00 wiki-Talk 1.07e+01 3.00e+01 2.20e+01 3.11e+00 1.35e+01 2.09e+01 2.10e+01 1.28e+00 2.40e+00 2.51e+00 2.40e+00 2.40e+00 2.54e+00 4.52e+00 ego-Gplus 8.55e+02 2.42e+03 1.73e+03 2.08e+01 1.63e+03 2.07e+03 2.17e+03 4.42e+00 1.68e+01 2.11e+01 1.68e+01 1.68e+01 2.57e+01 4.42e+01 uk-2014-tpd 8.10e+01 5.31e+02 4.07e+02 2.56e+01 1.15e+02 3.04e+02 2.85e+02 3.59e+00 9.66e+00 9.92e+00 4.35e+00 9.66e+00 2.10e+01 2.16e+01 soc-Pokec 4.17e+01 1.34e+02 1.21e+02 3.04e+01 4.88e+01 1.00e+02 1.04e+02 1.96e+00 1.75e+01 3.91e+01 1.75e+01 1.75e+01 2.39e+01 2.45e+01 uk-2014-host 9.98e+02 4.68e+03 2.76e+03 8.90e+01 1.32e+03 2.89e+03 2.99e+03 1.81e+01 4.38e+01 6.80e+01 2.04e+01 4.38e+01 8.28e+01 8.73e+01 soc-LiveJournal1 9.08e+01 7.66e+02 6.24e+02 1.24e+02 1.24e+02 4.41e+02 4.49e+02 2.32e+00 2.20e+01 1.06e+02 2.20e+01 2.20e+01 4.49e+01 6.13e+01 enwiki-2013 8.36e+02 9.62e+02 7.09e+02 3.13e+01 9.77e+02 8.19e+02 8.38e+02 2.18e+01 7.58e+01 8.45e+01 7.58e+01 7.58e+01 2.14e+02 1.48e+02 uk-2002 1.47e+03 8.59e+03 5.17e+03 2.45e+02 1.73e+03 4.53e+03 5.29e+03 1.66e+01 8.65e+01 2.52e+02 8.65e+01 8.65e+01 7.87e+02 5.32e+02 arabic-2005 6.51e+03 7.64e+04 6.05e+04 6.08e+03 8.39e+03 3.59e+04 3.69e+04 1.98e+01 1.64e+02 4.80e+02 3.26e+02 1.64e+02 1.95e+03 1.40e+03 twitter-2010 1.21e+04 1.38e+05 1.31e+05 3.33e+04 1.99e+04 8.03e+04 7.65e+04 2.23e+02 1.23e+03 1.95e+03 1.23e+03 1.23e+03 2.22e+03 2.18e+03 sk-2005 5.52e+04 1.63e+05 1.29e+05 1.55e+04 5.23e+04 9.64e+04 8.42e+04 5.73e+01 2.94e+02 7.98e+02 2.94e+02 2.94e+02 5.83e+03 3.81e+03\nTable S3: The 95% confidence interval (CI) for the regression coefficient of the regressor log(m) in a linear model for predicting the time to compute WM , based on the computational results for the networks in Table S1. The algorithm runs is guranteed to run in time O(m3/2). \u201cCombined\u201d refers to the regression coefficient when considering all of the times.\nMotif M1 M2 M3 M4 M5 M6 M7 Combined\n95% CI 1.20\u00b1 0.19 1.30\u00b1 0.20 1.31\u00b1 0.19 0.90\u00b1 0.31 1.20\u00b1 0.20 1.27\u00b1 1.21 1.27\u00b1 0.21 1.17\u00b1 0.09\nS33\nS2.3 Experimental results on k-cliques\nOn smaller graphs, we can compute larger motifs. To illustrate the computation time, we formed the motif adjacency matrix W based on the k-cliques motif for k = 4, . . . , 9. We implemented the k-clique enumeration algorithm by Chiba and Nishizeki with the additional pre-processing of computing the (k \u2212 1)-core of the graph. (This pre-processing improves the running time in practice but does not affect the asymptotic complexity.) The motif adjacency matrices for k-cliques are sparser than the adjacency matrix of the original graph. Thus, we do not worry about spatial complexity for these motifs.\nWe ran the algorithm on nine real-world networks, ranging from roughly four thousande nodes and 88 thousand edges to over two million nodes and around five million edges (see Table S4.) Each network contained at least one 9-clique and hence at least one k-clique for k < 9. All networks were downloaded from the SNAP collection at http://snap.stanford. edu/data/ (56). All computations ran on the same server as for the triangular motifs and again there was no parallelism. We terminated computations after two hours. For five of the nine networks, the time to compute WM for the k-clique motif was under two hours for k = 4, . . . , 9 (Table S5). And for each network, the computation finished within two hours for k = 4, 5, 6. The smallest network (in terms of number of nodes and number of edges) was the Facebook ego network, where it took just under two hours to comptue WM for the 6-clique motif and over two hours for the 7-clique motif. This network has around 80,000 edges. On the other hand, for the YouTube network, which contains nearly 3 million edges, we could compute WM for the 9-clique motif in under a minute.\nWe conclude that it is possible to use our frameworks with motifs much larger than the three-node motifs on which we performed many of our experiments. However, the number of edges is not that correlated with the running time to compute WM . This makes sense becuse the Chiba and Nishizeki algorithm complexity is O(ak\u22122m), where a is the arboricity of the graph.\nS34\nHence, the dependence on the number of edges is always linear.\nTable S4: Summary of networks used in scalability experiments with k-clique motifs. For each graph, we consider all edges as undirected.\nNetwork description # nodes # edges ego-Facebook Facebook friendships 4.04K 88.2K wiki-RfA Adminship voting on Wikipedia 10.8K 182K ca-AstroPh author co-authorship 18.8K 198K email-EuAll Emails in a research institution 265K 364K cit-HepPh paper citations 34.5K 421K soc-Slashdot0811 Slashdot user interactions 77.4K 469K com-DBLP author co-authorship 317K 1.05M com-Youtube User friendships 1.13M 2.99M wiki-Talk Wikipedia users interactions 2.39M 4.66M\nTable S5: Time to compute WM for k-clique motifs (seconds). Only computations that finished within two hours are listed.\nNumber of nodes in clique (k) Network 4 5 6 7 8 9 ego-Facebook 14 317 6816 \u2013 \u2013 \u2013 wiki-RfA 6 22 63 134 218 286 ca-AstroPh 5 35 285 2164 \u2013 \u2013 email-EuAll 1 2 4 5 6 6 cit-HepPh 3 6 11 18 30 36 soc-Slashdot0811 3 12 55 282 1018 2836 com-DBLP 9 129 3234 \u2013 \u2013 \u2013 com-Youtube 12 17 25 33 35 33 wiki-Talk 64 466 2898 \u2013 \u2013 \u2013\nS35"
                },
                {
                    "heading": "S3 Matrix-based interpretation of the motif-weighted adja-",
                    "text": "cency matrix\nFor several motifs, the motif adjacency matrix WM (Equation S19) has a simple formula in terms of the adjacency matrix of the original, directed, unweighted graph, G. Let A be the adjacency matrix for G and let U and B be the adjacency matrix of the unidirectional and bidirectional links ofG. Formally, B = A\u25e6AT and U = A\u2212B, where \u25e6 denotes the Hadamard (entry-wise) product. Table S6 lists the formula of WM for motifs M1, M2, M3, M4, M5, M6, and M7 (see Figure S4) in terms of the matrices U and B. The central computational kernel in these computations is (X \u00b7 Y ) \u25e6 Z. When X , Y , and Z are sparse, efficient parallel algorithms have been developed and analyzed (61). If the adjacency matrix is sparse, then computing WM for these motifs falls into this framework. Table S6: Matrix-based formulations of the weighted motif adjacency matrix WM (Equation S19) for all triangular three-node simple motifs. P \u25e6Q denotes the Hadamard (entry-wise) products of matrices P and Q. If A is the adjacency matrix of a directed, unweighted graph G, then B = A \u25e6 AT and U = A\u2212B. Note that in all cases, WM is symmetric.\nMotif Matrix computations WM =\nM1 C = (U \u00b7 U) \u25e6 UT C + CT M2 C = (B \u00b7 U) \u25e6 UT + (U \u00b7B) \u25e6 UT + (U \u00b7 U) \u25e6B C + CT M3 C = (B \u00b7B) \u25e6 U + (B \u00b7 U) \u25e6B + (U \u00b7B) \u25e6B C + CT M4 C = (B \u00b7B) \u25e6B C M5 C = (U \u00b7 U) \u25e6 U + (U \u00b7 UT ) \u25e6 U + (UT \u00b7 U) \u25e6 U C + CT M6 C = (U \u00b7B) \u25e6 U + (B \u00b7 UT ) \u25e6 UT + (UT \u00b7 U) \u25e6B C M7 C = (U T \u00b7B) \u25e6 UT + (B \u00b7 U) \u25e6 U + (U \u00b7 UT ) \u25e6B C\nWith these matrix formulations, implementing the motif-based spectral partitioning algorithm for modestly sized graphs is straightforward. However, these computations become slower than standard fast triangle enumeration algorithms when the networks are large and sparse. Nevertheless, the matrix formulations provide a simple and elegant computational\nS36\n1 f u n c t i o n [ S , Sbar , c o n d u c t a n c e s ] = M o t i f S p e c t r a l P a r t i t i o n M 6 (A) 2 % S p e c t r a l p a r t i t i o n i n g f o r m o t i f M 6 3 4 B = sp on es (A & A \u2019 ) ; % b i d i r e c t i o n a l l i n k s 5 U = A \u2212 B ; % u n i d i r e c t i o n a l l i n k s 6 7 % Form m o t i f a d j a c e n c y m a t r i x f o r m o t i f M 6 . 8 % For d i f f e r e n t m o t i f s , r e p l a c e t h i s l i n e wi th a n o t h e r m a t r i x f o r m u l a t i o n . 9 W = (B \u2217 U\u2019 ) .\u2217 U\u2019 + (U \u2217 B) .\u2217 U + (U\u2019 \u2217 U) . \u2217 B ;\n10 11 % Compute e i g e n v e c t o r o f m o t i f n o r m a l i z e d L a p l a c i a n 12 D s q r t = f u l l ( sum (W, 2 ) ) ; 13 D s q r t ( D s q r t \u02dc= 0 ) = 1 . / s q r t ( D s q r t ( D s q r t \u02dc= 0 ) ) ; 14 [ I , J , V] = f i n d (W) ; 15 Ln = s p a r s e ( I , J , \u2212V .\u2217 ( D s q r t ( I ) .\u2217 D s q r t ( J ) ) , s i z e (A, 1 ) , s i z e (A, 2 ) ) ; 16 [ Z , lambdas ] = e i g s ( Ln , 2 , \u2019 s a \u2019 ) ; 17 % Matlab \u2019 s e i g s i s somet imes o u t o f o r d e r 18 [ \u02dc , e i g o r d e r ] = s o r t ( d i a g ( lambdas ) ) ; 19 y = D s q r t . \u2217 Z ( : , e i g o r d e r ( end ) ) ; 20 21 % L i n e a r t ime sweep p r o c e d u r e 22 [ \u02dc , o r d e r ] = s o r t ( y ) ; 23 C = W( o r d e r , o r d e r ) ; 24 C sums = f u l l ( sum (C , 2 ) ) ; 25 volumes = cumsum ( C sums ) ; 26 v o l u m e s o t h e r = f u l l ( sum ( sum (W) ) ) \u2217 ones ( l e n g t h ( o r d e r ) , 1 ) \u2212 volumes ; 27 c o n d u c t a n c e s = cumsum ( C sums \u2212 2 \u2217 sum ( t r i l (C) , 2 ) ) . / min ( volumes , v o l u m e s o t h e r ) ; 28 [ \u02dc , s p l i t ] = min ( c o n d u c t a n c e s ) ; 29 S = o r d e r ( 1 : s p l i t ) ; 30 Sbar = o r d e r ( ( s p l i t + 1 ) : end ) ;\nFigure S6: MATLAB implementation of the motif-based spectral partitioning algorithm for motif M6. For other motifs, line 9 can be replaced with the formulations from Table S6.\nmethod for the motif adjacency matrix WM . To demonstrate, Figure S6 provides a complete MATLAB implementation of Algorithm 1 for M6 (Figure S4). The entire algorithm including comments comrpises 28 lines of code.\nAn alternative matrix formulation comes from a motif-node adjacency matrix. LetM(B,A) be a motif set and number the instances of the motif 1, . . . , |M |, so that (vi, \u03c7A(vi)) is the ith motif. Define the |M | \u00d7n motif-node adjacency matrix AM by (AM)ij = 1(j \u2208 \u03c7A(vi)). Then\nS37\n(WM)ij = (A T MAM)ij, i 6= j. (S26)\nThis provides a convenient algebraic formulation for defining the weighted motif adjacency matrix. However, in practice, we do not use this formulation for any computations."
                },
                {
                    "heading": "S4 Alternative clustering algorithms for evaluation",
                    "text": "For our experiments, we compare our spectral motif-based custering to the following methods:\n\u2022 Standard, edge-based spectral clustering, which is a special case of motif-based cluster-\ning. In particular, the motifs\nB1 = [ 0 1 1 0 ] , B2 = [ 0 1 0 0 ] , A = {1, 2} (S27)\ncorrespond to removing directionality from a directed graph. We refer to the union of these two motifs as Medge.\n\u2022 Infomap, which is based on the map equation (62). Software for Infomap was down-\nloaded from http://mapequation.org/code.html. We run the algorithm the algorithm for directed links when the network under consideration is directed.\n\u2022 The Louvain method (63). Software for the Louvain method was downloaded from\nhttps://perso.uclouvain.be/vincent.blondel/research/louvain. html We use the \u201coriented\u201d version of the Louvain method for directed graphs.\nInfomap and the Louvain method are purely clustering methods in the sense that they take as input the graph and produce as output a set of labels for the nodes in the graph. In contrast to the spectral methods, we do not have control over the number of clusters. Also, only the spectral methods provide embeddings of the nodes into euclidean space, which is useful for\nS38\nvisualization. Thus, for our analysis of the transportation reachiability network in Section S6, we only compare spectral methods."
                },
                {
                    "heading": "S5 Details and comparison against existing methods for the",
                    "text": "C. elegans network\nWe now provide more details on the cluster found for the C. elegans network of frontal neurons (28). In this network, the nodes are neurons and the edges are synapses. The network data was downloaded from http://www.biological-networks.org/pubs/ suppl/celegans131.zip.\nS5.1 Connected components of the motif adjacency matrices\nWe again first onsider the connected components of the motif adjacency matrices as a preprocessing step. For our analysis, we consider use Mbifan, M8, and Medge (Figure S4). The original network has 131 nodes and 764 edges. The largest connected component of the motif adjacency matrix for motif Mbifan contains 112 nodes. The remaining 19 nodes are isolated and correspond to the neurons AFDL, AIAR, AINR, ASGL/R, ASIL/R, ASJL/R, ASKL/R, AVL, AWAL, AWCR, RID, RMFL, SIADR, and SIBDL/R. The largest connected component of the motif adjacency matrix for motif M8 contains 127 nodes. The remaining 4 nodes are isolated and correspond to the neurons ASJL/R and SIBDL/R. The original network is weakly connected, so the motif adjacency matrix for Medge is connected.\nS5.2 Comparison of bi-fan motif cluster to clusters found by existing methods\nWe found the motif-based clusters for motifs Mbifan, M8, and Medge by running Algorithm 1 on the largest connected component of the motif adjacency matrix. Sweep profile plots (\u03c6M(S) as\nS39\n10 0\n10 1\n10 2\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nM o ti f c o n d\nu c ta\nn c e\nSets Sr\nFigure S7: Sweep profile plot (\u03c6M(S) as a function of S from the sweep in Algorithm 1) for Mbifan (green) M8 (dark blue), and Medge (light blue).\na function of S from the sweep in Algorithm 1) are shown in Figure S7 and show that the size of the Mbifan returned by Algorithm 1 cluster is smaller than the clusters for M8 and Medge. In fact, the motif-based clusters for M8 and Medge essentially bisect the graph, containing 63 of 127 and 64 of 131 nodes, respectively. Of the 63 nodes in the M8-based cluster, only 2 are in the edge-based cluster, so these partitions give roughly the same information.\nNext, we compare the clusters found by existing methods to the Mbifan-based cluster found by Algorithm 1. We will show that existing methods do not find the same group of nodes. Let Sbifan be the Mbifan-based cluster, which consists of 20 nodes. The nodes correspond to the following neurons: IL1DL/VL, IL2DL/DR/VL/VR/L/R, OLQDL/R, RIH, RIPL/R, RMEL/R/V, and URADL/DR/VL/VR. The partitions based onM8 andMedge provide two sets of nodes each. For the subsequent analysis, we consider the set with the largest number of overlapping nodes with Sbifan. Call these sets SM8 and Sedge. We also consider the cluster found by Infomap and the Louvain method with the largest overlap with Sbifan. Call these sets SI and SL.\nTo compare the most similar clusters found by other methods to Sbifan, we look at two metrics. First, how many neurons in Sbifan are in a cluster found by existing methods (in other\nS40\nwords, the overlap). A cluster consisting of all nodes in the graph would trivially have 100% overlap with Sbifan but loses all precision in the cluster identification. Thus, we also consider the sizes of the clusters. These metrics are summarized as follows:\n|Sbifan \u2229 SM8| = 20, |SM8| = 68 |Sbifan \u2229 Sedge| = 20, |Sedge| = 64\n|Sbifan \u2229 SL| = 13, |SL| = 27 |Sbifan \u2229 SI | = 19, |SI | = 114\nWe see that Sbifan is a subset of SM8 and Sedge and has substantial overlap with SI . However, Sbifan is by far the smallest of all of these sets. We conclude that existing methods do not capture the same information as motif Mbifan.\nTo further investigate the structure found by existing methods, we show the clusters Sedge and SM8 in Figure S8. From the figure, we see that spectral clustering based on edges or motif M8 simply finds a spatially coherent cluster, rather than the control structure formed by the nodes in Sbifan.\nS41\nFigure S8: Illustration of motif-based clusters with true two-dimensional spatial dimensions of the frontal neurons of C. elegans. A: The Mbifan-based cluster consists of the labeled dark blue nodes. B: Partitioning the graph based on motif M8, where the labeled dark blue nodes are the nodes on the side of the partition with largest overlap of the nodes in A. C: Partitioning the graph based on edges, where the labeled dark blue nodes are the nodes on the side of the partition with largest overlap of the nodes in A. Note that the partitions in Figures B and C capture the cluster in Figure A, but also contain many other nodes. Essentially, the partitions in B and C are just capturing spatial information.\nS42"
                },
                {
                    "heading": "S6 Details and comparison against existing methods for the",
                    "text": "transportation reachability network\nThe nodes in the transportation reachability network are airports in the United States and Canada. There is an edge from city i to city j if the estimated travel time from i to j is less than some threshold (23). The network is not symmetric. The network with estimated travel times was downloaded from\nhttp://www.psi.toronto.edu/affinitypropagation/TravelRouting.mat and http://www.psi.toronto.edu/affinitypropagation/TravelRoutingCityNames. txt. We collected the latitude, longitude, and metropolitan populations of the cities using WolframAlpha and Wikipedia. All of the data is available on our project web page: http: //snap.stanford.edu/higher-order/.\nS6.1 Methods for spectral embeddings\nWe compared the motif-based spectral embedding of the transportation reachability network to spectral embeddings from other connectivity matrices. For this analysis, we ignore the travel times times and only consider the topology of the network. The two-dimensional spectral embedding for a graph defined by a (weighted) adjacency matrix W \u2208 Rn\u00d7n comes from Algorithm 2:\n1. Form the normalized Laplacian L = I \u2212D\u22121/2WD\u22121/2, where D is the diagonal degree\nmatrix with Dii = \u2211 jWij .\n2. Compute the first 3 eigenvectors z1, z2, z3 of smallest eigenvalues for L (z1 has the small-\nest eigenvalue).\n3. Form the normalized matrix Y \u2208 Rn\u00d73 by Yij = zij/ \u221a\u22113 j=1 z 2 ij .\nS43\n4. Define the primary and secondary spectral coordinates of node i to be Yi2 and Yi3, respec-\ntively.\nWe consider the following three matrices W .\n1. Motif: The sum of the motif adjacency matrix (Equation S20) for three different anchored\nmotifs:\nB1 = 0 1 11 0 1 1 1 0  , B2 = 0 1 11 0 1 0 1 0  , B3 = 0 1 01 0 1 0 1 0  , A = {1, 3}. (S28) If S is the matrix of bidirectional links in the graph (Sij = 1 if and only ifAij = Aji = 1), then the motif adjacency matrix for these motifs is WM = S2. The resulting embedding is shown in Figure 4C of the main text.\n2. Undirected: The adjacency matrix is formed by ignoring edge direction. This is the\nstandard spectral embedding. The resulting embedding is shown in Figure 4D of the main text.\n3. Undirected complement: The adjacency matrix is formed by taking the complement of\nthe undirected adjacency matrix. This matrix tends to connect non-hubs to each other.\nThe networks represented by each adjacency matrices are all connected.\nS6.2 Comparison of motif-based embedding to other embeddings\nWe computed 99% confidence intervals for the Pearson correlation of the primary spectral coordinate with the metropolitan population of the city using the Pearson correlation coefficient. Table S7 lists the confidence intervals. (Since eigenvectors are only unique up to sign, the confidence intervals are symmetric about 0. We list the interval with the largest positive end point\nS44\nTable S7: Summary of Pearson correlations for spectral embeddings of the transportation reachability network. We list the 99% confidence interval for the Pearson correlation coefficient.\nPrimary spectral coordinate Secondary spectral coordinate and metropolitan population and longitude\nEmbedding 99% confidence interval 99% confidence interval Motif 0.43 \u00b1 0.09 0.59 \u00b1 0.08 Undirected 0.11 \u00b1 0.12 0.39 \u00b1 0.11 Undirected complement 0.31 \u00b1 0.11 0.10 \u00b1 0.12\nunder this permutation to be consistent across embeddings.) The motif-based primary spectral coordinate has the strongest correlation with the city populations.\nWe repeated the computations for the correlation between the secondary spectral coordinate and the longitude of the city. Again, the motif-based clustering has the strongest correlation. Furthermore, the lower end of the confidence interval for the motif-based embedding was above the higher end of the confidence interval for the other three embeddings.\nFinally, in order to visualize these relationships, we computed Loess regressions of city metropolitan population and longitude against the primary and secondary spectral coordinates for each of the embeddings (Figure S9). The sign of the eigenvector used in each regression was chosen to match correlation shown in Figures 3C and 3D in the main text (primary spectral coordinate positively correlated with population and secondary spectral coordinate negatively correlated with longitude). The Loess regressions visualize the stronger correlation of the motifbased spectral coordinates with the metropolitan popuatlion and longitude.\nWe conclude that the embedding provided by the motif adjacency matrix more strongly captures the hub nature of airports and West-East geography of the network. To gain further insight into the relationship of the primary spectral coordinate\u2019s relationship with the hub airports, we visualize the adjacency matrix in Figure S10, where the nodes are ordered by the spectral ordering. We see a clear relationship between the spectral ordering and the connectivity.\nS45\nFigure S9: Loess regressions of city metropolitan population against the primary spectral coordinate (top) and longitude against secondary spectral coordinate (bottom) for the motif (left), undirected (middle), and undirected complement (right) adjacency matrices.\nFigure S10: Visualization of transportation reachability network. Nodes are ordered by the spectral ordering provided by the motif adjacency matrix. A black dot means no edge exists in the network. For the edges in the network, lighter colors mean longer estimated travel times.\nS46"
                },
                {
                    "heading": "S7 Additional case studies",
                    "text": "We next use motif-based clustering to analyze several additional networks. Our main goal is to show that motif-based clusters find markedly different structures in many real-world networks compared to edge-based clusters. For the case of a transcription regulation network of yeast, we also show that motif-based clustering more accurately finds known functional modules compared to existing methods. On the English Wikipedia article network and the Twitter network, we identify motifs that find anomalous clusters. On the Stanford web graph and in collaboration networks, we use motifs that have previously been studied in the literature and see how they reveal organizational structure in the networks.\nS7.1 Motif M6 in the Florida Bay food web\nWe now apply the higher-order clustering framework on the Florida Bay ecosystem food web (64). The dataset was downloaded from http://vlado.fmf.uni-lj.si/pub/networks/ data/bio/foodweb/Florida.paj. In this network, the nodes are compartments (roughly, organisms and species) and the edges represent directed carbon exchange (in many cases, this means that species j eats species i). Motifs model energy flow patterns between several species.\nS7.1.1 Identifying higher-order modular organization\nIn this case study, we use the framework to identify higher-order modular organization of networks. We focus on three motifs: M5 corresponds to a hierarchical flow of energy where species i and j are energy sources (prey) for species k, and i is also an energy source for j; M6 models two species that prey on each other and then compete to feed on a common third species; and M8 describes a single species serving as an energy source for two non-interacting species. Motif M5 is considered a building block for food webs (65, 66), and the prevalence of motif M6 is predicted by a certain niche model (67).\nS47\nThe framework reveals that low motif conductance (high-quality) clusters only exist for motif M6 (motif conductance 0.12), whereas clusters based on motifs M5 or M8 have high motif conductance (see Figure S11). In fact, the motif Cheeger inequality (Theorem 6) guarantees that clustering based on motif M5 or M8 will always have larger motif conductance that clustering based on M6. The inequality says that the motif conductance for any cluster in a connected motif adjacency matrix is at least half of the second smallest eigenvalue of the motif-normalized Laplacian. However, finding the cluster with optimal conductance is still computationally infeasible in general (68).\nThe lower bounds using the largest connected component of the motif adjacency matrix for motifs M5, M6, and M8 were 0.2195, 0.0335, and 0.2191, and the clusters found by the Algorithm 1 had motif conductances of 0.4414, 0.1200, and 0.4145. Thus, the cluster S found by the algorithm for M6 has smaller motif M6-conductance (0.12) than any possible cluster\u2019s motif-M5 or motif-M8 conductance. To state this formally, let C be the cluster found by the algorithm for motif M6 and let HM be the largest connected component of motif adjacency matrix for motif M . Then\n\u03c6M6(HM6 , C) \u2264 min {\nmin S \u03c6M5(HM5 , S), min S \u03c6M8(HM8 , S)\n} . (S29)\nThis means that, in terms of motif conductance, any cluster based on motifs M5 or M8 is worse than the cluser found by the algorithm in Theorem 6 for motif M6. We note that the same conclusions hold for edge-based clustering. For motif Medge, the lower bound on conductance was 0.2194 and the cluster found by the algorithm had conductance 0.4083.\nS7.1.2 Analysis of higher-order modular organization\nSubsequently, we used motif M6 to cluster the food web, revealing four clusters (Figure S11). Three represent well-known aquatic layers: (i) the pelagic system; (ii) the benthic predators of eels, toadfish, and crabs; (iii) the sea-floor ecosystem of macroinvertebrates. The fourth\nS48\ncluster identifies microfauna supported by particulate organic carbon in water and free bacteria. Table S9 lists the nodes in each cluster.\nWe also measured how well the motif-based clusters correlate to known ground truth system subgroup classifications of the nodes (64). These classes are microbial, zooplankton, and sediment organism microfauna; detritus; pelagic, demersal, and benthic fishes; demseral, seagrass, and algae producers; and macroinvertebrates (Table S9).2 We also consider a set of labels which does not include the subclassification for microfauna and producers. In this case, the labels are microfauna; detritus; pelagic, demersal, and benthic fishes; producers; and macroinvertebrates.\nTo quantify how well the clusters found by motif-based clustering reflect the ground truth labels, we used several standard evaluation criteria: adjusted rand index, F1 score, normalized mutual information, and purity (69). We compared these results to the clusters of several methods using the same evaluation criteria. In total, we evaluated six methods:\n1. Motif-based clustering with the embedding + k-means algorithm (Algorithm 2) with 500\niterations of k-means.\n2. Motif-based clustering with recursive bi-partitioning (repeated application of Algorithm 1\non the largest remaining compoennt). The process continues to cut the largest cluster until there are 4 total.\n3. Edge-based clustering with the embedding + k-means algorithm, again with 500 iterations\nof k-means.\n4. Edge-based clustering with recursive bi-partitioning with the same partitioning process.\n5. The Infomap algorithm.\n2The classifications are also available on our project web page: http://snap.stanford.edu/ higher-order/.\nS49\n6. The Louvain method.\nFor the first four algorithms, we control the number of clusters, which we set to 4. For the last two algorithms, we cannot control the number of clusters. However, both methods found 4 clusters.\nTable S10 shows that the motif-based clustering by embedding + k-means had the best performance for each classification criterion on both classifications. We conclude that the organization of compartments in the Florida Bay foodweb are better described motif M6 than by edges.\nS7.1.3 Connected components of the motif adjacency matrices\nFinally, we discuss the discuss the preprocessing step of our method, where we compute computed connected components of the motif adjacency matrices. The original network has 128 nodes and 2106 edges. The largest connected component of the motif adjacency matrix for motif M5 contains 127 of the 128 nodes. The node corresponding to the compartment of \u201croots\u201d is the only node not in the largest connected component. The two largest connected components of the motif adjacency matrix for motif M6 contain 12 and 50 nodes. The remaining 66 nodes are isolated. Table S8 lists the nodes in each component. We note that the group of 12 nodes corresponds to the green cluster in Figure S11. The motif adjacency matrix for M8 is connected. The original network is weakly connected, so the motif adjacency matrix for Medge is also connected.\nS50\nFigure S11: Higher-order organization of the Florida Bay food web. A: Sweep profile plot (\u03c6(G)M (S) as a function of S from the sweep in Algorithm 1) for different motifs on the Florida Bay ecosystem food web (64). A priori it is not clear whether the network is organized based on a given motif. For example, motifs M5 (green) and M8 (blue) do not reveal any higher-order organization (motif conductance has high values). However, the downward spikes of the red curve show that M6 reveals rich higher-order modular structure (7). Ecologically, motif M6 corresponds to two species mutually feeding on each other and also preying on a common third species. B: Clustering of the food web based on motif M6. (For illustration, edges not participating in at least one instance of the motif are omitted.) The clustering reveals three known aquatic layers: pelagic fishes (yellow), benthic fishes and crabs (red), and sea-floor macroinvertebrates (blue) as well as a cluster of microfauna and detritus (green). Our framework identifies these modules with higher accuracy (61%) than existing methods (48\u201353%). C: A higher-order cluster (yellow nodes in (B)) shows how motif M6 occurs in the pelagic layer. The needlefish and other pelagic fishes eat each other while several other fishes are prey for these two species. D: Another higher-order cluster (green nodes in (B)) shows how motif M6 occurs between microorganisms. Here, several microfauna decompose into Particulate Organic Carbon in the water (water POC) but also consume water POC. Free bacteria serves as an energy source for both the microfauna and water POC. S51\nTable S8: Connected components of the Florida Bay foodweb motif adjacency matrix for motif M6. There are 50 nodes in component 1, 12 nodes in component 2, and 66 isolated nodes.\nTwo largest components Isolated nodes Compartment (node) Component index Compartment (node) Benthic Phytoplankton 1 Barracuda Thalassia 1 2 \u00b5m Spherical Phytoplankt Halodule 1 Synedococcus Syringodium 1 Oscillatoria Drift Algae 1 Small Diatoms (<20 \u00b5m) Epiphytes 1 Big Diatoms (>20 \u00b5m) Predatory Gastropods 1 Dinoflagellates Detritivorous Polychaetes 1 Other Phytoplankton Predatory Polychaetes 1 Roots Suspension Feeding Polych 1 Coral Macrobenthos 1 Epiphytic Gastropods Benthic Crustaceans 1 Thor Floridanus Detritivorous Amphipods 1 Lobster Herbivorous Amphipods 1 Stone Crab Isopods 1 Sharks Herbivorous Shrimp 1 Rays Predatory Shrimp 1 Tarpon Pink Shrimp 1 Bonefish Benthic Flagellates 1 Other Killifish Benthic Ciliates 1 Snook Meiofauna 1 Sailfin Molly Other Cnidaridae 1 Hawksbill Turtle Silverside 1 Dolphin Echinoderma 1 Other Horsefish Bivalves 1 Gulf Pipefish Detritivorous Gastropods 1 Dwarf Seahorse Detritivorous Crabs 1 Grouper Omnivorous Crabs 1 Jacks Predatory Crabs 1 Pompano Callinectes sapidus (blue crab) 1 Other Snapper Mullet 1 Gray Snapper Blennies 1 Mojarra Code Goby 1 Grunt Clown Goby 1 Porgy Flatfish 1 Pinfish Sardines 1 Scianids Anchovy 1 Spotted Seatrout Bay Anchovy 1 Red Drum Lizardfish 1 Spadefish Catfish 1 Parrotfish Eels 1 Mackerel Toadfish 1 Filefishes Brotalus 1 Puffer Halfbeaks 1 Loon Needlefish 1 Greeb Goldspotted killifish 1 Pelican Rainwater killifish 1 Comorant Other Pelagic Fishes 1 Big Herons and Egrets Other Demersal Fishes 1 Small Herons and Egrets Benthic Particulate Organic Carbon (Benthic POC) 1 Ibis Free Bacteria 2 Roseate Spoonbill Water Flagellates 2 Herbivorous Ducks Water Cilitaes 2 Omnivorous Ducks Acartia Tonsa 2 Predatory Ducks Oithona nana 2 Raptors Paracalanus 2 Gruiformes Other Copepoda 2 Small Shorebirds Meroplankton 2 Gulls and Terns Other Zooplankton 2 Kingfisher Sponges 2 Crocodiles Water Particulate Organic Carbon (Water POC) 2 Loggerhead Turtle Input 2 Green Turtle\nManatee Dissolved Organic Carbon (DOC) Output Respiration\nS52\nTable S9: Ecological classification of nodes in the Florida Bay foodweb. Colors correspond to the colors in the clustering of Figure S11.\nCompartment (node) Classification 1 Classification 2 Assignment Free Bacteria Microbial microfauna Microfauna Green Water Flagellates Microbial microfauna Microfauna Green Water Cilitaes Microbial microfauna Microfauna Green Acartia Tonsa Zooplankton microfauna Microfauna Green Oithona nana Zooplankton microfauna Microfauna Green Paracalanus Zooplankton microfauna Microfauna Green Other Copepoda Zooplankton microfauna Microfauna Green Meroplankton Zooplankton microfauna Microfauna Green Other Zooplankton Zooplankton microfauna Microfauna Green Sponges Macroinvertebrates Macroinvertebrates Green Water POC Detritus Detritus Green Input Detritus Detritus Green Sardines Pelagic Fishes Pelagic Fishes Yellow Anchovy Pelagic Fishes Pelagic Fishes Yellow Bay Anchovy Pelagic Fishes Pelagic Fishes Yellow Halfbeaks Pelagic Fishes Pelagic Fishes Yellow Needlefish Pelagic Fishes Pelagic Fishes Yellow Goldspotted killifish Fishes Demersal Fishes Demersal Yellow Rainwater killifish Fishes Demersal Fishes Demersal Yellow Silverside Pelagic Fishes Pelagic Fishes Yellow Other Pelagic Fishes Pelagic Fishes Pelagic Fishes Yellow Detritivorous Crabs Macroinvertebrates Macroinvertebrates Red Predatory Crabs Macroinvertebrates Macroinvertebrates Red Callinectus sapidus Macroinvertebrates Macroinvertebrates Red Lizardfish Benthic Fishes Benthic Fishes Red Eels Fishes Demersal Fishes Demersal Red Code Goby Benthic Fishes Benthic Fishes Red Clown Goby Benthic Fishes Benthic Fishes Red Herbivorous Shrimp Macroinvertebrates Macroinvertebrates Red Benthic Phytoplankton Producer Demersal Producer Blue Thalassia Producer Seagrass Producer Blue Halodule Producer Seagrass Producer Blue Syringodium Producer Seagrass Producer Blue Drift Algae Producer Algae Producer Blue Epiphytes Producer Algae Producer Blue Benthic Flagellates Sediment Organism microfauna Microfauna Blue Benthic Ciliates Sediment Organism microfauna Microfauna Blue Meiofauna Sediment Organism microfauna Microfauna Blue Other Cnidaridae Macroinvertebrates Macroinvertebrates Blue Echinoderma Macroinvertebrates Macroinvertebrates Blue Bivalves Macroinvertebrates Macroinvertebrates Blue Detritivorous Gastropods Macroinvertebrates Macroinvertebrates Blue Predatory Gastropods Macroinvertebrates Macroinvertebrates Blue Detritivorous Polychaetes Macroinvertebrates Macroinvertebrates Blue Predatory Polychaetes Macroinvertebrates Macroinvertebrates Blue Suspension Feeding Polych Macroinvertebrates Macroinvertebrates Blue Macrobenthos Macroinvertebrates Macroinvertebrates Blue Benthic Crustaceans Macroinvertebrates Macroinvertebrates Blue Detritivorous Amphipods Macroinvertebrates Macroinvertebrates Blue Herbivorous Amphipods Macroinvertebrates Macroinvertebrates Blue Isopods Macroinvertebrates Macroinvertebrates Blue Predatory Shrimp Macroinvertebrates Macroinvertebrates Blue Pink Shrimp Macroinvertebrates Macroinvertebrates Blue Omnivorous Crabs Macroinvertebrates Macroinvertebrates Blue Catfish Benthic Fishes Benthic Fishes Blue Mullet Pelagic Fishes Pelagic Fishes Blue Benthic POC Detritus Detritus Blue Toadfish Benthic Fishes Benthic Fishes Blue Brotalus Fishes Demersal Fishes Demersal Blue Blennies Benthic Fishes Benthic Fishes Blue Flatfish Benthic Fishes Benthic Fishes Blue Other Demersal Fishes Fishes Demersal Fishes Demersal Blue\nS53\nTable S10: Comparison of motif-based algorithms against other methods in finding ground truth structure in the Florida Bay food web (64). Performance for identifying the two classifications provided in Table S9 was evaluated based on Adjusted Rand Index (ARI), F1 score, Normalized Mutual Information (NMI), and Purity. In all cases, the motif-based methods have the best performance.\nEvaluation Motif embedding Motif recursive Edge embedding Edge recursive Infomap Louvain\n+ k-means bi-partitioning + k-means bi-partitioning\nC la\nss ifi\nca tio\nn 1 ARI 0.3005 0.2156 0.1564 0.1226 0.1423 0.2207\nF1 0.4437 0.3853 0.3180 0.2888 0.3100 0.4068 NMI 0.5040 0.4468 0.4112 0.3879 0.4035 0.4220 Purity 0.5645 0.5323 0.4032 0.4194 0.4194 0.5323\nC la\nss ifi\nca tio\nn 2 ARI 0.3265 0.2356 0.1814 0.1190 0.1592 0.2207\nF1 0.4802 0.4214 0.3550 0.3035 0.3416 0.4068 NMI 0.4822 0.4185 0.3533 0.3034 0.3471 0.4220 Purity 0.6129 0.5806 0.4839 0.4355 0.4677 0.5323\nS7.2 Coherent feedforward loops in the S. cerevisiae transcriptional regulation network\nIn this network, each node is an operon (a group of genes in a mRNA molecule), and a directed edge from operon i to operon j means that i is regulated by a transcriptional factor encoded by j (29). Edges are directed and signed. A positive sign represents activation and a negative sign represents repression. The network data was downloaded from http://www.weizmann.\nac.il/mcb/UriAlon/sites/mcb.UriAlon/files/uploads/NMpaper/yeastdata. mat and http://www.weizmann.ac.il/mcb/UriAlon/sites/mcb.UriAlon/files/ uploads/DownloadableData/list_of_ffls.pdf.\nFor this case study, we examine the coherent feedforward loop motif (see Figure S12), which act as sign-sensitive delay elements in transcriptional regulation networks (2, 9). Formally, the feedforward loop is represented by the following signed motifs\nB1 = 0 + +0 0 + 0 0 0  , B2 = 0 \u2212 \u22120 0 + 0 0 0  , B3 = 0 + \u22120 0 \u2212 0 0 0  , B4 = 0 \u2212 +0 0 \u2212 0 0 0  . (S30) S54\nTable S11: Connected components of size greater than one for the motif adjacency matrix in the S. cerevisiae network for the coherent feedforward loop.\nSize operons\n18 ALPHA1, CLN1, CLN2, GAL11, HO, MCM1, MFALPHA1, PHO5, SIN3, SPT16, STA1, STA2, STE3, STE6, SWI1, SWI4/SWI6, TUP1, SNF2/SWI1\n9 HXT11, HXT9, IPT1, PDR1, PDR3, PDR5, SNQ2, YOR1, YRR1 9 GCN4, ILV1, ILV2, ILV5, LEU3, LEU4, MET16, MET17, MET4 6 CHO1, CHO2, INO2, INO2/INO4, OPI3, UME6 6 DAL80, DAL80/GZF3, GAP1, GAT1, GLN1, GLN3 5 CYC1, GAL1, GAL4, MIG1, HAP2/3/4/5 3 ADH2, CCR4, SPT6 3 CDC19, RAP1, REB1 3 DIT1, IME1, RIM101\nThese motifs have the same edge pattern and only differ in sign. All of the motifs are simple (A = {1, 2, 3}). For our analysis, we consider all coherent feedforward loops that are subgraphs on the induced subgraph of any three nodes. However, there is only one instance where the coherent feedforward loop itself is a subgraph but not an induced subgraph on three nodes. Specifically, the induced subgraph by DAL80, GAT1, and GLN3 contains a bi-directional edge between DAL80 and GAT1, unidirectional edges from DAL80 and GAT1 to GLN3.\nS7.2.1 Connected components of the adjacency matrices\nAgain, we analyze the component structure of the motif adjacency matrix as a pre-processing step. The original network consists of 690 nodes and 1082 edges, and its largest weakly connected component consists of 664 nodes and 1066 edges. Every coherent feedforward loop in the network resides in the largest weakly connected component, so we subsequently consider this sub-network in the following analysis. Of the 664 nodes in the network, only 62 participate in a coherent feedforward loop. Forming the motif adjacency matrix results in nine connected components, of sizes 18, 9, 9, 6, 6, 5, 3, 3, and 3. The operons for the connected components consisting of more than one node is listed in Table S11.\nS55\nS7.2.2 Comparison against existing methods\nWe note that, although the original network is connected, the motif adjacency matrix corresponds to a disconnected graph. This already reveals much of the structure in the network (Figure S12). Indeed, this \u201cshattering\u201d of the graph into components for the feedforward loop has previously been observed in transcriptional regulation networks (70). We additionally used Algorithm 1 to partition the largest connected component of the motif adjacency matrix (consisting of 18 nodes). This revealed the cluster {CLN2, CLN1, SWI4/SWI6, SPT16, HO}, which contains three coherent feedforward loops (Figure S12). All three instances of the motif correspond to the function \u201ccell cycle and mating type switch\u201d. The motifs in this cluster are the only feedforward loops for which the function is described in Reference (9). Using the same procedure on the undirected version of the induced subgraph of the 18 nodes (i.e., using motif Medge) results in the cluster {CLN1, CLN2, SPT16, SWI4/SWI6 }. This cluster breaks the coherent feedforward loop formed by HO, SWI4/SWI6, and SPT16.\nWe also evaluated our method based on the classification of motif functionality (9).3 In total, there are 12 different functionalities and 29 instances of labeled coherent feedforward loops. We considered the motif-based clustering of the graph to be the connected components of the motif adjacency matrix with the additional partition of the largest connected component. To form an edge-based clustering, we used the embedding + k-means algorithm on the undirected graph (i.e., motif Medge) with k = 12 clusters. We also clustered the graph using Infomap and the Louvain method. Table S12 summarizes the results. We see that the motif-based clustering coherently labels all 29 motifs in the sense that the three nodes in every instance of a labeled motif is placed in the same cluster. The edge-based spectral, Infomap, and Louvain clustering coherently labeled 25, 23, and 23 motifs, respectively.\n3The functionalities may be downloaded from our project web page: http://snap.stanford.edu/ higher-order/.\nS56\nWe measured the accuracy of each clustering method as the rand index (69) on the coherently labeled motifs, multiplied by the fraction of coherently labeled motifs. The motif-based clustering had the highest accuracy. We conclude that motif-based clustering provides an advantage over edge-based clustering methods in identifying functionalities of coherent feedforward loops in the the S. cerevisiae transcriptional regulation network.\nS57\nFigure S12: Higher-order organization of the S. cerevisiae transcriptional regulation network. A: The four higher-order structures used by our higher-order clustering method, which can model signed motifs. These are coherent feedfoward loop motifs, which act as sign-sensitive delay elements in transcriptional regulation networks (2). The edge signs refer to activation (positive) or repression (negative). B: Six higher-order clusters revealed by the motifs in (A). Clusters show functional modules consisting of several motifs (coherent feedforward loops), which were previously studied individually (9). The higher-order clustering framework identifies the functional modules with higher accuracy (97%) than existing methods (68\u201382%). C\u2013D: Two higher-order clusters from (B). In these clusters, all edges have positive sign. The functionality of the motifs in the modules correspond to drug resistance (C) or cell cycle and mating type match (D). The clustering suggests that coherent feedforward loops function together as a single processing unit rather than as independent elements.\nS58\nTable S12: Classification of coherent feedforward loop motifs by several clustering methods. In a given motif instance, we say that it is coherently labeled if the nodes comprising the motif are in the same cluster. If a motif is not coherently labeled, a \u201c-1\u201d is listed. The accuracy is the rand index on the labels and motif functionality on coherently labeled motifs, multiplied by the fraction of coherently labeled motifs.\nMotif nodes Function Class label Motif-based Edge-based Infomap Louvain\nGAL11 ALPHA1 MFALPHA1 pheromone response 1 1 -1 -1 GCN4 MET4 MET16 Metionine biosynthesis 2 2 1 -1 GCN4 MET4 MET17 Metionine biosynthesis 2 2 1 -1 GCN4 LEU3 ILV1 Leucine and branched amino acid biosynthesis 2 2 1 1 GCN4 LEU3 ILV2 Leucine and branched amino acid biosynthesis 2 2 1 1 GCN4 LEU3 ILV5 Leucine and branched amino acid biosynthesis 2 2 1 1 GCN4 LEU3 LEU4 Leucine and branched amino acid biosynthesis 2 2 1 1 GLN3 GAT1 GAP1 Nitrogen utilization 3 3 1 2 GLN3 GAT1 DAL80 Nitrogen utilization 3 3 1 2 GLN3 GAT1 DAL80/GZF3 Glutamate synthetase 3 3 1 2 GLN3 GAT1 GLN1 Glutamate synthetase 3 3 1 2 MIG1 HAP2/3/4/5 CYC1 formation of apocytochromes 4 4 -1 -1 MIG1 GAL4 GAL1 Galactokinase 4 -1 -1 -1 PDR1 YRR1 SNQ2 Drug resistance 5 5 2 3 PDR1 YRR1 YOR1 Drug resistance 5 5 2 3 PDR1 PDR3 HXT11 Drug resistance 5 5 2 3 PDR1 PDR3 HXT9 Drug resistance 5 5 2 3 PDR1 PDR3 PDR5 Drug resistance 5 5 2 3 PDR1 PDR3 IPT1 Drug resistance 5 5 2 3 PDR1 PDR3 SNQ2 Drug resistance 5 5 2 3 PDR1 PDR3 YOR1 Drug resistance 5 5 2 3 RIM101 IME1 DIT1 sporulation-specific 6 6 3 4 SPT16 SWI4/SWI6 CLN1 Cell cycle and mating type switch 7 -1 4 5 SPT16 SWI4/SWI6 CLN2 Cell cycle and mating type switch 7 -1 -1 5 SPT16 SWI4/SWI6 HO Cell cycle and mating type switch 7 -1 -1 -1 TUP1 ALPHA1 MFALPHA1 Mating factor alpha 1 1 -1 5 UME6 INO2/INO4 CHO1 Phospholipid biosynthesis 8 6 5 4 UME6 INO2/INO4 CHO2 Phospholipid biosynthesis 8 6 5 4 UME6 INO2/INO4 OPI3 Phospholipid biosynthesis 8 6 5 4\nFrac. coherently labeled 29 / 29 25 / 29 23 / 29 23 / 29 Accuracy 0.97 0.82 0.68 0.76\nS7.3 Motif M6 in the English Wikipedia article network\nThe English Wikipedia network (57\u201359) consists of 4.21 million nodes (representing articles) and 101.31 million edges, where an edge from node i to node j means that there is a hyperlink from the ith article to the jth article. The network data was downloaded from http://law. di.unimi.it/webdata/enwiki-2013/.\nWe used Algorithm 1 to find a motif-based cluster for motif M6 and Medge (the algorithm was run on the largest connected component of the motif adjacency matrix). The clusters are shown in Figure S13. The nodes in the motif-based cluster are cities and barangays (small\nS59\nadministrative divisions) in the Philippines. The cluster has a set of nodes with many outgoing links that form the source node in motif M6. In total, the cluster consists of 22 nodes and 338 edges. The linking pattern appears anomalous and suggests that perhaps the pages uplinking should receive reciprocated links. On the other hand, the edge-based cluster is much larger cluster and does not have too much structure. The cluster consists of several high-degree nodes and their neighbors.\nS60\nFigure S13: Clusters from the English Wikipedia hyperlink network (57\u201359). A\u2013C: Motifbased cluster (A) for motif M6 (B). The cluster consists of cities and small administrative divisions in the Philippines. The green nodes have many bi-direction links with each other and many incoming links from orange nodes at the bottom of the figure. The spy plot illustrates this network structure (C). D\u2013F: Cluster (D) for undirected edges (E). The cluster has a few very high-degree nodes, as evidenced by the spy plot (F).\nS61\nS7.4 Motif M6 in the Twitter follower network\nWe also analyzed the complete 2010 Twitter follower graph (58, 59, 71). The graph consists 41.65 million nodes (users) and 1.47 billion edges, where an edge from node i to node j signifies that user i is followed by user j on the social network. The network data was downloaded from http://law.di.unimi.it/webdata/twitter-2010/.\nWe used Algorithm 1 to find a motif-based cluster for motif M6 (the algorithm was run on the largest connected component of the motif adjacency matrix). The cluster contains 151 nodes and consists of two disconnected components. Here, we consider the smaller of the two components, which consists of 38 nodes. We also found an edge-based cluster on the undirected graph (using Algorithm 1 with motif Medge). This cluster consists of 44 nodes.\nFigure S14 illustrates the motif-based and edge-based clusters. Both clusters capture anomalies in the graph. The motif-based cluster consists of holding accounts for a photography company. The nodes that form bi-directional links have completed profiles (contain a profile picture) while several nodes with incomplete profiles (without a profile picture) are followed by the completed accounts. The edge-based cluster is a near clique, where the user screen names all begin with \u201cLC \u201d. We suspect that the similar usernames are either true social communities, holding accounts, or bots. (For the most part, their tweets are protected, so we could not verify if any of these scenarios are true). Interestingly, both M6 and Medge find anomalous clusters. However, their structures are quite different. We conclude that M6 can lead to the detection of new anomalous clusters in social networks.\nS62\nFigure S14: Clusters in the 2010 Twitter follower network (58, 59, 71). A\u2013C: Motif-based cluster (A) for motif M6 (B). All accounts are holding accounts for a photography company. The green nodes correspond to accounts that have completed profiles, while the orange accounts have incomplete profiles. The spy plot illustrates how the cluster is formed around this motif (C). D\u2013F: Cluster (D) for edge-based clustering (E). The cluster consists of a near-clique (F) where all users have the prefix \u201cLC \u201d.\nS63\nS7.5 Motif M7 in the Stanford web graph\nThe Stanford web graph (7, 56) consists of 281,903 nodes and 2,312,497 edges, where an edge from node i to node j means that there is a hyperlink from the ith web page to the jth web page. Here, all of the web pages come from the Stanford domain. The network data was downloaded from http://snap.stanford.edu/data/web-Stanford.html.\nWe used Algorithm 1 to find a motif-based cluster for motif M7, a motif that is overexpressed in web graphs (1). An illustration of the cluster and an edge-based cluster (i.e., using Algorithm 1 with Medge) are in Figure S15. Interestingly, both clusters exhibits a coreperiphery structure, albeit markedly different ones. The motif-based cluster contains several core nodes with large in-degree. Such core nodes comprise the sink node in motif M7. On the periphery are several clusters within which are many bi-directional links (as illustrated by the spy plot in Figure S15). The nodes in these clusters then up-link to the core nodes. This type of organizational unit suggests an explanation for why motif M7 is over-expressed: clusters of similar pages tend to uplink to more central pages. The edge-based cluster also has a few nodes with large in-degree, serving as a small core. On the periphery are the neighbors of these nodes, which themselves tend not to be connected (as illustrated by the spy plot).\nS64\nFigure S15: Clusters in the Stanford web graph (7). A\u2013C: Motif-based cluster (A) for motif M7 (B). The cluster has a core group of nodes with many incoming links (serving as the sink node in M7; shown in orange) and several periphery groups that are tied together (the bi-directional link in M7; shown in green) and also up-link to the core. This is evident from the spy plot (C). D\u2013F: Cluster (C) for undirected edges (B). The cluster contains a few high-degree nodes and their neighbors, and the neighbors tend to not be connected, as illustrated by the splot (F).\nS65\nS7.6 Semi-cliques in collaboration networks\nWe used Algorithm 1 to identify clusters of a four-node motif (the semi-clique) that has been studied in conjunction with researcher productivity in collaboration networks (72) (see Figure S16). We found a motif-based cluster in two different collaboration networks. Each one is derived from co-authorship in papers submitted to the arXiv under a certain category; here, we analyze the \u201dHigh Energy Physics\u2013Theory\u201d (HepTh) and \u201dCondensed Matter Physics\u201d (CondMat) categories (56,73). The HepTh network has 23,133 nodes and 93,497 edges and the CondMat network has 9,877 nodes and 25,998 edges. The HepTh network data was downloaded from http://snap.stanford.edu/data/ca-HepTh.html and the CondMat network data was downloaded from http://snap.stanford.edu/data/ca-CondMat. html.\nFigure S16 shows the two clusters for each of the collaboration networks. In both networks, the motif-based cluster consists of a core group of nodes and similarly-sized groups on the periphery. The core group of nodes correspond to the nodes of degree 3 in the motif and the periphery group nodes correspond to the nodes of degree 2. One explanation for this organization is that there is a small small group of authors that writes papers with different research groups. Alternatively, the co-authorship could come from a single research group, where senior authors are included on all of the papers and junior authors on a subset of the papers.\nOn the other hand, the edge-based clusters (i.e., result of Algorithm 1 for Medge) are a clique in the HepTh netowork and a clique with a few dangling nodes in the CondMat network. The dense clusters are quite different from the sparser clusters based on the semi-clique. Such dense clusters are not that surprising. For example, a clique could arise from a single paper published by a group of authors.\nS66\nFigure S16: Clusters in co-authorship networks (73). A\u2013E: Best motif-based cluster for the semi-clique motif (E) in the High Energy Physics\u2013Theory collaboration network (A) and the Condensed Matter Physics collaboration network (C). Corresponding spy plots are shown in (B) and (D). F\u2013I: Best edge-based (I) cluster in the High Energy Physics\u2013Theory collaboration network (F) and the Condensed Matter Physics collaboration network (H). Corresponding spy plots are shown in (G) and (I).\nS67"
                },
                {
                    "heading": "S8 Data availability",
                    "text": "All data is available at our project web site at http://snap.stanford.edu/higher-order/. The web site includes links to datasets used for experiments throughout the supplementary material (7, 56, 58\u201360, 74\u201383).\nS68\nReferences and Notes\n1. R. Milo, et al., Science 298, 824 (2002).\n2. S. Mangan, A. Zaslaver, U. Alon, Journal of molecular biology 334, 197 (2003).\n3. J. Yang, J. Leskovec, Proceedings of the IEEE 102, 1892 (2014).\n4. P. W. Holland, S. Leinhardt, American Journal of Sociology pp. 492\u2013513 (1970).\n5. M. Rosvall, A. V. Esquivel, A. Lancichinetti, J. D. West, R. Lambiotte, Nature communi-\ncations 5 (2014).\n6. N. Prz\u030culj, D. G. Corneil, I. Jurisica, Bioinformatics 20, 3508 (2004).\n7. J. Leskovec, K. J. Lang, A. Dasgupta, M. W. Mahoney, Internet Mathematics 6, 29 (2009).\n8. O\u0308. N. Yaverog\u0306lu, et al., Scientific reports 4 (2014).\n9. S. Mangan, U. Alon, Proceedings of the National Academy of Sciences 100, 11980 (2003).\n10. C. J. Honey, R. Ko\u0308tter, M. Breakspear, O. Sporns, Proceedings of the National Academy of\nSciences 104, 10240 (2007).\n11. S. E. Schaeffer, Computer Science Review 1, 27 (2007).\n12. Minimizing \u03c6M(S) is NP-hard, which follows from the NP-hardness of the traditional def-\ninition of conductance (68).\n13. See the Supplementary Material.\n14. Formally, when the motif has three nodes, the selected cluster S satisfies \u03c6M(S) \u2264 4 \u221a \u03c6\u2217M \u2264 1, where \u03c6\u2217M is the smallest motif conductance of any possible node set S.\nThis inequality is proved in the Supplementary Material.\nS69\n15. The normalized motif Laplacian matrix is LM = D\u22121/2(D\u2212WM)D\u22121/2, where D is a di-\nagonal matrix with the row-sums of WM on the diagonal (Dii = \u2211 j(WM)ij), and D \u22121/2 is\nthe same matrix with the inverse square-roots on the diagonal (D\u22121/2ii = 1/ \u221a\u2211 j(WM)ij). The spectral ordering \u03c3 is the by-value ordering of D\u22121/2z, where z is the eigenvector corresponding to the second smallest eigenvalue of LM , i.e., \u03c3i is the index of D\u22121/2z with the ith smallest value.\n16. C. Seshadhri, A. Pinar, T. G. Kolda, Statistical Analysis and Data Mining: The ASA Data\nScience Journal 7, 294 (2014).\n17. R. Andersen, F. Chung, K. Lang, Proceedings of the 47th Annual IEEE Symposium on\nFoundations of Computer Science (2006), pp. 475\u2013486.\n18. J. J. Whang, I. S. Dhillon, D. F. Gleich, SIAM Data Mining (2015).\n19. A. Y. Ng, M. I. Jordan, Y. Weiss, Advances in Neural Information Processing Systems 14\n(2002), pp. 849\u2013856.\n20. D. Boley, Data Mining and Knowledge Discovery 2, 325 (1998).\n21. D. L. Riddle, T. Blumenthal, B. J. Meyer, et al., eds., C. elegans II (Cold Spring Harbor\nLaboratory Press, 1997), second edn.\n22. H. Lee, et al., Nature neuroscience 15, 107 (2012).\n23. B. J. Frey, D. Dueck, Science 315, 972 (2007).\n24. B. Serrour, A. Arenas, S. Go\u0301mez, Computer Communications 34, 629 (2011).\n25. T. Michoel, A. Joshi, B. Nachtergaele, Y. Van de Peer, Molecular BioSystems 7, 2769\n(2011).\nS70\n26. A. R. Benson, D. F. Gleich, J. Leskovec, SIAM Data Mining (2015).\n27. F. Krzakala, et al., Proceedings of the National Academy of Sciences 110, 20935 (2013).\n28. M. Kaiser, C. C. Hilgetag, PLoS Computational Biology 2, e95 (2006).\n29. U. Alon, Nature Reviews Genetics 8, 450 (2007).\n30. O. Sporns, R. Ko\u0308tter, PLoS Biology 2, e369 (2004).\n31. A. Inokuchi, T. Washio, H. Motoda, Principles of Data Mining and Knowledge Discovery\n(Springer, 2000), pp. 13\u201323.\n32. F. R. Chung, Proceedings of ICCM (Citeseer, 2007), vol. 2, p. 378.\n33. J. R. Lee, S. O. Gharan, L. Trevisan, Journal of the ACM 61, 37 (2014).\n34. F. Chung, Annals of Combinatorics 9, 1 (2005).\n35. D. Boley, G. Ranjan, Z.-L. Zhang, Linear Algebra and its Applications 435, 224 (2011).\n36. F. D. Malliaros, M. Vazirgiannis, Physics Reports 533, 95 (2013).\n37. G. Karypis, R. Aggarwal, V. Kumar, S. Shekhar, Very Large Scale Integration (VLSI) Sys-\ntems, IEEE Transactions on 7, 69 (1999).\n38. S. Agarwal, K. Branson, S. Belongie, Proceedings of the 23rd International Conference on\nMachine Learning (ACM, 2006), pp. 17\u201324.\n39. D. Zhou, J. Huang, B. Scho\u0308lkopf, Advances in Neural Information Processing Systems 19\n(MIT Press, 2006), pp. 1601\u20131608.\n40. J. Rodr\u0131\u0301guez, Linear and Multilinear Algebra 50, 1 (2002).\nS71\n41. L. Trevisan, Lecture notes on expansion, sparsest cut, and spectral graph theory, http://\nwww.eecs.berkeley.edu/\u02dcluca/books/expanders.pdf. Accessed June 28, 2015.\n42. S. Demeyer, et al., PloS ONE 8, e61183 (2013).\n43. M. Houbraken, et al., PLoS ONE 9, e97896 (2014).\n44. S. Wernicke, IEEE/ACM Transactions on Computational Biology and Bioinformatics 3,\n347 (2006).\n45. S. Wernicke, F. Rasche, Bioinformatics 22, 1152 (2006).\n46. C. R. Aberger, A. No\u0308tzli, K. Olukotun, C. Re\u0301, arXiv preprint arXiv:1503.02368 (2015).\n47. M. Latapy, Theoretical Computer Science 407, 458 (2008).\n48. J. W. Berry, et al., Proceedings of the 5th Conference on Innovations in Theoretical Com-\nputer Science (ACM, New York, NY, USA, 2014), pp. 225\u2013234.\n49. D. Marcus, Y. Shavitt, IEEE 30th International Conference on Distributed Computing Sys-\ntems Workshops (2010), pp. 92\u201398.\n50. N. Chiba, T. Nishizeki, SIAM Journal on Computing 14, 210 (1985).\n51. T. Schank, D. Wagner, Experimental and Efficient Algorithms (Springer, 2005), pp. 606\u2013\n609.\n52. L. Becchetti, P. Boldi, C. Castillo, A. Gionis, Proceedings of the 14th ACM SIGKDD inter-\nnational conference on Knowledge discovery and data mining (ACM, 2008), pp. 16\u201324.\n53. J. Cohen, Computing in Science & Engineering 11, 29 (2009).\nS72\n54. B. N. Parlett, The Symmetric Eigenvalue Problem, vol. 7 (SIAM, 1980).\n55. K. J. Maschhoff, D. C. Sorensen, Applied Parallel Computing Industrial Computation and\nOptimization (Springer, 1996), pp. 478\u2013486.\n56. J. Leskovec, A. Krevl, SNAP Datasets: Stanford large network dataset collection, http:\n//snap.stanford.edu/data (2014).\n57. P. Boldi, B. Codenotti, M. Santini, S. Vigna, Software: Practice and Experience 34, 711\n(2004).\n58. P. Boldi, S. Vigna, Proceedings of the 13th International Conference on World Wide Web\n(ACM, 2004), pp. 595\u2013602.\n59. P. Boldi, M. Rosa, M. Santini, S. Vigna, Proceedings of the 20th International Conference\non World Wide Web (ACM, 2011), pp. 587\u2013596.\n60. P. Boldi, A. Marino, M. Santini, S. Vigna, Proceedings of the companion publication of the\n23rd international conference on World wide web companion (International World Wide Web Conferences Steering Committee, 2014), pp. 227\u2013228.\n61. A. Azad, A. Buluc\u0327, J. R. Gilbert, Proceedings of the IPDPSW, Workshop on Graph Algo-\nrithm Building Blocks (GABB) (2015), pp. 804\u2013811.\n62. M. Rosvall, C. T. Bergstrom, Proceedings of the National Academy of Sciences 105, 1118\n(2008).\n63. V. D. Blondel, J.-L. Guillaume, R. Lambiotte, E. Lefebvre, Journal of statistical mechanics:\ntheory and experiment 2008, P10008 (2008).\nS73\n64. R. E. Ulanowicz, C. Bondavalli, M. S. Egnotovich, Trophic Dynamics in South Florida\nEcosystem, FY 97: The Florida Bay Ecosystem, Tech. Rep. CBL 98-123, Chesapeake Biological Laboratory, Solomons, MD (1998).\n65. J. Bascompte, C. J. Melia\u0301n, E. Sala, Proceedings of the National Academy of Sciences of\nthe United States of America 102, 5443 (2005).\n66. J. Bascompte, et al., Science 325, 416 (2009).\n67. D. B. Stouffer, J. Camacho, W. Jiang, L. A. N. Amaral, Proceedings of the Royal Society of\nLondon B: Biological Sciences 274, 1931 (2007).\n68. D. Wagner, F. Wagner, Proceedings of the 18th International Symposium on Mathematical\nFoundations of Computer Science (1993), pp. 744\u2013750.\n69. C. D. Manning, P. Raghavan, H. Schu\u0308tze, et al., Introduction to Information Retrieval,\nvol. 1 (Cambridge university press Cambridge, 2008).\n70. R. Dobrin, Q. K. Beg, A.-L. Baraba\u0301si, Z. N. Oltvai, BMC bioinformatics 5, 10 (2004).\n71. H. Kwak, C. Lee, H. Park, S. Moon, Proceedings of the 19th International Conference on\nWorld Wide Web (ACM, 2010), pp. 591\u2013600.\n72. T. Chakraborty, N. Ganguly, A. Mukherjee, Advances in Social Networks Analysis and\nMining (ASONAM), 2014 IEEE/ACM International Conference on (IEEE, 2014), pp. 130\u2013 137.\n73. J. Leskovec, J. Kleinberg, C. Faloutsos, ACM Transactions on Knowledge Discovery from\nData (TKDD) 1, 2 (2007).\nS74\n74. R. West, H. S. Paskov, J. Leskovec, C. Potts, Transactions of the Association for Computa-\ntional Linguistics 2, 297 (2014).\n75. J. Leskovec, J. Kleinberg, C. Faloutsos, Proceedings of the eleventh ACM SIGKDD inter-\nnational conference on Knowledge discovery in data mining (ACM, 2005), pp. 177\u2013187.\n76. J. Gehrke, P. Ginsparg, J. Kleinberg, ACM SIGKDD Explorations Newsletter 5, 149 (2003).\n77. R. Albert, H. Jeong, A.-L. Baraba\u0301si, Nature 401, 130 (1999).\n78. J. Leskovec, L. A. Adamic, B. A. Huberman, ACM Transactions on the Web (TWEB) 1, 5\n(2007).\n79. J. Leskovec, D. P. Huttenlocher, J. M. Kleinberg, ICWSM (2010).\n80. J. Leskovec, J. J. Mcauley, Advances in neural information processing systems (2012), pp.\n539\u2013547.\n81. L. Takac, M. Zabovsky, International Scientific Conference and International Workshop\nPresent Day Trends of Innovations (2012), pp. 1\u20136.\n82. L. Backstrom, D. Huttenlocher, J. Kleinberg, X. Lan, Proceedings of the 12th ACM\nSIGKDD international conference on Knowledge discovery and data mining (ACM, 2006), pp. 44\u201354.\n83. J. Yang, J. Leskovec, 2012 IEEE 12th International Conference on Data Mining (IEEE,\n2012), pp. 745\u2013754.\nS75\nAuthors would like to thank Rok Sosic\u030c for insightful comments. ARB acknowledges the support of a Stanford Graduate Fellowship. DFG acknowledges the support of NSF CCF1149756 and IIS-1422918 and DARPA SIMPLEX. JL acknowledges the support of NSF IIS1149837 and CNS-1010921, NIH BD2K, DARPA XDATA and SIMPLEX, Boeing, Lightspeed, and Volkswagen.\nS76"
                }
            ],
            "year": 2016,
            "references": [
                {
                    "title": "U",
                    "authors": [
                        "S. Mangan"
                    ],
                    "venue": "Alon, Proceedings of the National Academy of Sciences 100,",
                    "year": 1198
                },
                {
                    "title": "O",
                    "authors": [
                        "C.J. Honey",
                        "R. K\u00f6tter",
                        "M. Breakspear"
                    ],
                    "venue": "Sporns, Proceedings of the National Academy of Sciences 104,",
                    "year": 1024
                },
                {
                    "title": "Statistical Analysis and Data Mining: The ASA Data",
                    "authors": [
                        "C. Seshadhri",
                        "A. Pinar",
                        "T.G. Kolda"
                    ],
                    "venue": "Science Journal 7,",
                    "year": 2014
                },
                {
                    "title": "Lecture notes on expansion, sparsest cut, and spectral graph theory, http:// www.eecs.berkeley.edu/ \u0303luca/books/expanders.pdf",
                    "authors": [
                        "L. Trevisan"
                    ],
                    "venue": "Accessed June",
                    "year": 2015
                },
                {
                    "title": "Krevl, SNAP Datasets: Stanford large network dataset collection, http: //snap.stanford.edu/data",
                    "authors": [
                        "A.J. Leskovec"
                    ],
                    "year": 2014
                },
                {
                    "title": "E",
                    "authors": [
                        "V.D. Blondel",
                        "J.-L. Guillaume",
                        "R. Lambiotte"
                    ],
                    "venue": "Lefebvre, Journal of statistical mechanics: theory and experiment",
                    "year": 2008
                },
                {
                    "title": "Introduction to Information Retrieval, vol. 1 (Cambridge university",
                    "authors": [
                        "C.D. Manning",
                        "P. Raghavan",
                        "H. Sch\u00fctze"
                    ],
                    "year": 2008
                },
                {
                    "title": "Advances in Social Networks Analysis and Mining (ASONAM), 2014",
                    "authors": [
                        "T. Chakraborty",
                        "N. Ganguly",
                        "A. Mukherjee"
                    ],
                    "venue": "IEEE/ACM International Conference on (IEEE,",
                    "year": 2014
                },
                {
                    "title": "Authors would like to thank Rok Sosi\u010d for insightful comments. ARB acknowledges the support of a Stanford Graduate Fellowship. DFG acknowledges the support of NSF CCF1149756 and IIS-1422918 and DARPA SIMPLEX. JL acknowledges the support of NSF IIS1149837 and CNS-1010921, NIH BD2K, DARPA XDATA and SIMPLEX, Boeing, Lightspeed, and Volkswagen",
                    "authors": [
                        "J. Yang",
                        "J. Leskovec"
                    ],
                    "venue": "IEEE 12th International Conference on Data Mining (IEEE,",
                    "year": 2012
                }
            ],
            "id": "SP:e25948ed2372ea83f18bc494d2fca7369bb102a5",
            "authors": [
                {
                    "name": "Austin R. Benson",
                    "affiliations": []
                },
                {
                    "name": "David F. Gleich",
                    "affiliations": []
                },
                {
                    "name": "Jure Leskovec",
                    "affiliations": []
                }
            ],
            "abstractText": "Networks are a fundamental tool for understanding and modeling complex systems in physics, biology, neuroscience, engineering, and social science. Many networks are known to exhibit rich, lower-order connectivity patterns that can be captured at the level of individual nodes and edges. However, higher-order organization of complex networks\u2014at the level of small network subgraphs\u2014 remains largely unknown. Here we develop a generalized framework for clustering networks based on higher-order connectivity patterns. This framework provides mathematical guarantees on the optimality of obtained clusters and scales to networks with billions of edges. The framework reveals higher-order organization in a number of networks including information propagation units in neuronal networks and hub structure in transportation networks. Results show that networks exhibit rich higher-order organizational structures that are exposed by clustering based on higher-order connectivity patterns.",
            "title": "Higher-order organization of complex networks"
        },
        "Y": {
            "blog_id": "higher-order-organization-of-complex-networks",
            "summary": [
                "The paper presents a generalized framework for graph clustering (clusters of network motifs) on the basis of higher-order connectivity patterns.",
                "Approach  Given a motif M , the framework aims to find a cluster of the set of nodes S such that nodes of S participate in many instances of M and avoid cutting instances of M (that is only a subset of nodes in instances of M appears in S).",
                "Mathematically, the aim is to minimise the motif conductance metric given as cutM(S, S\u2019) / min[volM(S), volM(S\u2019)] where S\u2019 is complement of S, cutM(S, S\u2019) = number of instances of M which have atleast one node from both S and S\u2019 and volM(S) = Number of nodes in instances of M that belong only to S.  Solving the above equation is computationally infeasible and an approximate solution is proposed using eigenvalues and matrices.",
                "The approximate solution is easy to implement, efficient and guaranteed to find clusters that are at most a quadratic factor away from the optimal.",
                "Algorithm  Given the network and motif M, form a motif adjacency matrix WM where WM(i, j) is the number of instances of M that contains i and j.  Compute spectral ordering of the nodes from normalized motif laplacian matrix.",
                "Compute prefix set of spectral ordering with small motif conductance.",
                "Scalability  Worst case O(m1.5), based on experiments O(m1.2) where m is the number of edges.",
                "Advantages  Applicable to directed, undirected and weighted graphs (allows for negative edge weights as well).",
                "In case the motif is not known beforehand, the framework can be used to compute significant motifs.",
                "The proposed framework unifies the two fundamental tools of network science (motif analysis and network partitioning) along with some worst-case guarantees for the approximations employed and can be extended to identify higher order modular organization of networks."
            ],
            "author_id": "shugan",
            "pdf_url": "https://arxiv.org/pdf/1612.08447.pdf",
            "author_full_name": "Shagun Sodhani",
            "source_website": "https://github.com/shagunsodhani/papers-I-read",
            "id": 78860785
        }
    },
    "25710556": {
        "X": {
            "sections": [
                {
                    "heading": "1 INTRODUCTION",
                    "text": "Generative methods that produce novel samples from high-dimensional data distributions, such as images, are finding widespread use, for example in speech synthesis (van den Oord et al., 2016a), image-to-image translation (Zhu et al., 2017; Liu et al., 2017; Wang et al., 2017), and image inpainting (Iizuka et al., 2017). Currently the most prominent approaches are autoregressive models (van den Oord et al., 2016b;c), variational autoencoders (VAE) (Kingma & Welling, 2014), and generative adversarial networks (GAN) (Goodfellow et al., 2014). Currently they all have significant strengths and weaknesses. Autoregressive models \u2013 such as PixelCNN \u2013 produce sharp images but are slow to evaluate and do not have a latent representation as they directly model the conditional distribution over pixels, potentially limiting their applicability. VAEs are easy to train but tend to produce blurry results due to restrictions in the model, although recent work is improving this (Kingma et al., 2016). GANs produce sharp images, albeit only in fairly small resolutions and with somewhat limited variation, and the training continues to be unstable despite recent progress (Salimans et al., 2016; Gulrajani et al., 2017; Berthelot et al., 2017; Kodali et al., 2017). Hybrid methods combine various strengths of the three, but so far lag behind GANs in image quality (Makhzani & Frey, 2017; Ulyanov et al., 2017; Dumoulin et al., 2016).\nTypically, a GAN consists of two networks: generator and discriminator (aka critic). The generator produces a sample, e.g., an image, from a latent code, and the distribution of these images should ideally be indistinguishable from the training distribution. Since it is generally infeasible to engineer a function that tells whether that is the case, a discriminator network is trained to do the assessment, and since networks are differentiable, we also get a gradient we can use to steer both networks to the right direction. Typically, the generator is of main interest \u2013 the discriminator is an adaptive loss function that gets discarded once the generator has been trained.\nThere are multiple potential problems with this formulation. When we measure the distance between the training distribution and the generated distribution, the gradients can point to more or less random directions if the distributions do not have substantial overlap, i.e., are too easy to tell apart (Arjovsky & Bottou, 2017). Originally, Jensen-Shannon divergence was used as a distance metric (Goodfellow et al., 2014), and recently that formulation has been improved (Hjelm et al., 2017) and a number of more stable alternatives have been proposed, including least squares (Mao et al., 2016b), absolute deviation with margin (Zhao et al., 2017), and Wasserstein distance (Arjovsky et al., 2017; Gulrajani\nar X\niv :1\n71 0.\n10 19\n6v 3\n[ cs\n.N E\n] 2\n6 Fe\nb 20\n18\net al., 2017). Our contributions are largely orthogonal to this ongoing discussion, and we primarily use the improved Wasserstein loss, but also experiment with least-squares loss.\nThe generation of high-resolution images is difficult because higher resolution makes it easier to tell the generated images apart from training images (Odena et al., 2017), thus drastically amplifying the gradient problem. Large resolutions also necessitate using smaller minibatches due to memory constraints, further compromising training stability. Our key insight is that we can grow both the generator and discriminator progressively, starting from easier low-resolution images, and add new layers that introduce higher-resolution details as the training progresses. This greatly speeds up training and improves stability in high resolutions, as we will discuss in Section 2.\nThe GAN formulation does not explicitly require the entire training data distribution to be represented by the resulting generative model. The conventional wisdom has been that there is a tradeoff between image quality and variation, but that view has been recently challenged (Odena et al., 2017). The degree of preserved variation is currently receiving attention and various methods have been suggested for measuring it, including inception score (Salimans et al., 2016), multi-scale structural similarity (MS-SSIM) (Odena et al., 2017; Wang et al., 2003), birthday paradox (Arora & Zhang, 2017), and explicit tests for the number of discrete modes discovered (Metz et al., 2016). We will describe our method for encouraging variation in Section 3, and propose a new metric for evaluating the quality and variation in Section 5.\nSection 4.1 discusses a subtle modification to the initialization of networks, leading to a more balanced learning speed for different layers. Furthermore, we observe that mode collapses traditionally plaguing GANs tend to happen very quickly, over the course of a dozen minibatches. Commonly they start when the discriminator overshoots, leading to exaggerated gradients, and an unhealthy competition follows where the signal magnitudes escalate in both networks. We propose a mechanism to stop the generator from participating in such escalation, overcoming the issue (Section 4.2).\nWe evaluate our contributions using the CELEBA, LSUN, CIFAR10 datasets. We improve the best published inception score for CIFAR10. Since the datasets commonly used in benchmarking generative methods are limited to a fairly low resolution, we have also created a higher quality version of the CELEBA dataset that allows experimentation with output resolutions up to 1024 \u00d7 1024 pixels. This dataset and our full implementation are available at https://github.com/tkarras/progressive_growing_of_gans, trained networks can be found at https://drive.google.com/open?id=0B4qLcYyJmiz0NHFULTdYc05lX0U along with result images, and a supplementary video illustrating the datasets, additional results, and latent space interpolations is at https://youtu.be/G06dEcZ-QTg."
                },
                {
                    "heading": "2 PROGRESSIVE GROWING OF GANS",
                    "text": "Our primary contribution is a training methodology for GANs where we start with low-resolution images, and then progressively increase the resolution by adding layers to the networks as visualized in Figure 1. This incremental nature allows the training to first discover large-scale structure of the image distribution and then shift attention to increasingly finer scale detail, instead of having to learn all scales simultaneously.\nWe use generator and discriminator networks that are mirror images of each other and always grow in synchrony. All existing layers in both networks remain trainable throughout the training process. When new layers are added to the networks, we fade them in smoothly, as illustrated in Figure 2. This avoids sudden shocks to the already well-trained, smaller-resolution layers. Appendix A describes structure of the generator and discriminator in detail, along with other training parameters.\nWe observe that the progressive training has several benefits. Early on, the generation of smaller images is substantially more stable because there is less class information and fewer modes (Odena et al., 2017). By increasing the resolution little by little we are continuously asking a much simpler question compared to the end goal of discovering a mapping from latent vectors to e.g. 10242 images. This approach has conceptual similarity to recent work by Chen & Koltun (2017). In practice it stabilizes the training sufficiently for us to reliably synthesize megapixel-scale images using WGAN-GP loss (Gulrajani et al., 2017) and even LSGAN loss (Mao et al., 2016b).\nthroughout the process. Here N \u00d7N refers to convolutional layers operating on N \u00d7 N spatial resolution. This allows stable synthesis in high resolutions and also speeds up training considerably. One the right we show six example images generated using progressive growing at 1024\u00d7 1024.\nAnother benefit is the reduced training time. With progressively growing GANs most of the iterations are done at lower resolutions, and comparable result quality is often obtained up to 2\u20136 times faster, depending on the final output resolution.\nThe idea of growing GANs progressively is related to the work of Wang et al. (2017), who use multiple discriminators that operate on different spatial resolutions. That work in turn is motivated by Durugkar et al. (2016) who use one generator and multiple discriminators concurrently, and Ghosh et al. (2017) who do the opposite with multiple generators and one discriminator. Hierarchical GANs (Denton et al., 2015; Huang et al., 2016; Zhang et al., 2017) define a generator and discriminator for each level of an image pyramid. These methods build on the same observation as our work \u2013 that the complex mapping from latents to high-resolution images is easier to learn in steps \u2013 but the crucial difference is that we have only a single GAN instead of a hierarchy of them. In contrast to early work on adaptively growing networks, e.g., growing neural gas (Fritzke, 1995) and neuro evolution of augmenting topologies (Stanley & Miikkulainen, 2002) that grow networks greedily, we simply defer the introduction of pre-configured layers. In that sense our approach resembles layer-wise training of autoencoders (Bengio et al., 2007)."
                },
                {
                    "heading": "3 INCREASING VARIATION USING MINIBATCH STANDARD DEVIATION",
                    "text": "GANs have a tendency to capture only a subset of the variation found in training data, and Salimans et al. (2016) suggest \u201cminibatch discrimination\u201d as a solution. They compute feature statistics not only from individual images but also across the minibatch, thus encouraging the minibatches of generated and training images to show similar statistics. This is implemented by adding a minibatch layer towards the end of the discriminator, where the layer learns a large tensor that projects the input activation to an array of statistics. A separate set of statistics is produced for each example in a minibatch and it is concatenated to the layer\u2019s output, so that the discriminator can use the statistics internally. We simplify this approach drastically while also improving the variation.\nOur simplified solution has neither learnable parameters nor new hyperparameters. We first compute the standard deviation for each feature in each spatial location over the minibatch. We then average these estimates over all features and spatial locations to arrive at a single value. We replicate the value and concatenate it to all spatial locations and over the minibatch, yielding one additional (constant) feature map. This layer could be inserted anywhere in the discriminator, but we have found it best to insert it towards the end (see Appendix A.1 for details). We experimented with a richer set of statistics, but were not able to improve the variation further. In parallel work, Lin et al. (2017) provide theoretical insights about the benefits of showing multiple images to the discriminator.\nresidual block, whose weight \u03b1 increases linearly from 0 to 1. Here 2\u00d7 and 0.5\u00d7 refer to doubling and halving the image resolution using nearest neighbor filtering and average pooling, respectively. The toRGB represents a layer that projects feature vectors to RGB colors and fromRGB does the reverse; both use 1 \u00d7 1 convolutions. When training the discriminator, we feed in real images that are downscaled to match the current resolution of the network. During a resolution transition, we interpolate between two resolutions of the real images, similarly to how the generator output combines two resolutions.\nAlternative solutions to the variation problem include unrolling the discriminator (Metz et al., 2016) to regularize its updates, and a \u201crepelling regularizer\u201d (Zhao et al., 2017) that adds a new loss term to the generator, trying to encourage it to orthogonalize the feature vectors in a minibatch. The multiple generators of Ghosh et al. (2017) also serve a similar goal. We acknowledge that these solutions may increase the variation even more than our solution \u2013 or possibly be orthogonal to it \u2013 but leave a detailed comparison to a later time."
                },
                {
                    "heading": "4 NORMALIZATION IN GENERATOR AND DISCRIMINATOR",
                    "text": "GANs are prone to the escalation of signal magnitudes as a result of unhealthy competition between the two networks. Most if not all earlier solutions discourage this by using a variant of batch normalization (Ioffe & Szegedy, 2015; Salimans & Kingma, 2016; Ba et al., 2016) in the generator, and often also in the discriminator. These normalization methods were originally introduced to eliminate covariate shift. However, we have not observed that to be an issue in GANs, and thus believe that the actual need in GANs is constraining signal magnitudes and competition. We use a different approach that consists of two ingredients, neither of which include learnable parameters."
                },
                {
                    "heading": "4.1 EQUALIZED LEARNING RATE",
                    "text": "We deviate from the current trend of careful weight initialization, and instead use a trivial N (0, 1) initialization and then explicitly scale the weights at runtime. To be precise, we set w\u0302i = wi/c, where wi are the weights and c is the per-layer normalization constant from He\u2019s initializer (He et al., 2015). The benefit of doing this dynamically instead of during initialization is somewhat subtle, and relates to the scale-invariance in commonly used adaptive stochastic gradient descent methods such as RMSProp (Tieleman & Hinton, 2012) and Adam (Kingma & Ba, 2015). These methods normalize a gradient update by its estimated standard deviation, thus making the update independent of the scale of the parameter. As a result, if some parameters have a larger dynamic range than others, they will take longer to adjust. This is a scenario modern initializers cause, and thus it is possible that a learning rate is both too large and too small at the same time. Our approach ensures that the dynamic range, and thus the learning speed, is the same for all weights. A similar reasoning was independently used by van Laarhoven (2017)."
                },
                {
                    "heading": "4.2 PIXELWISE FEATURE VECTOR NORMALIZATION IN GENERATOR",
                    "text": "To disallow the scenario where the magnitudes in the generator and discriminator spiral out of control as a result of competition, we normalize the feature vector in each pixel to unit length in the generator after each convolutional layer. We do this using a variant of \u201clocal response normaliza-\ntion\u201d (Krizhevsky et al., 2012), configured as bx,y = ax,y/ \u221a 1 N \u2211N\u22121 j=0 (a j x,y)2 + , where = 10\u22128, N is the number of feature maps, and ax,y and bx,y are the original and normalized feature vector in pixel (x, y), respectively. We find it surprising that this heavy-handed constraint does not seem to harm the generator in any way, and indeed with most datasets it does not change the results much, but it prevents the escalation of signal magnitudes very effectively when needed."
                },
                {
                    "heading": "5 MULTI-SCALE STATISTICAL SIMILARITY FOR ASSESSING GAN RESULTS",
                    "text": "In order to compare the results of one GAN to another, one needs to investigate a large number of images, which can be tedious, difficult, and subjective. Thus it is desirable to rely on automated methods that compute some indicative metric from large image collections. We noticed that existing methods such as MS-SSIM (Odena et al., 2017) find large-scale mode collapses reliably but fail to react to smaller effects such as loss of variation in colors or textures, and they also do not directly assess image quality in terms of similarity to the training set.\nWe build on the intuition that a successful generator will produce samples whose local image structure is similar to the training set over all scales. We propose to study this by considering the multiscale statistical similarity between distributions of local image patches drawn from Laplacian pyramid (Burt & Adelson, 1987) representations of generated and target images, starting at a low-pass resolution of 16 \u00d7 16 pixels. As per standard practice, the pyramid progressively doubles until the full resolution is reached, each successive level encoding the difference to an up-sampled version of the previous level.\nA single Laplacian pyramid level corresponds to a specific spatial frequency band. We randomly sample 16384 images and extract 128 descriptors from each level in the Laplacian pyramid, giving us 221 (2.1M) descriptors per level. Each descriptor is a 7 \u00d7 7 pixel neighborhood with 3 color channels, denoted by x \u2208 R7\u00d77\u00d73 = R147. We denote the patches from level l of the training set and generated set as {xli}2 21 i=1 and {yli}2 21\ni=1, respectively. We first normalize {xli} and {yli} w.r.t. the mean and standard deviation of each color channel, and then estimate the statistical similarity by computing their sliced Wasserstein distance SWD({xli}, {yli}), an efficiently computable randomized approximation to earthmovers distance, using 512 projections (Rabin et al., 2011).\nIntuitively a small Wasserstein distance indicates that the distribution of the patches is similar, meaning that the training images and generator samples appear similar in both appearance and variation at this spatial resolution. In particular, the distance between the patch sets extracted from the lowestresolution 16 \u00d7 16 images indicate similarity in large-scale image structures, while the finest-level patches encode information about pixel-level attributes such as sharpness of edges and noise."
                },
                {
                    "heading": "6 EXPERIMENTS",
                    "text": "In this section we discuss a set of experiments that we conducted to evaluate the quality of our results. Please refer to Appendix A for detailed description of our network structures and training configurations. We also invite the reader to consult the accompanying video (https://youtu.be/G06dEcZ-QTg) for additional result images and latent space interpolations. In this section we will distinguish between the network structure (e.g., convolutional layers, resizing), training configuration (various normalization layers, minibatch-related operations), and training loss (WGAN-GP, LSGAN)."
                },
                {
                    "heading": "6.1 IMPORTANCE OF INDIVIDUAL CONTRIBUTIONS IN TERMS OF STATISTICAL SIMILARITY",
                    "text": "We will first use the sliced Wasserstein distance (SWD) and multi-scale structural similarity (MSSSIM) (Odena et al., 2017) to evaluate the importance our individual contributions, and also perceptually validate the metrics themselves. We will do this by building on top of a previous state-of-theart loss function (WGAN-GP) and training configuration (Gulrajani et al., 2017) in an unsupervised setting using CELEBA (Liu et al., 2015) and LSUN BEDROOM (Yu et al., 2015) datasets in 1282\nresolution. CELEBA is particularly well suited for such comparison because the training images contain noticeable artifacts (aliasing, compression, blur) that are difficult for the generator to reproduce faithfully. In this test we amplify the differences between training configurations by choosing a relatively low-capacity network structure (Appendix A.2) and terminating the training once the discriminator has been shown a total of 10M real images. As such the results are not fully converged.\nTable 1 lists the numerical values for SWD and MS-SSIM in several training configurations, where our individual contributions are cumulatively enabled one by one on top of the baseline (Gulrajani et al., 2017). The MS-SSIM numbers were averaged from 10000 pairs of generated images, and SWD was calculated as described in Section 5. Generated CELEBA images from these configurations are shown in Figure 3. Due to space constraints, the figure shows only a small number of examples for each row of the table, but a significantly broader set is available in Appendix H. Intuitively, a good evaluation metric should reward plausible images that exhibit plenty of variation in colors, textures, and viewpoints. However, this is not captured by MS-SSIM: we can immediately see that configuration (h) generates significantly better images than configuration (a), but MS-SSIM remains approximately unchanged because it measures only the variation between outputs, not similarity to the training set. SWD, on the other hand, does indicate a clear improvement.\nThe first training configuration (a) corresponds to Gulrajani et al. (2017), featuring batch normalization in the generator, layer normalization in the discriminator, and minibatch size of 64. (b) enables progressive growing of the networks, which results in sharper and more believable output images. SWD correctly finds the distribution of generated images to be more similar to the training set.\nOur primary goal is to enable high output resolutions, and this requires reducing the size of minibatches in order to stay within the available memory budget. We illustrate the ensuing challenges in (c) where we decrease the minibatch size from 64 to 16. The generated images are unnatural, which is clearly visible in both metrics. In (d), we stabilize the training process by adjusting the hyperparameters as well as by removing batch normalization and layer normalization (Appendix A.2). As an intermediate test (e\u2217), we enable minibatch discrimination (Salimans et al., 2016), which somewhat surprisingly fails to improve any of the metrics, including MS-SSIM that measures output variation. In contrast, our minibatch standard deviation (e) improves the average SWD scores and images. We then enable our remaining contributions in (f) and (g), leading to an overall improvement in SWD\nand subjective visual quality. Finally, in (h) we use a non-crippled network and longer training \u2013 we feel the quality of the generated images is at least comparable to the best published results so far."
                },
                {
                    "heading": "6.2 CONVERGENCE AND TRAINING SPEED",
                    "text": "Figure 4 illustrates the effect of progressive growing in terms of the SWD metric and raw image throughput. The first two plots correspond to the training configuration of Gulrajani et al. (2017) without and with progressive growing. We observe that the progressive variant offers two main benefits: it converges to a considerably better optimum and also reduces the total training time by about a factor of two. The improved convergence is explained by an implicit form of curriculum learning that is imposed by the gradually increasing network capacity. Without progressive growing, all layers of the generator and discriminator are tasked with simultaneously finding succinct intermediate representations for both the large-scale variation and the small-scale detail. With progressive growing, however, the existing low-resolution layers are likely to have already converged early on, so the networks are only tasked with refining the representations by increasingly smaller-scale effects as new layers are introduced. Indeed, we see in Figure 4(b) that the largest-scale statistical similarity curve (16) reaches its optimal value very quickly and remains consistent throughout the rest of the training. The smaller-scale curves (32, 64, 128) level off one by one as the resolution is increased, but the convergence of each curve is equally consistent. With non-progressive training in Figure 4(a), each scale of the SWD metric converges roughly in unison, as could be expected.\nThe speedup from progressive growing increases as the output resolution grows. Figure 4(c) shows training progress, measured in number of real images shown to the discriminator, as a function of training time when the training progresses all the way to 10242 resolution. We see that progressive growing gains a significant head start because the networks are shallow and quick to evaluate at the beginning. Once the full resolution is reached, the image throughput is equal between the two methods. The plot shows that the progressive variant reaches approximately 6.4 million images in 96 hours, whereas it can be extrapolated that the non-progressive variant would take about 520 hours to reach the same point. In this case, the progressive growing offers roughly a 5.4\u00d7 speedup."
                },
                {
                    "heading": "6.3 HIGH-RESOLUTION IMAGE GENERATION USING CELEBA-HQ DATASET",
                    "text": "To meaningfully demonstrate our results at high output resolutions, we need a sufficiently varied high-quality dataset. However, virtually all publicly available datasets previously used in GAN literature are limited to relatively low resolutions ranging from 322 to 4802. To this end, we created a high-quality version of the CELEBA dataset consisting of 30000 of the images at 1024 \u00d7 1024 resolution. We refer to Appendix C for further details about the generation of this dataset.\nOur contributions allow us to deal with high output resolutions in a robust and efficient fashion. Figure 5 shows selected 1024 \u00d7 1024 images produced by our network. While megapixel GAN results have been shown before in another dataset (Marchesi, 2017), our results are vastly more varied and of higher perceptual quality. Please refer to Appendix F for a larger set of result images as well as the nearest neighbors found from the training data. The accompanying video shows latent space interpolations and visualizes the progressive training. The interpolation works so that we first randomize a latent code for each frame (512 components sampled individually from N (0, 1)), then blur the latents across time with a Gaussian (\u03c3 = 45 frames @ 60Hz), and finally normalize each vector to lie on a hypersphere.\nWe trained the network on 8 Tesla V100 GPUs for 4 days, after which we no longer observed qualitative differences between the results of consecutive training iterations. Our implementation used an adaptive minibatch size depending on the current output resolution so that the available memory budget was optimally utilized.\nIn order to demonstrate that our contributions are largely orthogonal to the choice of a loss function, we also trained the same network using LSGAN loss instead of WGAN-GP loss. Figure 1 shows six examples of 10242 images produced using our method using LSGAN. Further details of this setup are given in Appendix B."
                },
                {
                    "heading": "6.4 LSUN RESULTS",
                    "text": "Figure 6 shows a purely visual comparison between our solution and earlier results in LSUN BEDROOM. Figure 7 gives selected examples from seven very different LSUN categories at 2562. A larger, non-curated set of results from all 30 LSUN categories is available in Appendix G, and the video demonstrates interpolations. We are not aware of earlier results in most of these categories, and while some categories work better than others, we feel that the overall quality is high."
                },
                {
                    "heading": "6.5 CIFAR10 INCEPTION SCORES",
                    "text": "The best inception scores for CIFAR10 (10 categories of 32 \u00d7 32 RGB images) we are aware of are 7.90 for unsupervised and 8.87 for label conditioned setups (Grinblat et al., 2017). The large difference between the two numbers is primarily caused by \u201cghosts\u201d that necessarily appear between classes in the unsupervised setting, while label conditioning can remove many such transitions.\nWhen all of our contributions are enabled, we get 8.80 in the unsupervised setting. Appendix D shows a representative set of generated images along with a more comprehensive list of results from earlier methods. The network and training setup were the same as for CELEBA, progression limited to 32 \u00d7 32 of course. The only customization was to the WGAN-GP\u2019s regularization term Ex\u0302\u223cPx\u0302 [(||\u2207x\u0302D(x\u0302)||2 \u2212 \u03b3)2/\u03b32]. Gulrajani et al. (2017) used \u03b3 = 1.0, which corresponds to 1-Lipschitz, but we noticed that it is in fact significantly better to prefer fast transitions (\u03b3 = 750) to minimize the ghosts. We have not tried this trick with other datasets."
                },
                {
                    "heading": "7 DISCUSSION",
                    "text": "While the quality of our results is generally high compared to earlier work on GANs, and the training is stable in large resolutions, there is a long way to true photorealism. Semantic sensibility and understanding dataset-dependent constraints, such as certain objects being straight rather than curved, leaves a lot to be desired. There is also room for improvement in the micro-structure of the images. That said, we feel that convincing realism may now be within reach, especially in CELEBA-HQ."
                },
                {
                    "heading": "8 ACKNOWLEDGEMENTS",
                    "text": "We would like to thank Mikael Honkavaara, Tero Kuosmanen, and Timi Hietanen for the compute infrastructure. Dmitry Korobchenko and Richard Calderwood for efforts related to the CELEBA-HQ dataset. Oskar Elek, Jacob Munkberg, and Jon Hasselgren for useful comments."
                },
                {
                    "heading": "A NETWORK STRUCTURE AND TRAINING CONFIGURATION",
                    "text": "A.1 1024\u00d7 1024 NETWORKS USED FOR CELEBA-HQ\nTable 2 shows network architectures of the full-resolution generator and discriminator that we use with the CELEBA-HQ dataset. Both networks consist mainly of replicated 3-layer blocks that we introduce one by one during the course of the training. The last Conv 1 \u00d7 1 layer of the generator corresponds to the toRGB block in Figure 2, and the first Conv 1 \u00d7 1 layer of the discriminator similarly corresponds to fromRGB. We start with 4 \u00d7 4 resolution and train the networks until we have shown the discriminator 800k real images in total. We then alternate between two phases: fade in the first 3-layer block during the next 800k images, stabilize the networks for 800k images, fade in the next 3-layer block during 800k images, etc.\nOur latent vectors correspond to random points on a 512-dimensional hypersphere, and we represent training and generated images in [-1,1]. We use leaky ReLU with leakiness 0.2 in all layers of both networks, except for the last layer that uses linear activation. We do not employ batch normalization, layer normalization, or weight normalization in either network, but we perform pixelwise normalization of the feature vectors after each Conv 3\u00d73 layer in the generator as described in Section 4.2. We initialize all bias parameters to zero and all weights according to the normal distribution with unit variance. However, we scale the weights with a layer-specific constant at runtime as described in Section 4.1. We inject the across-minibatch standard deviation as an additional feature map at 4\u00d7 4 resolution toward the end of the discriminator as described in Section 3. The upsampling and downsampling operations in Table 2 correspond to 2 \u00d7 2 element replication and average pooling, respectively.\nWe train the networks using Adam (Kingma & Ba, 2015) with \u03b1 = 0.001, \u03b21 = 0, \u03b22 = 0.99, and = 10\u22128. We do not use any learning rate decay or rampdown, but for visualizing generator output at any given point during the training, we use an exponential running average for the weights of the generator with decay 0.999. We use a minibatch size 16 for resolutions 42\u20131282 and then gradually decrease the size according to 2562 \u2192 14, 5122 \u2192 6, and 10242 \u2192 3 to avoid exceeding the available memory budget. We use the WGAN-GP loss, but unlike Gulrajani et al. (2017), we alternate between optimizing the generator and discriminator on a per-minibatch basis, i.e., we set ncritic = 1. Additionally, we introduce a fourth term into the discriminator loss with an extremely\nsmall weight to keep the discriminator output from drifting too far away from zero. To be precise, we set L\u2032 = L+ driftEx\u2208Pr [D(x)2], where drift = 0.001.\nA.2 OTHER NETWORKS\nWhenever we need to operate on a spatial resolution lower than 1024\u00d7 1024, we do that by leaving out an appropriate number copies of the replicated 3-layer block in both networks.\nFurthermore, Section 6.1 uses a slightly lower-capacity version, where we halve the number of feature maps in Conv 3 \u00d7 3 layers at the 16 \u00d7 16 resolution, and divide by 4 in the subsequent resolutions. This leaves 32 feature maps to the last Conv 3 \u00d7 3 layers. In Table 1 and Figure 4 we train each resolution for a total 600k images instead of 800k, and also fade in new layers for the duration of 600k images.\nFor the \u201cGulrajani et al. (2017)\u201d case in Table 1, we follow their training configuration as closely as possible. In particular, we set \u03b1 = 0.0001, \u03b22 = 0.9, ncritic = 5, drift = 0, and minibatch size 64. We disable progressive resolution, minibatch stddev, as well as weight scaling at runtime, and initialize all weights using He\u2019s initializer (He et al., 2015). Furthermore, we modify the generator by replacing LReLU with ReLU, linear activation with tanh in the last layer, and pixelwise normalization with batch normalization. In the discriminator, we add layer normalization to all Conv 3\u00d7 3 and Conv 4\u00d7 4 layers. For the latent vectors, we use 128 components sampled independently from the normal distribution.\nB LEAST-SQUARES GAN (LSGAN) AT 1024\u00d7 1024\nWe find that LSGAN is generally a less stable loss function than WGAN-GP, and it also has a tendency to lose some of the variation towards the end of long runs. Thus we prefer WGAN-GP, but have also produced high-resolution images by building on top of LSGAN. For example, the 10242 images in Figure 1 are LSGAN-based.\nOn top of the techniques described in Sections 2\u20134, we need one additional hack with LSGAN that prevents the training from spiraling out of control when the dataset is too easy for the discriminator, and the discriminator gradients are at risk of becoming meaningless as a result. We adaptively increase the magnitude of multiplicative Gaussian noise in discriminator as a function of the discriminator\u2019s output. The noise is applied to the input of each Conv 3 \u00d7 3 and Conv 4 \u00d7 4 layer. There is a long history of adding noise to the discriminator, and it is generally detrimental for the image quality (Arjovsky et al., 2017) and ideally one would never have to do that, which according to our tests is the case for WGAN-GP (Gulrajani et al., 2017). The magnitude of noise is determined as 0.2 \u00b7 max(0, d\u0302t \u2212 0.5)2, where d\u0302t = 0.1d + 0.9d\u0302t\u22121 is an exponential moving average of the discriminator output d. The motivation behind this hack is that LSGAN is seriously unstable when d approaches (or exceeds) 1.0."
                },
                {
                    "heading": "C CELEBA-HQ DATASET",
                    "text": "In this section we describe the process we used to create the high-quality version of the CELEBA dataset, consisting of 30000 images in 1024 \u00d7 1024 resolution. As a starting point, we took the collection of in-the-wild images included as a part of the original CELEBA dataset. These images are extremely varied in terms of resolution and visual quality, ranging all the way from 43 \u00d7 55 to 6732 \u00d7 8984. Some of them show crowds of several people whereas others focus on the face of a single person \u2013 often only a part of the face. Thus, we found it necessary to apply several image processing steps to ensure consistent quality and to center the images on the facial region.\nOur processing pipeline is illustrated in Figure 8. To improve the overall image quality, we preprocess each JPEG image using two pre-trained neural networks: a convolutional autoencoder trained to remove JPEG artifacts in natural images, similar in structure to the proposed by Mao et al. (2016a), and an adversarially-trained 4x super-resolution network (Korobchenko & Foco, 2017) similar to Ledig et al. (2016). To handle cases where the facial region extends outside the image, we employ padding and filtering to extend the dimensions of the image as illustrated in Fig.8(c\u2013d). We then select an oriented crop rectangle based on the facial landmark annotations included in the\noriginal CELEBA dataset as follows:\nx\u2032 = e1 \u2212 e0 y\u2032 = 1\n2 (e0 + e1)\u2212\n1 2 (m0 +m1)\nc = 1\n2 (e0 + e1)\u2212 0.1 \u00b7 y\u2032\ns = max (4.0 \u00b7 |x\u2032|, 3.6 \u00b7 |y\u2032|) x = Normalize (x\u2032 \u2212 Rotate90(y\u2032)) y = Rotate90(x)\ne0, e1, m0, and m1 represent the 2D pixel locations of the two eye landmarks and two mouth landmarks, respectively, c and s indicate the center and size of the desired crop rectangle, and x and y indicate its orientation. We constructed the above formulas empirically to ensure that the crop rectangle stays consistent in cases where the face is viewed from different angles. Once we have calculated the crop rectangle, we transform the rectangle to 4096\u00d7 4096 pixels using bilinear filtering, and then scale it to 1024\u00d7 1024 resolution using a box filter. We perform the above processing for all 202599 images in the dataset, analyze the resulting 1024\u00d7 1024 images further to estimate the final image quality, sort the images accordingly, and discard all but the best 30000 images. We use a frequency-based quality metric that favors images whose power spectrum contains a broad range of frequencies and is approximately radially symmetric. This penalizes blurry images as well as images that have conspicuous directional features due to, e.g., visible halftoning patterns. We selected the cutoff point of 30000 images as a practical sweet spot between variation and image quality, because it appeared to yield the best results."
                },
                {
                    "heading": "D CIFAR10 RESULTS",
                    "text": "Figure 9 shows non-curated images generated in the unsupervised setting, and Table 3 compares against prior art in terms of inception scores. We report our scores in two different ways: 1) the highest score observed during training runs (here \u00b1 refers to the standard deviation returned by the inception score calculator) and 2) the mean and standard deviation computed from the highest scores seen during training, starting from ten random initializations. Arguably the latter methodology is much more meaningful as one can be lucky with individual runs (as we were). We did not use any kind of augmentation with this dataset."
                },
                {
                    "heading": "E MNIST-1K DISCRETE MODE TEST WITH CRIPPLED DISCRIMINATOR",
                    "text": "Metz et al. (2016) describe a setup where a generator synthesizes MNIST digits simultaneously to 3 color channels, the digits are classified using a pre-trained classifier (0.4% error rate in our case), and concatenated to form a number in [0, 999]. They generate a total of 25,600 images and count\nhow many of the discrete modes are covered. They also compute KL divergence as KL(histogram || uniform). Modern GAN implementations can trivially cover all modes at very low divergence (0.05 in our case), and thus Metz et al. specify a fairly low-capacity generator and two severely crippled discriminators (\u201cK/2\u201d has \u223c 2000 params and \u201cK/4\u201d only about \u223c 500) to tease out differences between training methodologies. Both of these networks use batch normalization.\nAs shown in Table 4, using WGAN-GP loss with the networks specified by Metz et al. covers much more modes than the original GAN loss, and even more than the unrolled original GAN with the smaller (K/4) discriminator. The KL divergence, which is arguably a more accurate metric than the raw count, acts even more favorably.\nReplacing batch normalization with our normalization (equalized learning rate, pixelwise normalization) improves the result considerably, while also removing a few trainable parameters from the discriminators. The addition of a minibatch stddev layer further improves the scores, while restoring the discriminator capacity to within 0.5% of the original. Progression does not help much with these tiny images, but it does not hurt either."
                },
                {
                    "heading": "F ADDITIONAL CELEBA-HQ RESULTS",
                    "text": "Figure 10 shows the nearest neighbors found for our generated images. Figure 11 gives additional generated examples from CELEBA-HQ. We enabled mirror augmentation for all tests using CELEBA and CELEBA-HQ. In addition to the sliced Wasserstein distance (SWD), we also quote the recently introduced Fre\u0301chet Inception Distance (FID) (Heusel et al., 2017) computed from 50K images."
                },
                {
                    "heading": "G LSUN RESULTS",
                    "text": "Figures 12\u201317 show representative images generated for all 30 LSUN categories. A separate network was trained for each category using identical parameters. All categories were trained using 100k images, except for BEDROOM and DOG that used all the available data. Since 100k images is a very limited amount of training data for most categories, we enabled mirror augmentation in these tests (but not for BEDROOM or DOG)."
                },
                {
                    "heading": "H ADDITIONAL IMAGES FOR TABLE 1",
                    "text": "Figure 18 shows larger collections of images corresponding to the non-converged setups in Table 1. The training time was intentionally limited to make the differences between various methods more visible."
                }
            ],
            "year": 2018,
            "references": [
                {
                    "title": "Towards principled methods for training generative adversarial networks",
                    "authors": [
                        "Martin Arjovsky",
                        "L\u00e9on Bottou"
                    ],
                    "venue": "In ICLR,",
                    "year": 2017
                },
                {
                    "title": "Do GANs actually learn the distribution? an empirical study",
                    "authors": [
                        "Sanjeev Arora",
                        "Yi Zhang"
                    ],
                    "year": 2017
                },
                {
                    "title": "Greedy layer-wise training of deep networks",
                    "authors": [
                        "Yoshua Bengio",
                        "Pascal Lamblin",
                        "Dan Popovici",
                        "Hugo Larochelle"
                    ],
                    "year": 2007
                },
                {
                    "title": "BEGAN: Boundary equilibrium generative adversarial networks",
                    "authors": [
                        "David Berthelot",
                        "Tom Schumm",
                        "Luke Metz"
                    ],
                    "year": 2017
                },
                {
                    "title": "Readings in computer vision: Issues, problems, principles, and paradigms. chapter The Laplacian Pyramid As a Compact Image Code",
                    "authors": [
                        "Peter J. Burt",
                        "Edward H. Adelson"
                    ],
                    "year": 1987
                },
                {
                    "title": "Photographic image synthesis with cascaded refinement",
                    "authors": [
                        "Qifeng Chen",
                        "Vladlen Koltun"
                    ],
                    "venue": "networks. CoRR,",
                    "year": 2017
                },
                {
                    "title": "Calibrating energy-based generative adversarial networks",
                    "authors": [
                        "Zihang Dai",
                        "Amjad Almahairi",
                        "Philip Bachman",
                        "Eduard H. Hovy",
                        "Aaron C. Courville"
                    ],
                    "venue": "In ICLR,",
                    "year": 2017
                },
                {
                    "title": "Deep generative image models using a Laplacian pyramid of adversarial networks",
                    "authors": [
                        "Emily L. Denton",
                        "Soumith Chintala",
                        "Arthur Szlam",
                        "Robert Fergus"
                    ],
                    "venue": "CoRR, abs/1506.05751,",
                    "year": 2015
                },
                {
                    "title": "A growing neural gas network learns topologies",
                    "authors": [
                        "Bernd Fritzke"
                    ],
                    "venue": "Advances in Neural Information Processing Systems",
                    "year": 1995
                },
                {
                    "title": "Multi-agent diverse generative adversarial networks",
                    "authors": [
                        "Arnab Ghosh",
                        "Viveka Kulharia",
                        "Vinay P. Namboodiri",
                        "Philip H.S. Torr",
                        "Puneet Kumar Dokania"
                    ],
                    "year": 2017
                },
                {
                    "title": "Generative Adversarial Networks",
                    "authors": [
                        "Ian Goodfellow",
                        "Jean Pouget-Abadie",
                        "Mehdi Mirza",
                        "Bing Xu",
                        "David Warde-Farley",
                        "Sherjil Ozair",
                        "Aaron Courville",
                        "Yoshua Bengio"
                    ],
                    "venue": "In NIPS,",
                    "year": 2014
                },
                {
                    "title": "Improved training of Wasserstein GANs",
                    "authors": [
                        "Ishaan Gulrajani",
                        "Faruk Ahmed",
                        "Mart\u0131\u0301n Arjovsky",
                        "Vincent Dumoulin",
                        "Aaron C. Courville"
                    ],
                    "year": 2017
                },
                {
                    "title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
                    "authors": [
                        "Kaiming He",
                        "Xiangyu Zhang",
                        "Shaoqing Ren",
                        "Jian Sun"
                    ],
                    "venue": "CoRR, abs/1502.01852,",
                    "year": 2015
                },
                {
                    "title": "GANs trained by a two time-scale update rule converge to a local Nash equilibrium",
                    "authors": [
                        "Martin Heusel",
                        "Hubert Ramsauer",
                        "Thomas Unterthiner",
                        "Bernhard Nessler",
                        "Sepp Hochreiter"
                    ],
                    "venue": "In NIPS,",
                    "year": 2017
                },
                {
                    "title": "Globally and locally consistent image completion",
                    "authors": [
                        "Satoshi Iizuka",
                        "Edgar Simo-Serra",
                        "Hiroshi Ishikawa"
                    ],
                    "venue": "ACM Trans. Graph.,",
                    "year": 2017
                },
                {
                    "title": "Batch normalization: Accelerating deep network training by reducing internal covariate",
                    "authors": [
                        "Sergey Ioffe",
                        "Christian Szegedy"
                    ],
                    "venue": "shift. CoRR,",
                    "year": 2015
                },
                {
                    "title": "Adam: A method for stochastic optimization",
                    "authors": [
                        "Diederik P. Kingma",
                        "Jimmy Ba"
                    ],
                    "venue": "In ICLR,",
                    "year": 2015
                },
                {
                    "title": "Auto-encoding variational bayes",
                    "authors": [
                        "Diederik P. Kingma",
                        "Max Welling"
                    ],
                    "venue": "In ICLR,",
                    "year": 2014
                },
                {
                    "title": "Improved variational inference with inverse autoregressive flow",
                    "authors": [
                        "Diederik P Kingma",
                        "Tim Salimans",
                        "Rafal Jozefowicz",
                        "Xi Chen",
                        "Ilya Sutskever",
                        "Max Welling"
                    ],
                    "venue": "In NIPS,",
                    "year": 2016
                },
                {
                    "title": "Single image super-resolution using deep learning, 2017",
                    "authors": [
                        "Dmitry Korobchenko",
                        "Marco Foco"
                    ],
                    "venue": "URL https://gwmt.nvidia.com/super-res/about. Machines Can See summit",
                    "year": 2017
                },
                {
                    "title": "Imagenet classification with deep convolutional neural networks",
                    "authors": [
                        "Alex Krizhevsky",
                        "Ilya Sutskever",
                        "Geoffrey E. Hinton"
                    ],
                    "venue": "In NIPS, pp",
                    "year": 2012
                },
                {
                    "title": "Photo-realistic single image super-resolution using a generative adversarial network",
                    "authors": [
                        "Christian Ledig",
                        "Lucas Theis",
                        "Ferenc Huszar",
                        "Jose Caballero",
                        "Andrew P. Aitken",
                        "Alykhan Tejani",
                        "Johannes Totz",
                        "Zehan Wang",
                        "Wenzhe Shi"
                    ],
                    "year": 2016
                },
                {
                    "title": "PacGAN: The power of two samples in generative adversarial networks",
                    "authors": [
                        "Zinan Lin",
                        "Ashish Khetan",
                        "Giulia Fanti",
                        "Sewoong Oh"
                    ],
                    "year": 2017
                },
                {
                    "title": "Unsupervised image-to-image translation",
                    "authors": [
                        "Ming-Yu Liu",
                        "Thomas Breuel",
                        "Jan Kautz"
                    ],
                    "venue": "networks. CoRR,",
                    "year": 2017
                },
                {
                    "title": "Deep learning face attributes in the wild",
                    "authors": [
                        "Ziwei Liu",
                        "Ping Luo",
                        "Xiaogang Wang",
                        "Xiaoou Tang"
                    ],
                    "venue": "In ICCV,",
                    "year": 2015
                },
                {
                    "title": "Image restoration using convolutional autoencoders with symmetric skip connections",
                    "authors": [
                        "Xiao-Jiao Mao",
                        "Chunhua Shen",
                        "Yu-Bin Yang"
                    ],
                    "venue": "CoRR, abs/1606.08921,",
                    "year": 2016
                },
                {
                    "title": "Least squares generative adversarial networks",
                    "authors": [
                        "Xudong Mao",
                        "Qing Li",
                        "Haoran Xie",
                        "Raymond Y.K. Lau",
                        "Zhen Wang"
                    ],
                    "venue": "CoRR, abs/1611.04076,",
                    "year": 2016
                },
                {
                    "title": "Megapixel size image creation using generative adversarial",
                    "authors": [
                        "Marco Marchesi"
                    ],
                    "venue": "networks. CoRR,",
                    "year": 2017
                },
                {
                    "title": "Conditional image synthesis with auxiliary classifier GANs",
                    "authors": [
                        "Augustus Odena",
                        "Christopher Olah",
                        "Jonathon Shlens"
                    ],
                    "venue": "In ICML,",
                    "year": 2017
                },
                {
                    "title": "Wasserstein barycenter and its application to texture mixing",
                    "authors": [
                        "Julien Rabin",
                        "Gabriel Peyr",
                        "Julie Delon",
                        "Marc Bernot"
                    ],
                    "venue": "In Scale Space and Variational Methods in Computer Vision (SSVM),",
                    "year": 2011
                },
                {
                    "title": "Unsupervised representation learning with deep convolutional generative adversarial networks",
                    "authors": [
                        "Alec Radford",
                        "Luke Metz",
                        "Soumith Chintala"
                    ],
                    "venue": "CoRR, abs/1511.06434,",
                    "year": 2015
                },
                {
                    "title": "Weight normalization: A simple reparameterization to accelerate training of deep neural networks",
                    "authors": [
                        "Tim Salimans",
                        "Diederik P. Kingma"
                    ],
                    "year": 2016
                },
                {
                    "title": "Improved techniques for training GANs",
                    "authors": [
                        "Tim Salimans",
                        "Ian J. Goodfellow",
                        "Wojciech Zaremba",
                        "Vicki Cheung",
                        "Alec Radford",
                        "Xi Chen"
                    ],
                    "year": 2016
                },
                {
                    "title": "Evolving neural networks through augmenting topologies",
                    "authors": [
                        "Kenneth O. Stanley",
                        "Risto Miikkulainen"
                    ],
                    "venue": "Evolutionary Computation,",
                    "year": 2002
                },
                {
                    "title": "Lecture 6.5 - RMSProp",
                    "authors": [
                        "Tijmen Tieleman",
                        "Geoffrey E. Hinton"
                    ],
                    "venue": "Technical report, COURSERA: Neural Networks for Machine Learning,",
                    "year": 2012
                },
                {
                    "title": "WaveNet: A generative model for raw audio",
                    "authors": [
                        "A\u00e4ron van den Oord",
                        "Sander Dieleman",
                        "Heiga Zen",
                        "Karen Simonyan",
                        "Oriol Vinyals",
                        "Alex Graves",
                        "Nal Kalchbrenner",
                        "Andrew W. Senior",
                        "Koray Kavukcuoglu"
                    ],
                    "venue": "CoRR, abs/1609.03499,",
                    "year": 2016
                },
                {
                    "title": "Pixel recurrent neural networks",
                    "authors": [
                        "A\u00e4ron van den Oord",
                        "Nal Kalchbrenner",
                        "Koray Kavukcuoglu"
                    ],
                    "venue": "In ICML, pp",
                    "year": 2016
                },
                {
                    "title": "Conditional image generation with PixelCNN",
                    "authors": [
                        "A\u00e4ron van den Oord",
                        "Nal Kalchbrenner",
                        "Oriol Vinyals",
                        "Lasse Espeholt",
                        "Alex Graves",
                        "Koray Kavukcuoglu"
                    ],
                    "venue": "decoders. CoRR,",
                    "year": 2016
                },
                {
                    "title": "L2 regularization versus batch and weight",
                    "authors": [
                        "Twan van Laarhoven"
                    ],
                    "venue": "normalization. CoRR,",
                    "year": 2017
                },
                {
                    "title": "High-resolution image synthesis and semantic manipulation with conditional GANs",
                    "authors": [
                        "Ting-Chun Wang",
                        "Ming-Yu Liu",
                        "Jun-Yan Zhu",
                        "Andrew Tao",
                        "Jan Kautz",
                        "Bryan Catanzaro"
                    ],
                    "year": 2017
                },
                {
                    "title": "Multi-scale structural similarity for image quality assessment",
                    "authors": [
                        "Zhou Wang",
                        "Eero P. Simoncelli",
                        "Alan C. Bovik"
                    ],
                    "venue": "In Proc. IEEE Asilomar Conf. on Signals, Systems, and Computers,",
                    "year": 2003
                },
                {
                    "title": "Improving generative adversarial networks with denoising feature matching",
                    "authors": [
                        "David Warde-Farley",
                        "Yoshua Bengio"
                    ],
                    "venue": "In ICLR,",
                    "year": 2017
                },
                {
                    "title": "LR-GAN: layered recursive generative adversarial networks for image generation",
                    "authors": [
                        "Jianwei Yang",
                        "Anitha Kannan",
                        "Dhruv Batra",
                        "Devi Parikh"
                    ],
                    "venue": "In ICLR,",
                    "year": 2017
                },
                {
                    "title": "LSUN: Construction of a large-scale image dataset using deep learning with humans",
                    "authors": [
                        "Fisher Yu",
                        "Yinda Zhang",
                        "Shuran Song",
                        "Ari Seff",
                        "Jianxiong Xiao"
                    ],
                    "venue": "in the loop. CoRR,",
                    "year": 2015
                },
                {
                    "title": "StackGAN: text to photo-realistic image synthesis with stacked generative adversarial networks",
                    "authors": [
                        "Han Zhang",
                        "Tao Xu",
                        "Hongsheng Li",
                        "Shaoting Zhang",
                        "Xiaolei Huang",
                        "Xiaogang Wang",
                        "Dimitris N. Metaxas"
                    ],
                    "year": 2017
                },
                {
                    "title": "Energy-based generative adversarial network",
                    "authors": [
                        "Junbo Jake Zhao",
                        "Micha\u00ebl Mathieu",
                        "Yann LeCun"
                    ],
                    "venue": "In ICLR,",
                    "year": 2017
                },
                {
                    "title": "Unpaired image-to-image translation using cycle-consistent adversarial networks",
                    "authors": [
                        "Jun-Yan Zhu",
                        "Taesung Park",
                        "Phillip Isola",
                        "Alexei A. Efros"
                    ],
                    "year": 2017
                },
                {
                    "title": "Published as a conference paper at ICLR 2018 small weight to keep the discriminator output from drifting too far away",
                    "authors": [
                        "L L"
                    ],
                    "year": 2018
                },
                {
                    "title": "To handle cases where the facial region extends outside the image",
                    "authors": [
                        "Ledig"
                    ],
                    "year": 2016
                },
                {
                    "title": "Improved GAN (Salimans et al., 2016",
                    "authors": [
                        "Durugkar"
                    ],
                    "venue": "CEGAN-Ent-VI (Dai et al.,",
                    "year": 2017
                },
                {
                    "title": "specify a fairly low-capacity generator and two severely crippled discriminators (\u201cK/2",
                    "authors": [
                        "Metz"
                    ],
                    "year": 2000
                },
                {
                    "title": "Results for MNIST discrete mode test using two tiny discriminators (K/4",
                    "authors": [
                        "Metz"
                    ],
                    "venue": "KL 2.58\u00b1",
                    "year": 2016
                }
            ],
            "id": "SP:744fe47157477235032f7bb3777800f9f2f45e52",
            "authors": [
                {
                    "name": "Tero Karras",
                    "affiliations": []
                },
                {
                    "name": "Timo Aila",
                    "affiliations": []
                },
                {
                    "name": "Samuli Laine",
                    "affiliations": []
                }
            ],
            "abstractText": "We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CELEBA images at 1024. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CELEBA dataset."
        },
        "Y": {
            "blog_id": "progressive_growing_of_gans",
            "summary": [
                "They suggest a new method to train GANs.",
                "They start training them at low resolution (4x4), wait until \"convergence\", then add more convolutions to the existing model to generate and discriminate higher resolutions.",
                "Each new block of convolutions is slowly blended in, instead of being added from one batch to the next.",
                "Combined with two new normalization techniques, they get good-looking images at up to 1024x1024 on their new CelebA-HQ dataset (CelebA in high resolution).",
                "They also suggest a new scoring method based on the approximated Wasserstein distance between real and generated image patches.",
                "According to that score, their progressive training method improves results significantly.",
                "What  They suggest a new, progressive training method for GANs.",
                "The method enables the training of high resolution GANs (1024x1024) that still produce good-looking, diverse images.",
                "They also introduce two new normalization techniques.",
                "They also suggest a new method to estimate/score the quality of the generated images.",
                "They introduce CelebA-HQ, a variation of CelebA containing high resolution images.",
                "How  Progressive growing/training  They train their GANs resolution by resolution, starting with 4x4 and going up to 1024x1024 (a bit similar to LAPGAN).",
                "Visualization:  Initially, their generator produces 4x4 images and the discriminator receives 4x4 images.",
                "Once training at 4x4 does not improve any more (measured by their new score, see below), they add an upscaling module (to 8x8) to the generator and add a downscaling one to the discriminator.",
                "They don't switch to the added convolutions instantly/suddenly, but give the model a grace period during which the upscaled features are computed from (1-alpha)*A + alpha*B, where A are the features after just upscaling, B are the features after upscaling AND the convolutions and alpha is the overlay factor, which is gradually increased over time.",
                "This is done for both the generator and the discriminator and at all resolutions.",
                "Visualization:  Note that all layers are always trained (after they were added to the models).",
                "Training for the earlier layers does not stop.",
                "Training in this way focuses most of the computation on the earlier resolutions.",
                "It also seems to increase stability, as the model does not have to learn all features of all resolutions at the same time.",
                "Minibatch Standard Deviation  They try to improve diversity by adding a method very similar to minibatch discrimination.",
                "They compute the standard deviation of each feature per spatial location (for one of the disciminator's last layers).",
                "They do this per example in each minibatch, resulting in B*H*W*C standard deviations.",
                "(B = batch size, H = height, W = width, C = channels/filters)  They average these values to one value, then replicate them to size H*W and concatenate that to the layer's output.",
                "This adds a channel with one constant value to each example in the minibatch.",
                "The value is the same for all examples.",
                "Equalized Learning Rate  They use Adam for their training.",
                "Adam updates weights roughly based on mean(gradient)/variance(gradient) (per weight).",
                "They argue that this has the downside of equalizing all weight's stepsizes.",
                "But some weights might require larger stepsizes and other smaller ones (large/small \"dynamic range\").",
                "As a result, the learning rate will be too small for some weights and too large for others.",
                "To evade this problem, they first stop using modern weight initialization techniques and instead simply sample weights from the standard normal distribution N(0,1).",
                "Then, they rescale each weight w_i continuously during runtime to w_i/c, where c is the per-layer normalization from He's initializer.",
                "(TODO exact formula for c?)",
                "(This looks an aweful lot like weight normalization .)",
                "Using simpler weight initialization equalizes the dynamic range of parameters.",
                "Doing the normalization then fixes problems related to the simpler weight initialization.",
                "Pixelwise Feature Vector Normalization in the Generator  They argue that collapses in GANs come from the discriminator making some temporary error, leading to high gradients, leading to bad outputs of the generator, leading to more problems in the discriminator and ultimately making both spiral out of control.",
                "They fix this by normalizing feature vectors in the generator, similar to local response normalization.",
                "They apply the following equation in the generator (per spatial location (x, y) with N = number of filters):  Scoring Images  They suggest a new method to score images generated by the generator.",
                "They perform the following steps:  Sample 16384 images from the generator and the dataset.",
                "Build a Laplacian Pyramid of each image.",
                "It begins at a 16x16 resolution of the image and progressively doubles that until the final image resolution.",
                "Each level of the pyramid only contains the difference between the sum of the previous scales and the final image (i.e. each step is a difference image, containing a frequency band).",
                "Sample per image 128 7x7 neighbourhoods/patches (randomly?)",
                "from each pyramid level.",
                "Per image set (generator/real) and pyramid level, compute the mean and standard deviations of each color channels of the sampled patches.",
                "Normalize each patch with respect to the computed means and standard deviations.",
                "Use Sliced Wasserstein Distance (SWD) to compute the similarity between the image sets (generator/real).",
                "The result is one value.",
                "Lower values are better.",
                "CelebA-HQ  They derive from CelebA images a new dataset containing 30k 1024x1024 images of celebrity faces.",
                "They use a convolutional autoencoder to remove JPEG artifacts from the CelebA images.",
                "They use an adversarially-trained superresolution model to upscale the images.",
                "They crop faces from the dataset based on their facial landmarks, so that each final face has a normalized position and rotation.",
                "They rescale the images to 1024x1024 using bilinear sampling and box filters.",
                "They manually select the 30k best looking images.",
                "Other stuff  They use Adam for training (alpha=0.001, beta1=0, beta2=0.99).",
                "They use the WGAN-WP method for training, but LSGAN also works.",
                "They set gamma to 750 (from 1) for CIFAR-10, incentivizing fast transitions.",
                "They also add regularization loss on the discriminator, punishing outputs that are very far away from 0.",
                "Their model for CelebA-HQ training is similar to a standard DCGAN model.",
                "The generator uses two convolutions after each upscaling, the discriminator analogously two convolutions after each downscaling.",
                "They start with 512 filters in the generator and end in 16 (before the output) - same for the discriminator.",
                "They use leaky ReLUs in the generator and discriminator.",
                "They remove batch normalization everywhere.",
                "Results  Scores  Results, according to their new scoring measure (Sliced Wasserstein Distance) and MS-SSIM measure:  So progressive growing (b) significantly improves results.",
                "Same -- to a smaller degree -- for minibatch standard deviation (e), equalized learning rate (f) and pixelwise normalization (g).",
                "Minibatch discrimination worsened the results.",
                "Using small batch sizes also worsened the results.",
                "In (d) they \"adjusted the hyperparameters\" (??)",
                "and removed batch normalization.",
                "They generate 1024x1024 CelebA images, while maintaining pixelwise quality compared to previous models.",
                "They achieve an Inception Score of 8.80 on CIFAR-10.",
                "Images look improved.",
                "CelebA-HQ example results:  LSUN dining room, horse, kitchen, churches:"
            ],
            "author_id": "ALEJU",
            "pdf_url": "https://arxiv.org/pdf/1710.10196.pdf",
            "author_full_name": "Alexander Jung",
            "source_website": "https://github.com/aleju/papers",
            "id": 25710556
        }
    },
    "20550713": {
        "X": {
            "sections": [
                {
                    "heading": "1 Introduction",
                    "text": "Convolutional neural networks (CNNs) [28] have proven successful across many tasks in machine learning and artificial intelligence, including image classification [28, 25], face recognition [26], speech recognition [21], text classification [45], and game playing [32, 37]. There are two principal advantages of a CNN over a fully-connected neural network: (i) sparsity\u2014that each nonlinear convolutional filter acts only on a local patch of the input, and (ii) parameter sharing\u2014that the same filter is applied to each patch.\nHowever, as with most neural networks, the standard approach to training CNNs is based on solving a nonconvex optimization problem that is known to be NP-hard [6]. In practice, researchers use some flavor of stochastic gradient method, in which gradients are computed via backpropagation [7]. This approach has two drawbacks: (i) the rate of convergence, which is at best only to a local optimum, can be slow due to nonconvexity (for instance, see the paper [19]), and (ii) its statistical properties are very difficult to understand, as the actual performance is determined by some combination of the CNN architecture along with the optimization algorithm.\nIn this paper, with the goal of addressing these two challenges, we propose a new model class known as convexified convolutional neural networks (CCNNs). These models have two desirable features. First, training a CCNN corresponds to a convex optimization problem, which can be solved efficiently and optimally via a projected gradient algorithm. Second, the statistical properties of CCNN models can be studied in a precise and rigorous manner. We obtain CCNNs by convexifying\n\u2217Computer Science Department, Stanford University, Stanford, CA 94305. Email: zhangyuc@cs.stanford.edu. \u2020Computer Science Department, Stanford University, Stanford, CA 94305. Email: pliang@cs.stanford.edu. \u2021Department of Electrical Engineering and Computer Science and Department of Statistics, University of California\nBerkeley, Berkeley, CA 94720. Email: wainwrig@eecs.berkeley.edu.\nar X\niv :1\n60 9.\n01 00\n0v 1\n[ cs\n.L G\n] 4\nS ep\n2 01\ntwo-layer CNNs; doing so requires overcoming two challenges. First, the activation function of a CNN is nonlinear. In order to address this issue, we relax the class of CNN filters to a reproducing kernel Hilbert space (RKHS). This approach is inspired by our earlier work [48], involving a subset of the current authors, in which we developed this relaxation step for fully-connected neural networks. Second, the parameter sharing induced by CNNs is crucial to its effectiveness and must be preserved. We show that CNNs with RKHS filters can be parametrized by a low-rank matrix. Further relaxing the low-rank constraint to a nuclear norm constraint leads to our final formulation of CCNNs.\nOn the theoretical front, we prove an oracle inequality on generalization error achieved by our class of CCNNs, showing that it is upper bounded by the best possible performance achievable by a two-layer CNN given infinite data\u2014a quantity to which we refer as the oracle risk\u2014plus a model complexity term that decays to zero polynomially in the sample size. Our results show that the sample complexity for CCNNs is significantly lower than that of the convexified fully-connected neural network [48], highlighting the importance of parameter sharing. For models with more than one hidden layer, our theory does not apply, but we provide encouraging empirical results using a greedy layer-wise training heuristic. We then apply CCNNs to the MNIST handwritten digit dataset as well as four variation datasets [43], and find that it achieves the state-of-the-art performance. On the CIFAR-10 dataset, CCNNs outperform CNNs of the same depths, as well as other baseline methods that do not involve nonconvex optimization. We also demonstrate that building CCNNs on top of existing CNN filters improves the performance of CNNs.\nThe remainder of this paper is organized as follows. We begin in Section 2 by introducing convolutional neural networks, and setting up the empirical risk minimization problem studied in this paper. In Section 3, we describe the algorithm for learning two-layer CCNNs, beginning with the simple case of convexifying CNNs with a linear activation function, then proceeding to convexify CNNs with a nonlinear activation. We show that the generalization error of a CCNN converges to that of the best possible CNN. In Section 4, we describe several extensions to the basic CCNN algorithm, including averaging pooling, multi-channel input processing, and the layer-wise learning of multi-layer CNNs. In Section 5, we report the empirical evaluations of CCNNs. We survey related work in Section 6 and conclude the paper in Section 7.\nNotation. For any positive integer n, we use [n] as a shorthand for the discrete set {1, 2, . . . , n}. For a rectangular matrix A, let \u2016A\u2016\u2217 be its nuclear norm, \u2016A\u20162 be its spectral norm (i.e., maximal singular value), and \u2016A\u2016F be its Frobenius norm. We use `2(N) to denote the set of countable dimensional vectors v = (v1, v2, . . . ) such that \u2211\u221e `=1 v 2 ` < \u221e. For any vectors u, v \u2208 `2(N), the\ninner product \u3008u, v\u3009 := \u2211\u221e `=1 uivi and the `2-norm \u2016u\u20162 := \u221a \u3008u, u\u3009 are well defined."
                },
                {
                    "heading": "2 Background and problem set-up",
                    "text": "In this section, we formalize the class of convolutional neural networks to be learned and describe the associated nonconvex optimization problem."
                },
                {
                    "heading": "2.1 Convolutional neural networks.",
                    "text": "At a high level, a two-layer CNN1 is a particular type of function that maps an input vector x \u2208 Rd0 (e.g., an image) to an output vector in y \u2208 Rd2 (e.g., classification scores for the d2 classes). This mapping is formed in the following manner:\n\u2022 First, we extract a collection of P vectors {zp(x)}Pj=1 of the full input vector x. Each vector zp(x) \u2208 Rd1 is referred to as a patch, and these patches may depend on overlapping components of x.\n\u2022 Second, given some choice of activation function \u03c3 : R \u2192 R and a collection of weight vectors {wj}rj=1 in Rd1 , we compute the functions\nhj(z) := \u03c3(w > j z) for each patch z \u2208 Rd1 . (1)\nEach function hj (for j \u2208 [r]) is known as a filter, and note that the same filters are applied to each patch\u2014this corresponds to the parameter sharing of a CNN.\n\u2022 Third, for each patch index p \u2208 [P ], filter index j \u2208 [r], and output coordinate k \u2208 [d2], we introduce a coefficient \u03b1k,j,p \u2208 R that governs the contribution of the filter hj on patch zp(x) to output fk(x). The final form of the CNN is given by f(x) : = (f1(x), . . . , fd2(x)), where the k th\ncomponent is given by\nfk(x) := r\u2211 j=1 P\u2211 p=1 \u03b1k,j,phj(zp(x)). (2)\nTaking the patch functions {zp}Pp=1 and activation function \u03c3 as fixed, the parameters of the CNN are the filter vectors w := {wj \u2208 Rd1 : j \u2208 [r]} along with the collection of coefficient vectors \u03b1 := {\u03b1k,j \u2208 RP : k \u2208 [d2], j \u2208 [r]}. We assume that all patch vectors zp(x) \u2208 Rd1 are contained in the unit `2-ball. This assumption can be satisfied without loss of generality by normalization: By multiplying a constant \u03b3 > 0 to every patch zp(x) and multiplying 1/\u03b3 to the filter vectors w, the assumption will be satisfied without changing the the output of the network.\nGiven some positive radii B1 and B2, we consider the model class Fcnn(B1, B2) := { f of the form (2) : max\nj\u2208[r] \u2016wj\u20162 \u2264 B1 and max k\u2208[d2],j\u2208[r] \u2016\u03b1k,j\u20162 \u2264 B2\n} . (3)\nWhen the radii (B1, B2) are clear from context, we adopt Fcnn as a convenient shorthand."
                },
                {
                    "heading": "2.2 Empirical risk minimization.",
                    "text": "Given an input-output pair (x, y) and a CNN f , we let L(f(x); y) denote the loss incurred when the output y is predicted via f(x). We assume that the loss function L is convex and L-Lipschitz in its first argument given any value of its second argument. As a concrete example, for multiclass classification with d2 classes, the output vector y takes values in the discrete set [d2] = {1, 2, . . . , d2}.\n1Average pooling and multiple channels are also an integral part of CNNs, but these do not present any new technical challenges, so that we defer these extensions to Section 4.\nFor example, given a vector f(x) = (f1(x), . . . , fd2(y)) \u2208 Rd2 of classification scores, the associated multiclass logistic loss for a pair (x, y) is given by L(f(x); y) := \u2212fy(x) + log (\u2211d2 y\u2032=1 exp(fy\u2032(x)) ) .\nGiven n training examples {(xi, yi)}ni=1, we would like to compute an empirical risk minimizer\nf\u0302cnn \u2208 arg min f\u2208Fcnn n\u2211 i=1 L(f(xi); yi). (4)\nRecalling that functions f \u2208 Fcnn depend on the parameters w and \u03b1 in a highly nonlinear way (2), this optimization problem is nonconvex. As mentioned earlier, heuristics based on stochastic gradient methods are used in practice, which makes it challenging to gain a theoretical understanding of their behavior. Thus, in the next section, we describe a relaxation of the class Fcnn that allows us to obtain a convex formulation of the associated empirical risk minimization problem."
                },
                {
                    "heading": "3 Convexifying CNNs",
                    "text": "We now turn to the development of the class of convexified CNNs. We begin in Section 3.1 by illustrating the procedure for the special case of the linear activation function. Although the linear case is not of practical interest, it provides intuition for our more general convexification procedure, described in Section 3.2, which applies to nonlinear activation functions. In particular, we show how embedding the nonlinear problem into an appropriately chosen reproducing kernel Hilbert space (RKHS) allows us to again reduce to the linear setting."
                },
                {
                    "heading": "3.1 Linear activation functions: low rank relaxations",
                    "text": "In order to develop intuition for our approach, let us begin by considering the simple case of the linear activation function \u03c3(t) = t. In this case, the filter hj when applied to the patch vector zp(x) outputs a Euclidean inner product of the form hj(zp(x)) = \u3008zp(x), wj\u3009. For each x \u2208 Rd0 , we first define the P \u00d7 d1-dimensional matrix\nZ(x) := z1(x) >\n... zP (x) >  . (5) We also define the P -dimensional vector \u03b1k,j := (\u03b1k,j,1, . . . , \u03b1k,j,P )\n>. With this notation, we can rewrite equation (2) for the kth output as\nfk(x) = r\u2211 j=1 P\u2211 p=1 \u03b1k,j,p\u3008zp(x), wj\u3009 = r\u2211 j=1 \u03b1>k,jZ(x)wj = tr ( Z(x) ( r\u2211 j=1 wj\u03b1 > k,j )) = tr(Z(x)Ak), (6)\nwhere in the final step, we have defined the d1\u00d7P -dimensional matrix Ak := \u2211r j=1wj\u03b1 > k,j . Observe that fk now depends linearly on the matrix parameter Ak. Moreover, the matrix Ak has rank at most r, due to the parameter sharing of CNNs. See Figure 1 for a graphical illustration of this model structure.\nLetting A := (A1, . . . , Ad2) be a concatenation of these matrices across all d2 output coordinates, we can then define a function fA : Rd1 \u2192 Rd2 of the form\nfA(x) := (tr(Z(x)A1), . . . , tr(Z(x)Ad2)). (7)\nNote that these functions have a linear parameterization in terms of the underlying matrix A. Our model class corresponds to a collection of such functions based on imposing certain constraints on the underlying matrix A: in particular, we define\nFcnn(B1, B2) := { fA : max\nj\u2208[r] \u2016wj\u20162 \u2264 B1 and max\nk\u2208[d2] j\u2208[r]\n\u2016\u03b1k,j\u20162 \u2264 B2\n\ufe38 \ufe37\ufe37 \ufe38 Constraint (C1)\nand rank(A) = r\ufe38 \ufe37\ufe37 \ufe38 Constraint (C2)\n} .\nThis is simply an alternative formulation of our original class of CNNs. Notice that if the filter weights wj are not shared across all patches, then the constraint (C1) still holds, but constraint (C2) no longer holds. Thus, the parameter sharing of CNNs is realized by the low-rank constraint (C2). The matrix A of rank r can be decomposed as A = UV >, where both U and V have r columns. The column space of matrix A contains the convolution parameters {wj}, and the row space of A contains to the output parameters {\u03b1k,j}.\nThe rank-r matrices satisfying constraints (C1) and (C2) form a nonconvex set. A standard convex relaxation of a rank constraint is based on the nuclear norm \u2016A\u2016\u2217 corresponding to the sum of the singular values of A. It is straightforward to verify that any matrix A satisfying the constraints (C1) and (C2) must have nuclear norm bounded as \u2016A\u2016\u2217 \u2264 B1B2r \u221a d2. Consequently, if we define the function class\nFccnn := { fA : \u2016A\u2016\u2217 \u2264 B1B2r \u221a d2 } , (8)\nthen we are guaranteed that Fccnn \u2287 Fcnn. Overall, we propose to minimize the empirical risk (4) over Fccnn instead of Fcnn; doing so\ndefines a convex optimization problem over this richer class of functions\nf\u0302ccnn := arg min fA\u2208Fccnn n\u2211 i=1 L(fA(xi); yi). (9)\nIn Section 3.3, we describe iterative algorithms that can be used to solve this form of convex program in the more general setting of nonlinear activation functions."
                },
                {
                    "heading": "3.2 Nonlinear activations: RKHS filters",
                    "text": "For nonlinear activation functions \u03c3, we relax the class of CNN filters to a reproducing kernel Hilbert space (RKHS). As we show, this relaxation allows us to reduce the problem to the linear activation case.\nLet K : Rd1 \u00d7 Rd1 \u2192 R be a positive semidefinite kernel function. For particular choices of kernels (e.g., the Gaussian RBF kernel) and some sufficiently smooth activation function \u03c3, we are able to show that the filter h : z 7\u2192 \u03c3(\u3008w, z\u3009) is contained in the RKHS induced by the kernel function K. See Section 3.4 for the choice of the kernel function and the activation function. Let S := {zp(xi) : p \u2208 [P ], i \u2208 [n]} be the set of patches in the training dataset. The representer theorem then implies that for any patch zp(xi) \u2208 S, the function value can be represented by\nh(zp(xi)) = \u2211\n(i\u2032,p\u2032)\u2208[n]\u00d7[P ]\nci\u2032,p\u2032k(zp(xi), zp\u2032(xi\u2032)) (10)\nfor some coefficients {ci\u2032,p\u2032}(i\u2032,p\u2032)\u2208[n]\u00d7[P ]. Filters taking the form (10) are members of the RKHS, because they are linear combinations of basis functions z 7\u2192 k(z, zp\u2032(xi\u2032)). Such filters are parametrized by a finite set of coefficients, which can be estimated via empirical risk minimization.\nLet K \u2208 RnP\u00d7nP be the symmetric kernel matrix, where with rows and columns indexed by the example-patch index pair (i, p) \u2208 [n]\u00d7 [P ]. The entry at row (i, p) and column (i\u2032, p\u2032) of matrix K is equal to K(zp(xi), zp\u2032(xi\u2032)). So as to avoid re-deriving everything in the kernelized setting, we perform a reduction to the linear setting of Section 3.1. Consider a factorization K = QQ> of the kernel matrix, where Q \u2208 RnP\u00d7m; one example is the Cholesky factorization with m = nP . We can interpret each row Q(i,p) \u2208 Rm as a feature vector in place of the original zp(xi) \u2208 Rd1 , and rewrite equation (10) as\nh(zp(xi)) = \u3008Q(i,p), w\u3009 where w := \u2211 (i\u2032,p\u2032) ci\u2032,p\u2032Q(i\u2032,p\u2032).\nIn order to learn the filter h, it suffices to learn the m-dimensional vector w. To do this, define patch matrices Z(xi) \u2208 RP\u00d7m for each i \u2208 [n] so that its p-th row is Q(i,p). Then we carry out all of Section 3.1; solving the ERM gives us a parameter matrix A \u2208 Rm\u00d7Pd2 . The only difference is that the B1 norm constraint needs to be relaxed as well. See Appendix B for details.\nAt test time, given a new input x \u2208 Rd0 , we can compute a patch matrix Z(x) \u2208 RP\u00d7m as follows:\n\u2022 The p-th row of this matrix is the feature vector for patch p, which is equal to Q\u2020v(zp(x)) \u2208 Rm. Here, for any patch z, the vector v(z) is defined as a nP -dimensional vector whose (i, p)-th coordinate is equal to K(z, zp(xi)). We note that if x is an instance xi in the training set, then the vector Q\u2020v(zp(x)) is exactly equal to Q(i,p). Thus the mapping Z(x) applies to both training and testing.\nAlgorithm 1: Learning two-layer CCNNs\nInput: Data {(xi, yi)}ni=1, kernel function K, regularization parameter R > 0, number of filters r.\n1. Construct a kernel matrix K \u2208 RnP\u00d7nP such that the entry at column (i, p) and row (i\u2032, p\u2032) is equal to K(zp(xi), zp\u2032(xi\u2032)). Compute a factorization K = QQ> or an approximation K \u2248 QQ>, where Q \u2208 RnP\u00d7m.\n2. For each xi, construct patch matrix Z(xi) \u2208 RP\u00d7m whose p-th row is the (i, p)-th row of Q, where Z(\u00b7) is defined in Section 3.2.\n3. Solve the following optimization problem to obtain a matrix A\u0302 = (A\u03021, . . . , A\u0302d2):\nA\u0302 \u2208 argmin \u2016A\u2016\u2217\u2264R L\u0303(A) where L\u0303(A) := n\u2211 i=1 L (( tr(Z(xi)A1), . . . , tr(Z(xi)Ad2) ) ; yi ) . (12)\n4. Compute a rank-r approximation A\u0303 \u2248 U\u0302 V\u0302 > where U\u0302 \u2208 Rm\u00d7r and V\u0302 \u2208 RPd2\u00d7r.\nOutput: Return the predictor f\u0302ccnn(x) := ( tr(Z(x)A\u03021), . . . , tr(Z(x)A\u0302d2) ) and the convolutional layer output H(x) := U\u0302>(Z(x))>.\n\u2022 We can then compute the predictor fk(x) = tr(Z(x)Ak) via equation (6). Note that we do not explicitly need to compute the filter values hj(zp(x)) to compute the output under the CCNN.\nRetrieving filters. However, when we learn multi-layer CCNNs, we need to compute the filters explicitly. Recall from Section 3.1 that the column space of matrix A corresponds to parameters of the convolutional layer, and the row space of A corresponds to parameters of the output layer. Thus, once we obtain the parameter matrix A, we compute a rank-r approximation A \u2248 U\u0302 V\u0302 >. Then set the j-th filter hj to the mapping\nz 7\u2192 \u3008U\u0302j , Q\u2020v(z)\u3009 for any patch z \u2208 Rd1 , (11)\nwhere U\u0302j \u2208 Rm is the j-th column of matrix U\u0302 , and Q\u2020v(z) represents the feature vector for patch z. The matrix V\u0302 > encodes parameters of the output layer, thus doesn\u2019t appear in the filter expression (11). It is important to note that the filter retrieval is not unique, because the rankr approximation of the matrix A is not unique. One feasible way is to form the singular value decomposition A = U\u039bV >, then define U\u0302 to be the first r columns of U , and define V\u0302 > to be the first r rows of \u039bV >.\nWhen we apply all of the r filters to all patches of an input x \u2208 Rd0 , the resulting output is H(x) := U\u0302>(Z(x))> \u2014 this is an r \u00d7 P matrix whose element at row j and column p is equal to hj(zp(x))."
                },
                {
                    "heading": "3.3 Algorithm",
                    "text": "The algorithm for learning a two-layer CCNN is summarized in Algorithm 1; it is a formalization of the steps described in Section 3.2. In order to solve the optimization problem (12), the simplest\napproach is to via projected gradient descent: At iteration t, using a step size \u03b7t > 0, it forms the new matrix At+1 based on the previous iterate At according to:\nAt+1 = \u03a0R ( At \u2212 \u03b7t \u2207AL\u0303(At) ) . (13)\nHere \u2207AL\u0303 denotes the gradient of the objective function defined in (12), and \u03a0R denotes the Euclidean projection onto the nuclear norm ball {A : \u2016A\u2016\u2217 \u2264 R}. This nuclear norm projection can be obtained by first computing the singular value decomposition of A, and then projecting the vector of singular values onto the `1-ball. This latter projection step can be carried out efficiently by the algorithm of Duchi et al. [16]. There are other efficient optimization algorithms for solving the problem (12), such as the proximal adaptive gradient method [17] and the proximal SVRG method [46]. All these algorithms can be executed in a stochastic fashion, so that each gradient step processes a mini-batch of examples.\nThe computational complexity of each iteration depends on the width m of the matrix Q. Setting m = nP allows us to solve the exact kernelized problem, but to improve the computation efficiency, we can use Nystro\u0308m approximation [15] or random feature approximation [33]; both are randomized methods to obtain a tall-and-thin matrix Q \u2208 RnP\u00d7m such that K \u2248 QQ>. Typically, the parameter m is chosen to be much smaller than nP . In order to compute the matrix Q, the Nystro\u0308m approximation method takes O(m2nP ) time. The random feature approximation takes O(mnPd1) time, but can be improved to O(mnP log d1) time using the fast Hadamard transform [27]. The complexity of computing a gradient vector on a batch of b images is O(mPd2b). The complexity of projecting the parameter matrix onto the nuclear norm ball is O(min{m2Pd2,mP 2d22}). Thus, the approximate algorithms provide substantial speed-ups on the projected gradient descent steps."
                },
                {
                    "heading": "3.4 Theoretical results",
                    "text": "In this section, we upper bound the generalization error of Algorithm 1, proving that it converges to the best possible generalization error of CNN. We focus on the binary classification case where the output dimension is d2 = 1. 2\nThe learning of CCNN requires a kernel function K. We consider kernel functions whose associated RKHS is large enough to contain any function taking the following form: z 7\u2192 q(\u3008w, z\u3009), where q is an arbitrary polynomial function and w \u2208 Rd1 is an arbitrary vector. As a concrete example, we consider the inverse polynomial kernel:\nK(z, z\u2032) := 1 2\u2212 \u3008z, z\u2032\u3009 , \u2016z\u20162 \u2264 1, \u2016z\u2032\u20162 \u2264 1. (14)\nThis kernel was studied by Shalev-Shwartz et al. [36] for learning halfspaces, and by Zhang et al. [48] for learning fully-connected neural networks. We also consider the Gaussian RBF kernel:\nK(z, z\u2032) := exp(\u2212\u03b3\u2016z \u2212 z\u2032\u201622), \u2016z\u20162 = \u2016z\u2032\u20162 = 1, \u03b3 > 0. (15)\nAs we show in Appendix A, the inverse polynomial kernel and the Gaussian kernel satisfy the above notion of richness. We focus on these two kernels for the theoretical analysis.\n2We can treat the multiclass case by performing a standard one-versus-all reduction to the binary case.\nLet f\u0302ccnn be the CCNN that minimizes the empirical risk (12) using one of the two kernels above. Our main theoretical result is that for suitably chosen activation functions, the generalization error of f\u0302ccnn is comparable to that of the best CNN model. In particular, the following theorem applies to activation functions \u03c3 of the following types:\n(a) arbitrary polynomial functions (e.g., used by [10, 29]).\n(b) sinusoid activation function \u03c3(t) := sin(t) (e.g., used by [39, 22]). (c) erf function \u03c3erf(t) := 2/ \u221a \u03c0 \u222b t 0 e \u2212z2dz, which represents an approximation to the sigmoid\nfunction (See Figure 2(a)). (d) a smoothed hinge loss \u03c3sh(t) := \u222b t \u2212\u221e 1 2(\u03c3erf(z) + 1)dz, which represents an approximation to\nthe ReLU function (See Figure 2(b)).\nTo understand why these activation functions pair with our choice of kernels, we consider polynomial expansions of the above activation functions: \u03c3(t) = \u2211\u221e j=0 ajt\nj , and note that the smoothness of these functions are characterized by the rate of their coefficients {aj}\u221ej=0 converging to zero. If \u03c3 is a polynomial in category (a), then the richness of the RKHS guarantees that it contains the class of filters activated by function \u03c3. If \u03c3 is a non-polynomial function in categories (b),(c),(d), then as Appendix A shows, the RKHS contains the filter only if the coefficients {aj}\u221ej=0 converge quickly enough to zero (the criterion depends on the choice of the kernel). Concretely, the inverse polynomial kernel is shown to capture all of the four categories of activations: they are referred as valid activation functions for the inverse polynomial kernel. The Gaussian kernel induces a smaller RKHS, and is shown to capture categories (a),(b), so that these functions are referred as valid activation functions for the Gaussian kernel. In contrast, the sigmoid function and the ReLU function are not valid for either kernel, because their polynomial expansions fail to converge quickly enough, or more intuitively speaking, because they are not smooth enough functions to be contained in the RKHS.\nWe are ready to state the main theoretical result. In the theorem statement, we use K(X) \u2208 RP\u00d7P to denote the random kernel matrix obtained from an input vector X \u2208 Rd0 drawn randomly from the population. More precisely, the (p, q)-th entry of K(X) is given by K(zp(X), zq(X)).\nTheorem 1. Assume that the loss function L(\u00b7; y) is L-Lipchitz continuous for every y \u2208 [d2] and that K is the inverse polynomial kernel or the Gaussian kernel. For any valid activation function \u03c3, there is a constant C\u03c3(B1) such that with the radius R := C\u03c3(B1)B2r, the expected generalization error is at most\nEX,Y [L(f\u0302ccnn(X);Y )] \u2264 inf f\u2208Fcnn\nEX,Y [L(f(X);Y )] + c LC\u03c3(B1)B2r \u221a log(nP ) EX [\u2016K(X)\u20162]\u221a\nn , (16)\nwhere c > 0 is a universal constant.\nProof sketch The proof of Theorem 1 consists of two parts: First, we consider a larger function class that contains the class of CNNs. This function class is defined as:\nFccnn := { x 7\u2192 r\u2217\u2211 j=1 P\u2211 p=1 \u03b1j,phj(zp(x)) : r \u2217 <\u221e and r\u2217\u2211 j=1 \u2016\u03b1j\u20162\u2016hj\u2016H \u2264 C\u03c3(B1)B2d2 } .\nwhere \u2016\u00b7\u2016H is the norm of the RKHS associated with the kernel. This new function class relaxes the class of CNNs in two ways: 1) the filters are relaxed to belong to the RKHS, and 2) the `2-norm bounds on the weight vectors are replaced by a single constraint on \u2016\u03b1j\u20162 and \u2016hj\u2016H. We prove the following property for the predictor f\u0302ccnn: it must be an empirical risk minimizer of Fccnn, even though the algorithm has never explicitly optimized the loss within this nonparametric function class.\nSecond, we characterize the Rademacher complexity of this new function class Fccnn, proving an upper bound for it based on the matrix concentration theory. Combining this bound with the classical Rademacher complexity theory [4], we conclude that the generalization loss of f\u0302ccnn converges to the least possible generalization error of Fccnn. The later loss is bounded by the generalization loss of CNNs (because Fcnn \u2286 Fccnn), which establishes the theorem. See Appendix C for the full proof of Theorem 1.\nRemark on activation functions. It is worth noting that the quantity C\u03c3(B1) depends on the activation function \u03c3, and more precisely, depends on the convergence rate of the polynomial expansion of \u03c3. Appendix A shows that if \u03c3 is a polynomial function of degree `, then C\u03c3(B1) = O(B`1). If \u03c3 is the sinusoid function, the erf function or the smoothed hinge loss, then the quantity C\u03c3(B1) will be exponential in B1. In an algorithmic perspective, we don\u2019t need to know the activation function for executing Algorithm 1. In a theoretical perspective, however, the choice of \u03c3 is relevant from the point of Theorem 1 to compare f\u0302ccnn with the best CNN, whose representation power is characterized by the choice of \u03c3. Therefore, if a CNN with a low-degree polynomial \u03c3 performs well on a given task, then CCNN also enjoys correspondingly strong generalization. Empirically, this is actually borne out: in Section 5, we show that the quadratic activation function performs almost as well as the ReLU function for digit classification.\nRemark on parameter sharing. In order to demonstrate the importance of parameter sharing, consider a CNN without parameter sharing, so that we have filter weights wj,p for each filter index\nj and patch index p. With this change, the new CNN output (2) is\nf(x) = r\u2211 j=1 P\u2211 p=1 \u03b1j,p\u03c3(w > j,pzp(x)), where \u03b1j,p \u2208 R and wj,p \u2208 Rd1 . (17)\nNote that the hidden layer of this new network has P times more parameters than that of the convolutional neural network with parameter sharing. These networks without parameter sharing can be learned by the recursive kernel method proposed by Zhang et al. [48]. This paper shows that under the norm constraints \u2016wj\u20162 \u2264 B\u20321 and \u2211r j=1 \u2211P p=1 |\u03b1j,p| \u2264 B\u20322, the excess risk of the\nrecursive kernel method is at most O(LC\u03c3(B\u20321)B\u20322 \u221a Kmax/n), where Kmax = maxz:\u2016z\u20162\u22641K(z, z) is the maximal value of the kernel function. Plugging in the norm constraints of the function class Fcnn, we have B\u20321 = B1 and B\u20322 = B2r \u221a P . Thus, the expected risk of the estimated f\u0302 is bounded by:\nEX,Y [L(f\u0302(X);Y )] \u2264 inf f\u2208Fcnn\nEX,Y [L(f(X);Y )] + c LC\u03c3(B1)B2r \u221a PKmax\u221a\nn . (18)\nComparing this bound to Theorem 1, we see that (apart from the logarithmic terms) they differ in the multiplicative factors of \u221a P Kmax versus \u221a E[\u2016K(X)\u20162]. Since the matrix K(X) is P -dimensional, we have\n\u2016K(X)\u20162 \u2264 max p\u2208[P ] \u2211 q\u2208[P ] |K(zp(X), zq(X))| \u2264 P Kmax.\nThis demonstrates that \u221a P Kmax is always greater than \u221a E[\u2016K(X)\u20162]. In general, the first term\ncan be up to factor of \u221a P times greater, which implies that the sample complexity of the recursive kernel method is up to P times greater than that of the CCNN. This difference corresponds to the fact that the recursive kernel method learns a model with P times more parameters. Although comparing the upper bounds doesn\u2019t rigorously show that one method is better than the other, it gives the right intuition for understanding the importance of parameter sharing."
                },
                {
                    "heading": "4 Learning multi-layer CCNNs",
                    "text": "In this section, we describe a heuristic method for learning CNNs with more layers. The idea is to estimate the parameters of the convolutional layers incrementally from bottom to the top. Before presenting the multi-layer algorithm, we present two extensions, average pooling and multi-channel inputs.\nAverage pooling. Average pooling is a technique to reduce the output dimension of the convolutional layer from dimensions P \u00d7 r to dimensions P \u2032 \u00d7 r with P \u2032 < P . Suppose that the filter hj applied to all the patch vectors produces the output vector Hj(x) := (hj(z1(x)), \u00b7 \u00b7 \u00b7 , hj(zP (x))) \u2208 RP\u00d7r. Average pooling produces a P \u2032 \u00d7 r matrix, where each row is the average of the rows corresponding to a small subset of the P patches. For example, we might average every pair of adjacent patches, which would produce P \u2032 = P/2 rows. The operation of average pooling can be represented via left-multiplication using a fixed matrix G \u2208 RP \u2032\u00d7P .\nAlgorithm 2: Learning multi-layer CCNNs\nInput:Data {(xi, yi)}ni=1, kernel function K, number of layers m, regularization parameters R1, . . . , Rm, number of filters r1, . . . , rm. Define H1(x) = x. For each layer s = 2, . . . ,m:\n\u2022 Train a two-layer network by Algorithm 1, taking {(Hs\u22121(xi), yi)}ni=1 as training examples and Rs, rs as parameters. Let Hs be the output of the convolutional layer and f\u0302s be the predictor.\nOutput: Predictor f\u0302m and the top convolutional layer output Hm.\nFor the CCNN model, if we apply average pooling after the convolutional layer, then the kth output of the CCNN model becomes tr(GZ(x)Ak) where Ak \u2208 Rm\u00d7P \u2032 is the new (smaller) parameter matrix. Thus, performing a pooling operation requires only replacing every matrix Z(xi) in problem (12) by the pooled matrix GZ(xi). Note that the linearity of the CCNN allows us to effectively pool before convolution, even though for the CNN, pooling must be done after applying the nonlinear filters. The resulting ERM problem is still convex, and the number of parameters have been reduced by P/P \u2032-fold. Although average pooling is straightforward to incorporate in our framework, unfortunately, max pooling does not fit into our framework due to its nonlinearity.\nProcessing multi-channel inputs. If our input has C channels (corresponding to RGB colors, for example), then the input becomes a matrix x \u2208 RC\u00d7d0 . The c-th row of matrix x, denoted by x[c] \u2208 Rd0 , is a vector representing the c-th channel. We define the multi-channel patch vector as a concatenation of patch vectors for each channel:\nzp(x) := (zp(x[1]), . . . , zp(x[C])) \u2208 RCd1 .\nThen we construct the feature matrix Z(x) using the concatenated patch vectors {zp(x)}Pp=1. From here, everything else of Algorithm 1 remains the same. We note that this approach learns a convex relaxation of filters taking the form \u03c3( \u2211C c=1\u3008wc, zp(x[c])\u3009), parametrized by the vectors {wc}Cc=1.\nMulti-layer CCNN. Given these extensions, we are ready to present the algorithm for learning multi-layer CCNNs. The algorithm is summarized in Algorithm 2. For each layer s, we call Algorithm 1 using the output of previous convolutional layers as input\u2014note that this consists of r channels (one from each previous filter) and thus we must use the multi-channel extension. Algorithm 2 outputs a new convolutional layer along with a prediction function, which is kept only at the last layer. We optionally use averaging pooling after each successive layer. to reduce the output dimension of the convolutional layers."
                },
                {
                    "heading": "5 Experiments",
                    "text": "In this section, we compare the CCNN approach with other methods. The results are reported on the MNIST dataset and its variations for digit recognition, and on the CIFAR-10 dataset for object classification."
                },
                {
                    "heading": "5.1 MNIST and variations",
                    "text": "Since the basic MNIST digits are relatively easy to classify, we also consider more challenging variations [43]. These variations are known to be hard for methods without a convolution mechanism (for instance, see the paper [44]). Figure 3 shows a number of sample images from these different datasets. All the images are of size 28 \u00d7 28. For all datasets, we use 10,000 images for training, 2,000 images for validation and 50,000 images for testing. This 10k/2k/50k partitioning is standard for MNIST variations [43].\nImplementation details. For the CCNN method and the baseline CNN method, we train twolayer and three-layer models respectively. The models with k convolutional layers are denoted by CCNN-k and CNN-k. Each convolutional layer is constructed on 5 \u00d7 5 patches with unit stride, followed by 2\u00d7 2 average pooling. The first and the second convolutional layers contains 16 and 32 filters, respectively. The loss function is chosen as the 10-class logistic loss. We use the Gaussian kernel K(z, z\u2032) = exp(\u2212\u03b3\u2016z \u2212 z\u2032\u201622) and set hyperparameters \u03b3 = 0.2 for the first convolutional layer and \u03b3 = 2 for the second. The feature matrix Z(x) is constructed via random feature approximation [33] with dimension m = 500 for the first convolutional layer and m = 1000 for the second. Before training each CCNN layer, we preprocess the input vectors zp(xi) using local contrast normalization and ZCA whitening [12]. The convex optimization problem is solved by projected SGD with mini-batches of size 50.\nAs a baseline approach, the CNN models are activated by the ReLU function \u03c3(t) = max{0, t} or the quadratic function \u03c3(t) = t2. We train them using mini-batch SGD. The input images are preprocessed by global contrast normalization and ZCA whitening [see, e.g. 40]. We compare our method against several alternative baselines. The CCNN-1 model is compared against an SVM with the Gaussian RBF kernel (SVMrbf ) and a fully connected neural network with one hidden layer (NN-1). The CCNN-2 model is compared against methods that report the state-of-the-art results on these datasets, including the translation-invariant RBM model (TIRBM) [38], the stacked denoising auto-encoder with three hidden layers (SDAE-3) [44], the ScatNet-2 model [8] and the PCANet-2 model [9].\nResults. Table 1 shows the classification errors on the test set. The models are grouped with respect to the number of layers that they contain. For models with one convolutional layer, the\nerrors of CNN-1 are significantly lower than that of NN-1, highlighting the benefits of parameter sharing. The CCNN-1 model outperforms CNN-1 on all datasets. For models with two or more hidden layers, the CCNN-2 model outperforms CNN-2 on all datasets, and is competitive against the state-of-the-art. In particular, it achieves the best accuracy on the rand, img and img+rot dataset, and is comparable to the state-of-the-art on the remaining two datasets.\nIn order to understand the key factors that affect the training of CCNN filters, we evaluate five variants:\n(1) replace the Gaussian kernel by a linear kernel; (2) remove the ZCA whitening in preprocessing; (3) use fewer random features (m = 200 rather than m = 500) to approximate the kernel matrix; (4) regularize the parameter matrix by the Frobenius norm instead of the nuclear norm; (5) stop the mini-batch SGD early before it converges.\nWe evaluate the obtained filters by training a second convolutional layer on top of them, then evaluating the classification error on the hardest dataset img+rot. As Table 2 shows, switching to the linear kernel or removing the ZCA whitening significantly degenerates the performance. This is because that both variants equivalently modify the kernel function. Decreasing the number of random features also has a non-negligible effect, as it makes the kernel approximation less accurate. These observations highlight the impact of the kernel function. Interestingly, replacing the nuclear\nnorm by a Frobenius norm or stopping the algorithm early doesn\u2019t hurt the performance. To see their impact on the parameter matrix, we compute the effective rank (ratio between the nuclear norm and the spectral norm, see [18]) of matrix A\u0302. The effective rank obtained by the last two variants are equal to 77 and 24, greater than that of the original CCNN (equal to 12). It reveals that the last two variants have damaged the algorithm\u2019s capability of enforcing a low-rank solution. However, the CCNN filters are retrieved from the top-r singular vectors of the parameter matrix, hence the performance will remain stable as long as the top singular vectors are robust to the variation of the matrix.\nIn Section 3.4, we showed that if the activation function is a polynomial function, then the CCNN requires lower sample complexity to match the performance of the best possible CNN. More precisely, if the activation function is degree-` polynomial, then C\u03c3(B) in the upper bound will be controlled by O(B`). This motivates us to study the performance of low-degree polynomial activations. Table 1 shows that the CNN-2 model with a quadratic activation function achieves error rates comparable to that with a ReLU activation: CNN-2 (Quad) outperforms CNN-2 (ReLU) on the basic and rand datasets, and is only slightly worse on the rot and img dataset. Since the performance of CCNN matches that of the best possible CNN, the good performance of the quadratic activation in part explains why the CCNN is also good."
                },
                {
                    "heading": "5.2 CIFAR-10",
                    "text": "In order to test the capability of CCNN in complex classification tasks, we report its performance on the CIFAR-10 dataset [24]. The dataset consists of 60000 images divided into 10 classes. Each image has 32\u00d732 pixels in RGB colors. We use 50k images for training and 10k images for testing.\nImplementation details. We train CNN and CCNN models with two, three, and four layers Each convolutional layer is constructed on 5\u00d7 5 patches with unit stride, followed by 3\u00d7 3 average pooling with two-pixel stride. We train 32, 32, 64 filters for the three convolutional layers from bottom to the top. For any s\u00d7 s input, zero pixels are padded on its borders so that the input size becomes (s+4)\u00d7 (s+4), and the output size of the convolutional layer is (s/2)\u00d7 (s/2). The CNNs are activated by the ReLU function. For CCNNs, we use the Gaussian kernel with hyperparameter \u03b3 = 1, 2, 2 (for the three convolutional layers). The feature matrix Z(x) is constructed via random feature approximation with dimension m = 2000. The preprocessing steps are the same as in the MNIST experiments. It was known that the generalization performance of the CNN can be improved by training on random crops of the original image [25], so we train the CNN on random 24\u00d724 patches of the image, and test on the central 24\u00d724 patch. We also apply random cropping to training the the first and the second layer of the CCNN.\nWe compare the CCNN against other baseline methods that don\u2019t involve nonconvex optimization: the kernel SVM with Fastfood Fourier features (SVMFastfood) [27], the PCANet-2 model [9] and the convolutional kernel networks (CKN) [30].\nResults. We report the classification errors on in Table 3. For models of all depths, the CCNN model outperforms the CNN model. The CCNN-2 and the CCNN-3 model also outperform the three baseline methods. The advantage of the CCNN is substantial for learning two-layer networks, when the optimality guarantee of CCNN holds. The performance improves as more layers are stacked, but as we observe in Table 3, the marginal gain of CCNN diminishes as the network grows deeper. We suspect that this is due to the greedy fashion in which the CCNN layers are constructed. Once\ntrained, the low-level filters of the CCNN are no longer able to adapt to the final classifier. In contrast, the low-level filters of the CNN model are continuously adjusted via backpropagation.\nIt is worth noting that the performance of the CNN can be further improved by adding more layers, switching from average pooling to max pooling, and being regularized by local response normalization and dropout (see, e.g. [25]). The figures in Table 3 are by no means the state-ofthe-art result on CIFAR-10. However, it does demonstrate that the convex relaxation is capable of improving the performance of convolutional neural networks. For future work, we propose to study a better way for convexifying deep CNNs.\nIn Figure 4, we compare the computational efficiency of CNN-3 to its convexified version CCNN3. Both models are trained by mini-batch SGD (with batchsize equal to 50) on a single processor. We optimized the choice of step-size for each algorithm. From the plot, it is easy to identify the three stages of the CCNN-3 curve for training the three convolutional layers from bottom to the top. We also observe that the CCNN converges faster than the CNN. More precisely, the CCNN takes half the runtime of the CNN to reach an error rate of 28%, and one-fifth of the runtime to reach an error rate of 23%. The per-iteration cost for training the first layer of CCNN is about 109% of the per-iteration cost of CNN, but the per-iteration cost for training the remaining two layers are about 18% and 7% of that of CNN. Thus, training CCNN scales well to large datasets.\nTraining a CCNN on top of a CNN. Instead of training a CCNN from scratch, we can also train CCNNs on top of existing CNN layers. More concretely, once a CNN-k model is obtained, we train a two-layer CCNN by taking the (k\u2212 1)-th hidden layer of CNN as input. This approach preserves the low-level features learned by CNN, only convexifying its top convolutional layer. The underlying motivation is that the traditional CNN is good at learning low-level features through\nbackpropagation, while the CCNN is optimal in learning two-layer networks. In this experiment, we convexify the top convolutional layer of CNN-2 and CNN-3 using the CCNN approach, with a smaller Gaussian kernel parameter (i.e. \u03b3 = 0.1) and keeping other hyperparameters the same as in the training of CCNN-2 and CCNN-3. The results are shown in Table 4. The convexified CNN achieves better accuracy on all network depths. It is worth noting that the time for training a convexified layer is only a small fraction of the time for training the original CNN."
                },
                {
                    "heading": "6 Related work",
                    "text": "With the empirical success of deep neural networks, there has been an increasing interest in theoretical understanding. Bengio et al. [5] showed how to formulate neural network training as a convex optimization problem involving an infinite number of parameters. This perspective encourages incrementally adding neurons to the network, whose generalization error was studied by Bach [3]. Zhang et al. [47] propose a polynomial-time ensemble method for learning fully-connected neural networks, but their approach handles neither parameter sharing nor the convolutional setting. Other relevant works for learning fully-connected networks include [35, 23, 29]. Aslan et al. [1, 2] propose a method for learning multi-layer latent variable models. They showed that for certain activation functions, the proposed method is a convex relaxation for learning the fully-connected neural network.\nAnother line of work is devoted to understanding the energy landscape of a neural network. Under certain assumptions on the data distribution, it can be shown that any local minimum of a two-layer fully connected neural network has an objective value that is close to the global minimum value [14, 11]. If this property holds, then gradient descent can find a solution that is \u201cgood enough\u201d. Similar results have also been established for over-specified neural networks [34], or neural networks that has a certain parallel topology [20]. However, these results are not applicable to a CNN, since the underlying assumptions are not satisfied by CNNs.\nPast work has studied learning translation invariant features without backpropagation. Mairal et al. [30] present convolutional kernel networks. They propose a translation-invariant kernel whose feature mapping can be approximated by a composition of the convolution, non-linearity and pooling operators, obtained through unsupervised learning. However, this method is not equipped with the optimality guarantees that we have provided for CCNNs in this paper, even for learning one convolution layer. The ScatNet method [8] uses translation and deformation-invariant filters constructed by wavelet analysis; however, these filters are independent of the data, unlike the analysis in this paper. Daniely et al. [13] show that a randomly initialized CNN can extract features as powerful as kernel methods, but it is not clear how to provably improve the model from a random initialization."
                },
                {
                    "heading": "7 Conclusion",
                    "text": "In this paper, we have shown how convex optimization can be used to efficiently optimize CNNs as well as understand them statistically. Our convex relaxation consists of two parts: the nuclear norm relaxation for handling parameter sharing, and the RKHS relaxation for handling non-linearity. For the two-layer CCNN, we proved that its generalization error converges to that of the best possible two-layer CNN. We handled multi-layer CCNNs only heuristically, but observed that adding more\nlayers improves the performance in practice. On real data experiments, we demonstrated that CCNN outperforms the traditional CNN of the same depth, is computationally efficient, and can be combined with the traditional CNN to achieve better performance. A major open problem is to formally study the convex relaxation of deep CNNs."
                },
                {
                    "heading": "Acknowledgements",
                    "text": "This work was partially supported by Office of Naval Research MURI grant DOD-002888, Air Force Office of Scientific Research Grant AFOSR-FA9550-14-1-001, Office of Naval Research grant ONR-N00014, National Science Foundation Grant CIF-31712-23800, as well as a Microsoft Faculty Research Award to the second author.\nA Inverse polynomial kernel and Gaussian kernel\nIn this appendix, we describe the properties of the two types of kernels \u2014 the inverse polynomial kernel (14) and the Gaussian RBF kernel (15). We prove that the associated reproducing kernel Hilbert Spaces (RKHS) of these kernels contain filters taking the form h : z 7\u2192 \u03c3(\u3008w, z\u3009) for particular activation functions \u03c3.\nA.1 Inverse polynomial kernel\nWe first verify that the function (14) is a kernel function. This holds since that we can find a mapping \u03d5 : Rd1 \u2192 `2(N) such that K(z, z\u2032) = \u3008\u03d5(z), \u03d5(z\u2032)\u3009. We use zi to represent the i-th coordinate of an infinite-dimensional vector z. The (k1, . . . , kj)-th coordinate of \u03d5(z), where j \u2208 N and k1, . . . , kj \u2208 [d1], is defined as 2\u2212 j+1 2 xk1 . . . xkj . By this definition, we have\n\u3008\u03d5(x), \u03d5(y)\u3009 = \u221e\u2211 j=0 2\u2212(j+1) \u2211 (k1,...,kj)\u2208[d1]j zk1 . . . zkjz \u2032 k1 . . . z \u2032 kj . (19)\nSince \u2016z\u20162 \u2264 1 and \u2016z\u2032\u20162 \u2264 1, the series on the right-hand side is absolutely convergent. The inner term on the right-hand side of equation (19) can be simplified to\u2211\n(k1,...,kj)\u2208[d1]j zk1 . . . zkjz\n\u2032 k1 . . . z \u2032 kj = (\u3008z, z\u2032\u3009)j . (20)\nCombining equations (19) and (20) and using the fact that |\u3008z, z\u2032\u3009| \u2264 1, we have\n\u3008\u03d5(z), \u03d5(z\u2032)\u3009 = \u221e\u2211 j=0 2\u2212(j+1)(\u3008z, z\u2032\u3009)j (i)= 1 2\u2212 \u3008z, z\u2032\u3009 = K(z, z\u2032),\nwhich verifies that K is a kernel function and \u03d5 is the associated feature map. Next, we prove that the associated RKHS contains the class of nonlinear filters. The lemma was proved by Zhang et al. [48]. We include the proof to make the paper self-contained.\nLemma 1. Assume that the function \u03c3(x) has a polynomial expansion \u03c3(t) = \u2211\u221e\nj=0 ajt j. Let C\u03c3(\u03bb) := \u221a\u2211\u221e j=0 2 j+1a2j\u03bb\n2j. If C\u03c3(\u2016w\u20162) <\u221e, then the RKHS induced by the inverse polynomial kernel contains function h : z 7\u2192 \u03c3(\u3008w, z\u3009) with Hilbert norm \u2016h\u2016H = C\u03c3(\u2016w\u20162).\nProof. Let \u03d5 be the feature map that we have defined for the polynomial inverse kernel. We define vector w \u2208 `2(N) as follow: the (k1, . . . , kj)-th coordinate of w, where j \u2208 N and k1, . . . , kj \u2208 [d1], is equal to 2\nj+1 2 ajwk1 . . . wkj . By this definition, we have\n\u03c3(\u3008w, z\u3009) = \u221e\u2211 t=0 aj(\u3008w, z\u3009)j = \u221e\u2211 j=0 aj \u2211 (k1,...,kj)\u2208[d1]j wk1 . . . wkjzk1 . . . zkj = \u3008w,\u03d5(z)\u3009, (21)\nwhere the first equation holds since \u03c3(x) has a polynomial expansion \u03c3(x) = \u2211\u221e\nj=0 ajx j , the second\nby expanding the inner product, and the third by definition of w and \u03d5(z). The `2-norm of w is equal to:\n\u2016w\u201622 = \u221e\u2211 j=0 2j+1a2j \u2211 (k1,...,kj)\u2208[d1]j w2k1w 2 k2 \u00b7 \u00b7 \u00b7w 2 kj = \u221e\u2211 j=0 2j+1a2j\u2016w\u2016 2j 2 = C 2 \u03c3(\u2016w\u20162) <\u221e. (22)\nBy the basic property of the RKHS, the Hilbert norm of h is equal to the `2-norm of w. Combining equations (21) and (22), we conclude that h \u2208 H and \u2016h\u2016H = \u2016w\u20162 = C\u03c3(\u2016w\u20162).\nAccording to Lemma 1, it suffices to upper bound C\u03c3(\u03bb) for a particular activation function \u03c3. To make C\u03c3(\u03bb) < \u221e, the coefficients {aj}\u221ej=0 must quickly converge to zero, meaning that the activation function must be sufficiently smooth. For polynomial functions of degree `, the definition of C\u03c3 implies that C\u03c3(\u03bb) = O(\u03bb`). For the sinusoid activation \u03c3(t) := sin(t), we have\nC\u03c3(\u03bb) = \u221a\u221a\u221a\u221a \u221e\u2211 j=0\n22j+2\n((2j + 1)!)2 \u00b7 (\u03bb2)2j+1 \u2264 2e\u03bb2 .\nFor the erf function and the smoothed hinge loss function defined in Section 3.4, Zhang et al. [48, Proposition 1] proved that C\u03c3(\u03bb) = O(ec\u03bb 2 ) for universal numerical constant c > 0.\nA.2 Gaussian kernel\nThe Gaussian kernel also induces an RKHS that contains a particular class of nonlinear filters. The proof is similar to that of Lemma 1.\nLemma 2. Assume that the function \u03c3(x) has a polynomial expansion \u03c3(t) = \u2211\u221e\nj=0 ajt j. Let C\u03c3(\u03bb) := \u221a\u2211\u221e j=0 j!e2\u03b3 (2\u03b3)j a2j\u03bb\n2j. If C\u03c3(\u2016w\u20162) < \u221e, then the RKHS induced by the Gaussian kernel contains the function h : z 7\u2192 \u03c3(\u3008w, z\u3009) with Hilbert norm \u2016h\u2016H = C\u03c3(\u2016w\u20162).\nProof. When \u2016z\u20162 = \u2016z\u2032\u20162 = 1, It is well-known [see, e.g. 41] the following mapping \u03d5 : Rd1 \u2192 `2(N) is a feature map for the Gaussian RBF kernel: the (k1, . . . , kj)-th coordinate of \u03d5(z), where j \u2208 N and k1, . . . , kj \u2208 [d1], is defined as e\u2212\u03b3((2\u03b3)j/j!)1/2xk1 . . . xkj . Similar to equation (21), we define a vector w \u2208 `2(N) as follow: the (k1, . . . , kj)-th coordinate of w, where j \u2208 N and k1, . . . , kj \u2208 [d1], is equal to e\u03b3((2\u03b3)j/j!)\u22121/2ajwk1 . . . wkj . By this definition, we have\n\u03c3(\u3008w, z\u3009) = \u221e\u2211 t=0 aj(\u3008w, z\u3009)j = \u221e\u2211 j=0 aj \u2211 (k1,...,kj)\u2208[d1]j wk1 . . . wkjzk1 . . . zkj = \u3008w,\u03d5(z)\u3009. (23)\nThe `2-norm of w is equal to:\n\u2016w\u201622 = \u221e\u2211 j=0 j!e2\u03b3 (2\u03b3)j a2j \u2211 (k1,...,kj)\u2208[d1]j w2k1w 2 k2 \u00b7 \u00b7 \u00b7w 2 kj = \u221e\u2211 j=0 j!e2\u03b3 (2\u03b3)j a2j\u2016w\u2016 2j 2 = C 2 \u03c3(\u2016w\u20162) <\u221e. (24)\nCombining equations (21) and (22), we conclude that h \u2208 H and \u2016h\u2016H = \u2016w\u20162 = C\u03c3(\u2016w\u20162).\nComparing Lemma 1 and Lemma 2, we find that the Gaussian kernel imposes a stronger condition on the smoothness of the activation function. For polynomial functions of degree `, we still have C\u03c3(\u03bb) = O(\u03bb`). For the sinusoid activation \u03c3(t) := sin(t), it can be verified that\nC\u03c3(\u03bb) = \u221a\u221a\u221a\u221ae2\u03b3 \u221e\u2211 j=0\n1 (2j + 1)! \u00b7 (\u03bb2 2\u03b3 )2j+1 \u2264 e\u03bb2/(4\u03b3)+\u03b3 .\nHowever, the value of C\u03c3(\u03bb) is infinite when \u03c3 is the erf function or the smoothed hinge loss, meaning that the Gaussian kernel\u2019s RKHS doesn\u2019t contain filters activated by these two functions."
                },
                {
                    "heading": "B Convex relaxation for nonlinear activation",
                    "text": "In this appendix, we provide a detailed derivation of the relaxation for nonlinear activation functions that we previously sketched in Section 3.2. Recall that the filter output is \u03c3(\u3008wj , z\u3009). Appendix A shows that given a sufficiently smooth activation function \u03c3, we can find some kernel function K : Rd1\u00d7Rd1 \u2192 R and a feature map \u03d5 : Rd1 \u2192 `2(N) satisfying K(z, z\u2032) \u2261 \u3008\u03d5(z), \u03d5(z\u2032)\u3009, such that\n\u03c3(\u3008wj , z\u3009) \u2261 \u3008wj , \u03d5(z)\u3009. (25)\nHere wj \u2208 `2(N) is a countable-dimensional vector and \u03d5 := (\u03d51, \u03d52, . . . ) is a countable sequence of functions. Moreover, the `2-norm of wj is bounded as \u2016wj\u20162 \u2264 C\u03c3(\u2016wj\u20162) for a monotonically increasing function C\u03c3 that depends on the kernel (see Lemma 1 and Lemma 2). As a consequence, we may use \u03d5(z) as the vectorized representation of the patch z, and use wj as the linear transformation weights, then the problem is reduced to training a CNN with the identity activation function.\nThe filter is parametrized by an infinite-dimensional vector wj . Our next step is to reduce the original ERM problem to a finite-dimensional one. In order to minimize the empirical risk, one only needs to concern the output on the training data, that is, the output of \u3008wj , \u03d5(zp(xi))\u3009 for all (i, p) \u2208 [n] \u00d7 [P ]. Let T be the orthogonal projector onto the linear subspace spanned by the vectors {\u03d5(zp(xi)) : (i, p) \u2208 [n]\u00d7 [P ]}. Then we have\n\u2200 (i, p) \u2208 [n]\u00d7 [P ] : \u3008wj , \u03d5(zp(xi))\u3009 = \u3008wj , T\u03d5(zp(xi))\u3009 = \u3008Twj , \u03d5(zp(xi))\u3009.\nThe last equation follows since the orthogonal projector T is self-adjoint. Thus, for empirical risk minimization, we can without loss of generality assume that wj belongs to the linear subspace spanned by {\u03d5(zp(xi)) : (i, p) \u2208 [n]\u00d7 [P ]} and reparametrize it by:\nwj = \u2211\n(i,p)\u2208[n]\u00d7[P ]\n\u03b2j,(i,p)\u03d5(zp(xi)). (26)\nLet \u03b2j \u2208 RnP be a vector whose whose (i, p)-th coordinate is \u03b2j,(i,p). In order to estimate wj , it suffices to estimate the vector \u03b2j . By definition, the vector satisfies the relation \u03b2 > j K\u03b2j = \u2016wj\u201622, where K is the nP \u00d7 nP kernel matrix defined in Section 3.2. As a consequence, if we can find a matrix Q such that QQ> = K, then we have the norm constraint\n\u2016Q>\u03b2j\u20162 = \u221a \u03b2>j K\u03b2j = \u2016wj\u20162 \u2264 C\u03c3(\u2016wj\u20162) \u2264 C\u03c3(B). (27)\nLet v(z) \u2208 RnP be a vector whose (i, p)-th coordinate is equal to K(z, zp(xi)). Then by equations (25) and (26), the filter output can be written as\n\u03c3 ( \u3008wj , z\u3009 ) \u2261 \u3008wj , \u03d5(z)\u3009 \u2261 \u3008\u03b2j , v(z)\u3009. (28)\nFor any patch zp(xi) in the training data, the vector v(zp(xi)) belongs to the column space of the kernel matrix K. Therefore, letting Q\u2020 represent the pseudo-inverse of matrix Q, we have\n\u2200 (i, p) \u2208 [n]\u00d7 [P ] : \u3008\u03b2j , v(zp(xi))\u3009 = \u03b2>j QQ\u2020v(zp(xi)) = \u3008(Q>)\u2020Q>\u03b2j , v(zp(xi))\u3009.\nIt means that if we replace the vector \u03b2j on the right-hand side of equation (28) by the vector (Q>)\u2020Q>\u03b2j , then it won\u2019t change the empirical risk. Thus, for ERM we can parametrize the filters by\nhj(z) := \u3008(Q>)\u2020Q>\u03b2j , v(z)\u3009 = \u3008Q\u2020v(z), Q>\u03b2j\u3009. (29)\nLet Z(x) be an P \u00d7 nP matrix whose p-th row is equal to Q\u2020v(zp(x)). Similar to the steps in equation (6), we have\nfk(x) = r\u2211 j=1 \u03b1>k,jZ(x)K 1/2\u03b2j = tr ( Z(x) ( r\u2211 j=1 K1/2\u03b2j\u03b1 > k,j )) = tr(Z(x)Ak),\nwhere Ak := \u2211r j=1Q >\u03b2j\u03b1 > k,j . If we let A := (A1, . . . , Ad2) denote the concatenation of these matrices, then this larger matrix satisfies the constraints:\nConstraint (C1): max j\u2208[r] \u2016Q>\u03b2j\u20162 \u2264 C\u03c3(B1) and max (k,j)\u2208[d2]\u00d7[r] \u2016\u03b1k,j\u20162 \u2264 B2.\nConstraint (C2): The matrix A has rank at most r.\nWe relax these two constraints to the nuclear norm constraint: \u2016A\u2016\u2217 \u2264 C\u03c3(B1)B2r \u221a d2. (30)\nBy comparing constraints (8) and (30), we see that the only difference is that the term B1 in the norm bound has been replaced by C\u03c3(B1). This change is needed because we have used the kernel trick to handle nonlinear activation functions."
                },
                {
                    "heading": "C Proof of Theorem 1",
                    "text": "Since the output is one-dimensional in this case, we can adopt the simplified notation (A,\u03b1j,p) for the matrix (A1, \u03b11,j,p). Letting H be the RKHS associated with the kernel function K, and letting \u2016\u00b7\u2016H be the associated Hilbert norm, consider the function class\nFccnn := { x 7\u2192 r\u2217\u2211 j=1 P\u2211 p=1 \u03b1j,phj(zp(x)) : r \u2217 <\u221e and r\u2217\u2211 j=1 \u2016\u03b1j\u20162\u2016hj\u2016H \u2264 C\u03c3(B1)B2d2 } . (31)\nHere \u03b1j,p denotes the p-th entry of vector \u03b1j \u2208 RP , whereas the quantity C\u03c3(B1) only depends on B1 and the activation function \u03c3. The following lemma shows that the function class Fccnn is rich enough so that it contains family of CNN predictors as a subset. The reader should recall the notion of a valid activation function, as defined prior to the statement of Theorem 1.\nLemma 3. For any valid activation function \u03c3, there is a quantity C\u03c3(B1), depending only on B1 and \u03c3, such that Fcnn \u2282 Fccnn.\nSee Appendix C.1 for the proof.\nNext, we connect the function class Fccnn to the CCNN algorithm. Recall that f\u0302ccnn is the predictor trained by the CCNN algorithm. The following lemma shows that f\u0302ccnn is an empirical risk minimizer within Fccnn.\nLemma 4. With the CCNN hyper-parameter R = C\u03c3(B1)B2d2, the predictor f\u0302ccnn is guaranteed to satisfy the inclusion\nf\u0302ccnn \u2208 arg min f\u2208Fccnn n\u2211 i=1 L(f(xi); yi).\nSee Appendix C.2 for the proof.\nOur third lemma shows that the function class Fccnn is not \u201ctoo big\u201d, which we do by upper bounding its Rademacher complexity. The Rademacher complexity of a function class F = {f : X \u2192 R} with respect to n i.i.d. samples {Xi}ni=1 is given by\nRn(F) := EX, [ sup f\u2208F 1 n n\u2211 i=1 if(Xi) ] ,\nwhere { i}ni=1 are an i.i.d. sequence of uniform {\u22121,+1}-valued variables. Rademacher complexity plays an important role in empirical process theory, and in particular can be used to bound the generalization loss of our empirical risk minimization problem. We refer the reader to Bartlett and Mendelson [4] for an introduction to the theoretical properties of Rademacher complexity.\nThe following lemma involves the kernel matrix K(x) \u2208 RP\u00d7P whose (i, j)-th entry is equal to K(zi(x), zj(x)), as well as the expectation E[\u2016K(X)\u20162] of the spectral norm of this matrix when X is drawn randomly.\nLemma 5. There is a universal constant c such that\nRn(Fccnn) \u2264 c C\u03c3(B1)B2r \u221a log(nP )E[\u2016K(X)\u20162]\u221a n . (32)\nSee Appendix C.3 for the proof of this claim.\nCombining Lemmas 3 through 5 allows us to compare the CCNN predictor f\u0302ccnn against the best model in the CNN class. Lemma 4 shows that f\u0302ccnn is the empirical risk minimizer within function class Fccnn. Thus, the theory of Rademacher complexity [4] guarantees that\nE[L(Fccnn(X);Y )] \u2264 inf f\u2208Fccnn E[L(f(x); y)] + 2L \u00b7 Rn(Fccnn) + c\u221a n , (33)\nwhere c is a universal constant. By Lemma 3, we have\ninf f\u2208Fccnn E[L(f(X);Y )] \u2264 inf f\u2208Fcnn E[L(f(X);Y )].\nPlugging this upper bound into inequality (33) and applying Lemma 5 completes the proof.\nC.1 Proof of Lemma 3\nWith the activation functions specified in the lemma statement, Lemma 1 and Lemma 2 show that there is a quantity C\u03c3(B1), such any filter of CNN belongs to the reproducing kernel Hilbert space H and its Hilbert norm is bounded by C\u03c3(B1). As a consequence, any function f \u2208 Fcnn can be represented by\nf(x) := r\u2211 j=1 P\u2211 p=1 \u03b1j,phj(zp(x)) where \u2016hj\u2016H \u2264 C\u03c3(B1) and \u2016\u03b1j\u20162 \u2264 B2.\nIt is straightforward to verify that function f satisfies the constraint in equation (31), and consequently belongs to Fccnn.\nC.2 Proof of Lemma 4 Let CR denote the function class { x 7\u2192 tr(Z(x)A) : \u2016A\u2016\u2217 \u2264 R } . We first prove that CR \u2282 Fccnn. Consider an arbitrary function fA(x) : = tr(Z(x)A) belonging to CR. Note that the matrix A has a singular value decomposition (SVD) A = \u2211r\u2217 j=1 \u03bbjwju > j for some r\n\u2217 < \u221e, where wj and uj are unit vectors and \u03bbj are real numbers. Using this notation, the function fA can be represented as the sum\nfA(x) = r\u2217\u2211 j=1 \u03bbju > j Z(x)wj .\nLet v(z) be an nP -dimensional vector whose (i, p)-th coordinate is equal to K(z, zp(xi)). Then Q\u2020v(zp(x)) is the p-th row of matrix Z(x). Letting hj denote the mapping z 7\u2192 \u3008Q\u2020v(z), wj\u3009, we have\nfA(x) = r\u2217\u2211 j=1 P\u2211 p=1 \u03bbjuj,phj(zp(x)).\nThe function hj can also be written as z 7\u2192 \u3008(Q>)\u2020wj , v(z)\u3009. Equation (27) implies that the Hilbert norm of this function is equal to \u2016Q>(Q>)\u2020wj\u20162, which is bounded by \u2016wj\u20162 = 1. Thus we have\nr\u2217\u2211 j=1 \u2016\u03bbjuj\u20162\u2016hj\u2016H = r\u2217\u2211 j=1 |\u03bbj | = \u2016A\u2016\u2217 \u2264 C\u03c3(B1)B2r,\nwhich implies that fA \u2208 Fccnn. Next, it suffices to prove that for some empirical risk minimizer f in function class Fccnn, it also belongs to the function class CR. Recall that any function f \u2208 Fccnn can be represented in the form\nf(x) = r\u2217\u2211 j=1 P\u2211 p=1 \u03b1j,phj(zp(x)).\nwhere the filter hj belongs to the RKHS. Let \u03d5 : Rd1 \u2192 `2(N) be a feature map of the kernel function. The function hj(z) can be represented by \u3008w, \u03d5(z)\u3009 for some w \u2208 `2(N). In Appendix B, we have shown that any function taking this form can be replaced by \u3008(Q>)\u2020Q>\u03b2j , v(z)\u3009 for some vector \u03b2j \u2208 RnP without changing the output on the training data. Thus, there exists at least one empirical risk minimizer f of Fccnn such that all of its filters take the form\nhj(z) = \u3008(Q>)\u2020Q>\u03b2j , v(z)\u3009. (34)\nBy equation (27), the Hilbert norm of these filters satisfy:\n\u2016hj\u2016H = \u2016Q>(Q>)\u2020Q>\u03b2j\u20162 = \u2016Q>\u03b2j\u20162.\nAccording to Appendix B, if all filters take the form (34), then the function f can be represented by tr(Z(x)A) for matrix A := \u2211r\u2217 j=1Q >\u03b2j\u03b1 > j . Consequently, the nuclear norm is bounded as\n\u2016A\u2016\u2217 \u2264 r\u2217\u2211 j=1 \u2016\u03b1j\u20162\u2016Q>\u03b2j\u20162 = r\u2217\u2211 j=1 \u2016\u03b1j\u20162\u2016hj\u2016H \u2264 C\u03c3(B1)B2r = R,\nwhich establishes that the function f belongs to the function class CR.\nC.3 Proof of Lemma 5\nThroughout this proof, we use the shorthand notation R := C\u03c3(B1)B2r. Recall that any function f \u2208 Fccnn can be represented in the form\nf(x) = r\u2217\u2211 j=1 P\u2211 p=1 \u03b1j,phj(zp(x)) where hj \u2208 H, (35)\nand the Hilbert space H is induced by the kernel function K. Since any patch z belongs to the compact space {z : \u2016z\u20162 \u2264 1} and K is a continuous kernel satisfying K(z, z) \u2264 1, Mercer\u2019s theorem [41, Theorem 4.49] implies that there is a feature map \u03d5 : Rd1 \u2192 `2(N) such that \u2211\u221e `=1 \u03d5`(z)\u03d5`(z\n\u2032) converges uniformly and absolutely to K(z, z\u2032). Thus, we can write K(z, z\u2032) = \u3008\u03d5(z), \u03d5(z\u2032)\u3009. Since \u03d5 is a feature map, every any function h \u2208 H can be written as h(z) = \u3008\u03b2, \u03d5(z)\u3009 for some \u03b2 \u2208 `2(N), and the Hilbert norm of h is equal to \u2016\u03b2\u20162.\nUsing this notation, we can write the filter hj in equation (35) as hj(z) = \u3008\u03b2j , \u03d5(z)\u3009 for some vector \u03b2j \u2208 `2(N), with Hilbert norm \u2016hj\u2016H = \u2016\u03b2j\u20162. For each x, let \u03a8(x) denote the linear operator that maps any sequence \u03b8 \u2208 `2(N) to the vector in RP with elements[\n\u3008\u03b8, \u03d5(z1(x))\u3009 . . . \u3008\u03b8, \u03d5(zP (x))\u3009 ]T .\nInformally, we can think of \u03a8(x) as a matrix whose p-th row is equal to \u03d5(zp(x)). The function f can then be written as\nf(xi) = r\u2217\u2211 j=1 \u03b1>j \u03a8(xi)\u03b2j = tr \u03a8(xi)( r\u2217\u2211 j=1 \u03b2j\u03b1 > j ) . (36) The matrix \u2211r\u2217 j=1 \u03b2j\u03b1\n> j satisfies the constraint\u2225\u2225\u2225\u2225\u2225\u2225 r\u2217\u2211 j=1 \u03b2j\u03b1 > j \u2225\u2225\u2225\u2225\u2225\u2225 \u2217 \u2264 r\u2217\u2211 j=1 \u2016\u03b1j\u20162 \u00b7 \u2016\u03b2j\u20162 = r\u2217\u2211 j=1 \u2016\u03b1j\u20162 \u00b7 \u2016hj\u2016H \u2264 R. (37)\nCombining equation (36) and inequality (37), we find that the Rademacher complexity is bounded by\nRn(Fccnn) = 1\nn E\n[ sup\nf\u2208Fccnn n\u2211 i=1 if(xi) ] \u2264 1 n E [ sup A:\u2016A\u2016\u2217\u2264R tr (( n\u2211 i=1 i\u03a8(xi) ) A )]\n= R\nn E [\u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 i\u03a8(xi) \u2225\u2225\u2225\u2225\u2225 2 ] , (38)\nwhere the last equality uses Ho\u0308lder\u2019s inequality\u2014that is, the duality between the nuclear norm and the spectral norm.\nAs noted previously, we may think informally of the quantity \u2211n\ni=1 i\u03a8(xi) as a matrix with P rows and infinitely many columns. Let \u03a8(d)(xi) denote the submatrix consisting of the first d columns of \u03a8(xi) and let \u03a8 (\u2212d)(xi) denote the remaining sub-matrix. We have\nE [\u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 i\u03a8(xi) \u2225\u2225\u2225\u2225\u2225 2 ] \u2264 E [\u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 i\u03a8 (d)(xi) \u2225\u2225\u2225\u2225\u2225 2 ] + E \u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 i\u03a8 (\u2212d)(xi) \u2225\u2225\u2225\u2225\u2225 2 F 1/2\n\u2264 E [\u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 i\u03a8 (d)(xi) \u2225\u2225\u2225\u2225\u2225 2 ] + ( nP \u00b7 E [ \u221e\u2211 `=d+1 \u03d52` (z) ])1/2 .\nSince \u2211\u221e\n`=1 \u03d5 2 ` (z) uniformly converges to K(z, z), the second term on the right-hand side converges\nto zero as d \u2192 \u221e. Thus it suffices to bound the first term and take the limit. In order to upper bound the spectral norm \u2225\u2225\u2211n i=1 i\u03a8 (d)(xi) \u2225\u2225 2 , we use a matrix Bernstein inequality due to Minsker [31, Theorem 2.1]. In particular, whenever tr(\u03a8(d)(xi)(\u03a8 (d)(xi))\n>) \u2264 C1, there is a universal constant c such that the expected spectral norm is upper bounded as\nE [\u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 i\u03a8 (d)(xi) \u2225\u2225\u2225\u2225\u2225 2 ] \u2264 c \u221a log(nC1)E [( n\u2211 i=1 \u2016\u03a8(d)(xi)(\u03a8(d)(xi))>\u20162 )1/2]\n\u2264 c \u221a log(nC1) ( nE[\u2016\u03a8(X)\u03a8>(X)\u20162] )1/2 .\nNote that the uniform kernel expansion K(z, z\u2032) = \u2211\u221e\n`=1 \u03d5`(z)\u03d5`(z \u2032) implies the trace norm bound\ntr(\u03a8(d)(xi)(\u03a8 (d)(xi)) >) \u2264 tr(K(xi)). Since all patches are contained in the unit `2-ball, the kernel function K is uniformly bounded by 1, and hence C1 \u2264 P . Taking the limit d\u2192\u221e, we find that\nE [\u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 i\u03a8(xi) \u2225\u2225\u2225\u2225\u2225 2 ] \u2264 c \u221a log(nP ) ( nE[\u2016K(X)\u20162] )1/2 .\nFinally, substituting this upper bound into inequality (38) yields the claimed bound (32)."
                }
            ],
            "year": 2016,
            "references": [
                {
                    "title": "Convex two-layer modeling",
                    "authors": [
                        "\u00d6. Aslan",
                        "H. Cheng",
                        "X. Zhang",
                        "D. Schuurmans"
                    ],
                    "venue": "In Advances in Neural Information Processing Systems,",
                    "year": 2013
                },
                {
                    "title": "Convex deep learning via normalized kernels",
                    "authors": [
                        "\u00d6. Aslan",
                        "X. Zhang",
                        "D. Schuurmans"
                    ],
                    "venue": "In Advances in Neural Information Processing Systems,",
                    "year": 2014
                },
                {
                    "title": "Breaking the curse of dimensionality with convex neural networks",
                    "authors": [
                        "F. Bach"
                    ],
                    "venue": "arXiv preprint arXiv:1412.8690,",
                    "year": 2014
                },
                {
                    "title": "Rademacher and Gaussian complexities: Risk bounds and structural results",
                    "authors": [
                        "P.L. Bartlett",
                        "S. Mendelson"
                    ],
                    "venue": "The Journal of Machine Learning Research,",
                    "year": 2003
                },
                {
                    "title": "Convex neural networks",
                    "authors": [
                        "Y. Bengio",
                        "N.L. Roux",
                        "P. Vincent",
                        "O. Delalleau",
                        "P. Marcotte"
                    ],
                    "venue": "In Advances in Neural Information Processing Systems,",
                    "year": 2005
                },
                {
                    "title": "Training a 3-node neural network is NP-complete",
                    "authors": [
                        "A.L. Blum",
                        "R.L. Rivest"
                    ],
                    "venue": "Neural Networks,",
                    "year": 1992
                },
                {
                    "title": "Online learning and stochastic approximations",
                    "authors": [
                        "L. Bottou"
                    ],
                    "venue": "On-line learning in neural networks,",
                    "year": 1998
                },
                {
                    "title": "Invariant scattering convolution networks. Pattern Analysis and Machine Intelligence",
                    "authors": [
                        "J. Bruna",
                        "S. Mallat"
                    ],
                    "venue": "IEEE Transactions on,",
                    "year": 2013
                },
                {
                    "title": "Pcanet: A simple deep learning baseline for image classification",
                    "authors": [
                        "T.-H. Chan",
                        "K. Jia",
                        "S. Gao",
                        "J. Lu",
                        "Z. Zeng",
                        "Y. Ma"
                    ],
                    "venue": "IEEE Transactions on Image Processing,",
                    "year": 2015
                },
                {
                    "title": "A fast and accurate dependency parser using neural networks",
                    "authors": [
                        "D. Chen",
                        "C.D. Manning"
                    ],
                    "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
                    "year": 2014
                },
                {
                    "title": "The loss surface of multilayer networks",
                    "authors": [
                        "A. Choromanska",
                        "M. Henaff",
                        "M. Mathieu",
                        "G.B. Arous",
                        "Y. LeCun"
                    ],
                    "year": 2014
                },
                {
                    "title": "An analysis of single-layer networks in unsupervised feature learning",
                    "authors": [
                        "A. Coates",
                        "H. Lee",
                        "A.Y. Ng"
                    ],
                    "venue": "Ann Arbor,",
                    "year": 2010
                },
                {
                    "title": "Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity",
                    "authors": [
                        "A. Daniely",
                        "R. Frostig",
                        "Y. Singer"
                    ],
                    "venue": "arXiv preprint arXiv:1602.05897,",
                    "year": 2016
                },
                {
                    "title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization",
                    "authors": [
                        "Y.N. Dauphin",
                        "R. Pascanu",
                        "C. Gulcehre",
                        "K. Cho",
                        "S. Ganguli",
                        "Y. Bengio"
                    ],
                    "venue": "In Advances in Neural Information Processing Systems,",
                    "year": 2014
                },
                {
                    "title": "On the Nystr\u00f6m method for approximating a Gram matrix for improved kernel-based learning",
                    "authors": [
                        "P. Drineas",
                        "M.W. Mahoney"
                    ],
                    "venue": "The Journal of Machine Learning Research,",
                    "year": 2005
                },
                {
                    "title": "Efficient projections onto the `1ball for learning in high dimensions",
                    "authors": [
                        "J. Duchi",
                        "S. Shalev-Shwartz",
                        "Y. Singer",
                        "T. Chandra"
                    ],
                    "venue": "In Proceedings of the 25th International Conference on Machine Learning,",
                    "year": 2008
                },
                {
                    "title": "Adaptive subgradient methods for online learning and stochastic optimization",
                    "authors": [
                        "J. Duchi",
                        "E. Hazan",
                        "Y. Singer"
                    ],
                    "venue": "The Journal of Machine Learning Research,",
                    "year": 2011
                },
                {
                    "title": "Compressed sensing: theory and applications",
                    "authors": [
                        "Y.C. Eldar",
                        "G. Kutyniok"
                    ],
                    "year": 2012
                },
                {
                    "title": "An empirical study of learning speed in back-propagation networks",
                    "authors": [
                        "S.E. Fahlman"
                    ],
                    "venue": "Journal of Heuristics,",
                    "year": 1988
                },
                {
                    "title": "Global optimality in tensor factorization, deep learning, and beyond",
                    "authors": [
                        "B.D. Haeffele",
                        "R. Vidal"
                    ],
                    "venue": "arXiv preprint arXiv:1506.07540,",
                    "year": 2015
                },
                {
                    "title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups",
                    "authors": [
                        "G. Hinton",
                        "L. Deng",
                        "D. Yu",
                        "G.E. Dahl",
                        "A.-r. Mohamed",
                        "N. Jaitly",
                        "A. Senior",
                        "V. Vanhoucke",
                        "P. Nguyen",
                        "T.N. Sainath"
                    ],
                    "year": 2012
                },
                {
                    "title": "Suitable mlp network activation functions for breast cancer and thyroid disease detection",
                    "authors": [
                        "I. Isa",
                        "Z. Saad",
                        "S. Omar",
                        "M. Osman",
                        "K. Ahmad",
                        "H.M. Sakim"
                    ],
                    "venue": "In 2010 Second International Conference on Computational Intelligence, Modelling and Simulation,",
                    "year": 2010
                },
                {
                    "title": "Generalization bounds for neural networks through tensor factorization",
                    "authors": [
                        "M. Janzamin",
                        "H. Sedghi",
                        "A. Anandkumar"
                    ],
                    "year": 2015
                },
                {
                    "title": "Learning multiple layers of features from tiny images",
                    "authors": [
                        "A. Krizhevsky",
                        "G. Hinton"
                    ],
                    "venue": "Master Thesis,",
                    "year": 2009
                },
                {
                    "title": "Imagenet classification with deep convolutional neural networks",
                    "authors": [
                        "A. Krizhevsky",
                        "I. Sutskever",
                        "G.E. Hinton"
                    ],
                    "venue": "In Advances in Neural Information Processing Systems,",
                    "year": 2012
                },
                {
                    "title": "Face recognition: A convolutional neural-network approach",
                    "authors": [
                        "S. Lawrence",
                        "C.L. Giles",
                        "A.C. Tsoi",
                        "A.D. Back"
                    ],
                    "venue": "Neural Networks, IEEE Transactions on,",
                    "year": 1997
                },
                {
                    "title": "Fastfood-approximating kernel expansions in loglinear time",
                    "authors": [
                        "Q. Le",
                        "T. Sarl\u00f3s",
                        "A. Smola"
                    ],
                    "venue": "In Proceedings of the International Conference on Machine Learning,",
                    "year": 2013
                },
                {
                    "title": "Gradient-based learning applied to document recognition",
                    "authors": [
                        "Y. LeCun",
                        "L. Bottou",
                        "Y. Bengio",
                        "P. Haffner"
                    ],
                    "venue": "Proceedings of the IEEE,",
                    "year": 1998
                },
                {
                    "title": "On the computational efficiency of training neural networks",
                    "authors": [
                        "R. Livni",
                        "S. Shalev-Shwartz",
                        "O. Shamir"
                    ],
                    "venue": "In Advances in Neural Information Processing Systems,",
                    "year": 2014
                },
                {
                    "title": "Convolutional kernel networks",
                    "authors": [
                        "J. Mairal",
                        "P. Koniusz",
                        "Z. Harchaoui",
                        "C. Schmid"
                    ],
                    "venue": "In Advances in Neural Information Processing Systems,",
                    "year": 2014
                },
                {
                    "title": "On some extensions of Bernstein\u2019s inequality for self-adjoint operators",
                    "authors": [
                        "S. Minsker"
                    ],
                    "venue": "arXiv preprint arXiv:1112.5448,",
                    "year": 2011
                },
                {
                    "title": "Human-level control through deep reinforcement learning",
                    "authors": [
                        "V. Mnih",
                        "K. Kavukcuoglu",
                        "D. Silver",
                        "A.A. Rusu",
                        "J. Veness",
                        "M.G. Bellemare",
                        "A. Graves",
                        "M. Riedmiller",
                        "A.K. Fidjeland",
                        "G. Ostrovski"
                    ],
                    "venue": "Nature, 518(7540):529\u2013533,",
                    "year": 2015
                },
                {
                    "title": "Random features for large-scale kernel machines",
                    "authors": [
                        "A. Rahimi",
                        "B. Recht"
                    ],
                    "venue": "In Advances in Neural Information Processing Systems,",
                    "year": 2007
                },
                {
                    "title": "On the quality of the initial basin in overspecified neural networks",
                    "authors": [
                        "I. Safran",
                        "O. Shamir"
                    ],
                    "venue": "arXiv preprint arXiv:1511.04210,",
                    "year": 2015
                },
                {
                    "title": "Provable methods for training neural networks with sparse connectivity",
                    "authors": [
                        "H. Sedghi",
                        "A. Anandkumar"
                    ],
                    "year": 2014
                },
                {
                    "title": "Learning kernel-based halfspaces with the 0-1 loss",
                    "authors": [
                        "S. Shalev-Shwartz",
                        "O. Shamir",
                        "K. Sridharan"
                    ],
                    "venue": "SIAM Journal on Computing,",
                    "year": 2011
                },
                {
                    "title": "Mastering the game of Go with deep neural networks and tree",
                    "authors": [
                        "D. Silver",
                        "A. Huang",
                        "C.J. Maddison",
                        "A. Guez",
                        "L. Sifre",
                        "G. Van Den Driessche",
                        "J. Schrittwieser",
                        "I. Antonoglou",
                        "V. Panneershelvam",
                        "M. Lanctot"
                    ],
                    "venue": "search. Nature,",
                    "year": 2016
                },
                {
                    "title": "Learning invariant representations with local transformations",
                    "authors": [
                        "K. Sohn",
                        "H. Lee"
                    ],
                    "venue": "In Proceedings of the 29th International Conference on Machine Learning",
                    "year": 2012
                },
                {
                    "title": "Neural networks with periodic and monotonic activation functions: a comparative study in classification problems",
                    "authors": [
                        "J.M. Sopena",
                        "E. Romero",
                        "R. Alquezar"
                    ],
                    "venue": "In ICANN",
                    "year": 1999
                },
                {
                    "title": "Dropout: A simple way to prevent neural networks from overfitting",
                    "authors": [
                        "N. Srivastava",
                        "G. Hinton",
                        "A. Krizhevsky",
                        "I. Sutskever",
                        "R. Salakhutdinov"
                    ],
                    "venue": "The Journal of Machine Learning Research,",
                    "year": 1929
                },
                {
                    "title": "Support vector machines",
                    "authors": [
                        "I. Steinwart",
                        "A. Christmann"
                    ],
                    "year": 2008
                },
                {
                    "title": "Deep learning using linear support vector machines",
                    "authors": [
                        "Y. Tang"
                    ],
                    "venue": "arXiv preprint arXiv:1306.0239,",
                    "year": 2013
                },
                {
                    "title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion",
                    "authors": [
                        "P. Vincent",
                        "H. Larochelle",
                        "I. Lajoie",
                        "Y. Bengio",
                        "P.-A. Manzagol"
                    ],
                    "venue": "The Journal of Machine Learning Research,",
                    "year": 2010
                },
                {
                    "title": "End-to-end text recognition with convolutional neural networks",
                    "authors": [
                        "T. Wang",
                        "D.J. Wu",
                        "A. Coates",
                        "A.Y. Ng"
                    ],
                    "venue": "In Pattern Recognition (ICPR),",
                    "year": 2012
                },
                {
                    "title": "A proximal stochastic gradient method with progressive variance reduction",
                    "authors": [
                        "L. Xiao",
                        "T. Zhang"
                    ],
                    "venue": "SIAM Journal on Optimization,",
                    "year": 2014
                },
                {
                    "title": "Learning halfspaces and neural networks with random initialization",
                    "authors": [
                        "Y. Zhang",
                        "J.D. Lee",
                        "M.J. Wainwright",
                        "M.I. Jordan"
                    ],
                    "venue": "arXiv preprint arXiv:1511.07948,",
                    "year": 2015
                },
                {
                    "title": "`1-regularized neural networks are improperly learnable in polynomial time",
                    "authors": [
                        "Y. Zhang",
                        "J.D. Lee",
                        "M.I. Jordan"
                    ],
                    "venue": "In Proceedings on the 33rd International Conference on Machine Learning,",
                    "year": 2016
                }
            ],
            "id": "SP:165fb135ffbaa1ab63bc4e59dc2bbc8f5ea7bfdc",
            "authors": [
                {
                    "name": "Yuchen Zhang",
                    "affiliations": []
                },
                {
                    "name": "Percy Liang",
                    "affiliations": []
                },
                {
                    "name": "Martin J. Wainwright",
                    "affiliations": []
                }
            ],
            "abstractText": "We describe the class of convexified convolutional neural networks (CCNNs), which capture the parameter sharing of convolutional neural networks in a convex manner. By representing the nonlinear convolutional filters as vectors in a reproducing kernel Hilbert space, the CNN parameters can be represented as a low-rank matrix, which can be relaxed to obtain a convex optimization problem. For learning two-layer convolutional neural networks, we prove that the generalization error obtained by a convexified CNN converges to that of the best possible CNN. For learning deeper networks, we train CCNNs in a layer-wise manner. Empirically, CCNNs achieve performance competitive with CNNs trained by backpropagation, SVMs, fully-connected neural networks, stacked denoising auto-encoders, and other baseline methods.",
            "title": "Convexified Convolutional Neural Networks"
        },
        "Y": {
            "blog_id": "convexified-cnns",
            "summary": [
                "In this paper, the authors proposed a method for convexifying convolutional neural networks to train them without backpropagation.",
                "Furthermore, this relaxation to the convex setting allows for theoretical proofs of bounds on the generalization error.",
                "Succinctly, they propose to use RKHS and the kernel trick to lift the data into a high-dimensional space that is expressive enough to capture certain nonlinear activation functions.",
                "Hence, on experiments on MNIST and CIFAR-10, they show that they can outperform smaller CNNs by \u201cconvexifying\u201d them.",
                "They note that their method doesn\u2019t work with max pooling or very deep CNNs with lots of bells and whistles.",
                "This is a thought-provoking paper.",
                "I like how the authors pursued a theoretically interesting question, even though there isn\u2019t much practical use yet for this.",
                "I don\u2019t have personal experience writing theory papers, but I imagine that this is a good(?)",
                "representation of how they often go in ML.",
                "The research is driven by an interesting theoretical question, not a practical application that needs solving/SOTA results."
            ],
            "author_id": "pemami",
            "pdf_url": "https://arxiv.org/pdf/1609.01000",
            "author_full_name": "Patrick Emami",
            "source_website": "https://pemami4911.github.io/index.html",
            "id": 20550713
        }
    },
    "68642594": {
        "X": {
            "sections": [
                {
                    "heading": "1 Introduction",
                    "text": "A machine capable of performing complex tasks without requiring laborious programming would be tremendously useful in almost any human endeavor, from performing menial jobs for us to helping the advancement of basic and applied research. Given the current availability of powerful hardware and large amounts of machine-readable data, as well as the widespread interest in sophisticated machine learning methods, the times should be ripe for the development of intelligent machines.\nStill, since \u201csolving AI\u201d seems too complex a task to be pursued all at once, in the last decades the computational community has preferred to focus on solving relatively narrow empirical problems that are important for specific applications, but do not address the overarching goal of developing general-purpose intelligent machines. In this article, we propose an alternative approach: we first define the general characteristics we think intelligent machines should possess, and then we present a concrete roadmap to develop them in realistic, small steps, that are however incrementally structured in such a way that, jointly, they should lead us close to the ultimate goal of implementing a powerful AI.\nThe article is organized as follows. In Section 2 we specify the two fundamental characteristics that we consider crucial for developing intelligence\u2013at least the sort of intelligence we are interested in\u2013namely communication and learning. Our goal is\nar X\niv :1\n51 1.\n08 13\n0v 2\n[ cs\n.A I]\n2 6\nFe b\nto build a machine that can learn new concepts through communication at a similar rate as a human with similar prior knowledge. That is, if one can easily learn how subtraction works after mastering addition, the intelligent machine, after grasping the concept of addition, should not find it difficult to learn subtraction as well. Since, as we said, achieving the long-term goal of building an intelligent machine equipped with the desired features at once seems too difficult, we need to define intermediate targets that can lead us in the right direction. We specify such targets in terms of simplified but self-contained versions of the final machine we want to develop. At any time during its \u201ceducation\u201d, the target machine should act like a stand-alone intelligent system, albeit one that will be initially very limited in what it can do. The bulk of our proposal (Section 3) thus consists in the plan for an interactive learning environment fostering the incremental development of progressively more intelligent behavior. Section 4 briefly discusses some of the algorithmic capabilities we think a machine should possess in order to profitably exploit the learning environment. Finally, Section 5 situates our proposal in the broader context of past and current attempts to develop intelligent machines. As that review should make clear, our plan encompasses many ideas that have already appeared in different research strands. What we believe to be novel in our approach is the way in which we are combining such ideas into a coherent program."
                },
                {
                    "heading": "2 Desiderata for an intelligent machine",
                    "text": "Rather than attempting to formally characterize intelligence, we propose here a set of desiderata we believe to be crucial for a machine to be able to autonomously make itself helpful to humans in their endeavors. The guiding principles we implicitly considered in formulating the desiderata are to minimize the complexity of the machine, and to maximize interpretability of its behavior by humans."
                },
                {
                    "heading": "2.1 Ability to communicate",
                    "text": "Any practical realization of an intelligent machine will have to communicate with us. It would be senseless to build a machine that is supposed to perform complex operations if there is no way for us to specify the aims of these operations, or to understand the output of the machine. While other communication means could be entertained, natural language is by far the easiest and most powerful communication device we possess, so it is reasonable to require an intelligent machine to be able to communicate through language. Indeed, the intelligent machine we aim for could be seen as a computer that can be programmed through natural language, or as the interface between natural language and a traditional programming language. Importantly, humans have encoded a very large portion of their knowledge into natural language (ranging from mathematics treatises to cooking books), so a system mastering natural language will have access to most of the knowledge humans have assembled over the course of their\nhistory. Communication is, by its very nature, interactive: the possibility to hold a conversation is crucial both to gather new information (asking for explanation, clarification, instructions, feedback, etc.) and to optimize its transmission (compare a good lecture or studying with a group of peers to reading a book alone). Our learning environment will thus emphasize the interactive nature of communication.\nNatural language can also channel, to a certain extent, non-linguistic information, because much of the latter can be conveyed through linguistic means. For example, we can use language to talk about what we perceive with our senses, or to give instructions on how to operate in the world (see Louwerse, 2011, among others, for evidence that language encodes many perceptual aspects of our knowledge). Analogously, in the simulation we discuss below, a Teacher uses natural language to teach the Learner (the intelligent machine being trained) a more limited and explicit language (not unlike a simple programming language) in which the Learner can issue instructions to its environment through the same communication channels it uses to interact with the Teacher. The intelligent machine can later be instructed to browse the Internet by issuing commands in the appropriate code through its usual communication channels, mastering in this way a powerful tool to interact with the world at large. Language can also serve as an interface to perceptual components, and thus update the machine about its physical surroundings. For example, an object recognition system could transform raw pixel data into object labels, allowing the machine to \u201csee\u201d its real-life environment through a controlled-language modality.\nStill, we realize that our focus on the language-mediated side of intelligence may limit the learning machine in the development of skills that we naturally gain by observing the world around us. There seems to be a fundamental difference between the symbolic representations of language and the continuous nature of the world as we perceive it. If this will turn out to be an issue, we can extend the training phase of the machine (its development in a simulated environment such as the one we will sketch below) with tasks that are more perception-oriented. While in the tasks we will describe here the machine will be taught how to use its I/O channels to receive and transmit linguistic symbols, the machine could also be exposed, through the same interface, to simple encodings (bit streams) of continuous input signals, such as images. The machine could thus be trained, first, to understand the basic properties of continuous variables, and then to perform more complex operations in a continuous space, such as identifying shapes in 2D images. Note that including such tasks would not require us to change the design of our learning framework, only to introduce novel scripts.\nOne big advantage of the single-interface approach we are currently pursuing is that the machine only needs to be equipped with bit-based I/O channels, thus being maximally simple in its interface. The machine can learn an unlimited number of new codes enabling it to interface, through the same channels, with all sorts of interlocutors (people, other machines, perceptual data encoded as described above, etc.). By\nequipping the machine with only a minimalistic I/O bit-stream interface, we ensure moreover that no prior knowledge about the challenges the machine will encounter is encoded into the structure of the input and output representations, harming the generality of the strategies the machine will learn (compare the difficulty of processing an image when it\u2019s already encoded into pixels vs. as raw bits).\nFinally, while we propose language as the general interface to the machine, we are agnostic about the nature of the internal representations the machine must posit to deal with the challenges it faces. In particular, we are not making claims about the internal representations of the machine being based on an interpretable \u201clanguage of thought\u201d (Fodor, 1975). In other words, we are not claiming that the machine should carry out its internal reasoning in a linguistic form: only that its input and output are linguistic in nature.\nTo give a few examples of how a communication-based intelligent machine can be useful, consider a machine helping a scientist with research. First of all, the communication-endowed machine does not need to pre-encode a large static database of facts, since it can retrieve the relevant information from the Internet. If the scientist asks a simple question such as: What is the density of gold?, the machine can search the Web to answer: 19.3g/cm3.\nMost questions will however require the machine to put together multiple sources of information. For example, one may ask: What is a good starting point to study reinforcement learning?. The machine might visit multiple Web sites to search for materials and get an idea of their relative popularity. Moreover, interaction can make even a relatively simple query such as the latter more successful. For example, the machine can ask the user if she prefers videos or articles, what is the mathematical background to be assumed, etc.\nHowever, what we are really interested in is a machine that can significantly speed up research progress by being able to address questions such as: What is the most promising direction to cure cancer, and where should I start to meaningfully contribute? This question may be answered after the machine reads a significant number of research articles online, while keeping in mind the perspective of the person asking the question. Interaction will again play a central role, as the best course of action for the intelligent machine might involve entering a conversation with the requester, to understand her motivation, skills, the time she is willing to spend on the topic, etc. Going further, in order to fulfill the request above, the machine might even conduct some independent research by exploiting information available online, possibly consult with experts, and direct the budding researcher, through multiple interactive sessions, towards accomplishing her goal."
                },
                {
                    "heading": "2.2 Ability to learn",
                    "text": "Arguably, the main flaw of \u201cgood old\u201d symbolic AI research (Haugeland, 1985) lied in the assumption that it would be possible to program an intelligent machine largely\nby hand. We believe it is uncontroversial that a machine supposed to be helping us in a variety of scenarios, many unforeseen by its developers, should be endowed with the capability of learning. A machine that does not learn cannot adapt or modify itself based on experience, as it will react in the same way to a given situation for its whole lifetime. However, if the machine makes a mistake that we want to correct, it is necessary for it to change its behavior\u2013thus, learning is a mandatory component.\nTogether with learning comes motivation. Learning allows the machine to adapt itself to the external environment, helping it to produce outputs that maximize the function defined by its motivation. Since we want to develop machines that make themselves useful to humans, the motivation component should be directly controlled by users through the communication channel. By specifying positive and negative rewards, one may shape the behavior of the machine so that it can become useful for concrete tasks (this is very much in the spirit of reinforcement learning, see, e.g., Sutton and Barto, 1998, and discussion in Section 5 below).\nNote that we will often refer to human learning as a source of insight and an ideal benchmark to strive for. This is natural, since we would like our machines to develop human-like intelligence. At the same time, children obviously grow in a very different environment from the one in which we tutor our machines, they soon develop a sophisticated sensorimotor system to interact with the world, and they are innately endowed with many other cognitive capabilities. An intelligent machine, on the other hand, has no senses, and it will start its life as a tabula rasa, so that it will have to catch up not only on human ontogeny, but also on their phylogeny (the history of AI indicates that letting a machine learn from data is a more effective strategy than manually pre-encoding \u201cinnate\u201d knowledge into it). On the positive side, the machine is not subject to the same biological constraints of children, and we can, for example, expose it to explicit tutoring at a rate that would not be tolerable for children. Thus, while human learning can provide useful inspiration, we are by no means trying to let our machines develop in human-like ways, and we claim no psychological plausibility for the methods we propose."
                },
                {
                    "heading": "3 A simulated ecosystem to educate communicat-",
                    "text": "ion-based intelligent machines\nIn this section, we describe a simulated environment designed to teach the basics of linguistic interaction to an intelligent machine, and how to use it to learn to operate in the world. The simulated ecosystem should be seen as a \u201ckindergarten\u201d providing basic education to intelligent machines. The machines are trained in this controlled environment to later be connected to the real world in order to learn how to help humans with their various needs.\nThe ecosystem I/O channels are controlled by an automatic mechanism, avoiding the complications that would arise from letting the machine interact with the \u201creal\nworld\u201d from the very beginning, and allowing us to focus on challenges that should directly probe the effectiveness of new machine learning techniques.\nThe environment must be challenging enough to force the machine to develop sophisticated learning strategies (essentially, it should need to \u201clearn how to learn\u201d). At the same time, complexity should be manageable, i.e., a human put into a similar environment should not find it unreasonably difficult to learn to communicate and act within it, even if the communication takes place in a language the human is not yet familiar with. After mastering the basic language and concepts of the simulated environment, the machine should be able to interact with and learn from human teachers. This puts several restrictions on the kind of learning the machine must come to be able to perform: most importantly, it will need to be capable to extract the correct generalizations from just a few examples, at a rate comparable to human learners.\nOur ecosystem idea goes against received wisdom from the last decades of AI research. This received wisdom suggests that systems should be immediately exposed to real-world problems, so that they don\u2019t get stuck into artificial \u201cblocks worlds\u201d (Winograd, 1971), whose experimenter-designed properties might differ markedly from those characterizing realistic setups. Our strategy is based on the observation, that we will discuss in Section 4, that current machine learning techniques cannot handle the sort of genuinely incremental learning of algorithms that is necessary for the development of intelligent machines, because they lack the ability to store learned skills in long-term memory and compose them. To bring about an advance in such techniques, we have of course many choices. It seems sensible to pick the simplest one. The environment we propose is sufficient to demonstrate the deficiencies of current techniques, yet it is simple enough that we can fully control the structure and nature of the tasks we propose to the machines, make sure they have a solution, and use them to encourage the development of novel techniques. Suppose we were instead to work in a more natural environment from the very beginning, for example from video input. This would impose large infrastructure requirements on the developers, it would make data pre-processing a big challenge in itself, and training even the simplest models would be very time-consuming. Moreover, it would be much more difficult to formulate interrelated tasks in a controlled way, and define the success criterion. Once we have used our ecosystem to develop a system capable of learning compositional skills from extremely sparse reward, it should be simple to plug in more natural signals, e.g., through communication with real humans and Internet access, so that the system would learn how to accomplish the tasks that people really want it to perform.\nThe fundamental difference between our approach and classic AI blocks worlds is that we do not intend to use our ecosystem to script an exhaustive set of functionalities, but to teach the machine the fundamental ability to learn how to efficiently learn by creatively combining already acquired skills. Once such machine gets connected with the real world, it should quickly learn to perform any new task its Teacher will choose. Our environment can be seen as analogous to explicit schooling. Pupils are taught\nmath in primary school through rather artificial problems. However, once they have interiorized basic math skills in this setup, they can quickly adapt them to the problems they encounter in their real life, and rely on them to rapidly acquire more sophisticated mathematical techniques."
                },
                {
                    "heading": "3.1 High-level description of the ecosystem",
                    "text": "Agents To develop an artificial system that is able to incrementally acquire new skills through linguistic interaction, we should not look at the training data as a static set of labeled examples, as in common machine learning setups. We propose instead a dynamic ecosystem akin to that of a computer game. The Learner (the system to be trained) is an actor in this ecosystem.\nThe second fundamental agent in the ecosystem is the Teacher. The Teacher assigns tasks and rewards the Learner for desirable behaviour, and it also provides helpful information, both spontaneously and in response to Learner\u2019s requests. The Teacher\u2019s behaviour is entirely scripted by the experimenters. Again, this might be worryingly reminiscent of entirely hand-coded good-old AIs. However, the Teacher need not be a very sophisticated program. In particular, for each task it presents to the learner, it will store a small set of expected responses, and only reward the Learner if its behaviour exactly matches one response. Similarly, when responding to Learner\u2019s requests, the Teacher is limited to a fixed list of expressions it knows how to respond to. The reason why this suffices is that the aim of our ecosystem is to kickstart the Learner\u2019s efficient learning capabilities, and not to provide enough direct knowledge for it to be self-sufficient in the world. For example, given the limitations of the scripted Teacher, the Learner will only be able to acquire a very impoverished version of natural language in the ecosystem. At the same time, the Learner should acquire powerful learning and generalization strategies. Using the minimal linguistic skills and strong learning abilities it acquired, the Learner should then be able to extend its knowledge of language fast, once it is put in touch with actual human users.\nLike in classic text-based adventure games (Wikipedia, 2015b), the Environment is entirely linguistically defined, and it is explored by the Learner by giving orders, asking questions and receiving feedback (although graphics does not play an active role in our simulation, it is straightforward to visualize the 2D world in order to better track the Learner\u2019s behaviour, as we show through some examples below). The Environment is best seen as the third fundamental agent in the ecosystem. The Environment behaviour is also scripted. However, since interacting with the Environment serves the purpose of observation and navigation of the Learner surroundings (\u201csensorimotor experience\u201d), the Environment uses a controlled language that, compared to that of the Teacher, is more restricted, more explicit and less ambiguous. One can thus think of the Learner as a higher-level programming language, that accepts instructions from the programmer (the Teacher) in a simple form of natural language, and converts them into the machine code understood by the Environment.\nIn the examples to follow, we assume the world defined by the Environment to be split into discrete cells that the Learner can traverse horizontally and vertically. The world includes barriers, such as walls and water, and a number of objects the Learner can interact with (a pear, a mug, etc).\nNote that, while we do not explore this possibility here, it might be useful to add other actors to the simulation: for example, training multiple Learners in parallel, encouraging them to teach/communicate with each other, while also interacting with the scripted Teacher.\nInterface channels The Learner experience is entirely defined by generic input and output channels. The Teacher, the Environment and any other language-endowed agent write to the input stream. Reward (a scalar value, as discussed next) is also written to the input stream (we assume, however, that the Learner does not need to discover which bits encode reward, as it will need this information to update its objective function). Ambiguities are avoided by prefixing a unique string to the messages produced by each actor (e.g., messages from the Teacher might be prefixed by the string T:, as in our examples below). The Learner writes to its output channel, and it is similarly taught to use unambiguous prefixes to address the Teacher, the Environment and any other agent or service it needs to communicate with. Having only generic input and output communication channels should facilitate the seamless addition of new interactive entities, as long as the Learner is able to learn the language they communicate in.\nReward Reward can be positive or negative (1/-1), the latter to be used to speed up instruction by steering away the Learner from dead ends, or even damaging behaviours. The Teacher, and later human users, control reward in order to train the Learner. We might also let the Environment provide feedback through hard-coded rewards, simulating natural events such as eating or getting hurt. Like in realistic biological scenarios, reward is sparse, mostly being awarded after the Learner has accomplished some task. As intelligence grows, we expect the reward to become very sparse, with the Learner able to elaborate complex plans that are only rewarded on successful completion, and even displaying some degree of self-motivation. Indeed, the Learner should be taught that short-term positive reward might lead to loss at a later stage (e.g., hoarding on food with poor nutrition value instead of seeking further away for better food), and that sometimes reward can be maximized by engaging in activities that in the short term provide no benefit (learning to read might be boring and time-consuming, but it can enormously speed up problem solving\u2013and the consequent reward accrual\u2013 by making the Learner autonomous in seeking useful information on the Internet). Going even further, during the Learner \u201cadulthood\u201d explicit external reward could stop completely. The Learner will no longer be directly motivated to learn in new ways, but ideally the policies it has already acquired will include strategies such as curiosity (see below) that would lead it to continue to acquire new skills\nfor its own sake. Note that, when we say that reward could stop completely, we mean that users do not need to provide explicit reward, in the form of a scalar value, to the Learner. However, from a human perspective, we can look at this as the stage in which the Learner has interiorized its own sources of reward, and no longer needs external stimuli.\nWe assume binary reward so that human users need not worry about relative amounts of reward to give to the Learner (if they do want to control the amount of reward, they can simply reward the Learner multiple times). The Learner objective should however maximize average reward over time, naturally leading to different degrees of cumulative reward for different courses of action (this is analogous to the notion of expected cumulative reward in reinforcement learning, which is a possible way to formalize the concept). Even if two solutions to a task are rewarded equally on its completion, the faster strategy will be favored, as it leaves the Learner more time to accumulate further reward. This automatically ensures that efficient solutions are preferred over wasteful ones. Moreover, by measuring time independently from the number of simulation steps, e.g., using simple wall-clock time, one should penalize inefficient learners spending a long time performing offline computations.\nAs already mentioned, our approach to reward-based learning shares many properties with reinforcement learning. Indeed, our setup fits into the general formulation of the reinforcement learning problem (Kaelbling et al., 1996; Sutton and Barto, 1998)\u2013 see Section 5 for further discussion of this point.\nIncremental structure In keeping with the game idea, it is useful to think of the Learner as progressing through a series of levels, where skills from earlier levels are required to succeed in later ones. Within a level, there is no need to impose a strict ordering of tasks (even when our intuition suggests a natural incremental progression across them), and we might let the Learner discover its own optimal learning path by cycling multiple times through blocks of them.\nAt the beginning, the Teacher trains the Learner to perform very simple tasks in order to kick-start linguistic communication and the discovery of very simple algorithms. The Teacher first rewards the Learner when the latter repeats single characters, then words, delimiters and other control strings. The Learner is moreover taught how to repeat and manipulate longer sequences. In a subsequent block of tasks, the Teacher leads the Learner to develop a semantics for linguistic symbols, by encouraging it to associate linguistic expressions with actions. This is achieved through practice sessions in which the Learner is trained to repeat strings that function as Environment commands, and it is rewarded only when it takes notice of the effect the commands have on its state (we present concrete examples below). At this stage, the Learner should become able to associate linguistic strings to primitive moves and actions (turn left). Next, the Teacher will assign tasks involving action sequences (find an apple), and the Learner should convert them into sets of primitive commands (simple \u201cprograms\u201d). The Teacher will, increasingly, limit itself to specify an abstract end goal (bring back\nfood), but not recipes to accomplish it, in order to spur creative thinking on behalf of the Learner (e.g., if the Learner gets trapped somewhere while looking for food, it may develop a strategy to go around obstacles). In the process of learning to parse and execute higher-level commands, the Learner should also be trained to ask clarification questions to the Teacher (e.g., by initially granting reward when it spontaneously addresses the Teacher, and by the repetition-based strategy we illustrate in the examples below). With the orders becoming more general and complex, the language of the Teacher will also become (within the limits of what can be reasonably scripted) richer and more ambiguous, challenging the Learner capability to handle restricted specimens of common natural language phenomena such as polysemy, vagueness, anaphora and quantification.\nTo support user scenarios such as the ones we envisaged in Section 2 above and those we will discuss at the end of this section, the Teacher should eventually teach the Learner how to \u201cread\u201d natural text, so that the Learner, given access to the Internet, can autonomously seek for information online. Incidentally, notice that once the machine can read text, it can also exploit distributional learning from large amounts of text (Erk, 2012; Mikolov et al., 2013; Turney and Pantel, 2010) to induce word and phrase representations addressing some of the challenging natural language phenomena we just mentioned, such as polysemy and vagueness.\nThe Learner must take its baby steps first, in which it is carefully trained to accomplish simple tasks such as learning to compose basic commands. However, for the Learner to have any hope to develop into a fully-functional intelligent machine, we need to aim for a \u201csnow-balling\u201d effect to soon take place, such that later tasks, despite being inherently more complex, will require a lot less explicit coaching, thanks to a combinatorial explosion in the background abilities the Leaner can creatively compose (like for humans, learning how to surf the Web should take less time than learning how to spell).\nTime off Throughout the simulation, we foresee phases in which the Learner is free to interact with the Environment and the Teacher without a defined task. Systems should learn to exploit this time off for undirected exploration, that should in turn lead to better performance in active training stages, just like, in the dead phases of a video-game, a player is more likely to try out her options than to just sit waiting for something to happen, or when arriving in a new city we\u2019d rather go sightseeing than staying in the hotel. Since curiosity is beneficial in many situations, such behaviour should naturally lead to higher later rewards, and thus be learnable. Time off can also be used to \u201cthink\u201d or \u201ctake a nap\u201d, in which the Learner can replay recent experiences and possibly update its inner structure based on a more global view of the knowledge it has accumulated, given the extra computational resources that the free time policy offers.\nEvaluation Learners can be quantitatively evaluated and compared in terms of the number of new tasks they accomplish successfully in a fixed amount of time, a measure in line with the reward-maximization-over-time objective we are proposing. Since the interactive, multi-task environment setup does not naturally support a distinction between a training and a test phase, the machine must carefully choose rewardmaximizing actions from the very beginning. In contrast, evaluating the machine only on its final behavior would overlook the number of attempts it took to reach the solution. Such alternative evaluation would favor models which are simply able to memorize patterns observed in large amounts of training data. In many practical domains, this approach is fine, but we are interested in machines capable of learning truly general problem-solving strategies. As the tasks become incrementally more difficult, the amount of required computational resources for naive memorization-based approaches scales exponentially, so only a machine that can efficiently generalize can succeed in our environment. We will discuss the limitations of machines that rely on memorization instead of algorithmic learning further in Section 4.3 below.\nWe would like to foster the development of intelligent machines by employing our ecosystem in a public competition. Given what we just said, the competition would not involve distributing a static set of training/development data similar in nature to the final test set. We foresee instead a setup in which developers have access to the full pre-programmed environment for a fixed amount of time. The Learners are then evaluated on a set of new tasks that are considerably different from the ones exposed in the development phase. Examples of how test tasks might differ from those encountered during development include the Teacher speaking a new language, a different Environment topography, new obstacles and objects with new affordances, and novel domains of endeavor (e.g., test tasks might require selling and buying things, when the Learner was not previously introduced to the rules of commerce)."
                },
                {
                    "heading": "3.2 Early stages of the simulation",
                    "text": "Preliminaries At the very beginning, the Learner has to learn to pay attention to the Teacher, to identify the basic units of language (find regularity in bit patterns, learn characters, then words and so on). It must moreover acquire basic sequence repetition and manipulation skills, and develop skills to form memory and learn efficiently. These very initial stages of learning are extremely important, as we believe they constitute the building blocks of intelligence.\nHowever, as bit sequences do not make for easy readability, we focus here on an immediately following phase, in which the Learner has already learned how to pay attention to the Teacher and manipulate character strings. We show how the Teacher guides the Learner from these basic skills to being able to solve relatively sophisticated Environment navigation problems by exploiting interactive communication. Because of the \u201cfractal-like\u201d structure we envisage in the acquisition of increasingly higher-level skills, these steps will illustrate many of the same points we could have demonstrated\nthrough the lower-level initial routines. The tasks we describe are also incrementally structured, starting with the Learner learning to issue Environment commands, then being led to take notice of the effect these commands have, then understanding command structure, in order to generalize across categories of actions and objects, leading it in turn to being able to process higher-level orders. At this point, the Learner is initiated to interactive communication.\nNote that we only illustrate here \u201cpolite\u201d turn-taking, in which messages do not overlap, and agents start writing to the communication channels only after the endof-message symbol has been issued. We do not however assume that interaction must be constrained in this way. On the contrary, there are advantages in letting entities write to the communication channels whenever they want: for example, the Teacher might interrupt the Learner to prevent him from completing a command that would have disastrous consequences, or the Learner may interrupt the Teacher as soon as it figured out what to do, in order to speed up reward (a simple priority list can be defined to solve conflicts, e.g., Teacher\u2019s voice is \u201clouder\u201d than that of Environment, etc.).\nNote also that our examples are meant to illustrate specific instances from a larger set of trials following similar templates, that should involve a variety of objects, obstacles and possible actions. Moreover, the presented examples do not aim to exhaustively cover all learning-fostering strategies that might be implemented in the ecosystem. Finally, we stress again that we are not thinking of a strict ordering of tasks (not least because it would be difficult to fix, a priori, an ordering that is based on some objective, learning-model-independent difficulty criterion), but rather about clusters of related tasks organized into levels, such that the Learner could cycle through them and discover its own optimal progression to solve them.\nNotation We present the input to the Learner (messages from Teacher and Environment, as well as Reward, prefixed by T:, E: and R:, respectively) in the left column, and the Learner output on the right (messages addressed to the Teacher are prefixed by @T:, those for the Environment are prefixed by @E:). We assume different I/O identification prefixes to disambiguate meta-linguistic statements (e.g., \u201cT: give order @E: I move.\u201d in the first exchange below, where it\u2019s only the Teacher speaking all the way). The full stop serves as end-of-message delimiter (line breaks are used for formatting purposes, and should be treated as white space; similarly, hyphens are inserted for readability and should be ignored). We use an ellipsis (three dots) to mark a sequence of messages on any channel we do not report (e.g., the Learner exploring a number of solutions before zeroing in on the right one). Finally, we interleave the conversations with comments, shown in grey.\nThe Learner learns to issue Environment commands We start with the Teacher teaching the Learner to produce Environment commands, first by rote repetition channeled to the right message recipient (@E), then by encouraging it to translate from\nthe Teacher\u2019s own natural language to the controlled Environment language. At this stage, reward is not tied to understanding what happened (the \u201csemantics\u201d of the commands). Still, a good Learner might take note of the actions triggered by the commands. The following are example conversations that may happen during the learning process:\nInput: Output: T: give order @E: I move. fdakjf13ktj53u9gs5 5 3#. character soup without addressee, no reward\n. . . @T: dsfjkasjfk;aljsf.\ncharacter soup with wrong addressee, no reward . . . @E: fglk4$3wfgg. right addressee, character soup, no reward\n. . . give order @E: I move.\nE: you moved. R: 1. Learner moves and is rewarded, although the solution is inefficient T: give order @E: I move.\n. . . @E: I move.\nE: you moved. R: 1. Learner figured out minimal string leading to reward\nFurther skills the Learner will need to develop include segmentation of Teacher\u2019s input, and production of separate commands for the Environment. At first, this may be rather trivial, as shown in this example:\nInput: Output: T: give orders @E: I move and @E: I look. @E: I move and @E: I look. no reward, because Environment does not parse complex commands . . . T: give orders @E: I move and @E: I look. @E: I move. E: you moved. @E: I look. E: there is an apple.\nInput: Output: R: 1.\nEventually, the Learner should learn to switch fluently between Teacher and Environment communication codes, translating the Teacher\u2019s linguistic messages into motor commands to the Environment (an example of such exchange is visualized in Figure 1).\nAssociating language to actions Next, the Learner is encouraged to take notice of what happens in the Environment and associate the Teacher\u2019s language to states of the world, by being rewarded only when its orders result in the corresponding actions being effectively carried out in the world. An example task is illustrated in Figure 2, where the Learner will not get reward because it is no longer sufficient for it to correctly translate instructions into Environment commands: the commands must also have the intended effect on the world. In this specific case, order execution fails because the Learner encountered a barrier.\nLearning to generalize By adding sufficient variety to the trials above, the Learner should start to get hold of the compositional nature of commands (turn right and turn left share systematic characteristics, e.g., they both cause a change in what the Learner sees; the Learner can pick X only when X is in the cell in front of him, etc.). The next trials help the Learner further move away from literal command string memorization, and generalize across categories of actions and objects (turn left/right are instances of turning, apples and pears are objects, etc.). This is demonstrated in the following example:\nInput: Output: T: turn and move. @E: I turn. E: (no action)\n. . . @E: I turn left.\nE: you turned left. @E: I move. E: you can\u2019t move. there is a barrier in front of Learner\n. . . @E: I turn right.\nE: you turned right. @E: I move. E: you moved. R: 1.\nIn the next example, the Learner is asked to pick some object that is in front of him, without specifying what the object is (we assume that the simple, unambiguous language of the Environment has no words for general categories such as that of object, so the Learner must eventually find out that it must name specific objects instead):\nInput: Output: T: pick an object. @E: I pick an object. E: (no response) @E: I pick the apple. E: (no response) @E: I pick the pear. E: you picked the pear. R: 1.\nAs we just illustrated, initially the Learner will apply an exhaustive search strategy, listing all objects it knows of to find one it can pick. Later, the Teacher should teach the look command, and the Learner should be able to discover a faster strategy than exhaustive search:\nInput: Output: T: pick an object. @E: I look. E: you see a pear. @E: I pick the pear. E: you picked the pear. R: 1.\nLearning new strategies of course does not imply that the Learner can safely forget the ones it has previously acquired. For example, in some tasks the look command might not work (because, say, it is too dark and the Learner cannot see what is in front of him). In such case, an efficient Learner should find it easy to re-use previously learned skills, such as exhaustive search.\nUnderstanding higher-level orders The next batch of trials aims at developing the ability to decompose higher-level commands into a sequence of basic actions, first fixed (move twice), then flexible (find an apple). The general teaching strategy is to provide sets of paired trials: In the first trial, the Teacher describes the task explicitly as a sequence of primitive actions, and gives the high-level name of the order. In the second trial, the Teacher issues an equivalent high-level order, and the Learner is rewarded if it goes through the same sequence as in the first trial (we are aware that many complex linguistic expressions are not amenable to a straightforward decompositional analysis, see, e.g., Boleda and Erk, 2015: we will use some that are in order to\nteach the Learner about how to decompose complex concepts into step-by-step plans). The Teacher could start by teaching iteration counters:\nInput: Output: T: move and move. @E: I move. E: you moved. @E: I move. E: you moved. R: 1. T: this is called move two times. . . . T: move two times.\n. . . @E: I move.\nE: you moved. @E: I move. E: you moved. R: 1. . . . T: move and move and move. @E: I move. E: you moved. @E: I move. E: you moved. @E: I move. E: you moved. R: 1. T: this is called move three times. . . . T: move three times.\n. . . @E: I move.\nE: you moved. @E: I move. E: you moved. @E: I move. E: you moved. R: 1.\nThe Learner should eventually be able to apply modifiers productively to different actions without much further training (e.g., apply three times to turn left).\nNext, the Teacher can show how to segment high-level tasks, such as finding some-\nthing, into atomic action sequences. For example, to find an apple, one can develop a simple strategy of going forward until an apple is found. The Teacher might initiate the Learner to this new skill as follows:\nInput: Output: T: move and look. @E: I move. E: you moved. @E: I look. E: you see grass. T: move and look. @E: I move. E: you moved. @E: I look. E: you see an apple. R: 1. T: this is called find an apple.\nNote that one such example is not sufficient for the Learner to understand what exactly we call finding an apple, as multiple interpretations are valid: maybe we just wanted it to execute the given commands twice. Thus, there should be multiple training sequences of variable length, to clarify that the task is really to apply an until loop, i.e., to iterate move and look commands until the object the Learner is searching for is found.\nFurther tasks can define composite skills, such as getting an apple, which would consist of first finding an apple, and then picking it up. Another generalization would be to include multiple objects the Learner should be searching for. Adding obstacles to the Environment would further complicate the challenge. The previous strategy of simply going forward until an object is found will not work anymore, as the Learner would stop at the first obstacle. We can either expect the Learner to independently develop better search strategies involving turning, or we can add further examples where the Teacher shows the Learner how to improve through more direct supervision.\nInteractive communication Tasks such as finding an apple without directions might involve a long random search. Thus, we want to kick-start interactive communication, so that the Learner can be efficiently directed by the Teacher (and eventually by humans) to the right cell. In a first set of trials, the Learner is rewarded for repeating a how to request uttered by the Teacher (addressing it back to the teacher via the @T: prefix), and following the precise instructions produced by the Teacher in response to the request:\nInput: Output: T: ask me how to find an apple.\nInput: Output: @E: ask me how to find an apple. wrong addressee, wrong request prefix . . . @T: how to find an apple. T: turn right and move and move and pick the apple. @E: I turn right. E: you turned right. @E: I move. E: you moved. @E: I move. E: you moved. @E: I pick the apple. E: you picked the apple. R: 1.\nTrials such as this one are later interspersed with trials where the Learner is assigned a task it can in principle accomplish by random search, but taking the initiative by issuing a how to request and then following the precise directions provided by the Teacher will considerably speed up reward.\nAlgorithmic knowledge Some tasks illustrated above require understanding basic control flow structures. For example, parsing action modifiers implies a simple form of counting, and in order to find things the Learner must implement an until (equivalently, while not) loop. Similarly, the command get out of the grass calls for a while loop. Efficient completion of more advanced tasks, e.g., return home, implies development of more complex algorithms, such as path-finding. After acquiring a bank of such algorithms (which might be encoded in the internal machine representation by something very different from programming language constructs), the Learner should be able, in advanced stages of the simulation, to productively combine them in order to succeed in full-fledged novel missions that involve accomplishing a large number of hierarchically-structured sub-goals (find somebody who will trade two apples for a banana).\nAs we discussed in Section 3.1, the Learner\u2019s functionality could essentially be interpreted as learning how to compose programs based on the descriptions given in natural language by the Teacher. The programs produce very simple instructions that are understood by the Environment, which can be seen as a sort of CPU. From this point of view, the intelligent system we aim to train is a bridge between the Teacher (later to be replaced by a human operator) and a traditional computer that understands only a limited set of basic commands and needs to be manually programmed for each single task. Thus, we believe that successful construction of intelligent machines\ncould automate computer programming, which will likely be done in the future simply through communication in natural language."
                },
                {
                    "heading": "3.3 Interacting with the trained intelligent machine",
                    "text": "To conclude the illustration of our plan, we provide a motivating example of how an intelligent machine schooled in our ecosystem could later make itself useful in the real world. We consider a scenario in which the machine works as an assistant to Alice, an elderly person living alone. Bob is Alice\u2019s son, and he also interacts with the machine.\nWe assume that, as part of its training, the machine has been taught how to issue Internet commands and process their outcomes. In the example dialogue, we give a general idea of how the machine would interface to the Internet, without attempting to precisely define the syntax of this interaction. Most importantly, the Internet queries in the example are meant to illustrate how the machine does not need to store all the knowledge it needs to accomplish its duties, as it can retrieve useful information from the Web on demand, and reason about it.\nInput: Output: Bob: I just spoke to the doctor, who said my mother needs to move for at least one hour per day, please make sure she does get enough exercise. . . . following conversation takes place the next day:\n@Alice: Are you in the mood for some light physical exercise today?\nAlice: Yes, but no more than 15 minutes, please.\n@INTERNET: [query search engine for keywords elderly, light activity, 15 minutes ]\nshortly afterwards. . . @Alice: I have downloaded a YouTube video with a 15-minute yoga routine, please watch it whenever you can. a few hours later. . . Alice: I\u2019m going out to buy groceries.\n@INTERNET: [query search engine with keywords average walking speed, elderly person]\nInput: Output: @INTERNET: [search maps app for distance to grocery stores in Alice\u2019s neighborhood] @Alice: Hey, why don\u2019t you walk to the Yummy Food Market today? It should take you about 45 minutes to and from, so you\u2019ll get the rest of your daily exercise. @Alice: Thanks for the suggestion.\nThe communication-based intelligent machine should adapt to a whole range of tasks it was not explicitly programmed for. If necessary, the user can give it further explicit positive and negative reward to motivate it to change its behavior. This may be needed only rarely, as the machine should be shipped to the end users after it already mastered good communication abilities, and further development should mostly occur through language. For example, when the user says No, don\u2019t do this again, the machine will understand that repeating the same type of behavior might lead to negative reward, and it will change its course of action even when no explicit reward signal is given (again, another way to put this is that the machine should associate similar linguistic strings to an \u201cinteriorized\u201d negative reward).\nThe range of tasks for intelligent machines can be very diverse: besides the everydaylife assistant we just considered, it could explain students how to accomplish homework assignments, gather statistical information from the Internet to help medical researchers (see also the examples in Section 2.1 above), find bugs in computer programs, or even write programs on its own. Intelligent machines should extend our intellectual abilities in the same way current computers already function as an extension to our memory. This should enable us to perform intellectual tasks beyond what is possible today.\nWe realize the intelligent machines we aim to construct could become powerful tools that may be possibly used for dubious purposes (the same could be said about any advanced technology, including airplanes, space rockets and computers). We believe the perception of AI is skewed by popular science fiction movies. Instead of thinking of computers that take over the world for their own reasons, we think AI will be realized as a tool: A machine that will extend our capability to reason and solve complex problems. Further, given the current state of the technology, we believe any discussion on \u201cfriendliness\u201d of the AI is at this moment premature. We expect it will take years, if not decades to scale basic intelligent machines to become competitive with humans, giving us enough time to discuss any possible existential threats."
                },
                {
                    "heading": "4 Towards the development of intelligent machines",
                    "text": "In this section, we will outline some of our ideas about how to build intelligent machines that would benefit from the learning environment we described. While we do not have a concrete proposal yet about how exactly such machines should be implemented, we will discuss some of the properties and components we think are needed to support the desired functionalities. We have no pretense of completeness, we simply want to provide some food for thought. As in the previous sections, we try to keep the complexity of the machine at the minimum, and only consider the properties that seem essential."
                },
                {
                    "heading": "4.1 Types of learning",
                    "text": "There are many types of behavior that we collectively call learning, and it is useful to discuss some of them first. Suppose our goal is to build an intelligent machine working as a translator between two languages (we take here a simplified word-based view of the translation task). First, we will teach the machine basic communication skills in our simulated environment so that it can react to requests given by the user. Then, we will start teaching it, by example, how various words are translated.\nThere are different kinds of learning happening here. To master basic communication skills, the machine will have to understand the concept of positive and negative reward, and develop complex strategies to deal with novel linguistic inputs. This requires discovery of algorithms, and the ability to remember facts, skills and even learning strategies.\nNext, in order to translate, the machine needs to store pairs of words. The number of pairs is unknown and a flexible growing mechanism may be required. However, once the machine understands how to populate the dictionary with examples, the learning left to do is of a very simple nature: the machine does not have to update its learning strategy, but only to store and organize the incoming information into long-term memory using previously acquired skills. Finally, once the vocabulary memorization process is finished and the machine starts working as a translator, no further learning might be required, and the functionality of the machine can be fixed.\nThe more specialized and narrow the functionality of the machine is, the less learning is required. For very specialized forms of behavior, it should be possible to program the solution manually. However, as we move from roles such as a simple translator of words, a calculator, a chess player, etc., to machines with open-ended goals, we need to rely more on general learning from a limited number of examples.\nOne can see the current state of the art in machine learning as being somewhere in the middle of this hierarchy. Tasks such as automatic speech recognition, classification of objects in images or machine translation are already too hard to be solved purely through manual programming, and the best systems rely on some form of statistical learning, where parameters of hand-coded models are estimated from large datasets of examples. However, the capabilities of state-of-the-art machine learning systems\nare severely limited, and only allow a small degree of adaptability of the machine\u2019s functionality. For example, a speech recognition system will never be able to perform speech translation by simply being instructed to do so\u2013a human programmer is required to implement additional modules manually."
                },
                {
                    "heading": "4.2 Long-term memory and compositional learning skills",
                    "text": "We see a special kind of long-term memory as the key component of the intelligent machine. This long-term memory should be able to store facts and algorithms corresponding to learned skills, making them accessible on demand. In fact, even the ability to learn should be seen as a set of skills that are stored in the memory. When the learning skills are triggered by the current situation, they should compose new persistent structures in the memory from the existing ones. Thus, the machine should have the capacity to extend itself.\nWithout being able to store previously learned facts and skills, the machine could not deal with rather trivial assignments, such as recalling the solution to a task that has been encountered before. Moreover, it is often the case that the solution to a new task is related to that of earlier tasks. Consider for example the following sequence of tasks in our simulated environment:\n\u2022 find and pick an apple;\n\u2022 bring the apple back home;\n\u2022 find two apples;\n\u2022 find one apple and two bananas and bring them home.\nSkills required to solve these tasks include:\n\u2022 the ability to search around the current location;\n\u2022 the ability to pick things;\n\u2022 the ability to remember the location of home and return to it;\n\u2022 the ability to understand what one and two mean;\n\u2022 the ability to combine the previous skills (and more) to deal with different requests.\nThe first four abilities correspond to simple facts or skills to be stored in memory: a sequence of symbols denoting something, the steps needed to perform a certain action, etc. The last ability is an example of a compositional learning skill, with the capability of producing new structures by composing together known facts and skills. Thanks to such learning skills, the machine will be able to combine several existing abilities to\ncreate a new one, often on the fly. In this way, a well-functioning intelligent machine will not need a myriad of training examples whenever it faces a slightly new request, but it could succeed given a single example of the new functionality. For example, when the Teacher asks the Learner to find one apple and two bananas and bring them home, if the Learner already understands all the individual abilities involved, it can retrieve the relevant compositional learning skill to put together a plan and execute it step by step. The Teacher may even call the new skill generated in this way prepare breakfast, and refer to it later as such. Understanding this new concept should not require any further training of the Learner, and the latter should simply store the new skill together with its label in its long-term memory.\nAs we have seen in the previous examples, the Learner can continue extending its knowledge of words, commands and skills in a completely unsupervised way once it manages to acquire skills that allow it to compose structures in its long-term memory. It may be that discovering the basic learning skills, something we usually take for granted, is much more intricate than it seems to us. But once we will be able to build a machine which can effectively construct itself based on the incoming signals \u2013even when no explicit supervision in the form of rewards is given, as discussed above\u2013 we should be much closer to the development of intelligent machines."
                },
                {
                    "heading": "4.3 Computational properties of intelligent machines",
                    "text": "Another aspect of the intelligent machine that deserves discussion is the computational model that the machine will be based on. We are convinced that such model should be unrestricted, that is, able to represent any pattern in the data. Humans can think of and talk about algorithms without obvious limitations (although, to apply them, they might need to rely on external supports, such as paper and pencil). A useful intelligent machine should be able to handle such algorithms as well.\nA more precise formulation of our claim in the context of the theory of computation is that the intelligent machine needs to be based on a Turing-complete computational model. That is, it has to be able to represent any algorithm in fixed length, just like the Turing machine (the very fact that humans can describe Turing-complete systems shows that they are, in practical terms, Turing-complete: it is irrelevant, for our purposes, whether human online processing capabilities are strictly Turing-complete\u2013 what matters is that their reasoning skills, at least when aided by external supports, are). Note that there are many Turing-complete computational systems, and Turing machines in particular are a lot less efficient than some alternatives, e.g., Random Access Machines. Thus, we are not interested in building the intelligent machine around the concept of the Turing machine; we just aim to use a computational model that does not have obvious limitations in ability to represent patterns.\nA system that is weaker than Turing-complete cannot represent certain patterns in the data efficiently, which in turn means it cannot truly learn them in a general sense. However, it is possible to memorize such complex patterns up to some finite level\nof complexity. Thus, even a computationally restricted system may appear to work as intended up to some level of accuracy, given that a sufficient number of training examples is provided.\nFor example, we may consider a sequence repetition problem. The machine is supposed to remember a sequence of symbols and reproduce it later. Further, let\u2019s assume the machine is based on a model with the representational power of finite state machines. Such system is not capable to represent the concept of storing and reproducing a sequence. However, it may appear to do so if we design our experiment imperfectly. Assume there is a significant overlap between what the machine sees as training data, and the test data we use to evaluate performance of the machine. A trivial machine that can function as a look-up table may appear to work, simply by storing and recalling the training examples. With an infinite number of training examples, a look-up-table-based machine would appear to learn any regularity. It will work indistinguishably from a machine that can truly represent the concept of repetition; however, it will need to have infinite size. Clearly, such memorizationbased system will not perform well in our setting, as we aim to test the Learner\u2019s ability to generalize from a few examples.\nSince there are many Turing-complete computational systems, one may wonder which one should be preferred as the basis for machine intelligence. We cannot answer this question yet, however we hypothesize that the most natural choice would be a system that performs computation in a parallel way, using elementary units that can grow in number based on the task at hand. The growing property is necessary to support the long-term memory, if we assume that the basic units themselves are finite. An example of an existing computational system with many of the desired properties is the cellular automaton of Von Neumann et al. (1966). We might also be inspired by string rewriting systems, for example some versions of the L-systems (Prusinkiewicz and Lindenmayer, 2012).\nAn apparent alternative would be to use a non-growing model with immensely large capacity. There is however an important difference. In a growing model, the new cells can be connected to those that spawned them, so that the model is naturally able to develop a meaningful topological structure based on functional connectivity. We conjecture that such structure would in itself contribute to learning in a crucial way. On the other hand, it is not clear if such topological structure can arise in a large-capacity unstructured model. Interestingly, some of the more effective machinelearning models available today, such as recurrent and convolutional neural networks, are characterized by (manually constrained) network topologies that are well-suited to the domains they are applied to."
                },
                {
                    "heading": "5 Related ideas",
                    "text": "We owe, of course, a large debt to the seminal work of Turing (1950). Note that, while Turing\u2019s paper is most often cited for the \u201cimitation game\u201d, there are other very\ninteresting ideas in it, worthy of more attention from curious readers, especially in the last section on learning machines. Turing thought that a good way to construct a machine capable of passing his famous test would be to develop a child machine, and teach it further skills through various communication channels. These would include sparse rewards shaping the behavior of the child machine, and other information-rich channels such as language input from a teacher and sensory information.\nWe share Turing\u2019s goal of developing a child machine capable of independent communication through natural language, and we also stress the importance of sparse rewards. The main distinction between his and our vision is that Turing assumed that the child machine would be largely programmed (he gives an estimate of sixty programmers working on it for fifty years). We rather think of starting with a machine only endowed with very elementary skills, and focus on the capability to learn as the fundamental ability that needs to be developed. This further assumes educating the machine at first in a simulated environment where an artificial teacher will train it, as we outlined in our roadmap. We also diverge with respect to the imitation game, since the purpose of our intelligent machine is not to fool human judges into believing it is actually a real person. Instead, we aim to develop a machine that can perform a similar set of tasks to those a human can do by using a computer, an Internet connection and the ability to communicate.\nThere has been a recent revival of interest in tasks measuring computational intelligence, spurred by the empirical advances of powerful machine-learning architectures such as multi-layered neural networks (LeCun et al., 2015), and by the patent inadequacy of the classic version of Turing test (Wikipedia, 2015c). For example, Levesque et al. (2012) propose to test systems on their ability to resolve coreferential ambiguities (The trophy would not fit in the brown suitcase because it was too big. . . What was too big? ). Geman et al. (2015) propose a \u201cvisual\u201d Turing test in which a computational system is asked to answer a set of increasingly specific questions about objects, attributes and relations in a picture (Is there a person in the blue region? Is the person carrying something? Is the person interacting with any other object? ). Similar initiatives differ from ours in that they focus on a specific set of skills (coreference, image parsing) rather than testing if an agent can learn new skills. Moreover, these are traditional evaluation benchmarks, unlike the hybrid learning/evaluation ecosystem we are proposing.\nThe idea of developing an AI living in a controlled synthetic environment and interacting with other agents through natural language is quite old. The Blocks World of Winograd (1971) is probably the most important example of early research in this vein. The approach was later abandoned, when it became clear that the agents developed within this framework did not scale up to real-world challenges (see, e.g., Morelli et al., 1992). The knowledge encoded in the systems tested by these early simulations was manually programmed by their creators, since they had very limited learning capabilities. Consequently, scaling up to the real world implied manual coding of all the knowledge necessary to cope with it, and this proved infeasible. Our simulation\nis instead aiming at systems that encode very little prior knowledge and have strong capabilities to learn from data. Importantly, our plan is not to try to manually program all possible scripts our system might encounter later, as in some of the classic AI systems. We plan to program only the initial environment, in order to kickstart the machine\u2019s ability to learn and adapt to different problems and scenarios. After the simulated environment is mastered, scaling up the functionality of our Learner will not require further manual work on scripting new situations, but will rather focus on integrating real world inputs, such as those coming from human users. The toy world itself is already designed to feature novel tasks of increasing complexity, explicitly testing the abilities of systems to autonomously scale up.\nStill, we should not underestimate the drawbacks of synthetic simulations. The tasks in our environment might directly address some challenging points in the development of AI, such as learning with very weak supervision, being able to form a structured long-term memory, and the ability of the child machine to grow in size and complexity when encountering new problems. However, simulating the real world can only bring us so far, and we might end up overestimating the importance of some arbitrary phenomena at the expense of others, that might turn out to be more common in natural settings. It may be important to bring reality into the picture relatively soon. Our toy world should let the intelligent machine develop to the point at which it is able to learn from and cooperate with actual humans. Interaction with real-life humans will then naturally lead the machine to deal with real-world problems. The issue of when exactly a machine trained in our controlled synthetic environment is ready to go out in the human world is open, and it should be explored empirically. However, at the same time, we believe that having the machine interact with humans before it can deal with basic problems in the controlled environment would be pointless, and possibly even strongly misleading.\nOur intelligent machine shares some of its desired functionalities with the current generation of automated personal assistants such as Apple\u2019s Siri ad Microsoft\u2019s Cortana. However, these are heavily engineered systems that aim to provide a natural language interface for human users to perform a varied but fixed set of tasks (similar considerations also apply to artificial human companions and digital pets such as Tamagotchi, see Wikipedia, 2015a). Such systems can be developed by defining the most frequent use cases, choosing those that can be solved with the current technology (e.g., book an air ticket, look at the weather forecast and set the alarm clock for tomorrow\u2019s morning), and implementing specific solutions for each such use case. Our intelligent machine is not intended to handle just a fixed set of tasks. As exemplified by the example in Section 3.3, the machine should be capable to learn efficiently how to perform tasks such as those currently handled by personal assistants, and more, just from interaction with the human user (without a programmer or machine learning expert in the loop).\nArchitectures for software agents, and more specifically intelligent agents, are widely studied in AI and related fields (Nwana, 1996; Russell and Norvig, 2009). We\ncannot review this ample literature here, in order to position our proposal precisely with respect to it. We simply remark that we are not aware of other architectures that are as centered on learning and communication as ours. Interaction plays a central role in the study of multiagent systems (Shoham and Leyton-Brown, 2009). However, the emphasis in this research tradition is on how conflict resolution and distributed problem solving evolve in typically large groups of simple, mostly scripted agents. For example, traffic modeling is a classic application scenario for multiagent systems. This is very different from our emphasis on linguistic interaction for the purposes of training a single agent that should become independently capable of very complex behaviours.\nTenenbaum (2015), like us, emphasizes the need to focus on basic abilities that form the core of intelligence. However, he takes naive physics problems as the starting point, and discusses specific classes of probabilistic models, rather than proposing a general learning scenario. There are also some similarities between our proposal and the research program of Luc Steels (e.g., Steels, 2003, 2005), who lets robots evolve vocabularies and grammatical constructions through interaction in a situated environment. However, on the one hand his agents are actual robots subject to the practical hardware limitations imposed by the need to navigate a complex natural environment from the start; on the other, the focus of the simulations is narrowly on language acquisition, with no further aim to develop broadly intelligent agents.\nWe have several points of contact with the semantic parsing literature, such as navigation tasks in an artificial world (MacMahon et al., 2006) and reward-based learning from natural language instructions (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013). The agents developed in this area can perform tasks, such as learning to execute instructions in natural environments by interacting with humans (Thomason et al., 2015), or improving performance on real-life video-games by consulting the instruction manual (Branavan et al., 2012), that we would want our intelligent machines to also be able to carry out. However, current semantic-parsing-based systems achieve these impressive feats by exploiting architectures tuned to the specific tasks at hand, and they rely on a fair amount of hard-wired expert knowledge, in particular about language structures (although recent work is moving towards a more knowledge-lean direction, see for example Narasimhan et al., 2015, who train a neural network to play text-based adventure games using only text descriptions as input and game reward as signal). Our framework is meant to encourage the development of systems that should eventually be able to perform similar tasks, but getting there incrementally, starting with almost no prior knowledge and first learning from their environment a set of simpler skills, and how to creatively merge them to tackle more ambitious goals.\nThe last twenty years have witnessed several related proposals on learning to learn (Thrun and Pratt, 1997), lifelong learning (Silver et al., 2013) and continual learning (Ring, 1997). Much of this work is theoretical in nature and focuses on algorithms rather than on empirical challenges for the proposed models. Still, the general ideas being pursued are in line with our program. Ring (1997), in particular, defines a continual-learning agent whose experiences \u201coccur sequentially, and what it learns at\none time step while solving one task, it can use later, perhaps to solve a completely different task.\u201d Ring\u2019s desiderata for the continual learner are remarkably in line with ours. It is \u201can autonomous agent. It senses, takes actions, and responds to the rewards in its environment. It learns behaviors and skills while solving its tasks. It learns incrementally. There is no fixed training set; learning occurs at every time step; and the skills the agent learns now can be used later. It learns hierarchically. Skills it learns now can be built upon and modified later. It is a black box. The internals of the agent need not be understood or manipulated. All of the agent\u2019s behaviors are developed through training, not through direct manipulation. Its only interface to the world is through its senses, actions, and rewards. It has no ultimate, final task. What the agent learns now may or may not be useful later, depending on what tasks come next.\u201d Our program is definitely in the same spirit, with an extra emphasis on interaction.\nMitchell et al. (2015) discuss NELL, the most fully realized concrete implementation of a lifelong learning architecture. NELL is an agent that has been \u201creading the Web\u201d for several years to extract a large knowledge base. Emphasis is on the neverending nature of the involved tasks, on their incremental refinement based on what NELL has learned, and on sharing information across tasks. In this latter respect, this project is close to multi-task learning (Ando and Zhang, 2005; Caruana, 1997; Collobert et al., 2011), that focuses on the idea of parameter sharing across tasks. It is likely that a successful learner in our framework will exploit similar strategies, but our current focus lies on defining the tasks, rather than on how to pursue them.\nBengio et al. (2009) propose the related idea of curriculum learning, whereby training data for a single task are ordered according to a difficulty criterion, in the hope that this will lead to better learning. This is motivated by the observation that humans learn incrementally when developing complex skills, an idea that has also previously been studied in the context of recurrent neural network training by Elman (1993). The principle of incremental learning is also central to our proposal. However, the fundamental aspect for us is not a strict ordering of the training data for a specific task, but incrementality in the skills that the intelligent machine should develop. This sort of incrementality should in turn be boosted by designing separate tasks with a compositional structure, such that the skills acquired from the simpler tasks will help to solve the more advanced ones more efficiently.\nThe idea of incremental learning, motivated by the same considerations as in the papers we just mentioned, also appears in Solomonoff (2002), a work which has much earlier roots in research on program induction (Solomonoff, 1964, 1997; Schmidhuber, 2004). Within this tradition, Schmidhuber (2015) reviews a large literature and presents some general ideas on learning that might inspire our search for novel algorithms. Genetic programming (Poli et al., 2008) also focuses on the reuse of previously found sub-solutions, speeding up the search procedure in this way. Our proposal is also related to that of Bottou (2014), in its vision of compositional machine learning, although he only considers composition in limited domains, such as sentence and\nimage processing. We share many ideas with the reinforcement learning framework (Sutton and Barto, 1998). In reinforcement learning, the agent chooses actions in an environment in order to maximize some cumulative reward over time. Reinforcement learning is particularly popular for problems where the agent can collect information only by interacting with the environment. Given how broad this definition is, our framework could be considered as a particular instance of it. Our proposal is however markedly different from standard reinforcement learning work (Kaelbling et al., 1996) in several respects. Specifically, we emphasize language-mediated, interactive communication, we focus on incremental strategies that encourage agents to solve tasks by reusing previously learned knowledge and we aim to limit the number of trials an agent gets in order to accomplish a certain goal.\nMnih et al. (2015) recently presented a single neural network architecture capable of learning a set of classic Atari games using only pixels and game scores as input (see also the related idea of \u201cgeneral game playing\u201d, e.g., Genesereth et al., 2005). We pursue a similar goal of learning from a low-level input stream and reward. However, unlike these authors, we do not aim for a single architecture that can, disjointly, learn an array of separate tasks, but for one that can incrementally build on skills learned on previous tasks to perform more complex ones. Moreover, together with reward, we emphasize linguistic interaction as a fundamental mean to foster skill extension. Sukhbaatar et al. (2015) introduce a sandbox to design games with the explicit purpose to train computational agents in planning and reasoning tasks. Moreover, they stress a curriculum strategy to foster learning (making the agent progress through increasingly more difficult versions of the game). Their general program is aligned with ours, and the sandbox might be useful to develop our environment. However, they do not share our emphasis on communication and interaction, and their approach to incremental learning is based on increasingly more difficult versions of the same task (e.g., increasing the number of obstacles), rather than on defining progressively more complex tasks, such that solving the later ones requires composing solutions to earlier ones, as we are proposing. Furthermore, the tasks currently considered within the sandbox do not seem to be challenging enough to require new learning approaches, and may be solvable with current techniques or minor modifications thereof.\nMikolov (2013) originally discussed a preliminary version of the incremental taskbased approach we are more fully outlying here. In a similar spirit, Weston et al. (2015) present a set of question answering tasks based on synthetically generated stories. They also want to foster non-incremental progress in AI, but their approach differs from ours in several crucial aspects. Again, there is no notion of interactive, language-mediated learning, a classic train/test split is enforced, and the tasks are not designed to encourage compositional skill learning (although Weston and colleagues do emphasize that the same system should be used for all tasks). Finally, the evaluation metric is notably different from ours - while we aim to minimize the number of trials it takes for the machine to master the tasks, their goal is to have a good performance\non held out data. This could be a serious drawback for works that involve artificial tasks, as in our view the goal should be to develop a machine that can learn as fast as possible, to have any hope to scale up and be able to generalize in more complex scenarios.\nOne could think of solving sequence-manipulation problems such as those constituting the basis of our learning routine with relatively small extensions of established machine learning techniques (Graves et al., 2014; Grefenstette et al., 2015; Joulin and Mikolov, 2015). As discussed in the previous section, for simple tasks that involve only a small, finite number of configurations, one could be apparently successful even just by using a look-up table storing all possible combinations of inputs and outputs. The above mentioned works, that aim to learn algorithms from data, also add a long-term memory (e.g., a set of stacks), but they use it to store the data only, not the learned algorithms. Thus, such approaches fail to generalize in environments where solutions to new tasks are composed of already learned algorithms.\nSimilar criticism holds for approaches that try to learn certain algorithms by using an architecture with a strong prior towards their discovery, but not general enough to represent even small modifications. To give an example from our own work: a recurrent neural network augmented with a stack structure can form a simple kind of long-term memory and learn to memorize and repeat sequences in the reversed order, but not in the original one (Joulin and Mikolov, 2015). We expect a valid solution to the algorithmic learning challenge to utilize a small number of training examples, and to learn tasks that are closely related at an increasing speed, i.e., to require less and less examples to master new skills that are related to what is already known. We are not aware of any current technique addressing these issues, which were the very reason why algorithmic tasks were originally proposed by Mikolov (2013). We hope that this paper will motivate the design of the genuinely novel methods we need in order to develop intelligent machines."
                },
                {
                    "heading": "6 Conclusion",
                    "text": "We defined basic desiderata for an intelligent machine, stressing learning and communication as its fundamental abilities. Contrary to common practice in current machine learning, where the focus is on modeling single skills in isolation, we believe that all aspects of intelligence should be holistically addressed within a single system.\nWe proposed a simulated environment that requires the intelligent machine to acquire new facts and skills through communication. In this environment, the machine must learn to perform increasingly more ambitious tasks, being naturally induced to develop complex linguistic and reasoning abilities.\nWe also presented some conjectures on the properties of the computational system that the intelligent machine may be based on. These include learning of algorithmic patterns from a few examples without strong supervision, and development of a longterm memory to store both data and learned skills. We tried to put this in contrast with\ncurrently accepted paradigms in machine learning, to show that current methods are far from adequate, and we must strive to develop non-incrementally novel techniques.\nThis roadmap constitutes only the beginning of a long journey towards AI, and we hope other researchers will be joining it in pursuing the goals it outlined."
                },
                {
                    "heading": "Acknowledgments",
                    "text": "We thank Le\u0301on Bottou, Yann LeCun, Gabriel Synnaeve, Arthur Szlam, Nicolas Usunier, Laurens van der Maaten, Wojciech Zaremba and others from the Facebook AI Research team, as well as Gemma Boleda, Katrin Erk, Germa\u0301n Kruszewski, Angeliki Lazaridou, Louise McNally, Hinrich Schu\u0308tze and Roberto Zamparelli for many stimulating discussions. An early version of this proposal has been discussed in several research groups since 2013 under the name Incremental learning of algorithms (Mikolov, 2013)."
                }
            ],
            "year": 2016,
            "references": [
                {
                    "title": "A framework for learning predictive structures from multiple tasks and unlabeled data",
                    "authors": [
                        "R. Ando",
                        "T. Zhang"
                    ],
                    "venue": "Journal of Machine Learning Research, 5:1817\u2013 1853.",
                    "year": 2005
                },
                {
                    "title": "Weakly supervised learning of semantic parsers for mapping instructions to actions",
                    "authors": [
                        "Y. Artzi",
                        "L. Zettlemoyer"
                    ],
                    "venue": "Transactions of the Association for Computational Linguistics, 1(1):49\u201362.",
                    "year": 2013
                },
                {
                    "title": "Curriculum learning",
                    "authors": [
                        "Y. Bengio",
                        "J. Louradour",
                        "R. Collobert",
                        "J. Weston"
                    ],
                    "venue": "Proceedings of ICML, pages 41\u201348, Montreal, Canada.",
                    "year": 2009
                },
                {
                    "title": "Distributional semantic features as semantic primitives\u2013or not",
                    "authors": [
                        "G. Boleda",
                        "K. Erk"
                    ],
                    "venue": "Proceedings of the AAAI Spring Symposium on Knowledge Representation and Reasoning: Integrating Symbolic and Neural Approaches, pages 2\u20135, Stanford, CA.",
                    "year": 2015
                },
                {
                    "title": "From machine learning to machine reasoning: an essay",
                    "authors": [
                        "L. Bottou"
                    ],
                    "venue": "Machine Learning, 94:133\u2013149.",
                    "year": 2014
                },
                {
                    "title": "Learning to win by reading manuals in a Monte-Carlo framework",
                    "authors": [
                        "S. Branavan",
                        "D. Silver",
                        "R. Barzilay"
                    ],
                    "venue": "Journal of Artificial Intelligence Research, 43:661\u2013704.",
                    "year": 2012
                },
                {
                    "title": "Multitask learning",
                    "authors": [
                        "R. Caruana"
                    ],
                    "venue": "Machine Learning, 28:41\u201375.",
                    "year": 1997
                },
                {
                    "title": "Learning to interpret natural language navigation instructions from observations",
                    "authors": [
                        "D. Chen",
                        "R. Mooney"
                    ],
                    "venue": "Proceedings of AAAI, pages 859\u2013865, San Francisco, CA. 32",
                    "year": 2011
                },
                {
                    "title": "Natural language processing (almost) from scratch",
                    "authors": [
                        "R. Collobert",
                        "J. Weston",
                        "L. Bottou",
                        "M. Karlen",
                        "K. Kavukcuoglu",
                        "P. Kuksa"
                    ],
                    "venue": "Journal of Machine Learning Research, 12:2493\u20132537.",
                    "year": 2011
                },
                {
                    "title": "Learning and development in neural networks: the importance of starting small",
                    "authors": [
                        "J. Elman"
                    ],
                    "venue": "Cognition, 48:71\u201399.",
                    "year": 1993
                },
                {
                    "title": "Vector space models of word meaning and phrase meaning: A survey",
                    "authors": [
                        "K. Erk"
                    ],
                    "venue": "Language and Linguistics Compass, 6(10):635\u2013653.",
                    "year": 2012
                },
                {
                    "title": "The Language of Thought",
                    "authors": [
                        "J. Fodor"
                    ],
                    "venue": "Crowell Press, New York.",
                    "year": 1975
                },
                {
                    "title": "Visual Turing test for computer vision systems",
                    "authors": [
                        "D. Geman",
                        "S. Geman",
                        "N. Hallonquist",
                        "L. Younes"
                    ],
                    "venue": "Proceedings of the National Academy of Sciences, 112(12):3618\u20133623.",
                    "year": 2015
                },
                {
                    "title": "General game playing: Overview of the AAAI competition",
                    "authors": [
                        "M. Genesereth",
                        "N. Love",
                        "B. Pell"
                    ],
                    "venue": "AI Magazine, 26(2):62\u201372.",
                    "year": 2005
                },
                {
                    "title": "Neural turing machines",
                    "authors": [
                        "A. Graves",
                        "G. Wayne",
                        "I. Danihelka"
                    ],
                    "venue": "http: //arxiv.org/abs/1410.5401.",
                    "year": 2014
                },
                {
                    "title": "Learning to transduce with unbounded memory",
                    "authors": [
                        "E. Grefenstette",
                        "K. Hermann",
                        "M. Suleyman",
                        "P. Blunsom"
                    ],
                    "venue": "Proceedings of NIPS, Montreal, Canada. In press.",
                    "year": 2015
                },
                {
                    "title": "Artificial Intelligence: The Very Idea",
                    "authors": [
                        "J. Haugeland"
                    ],
                    "venue": "MIT Press, Cambridge, MA.",
                    "year": 1985
                },
                {
                    "title": "Inferring algorithmic patterns with stackaugmented recurrent nets",
                    "authors": [
                        "A. Joulin",
                        "T. Mikolov"
                    ],
                    "venue": "Proceedings of NIPS, Montreal, Canada. In press.",
                    "year": 2015
                },
                {
                    "title": "Reinforcement learning: A survey",
                    "authors": [
                        "L.P. Kaelbling",
                        "M.L. Littman",
                        "A.W. Moore"
                    ],
                    "venue": "Journal of artificial intelligence research, pages 237\u2013285.",
                    "year": 1996
                },
                {
                    "title": "Deep learning",
                    "authors": [
                        "Y. LeCun",
                        "Y. Bengio",
                        "G. Hinton"
                    ],
                    "venue": "Nature, 521:436\u2013444.",
                    "year": 2015
                },
                {
                    "title": "The Winograd schema challenge",
                    "authors": [
                        "H.J. Levesque",
                        "E. Davis",
                        "L. Morgenstern"
                    ],
                    "venue": "Proceedings of KR, pages 362\u2013372, Rome, Italy.",
                    "year": 2012
                },
                {
                    "title": "Symbol interdependency in symbolic and embodied cognition",
                    "authors": [
                        "M. Louwerse"
                    ],
                    "venue": "Topics in Cognitive Science, 3:273\u2013302.",
                    "year": 2011
                },
                {
                    "title": "Walk the talk: Connecting language, knowledge, and action in route instructions",
                    "authors": [
                        "M. MacMahon",
                        "B. Stankiewicz",
                        "B. Kuipers"
                    ],
                    "venue": "Proceedings of AAAI, pages 1475\u20131482, Boston, MA.",
                    "year": 2006
                },
                {
                    "title": "Incremental learning of algorithms",
                    "authors": [
                        "T. Mikolov"
                    ],
                    "venue": "Unpublished manuscript.",
                    "year": 2013
                },
                {
                    "title": "Efficient estimation of word representations in vector space",
                    "authors": [
                        "T. Mikolov",
                        "K. Chen",
                        "G. Corrado",
                        "J. Dean"
                    ],
                    "venue": "http://arxiv.org/abs/1301.3781/.",
                    "year": 2013
                },
                {
                    "title": "Never-ending learning",
                    "authors": [
                        "T. Mitchell",
                        "W. Cohen",
                        "E. Hruschka",
                        "P. Talukdar",
                        "J. Betteridge",
                        "A. Carlson",
                        "B. Mishra",
                        "M. Gardner",
                        "B. Kisiel",
                        "J. Krishnamurthy",
                        "N. Lao",
                        "K. Mazaitis",
                        "T. Mohamed",
                        "N. Nakashole",
                        "E. Platanios",
                        "A. Ritter",
                        "M. Samadi",
                        "B. Settles",
                        "R. Wang",
                        "D. Wijaya",
                        "A. Gupta",
                        "X. Chen",
                        "A. Saparov",
                        "M. Greaves",
                        "J. Welling"
                    ],
                    "venue": "Proceedings of AAAI, pages 2302\u20132310, Austin,",
                    "year": 2015
                },
                {
                    "title": "Human-level control through deep reinforcement learning",
                    "authors": [
                        "V. Mnih",
                        "K. Kavukcuoglu",
                        "D. Silver",
                        "A. Rusu",
                        "J. Veness",
                        "M. Bellemare",
                        "A. Graves",
                        "M. Riedmiller",
                        "A. Fidjeland",
                        "G. Ostrovski",
                        "S. Petersen",
                        "C. Beattie",
                        "A. Sadik",
                        "I. Antonoglou",
                        "H. King",
                        "D. Kumaran",
                        "D. Wierstra",
                        "S. Legg",
                        "D. Hassabis"
                    ],
                    "venue": "Nature, 518:529\u2013 533.",
                    "year": 2015
                },
                {
                    "title": "Minds, Brains, and Computers: Perspectives in Cognitive Science and Artificial Intelligence",
                    "authors": [
                        "R. Morelli",
                        "M. Brown",
                        "D. Anselmi",
                        "K. Haberlandt",
                        "D. Lloyd"
                    ],
                    "year": 1992
                },
                {
                    "title": "Language understanding for text-based games using deep reinforcement learning",
                    "authors": [
                        "K. Narasimhan",
                        "T. Kulkarni",
                        "R. Barzilay"
                    ],
                    "venue": "Proceedings of EMNLP, pages 1\u201311, Lisbon, Portugal.",
                    "year": 2015
                },
                {
                    "title": "Software agents: An overview",
                    "authors": [
                        "H. Nwana"
                    ],
                    "venue": "Knowledge Engineering Review, 11(2):1\u201340.",
                    "year": 1996
                },
                {
                    "title": "A field guide to genetic programming",
                    "authors": [
                        "R. Poli",
                        "W. Langdon",
                        "N. McPhee",
                        "J. Koza"
                    ],
                    "venue": "http://www.gp-field-guide.org.uk.",
                    "year": 2008
                },
                {
                    "title": "The algorithmic beauty of plants",
                    "authors": [
                        "P. Prusinkiewicz",
                        "A. Lindenmayer"
                    ],
                    "venue": "Springer Science & Business Media.",
                    "year": 2012
                },
                {
                    "title": "CHILD: A first step towards continual learning",
                    "authors": [
                        "M. Ring"
                    ],
                    "venue": "Machine Learning, 28:77\u2013104.",
                    "year": 1997
                },
                {
                    "title": "Artificial Intelligence: A Modern Approach, 3d ed",
                    "authors": [
                        "S. Russell",
                        "P. Norvig"
                    ],
                    "venue": "Pearson Education, New York.",
                    "year": 2009
                },
                {
                    "title": "Optimal ordered problem solver",
                    "authors": [
                        "J. Schmidhuber"
                    ],
                    "venue": "Machine Learning, 54(3):211\u2013254.",
                    "year": 2004
                },
                {
                    "title": "On learning to think: Algorithmic information theory for novel combinations of reinforcement learning controllers and recurrent neural world models",
                    "authors": [
                        "J. Schmidhuber"
                    ],
                    "venue": "http://arxiv.org/abs/1511.09249. 34",
                    "year": 2015
                },
                {
                    "title": "Multiagent Systems",
                    "authors": [
                        "Y. Shoham",
                        "K. Leyton-Brown"
                    ],
                    "venue": "Cambridge University Press, Cambridge.",
                    "year": 2009
                },
                {
                    "title": "Lifelong machine learning systems: Beyond learning algorithms",
                    "authors": [
                        "D. Silver",
                        "Q. Yang",
                        "L. Li"
                    ],
                    "venue": "Proceedings of the AAAI Spring Symposium on Lifelong Machine Learning, pages 49\u201355, Stanford, CA.",
                    "year": 2013
                },
                {
                    "title": "A formal theory of inductive inference",
                    "authors": [
                        "R.J. Solomonoff"
                    ],
                    "venue": "Part I. Information and control, 7(1):1\u201322.",
                    "year": 1964
                },
                {
                    "title": "The discovery of algorithmic probability",
                    "authors": [
                        "R.J. Solomonoff"
                    ],
                    "venue": "Journal of Computer and System Sciences, 55(1):73\u201388.",
                    "year": 1997
                },
                {
                    "title": "Progress in incremental machine learning",
                    "authors": [
                        "R.J. Solomonoff"
                    ],
                    "venue": "NIPS Workshop on Universal Learning Algorithms and Optimal Search, Whistler, BC. Citeseer.",
                    "year": 2002
                },
                {
                    "title": "Social language learning",
                    "authors": [
                        "L. Steels"
                    ],
                    "venue": "Tokoro, M. and Steels, L., editors, The Future of Learning, pages 133\u2013162. IOS, Amsterdam.",
                    "year": 2003
                },
                {
                    "title": "What triggers the emergence of grammar",
                    "authors": [
                        "L. Steels"
                    ],
                    "venue": "In Proceedings of EELC,",
                    "year": 2005
                },
                {
                    "title": "MazeBase: a sandbox for learning from games",
                    "authors": [
                        "S. Sukhbaatar",
                        "A. Szlam",
                        "G. Synnaeve",
                        "S. Chintala",
                        "R. Fergus"
                    ],
                    "venue": "http://arxiv.org/abs/1511.07401.",
                    "year": 2015
                },
                {
                    "title": "Reinforcement Learning: An Introduction",
                    "authors": [
                        "R. Sutton",
                        "A. Barto"
                    ],
                    "venue": "MIT Press, Cambridge, MA.",
                    "year": 1998
                },
                {
                    "title": "Cognitive foundations for knowledge representation in AI",
                    "authors": [
                        "J. Tenenbaum"
                    ],
                    "venue": "Presented at the AAAI Spring Symposium on Knowledge Representation and Reasoning.",
                    "year": 2015
                },
                {
                    "title": "Learning to interpret natural language commands through human-robot dialog",
                    "authors": [
                        "J. Thomason",
                        "S. Zhang",
                        "R. Mooney",
                        "P. Stone"
                    ],
                    "venue": "Proceedings IJCAI, pages 1923\u20131929, Buenos Aires, Argentina.",
                    "year": 2015
                },
                {
                    "title": "Computing machinery and intelligence",
                    "authors": [
                        "A. Turing"
                    ],
                    "venue": "Mind, 59:433\u2013460.",
                    "year": 1950
                },
                {
                    "title": "From frequency to meaning: Vector space models of semantics",
                    "authors": [
                        "P. Turney",
                        "P. Pantel"
                    ],
                    "venue": "Journal of Artificial Intelligence Research, 37:141\u2013188.",
                    "year": 2010
                },
                {
                    "title": "Towards AI-complete question answering: A set of prerequisite toy tasks",
                    "authors": [
                        "J. Weston",
                        "A. Bordes",
                        "S. Chopra",
                        "T. Mikolov"
                    ],
                    "venue": "http://arxiv.org/abs/1502. 05698.",
                    "year": 2015
                },
                {
                    "title": "Artificial human companion",
                    "authors": [
                        "Wikipedia"
                    ],
                    "venue": "https://en.wikipedia.org/ w/index.php?title=Artificial_human_companion&oldid=685507143. Accessed 15-October-2015.",
                    "year": 2015
                },
                {
                    "title": "Interactive fiction",
                    "authors": [
                        "Wikipedia"
                    ],
                    "venue": "https://en.wikipedia.org/w/index.php? title=Interactive_fiction&oldid=693926750. Accessed 19-December-2015.",
                    "year": 2015
                },
                {
                    "title": "Turing test",
                    "authors": [
                        "Wikipedia"
                    ],
                    "venue": "https://en.wikipedia.org/w/index.php?title= Turing_test&oldid=673582926. Accessed 30-July-2015.",
                    "year": 2015
                },
                {
                    "title": "Procedures as a representation for data in a computer program for understanding natural language",
                    "authors": [
                        "T. Winograd"
                    ],
                    "venue": "Technical Report AI 235, Massachusetts Institute of Technology. 36",
                    "year": 1971
                }
            ],
            "id": "SP:809795994cfaf0ce464848c99816b161814a2924",
            "authors": [
                {
                    "name": "Tomas Mikolov",
                    "affiliations": []
                },
                {
                    "name": "Armand Joulin",
                    "affiliations": []
                },
                {
                    "name": "Marco Baroni",
                    "affiliations": []
                }
            ],
            "abstractText": "The development of intelligent machines is one of the biggest unsolved challenges in computer science. In this paper, we propose some fundamental properties these machines should have, focusing in particular on communication and learning. We discuss a simple environment that could be used to incrementally teach a machine the basics of natural-language-based communication, as a prerequisite to more complex interaction with human users. We also present some conjectures on the sort of algorithms the machine should support in order to profitably learn from the environment.",
            "title": "A Roadmap towards Machine Intelligence"
        },
        "Y": {
            "blog_id": "411f15b71ed6a664f9d5ac46409b42",
            "summary": [
                "Proposes a novel, end-to-end architecture for generating short email responses.",
                "Single most important benchmark of its success is that it is deployed in Inbox by Gmail and assists with around 10% of all mobile responses.",
                ".",
                "Challenges in deploying Smart Reply in a user-facing product  Responses must always be of high quality.",
                "Ensured by constructing a target response set to select responses from.",
                "The likelihood of choosing the responses must be maximised.",
                "Ensured by normalising the responses and enforcing diversity.",
                "The system should not add latency to emails.",
                "Ensured by using a triggering model to decide if the email is suitable to undergo the response generation pipeline.",
                "Computation time is further reduced by finding approximate best result instead of the best result.",
                "Ensure privacy by encrypting all the data which adds challenge in verifying the model's quality and debugging the system.",
                "Architecture  Preprocess Email  Perform actions like language detection, tokenization, sentence segmentation etc on the input email.",
                "Triggering Model  A feed-forward neural network (with embedding layer and 3 fully connected hidden layers) to decide if the input email is suitable for suggesting responses.",
                "Data  Training set of pairs (o, y) where o is the incoming message and y is a boolean variable to indicate if the message had a response.",
                "Features  Unigrams, bigrams from the messages.",
                "Signals like - is the recipient in the contact list of the sender.",
                "Response Selection  LSTM network to predict the approximate best response for an incoming message o  Network  Sequence to Sequence Learning.",
                "Reads the input message (token by token) and encode a vector representation.",
                "Compute softmax to get the probability of first output token given the input token sequence.",
                "Keep feeding in the previous response tokens and the input token sequence to compute the probability of next output token.",
                "During inference, approximate the most likely response greedily by taking the most likely response at each timestamp and feeding it back or by using the beam search approach.",
                "Response Set Generation  Generate a set of high-quality responses that also capture the variability in the intent of the response.",
                "Canonicalize the email response by extracting the semantic structure using a dependency parser.",
                "Partition all response messages into \"semantic\" clusters.",
                "These semantic clusters define the response space for scoring and selecting possible responses and for promoting diversity among the responses.",
                "Semantic Intent Clustering  Since a large, labelled dataset is not available, a graph based, semi-supervised approach is used.",
                "Graph Construction  Manually define a few clusters with a small number of example responses for each cluster.",
                "Construct a graph with frequent response messages (including the labelled nodes) as response nodes (VR).",
                "For each response node, extract a set of feature nodes (VF) corresponding to features like skip-gram and n-grams and add an edge between the response node and the feature node.",
                "Learn a semantic labelling for all response nodes by propagating semantic intent information (available because of labelled nodes) throughout the graph.",
                "After some iterations, sample some of the unlabeled nodes from the graph, manually label these sample nodes and repeat this algorithm until convergence.",
                "For validation, extract the top k members of each cluster and validate the quality with help of human evaluators.",
                "Suggestion Diversity  Provide users with a varied set of response by omitting redundant response (by not selecting more than one response from any semantic cluster) and by enforcing negative (or positive) responses.",
                "If the top two responses contain at least one positive (negative) response and none of the top three responses is negative (positive), the third response is replaced with a negative (positive) one.",
                "This is done by performing a second LSTM pass where the search is restricted to only positive (or negative) responses in the target set.",
                "Strengths  The system is already in production and assists with around 10% of all mobile responses.",
                "This comment has been minimized.",
                "Sign in to view  Copy link  Quote reply  vegetakarthhik commented  Nov 26, 2018  hey do you have python implementation?"
            ],
            "author_id": "shugan",
            "pdf_url": "https://arxiv.org/pdf/1511.08130",
            "author_full_name": "Shagun Sodhani",
            "source_website": "https://github.com/shagunsodhani/papers-I-read",
            "id": 68642594
        }
    },
    "77684000": {
        "X": {
            "sections": [
                {
                    "text": "Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure."
                },
                {
                    "heading": "1 Introduction",
                    "text": "Progress on the path from shallow bag-of-words information retrieval algorithms to machines capable of reading and understanding documents has been slow. Traditional approaches to machine reading and comprehension have been based on either hand engineered grammars [1], or information extraction methods of detecting predicate argument triples that can later be queried as a relational database [2]. Supervised machine learning approaches have largely been absent from this space due to both the lack of large scale training datasets, and the difficulty in structuring statistical models flexible enough to learn to exploit document structure.\nWhile obtaining supervised natural language reading comprehension data has proved difficult, some researchers have explored generating synthetic narratives and queries [3, 4]. Such approaches allow the generation of almost unlimited amounts of supervised data and enable researchers to isolate the performance of their algorithms on individual simulated phenomena. Work on such data has shown that neural network based models hold promise for modelling reading comprehension, something that we will build upon here. Historically, however, many similar approaches in Computational Linguistics have failed to manage the transition from synthetic data to real environments, as such closed worlds inevitably fail to capture the complexity, richness, and noise of natural language [5].\nIn this work we seek to directly address the lack of real natural language training data by introducing a novel approach to building a supervised reading comprehension data set. We observe that summary and paraphrase sentences, with their associated documents, can be readily converted to context\u2013query\u2013answer triples using simple entity detection and anonymisation algorithms. Using this approach we have collected two new corpora of roughly a million news stories with associated queries from the CNN and Daily Mail websites.\nWe demonstrate the efficacy of our new corpora by building novel deep learning models for reading comprehension. These models draw on recent developments for incorporating attention mechanisms into recurrent neural network architectures [6, 7, 8, 4]. This allows a model to focus on the aspects of a document that it believes will help it answer a question, and also allows us to visualises its inference process. We compare these neural models to a range of baselines and heuristic benchmarks based upon a traditional frame semantic analysis provided by a state-of-the-art natural language processing\nar X\niv :1\n50 6.\n03 34\n0v 3\n[ cs\n.C L\n] 1\n9 N\nov 2\n(NLP) pipeline. Our results indicate that the neural models achieve a higher accuracy, and do so without any specific encoding of the document or query structure."
                },
                {
                    "heading": "2 Supervised training data for reading comprehension",
                    "text": "The reading comprehension task naturally lends itself to a formulation as a supervised learning problem. Specifically we seek to estimate the conditional probability p(a|c, q), where c is a context document, q a query relating to that document, and a the answer to that query. For a focused evaluation we wish to be able to exclude additional information, such as world knowledge gained from co-occurrence statistics, in order to test a model\u2019s core capability to detect and understand the linguistic relationships between entities in the context document.\nSuch an approach requires a large training corpus of document\u2013query\u2013answer triples and until now such corpora have been limited to hundreds of examples and thus mostly of use only for testing [9]. This limitation has meant that most work in this area has taken the form of unsupervised approaches which use templates or syntactic/semantic analysers to extract relation tuples from the document to form a knowledge graph that can be queried.\nHere we propose a methodology for creating real-world, large scale supervised training data for learning reading comprehension models. Inspired by work in summarisation [10, 11], we create two machine reading corpora by exploiting online newspaper articles and their matching summaries. We have collected 93k articles from the CNN1 and 220k articles from the Daily Mail2 websites. Both news providers supplement their articles with a number of bullet points, summarising aspects of the information contained in the article. Of key importance is that these summary points are abstractive and do not simply copy sentences from the documents. We construct a corpus of document\u2013query\u2013 answer triples by turning these bullet points into Cloze [12] style questions by replacing one entity at a time with a placeholder. This results in a combined corpus of roughly 1M data points (Table 1). Code to replicate our datasets\u2014and to apply this method to other sources\u2014is available online3."
                },
                {
                    "heading": "2.1 Entity replacement and permutation",
                    "text": "Note that the focus of this paper is to provide a corpus for evaluating a model\u2019s ability to read and comprehend a single document, not world knowledge or co-occurrence. To understand that distinction consider for instance the following Cloze form queries (created from headlines in the Daily Mail validation set): a) The hi-tech bra that helps you beat breast X; b) Could Saccharin help beat X ?; c) Can fish oils help fight prostate X ? An ngram language model trained on the Daily Mail would easily correctly predict that (X = cancer), regardless of the contents of the context document, simply because this is a very frequently cured entity in the Daily Mail corpus.\n1www.cnn.com 2www.dailymail.co.uk 3http://www.github.com/deepmind/rc-data/\nTo prevent such degenerate solutions and create a focused task we anonymise and randomise our corpora with the following procedure, a) use a coreference system to establish coreferents in each data point; b) replace all entities with abstract entity markers according to coreference; c) randomly permute these entity markers whenever a data point is loaded.\nCompare the original and anonymised version of the example in Table 3. Clearly a human reader can answer both queries correctly. However in the anonymised setup the context document is required for answering the query, whereas the original version could also be answered by someone with the requisite background knowledge. Therefore, following this procedure, the only remaining strategy for answering questions is to do so by exploiting the context presented with each question. Thus performance on our two corpora truly measures reading comprehension capability. Naturally a production system would benefit from using all available information sources, such as clues through language and co-occurrence statistics.\nTable 2 gives an indication of the difficulty of the task, showing how frequent the correct answer is contained in the top N entity markers in a given document. Note that our models don\u2019t distinguish between entity markers and regular words. This makes the task harder and the models more general."
                },
                {
                    "heading": "3 Models",
                    "text": "So far we have motivated the need for better datasets and tasks to evaluate the capabilities of machine reading models. We proceed by describing a number of baselines, benchmarks and new models to evaluate against this paradigm. We define two simple baselines, the majority baseline (maximum frequency) picks the entity most frequently observed in the context document, whereas the exclusive majority (exclusive frequency) chooses the entity most frequently observed in the context but not observed in the query. The idea behind this exclusion is that the placeholder is unlikely to be mentioned twice in a single Cloze form query."
                },
                {
                    "heading": "3.1 Symbolic Matching Models",
                    "text": "Traditionally, a pipeline of NLP models has been used for attempting question answering, that is models that make heavy use of linguistic annotation, structured world knowledge and semantic parsing and similar NLP pipeline outputs. Building on these approaches, we define a number of NLP-centric models for our machine reading task.\nFrame-Semantic Parsing Frame-semantic parsing attempts to identify predicates and their arguments, allowing models access to information about \u201cwho did what to whom\u201d. Naturally this kind of annotation lends itself to being exploited for question answering. We develop a benchmark that\nmakes use of frame-semantic annotations which we obtained by parsing our model with a state-ofthe-art frame-semantic parser [13, 14]. As the parser makes extensive use of linguistic information we run these benchmarks on the unanonymised version of our corpora. There is no significant advantage in this as the frame-semantic approach used here does not possess the capability to generalise through a language model beyond exploiting one during the parsing phase. Thus, the key objective of evaluating machine comprehension abilities is maintained. Extracting entity-predicate triples\u2014 denoted as (e1, V, e2)\u2014from both the query q and context document d, we attempt to resolve queries using a number of rules with an increasing recall/precision trade-off as follows (Table 4).\nFor reasons of clarity, we pretend that all PropBank triples are of the form (e1, V, e2). In practice, we take the argument numberings of the parser into account and only compare like with like, except in cases such as the permuted frame rule, where ordering is relaxed. In the case of multiple possible answers from a single rule, we randomly choose one.\nWord Distance Benchmark We consider another baseline that relies on word distance measurements. Here, we align the placeholder of the Cloze form question with each possible entity in the context document and calculate a distance measure between the question and the context around the aligned entity. This score is calculated by summing the distances of every word in q to their nearest aligned word in d, where alignment is defined by matching words either directly or as aligned by the coreference system. We tune the maximum penalty per word (m = 8) on the validation data."
                },
                {
                    "heading": "3.2 Neural Network Models",
                    "text": "Neural networks have successfully been applied to a range of tasks in NLP. This includes classification tasks such as sentiment analysis [15] or POS tagging [16], as well as generative problems such as language modelling or machine translation [17]. We propose three neural models for estimating the probability of word type a from document d answering query q:\np(a|d, q) \u221d exp (W (a)g(d, q)) , s.t. a \u2208 V,\nwhere V is the vocabulary4, and W (a) indexes row a of weight matrix W and through a slight abuse of notation word types double as indexes. Note that we do not privilege entities or variables, the model must learn to differentiate these in the input sequence. The function g(d, q) returns a vector embedding of a document and query pair.\nThe Deep LSTM Reader Long short-term memory (LSTM, [18]) networks have recently seen considerable success in tasks such as machine translation and language modelling [17]. When used for translation, Deep LSTMs [19] have shown a remarkable ability to embed long sequences into a vector representation which contains enough information to generate a full translation in another language. Our first neural model for reading comprehension tests the ability of Deep LSTM encoders to handle significantly longer sequences. We feed our documents one word at a time into a Deep LSTM encoder, after a delimiter we then also feed the query into the encoder. Alternatively we also experiment with processing the query then the document. The result is that this model processes each document query pair as a single long sequence. Given the embedded document and query the network predicts which token in the document answers the query.\n4The vocabulary includes all the word types in the documents, questions, the entity maskers, and the question unknown entity marker.\nWe employ a Deep LSTM cell with skip connections from each input x(t) to every hidden layer, and from every hidden layer to the output y(t):\nx\u2032(t, k) = x(t)||y\u2032(t, k \u2212 1), y(t) = y\u2032(t, 1)|| . . . ||y\u2032(t,K) i(t, k) = \u03c3 (Wkxix\n\u2032(t, k) +Wkhih(t\u2212 1, k) +Wkcic(t\u2212 1, k) + bki) f(t, k) = \u03c3 (Wkxfx(t) +Wkhfh(t\u2212 1, k) +Wkcfc(t\u2212 1, k) + bkf ) c(t, k) = f(t, k)c(t\u2212 1, k) + i(t, k) tanh (Wkxcx\u2032(t, k) +Wkhch(t\u2212 1, k) + bkc) o(t, k) = \u03c3 (Wkxox\n\u2032(t, k) +Wkhoh(t\u2212 1, k) +Wkcoc(t, k) + bko) h(t, k) = o(t, k) tanh (c(t, k))\ny\u2032(t, k) =Wkyh(t, k) + bky\nwhere || indicates vector concatenation h(t, k) is the hidden state for layer k at time t, and i, f , o are the input, forget, and output gates respectively. Thus our Deep LSTM Reader is defined by gLSTM(d, q) = y(|d|+ |q|) with input x(t) the concatenation of d and q separated by the delimiter |||.\nThe Attentive Reader The Deep LSTM Reader must propagate dependencies over long distances in order to connect queries to their answers. The fixed width hidden vector forms a bottleneck for this information flow that we propose to circumvent using an attention mechanism inspired by recent results in translation and image recognition [6, 7]. This attention model first encodes the document and the query using separate bidirectional single layer LSTMs [19].\nWe denote the outputs of the forward and backward LSTMs as \u2212\u2192y (t) and \u2190\u2212y (t) respectively. The encoding u of a query of length |q| is formed by the concatenation of the final forward and backward outputs, u = \u2212\u2192yq(|q|) || \u2190\u2212yq(1). For the document the composite output for each token at position t is, yd(t) = \u2212\u2192yd(t) || \u2190\u2212yd(t). The representation r of the document d is formed by a weighted sum of these output vectors. These weights are interpreted as the degree to which the network attends to a particular token in the document when answering the query:\nm(t) = tanh (Wymyd(t) +Wumu) ,\ns(t) \u221d exp (w\u1d40msm(t)) , r = yds,\nwhere we are interpreting yd as a matrix with each column being the composite representation yd(t) of document token t. The variable s(t) is the normalised attention at token t. Given this attention\nscore the embedding of the document r is computed as the weighted sum of the token embeddings. The model is completed with the definition of the joint document and query embedding via a nonlinear combination:\ngAR(d, q) = tanh (Wrgr +Wugu) .\nThe Attentive Reader can be viewed as a generalisation of the application of Memory Networks to question answering [3]. That model employs an attention mechanism at the sentence level where each sentence is represented by a bag of embeddings. The Attentive Reader employs a finer grained token level attention mechanism where the tokens are embedded given their entire future and past context in the input document.\nThe Impatient Reader The Attentive Reader is able to focus on the passages of a context document that are most likely to inform the answer to the query. We can go further by equipping the model with the ability to reread from the document as each query token is read. At each token i of the query q the model computes a document representation vector r(i) using the bidirectional embedding yq(i) = \u2212\u2192yq(i) || \u2190\u2212yq(i):\nm(i, t) = tanh (Wdmyd(t) +Wrmr(i\u2212 1) +Wqmyq(i)) , 1 \u2264 i \u2264 |q|, s(i, t) \u221d exp (w\u1d40msm(i, t)) , r(0) = r0, r(i) = y \u1d40 ds(i) + tanh (Wrrr(i\u2212 1)) 1 \u2264 i \u2264 |q|.\nThe result is an attention mechanism that allows the model to recurrently accumulate information from the document as it sees each query token, ultimately outputting a final joint document query representation for the answer prediction,\ngIR(d, q) = tanh (Wrgr(|q|) +Wqgu) ."
                },
                {
                    "heading": "4 Empirical Evaluation",
                    "text": "Having described a number of models in the previous section, we next evaluate these models on our reading comprehension corpora. Our hypothesis is that neural models should in principle be well suited for this task. However, we argued that simple recurrent models such as the LSTM probably have insufficient expressive power for solving tasks that require complex inference. We expect that the attention-based models would therefore outperform the pure LSTM-based approaches.\nConsidering the second dimension of our investigation, the comparison of traditional versus neural approaches to NLP, we do not have a strong prior favouring one approach over the other. While numerous publications in the past few years have demonstrated neural models outperforming classical methods, it remains unclear how much of that is a side-effect of the language modelling capabilities intrinsic to any neural model for NLP. The entity anonymisation and permutation aspect of the task presented here may end up levelling the playing field in that regard, favouring models capable of dealing with syntax rather than just semantics.\nWith these considerations in mind, the experimental part of this paper is designed with a threefold aim. First, we want to establish the difficulty of our machine reading task by applying a wide range of models to it. Second, we compare the performance of parse-based methods versus that of neural models. Third, within the group of neural models examined, we want to determine what each component contributes to the end performance; that is, we want to analyse the extent to which an LSTM can solve this task, and to what extent various attention mechanisms impact performance.\nAll model hyperparameters were tuned on the respective validation sets of the two corpora.5 Our experimental results are in Table 5, with the Attentive and Impatient Readers performing best across both datasets.\n5For the Deep LSTM Reader, we consider hidden layer sizes [64, 128, 256], depths [1, 2, 4], initial learning rates [1E\u22123, 5E\u22124, 1E\u22124, 5E\u22125], batch sizes [16, 32] and dropout [0.0, 0.1, 0.2]. We evaluate two types of feeds. In the cqa setup we feed first the context document and subsequently the question into the encoder, while the qca model starts by feeding in the question followed by the context document. We report results on the best model (underlined hyperparameters, qca setup). For the attention models we consider hidden layer sizes [64, 128, 256], single layer, initial learning rates [1E\u22124, 5E\u22125, 2.5E\u22125, 1E\u22125], batch sizes [8, 16, 32] and dropout [0, 0.1, 0.2, 0.5]. For all models we used asynchronous RmsProp [20] with a momentum of 0.9 and a decay of 0.95. See Appendix A for more details of the experimental setup.\nFrame-semantic benchmark While the one frame-semantic model proposed in this paper is clearly a simplification of what could be achieved with annotations from an NLP pipeline, it does highlight the difficulty of the task when approached from a symbolic NLP perspective.\nTwo issues stand out when analysing the results in detail. First, the frame-semantic pipeline has a poor degree of coverage with many relations not being picked up by our PropBank parser as they do not adhere to the default predicate-argument structure. This effect is exacerbated by the type of language used in the highlights that form the basis of our datasets. The second issue is that the frame-semantic approach does not trivially scale to situations where several sentences, and thus frames, are required to answer a query. This was true for the majority of queries in the dataset.\nWord distance benchmark More surprising perhaps is the relatively strong performance of the word distance benchmark, particularly relative to the frame-semantic benchmark, which we had expected to perform better. Here, again, the nature of the datasets used can explain aspects of this result. Where the frame-semantic model suffered due to the language used in the highlights, the word distance model benefited. Particularly in the case of the Daily Mail dataset, highlights frequently have significant lexical overlap with passages in the accompanying article, which makes it easy for the word distance benchmark. For instance the query \u201cTom Hanks is friends with X\u2019s manager, Scooter Brown\u201d has the phrase \u201c... turns out he is good friends with Scooter Brown, manager for Carly Rae Jepson\u201d in the context. The word distance benchmark correctly aligns these two while the frame-semantic approach fails to pickup the friendship or management relations when parsing the query. We expect that on other types of machine reading data where questions rather than Cloze queries are used this particular model would perform significantly worse.\nNeural models Within the group of neural models explored here, the results paint a clear picture with the Impatient and the Attentive Readers outperforming all other models. This is consistent with our hypothesis that attention is a key ingredient for machine reading and question answering due to the need to propagate information over long distances. The Deep LSTM Reader performs surprisingly well, once again demonstrating that this simple sequential architecture can do a reasonable job of learning to abstract long sequences, even when they are up to two thousand tokens in length. However this model does fail to match the performance of the attention based models, even though these only use single layer LSTMs.6\nThe poor results of the Uniform Reader support our hypothesis of the significance of the attention mechanism in the Attentive model\u2019s performance as the only difference between these models is that the attention variables are ignored in the Uniform Reader. The precision@recall statistics in Figure 2 again highlight the strength of the attentive approach.\nWe can visualise the attention mechanism as a heatmap over a context document to gain further insight into the models\u2019 performance. The highlighted words show which tokens in the document were attended to by the model. In addition we must also take into account that the vectors at each\n6Memory constraints prevented us from experimenting with deeper Attentive Readers.\ntoken integrate long range contextual information via the bidirectional LSTM encoders. Figure 3 depicts heat maps for two queries that were correctly answered by the Attentive Reader.7 In both cases confidently arriving at the correct answer requires the model to perform both significant lexical generalsiation, e.g. \u2018killed\u2019\u2192 \u2018deceased\u2019, and co-reference or anaphora resolution, e.g. \u2018ent119 was killed\u2019\u2192 \u2018he was identified.\u2019 However it is also clear that the model is able to integrate these signals with rough heuristic indicators such as the proximity of query words to the candidate answer."
                },
                {
                    "heading": "5 Conclusion",
                    "text": "The supervised paradigm for training machine reading and comprehension models provides a promising avenue for making progress on the path to building full natural language understanding systems. We have demonstrated a methodology for obtaining a large number of document-queryanswer triples and shown that recurrent and attention based neural networks provide an effective modelling framework for this task. Our analysis indicates that the Attentive and Impatient Readers are able to propagate and integrate semantic information over long distances. In particular we believe that the incorporation of an attention mechanism is the key contributor to these results.\nThe attention mechanism that we have employed is just one instantiation of a very general idea which can be further exploited. However, the incorporation of world knowledge and multi-document queries will also require the development of attention and embedding mechanisms whose complexity to query does not scale linearly with the data set size. There are still many queries requiring complex inference and long range reference resolution that our models are not yet able to answer. As such our data provides a scalable challenge that should support NLP research into the future. Further, significantly bigger training data sets can be acquired using the techniques we have described, undoubtedly allowing us to train more expressive and accurate models.\n7Note that these examples were chosen as they were short, the average CNN validation document contained 763 tokens and 27 entities, thus most instances were significantly harder to answer than these examples."
                },
                {
                    "heading": "A Model hyperparameters",
                    "text": "The precise hyperparameters used for the various attentive models are as in Table 6. All models were trained using asynchronous RmsProp [20] with a momentum of 0.9 and a decay of 0.95."
                },
                {
                    "heading": "B Performance across document length",
                    "text": "To understand how the model performance depends on the size of the context, we plot performance versus document lengths in Figures 4 and 5. The first figure (Fig. 4) plots a sliding window of performance across document length, showing that performance of the attentive models degrades slightly as documents increase in length. The second figure (Fig. 5) shows the cumulative performance with documents up to lengthN , showing that while the length does impact the models\u2019 performance, that effect becomes negligible after reaching a length of ~500 tokens.\nFigure 4: Precision@Document Length for the attention models on the CNN validation data. The chart shows the precision for each decile in document lengths across the corpus as well as the precision for the 5% longest articles.\nFigure 5: Aggregated precision for documents up to a certain lengths. The points mark the ith decile in document lengths across the corpus."
                },
                {
                    "heading": "C Additional Heatmap Analysis",
                    "text": "We expand on the analysis of the attention mechanism presented in the paper by including visualisations for additional queries from the CNN validation dataset below. We consider examples from the Attentive Reader as well as the Impatient Reader in this appendix.\nC.1 Attentive Reader\nPositive Instances Figure 6 shows two positive examples from the CNN validation set that require reasonable levels of lexical generalisation and co-reference in order to be answered. The first query in Figure 7 contains strong lexical cues through the quote, but requires identifying the entity\nquoted, which is non-trivial in the context document. The final positive example (also in Figure 7) demonstrates the fearlessness of our model.\nNegative Instances Figures 8 and 9 show examples of queries where the Attentive Reader fails to select the correct answer. The two examples in Figure 8 highlight a fairly common phenomenon in the data, namely ambiguous queries, where\u2014at least following the anonymisation process\u2014 multiple entities are plausible answers even when evaluated manually. Note that in both cases the query searches for an entity marker that describes a geographic location, preceded by the word \u201cin\u201d. Here it is unclear whether the placeholder refers to a part of town, town, region or country.\nFigure 9 contains two additional negative cases. The first failure is caused by the co-reference entity selection process. The correct entity, ent15, and the predicted one, ent81, both refer to the same person, but not being clustered together. Arguably this is a difficult clustering as one entity refers to \u201cKate Middleton\u201d and the other to \u201cThe Duchess of Cambridge\u201d. The right example shows a situation in which the model fails as it perhaps gets too little information from the short query and then selects the wrong cue with the term \u201cclaims\u201d near the wrongly identified entity ent1 (correct: ent74).\nC.2 Impatient Reader\nTo give a better intuition for the behaviour of the Impatient Reader, we use a similar visualisation technique as before. However, this time around we highlight the attention at every time step as the model updates its focus while moving through a given query. Figures 10\u201313 shows how the attention of the Impatient Reader changes and becomes increasingly more accurate as the model\nconsiders larger parts of the query. Note how the attention is distributed fairly arbitraty at first, slowly focussing on the correct entity ent5 only once the question has sufficiently been parsed."
                }
            ],
            "year": 2015,
            "references": [
                {
                    "title": "Machine reading at the University of Washington",
                    "authors": [
                        "Hoifung Poon",
                        "Janara Christensen",
                        "Pedro Domingos",
                        "Oren Etzioni",
                        "Raphael Hoffmann",
                        "Chloe Kiddon",
                        "Thomas Lin",
                        "Xiao Ling",
                        "Mausam",
                        "Alan Ritter",
                        "Stefan Schoenmackers",
                        "Stephen Soderland",
                        "Dan Weld",
                        "Fei Wu",
                        "Congle Zhang"
                    ],
                    "venue": "In Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading",
                    "year": 2010
                },
                {
                    "title": "Understanding Natural Language",
                    "authors": [
                        "Terry Winograd"
                    ],
                    "year": 1972
                },
                {
                    "title": "Neural machine translation by jointly learning to align and translate",
                    "authors": [
                        "Dzmitry Bahdanau",
                        "Kyunghyun Cho",
                        "Yoshua Bengio"
                    ],
                    "venue": "CoRR, abs/1409.0473,",
                    "year": 2014
                },
                {
                    "title": "DRAW: A recurrent neural network for image generation",
                    "authors": [
                        "Karol Gregor",
                        "Ivo Danihelka",
                        "Alex Graves",
                        "Daan Wierstra"
                    ],
                    "venue": "CoRR, abs/1502.04623,",
                    "year": 2015
                },
                {
                    "title": "Automatic generation of story highlights",
                    "authors": [
                        "Kristian Woodsend",
                        "Mirella Lapata"
                    ],
                    "venue": "In Proceedings of ACL,",
                    "year": 2010
                },
                {
                    "title": "Cloze procedure\u201d: a new tool for measuring readability",
                    "authors": [
                        "Wilson L Taylor"
                    ],
                    "venue": "Journalism Quarterly,",
                    "year": 1953
                },
                {
                    "title": "Semantic frame identification with distributed word representations",
                    "authors": [
                        "Karl Moritz Hermann",
                        "Dipanjan Das",
                        "Jason Weston",
                        "Kuzman Ganchev"
                    ],
                    "venue": "In Proceedings of ACL,",
                    "year": 2014
                },
                {
                    "title": "A convolutional neural network for modelling sentences",
                    "authors": [
                        "Nal Kalchbrenner",
                        "Edward Grefenstette",
                        "Phil Blunsom"
                    ],
                    "venue": "In Proceedings of ACL,",
                    "year": 2014
                },
                {
                    "title": "Natural language processing (almost) from scratch",
                    "authors": [
                        "Ronan Collobert",
                        "Jason Weston",
                        "L\u00e9on Bottou",
                        "Michael Karlen",
                        "Koray Kavukcuoglu",
                        "Pavel Kuksa"
                    ],
                    "venue": "Journal of Machine Learning Research,",
                    "year": 2011
                },
                {
                    "title": "Long short-term memory",
                    "authors": [
                        "Sepp Hochreiter",
                        "J\u00fcrgen Schmidhuber"
                    ],
                    "venue": "Neural Computation,",
                    "year": 1997
                },
                {
                    "title": "Supervised Sequence Labelling with Recurrent Neural Networks, volume 385 of Studies in Computational Intelligence",
                    "authors": [
                        "Alex Graves"
                    ],
                    "year": 2012
                }
            ],
            "id": "SP:2cb8497f9214735ffd1bd57db645794459b8ff41",
            "authors": [
                {
                    "name": "Karl Moritz Hermann",
                    "affiliations": []
                },
                {
                    "name": "Tom\u00e1\u0161 Ko\u010disk\u00fd",
                    "affiliations": []
                },
                {
                    "name": "Edward Grefenstette",
                    "affiliations": []
                },
                {
                    "name": "Lasse Espeholt",
                    "affiliations": []
                },
                {
                    "name": "Will Kay",
                    "affiliations": []
                },
                {
                    "name": "Mustafa Suleyman",
                    "affiliations": []
                },
                {
                    "name": "Phil Blunsom",
                    "affiliations": []
                }
            ],
            "abstractText": "Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.",
            "title": "Teaching Machines to Read and Comprehend"
        },
        "Y": {
            "blog_id": "63eb099bb7a1ab4831cd37bffffb04",
            "summary": [
                "Build a supervised reading comprehension data set using news corpus.",
                "Compare the performance of neural models and state-of-the-art natural language processing model on reading comprehension task.",
                "Reading Comprehension  Estimate conditional probability p(a|c, q), where c is a context document, q is a query related to the document, and a is the answer to that query.",
                "Dataset Generation  Use online newspapers (CNN and DailyMail) and their matching summaries.",
                "Parse summaries and bullet points into Cloze style questions.",
                "Generate corpus of document-query-answer triplets by replacing one entity at a time with a placeholder.",
                "Data anonymized and randomised using coreference systems, abstract entity markers and random permutation of the entity markers.",
                "The processed data set is more focused in terms of evaluating reading comprehension as models can not exploit co-occurrence.",
                "Models  Baseline Models  Majority Baseline  Picks the most frequently observed entity in the context document.",
                "Exclusive Majority  Picks the most frequently observed entity in the context document which is not observed in the query.",
                "Symbolic Matching Models  Frame-Semantic Parsing  Parse the sentence to find predicates to answer questions like \"who did what to whom\".",
                "Extracting entity-predicate triples (e1,V, e2) from query q and context document d  Resolve queries using rules like exact match, matching entity etc.",
                "Word Distance Benchmark  Align placeholder of Cloze form questions with each possible entity in the context document and calculate the distance between the question and the context around the aligned entity.",
                "Sum the distance of every word in q to their nearest aligned word in d  Neural Network Models  Deep LSTM Reader  Test the ability of Deep LSTM encoders to handle significantly longer sequences.",
                "Feed the document query pair as a single large document, one word at a time.",
                "Use Deep LSTM cell with skip connections from input to hidden layers and hidden layer to output.",
                "Attentive Reader  Employ attention model to overcome the bottleneck of fixed width hidden vector.",
                "Encode the document and the query using separate bidirectional single layer LSTM.",
                "Query encoding is obtained by concatenating the final forward and backwards outputs.",
                "Document encoding is obtained by a weighted sum of output vectors (obtained by concatenating the forward and backwards outputs).",
                "The weights can be interpreted as the degree to which the network attends to a particular token in the document.",
                "Model completed by defining a non-linear combination of document and query embedding.",
                "Impatient Reader  As an add-on to the attentive reader, the model can re-read the document as each query token is read.",
                "Model accumulates the information from the document as each query token is seen and finally outputs a joint document query representation in the form of a non-linear combination of document embedding and query embedding.",
                "Result  Attentive and Impatient Readers outperform all other models highlighting the benefits of attention modelling.",
                "Frame-Semantic pipeline does not scale to cases where several methods are needed to answer a query.",
                "Moreover, they provide poor coverage as a lot of relations do not adhere to the default predicate-argument structure.",
                "Word Distance approach outperformed the Frame-Semantic approach as there was significant lexical overlap between the query and the document.",
                "The paper also includes heat maps over the context documents to visualise the attention mechanism.",
                "This comment has been minimized.",
                "Sign in to view  Copy link  Quote reply  yauhen-info commented  Apr 28, 2017  Thank you for sharing a good piece of work.",
                "Let me also ask if you had found a link to an implementation of the Attentive and Impatient Readers?"
            ],
            "author_id": "shugan",
            "pdf_url": "http://arxiv.org/pdf/1506.03340v3",
            "author_full_name": "Shagun Sodhani",
            "source_website": "https://github.com/shagunsodhani/papers-I-read",
            "id": 77684000
        }
    },
    "85505359": {
        "X": {
            "sections": [
                {
                    "text": "We introduce RPCValet, an NI-driven RPC load-balancing design for architectures with hardware-terminated protocols and integrated NIs, that delivers near-optimal tail latency. RPCValet\u2019s RPC dispatch decisions emulate the theoretically optimal single-queue system, without incurring synchronization overheads currently associated with single-queue implementations. Our design improves throughput under tight tail latency goals by up to 1.4\u00d7, and reduces tail latency before saturation by up to 4\u00d7 for RPCs with \u00b5s-scale service times, as compared to current systems with hardware support for RPC load distribution. RPCValet performs within 15% of the theoretically optimal single-queue system.\nACM Reference Format: Alexandros Daglis\u2217, Mark Sutherland, and Babak Falsafi. 2019. RPCValet: NI-Driven Tail-Aware Balancing of \u00b5s-Scale RPCs. In 2019 Architectural Support for Programming Languages and Operating Systems (ASPLOS \u201919), April 13\u201317, 2019, Providence, RI, USA.ACM, New York, NY, USA, 14 pages. https://doi.org/10.1145/3297858.3304070\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ASPLOS \u201919, April 13\u201317, 2019, Providence, RI, USA \u00a9 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6240-5/19/04. . . $15.00 https://doi.org/10.1145/3297858.3304070"
                },
                {
                    "heading": "1 Introduction",
                    "text": "Modern datacenters deliver a breadth of online services to millions of daily users. In addition to their huge scale, online services come with stringent Service Level Objectives (SLOs) to guarantee responsiveness. Often expressed in terms of tail latency, SLOs target the latency of the slowest requests, and thus bound the slowest interaction a user may have with the service. Tail-tolerant computing is one of the major ongoing challenges in the datacenter space, as long-tail events are rare and rooted in convoluted hardware-software interactions. A key contributor to the well-known \"Tail at Scale\" challenge [15] is the deployment of online services\u2019 software stacks in numerous communicating tiers, where the interactions between a service\u2019s tiers take the form of Remote Procedure Calls (RPCs). Large-scale software is often built in this fashion to ensure modularity, portability, and development velocity [26]. Not only does each incoming request result in a wide fan-out of inter-tier RPCs [10, 23], each one lies directly on the critical path between the user and the online service [6, 16, 29, 50]. The amalgam of the tail latency problem with the trend towards ephemeral and fungible software tiers has created a challenge to preserve the benefits of multi-tiered software while making it tail tolerant. To lower communication overheads and tighten tail latency, there has been an intensive evolution effort in datacenter-scale networking hardware and software, away from traditional POSIX sockets and TCP/IP and towards lean userlevel protocols such as InfiniBand/RDMA [21] or dataplanes such as IX and ZygOS [7, 47]. Coupling protocol innovations with state-of-the-art hardware architectures such as Firebox [4], Scale-Out NUMA [43] or Mellanox\u2019s BlueField Smart-NIC [37], which offer tight coupling of the network interface (NI) with compute logic, promises even lower communication latency. The net result of rapid advancements in the networking world is that inter-tier communication latency will approach the fundamental lower bound of speedof-light propagation in the foreseeable future [20, 50]. The focus of optimization hence will completely shift to efficiently handling RPCs at the endpoints as soon as they are delivered from the network. The growing number of cores on server-grade CPUs [36, 38] exacerbates the challenge of distributing incoming RPCs to handler cores. Any delay or load imbalance caused by\n\u2217 This work was done while the author was at EPFL.\nthis initial stage of the RPC processing pipeline directly impacts tail latency and thus overall service quality. Modern NIC mechanisms such as Receive-Side Scaling (RSS) [42] and Flow Direction [24] offer load distribution and connection affinity, respectively. However, the key issue with these mechanisms, which apply static rules to split incoming traffic into multiple receive queues, is that they do not truly achieve load balancing across the server\u2019s cores. Any resulting load imbalance after applying these rules must be handled by system software, introducing unacceptable latency for the most latency-sensitive RPCs with \u00b5s-scale service times [47, 53]. In this paper, we propose RPCValet, a co-designed hardware and software system to achieve dynamic load balancing across CPU cores, based on the key insight that on-chip NIs offer the ability to monitor per-core load in real time and steer RPCs to lightly loaded cores. The enabler for this style of dynamic load balancing is tight CPU-NI integration, which allows fine-grained, nanosecond-scale communication between the two, unlike conventional PCIe-attached NIs. To demonstrate the benefits of our design, we first classify existing load-distribution mechanisms from both the hardware and software worlds as representative of different queuing models, and show how none of them is able to reach the performance of the theoretical best case. We then design a minimalistic set of hardware and protocol extensions to Scale-Out NUMA (soNUMA) [43], an architecture with on-chip integrated NIs, to show that a carefully architected system can indeed approach the best queuing model\u2019s performance, significantly outperforming prior load-balancing mechanisms. To summarize, our contributions include: \u2022 RPCValet, an NI-driven dynamic load-balancing design that outperforms existing hardware mechanisms for load distribution, and approaches the theoretical maximum performance predicted by queuing models. \u2022 Hardware and protocol extensions to soNUMA for native messaging support, a required feature for efficient RPC handling. We find that, in contrast to prior judgment [43], native messaging support is not disruptive to the key premise of NI hardware simplicity, which such architectures leverage to enable on-chip NI integration. \u2022 An RPCValet implementation on soNUMA that delivers near-ideal RPC throughput under strict SLOs, attaining within 3\u201316% of the theoretically optimal queuing model. For \u00b5s-scale RPCs, RPCValet outperforms software-based and RSS-like hardware-driven load distribution by 2.3\u2013 2.7\u00d7 and 29\u201376%, respectively. The paper is organized as follows: \u00a72 outlines the performance differences between multi- and single-queue systems, highlighting the challenges in balancing incoming RPCs with short service times among cores. \u00a73 presents RPCValet\u2019s design principles, followed by an implementation using soNUMA as a base architecture in \u00a74. We detail our methodology in \u00a75 and evaluate RPCValet in \u00a76. Finally, we discuss related work in \u00a77 and conclude in \u00a78."
                },
                {
                    "heading": "2 Background",
                    "text": ""
                },
                {
                    "heading": "2.1 Application and technology trends",
                    "text": "Modern online services are decomposed into deep hierarchies of mutually reliant tiers [26], which typically interact using RPCs. The deeper the software hierarchy, the shorter each RPC\u2019s runtime, as short as a few \u00b5s for common software tiers such as data stores. Fine-grained RPCs exacerbate the tail latency challenge for services with strict SLOs, as accumulated \u00b5s-scale overheads can result in a long-tail event. To mitigate the overheads of RPC-based communication, network technologies have seen renewed interest, with the InfiniBand fabric and protocol beginning to appear in datacenters [21] due to its low latency and high IOPS. With networking latency approaching the fundamental limits of propagation delays [20], any overhead added to the raw RPC processing time at a receiving server critically impacts latency. For example, while InfiniBand significantly reduces latency compared to traditional TCP/IP over Ethernet, InfiniBand adapters still remain attached to servers over PCIe, which contributes an extra \u00b5s of latency to each message [33, 43]. Efficiently handling \u00b5s-scale RPCs requires the elimination of these \u00b5s-scale overheads, which is the goal of fully integrated solutions (e.g., Firebox [4], soNUMA [43]). Such architectures employ lean, hardware-terminated network stacks and integrated NIs to achieve sub-\u00b5s inter-server communication, representing the best fit for latency-sensitive RPC services. NI integration enables rapid fine-grained interaction between the CPU, NI, and memory hierarchy, a feature leveraged previously to accelerate performance-critical operations, such as atomic data object reads from remote memory [14]. In this paper, we leverage NI integration to break existing tradeoffs in balancing RPCs across CPU cores and significantly improve throughput under SLO."
                },
                {
                    "heading": "2.2 Load Balancing: Theory",
                    "text": "To study the effect of load balancing across cores on tail latency, we conduct a first-order analysis using basic queuing theory. We model a hypothetical 16-core server after a queuing system that features a variable number of input queues and 16 serving units. Fig. 1 shows three different queuing system organizations. The notation Model Q \u00d7 U denotes a queuing system with Q FIFOs where incoming messages arrive and U serving units per FIFO. The invariant across the three illustrated models isQ \u00d7U = 16. The 16\u00d7 1 system cannot perform any load balancing; incoming requests are uniformly distributed across 16 queues, each with a single serving unit. 1 \u00d7 16 represents the most flexible option that achieves the best load balancing: all serving units pull requests from a single FIFO. Finally, 4 \u00d7 4 represents a middle ground: incoming messages are uniformly distributed across four FIFOs with four serving units each. To evaluate different queuing organizations, we employ discrete event simulationsmodeling Poisson arrivals and four\ndifferent service time distributions: fixed, uniform, exponential, and generalized extreme value (GEV). Poisson arrivals are commonly used to model the independent nature of incoming requests. \u00a75 details each distribution\u2019s parameters. Fig. 2a shows the performance of five queuing systems Q \u00d7 U with (Q,U ) = (1,16), (2,8), (4,4), (8,2), (16,1), for an exponential service time distribution. The system\u2019s achieved performance is directly connected to its ability to assign requests to idle serving units. As expected, performance is proportional toU . The best and worst performing configurations are 1 \u00d7 16 and 16 \u00d7 1 respectively, while 2 \u00d7 8, 4 \u00d7 4 and 8 \u00d7 2 lie in between these two. Fig. 2b and 2c show the relation of throughput and 99th percentile latency for the two extreme queuing system configurations, namely 1\u00d716 and 16\u00d71. As seen in Fig. 2a, 1\u00d716 significantly outperforms 16\u00d71. 16\u00d71\u2019s inability to assign requests to idle cores results in higher tail latencies and a peak throughput 25\u201373% lower than 1 \u00d7 16 under a tail latency SLO at 10\u00d7 the mean service time S\u0304 . In addition, the degree of performance degradation is affected by the service time distribution. For both queuing models, we observe that the higher a distribution\u2019s variance, the higher the tail latency (TL) before the saturation point is reached, hence TLf ixed < TLuni < TLexp < TLGEV . Also, the higher the distribution\u2019s variance, the more dramatic the performance gap between 1 \u00d7 16 and 16 \u00d7 1, as is clearly seen for GEV.\nThe application\u2019s service time distribution is beyond an architect\u2019s control, as it is affected by numerous software and hardware factors. However, they can control the queuing model that the underlying system implements. The theoretical results suggest that systems should implement a queuing configuration that is as close as possible to a single-queue (1 \u00d7 16) configuration."
                },
                {
                    "heading": "2.3 Load Balancing: Practice",
                    "text": "A subtlety not captured by our queuing models is the practical overhead associated with sharing resources (i.e., the input queue). In a manycore CPU, allowing all the cores to pull incoming network messages from a single queue requires synchronization. We refer to this RPC dispatch mode\nas \"pull-based\". Especially for short-lived RPCs, with service times of a few \u00b5s, such synchronization represents significant overhead. Architectures that share a pool of connections between cores have this pitfall; common examples include using variants of Linux\u2019s poll system call, or locked event queues supported by libevent. An alternative approach for distributing load to multiple cores, advocated by recent research, is dedicating a private queue of incoming network messages to each core [7, 45]. Although this design choice corresponds to a rigid N \u00d7 1 queuing model (N being the number of cores), it completely eschews overheads related to sharing (i.e., synchronization and coherence), delivering significant throughput gains. By leveraging RSS [42] inside the NI, messages are consistently distributed at arrival time to one of the N input queues. This ultimately results in a different mode of communication: instead of the cores pullingmessages from a single queue, the NI hardware actively pushesmessages into each core\u2019s queue. We refer to this load distribution mode as \"push-based\".\nFlexNIC [30] extends the push-based model by proposing a P4-inspired domain-specific language, allowing software to install match-action rules into the NI. Despite their many differences, both FlexNIC and RSS completely rely on decisions based on the RPC packets\u2019 header content. Whether configured statically or by the application, push-based load distribution still fundamentally embodies a multi-queue system vulnerable to load imbalance, as no information pertaining to the system\u2019s current load is taken into account. \u00a72.2\u2019s queuing models demonstrate the effect of this imbalance as compared to a system with balanced queues.\nThe two aforementioned approaches to load distribution, pull- and push-based, represent a tradeoff between synchronization and load imbalance. In this paper, we leverage the onchip NI logic featured in emerging fully integrated architectures such as soNUMA [43] to introduce a novel push-based NI-driven load-balancing mechanism capable of breaking that tradeoff by making dynamic load-balancing decisions.\n3 RPCValet Load-Balancing Design This section describes the insights and foundations guiding RPCValet\u2019s design. Our goal is to achieve a synchronizationfree system that behaves like the theoretical best singlequeue model. We begin by setting forth our basic assumptions about the underlying hardware and software, then explain the roadblocks to achieving dynamic load balancing, and conclude with the principles of RPCValet\u2019s design."
                },
                {
                    "heading": "3.1 Basic Architecture",
                    "text": "We design RPCValet for emerging architectures featuring fully integrated NIs and hardware-terminated transport protocols.We target these architectures for two reasons. First, an important class of online services exhibits RPCs with service times that are frequently only a few \u00b5s long. For example,\nthe average service time for Memcached [2] is \u223c 2\u00b5s [47]. Even software with functionality richer than simple data retrieval can exhibit \u00b5s-scale service times: the average TPC-C query service time on the Silo in-memory database [53] is only 33\u00b5s [47]. Software tiers with such short service times necessitate network architectures optimized for the lowest possible latency, using techniques such as kernel bypass and polling rather than receiving interrupts.\nSecond, unpredictable tail-inducing events for these shortlived RPCs often disrupt application execution for periods of time that are comparable to the RPCs themselves [6]. For example, the extra latency imposed by TLB misses or context switches spans from a few hundred ns to a few \u00b5s. At such fine granularities, any load-balancing policy implemented at the distal end of an I/O-attached NI is simply too far from the CPU cores to adjust its load dispatch decisions appropriately. Therefore, we argue that mitigating load imbalance at the \u00b5s level requires \u00b5s-optimized hardware.\nThe critical feature of our \u00b5s-optimized hardware is a fully integrated NI with direct access to the server\u2019s memory hierarchy, eliminating costly roundtrips over traditional I/O fabrics such as PCIe. Each server registers a part of its DRAM in advance with a particular context that is then exported to all participating servers, creating a partitioned global address space (PGAS) where every server can read/write remote memory in RDMA fashion. The architecture\u2019s programming model is a concrete instantiation of the Virtual Interface Architecture (VIA) [18], where each CPU core communicates with the NI through memory-mapped queue pairs (QPs). Each QP consists of a Work Queue (WQ) where the core writes entries (WQEs) to be processed by the NI, and a Completion Queue (CQ), where the NI writes entries (CQEs) to indicate that the cores\u2019 WQEs were completed. For more details, refer to the original VIA [18] and soNUMA [43] work."
                },
                {
                    "heading": "3.2 NI Integration: The Key Enabler",
                    "text": "The NI\u2019s integration on the same piece of silicon as the CPU is the key enabler for handling \u00b5s-scale events. By leveraging the fact that such integration enables fine-grained real-time (nanosecond-scale) information to be passed back and forth between the NI and the server\u2019s CPU, the NI has the ability to\nrespond to rapidly changing load levels and make dynamic load-balancing decisions. To illustrate the importance of ns-scale interactions, consider a data serving tier such as Redis [3], maintaining a sorted array in memory. Since the implementation of its sorted list container uses a skip list to provide add/remove operations in O (lo\u0434 (N )) time, an RPC to add a new entry may incur multiple TLB misses, stalling the core for a few \u00b5s while new translations are installed. While this core is stalled on the TLB miss(es), it is best to dispatch RPCs to other available cores on the server.\nAn integrated NI can, with proper hardware support, monitor each core\u2019s state and steer RPCs to the least loaded cores. Such monitoring is implausible without NI integration, as the latency of transferring load information over an I/O bus (e.g., \u223c 1.5\u00b5s for a 3-hop posted PCIe transaction) would mean that the NI will make delayed\u2014hence sub-optimal, or even wrong\u2014decisions until the information arrives.\nThe active feedback of information from the server\u2019s compute units (which are not restricted to CPU cores) to the NI can take many forms, ranging from monitoring memory hierarchy events to metadata directly exposed by the application. Regardless of the exact policy, the underlying enabler for RPCValet\u2019s ability to handle \u00b5s-scale load imbalance is that load dispatch decisions are driven by an integrated NI."
                },
                {
                    "heading": "3.3 Design Principles",
                    "text": "Our design goal is to break the tradeoff between the load imbalance inherent in multi-queue systems and the synchronization associated with pulling load from a single queue. To begin, we retain the VIA\u2019s design principle of allocating a single virtual interface (identical to a QP in IB/soNUMA terminology) to each participating thread, which is critically important for handling \u00b5s-scale RPCs. Registering independent QPs with the NI helps us achieve the goal of eliminating synchronization, as each thread polls on its own QP and waits for the arrival of new RPCs. This simplifies the load-balancing problem to simply choosing the correct QP to dispatch the RPC to. By allowing the NI to choose the QP at message arrival time, based on one of the many possible heuristics for estimating per-core load, our design achieves the goal of synchronization-free push-based load balancing.\nUnfortunately, realizing such a design with our baseline architecture (\u00a7 3.1) is not possible, as existing primitives are not expressive enough for push-based dispatch. In particular, architectures with on-chip NIs such as soNUMA [43] do not provide native support for messaging operations, favoring RDMA operations for hardware simplicity that facilitates NI integration. These RDMA operations (a.k.a. \"one-sided\" ops) enable direct read/write access of remote memory locations, without involving a CPU at the remote end. Hence, a reception of a one-sided op is not associated with a creation of a CPU notification event by the NI. Messaging can be emulated on top of one-sided ops by allocating shared bounded buffers in the PGAS [17, 27, 43], into which threads directly place messages using one-sided writes. Fig. 3a illustrates the high-level operation of emulated messaging. As emulated messaging is performed in a connection-oriented fashion from thread to thread, each RPC-handling thread allocates N bounded buffers, each with S message slots; N is the number of nodes that can send messages. Each of the C cores polls at the head slots of its corresponding N buffers for incoming RPCs.\nThe fundamental drawback of such emulated messaging is that the sending thread implicitly determines which thread at the remote end will process its RPC request, because the memory location the RPC is written to is tied to a specific thread. The result is a multi-queue system, vulnerable to load imbalance. Although it may be possible to implement some form of load-aware messaging (e.g., per-thread client-server flow control), such mechanisms will have little to no benefit due to the relatively high network round-trip time for load information to diffuse between the two endpoints, especially when serving short-lived RPCs.\nA key reason why, in the case of emulated messaging, the NI at the destination cannot affect the a priori assignment of an incoming RPC to a thread is that the protocol does not enable the NI to distinguish a \"message\" (i.e., a one-sided write triggering two-sided communication) from a default one-sided op. Protocol support for native messaging with innate semantics of two-sided operations overcomes this limitation and enables the NI at the message\u2019s destination node\nto perform push-based load balancing. Fig. 3b demonstrates RPCValet\u2019s high-level operation. The NI first writes every incoming message into a single PGAS-resident message buffer of N \u00d7 S slots, as in the case of emulated messaging. Then, the NI uses a selected core\u2019s QP to notify it to process the incoming RPC request. In effect, RPCValet decouples a message\u2019s arrival and location in memory from its assignment to a core for processing, thus achieving the best of both worlds: the load-balancing flexibility of a single-queue system, and the synchronization-free, zero-copy behavior of partitioned multi-queue architectures. Fig. 3b demonstrates how NI-driven dynamic dispatch decisions result in balanced load, in contrast to Fig. 3a\u2019s example. In conclusion, RPCValet requires extensions to both the on-chip NI hardware and the networking protocol, to first provide support for native messaging and, second, realize dynamic load-balancing decisions. In the following section, we describe an implementation of an architecture featuring both of these mechanisms.\n4 RPCValet Implementation In this section, we describe our RPCValet implementation as an extension of the soNUMA architecture [43], including a lightweight extension of the baseline protocol for native messaging and support for NI-driven load balancing."
                },
                {
                    "heading": "4.1 Scale-Out NUMA with Manycore NI",
                    "text": "soNUMA enables rapid remote memory access through a lean hardware-terminated protocol and on-chip NI integration. soNUMA deploys a QP interface for CPU-NI interaction (\u00a73.1) and leverages on-chip cache coherence to accelerate QP-entry transfers between the CPU and NI.\nFig. 4 shows soNUMA\u2019s scalable NI architecture for manycore CPUs [13]. The conventionally monolithic NI is split into two heterogeneous parts, a frontend and a backend. The frontend is the \"control\" component, and is collocated with each core to drastically accelerate QP interactions. The backend is replicated across the chip\u2019s edge, to scale the NI\u2019s capability with growing network bandwidth, and handles all data and network packets. Pairs of frontend and backend entities, which together logically comprise a complete\nNI, communicate with special packets over the chip\u2019s interconnect. Our RPCValet implementation relies on such a Manycore NI architecture."
                },
                {
                    "heading": "4.2 Lightweight Native Messaging",
                    "text": "We devise a lightweight implementation of native messaging as a required building block for dynamic load-balancing decisions at the NI. A key difficulty to overcome is support for multi-packet messages, that must be reassembled by the destination NI. This goal conflicts with soNUMA\u2019s stateless request-response protocol, which unrolls large requests into independent packets each carrying a single cache block payload. Emulated messaging (see \u00a73.3) does not require any reassembly at the destination, because all packets are directly written to the bounded buffer specified by the sender.\nOne workaround to avoid message reassembly complications would be to limit the maximum message size to the link layer\u2019s MTU. Prior work has adopted this approach to build an RPC framework on an IB cluster [27]. Such a design choice may be an acceptable limitation for IB networks which have a relatively large MTU of 4KB. However, fully integrated solutions with on-chip NIs will likely feature small MTUs (e.g., a single cache line in soNUMA), so limiting the maximum message size to the link-layer MTU is impractical.\nOur approach to avoiding the hardware overheads associated with message reassembly is keeping the buffer provisioning of the emulated messaging mechanism, which allows the sender to determine the memory location the message will be written to. Therefore, soNUMA\u2019s request-response protocol can still handle the message as a series of independent cache-block-sized writes to the requester-specified memory location. While this mechanism may seem identical to one-sided operations, we introduce a new pair of send and replenish operations which expose the semantics of multi-packet messages to the NI\u2014it can then distinguish true one-sided operations from messaging operations, which are eligible for load balancing. The NI keeps track of packet receptions belonging to a send, deduces when it has been fully received, and then hands it off to a core for processing.\nFig. 5 shows the delivery of amessage fromNode 0 to Node 1 in steps. Completing the message delivery requires the execution of a send operation on Node 0 and a replenish operation on Node 1. Fig. 5 only shows NI backends; NI frontends are collocated with every core. We start with the required buffer provisioning associated with messaging.\nBuffer provisioning. We introduce the notion of a messaging domain, which includes N nodes that can exchange messages and is defined by a pair of buffers allocated in each node\u2019s memory, the send buffer and the receive buffer. The send buffer comprises N \u00d7 S slots, as described in \u00a73.3. Fig. 5a illustrates a send buffer with S=3 and different shades of gray distinguishing the send slots per participating node. Each send slot contains bookkeeping information for the local cores to keep track of their outstanding messages. It contains a valid bit, indicating whether the send slot is currently being used, a pointer to a buffer in local memory containing the message\u2019s payload, and a field indicating the size of the payload to be sent. A separate in-memory data structure maintains the head pointer for each of the N sets of send slots, which the cores use to atomically enqueue new send requests (not shown). The receive buffer, shown in Fig. 5b, is the dual of the send buffer, where incoming send messages from remote nodes end up, and is sized similarly (N \u00d7 S receive slots). Unlike send slots, receive slots are sized to accommodate message payloads. Each receive slot also contains a counter field, used to determine whether all of a message\u2019s packets have arrived. The counter field should provide enough bits to represent the number of cache blocks comprising the largest message; we overprovision by allocating a full cache block (64B), to avoid unaligned accesses for incoming payloads.\nOverall, the messaging mechanism\u2019s memory footprint is 32 \u00d7 N \u00d7 S + (max_ms\u0434_size + 64) \u00d7 N \u00d7 S bytes. We expect that for current deployments, that number should not exceed a few tens of MBs. Systems adopting fully integrated solutions will likely be of contained scale (e.g., rack-scale systems), featuring a few hundred nodes, hence bounding the N parameter. In addition, most communication-intensive\nlatency-sensitive applications send small messages, boundingmax_ms\u0434_size . For instance, the vast majority of objects in object stores like Memcached are <500B [5], while 90% of all packets sent within Facebook\u2019s datacenters are smaller than 1KB [49]. Finally, given the low network latency fully integrated solutions like soNUMA deliver, the number of concurrent outstanding requests S required to sustain peak throughput per node pair would be modest (a few tens). Dynamic buffer management mechanisms to reduce memory footprint are possible, but beyond the scope of this paper.\nImportantly, a fixedmax_ms\u0434_size does not preclude the exchange of larger messages altogether. A rendezvous mechanism [51] can be used, where the sending node\u2019s initial message specifies the location and size of the data, and the receiving node uses a one-sided read operation to directly pull the message\u2019s payload from the sending node\u2019s memory.\nSend operation. Sending a message to a remote node involves the following steps. First, the core writes the message in a local core-private buffer (Fig. 5a, 1 ), updates the tail entry of the send buffer set corresponding to the target node (e.g., Node 1) 2 and enqueues a send operation in its private WQ 3 . The send operation specifies a messaging domain, the target node id, the remote receive buffer slot\u2019s address, a pointer to the local buffer containing the outgoing message, and the message\u2019s size. The target receive buffer slot\u2019s address can be trivially computed, as the number of nodes in the messaging domain, the number of send/receive slots per node, and themax_ms\u0434_size are all defined at the messaging domain\u2019s setup time. The NI polls on the WQ 4 , parses the command, reads the message from the local memory buffer 5 , and sends it to the destination node. At the destination, the NI writes each send packet directly in the local memory hierarchy, into the specified receive slot, and increments that receive slot\u2019s counter (Fig. 5b, 6 ). When the counter matches the send operation\u2019s total packet count (contained in each packet\u2019s header), the NI writes a message arrival notification entry in a shared CQ 7 . The shared CQ is a memory-mapped and cacheable FIFO where the NI enqueues pointers to received send requests. When it is time for a dispatch decision, the NI selects a core and assigns the head entry of the shared CQ to it by writing the receive slot\u2019s index, contained in the shared CQ entry, into that core\u2019s corresponding CQ 8 . This is a crucial step that enables RPCValet\u2019s NI-driven dynamic load balancing, which we expand in \u00a74.3. Finally, the core receives the new send request 9 polling the head of its private CQ, then directly reads the message from the receive buffer and processes it.\nReplenish operation. A replenish operation always follows the receipt of a send operation as a form of end-toend flow control: a replenish notifies the send operation\u2019s source node that the request has been processed and hence its corresponding send buffer slot is free and can be reused. In Fig. 5b\u2019s example, when core 3 is done processing the\nsend request, it enqueues a replenish in its private WQ A . The replenish only contains the target node and the target send buffer slot\u2019s address, trivially deduced from the receive buffer index the corresponding send was retrieved from. The NI, which is polling at the head of core 3\u2019s WQ, reads the new replenish request B and sends the message to node 0. When the replenish message arrives at node 0, the NI invalidates the corresponding send buffer slot by resetting its valid field (Fig. 5a, C ), indicating its availability to be reused. In practice, a replenish operation is syntactic sugar for a special remote write operation, which resets the valid field of a send buffer slot."
                },
                {
                    "heading": "4.3 NI-driven Dynamic Load Balancing",
                    "text": "With the NI\u2019s newly added ability to recognize and manage message arrivals, we now proceed to introduce NI-driven dynamic load balancing. Load-balancing policies implemented by the NIs can be sophisticated and can take various affinities and parameters into account (e.g., certain types of RPCs serviced by specific cores, or data-locality awareness). Implementations can range from simple hardwired logic to microcoded state machines. However, we opt to keep a simple proof-of-concept design, to illustrate the feasibility and effectiveness of load-balancing decisions at the NIs and demonstrate that we can achieve the load-balancing quality of a single-queue system without synchronization overheads. Fig. 5b\u2019s step 8 is the crucial step that determines the balancing of incoming requests to cores. In RPCValet, the receiving node\u2019s NI keeps track of the number of outstanding send requests assigned to each core. Receiving a replenish operation from a core implies that the core is done processing a previously assigned send. Allowing only one outstanding request per core and dispatching a new request only after receiving a notification of the previous one\u2019s completion corresponds to true single-queue system behavior, but leaves a small execution bubble at the core. The bubble can be eliminated by setting the number of outstanding requests per core to two. We found that introducing a small multiqueue effect is offset by eliminating the bubble, resulting in marginal performance gains for ultra-fast RPCs with service times of a few 100s of nanoseconds. A challenge that emerges from the distributed nature of a Manycore NI architecture is that the otherwise independent NI backends, each of which is handling send message arrivals from the network, need to coordinate to balance incoming load across cores. Our proposed solution is simple, yet effective: centralize the last step of message reception and dispatch. One of the NI backends\u2014henceforth referred to as the NI dispatcher\u2014is statically assigned to handle message dispatch to all the available cores. Network packet and data handling still benefit from the parallelism offered by the Manycore NI architecture, as all NI backends still independently handle incoming network packets and access memory\ndirectly. However, once an NI backendwrites all packets comprising a message in their corresponding receive buffer slots, it creates a special message completion packet and forwards it to the NI dispatcher over the on-chip interconnect. Once the NI dispatcher receives the message completion packet, it enqueues the information in the shared CQ, from which it dispatches messages to cores in FIFO order as soon as it receives a replenish operation. As all the incoming messages are dispatched from a single queue to all available cores, RPCValet behaves like a true single-queue queuing system.\nHaving a single NI dispatcher eschews software synchronization, but raises scalability questions. However, for modern server processor core counts, the required dispatch throughput should be easily sustainable by a single centralized hardware unit, while the additional latency due to the indirection from any NI backend to the NI dispatcher is negligible. From the throughput perspective, even an RPC service time as low as 500ns corresponds to a new dispatch decision every \u223c31/8ns for a 16/64-core chip, respectively. Both dispatch frequencies are modest enough for a single hardware dispatch component to handle, especially for our simple greedy dispatch implementation. The same observation also holds for more sophisticated dispatch policies if their hardware implementation can be pipelined. Latency-wise, the indirection from any NI backend to the NI dispatcher costs a couple of on-chip interconnect hops, adding just a few ns to the end-to-end message delivery latency. In case of exotic system deployments where the above assumptions do not hold, an intermediary design point is possible where each NI backend can dispatch to a limited subset of cores on the chip. As an example of this design point, we also implement and evaluate a 4 \u00d7 4 queuing system in \u00a76.\n4.4 soNUMA Extensions for RPCValet We now briefly summarize the modifications to soNUMA\u2019s hardware to enable RPCValet, including the necessary protocol extensions for messaging and load balancing. Load balancing itself is transparent to the protocol and only affects a pipeline stage in the NI backends.\nAdditional hardware state. Most of the state required for messaging (i.e., send/receive buffers) is allocated in host memory. The only metadata kept in dedicated SRAM are the send and receive buffers\u2019 location and size, as they require constant fast access. On each node, the maintained state per registered soNUMA context includes a memory address range per node and a QP per local core. In total, we add 20B of stored state per context, including: the base virtual addresses for the send/receive buffers, the maximum message size (max_ms\u0434_size), the # of nodes (N ) in the messaging domain, and the # of messaging slots (S) per node.\nHardware logic extensions. soNUMA\u2019s NI features three distinct pipelines for handling Request Generations, Request Completions, and Remote Request Processing, respectively\n[43]. We extend these pipelines to support the new messaging primitives and load-balancing functionality. Receiving a new send or replenish request is very similar to the reception of a remote write operation in the original soNUMA design. To support our native messaging design, we add a field containing the total message size to the network layer header; this is necessary so the NI hardware can identify when all of a message\u2019s packets have been received.\nWe add five new stages to the NI pipelines in total. A new stage in Request Generation differentiates between send and replenish operations, and operates on the messaging domain metadata. All other modifications are limited to the Remote Request Processing Pipeline, which is only replicated across NI backends. When a send is received, the pipeline performs a fetch-and-increment operation to the corresponding counter field of the target receive buffer slot (\u00a74.2, \"Send operation\"). The next stage checks if the counter\u2019s new value matches the message\u2019s length, carried in each packet header. If all of the send operation\u2019s packets have arrived, the next stage enqueues a pointer to the corresponding receive buffer slot in the shared CQ. The final stage added to the Remote Request Processing pipeline, Dispatch, keeps track of the number of outstanding requests assigned to each core and determines when and to which core to dispatch send requests to from the shared CQ. A core is \"available\" when its number of outstanding requests is below the threshold defined; in our implementation, this number is two. Whenever there is an available core, the Dispatch stage dequeues the shared CQ\u2019s first entry and sends it to the target core\u2019s NI frontend, where the Request Completion pipeline writes it into the core\u2019s private CQ. The complexity of the Dispatch stage is very simple for our greedy algorithm, but varies based on the logic and algorithm involved in making load-balancing decisions. Finally, after completing the request, the core signals its availability by enqueuing a replenish operation in its WQ, which is propagated by the core\u2019s NI frontend to the NI backend that originally dispatched the request. In summary, the additional hardware complexity is modest, thus compatible with architectures featuring ultra-lightweight protocols and on-chip integratedNIs, such as soNUMA. Given the on-chip NI\u2019s fast access to its local memory hierarchy, it is possible to virtualizemost of the bulky state required for the messaging mechanism\u2019s send and receive buffers in the host\u2019s memory. The dedicated hardware requirements are limited to a small increase in SRAM capacity, while the NI logic extensions are contained and straightforward."
                },
                {
                    "heading": "5 Methodology",
                    "text": "We now detail our methodology for evaluating RPCValet\u2019s effectiveness in balancing load transparently in hardware.\nSystem organization. We model a single tiled 16-core chip implementing soNUMA with a Manycore NI, as illustrated\nin Fig. 4. The modeled chip is part of a 200-node cluster, with remote nodes emulated by a traffic generator which creates synthetic send requests following Poisson arrival rates, from randomly selected nodes of the cluster. The traffic generator also generates synthetic replies to the modeled chip\u2019s outgoing requests. We use Flexus [54] cycle-accurate simulation with Table 1\u2019s parameters.\nMicrobenchmark. Weuse amultithreadedmicrobenchmark that emulates different service time distributions, where each thread executes the following actions in a loop: (i) spins on its CQ, until a new send request arrives; (ii) emulates the execution of an RPC by spending processing time X , whereX follows a given distribution as detailed below; (iii) generates a synthetic RPC reply, which is sent back to the requester using a send operation with a 512B payload; and (iv) issues a replenish corresponding to the processed send request, marking the end of of the incoming RPC\u2019s processing. The overall service time for an emulated RPC (i.e., the total time a core is occupied) is the sum of steps (ii) to (iv).\nRPC processing time distributions. To evaluate RPCValet on a range of RPC profiles, we utilize processing time distributions generated with three different methods. First, we develop an RPC processing time generator that samples values from a selected distribution. We experiment with four different distributions: fixed, uniform, exponential, and GEV. Fixed represents the ideal case, where all requests take the same processing time. GEV represents a more challenging case with infrequent long tails, which may arise from events like page faults or interrupts. Uniform and exponential distributions fall between fixed and GEV in terms of impact on load balancing, as established in Fig. 2. For our synthetic processing time distributions, we use 300ns as a base latency and add an extra 300ns on average, following one of the four distributions. The parameters we use for GEV are (location, scale, shape) = (363, 100, 0.65), which result in a mean of 600 cycles (i.e., 300ns at 2GHz) [1]. Fig. 6a illustrates the PDFs of the four resulting processing time distributions.\nSecond, we run the HERD [27] key-value store and collect the distribution of the RPCs\u2019 processing times. We use a dualsocket Xeon E5-2680 Haswell server and pin 12 threads on an equal number of a single socket\u2019s physical cores. The second socket\u2019s cores generate load. Our parameters for HERD are:\n0 500 1000 0.00\n0.25\n0.50\n0.75\n1.00 \u00d710\u22122\nm ean\n(a) Synthetic\nGEV Uniform Exp\n0 500 1000\n(b) HERD\nm ean\n0.0 0.2 0.4 0.6 .8 1. Processing Time (ns)\npr ob\nab ili\nty (1\ne2)\n95/5% read/write query mix, uniform key popularity, and a 4GB dataset (256MB per thread). Fig. 6b displays a histogram of HERD\u2019s RPC processing times after the request has exited the network, which have a mean of 330ns. Finally, we evaluate the Masstree data store [40], which stores key-value pairs in a trie-like structure and supports ordered scans in addition to put/get operations. Ordered scans are common in database/analytics applications and compete with latency-critical operations for CPU time when accessing the same data store. To collect RPC processing times, we use the same platform and dataset we used for HERD and load the server with 99% single-key gets, interleaved with 1% long-running scans which return 100 consecutive keys. The resulting distribution for gets is shown in Figure 6c and has an average of 1.25 \u00b5s. The runtime of scans is 60\u2013120 \u00b5s (not shown in Fig. 6c due to the X-axis bounds).\nLoad-balancing implementations. We first compare the performance of two RPCValet variants, 1 \u00d7 16 and the less flexible 4 \u00d7 4. In 4 \u00d7 4, each NI backend is limited to balancing load across the four cores corresponding to its on-chip network row. We also consider a 16\u00d7 1 system, representing partitioned dataplanes where every incoming message is assigned to a core at arrival timewithout any rebalancing. 16\u00d71 is the only currently existing NI-driven load distribution mechanism. Next, we compare the best-performing hardware load-balancing implementation, 1 \u00d7 16, to a software-based counterpart. In our software implementation, NIs enqueue incoming send requests into a single CQ from which all 16 threads pull requests in FIFO order. We use an MCS queuebased lock [41] for the shared request queue.\nWe assume a 99th percentile Service Level Objective (SLO) of \u2a7d 10\u00d7 the mean service time S\u0304 we measure in each experiment and evaluate all configurations in terms of throughput under SLO. We measure each request\u2019s latency as the time from the reception of a send message until the thread that services the request posts a replenish operation."
                },
                {
                    "heading": "6 Evaluation",
                    "text": ""
                },
                {
                    "heading": "6.1 Load Balancing: Hardware Queuing Systems",
                    "text": "Fig. 7a shows the performance of HERD with each of the three evaluatedNI-driven load-balancing configurations.With a resulting S\u0304 of \u223c 550ns, 1 \u00d7 16 delivers 29MRPS, 1.16\u00d7 and 1.18\u00d7 higher throughput than 4 \u00d7 4 and 16 \u00d7 1, respectively. 1 \u00d7 16 consistently delivers the best performance, thanks to its superior flexibility in dynamically balancing load across all 16 available cores. In comparison, 4 \u00d7 4 offers limited flexibility, while 16 \u00d7 1 offers none at all. The flexibility to balance load from a single queue to multiple cores not only results in higher peak throughput under SLO, but also up to 4\u00d7 lower tail latency before reaching saturation load. Conversely, lower tail means that the throughput gap between RPCValet and 1 \u00d7 16 would be larger for SLOs stricter than the assumed 10 \u00d7 S\u0304 . Note that data points appearing slightly lower at mid load as compared to low load in Fig. 7a is a measurement artifact: for low arrival rates, the relatively small number of completed requests during our simulation\u2019s duration results in reduced tail calculation accuracy. Fig. 7b shows the tail latency of Masstree\u2019s get operations with each queuing configuration. We set the SLO for Masstree at 10\u00d7 the service time of the get operations, equalling 12.5\u00b5s; we do not consider the scan operations latency critical. Due to interference from the scans, 16\u00d71 cannot meet the SLO even for the lowest arrival rate of 2MRPS, while even 4 \u00d7 4 quickly violates the SLO at 3MRPS. 1 \u00d7 16 delivers 4.1MRPS at SLO, outperforming 4\u00d7 4 by 37%. Under a more relaxed SLO of 75\u00b5s, RPCValet\u2019s 1 \u00d7 16 configuration delivers 54% higher throughput than 16 \u00d7 1 and 20% higher than 4\u00d74. In the presence of long-running scans that occupy cores for many \u00b5s, RPCValet leverages occupancy feedback from the cores to eliminate excess queuing of latency-critical gets and improve throughput under SLO. Fig. 7c shows the results for two of our synthetic service time distributions, fixed and GEV. The results for uniform and exponential distributions fall between these two, are omitted for brevity, and are available in [12]. The results follow the expectations set in \u00a72.2. For the fixed distribution, 1 \u00d7 16 delivers 1.13\u00d7 and 1.2\u00d7 higher throughput than 4 \u00d7 4\nand 16 \u00d7 1 under SLO, respectively. For GEV, the throughput improvement grows to 1.17\u00d7 and 1.4\u00d7, respectively. Similar to HERD results, in addition to throughput gains, RPCValet also delivers up to 4\u00d7 lower tail latency before saturation. In all of Fig. 7\u2019s experiments we set RPCValet\u2019s number of outstanding requests per core to two (see \u00a74.3). Reducing this to one marginally degrades HERD\u2019s throughput, because of its short sub-\u00b5s service times, but has no measurable performance difference in the rest of our experiments. In conclusion, RPCValet significantly improves system throughput under tight tail latency goals. Implementations that enable request dispatch to all available cores (i.e., 1\u00d7 16) deliver the best performance. However, even implementations with limited balancing flexibility, such as 4 \u00d7 4, are competitive. As realizing a true single-queue system incurs additional design complexity, such limited-flexibility alternatives introduce viable options for system designers willing to sacrifice some performance in favor of simplicity."
                },
                {
                    "heading": "6.2 Hardware Versus Software Load Balancing",
                    "text": "Fig. 8 compares the performance of RPCValet to a software implementation, both of which implement the same theoretically optimal queuing system (i.e., 1 \u00d7 16). The difference between the two is how load is dispatched to a core. Software requires a synchronization primitive (in our case, an MCS lock) for cores to atomically pull incoming requests from the queue. In contrast, RPCValet does not incur any synchronization costs, as dispatch is driven by the NI.\nThe software implementation is competitive with the hardware implementation at low load, but because of contention on the single lock, it saturates significantly faster. As a result, our hardware implementation delivers 2.3\u20132.7\u00d7 higher throughput under SLO, depending on the request processing time distribution. A comparison between Fig. 7b and 8 reveals that the 1\u00d716 software implementation is not only inferior to the 1\u00d716 hardware implementation, but to all of the evaluated hardware implementations. The fact that even the 16 \u00d7 1 hardware implementation is superior to the software 1 \u00d7 16 implementation indicates that the software synchronization costs outweigh the dispatch flexibility they provide,\na direct consequence of the \u00b5s-scale RPCs we focus on. In addition, we corroborate the findings of prior work on dataplanes [7, 47]\u2014which effectively build a 16 \u00d7 1 system using RSS\u2014showing that elimination of software synchronization from the critical path offsets the resulting load imbalance."
                },
                {
                    "heading": "6.3 Comparison to Queuing Model",
                    "text": "Our results in \u00a76.1 qualitatively meet the expectations set by the queuing analysis presented in \u00a72.2. We now quantitatively compare the obtained results to the ones expected from purely theoretical models, to determine the performance gap between RPCValet and the theoretical 1 \u00d7 16 system.\nTo make RPCValet measurements comparable to the theoretical queuing results, we devise the following methodology. We measure the mean service time S\u0304 on our implementation; a part D of this service time is synthetically generated to follow one of the distributions in \u00a75, and the rest, S\u0304 \u2212 D, is spent on the rest of the microbenchmark\u2019s code (e.g., event loop, executing send for the RPC response and replenish to free the RPC slot). We conservatively assume that this S\u0304 \u2212 D part of the service time follows a fixed distribution. Using discrete-event simulation, we model and evaluate the performance of theoretical queuing systems with a service time S\u0304 , where DS\u0304 of the service time follows a certain distribution (fixed, uniform, exponential, GEV) and S\u0304\u2212DS\u0304 of the service time is fixed. Fig. 9 compares RPCValet to the theoretical 1 \u00d7 16. The graphs show the 99th percentile latency as a function of offered load, with four different distributions for the D part of the service time. RPCValet performs as close as 3% to 1 \u00d7 16, and within 15% in the worst case (GEV). We attribute the gap between the implementation and the model to contention that emerges under high load in the implemented systems, which is not captured by the model. Furthermore, assuming a fixed service time distribution for the S\u0304 \u2212 D part of the service time is a conservative simplifying assumption: modeling variable latency for this component would have a detrimental effect on the performance predicted by the model, thus shrinking the gap between the model and the implementation. In conclusion, RPCValet leaves no significant room for improvement; neither centralizing dispatch nor maintaining private request queues per core introduces performance concerns."
                },
                {
                    "heading": "7 Related Work",
                    "text": "Other Techniques toReduce Tail Latency. Priorwork aiming to control the tail latency of Web services deployed at datacenter scale introduced techniques that duplicate/hedge requests across multiple servers hosting replicated data [15]. The goal of such replication is to shrink the probability of an RPC experiencing a long-latency event and consequently affecting the response latency of its originating request. A natural side-effect of replication is the execution of more requests than strictly necessary, also necessitating extra serverside logic to reduce the load added by duplicated requests. As compared to ms-scale applications where the network RTT is a negligible latency contributor, applying the same technique for \u00b5s-scale applications requires a more aggressive duplication of requests, further increasing the generation of unnecessary server load. In contrast to such client-side techniques, RPCValet\u2019s server-side operation offers an alternative that does not increase the global server load. RPCValet improves tail latency by minimizing the effect of queuing. Queuing is only one of many sources of tail latency, which lie in all layers of the server\u2019s software stack. Therefore, no single solution can wholly address the tail challenge; a synergy of many techniques is necessary, each targeting specific issues in particular layers (e.g., IX [7] targets protocol and interrupt processing). However, despite the complex nature of the problem, managing on-server queuing is a universal approach that helps mitigate all sources of tail latency. Our work does not prevent straggler RPCs, but eliminates the chance that such stragglers will cascadingly impact the latency of other queued RPCs by providing a true single-queue system on each RPC-handling server. RPCValet is synergistic with techniques on both clients and servers to address specific sources of tail latency in the workflow of serving RPCs. A range of prior work also leverages queuing insights to balance web requests within a datacenter, by mainly focusing on algorithmic aspects of load distribution among backend servers rather than a single server\u2019s cores. Examples of such algorithms are Join-Shortest-Queue [22], Power-of-d [9], and Join-Idle-Queue [39]. Pegasus [34] is a rack-scale solution where the ToR switch applies load-aware request scheduling by either estimating per-server load, or by leveraging load statistics reported directly by the servers. In the context of balancing \u00b5s-scale RPCs among a single server\u2019s cores, challenges such as dispatcher-to-core latency are of minor importance, because of the integrated NI\u2019s proximity. Our results show that single-queue behavior is feasible by deferring dispatch until a core is free, which is unattainable at cluster scale due to the latency of the off-chip network.\nLoad Distribution Frameworks. Most modern NICs distribute load between multiple hardware queues, which can be privately assigned to cores, through Receive Side Scaling (RSS) [42] or Flow Director [24]. Systems like IX [7] and\nMICA [35] leverage these mechanisms to significantly boost their throughput under tail latency constraints. The disadvantage of RSS/Flow Director is that they blindly spread load across multiple receive queues based on specific network packet header fields, and are oblivious to load imbalance. ZygOS [47] ameliorates the shortcomings of partitioned dataplanes, which suffer from increased tail latency under load imbalance. ZygOS introduces an intermediate shuffling layer where idle CPU threads can performwork stealing from other input queues. Due to the added synchronization overhead of work stealing, there is a measurable performance gap between ZygOS and the best single-queue system, inversely proportional to the RPC service times. RPCValet achieves the best of both worlds, offering single-queue performance without synchronization; instead of adding layers to rebalance load, we co-design hardware and software to implement a single-queue system.\nThe Shinjuku operating system [25] improves throughput under SLO by preempting long-running RPCs instead of running every RPC to completion. Their approach is particularly effective for workloads with extreme service time variability and CPUs with limited core count. Shinjuku preempts requests every 5\u201315\u00b5s, which is higher than the vast majority of our evaluated RPC runtimes. A system combining Shinjuku and RPCValet would rigorously handle RPCs of a broad runtime range, from hundreds of ns to hundreds of \u00b5s.\nProgrammable Network Interfaces. Offloading compute to programmable network processors is an old idea that has seen rekindled interest; FLASH [32] and Typhoon [48] integrated general-purpose processors with the NI, enabling customhandler execution uponmessage reception. NI-controlled message dispatch to cores has been proposed in the context of parallel protocol handler execution for DSMs to eschew software synchronization overheads [19, 46]. Programming abstractions such as PDQ [19] could be deployed as loadbalancing decisions in RPCValet\u2019s NI dispatch pipeline.\nToday\u2019s commercial \u201cSmartNICs\u201d target protocol processing or high-level application acceleration, with the goal of reducing CPU load; they integrate either CPU cores (e.g., Mellanox\u2019s BlueField [37]), or FPGAs (e.g., Microsoft\u2019s Catapult [11, 33]). FlexNIC [30, 31] draws inspiration from SDN switches [8], deploying a match-action pipeline for line-rate\nheader processing. The programmable logic in these SmartNICs could be leveraged to implement non-static load balancing, adding flexibility to RSS or Flow Director. However, our dynamic load balancing scheme relies on ns-scale interaction between the NI and CPU logic, which is only attainable through tight NI integration and CPU-NI co-design.\nRPCLayers on InfiniBandNICs. Latency-critical software systems for key-value storage [27, 28], distributed transaction processing [17, 28], distributed durable data storage [44], and generalized datacenter RPCs [52], have already begun using RDMA NICs due to their low latency and high IOPS. All of these systems are fine-tuned to maximize RPC throughput given the underlying limitations of their discrete NICs and the IB verbs specification. We distinguish RPCValet from these software-only systems by our focus on balancing the load of incoming RPCs across the CPU cores. Furthermore, all of the above proposals are adversely affected by the shortcomings of PCIe-attached NICs, and use specific optimizations to ameliorate their inherent latency bottlenecks; this strengthens our insight that NI integration is the key enabler for handling RPCs in true single-queue fashion."
                },
                {
                    "heading": "8 Conclusion",
                    "text": "We introduced RPCValet, anNI-driven dynamic load-balancing mechanism for\u00b5s-scale RPCs. RPCValet behaves like a singlequeue system, without incurring the synchronization overheads typically associated with single-queue implementations. RPCValet performs within 3\u201315% of the ideal singlequeue system and significantly outperforms current RPC load-balancing approaches."
                },
                {
                    "heading": "Acknowledgements",
                    "text": "We thank Edouard Bugnion, James Larus, Dmitrii Ustiugov, Virendra Marathe, Dionisios Pnevmatikatos, Mario Drumond, Arash Pourhabibi, Marios Kogias and the anonymous reviewers for their precious feedback. This work was partially funded by Huawei Technologies, the Nano-Tera YINS project, the Oracle Labs Accelarating Distributed Systems with Advanced One-Sided Operations grant, and the SNSF\u2019s Memory-Centric Server Architecture for Datacenters project."
                }
            ],
            "year": 2019,
            "references": [
                {
                    "title": "A Hardware Building Block for 2020 Warehouse-Scale Computers",
                    "authors": [
                        "Krste Asanovi\u0107"
                    ],
                    "venue": "USENIX FAST Keynote,",
                    "year": 2014
                },
                {
                    "title": "Workload analysis of a large-scale key-value store",
                    "authors": [
                        "Berk Atikoglu",
                        "Yuehai Xu",
                        "Eitan Frachtenberg",
                        "Song Jiang",
                        "Mike Paleczny"
                    ],
                    "venue": "In Proceedings of the 2012 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems,",
                    "year": 2012
                },
                {
                    "title": "Attack of the killer microseconds",
                    "authors": [
                        "Luiz Andr\u00e9 Barroso",
                        "Mike Marty",
                        "David A. Patterson",
                        "Parthasarathy Ranganathan"
                    ],
                    "venue": "Commun. ACM,",
                    "year": 2017
                },
                {
                    "title": "IX: A Protected Dataplane Operating System for High Throughput and Low Latency",
                    "authors": [
                        "AdamBelay",
                        "George Prekas",
                        "Ana Klimovic",
                        "Samuel Grossman",
                        "Christos Kozyrakis",
                        "Edouard Bugnion"
                    ],
                    "venue": "In Proceedings of the 11th Symposium on Operating System Design and Implementation (OSDI),",
                    "year": 2014
                },
                {
                    "title": "P4: programming protocol-independent packet processors",
                    "authors": [
                        "Pat Bosshart",
                        "Dan Daly",
                        "Glen Gibb",
                        "Martin Izzard",
                        "Nick McKeown",
                        "Jennifer Rexford",
                        "Cole Schlesinger",
                        "Dan Talayco",
                        "Amin Vahdat",
                        "George Varghese",
                        "David Walker"
                    ],
                    "venue": "Computer Communication Review,",
                    "year": 2014
                },
                {
                    "title": "Randomized load balancing with general service time distributions",
                    "authors": [
                        "Maury Bramson",
                        "Yi Lu",
                        "Balaji Prabhakar"
                    ],
                    "venue": "In Proceedings of the 2010 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems,",
                    "year": 2010
                },
                {
                    "title": "TAO: Facebook\u2019s Distributed Data Store for the Social Graph",
                    "authors": [
                        "Nathan Bronson",
                        "Zach Amsden",
                        "George Cabrera",
                        "Prasad Chakka",
                        "Peter Dimov",
                        "Hui Ding",
                        "Jack Ferris",
                        "Anthony Giardullo",
                        "Sachin Kulkarni",
                        "Harry C. Li",
                        "Mark Marchukov",
                        "Dmitri Petrov",
                        "Lovro Puzar",
                        "Yee Jiun Song",
                        "Venkateshwaran Venkataramani"
                    ],
                    "venue": "In Proceedings of the 2013 USENIX Annual Technical Conference (ATC),",
                    "year": 2013
                },
                {
                    "title": "A cloud-scale acceleration architecture",
                    "authors": [
                        "Adrian M. Caulfield",
                        "Eric S. Chung",
                        "Andrew Putnam",
                        "Hari Angepat",
                        "Jeremy Fowers",
                        "Michael Haselman",
                        "Stephen Heil",
                        "Matt Humphrey",
                        "Puneet Kaur",
                        "Joo-Young Kim",
                        "Daniel Lo",
                        "Todd Massengill",
                        "Kalin Ovtcharov",
                        "Michael Papamichael",
                        "Lisa Woods",
                        "Sitaram Lanka",
                        "Derek Chiou",
                        "Doug Burger"
                    ],
                    "venue": "In Proceedings of the 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),",
                    "year": 2016
                },
                {
                    "title": "Network-Compute Co-Design for Distributed In-Memory Computing",
                    "authors": [
                        "Alexandros Daglis"
                    ],
                    "venue": "EPFL PhD Thesis,",
                    "year": 2018
                },
                {
                    "title": "Manycore network interfaces for in-memory rackscale computing",
                    "authors": [
                        "Alexandros Daglis",
                        "Stanko Novakovic",
                        "Edouard Bugnion",
                        "Babak Falsafi",
                        "Boris Grot"
                    ],
                    "venue": "In Proceedings of the 42nd International Symposium on Computer Architecture (ISCA),",
                    "year": 2015
                },
                {
                    "title": "SABRes: Atomic object reads for in-memory rack-scale computing",
                    "authors": [
                        "Alexandros Daglis",
                        "Dmitrii Ustiugov",
                        "Stanko Novakovic",
                        "Edouard Bugnion",
                        "Babak Falsafi",
                        "Boris Grot"
                    ],
                    "venue": "In Proceedings of the 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),",
                    "year": 2016
                },
                {
                    "title": "The tail at scale",
                    "authors": [
                        "Jeffrey Dean",
                        "Luiz Andr\u00e9 Barroso"
                    ],
                    "venue": "Commun. ACM,",
                    "year": 2013
                },
                {
                    "title": "Dynamo: amazon\u2019s highly available key-value store",
                    "authors": [
                        "Giuseppe DeCandia",
                        "Deniz Hastorun",
                        "Madan Jampani",
                        "Gunavardhan Kakulapati",
                        "Avinash Lakshman",
                        "Alex Pilchin",
                        "Swaminathan Sivasubramanian",
                        "Peter Vosshall",
                        "Werner Vogels"
                    ],
                    "venue": "In Proceedings of the 21st ACM Symposium on Operating Systems Principles (SOSP),",
                    "year": 2007
                },
                {
                    "title": "FaRM: Fast Remote Memory",
                    "authors": [
                        "Aleksandar Dragojevic",
                        "Dushyanth Narayanan",
                        "Miguel Castro",
                        "Orion Hodson"
                    ],
                    "venue": "In Proceedings of the 11th Symposium on Networked Systems Design and Implementation (NSDI),",
                    "year": 2014
                },
                {
                    "title": "The Virtual Interface Architecture",
                    "authors": [
                        "Dave Dunning",
                        "Greg J. Regnier",
                        "Gary L. McAlpine",
                        "Don Cameron",
                        "Bill Shubert",
                        "Frank Berry",
                        "Anne Marie Merritt",
                        "Ed Gronke",
                        "Chris Dodd"
                    ],
                    "venue": "IEEE Micro,",
                    "year": 1998
                },
                {
                    "title": "Parallel Dispatch Queue: A Queue- Based Programming Abstraction to Parallelize Fine-Grain Communication Protocols",
                    "authors": [
                        "Babak Falsafi",
                        "David A. Wood"
                    ],
                    "venue": "In Proceedings of the 5th IEEE Symposium on High- Performance Computer Architecture (HPCA),",
                    "year": 1999
                },
                {
                    "title": "Network Requirements for Resource Disaggregation",
                    "authors": [
                        "Peter Xiang Gao",
                        "Akshay Narayan",
                        "Sagar Karandikar",
                        "Joao Carreira",
                        "Sangjin Han",
                        "Rachit Agarwal",
                        "Sylvia Ratnasamy",
                        "Scott Shenker"
                    ],
                    "venue": "In Proceedings of the 12th Symposium on Operating System Design and Implementation (OSDI),",
                    "year": 2016
                },
                {
                    "title": "RDMA over Commodity Ethernet at Scale",
                    "authors": [
                        "Chuanxiong Guo",
                        "Haitao Wu",
                        "Zhong Deng",
                        "Gaurav Soni",
                        "Jianxi Ye",
                        "Jitu Padhye",
                        "Marina Lipshteyn"
                    ],
                    "venue": "In Proceedings of the ACM SIGCOMM 2016 Conference,",
                    "year": 2016
                },
                {
                    "title": "Analysis of join-the-shortest-queue routing for web server farms",
                    "authors": [
                        "Varun Gupta",
                        "Mor Harchol-Balter",
                        "Karl Sigman",
                        "andWardWhitt"
                    ],
                    "venue": "Perform. Eval.,",
                    "year": 2007
                },
                {
                    "title": "SVE: Distributed Video Processing at Facebook Scale",
                    "authors": [
                        "Qi Huang",
                        "Petchean Ang",
                        "Peter Knowles",
                        "Tomasz Nykiel",
                        "Iaroslav Tverdokhlib",
                        "Amit Yajurvedi",
                        "Paul Dapolito IV",
                        "Xifan Yan",
                        "Maxim Bykov",
                        "Chuen Liang",
                        "Mohit Talwar",
                        "AbhishekMathur",
                        "Sachin Kulkarni",
                        "Matthew Burke",
                        "Wyatt Lloyd"
                    ],
                    "venue": "In Proceedings of the 26th ACM Symposium on Operating Systems Principles (SOSP),",
                    "year": 2017
                },
                {
                    "title": "Shinjuku: Preemptive Scheduling for \u03bcsecond-scale Tail Latency",
                    "authors": [
                        "Kostis Kaffes",
                        "Timothy Chong",
                        "Jack Tigar Humphries",
                        "Adam Belay",
                        "David Mazieres",
                        "Christos Kozyrakis"
                    ],
                    "venue": "In Proceedings of the 16th USENIX Symposium on Networked Systems Design and Implementation (NSDI),",
                    "year": 2019
                },
                {
                    "title": "Service fabric: a distributed platform for building microservices in the cloud",
                    "authors": [
                        "Gopal Kakivaya",
                        "Lu Xun",
                        "Richard Hasha",
                        "Shegufta Bakht Ahsan",
                        "Todd Pfleiger",
                        "Rishi Sinha",
                        "Anurag Gupta",
                        "Mihail Tarta",
                        "Mark Fussell",
                        "Vipul Modi",
                        "Mansoor Mohsin",
                        "Ray Kong",
                        "Anmol Ahuja",
                        "Oana Platon",
                        "Alex Wun",
                        "Matthew Snider",
                        "Chacko Daniel",
                        "Dan Mastrian",
                        "Yang Li",
                        "Aprameya Rao",
                        "Vaishnav Kidambi",
                        "Randy Wang",
                        "Abhishek Ram",
                        "Sumukh Shivaprakash",
                        "Rajeet Nair",
                        "Alan Warwick",
                        "Bharat S. Narasimman",
                        "Meng Lin",
                        "Jeffrey Chen",
                        "Abhay Balkrishna Mhatre",
                        "Preetha Subbarayalu",
                        "Mert Coskun",
                        "Indranil Gupta"
                    ],
                    "venue": "Proceedings of the 2018 EuroSys Conference,",
                    "year": 2018
                },
                {
                    "title": "Using RDMA efficiently for key-value services",
                    "authors": [
                        "Anuj Kalia",
                        "Michael Kaminsky",
                        "David G. Andersen"
                    ],
                    "venue": "In Proceedings of the ACM SIGCOMM 2014 Conference,",
                    "year": 2014
                },
                {
                    "title": "FaSST: Fast, Scalable and Simple Distributed Transactions with Two-Sided (RDMA) Datagram RPCs",
                    "authors": [
                        "Anuj Kalia",
                        "Michael Kaminsky",
                        "David G. Andersen"
                    ],
                    "venue": "In Proceedings of the 12th Symposium on Operating System Design and Implementation (OSDI),",
                    "year": 2016
                },
                {
                    "title": "Profiling a warehouse-scale computer",
                    "authors": [
                        "Svilen Kanev",
                        "Juan Pablo Darago",
                        "Kim M. Hazelwood",
                        "Parthasarathy Ranganathan",
                        "Tipp Moseley",
                        "Gu-Yeon Wei",
                        "David M. Brooks"
                    ],
                    "venue": "In Proceedings of the 42nd International Symposium on Computer Architecture (ISCA),",
                    "year": 2015
                },
                {
                    "title": "FlexNIC: Rethinking Network DMA",
                    "authors": [
                        "Antoine Kaufmann",
                        "Simon Peter",
                        "Thomas E. Anderson",
                        "Arvind Krishnamurthy"
                    ],
                    "venue": "In Proceedings of The 15th Workshop on Hot Topics in Operating Systems (HotOS-XV),",
                    "year": 2015
                },
                {
                    "title": "High Performance Packet Processing with FlexNIC",
                    "authors": [
                        "Antoine Kaufmann",
                        "Simon Peter",
                        "Naveen Kr. Sharma",
                        "Thomas E. Anderson",
                        "Arvind Krishnamurthy"
                    ],
                    "venue": "In Proceedings of the 21st International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-XXI),",
                    "year": 2016
                },
                {
                    "title": "The Stanford FLASH Multiprocessor",
                    "authors": [
                        "Jeffrey Kuskin",
                        "David Ofelt",
                        "Mark Heinrich",
                        "John Heinlein",
                        "Richard Simoni",
                        "Kourosh Gharachorloo",
                        "John Chapin",
                        "David Nakahira",
                        "Joel Baxter",
                        "Mark Horowitz",
                        "Anoop Gupta",
                        "Mendel Rosenblum",
                        "John L. Hennessy"
                    ],
                    "venue": "In Proceedings of the 21st International Symposium on Computer Architecture (ISCA),",
                    "year": 1994
                },
                {
                    "title": "KV-Direct: High-Performance In-Memory Key-Value Store with Programmable NIC",
                    "authors": [
                        "Bojie Li",
                        "Zhenyuan Ruan",
                        "Wencong Xiao",
                        "Yuanwei Lu",
                        "Yongqiang Xiong",
                        "Andrew Putnam",
                        "Enhong Chen",
                        "Lintao Zhang"
                    ],
                    "venue": "In Proceedings of the 26th ACM Symposium on Operating Systems Principles (SOSP),",
                    "year": 2017
                },
                {
                    "title": "Pegasus: Load- Aware Selective Replication with an In-Network Coherence Directory",
                    "authors": [
                        "Jialin Li",
                        "Jacob Nelson",
                        "Xin Jin",
                        "Dan R.K. Ports"
                    ],
                    "venue": "UW CSE Technical Report,",
                    "year": 2018
                },
                {
                    "title": "MICA: A Holistic Approach to Fast In-Memory Key-Value Storage",
                    "authors": [
                        "Hyeontaek Lim",
                        "Dongsu Han",
                        "David G. Andersen",
                        "Michael Kaminsky"
                    ],
                    "venue": "In Proceedings of the 11th Symposium on Networked Systems Design and Implementation (NSDI),",
                    "year": 2014
                },
                {
                    "title": "Epyc Relaunches AMD Into Servers",
                    "authors": [
                        "Linley Group"
                    ],
                    "venue": "Microprocessor Report,",
                    "year": 2017
                },
                {
                    "title": "X-Gene 3 Up and Running",
                    "authors": [
                        "Linley Group"
                    ],
                    "venue": "Microprocessor Report,",
                    "year": 2017
                },
                {
                    "title": "Join-Idle-Queue: A novel load balancing algorithm for dynamically scalable web services",
                    "authors": [
                        "Yi Lu",
                        "Qiaomin Xie",
                        "Gabriel Kliot",
                        "Alan Geller",
                        "James R. Larus",
                        "Albert G. Greenberg"
                    ],
                    "venue": "Perform. Eval.,",
                    "year": 2011
                },
                {
                    "title": "Cache craftiness for fast multicore key-value storage",
                    "authors": [
                        "Yandong Mao",
                        "Eddie Kohler",
                        "Robert Tappan Morris"
                    ],
                    "venue": "In Proceedings of the 2012 EuroSys Conference,",
                    "year": 2012
                },
                {
                    "title": "Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors",
                    "authors": [],
                    "venue": "ACM Trans. Comput. Syst.,",
                    "year": 1991
                },
                {
                    "title": "Scale-out NUMA",
                    "authors": [
                        "Stanko Novakovic",
                        "Alexandros Daglis",
                        "Edouard Bugnion",
                        "Babak Falsafi",
                        "Boris Grot"
                    ],
                    "venue": "In Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-XIX),",
                    "year": 2014
                },
                {
                    "title": "Arrakis: The Operating System Is the Control Plane",
                    "authors": [
                        "Simon Peter",
                        "Jialin Li",
                        "Irene Zhang",
                        "Dan R.K. Ports",
                        "DougWoos",
                        "Arvind Krishnamurthy",
                        "Thomas E. Anderson",
                        "Timothy Roscoe"
                    ],
                    "venue": "ACM Trans. Comput. Syst.,",
                    "year": 2016
                },
                {
                    "title": "Address Partitioning in DSMClusters with Parallel Coherence Controllers",
                    "authors": [
                        "Ilanthiraiyan Pragaspathy",
                        "Babak Falsafi"
                    ],
                    "venue": "In Proceedings of the 9th International Conference on Parallel Architecture and Compilation Techniques (PACT),",
                    "year": 2000
                },
                {
                    "title": "ZygOS: Achieving Low Tail Latency for Microsecond-scale Networked Tasks",
                    "authors": [
                        "George Prekas",
                        "Marios Kogias",
                        "Edouard Bugnion"
                    ],
                    "venue": "In Proceedings of the 26th ACM Symposium on Operating Systems Principles (SOSP),",
                    "year": 2017
                },
                {
                    "title": "Tempest and Typhoon: User-Level Shared Memory",
                    "authors": [
                        "Steven K. Reinhardt",
                        "James R. Larus",
                        "David A. Wood"
                    ],
                    "venue": "In Proceedings of the 21st International Symposium on Computer Architecture (ISCA),",
                    "year": 1994
                },
                {
                    "title": "Inside the Social Network\u2019s (Datacenter) Network",
                    "authors": [
                        "Arjun Roy",
                        "Hongyi Zeng",
                        "Jasmeet Bagga",
                        "George Porter",
                        "Alex C. Snoeren"
                    ],
                    "venue": "In Proceedings of the ACM SIGCOMM 2015 Conference,",
                    "year": 2015
                },
                {
                    "title": "It\u2019s Time for Low Latency",
                    "authors": [
                        "Stephen M. Rumble",
                        "Diego Ongaro",
                        "Ryan Stutsman",
                        "Mendel Rosenblum",
                        "John K. Ousterhout"
                    ],
                    "venue": "In Proceedings of The 13th Workshop on Hot Topics in Operating Systems (HotOS-XIII),",
                    "year": 2011
                },
                {
                    "title": "RDMA read based rendezvous protocol for MPI over InfiniBand: design alternatives and benefits",
                    "authors": [
                        "Sayantan Sur",
                        "Hyun-Wook Jin",
                        "Lei Chai",
                        "Dhabaleswar K. Panda"
                    ],
                    "venue": "In Proceedings of the 11th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP),",
                    "year": 2006
                },
                {
                    "title": "LITE Kernel RDMA Support for Datacenter Applications",
                    "authors": [
                        "Shin-Yeh Tsai",
                        "Yiying Zhang"
                    ],
                    "venue": "In Proceedings of the 26th ACM Symposium on Operating Systems Principles (SOSP),",
                    "year": 2017
                },
                {
                    "title": "Speedy transactions in multicore in-memory databases",
                    "authors": [
                        "Stephen Tu",
                        "Wenting Zheng",
                        "Eddie Kohler",
                        "Barbara Liskov",
                        "Samuel Madden"
                    ],
                    "venue": "In Proceedings of the 24th ACM Symposium on Operating Systems Principles (SOSP),",
                    "year": 2013
                },
                {
                    "title": "SimFlex: Statistical Sampling of Computer System Simulation",
                    "authors": [
                        "Thomas F. Wenisch",
                        "Roland E. Wunderlich",
                        "Michael Ferdman",
                        "Anastassia Ailamaki",
                        "Babak Falsafi",
                        "James C. Hoe"
                    ],
                    "venue": "IEEE Micro,",
                    "year": 2006
                }
            ],
            "id": "SP:53a203c0e26150d35366419b7f5047b55b2ccf61",
            "authors": [
                {
                    "name": "Alexandros Daglis",
                    "affiliations": []
                },
                {
                    "name": "Mark Sutherland",
                    "affiliations": []
                },
                {
                    "name": "Babak Falsafi",
                    "affiliations": []
                }
            ],
            "abstractText": "Modern online services come with stringent quality requirements in terms of response time tail latency. Because of their decomposition into fine-grained communicating software layers, a single user request fans out into a plethora of short, \u03bcs-scale RPCs, aggravating the need for faster inter-server communication. In reaction to that need, we are witnessing a technological transition characterized by the emergence of hardware-terminated user-level protocols (e.g., InfiniBand/RDMA) and new architectures with fully integrated Network Interfaces (NIs). Such architectures offer a unique opportunity for a new NI-driven approach to balancing RPCs among the cores of manycore server CPUs, yielding major tail latency improvements for \u03bcs-scale RPCs. We introduce RPCValet, an NI-driven RPC load-balancing design for architectures with hardware-terminated protocols and integrated NIs, that delivers near-optimal tail latency. RPCValet\u2019s RPC dispatch decisions emulate the theoretically optimal single-queue system, without incurring synchronization overheads currently associated with single-queue implementations. Our design improves throughput under tight tail latency goals by up to 1.4\u00d7, and reduces tail latency before saturation by up to 4\u00d7 for RPCs with \u03bcs-scale service times, as compared to current systems with hardware support for RPC load distribution. RPCValet performs within 15% of the theoretically optimal single-queue system. ACM Reference Format: Alexandros Daglis\u2217, Mark Sutherland, and Babak Falsafi. 2019. RPCValet: NI-Driven Tail-Aware Balancing of \u03bcs-Scale RPCs. In 2019 Architectural Support for Programming Languages and Operating Systems (ASPLOS \u201919), April 13\u201317, 2019, Providence, RI, USA.ACM, New York, NY, USA, 14 pages. https://doi.org/10.1145/3297858.3304070 Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ASPLOS \u201919, April 13\u201317, 2019, Providence, RI, USA \u00a9 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6240-5/19/04. . . $15.00 https://doi.org/10.1145/3297858.3304070",
            "title": "RPCValet: NI-Driven Tail-Aware Balancing of s-Scale RPCs"
        },
        "Y": {
            "blog_id": "rpcvalet",
            "summary": [
                "RPCValet: NI-driven tail-aware balancing of \u00b5s-scale RPCs Daglis et al., ASPLOS\u201919  Last week we learned about the [increased tail-latency sensitivity of microservices based applications with high RPC fan-outs.",
                "Seer uses estimates of queue depths to mitigate latency spikes on the order of 10-100ms, in conjunction with a cluster manager.",
                "Today\u2019s paper choice, RPCValet, operates at latencies 3 orders of magnitude lower, targeting reduction in tail latency for services that themselves have service times on the order of a small number of \u00b5s (e.g., the average service time for memcached is approximately 2\u00b5s).",
                "The net result of rapid advancements in the networking world is that inter-tier communications latency will approach the fundamental lower bound of speed-of-light propagation in the foreseeable future.",
                "The focus of optimization hence will completely shift to efficiently handling RPCs at the endpoints as soon as they are delivered from the network.",
                "Furthermore, the evaluation shows that \u201cRPCValet leaves no significant room for improvement\u201d when compared against the theoretical ideal (it comes within 3-15%).",
                "So what we have here is a glimpse of the limits for low-latency RPCs under load.",
                "When it\u2019s no longer physically possible to go meaningfully faster, further application-level performance gains will have to come from eliminating RPCs altogether.",
                "RPCValet balances incoming RPC requests among the multiple cores of a server.",
                "Consider for example a Redis server maintaining a sorted array in memory\u2026  \u2026 an RPC to add a new entry may incur multiple TLB misses, stalling the core for a few \u00b5s while new translations are installed.",
                "While this core is stalled on the TLB miss(es), it is best to dispatch RPCs to other available cores on the server.",
                "In theory, how fast could we go?",
                "Consider a 16-core server handling 16 requests.",
                "We could put anywhere from 1 to 16 queues in front of those cores.",
                "At one extreme we have a \u201816 x 1\u2019 architecture with 16 queues each with one associated processing unit.",
                "At the other extreme is a \u20181 x 16\u2019 architecture with one shared queue serving all 16 processing units.",
                "Or we could have e.g. a \u20184 x 4\u2019 with 4 queues each serving 4 units, and so on\u2026  If you model that out with Poisson arrivals and variety of different service time distributions (fixed, uniform, exponential, and generalised extreme value, GEV) what you\u2019ll find is that the \u20181\u00d716\u2019 architecture performs the best, and the \u201816\u00d71\u2019 architecture performs the worst.",
                "1 x 16 significantly outperforms 16 x 1.",
                "16 x 1\u2019s inability to assign requests to idle cores results in higher tail latencies and a peak throughput 25-73% lower than 1 x 16 under a tail latency SLO at 10x the mean service time\u2026 The theoretical results suggest that systems should implement a queueing configuration that is as close as possible to a single queue (1 x 16) configuration.",
                "In practice those models failed to account for a significant source of overhead: synchronizing across multiple cores on the single queue.",
                "When you take synchronisation into account, a dedicated queue per core starts to look like a good idea again.",
                "NICs supporting Receive-Side Scaling (RSS) can push messages into each core\u2019s queue, but while RSS can help to achieve load distribution, it can\u2019t truly achieving load balancing.",
                "Any resulting load imbalance after applying these [RSS] rules must be handled by system software, introducing unacceptable latency for the most latency-sensitive RPCs with \u00b5s-scale service times.",
                "Introducing RPCValet  RPCValet uses a push-based model, while also taking into account the current loading of the cores.",
                "It\u2019s designed for \u201cemerging architectures featuring fully integrated NIs and hardware-terminated transport protocols.\u201d.",
                "The key hardware feature is that the network interface has direct access to the server\u2019s memory hierarchy, eliminating round trips over e.g. PCIe.",
                "Servers register a part of their DRAM into a partitioned global address space (PGAS), where every server can read and write memory in RDMA fashion.",
                "(We\u2019re not given any details on system reconfiguration etc.).",
                "An integrated NI can, with proper hardware support, monitor each core\u2019s state and steer RPCs to the least loaded cores.",
                "Such monitoring is implausible without NI integration, as the latency of transferring load information over an I/O bus (e.g. ~ 1.5 \u00b5s for a 3-hop posted PCIe transaction) would mean that the NI will make delayed\u2014hence sub-optimal, or even wrong\u2014 decisions until the information arrives.",
                "With N participants in the system, RPCValet first writes every incoming message into a single PGAS-resident message buffer of NxS slots.",
                "Then it notifies the selected core to process the request.",
                "Message arrival and memory location are thus decoupled from the assignment of a core for processing.",
                "We have the synchronization-free zero-copy behaviour of a partitioned multi-queue architecture, together with the load-balancing flexibility of a single queue system.",
                "In the implementation each node maintains a send and a receive buffer.",
                "Send buffers contain a valid bit indicating whether they are currently being used, and a pointer to a buffer in local memory containing the payload.",
                "Receive buffers on the other hand have slots that are size to accommodate message payloads directly.",
                "Overall, the messaging mechanism\u2019s memory footprint is 32 x N x S + (max_msg_size + 64) x N x S bytes.",
                "That is, a few tens of MB at most.",
                "The implementation uses a simple scheme to estimate core loads.",
                "RPCValet simply keeps track of the number of outstanding send requests assigned to each core.",
                "Allowing only one inflight request per core corresponds to a true single-system queue behaviour, but introduces a small inefficiency waiting for the notification of completed request processing.",
                "A practical compromise is to allow two outstanding requests per core.",
                "This results in marginal performance gains over the single request design for ultra-fast RPCs with service times of a few nanoseconds.",
                "All IN backends independently handle incoming network packets and access memory directly, but they hand off to a single NI dispatcher (over the on-chip interconnect) that is statically assigned to dispatch requests to cores for servicing.",
                "\u2026 for modern server processor core counts, the required dispatch throughput should be easily sustainable by a single centralized hardware unit, while the additional latency due to the indirection from any NI backend to the NI dispatcher is negligible (just a few ns).",
                "Evaluation  The evaluation is conducted using against three different service implementations: a synthetic service that emulates different service time distributions; the HERD key-value store, and the Masstree data store.",
                "An SLO of less than or equal to 10x the mean service time is assumed in each case, and configurations are evaluated in terms of throughput under SLO.",
                "Compared to a software implementation, which requires a synchronisation primitive, the hardware implementation delivers 2.3x-2.7x higher throughput under SLO.",
                "The following plots show the performance of different queueing arrangements under the three workloads, with the \u20181\u00d716\u2019 arrangement that RPCValet simulates performing the best as expected.",
                "( Enlarge )  Compared to a theoretically optimal 1 x 16 model, RPCValet gets within 3-15% (depending on the service time distribution).",
                "\u201cWe attribute the gap between the implementation and the model to contention that emerges under high load in the implemented systems, which is not captured by the model.\u201d  At the end of the day though:  RPCValet leaves no significant room for improvement; neither centralizing dispatch nor maintaining private request queues per core introduces performance concerns.",
                "Throughput under tight tail latency goals is improved by up to 1.4x, and tail latency before saturation is reduced by up to 4x."
            ],
            "author_id": "ACOLYER",
            "pdf_url": "https://www.cc.gatech.edu/~adaglis3/files/papers/RPCValet_asplos19.pdf",
            "author_full_name": "Adrian Colyer",
            "source_website": "https://blog.acolyer.org/about/",
            "id": 85505359
        }
    },
    "87411149": {
        "X": {
            "sections": [
                {
                    "heading": "1. INTRODUCTION",
                    "text": "Faced with growing amounts of data and unprecedented query volume, distributed databases increasingly split their data across multiple servers, or partitions, such that no one partition contains an entire copy of the database [7,13,18,19,22,29,43]. This strategy succeeds in allowing near-unlimited scalability for operations that access single partitions. However, operations that access multiple partitions must communicate across servers\u2014often synchronously\u2014 in order to provide correct behavior. Designing systems and algorithms that tolerate these communication delays is a difficult task but is key to maintaining scalability [17, 28, 29, 35].\nIn this work, we address a largely underserved class of applications requiring multi-partition, atomically visible1 transactional access: cases where all or none of each transaction\u2019s effects should be visible. The status quo for these multi-partition atomic transactions provides an uncomfortable choice between algorithms that 1Our use of \u201catomic\u201d (specifically, Read Atomic isolation) concerns all-or-nothing visibility of updates (i.e., the ACID isolation effects of ACID atomicity; Section 3). This differs from uses of \u201catomicity\u201d to denote serializability [8] or linearizability [4]. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGMOD\u201914, June 22\u201327, 2014, Snowbird, UT, USA. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2376-5/14/06 ...$15.00. http://dx.doi.org/10.1145/2588555.2588562.\nare fast but deliver inconsistent results and algorithms that deliver consistent results but are often slow and unavailable under failure. Many of the largest modern, real-world systems opt for protocols that guarantee fast and scalable operation but provide few\u2014if any\u2014transactional semantics for operations on arbitrary sets of data items [11, 13, 15, 22, 26, 38, 44]. This results in incorrect behavior for use cases that require atomic visibility, including secondary indexing, foreign key constraint enforcement, and materialized view maintenance (Section 2). In contrast, many traditional transactional mechanisms correctly ensure atomicity of updates [8, 17, 43]. However, these algorithms\u2014such as two-phase locking and variants of optimistic concurrency control\u2014are often coordination-intensive, slow, and, under failure, unavailable in a distributed environment [5, 18, 28, 35]. This dichotomy between scalability and atomic visibility has been described as \u201ca fact of life in the big cruel world of huge systems\u201d [25]. The proliferation of non-transactional multi-item operations is symptomatic of a widespread \u201cfear of synchronization\u201d at scale [9].\nOur contribution in this paper is to demonstrate that atomically visible transactions on partitioned databases are not at odds with scalability. Specifically, we provide high-performance implementations of a new, non-serializable isolation model called Read Atomic (RA) isolation. RA ensures that all or none of each transaction\u2019s updates are visible to others and that each transaction reads from an atomic snapshot of database state (Section 3)\u2014this is useful in the applications we target. We subsequently develop three new, scalable algorithms for achieving RA isolation that we collectively title Read Atomic Multi-Partition (RAMP) transactions (Section 4). RAMP transactions guarantee scalability and outperform existing atomic algorithms because they satisfy two key scalability constraints. First, RAMP transactions guarantee synchronization independence: one client\u2019s transactions cannot cause another client\u2019s transactions to stall or fail. Second, RAMP transactions guarantee partition independence: clients never need to contact partitions that their transactions do not directly reference. Together, these properties ensure guaranteed completion, limited coordination across partitions, and horizontal scalability for multi-partition access.\nRAMP transactions are scalable because they appropriately control the visibility of updates without inhibiting concurrency. Rather than force concurrent reads and writes to stall, RAMP transactions allow reads to \u201crace\u201d writes: RAMP transactions can autonomously detect the presence of non-atomic (partial) reads and, if necessary, repair them via a second round of communication with servers. To accomplish this, RAMP writers attach metadata to each write and use limited multi-versioning to prevent readers from stalling. The three algorithms we present offer a trade-off between the size of this metadata and performance. RAMP-Small transactions require constant space (a timestamp per write) and two round trip time delays (RTTs) for reads and writes. RAMP-Fast transactions require metadata size that is linear in the number of writes in the transaction but only require one RTT for reads in the common case and two in the worst case. RAMP-Hybrid transactions employ Bloom filters [10] to provide an intermediate solution. Traditional techniques like locking\ncouple atomic visibility and mutual exclusion; RAMP transactions provide the benefits of the former without incurring the scalability, availability, or latency penalties of the latter.\nIn addition to providing a theoretical analysis and proofs of correctness, we demonstrate that RAMP transactions deliver in practice. Our RAMP implementation achieves linear scalability to over 7 million operations per second on a 100 server cluster (at overhead below 5% for a workload of 95% reads). Moreover, across a range of workload configurations, RAMP transactions incur limited overhead compared to other techniques and achieve higher performance than existing approaches to atomic visibility (Section 5).\nWhile the literature contains an abundance of isolation models [2, 5], we believe that the large number of modern applications requiring RA isolation and the excellent scalability of RAMP transactions justify the addition of yet another model. RA isolation is too weak for some applications, but, for the many that it can serve, RAMP transactions offer substantial benefits."
                },
                {
                    "heading": "2. OVERVIEW AND MOTIVATION",
                    "text": "In this paper, we consider the problem of making transactional updates atomically visible to readers\u2014a requirement that, as we outline in this section, is found in several prominent use cases today. The basic property we provide is fairly simple: either all or none of each transaction\u2019s updates should be visible to other transactions. For example, if a transaction T1 writes x = 1 and y = 1, then another transaction T2 should not read x = 1 and y = null. Instead, T2 should either read x = 1 and y = 1 or, possibly, x = null and y = null. Informally, each transaction reads from an unchanging snapshot of database state that is aligned along transactional boundaries. We call this property atomic visibility and formalize it via the Read Atomic isolation guarantee in Section 3.\nThe classic strategy for providing atomic visibility is to ensure mutual exclusion between readers and writers. For example, if a transaction like T1 above wants to update data items x and y, it can acquire exclusive locks for each of x and y, update both items, then release the locks. No other transactions will observe partial updates to x and y, ensuring atomic visibility. However, this solution has a drawback: while one transaction holds exclusive locks on x and y, no other transactions can access x and y for either reads or writes. By using mutual exclusion to enforce the atomic visibility of updates, we have also limited concurrency. In our example, if x and y are located on different servers, concurrent readers and writers will be unable to perform useful work during communication delays. These communication delays form an upper bound on throughput: effectively, 1message delay operations per second.\nTo avoid this upper bound, we separate the problem of providing atomic visibility from the problem of maintaining mutual exclusion. By achieving the former but avoiding the latter, the algorithms we develop in this paper are not subject to the scalability penalties of many prior approaches. To ensure that all servers successfully execute a transaction (or that none do), our algorithms employ an atomic commitment protocol (ACP). When coupled with a blocking concurrency control mechanism like locking, ACPs are harmful to scalability and availability: arbitrary failures can (provably) cause any ACP implementation to stall [8]. (Optimistic concurrency control mechanisms can similarly block during validation.) We instead use ACPs with non-blocking concurrency control mechanisms; this means that individual transactions can stall due to failures or communication delays without forcing other transactions to stall. In a departure from traditional concurrency control, we allow multiple ACP rounds to proceed in parallel over the same data.\nThe end result\u2014our RAMP transactions\u2014provide excellent scalability and performance under contention (e.g., in the event of write\nhotspots) and are robust to partial failure. RAMP transactions\u2019 nonblocking behavior means that they cannot provide certain guarantees like preventing concurrent updates. However, applications that can use Read Atomic isolation will benefit from our algorithms. The remainder of this section identifies several relevant use cases from industry that require atomic visibility for correctness."
                },
                {
                    "heading": "2.1 Read Atomic Isolation in the Wild",
                    "text": "As a simple example, consider a social networking application: if two users, Sam and Mary, become \u201cfriends\u201d (a bi-directional relationship), other users should never see that Sam is a friend of Mary but Mary is not a friend of Sam: either both relationships should be visible, or neither should be. A transaction under Read Atomic isolation would correctly enforce this behavior, and we can further classify three general use cases for Read Atomic isolation: 1.) Foreign key constraints. Many database schemas contain information about relationships between records in the form of foreign key constraints. For example, Facebook\u2019s TAO [11], LinkedIn\u2019s Espresso [38], and Yahoo! PNUTS [15] store information about business entities such as users, photos, and status updates as well as relationships between them (e.g., the friend relationships above). Their data models often represent bi-directional edges as two distinct uni-directional relationships. For example, in TAO, a user performing a \u201clike\u201d action on a Facebook page produces updates to both the LIKES and LIKED_BY associations [11]. PNUTS\u2019s authors describe an identical scenario [15]. These applications require foreign key maintenance and often, due to their unidirectional relationships, multi-entity update and access. Violations of atomic visibility surface as broken bi-directional relationships (as with Sam and Mary above) and dangling or incorrect references (e.g., Frank is an employee of department.id=5, but no such department exists in the department table).\nWith RAMP transactions, when inserting new entities, applications can bundle relevant entities from each side of a foreign key constraint into a transaction. When deleting associations, users can \u201ctombstone\u201d the opposite end of the association (i.e., delete any entries with associations via a special record that signifies deletion) [45] to avoid dangling pointers. 2.) Secondary indexing. Data is typically partitioned across servers according to a primary key (e.g., user ID). This allows fast location and retrieval of data via primary key lookups but makes access by secondary attributes (e.g., birth date) challenging. There are two dominant strategies for distributed secondary indexing. First, the local secondary index approach co-locates secondary indexes and primary data, so each server contains a secondary index that only references (and indexes) data stored on its server [7,38]. This allows easy, single-server updates but requires contacting every partition for secondary attribute lookups (write-one, read-all), compromising scalability for read-heavy workloads [11, 17, 38]. Alternatively, the global secondary index approach locates secondary indexes (which may be partitioned, but by a secondary attribute) separately from primary data [7, 15]. This alternative allows fast secondary lookups (read-one) but requires multi-partition update (at least write-two).\nReal-world services employ either local secondary indexing (e.g., Espresso [38], Cassandra, and Google Megastore\u2019s local indexes [7]) or non-atomic (incorrect) global secondary indexing (e.g., Espresso and Megastore\u2019s global indexes, Yahoo! PNUTS\u2019s proposed secondary indexes [15]). The former is non-scalable but correct, while the latter is scalable but incorrect. For example, in a database partitioned by id with an incorrectly-maintained global secondary index on salary, the query \u2018SELECT id, salary WHERE salary > 6 , \u2019 might return records with salary less than $60,000 and omit some records with salary greater than $60,000.\nWith RAMP transactions, the secondary index entry for a given attribute can be updated atomically with base data. For example, if a secondary index is stored as a mapping from secondary attribute values to sets of item-versions matching the secondary attribute (e.g., the secondary index entry for users with blue hair would contain a list of user IDs and last-modified timestamps corresponding to all of the users with attribute hair-color=blue), then insertions of new primary data require additions to the corresponding index entry, deletions require removals, and updates require a \u201ctombstone\u201d deletion from one entry and an insertion into another. 3.) Materialized view maintenance. Many applications precompute (i.e., materialize) queries over data, as in Twitter\u2019s Rainbird service [44], Google\u2019s Percolator [36], and LinkedIn\u2019s Espresso systems [38]. As a simple example, Espresso stores a mailbox of messages for each user along with statistics about the mailbox messages: for Espresso\u2019s read-mostly workload, it is more efficient to maintain (i.e., pre-materialize) a count of unread messages rather than scan all messages every time a user accesses her mailbox [38]. In this case, any unread message indicators should remain in sync with the messages in the mailbox. However, atomicity violations will allow materialized views to diverge from the base data (e.g., Susan\u2019s mailbox displays a notification that she has unread messages but all 63,201 messages in her inbox are marked as read).\nWith RAMP transactions, base data and views can be updated atomically. The physical maintenance of a view depends on its specification [14, 27], but RAMP transactions provide appropriate concurrency control primitives for ensuring that changes are delivered to the materialized view partition. For select-project views, a simple solution is to treat the view as a separate table and perform maintenance as needed: new rows can be inserted/deleted according to the specification, and, if necessary, the view can be (re-)computed on demand (i.e., lazy view maintenance [46]). For more complex views, such as counters, users can execute RAMP transactions over specialized data structures such as the CRDT G-Counter [40]. In brief: Status Quo. Despite application requirements for Read Atomic isolation, few large-scale production systems provide it. For example, the authors of Tao, Espresso, and PNUTS describe several classes of atomicity anomalies exposed by their systems, ranging from dangling pointers to the exposure of intermediate states and incorrect secondary index lookups, often highlighting these cases as areas for future research and design [11, 15, 38]. These systems are not exceptions: data stores like Bigtable [13], Dynamo [22], and many popular \u201cNoSQL\u201d [34] and even some \u201cNewSQL\u201d [5] stores do not provide transactional guarantees for multi-item operations.\nThe designers of these Internet-scale, real-world systems have made a conscious decision to provide scalability at the expense of multi-partition transactional semantics. Our goal with RAMP transactions is to preserve this scalability but deliver correct, atomically visible behavior for the use cases we have described."
                },
                {
                    "heading": "3. SEMANTICS AND SYSTEM MODEL",
                    "text": "In this section, we formalize Read Atomic isolation and, to capture scalability, formulate a pair of strict scalability criteria: synchronization and partition independence. Readers more interested in RAMP algorithms may wish to proceed to Section 4."
                },
                {
                    "heading": "3.1 RA Isolation: Formal Specification",
                    "text": "To formalize RA isolation, as is standard [2], we consider ordered sequences of reads and writes to arbitrary sets of items, or transactions. We call the set of items a transaction reads from and writes to its read set and write set. Each write creates a version of an item and we identify versions of items by a unique timestamp taken from\na totally ordered set (e.g., rational numbers). Timestamps induce a total order on versions of each item (and a partial order across versions of different items). We denote version i of item x as xi.\nA transaction Tj exhibits fractured reads if transaction Ti writes versions xm and yn (in any order, with x possibly but not necessarily equal to y), Tj reads version xm and version yk, and k < n.\nA system provides Read Atomic isolation (RA) if it prevents fractured reads anomalies and also prevents transactions from reading uncommitted, aborted, or intermediate data. Thus, RA provides transactions with a \u201csnapshot\u201d view of the database that respects transaction boundaries (see the Appendix for more details, including a discussion of transitivity). RA is simply a restriction on write visibility\u2014if the ACID \u201cAtomicity\u201d property requires that all or none of a transaction\u2019s updates are performed, RA requires that all or none of a transaction\u2019s updates are made visible to other transactions."
                },
                {
                    "heading": "3.2 RA Implications and Limitations",
                    "text": "As outlined in Section 2.1, RA isolation matches many of our use cases. However, RA is not sufficient for all applications. RA does not prevent concurrent updates or provide serial access to data items. For example, RA is an incorrect choice for an application that wishes to maintain positive bank account balances in the event of withdrawals. RA is a better fit for our \u201cfriend\u201d operation because the operation is write-only and correct execution (i.e., inserting both records) is not conditional on concurrent updates.\nFrom a programmer\u2019s perspective, we have found RA isolation to be most easily understandable (at least initially) with read-only and write-only transactions; after all, because RA allows concurrent writes, any values that are read might be changed at any time. However, read-write transactions are indeed well defined under RA."
                },
                {
                    "heading": "3.3 System Model and Scalability",
                    "text": "We consider databases that are partitioned, with the set of items in the database spread over multiple servers. Each item has a single logical copy, stored on a server\u2014called the item\u2019s partition\u2014whose identity can be calculated using the item. Clients forward operations on each item to the item\u2019s partition, where they are executed. Transaction execution terminates in commit, signaling success, or abort, signaling failure. In our examples, all data items have the null value (?) at database initialization. We do not model replication of data items within a partition; this can happen at a lower level of the system than our discussion (see Section 4.6) as long as operations on each item are linearizable [4]. Scalability criteria. As we hinted in Section 1, large-scale deployments often eschew transactional functionality on the premise that it would be too expensive or unstable in the presence of failure and degraded operating modes [9, 11, 13, 15, 22, 25, 26, 38, 44]. Our goal in this paper is to provide robust and scalable transactional functionality, and, so we first define criteria for \u201cscalability\u201d:\nSynchronization independence ensures that one client\u2019s transactions cannot cause another client\u2019s to block and that, if a client can contact the partition responsible for each item in its transaction, the transaction will eventually commit (or abort of its own volition). This prevents one transaction from causing another to abort\u2014which is particularly important in the presence of partial failures\u2014and guarantees that each client is able to make useful progress. In the absence of failures, this maximizes useful concurrency. In the distributed systems literature, synchronization independence for replicated transactions is called transactional availability [5]. Note that \u201cstrong\u201d isolation models like serializability and Snapshot Isolation violate synchronization independence and limit scalability.\nWhile many applications can limit their data accesses to a single partition via explicit data modeling [7,19,25,38] or planning [18,35], this is not always possible. In the case of secondary indexing, there is a tangible cost associated with requiring single-partition updates (scatter-gather reads), while, in social networks like Facebook and large-scale hierarchical access patterns as in Rainbird, perfect partitioning of data accesses is near-impossible. Accordingly:\nPartition independence ensures that, in order to execute a transaction, a client never has to contact partitions that its transaction does not access. Thus, a partition failure only affects transactions that access items contained on the partition. This also reduces load on servers not directly involved in a transaction\u2019s execution. In the distributed systems literature, partition independence for replicated data is called replica availability [5] or genuine partial replication [39].\nIn addition to the above requirements, we limit the metadata overhead of algorithms. There are many potential solutions for providing atomic visibility that rely on storing prohibitive amounts of state. As a straw-man solution, each transaction could send copies of all of its writes to every partition it accesses so that readers observe all of its writes by reading a single item. This provides RA isolation but requires considerable storage. Other solutions may require extra data storage proportional to the number of servers in the cluster or, worse, the database size (Section 6). We will attempt to minimize this metadata\u2014that is, data that the transaction did not itself write but which is required for correct execution. In our algorithms, we will specifically provide constant-factor metadata overheads (RAMP-S, RAMP-H) or else overhead linear in transaction size (but independent of data size; RAMP-F)."
                },
                {
                    "heading": "4. RAMP TRANSACTION ALGORITHMS",
                    "text": "Given specifications for RA isolation and scalability, we present algorithms for achieving both. For ease of understanding, we first focus on providing read-only and write-only transactions with a \u201clast writer wins\u201d overwrite policy, then subsequently discuss how to perform read/write transactions. Our focus in this section is on intuition and understanding; we defer all correctness and scalability proofs to the Appendix, providing salient details inline.\nAt a high level, RAMP transactions allow reads and writes to proceed concurrently. This provides excellent performance but, in turn, introduces a race condition: one transaction might only read a subset of another transaction\u2019s writes, violating RA (i.e., fractured reads might occur). Instead of preventing this race (hampering scalability), RAMP readers autonomously detect the race (using metadata attached to each data item) and fetch any missing, in-flight writes from their respective partitions. To make sure that readers never have to block for writes to arrive at a partition, writers use a two-phase (atomic commitment) protocol that ensures that once a write is visible to readers on one partition, any other writes in the transaction are present on and, if appropriately identified by version, readable from their respective partitions.\nIn this section, we present three algorithms that provide a trade-off between the amount of metadata required and the expected number of extra reads to fetch missing writes. As discussed in Section 2, if techniques like distributed locking couple mutual exclusion with atomic visibility of writes, RAMP transactions correctly control visibility but allow concurrent and scalable execution."
                },
                {
                    "heading": "4.1 RAMP-Fast",
                    "text": "To begin, we present a RAMP algorithm that, in the race-free case, requires one RTT for reads and two RTTs for writes, called RAMP-Fast (abbreviated RAMP-F; Algorithm 1). RAMP-F stores metadata in the form of write sets (overhead linear in transaction size).\nOverview. Each write in RAMP-F (lines 14\u201321) contains a timestamp (line 15) that uniquely identifies the writing transaction as well as a set of items written in the transaction (line 16). For now, combining a unique client ID and client-local sequence number is sufficient for timestamp generation (see also Section 4.5).\nRAMP-F write transactions proceed in two phases: a first round of communication places each timestamped write on its respective partition. In this PREPARE phase, each partition adds the write to its local database (versions, lines 1, 17\u201319). A second round of communication marks versions as committed. In this COMMIT phase, each partition updates an index containing the highest-timestamped committed version of each item (lastCommit, lines 2, 20\u201321).\nRAMP-F read transactions begin by first fetching the last (highesttimestamped) committed version for each item from its respective partition (lines 23\u201330). Using the results from this first round of reads, each reader can calculate whether it is \u201cmissing\u201d any versions (that is, versions that were prepared but not yet committed on their partitions). Combining the timestamp and set of items from each version read (i.e., its metadata) produces a mapping from items to timestamps that represent the highest-timestamped write for each transaction that appears in this first-round read set (lines 26\u201329). If\nthe reader has read a version of an item that has a lower timestamp than indicated in the mapping for that item, the reader issues a second read to fetch the missing version (by timestamp) from its partition (lines 30\u201332). Once all missing versions are fetched (which can be done in parallel), the client can return the resulting set of versions\u2014the first-round reads, with any missing versions replaced by the optional, second round of reads. By example. Consider the RAMP-F execution depicted in Figure 1. T1 writes to both x and y, performing the two-round write protocol on two partitions, Px and Py. However, T2 reads from x and y while T1 is concurrently writing. Specifically, T2 reads from Px after Px has committed T1\u2019s write to x, but T2 reads from Py before Py has committed T1\u2019s write to y. Therefore, T2\u2019s first-round reads return x = x1 and y = ?, and returning this set of reads would violate RA. Using the metadata attached to its first-round reads, T2 determines that it is missing y1 (since vlatest [y] = 1 and 1 >?) and so T2 subsequently issues a second read from Py to fetch y1 by version. After completing its second-round read, T2 can safely return its result set. T1\u2019s progress is unaffected by T2, and T1 subsequently completes by committing y1 on Py. Why it works. RAMP-F writers use metadata as a record of intent: a reader can detect if it has raced with an in-progress commit round and use the metadata stored by the writer to fetch the missing data. Accordingly, RAMP-F readers only issue a second round of reads in the event that they read from a partially-committed write transaction (where some but not all partitions have committed a write). In this event, readers will fetch the appropriate writes from the not-yetcommitted partitions. Most importantly, RAMP-F readers never have to stall waiting for a write that has not yet arrived at a partition: the two-round RAMP-F write protocol guarantees that, if a partition commits a write, all of the corresponding writes in the transaction are present on their respective partitions (though possibly not committed locally). As long as a reader can identify the corresponding version by timestamp, the reader can fetch the version from the respective partition\u2019s set of pending writes without waiting. To enable this, RAMP-F writes contain metadata linear in the size of the writing transaction\u2019s write set (plus a timestamp per write).\nRAMP-F requires 2 RTTs for writes: one for PREPARE and one for COMMIT. For reads, RAMP-F requires one RTT in the absence of concurrent writes and two RTTs otherwise.\nRAMP timestamps are only used to identify specific versions and in ordering concurrent writes to the same item; RAMP-F transactions do not require a \u201cglobal\u201d timestamp authority. For example, if lastCommit[k] = 2, there is no requirement that a transaction with timestamp 1 has committed or even that such a transaction exists."
                },
                {
                    "heading": "4.2 RAMP-Small: Trading Metadata for RTTs",
                    "text": "While RAMP-F requires linearly-sized metadata but provides bestcase one RTT for reads, RAMP-Small (RAMP-S) uses constant-size metadata but always requires two RTT for reads (Algorithm 2). RAMP-S and RAMP-F writes are identical, but, instead of attaching the entire write set to each write, RAMP-S writers only store the transaction timestamp (line 7). Unlike RAMP-F, RAMP-S readers issue a first round of reads to fetch the highest committed timestamp for each item from its respective partition (lines 3, 9\u201311). Once RAMP-S readers have recieved the highest committed timestamp for each item, the readers send the entire set of timestamps they received to the partitions in a second round of communication (lines 13\u201314). For each item in the read request, RAMP-S servers return the highesttimestamped version of the item that also appears in the supplied set of timestamps (lines 5\u20136). Readers subsequently return the results from the mandatory second round of requests.\nAlgorithm 1 RAMP-Fast Server-side Data Structures 1: versions: set of versions hitem,value, timestamp tsv, metadata mdi 2: latestCommit[i]: last committed timestamp for item i\nServer-side Methods\n3: procedure PREPARE(v : version) 4: versions.add(v) 5: return 6: procedure COMMIT(tsc : timestamp) 7: Its {w.item | w 2 versions^w.tsv = tsc} 8: 8i 2 Its, latestCommit[i] max(latestCommit[i], tsc)\n9: procedure GET(i : item, tsreq : timestamp) 10: if tsreq = /0 then 11: return v 2 versions : v.item = i^ v.tsv = latestCommit[item] 12: else 13: return v 2 versions : v.item = i^ v.tsv = tsreq\nClient-side Methods\n14: procedure PUT_ALL(W : set of hitem,valuei) 15: tstx generate new timestamp 16: Itx set of items in W 17: parallel-for hi,vi 2W 18: v hitem = i,value = v, tsv = tstx,md = (Itx {i})i 19: invoke PREPARE(v) on respective server (i.e., partition) 20: parallel-for server s : s contains an item in W 21: invoke COMMIT(tstx) on s\n22: procedure GET_ALL(I : set of items) 23: ret {} 24: parallel-for i 2 I 25: ret[i] GET(i, /0) 26: vlatest {} (default value: 1) 27: for response r 2 ret do 28: for itx 2 r.md do 29: vlatest [itx] max(vlatest [itx],r.tsv) 30: parallel-for item i 2 I 31: if vlatest [i]> ret[i].tsv then 32: ret[i] GET(i,vlatest [i]) 33: return ret\nBy example. In Figure 1, under RAMP-S, Px and Py would respectively return the sets {1} and {?} in response to T2\u2019s first round of reads. T2 would subsequently send the set {1,?} to both Px and Py, which would return x1 and y1. (Including ? in the second-round request is unnecessary, but we leave it in for ease of understanding.) Why it works. In RAMP-S, if a transaction has committed on some but not all partitions, the transaction timestamp will be returned in the first round of any concurrent read transaction accessing the committed partitions\u2019 items. In the (required) second round of read requests, any prepared-but-not-committed partitions will find the committed timestamp in the reader-provided set and return the appropriate version. In contrast with RAMP-F, where readers explicitly provide partitions with a specific version to return in the (optional) second round, RAMP-S readers defer the decision of which version to return to the partition, which uses the reader-provided set to decide. This saves metadata but increases RTTs, and the size of the parameters of each second-round GET request is (worst-case) linear in the read set size. Unlike RAMP-F, there is no requirement to return the value of the last committed version in the first round (returning the version, lastCommit[k], suffices in line 3)."
                },
                {
                    "heading": "4.3 RAMP-Hybrid: An Intermediate Solution",
                    "text": "RAMP-Hybrid (RAMP-H; Algorithm 3) strikes a compromise between RAMP-F and RAMP-S. RAMP-H and RAMP-S write protocols are identical, but, instead of storing the entire write set (as in RAMP-F),\nAlgorithm 2 RAMP-Small Server-side Data Structures\nsame as in RAMP-F (Algorithm 1)\nServer-side Methods PREPARE, COMMIT same as in RAMP-F\n1: procedure GET(i : item, tsset : set of timestamps) 2: if tsset = /0 then 3: return v 2 versions : v.item = i^ v.tsv = latestCommit[k] 4: else 5: tsmatch = {t | t 2 tsset ^9v 2 versions : v.item = i^ v.tv = t} 6: return v 2 versions : v.item = i^ v.tsv = max(tsmatch)\nClient-side Methods\n7: procedure PUT_ALL(W : set of hitem,valuei) same as RAMP-F PUT_ALL but do not instantiate md on line 18\n8: procedure GET_ALL(I : set of items) 9: tsset {} 10: parallel-for i 2 I 11: tsset .add(GET(i, /0).tsv) 12: ret {} 13: parallel-for item i 2 I 14: ret[i] GET(i, tsset) 15: return ret\nRAMP-H writers store a Bloom filter [10] representing the transaction write set (line 1). RAMP-H readers proceed as in RAMP-F, with a first round of communication to fetch the last-committed version of each item from its partition (lines 3\u20135). Given this set of versions, RAMP-H readers subsequently compute a list of potentially highertimestamped writes for each item (lines 7\u201310). Any potentially missing versions are fetched in a second round of reads (lines 12). By example. In Figure 1, under RAMP-H, x1 would contain a Bloom filter with positives for x and y and y? would contain an empty Bloom filter. T2 would check for the presence of y in x1\u2019s Bloom filter (since x1\u2019s version is 1 and 1 > ?) and, finding a match, conclude that it is potentially missing a write (y1). T2 would subsequently fetch y1 from Py. Why it works. RAMP-H is effectively a hybrid between RAMP-F and RAMP-S. If the Bloom filter has no false positives, RAMP-H reads behave like RAMP-F reads. If the Bloom filter has all false positives, RAMP-H reads behave like RAMP-S reads. Accordingly, the number of (unnecessary) second-round reads (i.e., which would not be performed by RAMP-F) is controlled by the Bloom filter false positive rate, which is in turn (in expectation) proportional to the size of the Bloom filter. Any second-round GET requests are accompanied by a set of timestamps that is also proportional in size to the false positive rate. Therefore, RAMP-H exposes a trade-off between metadata size and expected performance. To understand why RAMP-H is safe, we simply have to show that any false positives (second-round reads) will not compromise the integrity of the result set; with unique timestamps, any reads due to false positives will return null."
                },
                {
                    "heading": "4.4 Summary of Basic Algorithms",
                    "text": "The RAMP algorithms allow readers to safely race writers without requiring either to stall. The metadata attached to each write allows readers in all three algorithms to safely handle concurrent and/or partial writes and in turn allows a trade-off between metadata size and performance (Table 1): RAMP-F is optimized for fast reads, RAMP-S is optimized for small metadata, and RAMP-H is, as the name suggests, a middle ground. RAMP-F requires metadata linear in transaction size, while RAMP-S and RAMP-H require constant metadata. However, RAMP-S and RAMP-H require more RTTs for reads compared to RAMP-F when there is no race between readers and writers.\nAlgorithm 3 RAMP-Hybrid Server-side Data Structures\nSame as in RAMP-F (Algorithm 1)\nServer-side Methods PREPARE, COMMIT same as in RAMP-F GET same as in RAMP-S\nClient-side Methods\n1: procedure PUT_ALL(W : set of hitem,valuei) same as RAMP-F PUT_ALL but instantiate md on line 18 with Bloom filter containing Itx\nWhen reads and writes race, in the worst case, all algorithms require two RTTs for reads. Writes always require two RTTs to prevent readers from stalling due to missing, unprepared writes.\nRAMP algorithms are scalable because clients only contact partitions relative to their transactions (partition independence), and clients cannot stall one another (synchronization independence). More specifically, readers do not interfere with other readers, writers do not interfere with other writers, and readers and writers can proceed concurrently. When a reader races a writer to the same items, the writer\u2019s new versions will only become visible to the reader (i.e., be committed) once it is guaranteed that the reader will be able to fetch all of them (possibly via a second round of communication). A reader will never have to stall waiting for writes to arrive at a partition (for details, see Invariant 1 in the Appendix)."
                },
                {
                    "heading": "4.5 Additional Details",
                    "text": "In this section, we discuss relevant implementation details.\nMulti-versioning and garbage collection. RAMP transactions rely on multi-versioning to allow readers to access versions that have not yet committed and/or have been overwritten. In our initial presentation, we have used a completely multi-versioned storage engine; in practice, multi-versioning can be implemented by using a single-versioned storage engine for retaining the last committed version of each item and using a \u201clook-aside\u201d store for access to both prepared-but-not-yet-committed writes and (temporarily) any overwritten versions. The look-aside store should make prepared versions durable but can\u2014at the risk of aborting transactions in the event of a server failure\u2014simply store any overwritten versions in\nmemory. Thus, with some work, RAMP algorithms are portable to legacy, non-multi-versioned storage systems.\nIn both architectures, each partition\u2019s data will grow without bound if old versions are not removed. If a committed version of an item is not the highest-timestamped committed version (i.e., a committed version v of item k where v < lastCommit[k]), it can be safely discarded (i.e., garbage collected, or GCed) as long as no readers will attempt to access it in the future (via second-round GET requests). It is easiest to simply limit the running time of read transactions and GC overwritten versions after a fixed amount of real time has elapsed. Any read transactions that take longer than this GC window can be restarted [32, 33]. Therefore, the maximum number of versions retained for each item is bounded by the item\u2019s update rate, and servers can reject any client GET requests for versions that have been GCed (and the read transaction can be restarted). As a more principled solution, partitions can also gossip the timestamps of items that have been overwritten and have not been returned in the first round of any ongoing read transactions. Read-write transactions. Until now, we have focused on readonly and write-only transactions. However, we can extend our algorithms to provide read-write transactions. If transactions predeclare the data items they wish to read, then the client can execute a GET_ALL transaction at the start of transaction execution to prefetch all items; subsequent accesses to those items can be served from this pre-fetched set. Clients can buffer any writes and, upon transaction commit, send all new versions to servers (in parallel) via a PUT_ALL request. As in Section 3, this may result in anomalies due to concurrent update but does not violate RA isolation. Given the benefits of pre-declared read/write sets [18, 35, 43] and write buffering [17, 41], we believe this is a reasonable strategy. For secondary index lookups, clients can first look up secondary index entries then subsequently (within the same transaction) read primary data (specifying versions from index entries as appropriate). Timestamps. Timestamps should be unique across transactions, and, for \u201csession\u201d consistency (Appendix), increase on a per-client basis. Given unique client IDs, a client ID and sequence number form unique transaction timestamps without coordination. Without unique client IDs, servers can assign unique timestamps with high probability using UUIDs and by hashing transaction contents. Overwrites. In our algorithms, we have depicted a policy in which versions are overwritten according to a highest-timestamp-wins policy. In practice, and, for commutative updates, users may wish to employ a different policy upon COMMIT: for example, perform set union. In this case, lastCommit[k] contains an abstract data type (e.g., set of versions) that can be updated with a merge operation [22, 42] (instead of updateI f Greater) upon commit. This treats each committed record as a set of versions, requiring additional metadata (that can be GCed as in Section 4.7)."
                },
                {
                    "heading": "4.6 Distribution and Fault Tolerance",
                    "text": "RAMP transactions operate in a distributed setting, which poses challenges due to latency, partial failure, and network partitions. Synchronization independence ensures that failed clients do not cause other clients to fail, while partition independence ensures that clients only have to contact partitions for items in their transactions. This provides fault tolerance and availability as long as clients can access relevant partitions, but here we further elucidate RAMP interactions with replication and stalled operations. Replication. A variety of mechanisms including traditional database master-slave replication with failover, quorum-based protocols, and state machine replication and can ensure availability of individual partitions in the event of individual server failure [8]. To control\ndurability, clients can wait until the effects of their operations (e.g., modifications to versions and lastCommit) are persisted locally on their respective partitions and/or to multiple physical servers before returning from PUT_ALL calls (either via master-to-slave replication or via quorum replication and by performing two-phase commit across multiple active servers). Notably, because RAMP transactions can safely overlap in time, replicas can process different transactions\u2019 PREPARE and COMMIT requests in parallel. Stalled Operations. RAMP writes use a two-phase atomic commitment protocol that ensures readers never block waiting for writes to arrive. As discussed in Section 2, every ACP may block during failures [8]. However, due to synchronization independence, a blocked transaction (due to failed clients, failed servers, or network partitions) cannot cause other transactions to block. Blocked writes instead act as \u201cresource leaks\u201d on partitions: partitions will retain prepared versions indefinitely unless action is taken.\nTo \u201cfree\u201d these leaks, RAMP servers can use the Cooperative Termination Protocol (CTP) described in [8]. CTP can always complete the transaction except when every partition has performed PREPARE but no partition has performed COMMIT. In CTP, if a server Sp has performed PREPARE for transaction T but times out waiting for a COMMIT, Sp can check the status of T on any other partitions for items in T \u2019s write set. If another server Sc has received COMMIT for T , then Sp can COMMIT T . If Sa, a server responsible for an item in T , has not received PREPARE for T , Sa and Sp can promise never to PREPARE or COMMIT T in the future and Sp can safely discard its versions. A client recovering from a failure can read from the servers to determine if they unblocked its write. Writes that block mid-COMMIT will also become visible on all partitions.\nCTP (evaluated in Section 5) only runs when writes block (or time-outs fire) and runs asynchronously with respect to other operations. CTP requires that PREPARE messages contain a list of servers involved in the transaction (a subset of RAMP-F metadata but a superset of RAMP-H and RAMP-S) and that servers remember when they COMMIT and \u201cabort\u201d writes (e.g., in a log file). Compared to alternatives (e.g., replicating clients [24]), we have found CTP to be both lightweight and effective."
                },
                {
                    "heading": "4.7 Further Optimizations",
                    "text": "RAMP algorithms also allow several possible optimizations: Faster commit detection. If a server returns a version in response to a GET request and the version\u2019s timestamp is greater than the highest committed version of that item (i.e., lastCommit), then transaction writing the version has committed on at least one partition. In this case, the server can mark the version as committed. This scenario will occur when all partitions have performed PREPARE and at least one server but not all partitions have performed COMMIT (as in CTP). This allows faster updates to lastCommit (and therefore fewer expected RAMP-F and RAMP-H RTTs). Metadata garbage collection. Once all of transaction T \u2019s writes are committed on each respective partition (i.e., are reflected in lastCommit), readers are guaranteed to read T \u2019s writes (or later writes). Therefore, non-timestamp metadata for T \u2019s writes stored in RAMP-F and RAMP-H (write sets and Bloom filters) can therefore be discarded. Detecting that all servers have performed COMMIT can be performed asynchronously via a third round of communication performed by either clients or servers. One-phase writes. We have considered two-phase writes, but, if a user does not wish to read her writes (thereby sacrificing session guarantees outlined in the Appendix), the client can return after issuing its PREPARE round (without sacrificing durability). The client can subsequently execute the COMMIT phase asynchronously,\nor, similar to optimizations presented in Paxos Commit [24], the servers can exchange PREPARE acknowledgements with one another and decide to COMMIT autonomously. This optimization is safe because multiple PREPARE phases can safely overlap."
                },
                {
                    "heading": "5. EXPERIMENTAL EVALUATION",
                    "text": "We proceed to experimentally demonstrate RAMP transaction scalability as compared to existing transactional and non-transactional mechanisms. RAMP-F, RAMP-H, and often RAMP-S outperform existing solutions across a range of workload conditions while exhibiting overheads typically within 8% and no more than 48% of peak throughput. As expected from our theoretical analysis, the performance of our RAMP algorithms does not degrade substantially under contention and scales linearly to over 7.1 million operations per second on 100 servers. These outcomes validate our choice to pursue synchronization- and partition-independent algorithms."
                },
                {
                    "heading": "5.1 Experimental Setup",
                    "text": "To demonstrate the effect of concurrency control on performance and scalability, we implemented several concurrency control algorithms in a partitioned, multi-versioned, main-memory database prototype. Our prototype is in Java and employs a custom RPC system with Kryo 2.20 for serialization. Servers are arranged as a distributed hash table with partition placement determined by random hashing. As in stores like Dynamo [22], clients can connect to any server to execute operations, which the server will perform on their behalf (i.e., each server acts as a client in our RAMP pseudocode). We implemented RAMP-F, RAMP-S, and RAMP-H and configure a wall-clock GC window of 5 seconds as described in Section 4.5. RAMP-H uses a 256-bit Bloom filter based on an implementation of MurmurHash2.0, with four hashes per entry; to demonstrate the effects of filter saturation, we do not modify these parameters in our experiments. Our prototype utilizes the \u201cFaster commit detection\u201d optimization from Section 4.5 but we chose not to employ the latter two optimizations in order to preserve session guarantees and because metadata overheads were generally minor. Algorithms for comparison. As a baseline, we do not employ any concurrency control (denoted NWNR, for no write and no read locks); reads and writes take one RTT and are executed in parallel.\nWe also consider three lock-based mechanisms: long write locks and long read locks, providing Repeatable Read isolation (PL-2.99; denoted LWLR), long write locks with short read locks, providing Read Committed isolation (PL-2L; denoted LWSR; does not provide RA), and long write locks with no read locks, providing Read Uncommitted isolation [2] (LWNR; also does not provide RA). While only LWLR provides RA, LWSR and LWNR provide a useful basis for comparison, particularly in measuring concurrency-related locking overheads. To avoid deadlocks, the system lexicographically orders lock requests by item and performs them sequentially. When locks are not used (as for reads in LWNR and reads and writes for NWNR), the system parallelizes operations.\nWe also consider an algorithm where, for each transaction, designated \u201ccoordinator\u201d servers enforce RA isolation\u2014effectively, the Eiger system\u2019s 2PC-PCI mechanism [33] (denoted E-PCI; Section 6). Writes proceed via prepare and commit rounds, but any reads that arrive at a partition and overlap with a concurrent write to the same item must contact a (randomly chosen, per-write-transaction) \u201ccoordinator\u201d partition to determine whether the coordinator\u2019s prepared writes have been committed. Writes require two RTTs, while reads require one RTT during quiescence and two RTTs in the presence of concurrent updates (to a variable number of coordinator partitions\u2014linear in the number of concurrent writes to the item).\nUsing a coordinator violates partition independence but not synchronization independence. We optimize 2PC-PCI reads by having clients determine a read timestamp for each transaction (eliminating an RTT) and do not include happens-before metadata.\nThis range of lock-based strategies (LWNR, LWSR, LWNR), recent comparable approach (E-PCI), and best-case (NWNR; no concurrency control) baseline provides a spectrum of strategies for comparison. Environment and benchmark. We evaluate each algorithm using the YCSB benchmark [16] and deploy variably-sized sets of servers on public cloud infrastructure. We employ cr1.8xlarge instances on Amazon EC2 and, by default, deploy five partitions on five servers. We group sets of reads and sets of writes into read-only and write-only transactions (default size: 4 operations), and use the default YCSB workload (workloada, with Zipfian distributed item accesses) but with a 95% read and 5% write proportion, reflecting read-heavy applications (Section 2, [11, 33, 44]; e.g., Tao\u2019s 500 to 1 reads-to-writes [11, 33], Espresso\u2019s 1000 to 1 Mailbox application [38], and Spanner\u2019s 3396 to 1 advertising application [17]).\nBy default, we use 5000 concurrent clients split across 5 separate EC2 instances and, to fully expose our metadata overheads, use a value size of 1 byte per write. We found that lock-based algorithms were highly inefficient for YCSB\u2019s default 1K item database, so we increased the database size to 1M items by default. Each version contains a timestamp (64 bits), and, with YCSB keys (i.e., item IDs) of size 11 bytes and a transaction length L, RAMP-F requires 11L bytes of metadata per version, while RAMP-H requires 32 bytes. We successively vary several parameters, including number of clients, read proportion, transaction length, value size, database size, and number of servers and report the average of three sixty-second trials."
                },
                {
                    "heading": "5.2 Experimental Results: Comparison",
                    "text": "Our first set of experiments focuses on two metrics: performance compared to baseline and performance compared to existing techniques. The overhead of RAMP algorithms is typically less than 8% compared to baseline (NWNR) throughput, is sometimes zero, and is never greater than 50%. RAMP-F and RAMP-H always outperform the lock-based and E-PCI techniques, while RAMP-S outperforms lock-based techniques and often outperforms E-PCI. We proceed to demonstrate this behavior over a variety of conditions: Number of clients. RAMP performance scales well with increased load and incurs little overhead (Figure 2). With few concurrent clients, there are few concurrent updates and therefore few secondround reads; performance for RAMP-F and RAMP-H is close to or even matches that of NWNR. At peak throughput (at 10,000 clients), RAMP-F and RAMP-H pay a throughput overhead of 4.2% compared to NWNR. RAMP-F and RAMP-H exhibit near-identical performance; the RAMP-H Bloom filter triggers few false positives (and therefore few extra RTTs compared to RAMP-F). RAMP-S incurs greater overhead and peaks at almost 60% of the throughput of NWNR. Its guaranteed two-round trip reads are expensive and it acts as an effective lower bound on RAMP-F and RAMP-H performance. In all configurations, the algorithms achieve low latency (RAMP-F, RAMP-H, NWNR less than 35ms on average and less than 10 ms at 5,000 clients; RAMP-S less than 53ms, 14.3 ms at 5,000 clients).\nIn comparison, the remaining algorithms perform less favorably. In contrast with the RAMP algorithms, E-PCI servers must check a coordinator server for each in-flight write transaction to determine whether to reveal writes to clients. For modest load, the overhead of these commit checks places E-PCI performance between that of RAMP-S and RAMP-H. However, the number of in-flight writes increases with load (and is worsened due to YCSB\u2019s Zipfian distributed accesses), increasing the number of E-PCI commit checks.\nThis in turn decreases throughput, and, with 10,000 concurrent clients, E-PCI performs so many commit checks per read (over 20% of reads trigger a commit check, and, on servers with hot items, each commit check requires indirected coordinator checks for an average of 9.84 transactions) that it underperforms the LWNR lockbased scheme. Meanwhile, multi-partition locking is expensive [35]: with 10,000 clients, the most efficient algorithm, LWNR, attains only 28.6% of the throughput of NWNR, while the least efficient, LWLR, attains only 1.6% (peaking at 3,412 transactions per second).\nWe subsequently varied several other workload parameters, which we briefly discuss below and plot in Figure 3: Read proportion. Increased write activity leads to a greater number of races between reads and writes and therefore additional second-round RTTs for RAMP-F and RAMP-H reads. With all write transactions, all RAMP algorithms are equivalent (two RTT) and achieve approximately 65% of the throughput of NWNR. With all reads, RAMP-F, RAMP-S, NWNR, and E-PCI are identical, with a single RTT. Between these extremes, RAMP-F and RAMP-S scale nearlinearly with the write proportion. In contrast, lock-based protocols fare poorly as contention increases, while E-PCI again incurs penalties due to commit checks. Transaction length. Increased transaction lengths have variable impact on the relative performance of RAMP algorithms. Synchronization independence does not penalize long-running transactions, but, with longer transactions, metadata overheads increase. RAMP-F relative throughput decreases due to additional metadata (linear in transaction length) and RAMP-H relative performance also decreases as its Bloom filters saturate. (However, YCSB\u2019s Zipfian-distributed access patterns result in a non-linear relationship between length and throughput.) As discussed above, we explicitly decided not to tune RAMP-H Bloom filter size but believe a logarithmic increase in filter size could improve RAMP-H performance for large transaction lengths (e.g., 1024 bit filters should lower the false positive rate for transactions of length 256 from over 92% to slightly over 2%). Value size. Value size similarly does not seriously impact relative throughput. At a value size of 1B, RAMP-F is within 2.3% of NWNR. However, at a value size of 100KB, RAMP-F performance nearly matches that of NWNR: the overhead due to metadata decreases, and write request rates slow, decreasing concurrent writes (and subse-\n1 10 100 1000 10000 100000 Value Size (bytes)\n0\n30K\n60K\n90K\n120K\n150K\n180K\nTh ro\nug hp\nut (tx\nn/ s)\n10 100 1000 10K 100K 1M 10M Database Size (items)\n0\n30K\n60K\n90K\n120K\n150K\n180K\nTh ro\nug hp\nut (tx\nn/ s)\nFigure 3: Algorithm performance across varying workload conditions. RAMP-F and RAMP-H exhibit similar performance to NWNR baseline, while RAMP-S\u2019s 2 RTT reads incur a greater performance penalty across almost all configurations. RAMP transactions consistently outperform RA isolated alternatives.\nquently second-round RTTs). Nonetheless, absolute throughput drops by a factor of 24 as value sizes moves from 1B to 100KB. Database size. RAMP algorithms are robust to high contention for a small set of items: with only 1000 items in the database, RAMP-F achieves throughput within 3.1% of NWNR. RAMP algorithms are largely agnostic to read/write contention, although, with fewer items in the database, the probability of races between readers and inprogress writers increases, resulting in additional second-round reads for RAMP-F and RAMP-H. In contrast, lock-based algorithms fare poorly under high contention, while E-PCI indirected commit checks again incurred additional overhead. By relying on clients (rather than additional partitions) to repair fractured writes, RAMP-F, RAMP-H, and RAMP-S performance is less affected by hot items.\nOverall, RAMP-F and RAMP-H exhibit performance close to that of no concurrency control due to their independence properties and guaranteed worst-case performance. As the proportion of writes\nincreases, an increasing proportion of RAMP-F and RAMP-H operations take two RTTs and performance trends towards that of RAMP-S, which provides a constant two RTT overhead. In contrast, lockbased protocols perform poorly under contention while E-PCI triggers more commit checks than RAMP-F and RAMP-H trigger second round reads (but still performs well without contention and for particularly read-heavy workloads). The ability to allow clients to independently verify read sets enables good performance despite a range of (sometimes adverse) conditions (e.g., high contention)."
                },
                {
                    "heading": "5.3 Experimental Results: CTP Overhead",
                    "text": "We also evaluated the overhead of blocked writes in our implementation of the Cooperative Termination Protocol discussed in Section 4.6. To simulate blocked writes, we artificially dropped a percentage of COMMIT commands in PUT_ALL calls such that clients returned from writes early and partitions were forced to complete the commit via CTP. This behavior is worse than expected because \u201cblocked\u201d clients continue to issue new operations. The table below reports the throughput reduction as the proportion of blocked writes increases (compared to no blocked writes) for a workload of 100% RAMP-F write transactions:\nBlocked % 0.01% 0.1% 25% 50% Throughput No change 99.86% 77.53% 67.92%\nAs these results demonstrate, CTP can reduce throughput because each commit check consumes resources (here, network and CPU capacity). However, CTP only performs commit checks in the event of blocked writes (or time-outs; set to 5s in our experiments), so a modest failure rate of 1 in 1000 writes has a limited effect. The higher failure rates produce a near-linear throughput reduction but, in practice, a blocking rate of even a few percent is likely indicative of larger systemic failures. As Figure 3 hints, the effect of additional metadata for the participant list in RAMP-H and RAMP-S is limited, and, for our default workload of 5% writes, we observe similar trends but with throughput degradation of 10% or less across the above configurations. This validates our initial motivation behind the choice of CTP: average-case overheads are small."
                },
                {
                    "heading": "5.4 Experimental Results: Scalability",
                    "text": "We finally validate our chosen scalability criteria by demonstrating linear scalability of RAMP transactions to 100 servers. We deployed an increasing number of servers within the us-west-2 EC2 region and, to mitigate the effects of hot items during scaling, configured uniform random access to items. We were unable\nto include more than 20 instances in an EC2 \u201cplacement group,\u201d which guarantees 10 GbE connections between instances, so, past 20 servers, servers communicated over a degraded network. Around 40 servers, we exhausted the us-west-2b \u201cavailability zone\u201d (datacenter) capacity and had to allocate our instances across the remaining zones, further degrading network performance. However, as shown in Figure 4, each RAMP algorithm scales linearly, even though in expectation, at 100 servers, all but one in 100M transactions is a multi-partition operation. In particular, RAMP-F achieves slightly under 7.1 million operations per second, or 1.79 million transactions per second on a set of 100 servers (71,635 operations per partition per second). At all scales, RAMP-F throughput was always within 10% of NWNR. With 100 servers, RAMP-F was within 2.6%, RAMP-S within 3.4%, and RAMP-S was within 45% of NWNR. In light of our scalability criteria, this behavior is unsurprising."
                },
                {
                    "heading": "6. RELATED WORK",
                    "text": "Replicated databases offer a broad spectrum of isolation guarantees at varying costs to performance and availability [8]: Serializability. At the strong end of the isolation spectrum is serializability, which provides transactions with the equivalent of a serial execution (and therefore also provides RA). A range of techniques can enforce serializability in distributed databases [3, 8], multi-version concurrency control (e.g. [37]) locking (e.g. [31]), and optimistic concurrency control [41]. These useful semantics come with costs in the form of decreased concurrency (e.g., contention and/or failed optimistic operations) and limited availability during partial failure [5, 21]. Many designs [19, 29] exploit cheap serializability within a single partition but face scalability challenges for distributed operations. Recent industrial efforts like F1 [41] and Spanner [17] have improved performance via aggressive hardware advances but, their reported throughput is still limited to 20 and 250 writes per item per second. Multi-partition serializable transactions are expensive and, especially under adverse conditions, are likely to remain expensive [18, 28, 35]. Weak isolation. The remainder of the isolation spectrum is more varied. Most real-world databases offer (and often default to) nonserializable isolation models [5, 34]. These \u201cweak isolation\u201d levels allow greater concurrency and fewer system-induced aborts compared to serializable execution but provide weaker semantic guarantees. For example, the popular choice of Snapshot Isolation prevents Lost Update anomalies but not Write Skew anomalies [2]; by preventing Lost Update, concurrency control mechanisms providing Snapshot Isolation violate synchronization independence [5]. In recent years, many \u201cNoSQL\u201d designs have avoided cross-partition transactions entirely, effectively providing Read Uncommitted isolation in many industrial databases such PNUTS [15], Dynamo [22], TAO [11], Espresso [38], Rainbird [44], and BigTable [13]. These systems avoid penalties associated with stronger isolation but in turn sacrifice transactional guarantees (and therefore do not offer RA). Related mechanisms. There are several algorithms that are closely related to our choice of RA and RAMP algorithm design.\nCOPS-GT\u2019s two-round read-only transaction protocol [32] is similar to RAMP-F reads\u2014client read transactions identify causally inconsistent versions by timestamp and fetch them from servers. While COPS-GT provides causal consistency (requiring additional metadata), it does not support RA isolation for multi-item writes.\nEiger provides its write-only transactions [33] by electing a coordinator server for each write. As discussed in Section 5 (E-PCI), the number of \u201ccommit checks\u201d performed during its read-only transactions is proportional to the number of concurrent writes. Using a coordinator violates partition independence but in turn provides\ncausal consistency. This coordinator election is analogous to GStore\u2019s dynamic key grouping [19] but with weaker isolation guarantees; each coordinator effectively contains a partitioned completed transaction list from [12]. Instead of relying on indirection, RAMP transaction clients autonomously assemble reads and only require constant factor (or, for RAMP-F, linear in transaction size) metadata size compared to Eiger\u2019s PL-2L (worst-case linear in database size).\nRAMP transactions are inspired by our earlier proposal for Monotonic Atomic View (MAV) isolation: transactions read from a monotonically advancing view of database state [5]. MAV is strictly weaker than RA and does not prevent fractured reads, as required for our applications (i.e., reads are not guaranteed to be transactionally aligned). The prior MAV algorithm we briefly sketched in [5] is similar to RAMP-F but, as a consequence of its weaker semantics, allows one-round read transactions. The RAMP algorithms described here are portable to the highly available (i.e., nonlinearizable, \u201cAP/EL\u201d [1, 23]) replicated setting of [5], albeit with necessary penalties to latency between updates and their visibility.\nOverall, we are not aware of a concurrency control mechanism for partitioned databases that provides synchronization independence, partition independence, and at least RA isolation."
                },
                {
                    "heading": "7. CONCLUSION",
                    "text": "This paper described how to achieve atomically visible multipartition transactions without incurring the performance and availability penalties of traditional algorithms. We first identified a new isolation level\u2014Read Atomic isolation\u2014that provides atomic visibility and matches the requirements of a large class of real-world applications. We subsequently achieved RA isolation via scalable, contention-agnostic RAMP transactions. In contrast with techniques that use inconsistent but fast updates, RAMP transactions provide correct semantics for applications requiring secondary indexing, foreign key constraints, and materialized view maintenance while maintaining scalability and performance. By leveraging multi-versioning with a variable but small (and, in two of three algorithms, constant) amount of metadata per write, RAMP transactions allow clients to detect and assemble atomic sets of versions in one to two rounds of communication with servers (depending on the RAMP implementation). The choice of synchronization and partition independent algorithms allowed us to achieve near-baseline performance across a variety of workload configurations and scale linearly to 100 servers. While RAMP transactions are not appropriate for all applications, the many for which they are well suited will benefit measurably.\nAcknowledgments The authors would like to thank Peter Alvaro, Giselle Cheung, Neil Conway, Aaron Davidson, Mike Franklin, Aurojit Panda, Nuno Pregui\u00e7a, Edward Ribeiro, Shivaram Venkataraman, and the SIGMOD reviewers for their insightful feedback. This research is supported by NSF CISE Expeditions award CCF1139158 and DARPA XData Award FA8750-12-2-0331, the National Science Foundation Graduate Research Fellowship (grant DGE-1106400), and gifts from Amazon Web Services, Google, SAP, Apple, Inc., Cisco, Clearstory Data, Cloudera, EMC, Ericsson, Facebook, GameOnTalis, General Electric, Hortonworks, Huawei, Intel, Microsoft, NetApp, NTT Multimedia Communications Laboratories, Oracle, Samsung, Splunk, VMware, WANdisco and Yahoo!."
                },
                {
                    "heading": "8. REFERENCES",
                    "text": "[1] D. J. Abadi. Consistency tradeoffs in modern distributed database system design:\nCAP is only part of the story. IEEE Computer, 45(2):37\u201342, 2012. [2] A. Adya. Weak consistency: a generalized theory and optimistic implementations\nfor distributed transactions. PhD thesis, MIT, 1999. [3] D. Agrawal and V. Krishnaswamy. Using multiversion data for non-interfering\nexecution of write-only transactions. In SIGMOD 1991.\n[4] H. Attiya and J. Welch. Distributed Computing: Fundamentals, Simulations and Advanced Topics (2nd edition). John Wiley Interscience, March 2004. [5] P. Bailis, A. Davidson, A. Fekete, A. Ghodsi, J. M. Hellerstein, and I. Stoica. Highly Available Transactions: Virtues and Limitations. In VLDB 2014. [6] P. Bailis, A. Fekete, A. Ghodsi, J. M. Hellerstein, and I. Stoica. The potential dangers of causal consistency and an explicit solution. In SOCC 2012. [7] J. Baker, C. Bond, J. Corbett, J. Furman, et al. Megastore: Providing scalable, highly available storage for interactive services. In CIDR 2011. [8] P. Bernstein, V. Hadzilacos, and N. Goodman. Concurrency control and recovery in database systems. Addison-wesley New York, 1987. [9] K. Birman, G. Chockler, and R. van Renesse. Toward a cloud computing research agenda. SIGACT News, 40(2):68\u201380, June 2009. [10] B. H. Bloom. Space/time trade-offs in hash coding with allowable errors. CACM, 13(7):422\u2013426, 1970. [11] N. Bronson, Z. Amsden, G. Cabrera, P. Chukka, P. Dimov, et al. TAO: Facebook\u2019s distributed data store for the social graph. In USENIX ATC 2013. [12] A. Chan and R. Gray. Implementing distributed read-only transactions. IEEE Transactions on Software Engineering, (2):205\u2013212, 1985. [13] F. Chang, J. Dean, S. Ghemawat, W. C. Hsieh, D. A. Wallach, M. Burrows, et al. Bigtable: A distributed storage system for structured data. In OSDI 2006. [14] R. Chirkova and J. Yang. Materialized views. Foundations and Trends in Databases, 4(4):295\u2013405, 2012. [15] B. F. Cooper, R. Ramakrishnan, U. Srivastava, A. Silberstein, P. Bohannon, et al. PNUTS: Yahoo!\u2019s hosted data serving platform. In VLDB 2008. [16] B. F. Cooper, A. Silberstein, E. Tam, R. Ramakrishnan, and R. Sears. Benchmarking cloud serving systems with YCSB. In ACM SOCC 2010. [17] J. C. Corbett, J. Dean, M. Epstein, A. Fikes, C. Frost, J. J. Furman, et al. Spanner: Google\u2019s globally-distributed database. In OSDI 2012. [18] C. Curino, E. Jones, Y. Zhang, and S. Madden. Schism: a workload-driven approach to database replication and partitioning. In VLDB 2010. [19] S. Das, D. Agrawal, and A. El Abbadi. G-store: a scalable data store for transactional multi key access in the cloud. In ACM SOCC 2010. [20] K. Daudjee and K. Salem. Lazy database replication with ordering guarantees. In ICDE 2004, pages 424\u2013435. [21] S. Davidson, H. Garcia-Molina, and D. Skeen. Consistency in partitioned networks. ACM Computing Surveys, 17(3):341\u2013370, 1985. [22] G. DeCandia, D. Hastorun, M. Jampani, G. Kakulapati, A. Lakshman, et al. Dynamo: Amazon\u2019s highly available key-value store. In SOSP 2007. [23] S. Gilbert and N. Lynch. Brewer\u2019s conjecture and the feasibility of consistent, available, partition-tolerant web services. SIGACT News, 33(2):51\u201359, 2002. [24] J. Gray and L. Lamport. Consensus on transaction commit. ACM TODS, 31(1):133\u2013160, Mar. 2006. [25] P. Helland. Life beyond distributed transactions: an apostate\u2019s opinion. In CIDR 2007. [26] S. Hull. 20 obstacles to scalability. Commun. ACM, 56(9):54\u201359, 2013. [27] N. Huyn. Maintaining global integrity constraints in distributed databases.\nConstraints, 2(3/4):377\u2013399, Jan. 1998. [28] E. P. Jones, D. J. Abadi, and S. Madden. Low overhead concurrency control for\npartitioned main memory databases. In SIGMOD 2010. [29] R. Kallman, H. Kimura, J. Natkins, A. Pavlo, et al. H-Store: a high-performance,\ndistributed main memory transaction processing system. In VLDB 2008. [30] R. J. Lipton and J. S. Sandberg. PRAM: a scalable shared memory. Technical\nReport TR-180-88, Princeton University, September 1988. [31] F. Llirbat, E. Simon, D. Tombroff, et al. Using versions in update transactions:\nApplication to integrity checking. In VLDB 1997. [32] W. Lloyd, M. J. Freedman, et al. Don\u2019t settle for eventual: scalable causal\nconsistency for wide-area storage with COPS. In SOSP 2011. [33] W. Lloyd, M. J. Freedman, M. Kaminsky, and D. G. Andersen. Stronger\nsemantics for low-latency geo-replicated storage. In NSDI 2013. [34] C. Mohan. History repeats itself: Sensible and NonsenSQL aspects of the\nNoSQL hoopla. In EDBT 2013. [35] A. Pavlo, C. Curino, and S. Zdonik. Skew-aware automatic database partitioning\nin shared-nothing, parallel OLTP systems. In SIGMOD 2012. [36] D. Peng and F. Dabek. Large-scale incremental processing using distributed\ntransactions and notifications. In OSDI 2010. [37] S. H. Phatak and B. Badrinath. Multiversion reconciliation for mobile databases.\nIn ICDE 1999. [38] L. Qiao, K. Surlaker, S. Das, T. Quiggle, et al. On brewing fresh Espresso:\nLinkedIn\u2019s distributed data serving platform. In SIGMOD 2013. [39] N. Schiper, P. Sutra, and F. Pedone. P-store: Genuine partial replication in wide\narea networks. In IEEE SRDS 2010. [40] M. Shapiro et al. A comprehensive study of convergent and commutative\nreplicated data types. Technical Report 7506, INRIA, 2011. [41] J. Shute et al. F1: A distributed SQL database that scales. In VLDB 2013. [42] D. B. Terry, A. J. Demers, K. Petersen, M. J. Spreitzer, M. M. Theimer, and B. B.\nWelch. Session guarantees for weakly consistent replicated data. In PDIS 1994. [43] A. Thomson, T. Diamond, S. Weng, K. Ren, P. Shao, and D. Abadi. Calvin: Fast\ndistributed transactions for partitioned database systems. In SIGMOD 2012. [44] K. Weil. Rainbird: Real-time analytics at Twitter. Strata 2011\nhttp://slidesha.re/hjMOui. [45] S. B. Zdonik. Object-oriented type evolution. In DBPL, pages 277\u2013288, 1987. [46] J. Zhou et al. Lazy maintenance of materialized views. In VLDB 2007."
                },
                {
                    "heading": "APPENDIX: Proofs and Isolation Details",
                    "text": "RAMP-F Correctness. To prove RAMP-F provides RA isolation, we show that the two-round read protocol returns a transactionally atomic set of versions. To do so, we formalize criteria for atomic (read) sets of versions in the form of companion sets. We will call the set of versions produced by a transaction sibling versions and call two items from the same write set sibling items.\nGiven two versions xi and y j , we say that xi is a companion to y j if xi is a transactional sibling of y j or x is a sibling item of y j and i > j. We say that a set of versions V is a companion set if, for every pair (xi,y j) of versions in V where x is a sibling item of y j , xi is a companion to y j . In Figure 1, the versions returned by T2\u2019s first round of reads ({x1,y?}) do not comprise a companion set because y? has a lower timestamp than x1\u2019s sibling version of y (that is, x1 has sibling version y1 and but ?< 1 so y? has too low of a timestamp). Subsets of companion sets are also companion sets and companion sets also have a useful property for RA isolation:\nClaim 1 (Companion sets are atomic). Companion sets do not contain fractured reads. Proof. Claim 1 follows from the definitions of companion sets and fractured reads. If V is a companion set, then every version xi 2V is also a companion to every other version y j 2V where v j contains x in its sibling items. If V contained fractured reads, V would contain two versions xi,y j such that the transaction that wrote y j also wrote a version xk , i < k. However, in this case, xi would not be a companion to y j , a contradiction. Therefore, V cannot contain fractured reads.\nTo provide RA, RAMP-F clients assemble a companion set for the requested items (in vlatest ), which we prove below:\nClaim 2. RAMP-F provides Read Atomic isolation. Proof. Each write in RAMP-F contains information regarding its siblings, which can be identified by item and timestamp. Given a set of RAMP-F versions, recording the highest timestamped version of each item (as recorded either in the version itself or via sibling metadata) yields a companion set of item-timestamp pairs: if a client reads two versions xi and y j such that x is in y j\u2019s sibling items but i < j, then vlatest [x] will contain j and not i. Accordingly, given the versions returned by the first round of RAMP-F reads, clients calculate a companion set containing versions of the requested items. Given this companion set, clients check the first-round versions against this set by timestamp and issue a second round of reads to fetch any companions that were not returned in the first round. The resulting set of versions will be a subset of the computed companion set and will therefore also be a companion set. This ensures that the returned results do not contain fractured reads. RAMP-F first-round reads access lastCommit, so each transaction corresponding to a first-round version is committed, and, therefore, any siblings requested in the (optional) second round of reads are also committed. Accordingly, RAMP-F never reads aborted or non-final (intermediate) writes. This establishes that RAMP-F provides RA.\nRAMP-F Scalability and Independence. RAMP-F also provides the independence guarantees from Section 3.3. The following invariant over lastCommit is core to RAMP-F GET request completion:\nInvariant 1 (Companions present). If a version xi is referenced by lastCommit (that is, lastCommit[x] = i), then each of xi\u2019s sibling versions are present in versions on their respective partitions.\nInvariant 1 is maintained by RAMP-F\u2019s two-phase write protocol. lastCommit is only updated once a transaction\u2019s writes have been placed into versions by a first round of PREPARE messages. Siblings will be present in versions (but not necessarily lastCommit).\nClaim 3. RAMP-F provides synchronization independence. Proof. Clients in RAMP-F do not communicate or coordinate with one another and only contact servers. Accordingly, to show that RAMP-F provides synchronization independence, it suffices to show that server-side operations always terminate. PREPARE and COMMIT methods only access data stored on the local partition and do not block due to external coordination or other method invocations; therefore, they complete. GET requests issued in the first round of reads have tsreq =? and therefore will return the version corresponding to lastCommit[k], which was placed into versions in a previously completed PREPARE round. GET requests issued in the second round of client reads have tsreq set to the client\u2019s calculated vlatest [k]. vlatest [k] is a sibling of a version returned from lastCommit in the first round, so, due to\nInvariant 1, the requested version will be present in versions. Therefore, GET invocations are guaranteed access to their requested version and can return without waiting. The success of RAMP-F operations do not depend on the success or failure of other clients\u2019 RAMP-F operations.\nClaim 4. RAMP-F provides partition independence. Proof. RAMP-F transactions do not access partitions that are unrelated to each transaction\u2019s specified data items and servers do not contact other servers in order to provide a safe response for operations.\nRAMP-S Correctness. RAMP-S writes and first-round reads proceed identically to RAMP-F writes, but the metadata written and returned is different. Therefore, the proof is similar to RAMP-F, with a slight modification for the second round of reads.\nClaim 5. RAMP-S provides Read Atomic isolation. Proof. To show that RAMP-S provides RA, it suffices to show that RAMP-S second-round reads (resp) are a companion set. Given two versions xi,y j 2 resp such that x 6= y, if x is a sibling item of y j , then xi must be a companion to y j . If xi were not a companion to y j , then it would imply that x is not a sibling item of y j (so we are done) or that j > i. If j > i, then, due to Invariant 1 (which also holds for RAMP-S writes due to identical write protocols), y j\u2019s sibling is present in versions on the partition for x and would have been returned by the server (line 6), a contradiction. Each second-round GET request returns only one version, so we are done.\nRAMP-S Scalability and Independence. RAMP-S provides synchronization independence and partition independence. For brevity, we again omit full proofs, which closely resemble those of RAMP-F. RAMP-H Correctness. The probabilistic behavior of the RAMP-H Bloom filter admits false positives. However, given unique transaction timestamps (Section 4.5), requesting false siblings by timestamp and item does not affect correctness:\nClaim 6. RAMP-H provides Read Atomic isolation. Proof. To show that RAMP-H provides Read Atomic isolation, it suffices to show that any versions requested by RAMP-H second-round reads that would not have been requested by RAMP-F second-round reads (call this set v f alse) do not compromise the validity of RAMP-H\u2019s returned companion set. Any versions in v f alse do not exist: timestamps are unique, so, for each version xi, there are no versions x j of non-sibling items with the same timestamp as xi (i.e., where i = j). Therefore, requesting versions in v f alse do not change the set of results collected in the second round.\nRAMP-H Scalability and Independence. RAMP-H provides synchronization independence and partition independence. We omit full proofs, which closely resemble those of RAMP-F. The only significant difference from RAMP-F is that second-round GET requests may return ?, but, as we showed above, these empty responses correspond to false positives in the Bloom filter and therefore do not affect correctness. Comparison to other isolation levels. The fractured reads anomaly is similar to Adya\u2019s \u201cMissing Transaction Updates\u201d definition, only applied to immediate read dependencies (rather than all transitive dependencies). RA is stronger than PL-2 (Read Committed), but weaker than PL-SI, PL-CS, and PL-2.99 (notably, RA does not prevent anti-dependency cycles, or Adya\u2019s G2 or G-SIa\u2014informally, it allows concurrent updates) [2].\nRA does not (by itself) provide ordering guarantees across transactions. Our RAMP implementations provide a variant of PRAM consistency, where, for each item, each user\u2019s writes are serialized [30] (i.e., \u201csession\u201d ordering [20]), and, once a user\u2019s operation completes, all other users will observe its effects (regular register semantics, applied at the transaction level). This provides transitivity with respect to each user\u2019s operations. For example, if a user updates her privacy settings and subsequently posts a new photo, the photo cannot be read without the privacy setting change [15]. However, PRAM does not respect the happens-before relation [4] across users. If Sam reads Mary\u2019s comment and replies to it, other users may read Sam\u2019s comment without Mary\u2019s comment. In this case, RAMP transactions can leverage explicit causality [6] via foreign key dependencies, but happensbefore is not provided by default. If required, we believe it is possible to enforce happens-before but, due to scalability concerns regarding metadata and partition independence (e.g., [6] and Section 5), do not further explore this possibility. An \u201cactive-active\u201d replicated implementation can provide available [5, 23] operation at the cost of these recency guarantees."
                }
            ],
            "year": 2014,
            "references": [
                {
                    "title": "Consistency tradeoffs in modern distributed database system design: CAP is only part of the story",
                    "authors": [
                        "D.J. Abadi"
                    ],
                    "venue": "IEEE Computer, 45(2):37\u201342,",
                    "year": 2012
                },
                {
                    "title": "Weak consistency: a generalized theory and optimistic implementations for distributed transactions",
                    "authors": [
                        "A. Adya"
                    ],
                    "venue": "PhD thesis, MIT,",
                    "year": 1999
                },
                {
                    "title": "Using multiversion data for non-interfering execution of write-only transactions",
                    "authors": [
                        "D. Agrawal",
                        "V. Krishnaswamy"
                    ],
                    "venue": "SIGMOD",
                    "year": 1991
                },
                {
                    "title": "Distributed Computing: Fundamentals, Simulations and Advanced Topics (2nd edition)",
                    "authors": [
                        "H. Attiya",
                        "J. Welch"
                    ],
                    "venue": "John Wiley Interscience, March",
                    "year": 2004
                },
                {
                    "title": "Highly Available Transactions: Virtues and Limitations",
                    "authors": [
                        "P. Bailis",
                        "A. Davidson",
                        "A. Fekete",
                        "A. Ghodsi",
                        "J.M. Hellerstein",
                        "I. Stoica"
                    ],
                    "venue": "VLDB",
                    "year": 2014
                },
                {
                    "title": "The potential dangers of causal consistency and an explicit solution",
                    "authors": [
                        "P. Bailis",
                        "A. Fekete",
                        "A. Ghodsi",
                        "J.M. Hellerstein",
                        "I. Stoica"
                    ],
                    "venue": "SOCC",
                    "year": 2012
                },
                {
                    "title": "Megastore: Providing scalable, highly available storage for interactive services",
                    "authors": [
                        "J. Baker",
                        "C. Bond",
                        "J. Corbett",
                        "J. Furman"
                    ],
                    "year": 2011
                },
                {
                    "title": "Concurrency control and recovery in database systems",
                    "authors": [
                        "P. Bernstein",
                        "V. Hadzilacos",
                        "N. Goodman"
                    ],
                    "venue": "Addison-wesley New York,",
                    "year": 1987
                },
                {
                    "title": "Toward a cloud computing research agenda",
                    "authors": [
                        "K. Birman",
                        "G. Chockler",
                        "R. van Renesse"
                    ],
                    "venue": "SIGACT News,",
                    "year": 2009
                },
                {
                    "title": "Space/time trade-offs in hash coding with allowable errors",
                    "authors": [
                        "B.H. Bloom"
                    ],
                    "venue": "CACM, 13(7):422\u2013426,",
                    "year": 1970
                },
                {
                    "title": "TAO: Facebook\u2019s distributed data store for the social graph",
                    "authors": [
                        "N. Bronson",
                        "Z. Amsden",
                        "G. Cabrera",
                        "P. Chukka",
                        "P. Dimov"
                    ],
                    "venue": "ATC",
                    "year": 2013
                },
                {
                    "title": "Implementing distributed read-only transactions",
                    "authors": [
                        "A. Chan",
                        "R. Gray"
                    ],
                    "venue": "IEEE Transactions on Software Engineering, (2):205\u2013212,",
                    "year": 1985
                },
                {
                    "title": "Bigtable: A distributed storage system for structured data",
                    "authors": [
                        "F. Chang",
                        "J. Dean",
                        "S. Ghemawat",
                        "W.C. Hsieh",
                        "D.A. Wallach",
                        "M. Burrows"
                    ],
                    "year": 2006
                },
                {
                    "title": "Materialized views",
                    "authors": [
                        "R. Chirkova",
                        "J. Yang"
                    ],
                    "venue": "Foundations and Trends in Databases, 4(4):295\u2013405,",
                    "year": 2012
                },
                {
                    "title": "PNUTS: Yahoo!\u2019s hosted data serving platform",
                    "authors": [
                        "B.F. Cooper",
                        "R. Ramakrishnan",
                        "U. Srivastava",
                        "A. Silberstein",
                        "P. Bohannon"
                    ],
                    "year": 2008
                },
                {
                    "title": "Benchmarking cloud serving systems with YCSB",
                    "authors": [
                        "B.F. Cooper",
                        "A. Silberstein",
                        "E. Tam",
                        "R. Ramakrishnan",
                        "R. Sears"
                    ],
                    "venue": "ACM SOCC",
                    "year": 2010
                },
                {
                    "title": "Spanner: Google\u2019s globally-distributed database",
                    "authors": [
                        "J.C. Corbett",
                        "J. Dean",
                        "M. Epstein",
                        "A. Fikes",
                        "C. Frost",
                        "J.J. Furman"
                    ],
                    "year": 2012
                },
                {
                    "title": "Schism: a workload-driven approach to database replication and partitioning",
                    "authors": [
                        "C. Curino",
                        "E. Jones",
                        "Y. Zhang",
                        "S. Madden"
                    ],
                    "venue": "VLDB",
                    "year": 2010
                },
                {
                    "title": "G-store: a scalable data store for transactional multi key access in the cloud",
                    "authors": [
                        "S. Das",
                        "D. Agrawal",
                        "A. El Abbadi"
                    ],
                    "venue": "ACM SOCC",
                    "year": 2010
                },
                {
                    "title": "Lazy database replication with ordering guarantees",
                    "authors": [
                        "K. Daudjee",
                        "K. Salem"
                    ],
                    "venue": "ICDE",
                    "year": 2004
                },
                {
                    "title": "Consistency in partitioned networks",
                    "authors": [
                        "S. Davidson",
                        "H. Garcia-Molina",
                        "D. Skeen"
                    ],
                    "venue": "ACM Computing Surveys, 17(3):341\u2013370,",
                    "year": 1985
                },
                {
                    "title": "Dynamo: Amazon\u2019s highly available key-value store",
                    "authors": [
                        "G. DeCandia",
                        "D. Hastorun",
                        "M. Jampani",
                        "G. Kakulapati",
                        "A. Lakshman"
                    ],
                    "year": 2007
                },
                {
                    "title": "Brewer\u2019s conjecture and the feasibility of consistent, available, partition-tolerant web services",
                    "authors": [
                        "S. Gilbert",
                        "N. Lynch"
                    ],
                    "venue": "SIGACT News, 33(2):51\u201359,",
                    "year": 2002
                },
                {
                    "title": "Consensus on transaction commit",
                    "authors": [
                        "J. Gray",
                        "L. Lamport"
                    ],
                    "venue": "ACM TODS, 31(1):133\u2013160, Mar.",
                    "year": 2006
                },
                {
                    "title": "Life beyond distributed transactions: an apostate\u2019s opinion",
                    "authors": [
                        "P. Helland"
                    ],
                    "venue": "CIDR",
                    "year": 2007
                },
                {
                    "title": "20 obstacles to scalability",
                    "authors": [
                        "S. Hull"
                    ],
                    "venue": "Commun. ACM, 56(9):54\u201359,",
                    "year": 2013
                },
                {
                    "title": "Maintaining global integrity constraints in distributed databases",
                    "authors": [
                        "N. Huyn"
                    ],
                    "venue": "Constraints, 2(3/4):377\u2013399, Jan.",
                    "year": 1998
                },
                {
                    "title": "Low overhead concurrency control for partitioned main memory databases",
                    "authors": [
                        "E.P. Jones",
                        "D.J. Abadi",
                        "S. Madden"
                    ],
                    "venue": "SIGMOD",
                    "year": 2010
                },
                {
                    "title": "H-Store: a high-performance, distributed main memory transaction processing system",
                    "authors": [
                        "R. Kallman",
                        "H. Kimura",
                        "J. Natkins",
                        "A. Pavlo"
                    ],
                    "year": 2008
                },
                {
                    "title": "PRAM: a scalable shared memory",
                    "authors": [
                        "R.J. Lipton",
                        "J.S. Sandberg"
                    ],
                    "venue": "Technical Report TR-180-88, Princeton University, September",
                    "year": 1988
                },
                {
                    "title": "Using versions in update transactions: Application to integrity checking",
                    "authors": [
                        "F. Llirbat",
                        "E. Simon",
                        "D. Tombroff"
                    ],
                    "year": 1997
                },
                {
                    "title": "Don\u2019t settle for eventual: scalable causal consistency for wide-area storage with COPS",
                    "authors": [
                        "W. Lloyd",
                        "M.J. Freedman"
                    ],
                    "year": 2011
                },
                {
                    "title": "Stronger semantics for low-latency geo-replicated storage",
                    "authors": [
                        "W. Lloyd",
                        "M.J. Freedman",
                        "M. Kaminsky",
                        "D.G. Andersen"
                    ],
                    "venue": "NSDI",
                    "year": 2013
                },
                {
                    "title": "History repeats itself: Sensible and NonsenSQL aspects of the NoSQL hoopla",
                    "authors": [
                        "C. Mohan"
                    ],
                    "venue": "EDBT",
                    "year": 2013
                },
                {
                    "title": "Skew-aware automatic database partitioning in shared-nothing, parallel OLTP systems",
                    "authors": [
                        "A. Pavlo",
                        "C. Curino",
                        "S. Zdonik"
                    ],
                    "venue": "SIGMOD",
                    "year": 2012
                },
                {
                    "title": "Large-scale incremental processing using distributed transactions and notifications",
                    "authors": [
                        "D. Peng",
                        "F. Dabek"
                    ],
                    "venue": "OSDI",
                    "year": 2010
                },
                {
                    "title": "Multiversion reconciliation for mobile databases",
                    "authors": [
                        "S.H. Phatak",
                        "B. Badrinath"
                    ],
                    "venue": "ICDE",
                    "year": 1999
                },
                {
                    "title": "On brewing fresh Espresso: LinkedIn\u2019s distributed data serving platform",
                    "authors": [
                        "L. Qiao",
                        "K. Surlaker",
                        "S. Das",
                        "T. Quiggle"
                    ],
                    "year": 2013
                },
                {
                    "title": "P-store: Genuine partial replication in wide area networks",
                    "authors": [
                        "N. Schiper",
                        "P. Sutra",
                        "F. Pedone"
                    ],
                    "venue": "IEEE SRDS",
                    "year": 2010
                },
                {
                    "title": "A comprehensive study of convergent and commutative replicated data types",
                    "authors": [
                        "M. Shapiro"
                    ],
                    "venue": "Technical Report 7506,",
                    "year": 2011
                },
                {
                    "title": "F1: A distributed SQL database that scales",
                    "authors": [
                        "J. Shute"
                    ],
                    "year": 2013
                },
                {
                    "title": "Session guarantees for weakly consistent replicated data",
                    "authors": [
                        "D.B. Terry",
                        "A.J. Demers",
                        "K. Petersen",
                        "M.J. Spreitzer",
                        "M.M. Theimer",
                        "B.B. Welch"
                    ],
                    "venue": "PDIS",
                    "year": 1994
                },
                {
                    "title": "Calvin: Fast distributed transactions for partitioned database systems",
                    "authors": [
                        "A. Thomson",
                        "T. Diamond",
                        "S. Weng",
                        "K. Ren",
                        "P. Shao",
                        "D. Abadi"
                    ],
                    "venue": "SIGMOD",
                    "year": 2012
                },
                {
                    "title": "Rainbird: Real-time analytics at Twitter. Strata 2011 http://slidesha.re/hjMOui",
                    "authors": [
                        "K. Weil"
                    ],
                    "year": 2011
                },
                {
                    "title": "Object-oriented type evolution",
                    "authors": [
                        "S.B. Zdonik"
                    ],
                    "venue": "DBPL, pages 277\u2013288,",
                    "year": 1987
                }
            ],
            "id": "SP:11115adf31476509f7b9600b6edada5c1340006e",
            "authors": [
                {
                    "name": "Peter Bailis",
                    "affiliations": []
                },
                {
                    "name": "Alan Fekete",
                    "affiliations": []
                },
                {
                    "name": "Ali Ghodsi",
                    "affiliations": []
                },
                {
                    "name": "Joseph M. Hellerstein",
                    "affiliations": []
                },
                {
                    "name": "Ion Stoica",
                    "affiliations": []
                }
            ],
            "abstractText": "Databases can provide scalability by partitioning data across several servers. However, multi-partition, multi-operation transactional access is often expensive, employing coordination-intensive locking, validation, or scheduling mechanisms. Accordingly, many realworld systems avoid mechanisms that provide useful semantics for multi-partition operations. This leads to incorrect behavior for a large class of applications including secondary indexing, foreign key enforcement, and materialized view maintenance. In this work, we identify a new isolation model\u2014Read Atomic (RA) isolation\u2014that matches the requirements of these use cases by ensuring atomic visibility: either all or none of each transaction\u2019s updates are observed by other transactions. We present algorithms for Read Atomic MultiPartition (RAMP) transactions that enforce atomic visibility while offering excellent scalability, guaranteed commit despite partial failures (via synchronization independence), and minimized communication between servers (via partition independence). These RAMP transactions correctly mediate atomic visibility of updates and provide readers with snapshot access to database state by using limited multi-versioning and by allowing clients to independently resolve non-atomic reads. We demonstrate that, in contrast with existing algorithms, RAMP transactions incur limited overhead\u2014even under high contention\u2014and scale linearly to 100 servers.",
            "title": "Scalable Atomic Visibility with RAMP Transactions"
        },
        "Y": {
            "blog_id": "scalable-atomic-visibility-with-ramp-transactions",
            "summary": [
                "Scalable Atomic Visibility with RAMP Transactions \u2013 Bailis et al. 2014  RAMP transactions came up last week as part of the secret sauce in Coordination avoidance in database systems that contributed to a 25x improvement on the TPC-C benchmark.",
                "So what exactly are RAMP transactions and why might we need them?",
                "As soon as you partition your database across multiple servers, things start to get interesting.",
                "We\u2019d like to maintain atomic isolation \u2013 either all of a transaction\u2019s effects are visible or none are \u2013 for transactions that span partitions\u2026  The status quo for these multi-partition atomic transactions provides an uncomfortable choice between algorithms that are fast but deliver inconsistent results and algorithms that deliver consistent results but are often slow and unavailable under failure.",
                "A lot of implemented systems have chosen to go with the fast-and-furious option resulting in incorrect behaviour for cases where atomic visibility matters.",
                "The RAMP (Read Atomic Multiple Partition) transaction models introduced in this paper show that you can have performance and scalability of transactions spanning multiple partitions with atomic visibility.",
                "\u2026data stores like Bigtable, Dynamo, and many popular \u201cNoSQL\u201d and even some \u201cNewSQL\u201d stores do not provide transactional guarantees for multi-item operations.",
                "The designers of these Internet-scale, real-world systems have made a conscious decision to provide scalability at the expense of multi-partition transactional semantics.",
                "Our goal with RAMP transactions is to preserve this scalability but deliver correct, atomically visible behavior for the use cases we have described.",
                "Under evaluation, the RAMP algorithms did not degrade substantially under contention, and scaled linearly to over 7.1 million operations per second on 100 servers.",
                "Bad things that can happen when you don\u2019t have atomic multi-partition isolation  Without atomic isolation foreign key constraints, secondary indexing, and materialized view maintenance can all break!",
                "Data models often represent bi-directional relationships as two distinct uni-directional relationships.",
                "\u201cFor example, in TAO, a user performing a \u2018like\u2019 action on a Facebook page produces updates to both the LIKES and LIKED_BY associations.\u201d  These applications require foreign key maintenance and often, due to their unidirectional relationships, multi-entity update and access.",
                "Without atomic isolation broken bi-directional relationships, and dangling or incorrect references can surface.",
                "With data partitioned across servers by primary key, access by secondary attributes becomes more challenging.",
                "There are two dominant strategies for distributed secondary indexing.",
                "First, the local secondary index approach co-locates secondary indexes and primary data, so each server contains a secondary index that only references (and indexes) data stored on its server.",
                "This allows easy, single-server updates but requires contacting every partition for secondary attribute lookups (write-one, read-all), compromising scalability for read-heavy workloads.",
                "Alternatively, the global secondary index approach locates secondary indexes (which may be partitioned, but by a secondary attribute) separately from primary data.",
                "This alternative allows fast secondary lookups (read-one) but requires multi-partition update (at least write-two)  Real-world services tend to use either local secondary indexing (non-scalable but correct), or non-atomic (scalable but incorrect) global indexes.",
                "In the latter cases queries involving the secondary attributes can return records that shouldn\u2019t match, and omit ones that should.",
                "Without atomic isolation, materialized views can diverge from the base data.",
                "For example, a count may become inaccurate.",
                "With RAMP transactions, base data and views can be updated atomically.",
                "The physical maintenance of a view depends on its specification, but RAMP transactions provide appropriate concurrency control primitives for ensuring that changes are delivered to the materialized view partition.",
                "For select-project views, a simple solution is to treat the view as a separate table and perform maintenance as needed: new rows can be inserted/deleted according to the specification, and, if necessary, the view can be (re-)computed on demand (i.e., lazy view maintenance).",
                "For more complex views, such as counters, users can execute RAMP transactions over specialized data structures such as the CRDT G-Counter.",
                "Scalability Requirements  Consider databases that are partitioned over multiple servers.",
                "Each item has a single logical copy stored on one of those partitions, which one can be calculated using the item itself (e.g. primary key).",
                "In order to achieve scalability the author\u2019s identify two key properties that must be preserved: synchronization independence, and partition independence.",
                "Synchronization independence ensures that one client\u2019s transactions cannot cause another client\u2019s to block and that, if a client can contact the partition responsible for each item in its transaction, the transaction will eventually commit (or abort of its own volition).",
                "(Also known as transactional availability).",
                "Partition independence ensures that, in order to execute a transaction, a client never has to contact partitions that its transaction does not access.",
                "Thus, a partition failure only affects transactions that access items contained on the partition.",
                "This also reduces load on servers not directly involved in a transaction\u2019s execution.",
                "In the distributed systems literature, partition independence for replicated data is called replica availability or genuine partial replication.",
                "A third constraint is that the metadata required to achieve synchronization and partition independence is not too large: \u201cthere are many potential solutions for providing atomic visibility that rely on storing prohibitive amounts of state.\u201d  The RAMP transaction algorithms  You may be wondering why I keep referring to algorithms (plural).",
                "This is because the authors actually define three RAMP variants: RAMP-Fast, RAMP-Small, and RAMP-Hybrid.",
                "These trade-off between performance and the amount of metadata that needs to be kept.",
                "At a high level, RAMP transactions allow reads and writes to proceed concurrently.",
                "This provides excellent performance but, in turn, introduces a race condition: one transaction might only read a subset of another transaction\u2019s writes, violating RA (i.e., fractured reads might occur).",
                "Instead of preventing this race (hampering scalability), RAMP readers autonomously detect the race (using metadata attached to each data item) and fetch any missing, in-flight writes from their respective partitions.",
                "To make sure that readers never have to block for writes to arrive at a partition, writers use a two-phase (atomic commitment) protocol that ensures that once a write is visible to readers on one partition, any other writes in the transaction are present on and, if appropriately identified by version, readable from their respective partitions.",
                "RAMP-Fast stores metadata in the form of write sets (thus the overhead is linear in transaction size), and has one RTT for reads in the best case (two in the worst case).",
                "RAMP-Small uses constant size metadata (it only stores the transaction timestamp) but always requires two RTT for reads.",
                "RAMP-Hybrid takes the same write set information as RAMP-Fast, but encodes it in a Bloom filter.",
                "With no false positives from the filter, Ramp-Hybrid would therefore behave as RAMP-Fast.",
                "And with all false positives, it behaves as RAMP-Small.",
                "All of the variants require two RTTs/transaction for writes.",
                "The two-phase atomic commitment protocol used by RAMP ensures readers never block waiting for writes to arrive.",
                "It is known that every atomic commitment protocol may block during failures.",
                "Blocked writes instead act as \u201cresource leaks\u201d on partitions: partitions will retain prepared versions indefinitely unless action is taken.",
                "To \u201cfree\u201d these leaks, RAMP servers can use the Cooperative Termination Protocol (CTP).",
                "CTP can always complete the transaction except when every partition has performed PREPARE but no partition has performed COMMIT\u2026 Compared to alternatives (e.g. replicating clients), we have found CTP to be both lightweight and effective.",
                "There is of course much more detail in the full paper, which I encourage you to go on and read.",
                "Section 6 on Related Work contains a nice short summary of isolation guarantees in the wild.",
                "\u201cIn recent years, many \u2018NoSQL\u2019 designs have avoided cross-partition transactions entirely, effectively providing Read Uncommitted isolation\u2026\u201d"
            ],
            "author_id": "ACOLYER",
            "pdf_url": "http://www.bailis.org/papers/ramp-sigmod2014.pdf",
            "author_full_name": "Adrian Colyer",
            "source_website": "https://blog.acolyer.org/about/",
            "id": 87411149
        }
    },
    "8984509": {
        "X": {
            "sections": [
                {
                    "text": "CCS CONCEPTS \u2022 Computer systems organization \u2192 Architectures;\nKEYWORDS Uncertainty Propagation, DAG Data Processing\nACM Reference Format: Ioannis Manousakis, \u00cd\u00f1igo Goiri, Ricardo Bianchini, Sandro Rigo, and Thu D. Nguyen. 2018. Uncertainty Propagation in Data Processing Systems. In Proceedings of ACM Symposium on Cloud Computing, Carlsbad, CA, USA, October 11\u201313, 2018 (SoCC \u201918), 12 pages. https://doi.org/10.1145/3267809.3267833\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SoCC \u201918, October 11\u201313, 2018, Carlsbad, CA, USA \u00a9 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-6011-1/18/10. . . $15.00 https://doi.org/10.1145/3267809.3267833"
                },
                {
                    "heading": "1 INTRODUCTION",
                    "text": "Data is being produced and collected at a tremendous pace. The need to process this vast amount of data has led to the design and deployment of data processing systems such as MapReduce, Spark and Scope [7, 32, 37]. These frameworks typically allow data processing applications to be expressed as directed acyclic graphs (DAGs) of side-effect free computation nodes, with data flowing through the edges for processing. The frameworks then run applications on clusters of servers, transparently handling issues such as task scheduling, data movement, and fault tolerance.\nAt the same time, there is an urgent need for processing an exploding body of data with uncertainties [4]. For example, data collected using sensors are always estimates that have uncertainties\u2014 the differences between the estimated and true values\u2014due to sensor inaccuracies. Data uncertainties also arise in many other contexts, including probabilistic modeling [10], machine learning [26], approximate storage [29], and the use of sampling-based approximation that produce estimated outputs with error bounds [2, 11].\nFor many applications, uncertain data should be represented as probability distributions or estimated values with error bounds rather than exact values. Failure to properly account for this uncertainty may lead to incorrect results. For example, Bornholt et al. have shown that computing speeds from recorded GPS positions can lead to absurd values (e.g., walking speeds above 30mph) when ignoring uncertainties in the recordings [4]. Unfortunately, developing applications for processing uncertain data is a major challenge from both the mathematical and performance perspectives. Thus, in this paper, we propose and evaluate a general framework that significantly eases this challenging task. Embedding such a framework in systems such as MapReduce and Spark will make it easily available to many developers working in many application domains.\nOur framework is based on techniques that allow programmers to modify precise implementations of DAG computation nodes to handle uncertain inputs with modest effort. Uncertainties can then be propagated locally across each node of the DAG from the point where they are first introduced to the final outputs of the computation. More specifically, we use Differential Analysis (DA) [3] to propagate uncertainties through DAG nodes that are continuous and differentiable functions. For semi-continuous functions, we propagate uncertainties through a combination of DA and Monte Carlo simulation, where our framework automatically selects the appropriate method based on the input distributions and the locations of function discontinuities. For all other function types, we use Monte Carlo simulation.\nAs an example of how a developer uses our framework, suppose a company needs to run a revenue prediction model implemented by a two-node logical DAG 1 shown in Figure 1(a). The first node approximates the number of customers belonging to different age groups in a database using BlinkDB [2]. The second node then computes the revenue as a weighted average, with the (uncertain) weights representing the predicted revenue per customer in a given age group. While the approximation can significantly reduce the execution time of the first node, it produces estimates with uncertainties (error bounds), rather than precise values. A developer can use our proposed framework to handle these uncertainties in the second node by providing the derivatives for the weighted average, which are essentially just the weights, with very few code changes to the precise version. This small amount of additional work will allow the answer to be computed as a distribution rather than an exact value that gives a misleading impression of precision. In particular, a precise answer, e.g., red line in Figure 1(c), may predict high revenue leading to profit while ignoring the left side of the distribution in Figure 1(c), which indicates a significant possibility of low revenue leading to an overall loss. Ignoring this possibility can be dangerous if the company is risk-averse.\nWe implement the proposed framework in UP-MapReduce, an extension of the Hadoop MapReduce, to handle uncertainty propagation (UP). UP-MapReduce allows programmers to develop applications with UP in much the same way as their precise counterparts. Added efforts come in the form of selecting the appropriate uncertain Mapper and Reducer classes provided by UP-MapReduce and respecting some required constraints on code structures (Section 5). Developers can optionally provide closed-form derivatives for DAG nodes that implement UP with DA to enhance performance.\nWe then leverage UP-MapReduce to build a toolbox of operations (e.g., sum, multiply, logarithm) on uncertain data and modify ten applications, including AI/ML, image processing, trend analysis,\n1As explained in Section 4, small logical DAGs will often map to extremely large execution DAGs with thousands of execution nodes running on large server clusters when processing large data sets.\nand model construction applications, to process uncertain data. Our experience shows that UP-MapReduce is easy to use. Running two of these applications on real data sets demonstrates the tremendous potential for combining sampling-based approximation (early in the DAG) with UP to reduce execution time while properly propagating the introduced uncertainties to the final outputs. This propagation allows users to intelligently trade off accuracy for execution time. For example, in one application, execution time can be reduced by 2.3x if the user can tolerate errors of up to 5%. Further, in one of the two applications, the original data set is a sample of network probes and so any computation on this sample necessarily has to deal with uncertainties. UP-MapReduce allows developers to easily tackle these uncertainties.\nWe also perform extensive sensitivity analyses on small to large execution DAGs (ranging up to tens of thousands of nodes), using eight of the applications with synthetic data, which allows us to adjust various input characteristics. Specifically, we explore the impact of UP on the magnitudes of uncertainties (e.g., whether uncertainties become worse after propagation), the accuracy of our UP techniques, overheads of UP, and scalability. Our results show that our UP techniques are highly accurate in most cases. Furthermore, the performance overheads of UP using DA are very low \u2013 average of 6% performance degradation \u2013 when closed-form derivatives are provided. Performance overheads are more significant when using DA with numerical differentiation or Monte Carlo simulation as input size increases, but this performance impact can be reduced by adding computation resources. Recall that these overheads arise from the need to process uncertain data instead of exact values. Finally, our results demonstrate that UP-MapReduce scales well to a cluster with 512 servers.\nIn summary, our contributions include: (1) identifying existing theories appropriate for UP and showing how to apply them to DAG-based data processing frameworks, (2) designing and implementing our proposed UP approach in a MapReduce framework called UP-MapReduce, (3) implementing a suite of data processing applications to explore the accuracy, performance, and scalability of UP-MapReduce, and (4) showing that our approach is highly effective in many scenarios, allowing applications to efficiently account for data uncertainties."
                },
                {
                    "heading": "2 BACKGROUND AND RELATEDWORK",
                    "text": "In this section we first motivate the necessity for UP by presenting a (non-exhaustive) list of common uncertainty sources where UP is required if the data uncertainties are not to be ignored. We then proceed to discuss related work and in particular recent approximate methods that generate uncertainty as byproducts of the approximation. Finally, we review previous work in uncertainty estimation and belief propagation. Sources of Uncertainty. Collecting data from imprecise instruments such as temperature, position or other analog sensors often introduces measurement uncertainty. In these applications, acquiring precise data is typically not an option, but it is usually possible to tune precision at the expense of resources such as more expensive sensors, higher response time or energy consumption. An example of such a trade-off is the potential for a sensor network to enter a\nlow-power state to conserve energy at the expense of providing lower quality measurements.\nSimilarly, model uncertainty is introduced when computational models used in applications do not precisely describe physical phenomena. For example, in structure strength analysis, one may simulate the macroscopic impact of wind on a high-level model of a bridge structure, rather than modeling the forces on the individual molecules (which might be intractable).\nApproximate computing is an emerging source of approximation uncertainty. In this setting, it may be possible for the user to tradeoff precision (how much uncertainty) against execution time and/or energy consumption. Examples include iterative refinement techniques or aggregate approximation schemes (via sampling) such as BlinkDB [2] and ApproxHadoop [11]. This is a particularly interesting scenario since execution time savings achieved via approximation may be offset by the necessity for UP in subsequent nodes of a computation DAG. Other types of approximation-induced uncertainty include statistical estimators (i.e., from maximum likelihood or a posteriori estimation) and approximate storage [29]. Approximate computing with bounded errors. Extensive past work has been done in approximate computing with quality estimates by the systems, hardware and database communities. The purpose of approximate computing is to reduce the required resources (e.g., execution time and/or energy consumption) by relaxing the precision of the output but also providing estimates on the (uncertain) output quality \u2013 for example, the mean and variance of the output values.\nMost prior works have focused on reducing the input set by sampling and/or dropping computation. For example, the database community has long considered the problem through approximate query processing. There, database systems sample the input data set and/or drop sub-queries to accelerate top-level queries with the ultimate goal of reducing response time, increasing throughput [1, 16, 38], and/or even providing response time guarantees [2].\nSome works identify computational blocks (at compile or runtime) that can be dropped for tunable approximation with or without accuracy estimates [24, 28]. ApproxHadoop accelerates the computation of large-scale aggregations (i.e., sum, count, average) by combining sampling and computation drop [11] while providing error estimates. Others provide energy bounds by online tuning of the approximation levels [14].\nFinally, the hardware community approaches the problem by trading hardware accuracy for energy efficiency, performance and transistors. For example, Esmaeilzadeh et al. [9] designed an ISA extension that provides approximate operations and proposed a micro-architecture that implements approximate functional units such as adders, multipliers, and approximate load-store units (a problem that was also tackled later by Miguel et al. [23]). Uncertainty estimation and belief propagation. Prior work has been proposed to handle the uncertainty introduced by approximate systems and to perform belief propagation where uncertainty and prior beliefs are combined to perform inference. Approximate programming, for example, seeks to design systems and programming languages that implement and bound the errors of various arithmetic and logical operators (addition, multiplication, and comparison) when handling uncertain (probabilistic) types.\nFor example, Uncertain<T> [4] is a language construct that can be used to estimate the output distribution of a graph of basic operations that compose a program. Uncertain<T> can be used for inference as well, by using Bayesian statistics to derive the posterior distribution. Others have worked on probabilistic programming to implement type systems [5, 34] and compiler transformations [6, 25] to handle uncertainty, error bounding, and inference for uncertain programs. Sampson et al. [30] worked on decision making under uncertainty which is necessary to implement branches and assertions in programs. In contrast with arithmetic operations, comparison operators are more challenging, as they involve estimating the tail of the (unknown) distribution \u2013 much like our approach for UP through semi-continuous functions. Differentiation from prior work. We differentiate from past work in uncertainty estimation as being the first to bring uncertainty propagation techniques to large-scale computational DAGs. In contrast to prior work (e.g., Uncertain <T> and ApproxHadoop) where uncertainty estimation is performed only for basic arithmetic and logical operations, we can handle arbitrary functions. At this high level of abstraction, new challenges arise. For example, accounting for covariances between uncertain data items may become a limiting factor in the performance and scalability of computations on uncertain data.\nOur method also offers, to the best of our knowledge, the only known computationally tractable (and as our evaluation will show, potentially with low overheads) large-scale uncertainty propagation. Several other UP methods, such as polynomial chaos expansion and fast integration can also be used (in fact to estimate the actual distribution of Y instead of just computing the first two moments) [13, 19]. However, these methods are very computationally expensive, especially with increasing number of variables as noted by Lee and Chen [19]. We also do not perform any inference or multi-dimensional convolutions (as in Uncertain <T>) which suffer from high computational complexity and limit their applicability to only a few hundred input variables per node in the execution DAG. On the contrary, we show that our approach can handle millions of input variables with relatively low overhead."
                },
                {
                    "heading": "3 UNCERTAINTY PROPAGATION",
                    "text": "In this section we introduce our proposed methods for handling uncertain inputs at a DAG node. Specifically, we discuss how to (approximately) compute Y = f (X), where f is an arbitrary function without side effects, representing the computation of a DAG node, X is a set of random variables representing inputs with uncertainties, and Y is a set of random variables representing outputs with uncertainties. Depending on the nature of f (continuous, semi-continuous or discrete), we leverage a set of three statistical methods to approximate the mean \u00b5Yi and the variance \u03c3 2Yi for each Yi in Y. These methods are described below."
                },
                {
                    "heading": "3.1 UP Through Continuous Functions",
                    "text": "We use first-order Differential Analysis (DA) to approximate the first two moments of Y, i.e., mean and variance, for functions f that are continuous and differentiable [3]. The general strategy is to compute Y by approximating f using its first-order Taylor series at the expected value of X. This approximation is accurate if f is\nroughly linear around the support (in other words, neighborhood) of X; errors are being introduced otherwise. As shall be seen in Section 7.3, using the first-order Taylor series gives good accuracy for the majority of the applications we study.\nFor simplicity, we present DA equations for a single output value Y; we refer the reader to [3] for the full derivation of the multiple input, multiple output case. Let Y = f (X), withX = {X1,X2, ...,Xn }. We can compute an approximation Y\u0302 of Y using the first-order Taylor series around a given point X0 = {X 01 ,X 02 , ...,X 0n } as:\nY\u0302 = \u03b10 + n\u2211 i=1 \u03b1i (Xi \u2212 X 0i ) (1)\n\u03b10 = f (X0) and \u03b1i = \u2202 f \u2202Xi (X0)\nWe then compute an approximate mean \u00b5\u0302Y by setting X0 = \u00b5X = {\u00b5X1 , \u00b5X2 , ..., \u00b5Xn } and computing the expected value of Y\u0302 .\n\u00b5\u0302Y = E[Y\u0302 ] = E [ \u03b10 + n\u2211 i=1 \u03b1i (Xi \u2212 \u00b5Xi ) ]\n(2)\n= \u03b10 + n\u2211 i=1 (\u03b1iE[Xi ] \u2212 \u03b1i \u00b5Xi )\n= \u03b10 + n\u2211 i=1 (\u03b1i \u00b5Xi \u2212 \u03b1i \u00b5Xi ) = f (\u00b5X)\nAnalogously, we can derive an estimate of the variance \u03c3\u0302 2Y using the first-order Taylor series:\n\u03c3\u0302 2Y = n\u2211 i=1 \u03b12i \u03c3 2 Xi + n\u2211 i=1 n\u2211 j=1, j,i \u03b12i \u03b1 2 j \u03c3 2 XiX j (3)\nwhere \u03c3 2XiX j is the covariance of Xi and X j . If we assume that the inputs are independent, so that \u03c3XiX j = 0, i , j, then Equation 3 reduces to the left summand.\nWe illustrate the computation of the mean and variance for the single-input, single-output case (Y = f (X )) in Figure 2(a). For the general case with multiple inputs and multiple outputs, one must also be concerned with the covariances between the outputs as shown in Figure 2(b). In general, these covariances may be nonzero. Thus, if the multiple outputs are being used as inputs to a later stage of computation as in Y = f (X),Z = \u0434(Y), then we cannot assume that Yi and Yj , i , j , in Y are independent. Rather, it would\nbe necessary to compute the covariances \u03c3 2YiYj and use them when approximating Z [3]."
                },
                {
                    "heading": "3.2 UP Through Semi-continuous Functions",
                    "text": "We can leverage the above approach for semi-continuous functions when the support of each Xi in X falls mostly or entirely within a continuous and differentiable part of the function. We adopt two approaches for checking with high confidence which intervals of X lie in continuous parts. The first assumes each Xi is approximately normal allowing the estimation of the support using any desired confidence interval through the corresponding covariance matrix of the input. The second approach makes no assumption about the distribution of X. It instead uses a multivariate generalization of the Chebyshev\u2019s inequality [17] to bound the probability that X lies within any interval. For example, suppose we define a filter function as f (X ) = 1 when X > \u03b1 and 0 otherwise. This is a simple semi-continuous function defined on two intervals. Our framework automatically performs the required run-time checks for each Xi . In this case, it will check if X lies entirely (or mostly) in (\u03b1,+\u221e) or (\u2212\u221e,\u03b1]. If the condition is satisfied, it will leverage DA to propagate through the filtering function which in this case leads to an exact result. If X \u2019s support spans the discontinuity, our framework is forced to resort to Monte Carlo simulation which we discuss next."
                },
                {
                    "heading": "3.3 UP Through Black-box Functions",
                    "text": "We use Monte Carlo simulation to approximate Y for functions f that do not meet (or the developers to not know whether they meet) the requirements for DA. Specifically, we evaluate f on n randomly drawn samples of X (input) and use the outputs as an approximation of Y. As n \u2192 \u221e, the empirical distribution obtained for each Yi converges to the true distribution. To choose n, we use the following expression which bounds the difference between the empirical and the true distribution [21]:\nP ( sup y\u2208R (F\u0302i ,n (y) \u2212 Fi (y)) > \u03f5 ) \u2264 2e\u22122n\u03f5 2 (4)\nwhere F\u0302i ,n (y) is the empirically derived CDF for Yi and Fi (Y ) is the actual CDF for Yi . For example, to approximate the CDF of Fi (y) with a 99% probability of achieving an accuracy of \u03f5 = 0.05, one would need n = 53 samples.\nTo generate accurate samples, one must know the joint density of X and pay the heavy computational cost of any rejection-sampling algorithm. Unfortunately, that cost grows exponentially with an increasing size of X and thus we resort to two approximations. The\nfirst generates samples from the input marginalsXi , when provided or previously estimated, and ignores covariances.\nIn the absence of full distributional information, the second approximation assumes that each input is normally distributed with the same mean and covariance matrix as the unknown distribution. Surprisingly, although the estimated distribution Y\u0302 is only a coarse approximation of the actual but unknown Y , their corresponding mean and variances are similar. To see why, recall that in Eq. 3 we showed that the mean and variance estimation of Y depends solely on the mean and variance of X. Thus, simulating (drawing samples) from any X that matches the required mean and variances, will accurately approximate the corresponding values for Y ."
                },
                {
                    "heading": "4 UP IN DAG DATA PROCESSING",
                    "text": "We now discuss how to apply the UP techniques introduced in the last section to data processing DAGs. Figure 3 shows a small example DAG, where uncertainty is introduced in the node labeled s (e.g., via a sampling-based approximation technique). Uncertainties then must be propagated through the two u nodes following s .\nFigure 4 shows an example detailed view of the two u nodes designed to highlight the challenges of implementing UP in DAG data processing. This example can correspond to transformations in a Spark program or Map and Reduce phases in a MapReduce program. This figure shows that, in general, we must handle UP through multi-input, multi-output functions for implementation in DAG data processing frameworks. Further, inputs may have nonzero covariances; e.g., Ym\u22122 and Ym\u22121 are generated from the same input, and thus are likely to have a non-zero covariance. Finally, the number of inputs and outputs may not be known statically at development time; e.g., a reduce() function in MapReduce has to accept an arbitrary number of values (> 0) for each key.\nIt is relatively straightforward to implement UP through blackbox functions using Monte Carlo simulation (henceforth called UPMC) despite the above complexities. This technique treats any node of the DAG as a black box, dynamically generates samples from the input set (each sample contains a single random value drawn from the distribution of each input data item), and dynamically computes the mean and variance for each output using the empirically derived distributions. Recall that we assume normal input distributions in the absence of this information and we ignore covariances between the inputs when constructing samples (Section 3.3), both of which may lead to inaccuracies.\nThe implementation of Differential Analysis (henceforth called UP-DA) is more challenging. Specifically, when a DAG node producesmultiple outputs, we view it as being implemented bymultiple sub-functions, each producing one of the output. For example, if a function H (X0,X1,X2) produces two outputs Y1 and Y2, then it is expressible as Y1 = h1(X0,X1,X2) and Y2 = h2(X0,X1,X2). In fact, each sub-function may depend only on a subset of the inputs; e.g., Y1 = h1(X0,X1) and Y2 = h2(X0,X2). In this case, the UP implementation must be able to identify the inputs used by each sub-function to correctly compute the (co) variances (Equation 3).\nThus, if a function such as f or \u0434 in Figure 4 produces multiple output values, each output must be produced by an invocation of a sub-function. The output values can be produced by multiple invocations of the same sub-function, or invocation of several different sub-functions. Each invocation must go through an UP interface so that we can track the input-to-output dependencies.\nInput covariances can require additional data flow to be added to the DAG for computing output variances and covariances. For example, consider the scenario where X1 and Xn have a non-zero covariance; even though Y2 and Ym are generated by different invocations of f , the covariance between X1 and Xn will affect the variance estimates for Y2 and Ym . The (previously independent) computation of Y2 and Ym now requires the read-only covariance matrix to be present in all nodes. In general, to propagate covariances properly, each node of the DAG must have the complete covariance matrix of the sibling inputs. This requirement is challenging to implement in practice since it introduces additional data propagation and dependencies among execution DAG nodes, both of which may degrade performance and limit scalability. Our current implementation of UP in MapReduce (Section 5) does not handle all possible covariances, leaving the exploration of the full issue for future work. Meanwhile, our results in Section 7 show that this limitation does not affect accuracy significantly in most applications that we study.\nAs shall be seen, having closed-form partial derivatives can significantly reduce the performance overheads of UP-DA compared to numerical differentiation. Thus, an UP-DA implementation should provide an interface for the programmer to provide closed-form partial derivative functions when available. Since the number of inputs may not be known at compile time, the interface must be sufficiently flexible to allow for a parameterized implementation of the partial derivatives. For example, a function that is symmetrical on all inputs (e.g., \u2211 X 2i ) has the same partial derivative for all inputs (e.g., 2Xi ). In this case, the partial derivative can be implemented using a single function parameterized by X and the index i .\nFinally, Figure 4 has some interesting performance implications. In the absence of covariances, UP is computed independently at each DAG node, allowing DAGs with UP to be sped up with added computation resources similar to without UP. However, speedup will ultimately be limited by the longest executing node as this \u201cstraggler\u201d will determine the minimum execution time of the DAG. For example,\u0434may be an aggregator function that takes inputs from many different invocations of f . If \u0434 has to aggregate a large number of inputs, then UP will require the evaluation of many partial derivatives (and possibly many numerical differentiations) for Differential Analysis or multiple evaluations of a function with many\ninputs for Monte Carlo. Thus, an invocation of \u0434 with a comparatively large number of inputs can become a performance bottleneck. Fortunately, we can limit the impact of these stragglers by giving more resources to them. In particular, numerical differentiation and derivative evaluations for different inputs are independent and so can be executed in parallel. Monte Carlo runs are also independent. Parallelizing execution in both cases is quite easy, especially for many-core servers."
                },
                {
                    "heading": "5 HADOOP UP-MAPREDUCE",
                    "text": "As a proof of concept, we extend Hadoop MapReduce to include the above UP techniques in multi-stage DAG applications. We first show how our approach can be applied to the MapReduce paradigm. We then describe our implementation called UP-MapReduce."
                },
                {
                    "heading": "5.1 UP-MapReduce Overview",
                    "text": "In MapReduce, each program runs in two phases, Map and Reduce. In the Map phase, a user-written side-effect-free map() function is invoked per each input (key, value) pair, and produces a set of intermediate (key, value) pairs, where multiple pairs may have the same key. In the Reduce phase, a user-written side-effect-free reduce() function is called per intermediate key and the set of values associated with that key (produced by all invocations of map() during the Map phase), and produces a set of keys, each with an associated set of values [8]. MapReduce programs can further be chained together to form complex DAGs.\nFigure 4 now maps readily to a MapReduce program (except that the keys are not shown), with the Map phase invoking the map function f and the Reduce phase invoking the reduce function \u0434. It is important to note that while the MapReduce model defines that map() only takes one (key, value) pair as input, the value may be a set; e.g., a line of words. Thus, we implement both map() and reduce() as multi-input, multi-output functions. Assuming that only values are uncertain (keys are exact), the discussion in Section 4 applies directly to the implementation of UP-MapReduce. Each map() or reduce() invocation expands to one or multiple UP calls through UP-MapReduce, which automatically estimates the uncertain outputs. UP-MapReduce then streams uncertain outputs from map()\u2192 reduce() while in the case of multiple chained programs, temporarily writes these values to HDFS where the next program in the DAG consumes them.\nTo support functions with multiple outputs, we introduce the notions of sub-maps and sub-reduces, with each map() (reduce()) containing one or more distinct sub-maps (sub-reduces). Each output must then be produced by the invocation of a sub-map (subreduce) on the correct subset of inputs. We adopt similar approach for implementing semi-continuous map() (reduce()) functions (Section 3.2); user-specified continuous intervals pair with exactly one sub-map (or sub-reducer respectively).\nDue to MapReduce limitations, where map() or reduce() invocations are independent, we currently do not handle the case where input covariances require additional data flow for computing output covariances; e.g., the previously mentioned case of X1 and Xn in Figure 4 having a non-zero covariance. We do support covariances for the inputs and outputs of a single invocation."
                },
                {
                    "heading": "5.2 Implementation",
                    "text": "We implement UP-MapReduce as an extension of Apache Hadoop 2.7. The extension comprises three Mapper and three Reducer classes that implement UP-MC, UP-DA for continuous functions, and UP-DA for semi-continuous functions, for Map and Reduce, respectively. Developers must choose the correct classes when implementing programs for UP-MapReduce. Our extension also introduces the uncertain type PV (probabilistic value) which implements random variables. A PV variable contains one or more random variables, each described by a mean, a variance-covariance matrix, and possibly an entire empirical distribution. Below, we briefly describe the necessary Reducer classes. UP is implemented similarly for the Mapper classes. UPMCReducer. This class implements UP-MC for reduce. It contains a PV object used to store the outputs, two abstract methods eval() and reduce(), and a reduceWithUP() method. The programmer needs to implement the reduce function (e.g., sum) in eval(), which accepts a variable number of double inputs and returns a variable number of double outputs. reduce() accepts a string key and a variable number of inputs in serialized form. reduceWithUP() implements UP-MC, and accepts a variable number of PV inputs. It computes the PV outputs using multiple invocations of eval() using samples derived from the PV inputs.\nA developer would then write her Reducer class by inheriting from this class, implementing eval() and reduce(). reduce() should first parse the input, then call reduceWith UP(), and finally emit the PV object. It is critical that reduce() does not perform any computation on the inputs that affect the output outside of eval(). The developer can specify that eval() should be invoked multiple times, with each invocation processing a particular subset of the inputs. This feature implements the multi-input, multi-output design via sub-maps and sub-reduces. UPDAContinuousReducer. This class implements UP-DA for continuous functions. The class adds an abstract method derivative() that accepts the inputs X in the form of an array of doubles, the\nindex i to compute \u2202f\u2202Xi , an array of constants that can be used as weights and outputs a double representing \u2202f\u2202Xi (X). The developer implements this method to provide a closed-form derivative for UP-DA. The class also implements a reduceWith UP()method that overrides its parent\u2019s method with an implementation of UP-DA. This method uses derivative() if it has been implemented, and numerical differentiation otherwise. It also uses input covariances, but expect the input covariance matrix to be loaded into the Hadoop read-only cache externally and prior to the execution of the Reduce phase. Then, it calls eval() as needed for evaluating the reduce function. The developer must implement eval() and reduce() as described above. UPDASemiContinuousReducer. This class implements UP-DA for semi-continuous functions and inherits from UPDAContinuousReducer. It allows the developer to specify a list of discontinuities in the reduce function and the range of the support of each input that must be within a continuous portion of the function. It implements a reduceWith UP() method that checks the support of each input against the discontinuities (with the desired accuracy), and chooses to use UP-MC or UP-DA as appropriate. No further implementation is required from the developer. Example UP-MapReduce program. Figure 5 shows the code for an UP-MapReduce program that computes a weighted average (secondDAGnode in Figure 1) in the presence of input uncertainties. Changes compared to a precise version are quite minimal. Parallelization ofMCandnumerical differentiation.Wehave extended the UP Reducer implementations to use multiple threads to speed up the execution of Monte Carlo simulation and numerical differentiation on servers with multi-core and/or hyperthreaded processors."
                },
                {
                    "heading": "6 APPLICATIONS",
                    "text": "We have built a toolbox of common operations (e.g., sum) and modified ten common data processing applications using UP-MapReduce to process uncertain data. We list the applications in Table 1, along with the kernels comprising each application and shorthand names which we use later in the evaluation section. Below, we briefly discuss each one. 1) Uncertain toolbox.We apply UP-DA to a variety of continuous operations such as summation, multiplication, logarithms, exponentiation and trigonometric functions with known simple closed-form derivatives. We also include comparison and min/max operations\n(via UP-DA and UP-MC, respectively). We combine all the above operations to create a toolbox of uncertain elementary operations which can be used as building blocks to construct richer applications. In UP-MapReduce, these uncertain blocks may represent either a logical UP-map or a logical UP-reducer but at runtime, they will expand according to the required dataflow to one or multiple nodes in the execution DAG. 2) Matrix multiplication (mm). The multiplication of two matricesA (n\u00d7m) andB (m\u00d7p) can be performed by computing the elements of the outputmatrixAB (n\u00d7p) asABi j = f (Arowi ,Bcolumnj ) = \u2211m k=1AikBk j (the inner product of Arowi and Bcolumnj ). A MapReduce implementation can use the Map phase to read A and B and emit pairs (ki j , Aik ) and (ki j , Bk j ) for 0 < i \u2264 n, 0 < j \u2264 p, and 0 < k \u2264 m. The reduce() function can then sort the Aik \u2019s and Bk j \u2019s into a sequence Ai ,1,Ai ,2, ...,Ai ,m,B1, j ,B2, j , ...,Bm, j , and then compute the inner product.\nApplying UP-DA is then done as follows. The only change needed for map() is the handling of PV rather than precise values. UP is not needed because no computation is being done. The reduce() is rewritten to call eval() after properly arranging the inputs, followed by a call to continuousUP(). eval() computes the inner product. The partial derivatives for inputs from A is \u2202f \u2202Aik = Bk j , and vice versa for inputs from B. 3) Regression (linreg). Fitting hyperplanes to observations is a frequent task in analytics. In particular, linear regression often relies on the least-squares method, where the sum of the squared differences between the hyperplane and the observed points is minimized. We base our application on linear regression, i.e, we are looking for Y = \u03b1X + \u03b2 . In the presence of noisy observations with known means and variances, we estimate the mean and variance of \u03b1 and \u03b2 . 4) Clustering (kmeans). Assigning observed data to clusters with k-means is frequent in data exploration. Given a fixed number of clusters and a sequence of observed data points, k-means performs an iterative algorithm, which (may) converge to a solution that minimizes the normed distance between all the points and their corresponding clusters. In the presence of uncertain data points, we extend the precise k-means algorithm with UP to estimate the mean and variance of the estimated cluster coordinates. The algorithm will then operate as a logical DAG with depth equal to the number of iterations required for k-means to converge. The logical DAG will then expand in runtime, to a large execution DAG where UPMapReduce will propagate the uncertainty at every node. As an example, ford data points, c centroids and n iterations the uncertain execution DAG will comprise of (d2 \u00d7 c + 1) \u00d7 n nodes. 5) k-nearest neighbors (kNN). A common classification method is performed by estimating the k nearest neighbors around a data point. This computation primarily involves calculating p-norms, which measure distance in multi-dimensional spaces. We extend the traditional notion of norm with UP to estimate the mean and variance, when the input coordinates are uncertain. 6) Solving systems of linear equations (linsolve). In order to solve large (n\u00d7n) systems of linear equations, in the form ofAx = b, one can use the Jacobi method to find the unknown x. Jacobi is an iterative procedure that progressively refines the solution x. We\nextend Jacobi to support uncertain A and/or b inputs. Then, we compute the mean and variance for each element of x. 7) Finding eigenpairs (eig).Computing eigenpairs (and especially the dominant eigenvalue and eigenvector) is the central task in solving differential equations and computing eigenfaces. The power iteration iteratively calculates the dominant eigenpair of an input matrix. We create our own version of the power iteration to handle uncertain input data. Specifically, we combine basic uncertain operations (division), mm and Euclidean norms as previously shown to build the necessary iteration. The output is then a random eigenvalue and a random eigenvector. 8) Compression (svd). An effective data compression method is the Singular Value Decomposition (SVD). The SVD of an input matrix A is the key kernel in solving problems such as data compression, but also principal component analysis, weather prediction, and signal processing. We can calculate the components of SVD (U , \u03a3, and V ) by finding the eigenvalues of AA\u2217 and A\u2217A. In case A is uncertain, we extend the precise SVD implementation with UP-DA and in particular by using the uncertain toolbox and eig. 9) Data filter (filter). Data filters are common data manipulation tasks in large-scale data processing systems, such as Apache Spark, and built-in procedures in programming languages such as Scala. We implement an uncertain compare-aggregator filter that handles uncertain inputs. During the compare phase, the (uncertain) input data are compared against a user-defined value. The statistics of the intermediate result are forwarded to an aggregator function which estimates the uncertainty of the final result. 10) Trends in social media (tsocial). A common task in social media analysis is to study potential trends between variables of the social graph. For example, one might want to discover correlations between peoples\u2019 age and number of followers in a social media site. Assuming the data is stored in a database, a two-phase workflow (a DAG whose logical nodes execute on different DAG processing systems) will first execute a GROUP BY query with stratified sampling to approximate the average number of friends per age group (with each group representing one day). This stage outputs the mean number of friends and a variance for each age group. The second phase performs uncertain linreg between the uncertain number of friends vs. age using linear regression. It then outputs the mean and variance for the slope and intercept of the fitted line. 11) Mean US internet latency estimation (latency). Suppose that a content delivery network (CDN) operator wants to improve the average perceived latency of its customers [35]. He then seeks to maximize the US-wide 10-mile average latency by altering the position of the CDN endpoints. To perform this task, the operator first estimates (via sampling) the mean (10-mile) latency of some candidate locations in the US. Obviously, the operator cannot estimate the desired latency mean on every possible location in US, but instead interpolates the nearby (unobserved) locations. To correctly perform the interpolation though, one should consider that each estimated mean is actually a distribution, as every estimate is being constructed from the appropriate samples.\nWe replicate such a scenario and illustrate how UP can be combined in a multi-stage uncertain workflow. The workflow comprises the following stages 1) collect traceroute measurements (within\nthe US) from the iPlanes dataset [20], 2) estimate the mean for each observed location using the samples, 3) use UP-MapReduce to perform bi-linear interpolations to estimate the mean latencies of unobserved locations and 4) use UP-MapReduce with an uncertain weighted average to simulate the frequency of packet transmission from each location based on known population density to ultimately obtain the mean and variance of the final estimate (population adjusted 10-mile mean latency)."
                },
                {
                    "heading": "7 EVALUATION",
                    "text": "In this section, we evaluate UP-MapReduce by studying it\u2019s accuracy, performance, and scalability. We begin by exploring the two applications, tsocial and latency, that include sampling-based approximations and trade precision for reduced execution times. We show that by developing these applications in UP-MapReduce we can drastically decrease the execution time of both, while propagating the uncertainties introduced by the approximations. We then explore the accuracy of our UP techniques, performance overheads, and scalability via an extensive sensitivity analysis."
                },
                {
                    "heading": "7.1 Evaluation Methodology",
                    "text": "Input data sets.We leverage real datasets for the two approximate applications under study. Specifically, we evaluate tsocial using the Facebook social structure from SNAP social circles [22] and latency using traceroute measurements from iPlanes [20]. For the purpose of the sensitivity analysis (performance, precision and scalability), we generate synthetic input data sets with varying sizes and amounts of uncertainty for each application, similarly to the synthetic data generation in [40]. For each data set, we first choose a random mean value \u00b5 for each input item according to a uniform distribution on a chosen range of values. We then set the variance \u03c3 2 for each input item to achieve a specific relative error defined as 3\u03c3/\u00b5. Baseline. We ran a large Monte Carlo experiment that executes a precise version of each application multiple times to accurately compute the empirical distributions for the outputs. Specifically, each experiment consists of n = 104 runs of a precise application, where each run is given inputs drawn randomly according to the actual (known) input distributions. Note that this is different than using UP-MC for each node of an application\u2019s DAG. Here, the entire application is run from beginning to end in each run as shown in Figure 6. For an iterative application, each run executes all iterations for a given input to generate an output sample. This way, all correlations between data items passing through the DAG are correctly preserved. The output samples from the n runs are then used to construct empirical output distributions from which we extract the mean and variance for each output. We consider three different distributions for input uncertainty: normal\n(Baseline-Normal), skewed with +0.5 skewness (Baseline-Skewed), and uniform (Baseline-Uniform). Comparing UP with Baseline. We compare the mean value and relative error for each output computed by UP-MapReduce against the values produced by the corresponding Baseline experiments. When an application produces one or a small number of outputs (e.g., linreg), we show the comparison for the output with the largest difference between the two approaches.When an application outputs a vector or matrix (e.g., svd), we show the comparison using the norm of the means \u2225\u00b5\u22252 and the relative error defined as \u22253\u03c3\u0302 \u22252/\u2225\u00b5\u22252. We expand on a case to show that using the norms do not obfuscate large differences for a subset of estimated outputs. We use the mean produced by Baseline-Normal to compute the relative error for UP in our comparisons (since the mean produced by UP is an estimate). All mean values computed by all methods were very close together, so this choice had little impact. Experimental platform. All (but scalability) experiments were run on a cluster of 2 servers. Each server is equipped with two Intel Xeon dual-core processors, 8 GB of DRAM, 1 Gbps network interface and two 480 GB HDDs. The servers in this cluster ran Ubuntu Linux Server LTS 14.04. Scalability experiments (Section 7.4) were run on a cluster of 512 servers, where each server is equipped with two Intel Xeon 16-core processors, 64GB of DRAM, a 10Gbps network interface and four 3TB HDDs. All servers in that cluster ran Windows Server 2012. Finally, all experiments were run with UP-MapReduce (Apache Hadoop 2.7)."
                },
                {
                    "heading": "7.2 Approximate Computing and UP",
                    "text": "We now leverage UP-MapReduce to build two multi-stage approximate workflows (tsocial and latency). Both first sample their initial dataset and produce uncertain intermediate values. Then, we leverage UP-MapReduce to process these uncertain values in subsequent stages, ultimately generating the final (uncertain) outputs. Our results show that UP is critical for propagating the introduced uncertainties, inform users of the magnitude of the final errors and provide guidelines to control them by adjusting the amount of initial approximation.\nSpecifically, tsocial is a two-stage approximate workflow comprising 1) the execution of an approximate query in BlinkDB [2] on 2 \u00d7 107 registered individuals, followed by 2) an uncertain linear regression (linreg) in UP-MapReduce. The execution of the approximate query in BlinkDB drastically reduces the execution time of the stage compared to a precise execution, but introduces uncertainties in the form of estimated errors (variance). UP-MapReduce is then used to propagate these uncertainties through the second stage of the computation. The second four-stage workflow (latency) approximates the mean US latency on a grid (2000 locations) by performing latency measurements only on 68 locations. This workflow comprises of 1) latency measurements which generate uncertainty due to sampling 2) generate an uncertain 2-dimensional latency surface on the obtained estimates from these 68 locations 3) perform uncertain bilinear interpolation on the (unobserved) remaining 1932 locations and 4) perform an uncertain weighted average to generate the population-weighted latency average.\nFigure 7 (top) shows the execution times of tsocial (right yaxis) for sampling rates ranging from 0.1% to 100% (precise). It also\nR eg\nre ss\nio n\ner ro\nr ( %\n)\n0\n10\n20\n30\n0\n5\n10\n15\nEx ec\nut io\nn tim\ne (s\n) Output Error - max(a,b) BlinkDB - Query UP-MapReduce - Regression\n-2 10-1 100 101 10210\n100 0\n50\n100\n0\n50\n100\nSampling rate (%)\nEx ec\nut io\nn tim\ne (s\n)\nLa st\ns ta\nge e\nrro r (\n% )\n101 102\nOutput Error UP-MapReduce Processing\nSampling rate (%)\ntsocial\nlatency\nFigure 7: Obtained relative errors and execution times for varying the sampling rate of two approximate workflows (tsocial-top and latency-bottom).\nshows the maximum relative error of the regression coefficients for slope and intercept (left y-axis). We only show UP-DA-numDiff for UP-MapReduce because execution times and errors for all three techniques are similar given the relatively small number of output items from BlinkDB (\u223c3\u00d7 104). We observe that significant savings in overall execution time can be achieved despite the overheads of UP. For example, a 5% sampling rate in the first logical DAG node leads to a relative error of just 1.35% and 51% savings in execution time (4s for BlinkDB and 2.9s for UP-MapReduce compared to 14.1s for BlinkDB without sampling plus a negligible amount of time for the precise linear regression computation). Overheads from UP require a sampling rate of 80% before approximation can lead to time savings for the workflow. After that, reduction in execution time increases as the sampling rate decreases since the UP overheads are relatively constant. Reduction in workflow execution time continues to increase until the smallest sampling rate of 0.1% for a maximum of 67.8% savings. However, the relative error increases rapidly to \u223c30% after a sampling rate of 1%.\nSimilarly, Figure 7 (bottom) shows the execution times of postprocessing the obtained traceroute data (excluding the time to perform the traceroutes themselves) and the duration of the subsequent stages (UP-MapReduce bi-linear interpolation and weighed average) for sampling latencies ranging from 1% to 100%. Note that a 100% sampling rate (\u223c2500 samples per observed location) indicates that we process all the available data; it does not correspond to sampling the entire network (which is not be possible to achieve). The estimated means are still uncertain and they include errors which should be propagated with UP.\nInitially, and for sampling rates of 10 \u2212 100%, we observe a generous reduction in execution time from \u223c82 \u2192 19s . For smaller sampling rates, the savings cap at \u223c12s . The execution times for UP-MapReduce again stay unaltered as the number of observed (60) and interpolant locations (\u223c5940) are constant (\u223c8.1s). The output error of the weighted average, increases quadratically as we decrease sampling rate. For example, sampling just 25% of the data, we can reduce the execution time by 62.5% with an output error\nof 9.01%. Similarly to tsocial, we only show UP-DA-numDiff, as it was the UP method with the longest running times.\nInterestingly in this case, there is no trade-off between postprocessing and UP-MapReduce execution times (in contrast to tsocial). As we always estimate the means from samples, UPMapReduce is necessary to propagate the uncertainties. It is then evident that without UP-MapReduce, we would be unaware of the high potential workflow error (which can be as high as 92.8%)."
                },
                {
                    "heading": "7.3 Accuracy and Performance",
                    "text": "We now perform a sensitivity analysis to evaluate the accuracy and performance of UP-MapReduce. We include results from all previously described applications except the toolbox,mm and tsocial, as they are included as part of the other applications under study. We start by exploring the accuracy of UP-MapReduce estimation of the means. Figure 8 plots the relative error (%) of the means (or the corresponding Euclidean norm in case of multivariate outputs) computed by UP-DA using numerical differentiation against the Baseline-Normal. These results are identical for UP-MC. We observe that UP-MapReduce estimates the means with very low bias, especially when the input relative errors are small (< 3%).\nWe next study the accuracy of the estimated relative errors. Figure 9(left) plots the relative errors computed by the three variants of UP-MapReduce as a function of the input relative error for 3 representative applications. The figure also plots the values produced by the three Baseline variants. Figure 9(right) plots the execution times of UP-MapReduce as a function of input size (the relative error of the input does not affect execution time). The figure also plots the execution times of precise versions, where there is zero input variance.\nWe observe that input uncertainties can be relatively stable, contract, or expand after propagation depending on the application. UP-MapReduce is highly accurate in most cases; i.e., its estimated relative errors are very close to the baseline values for 6 of the applications (linreg, kmeans, latency filter, kNN and linsolve). On the other hand, its estimated relative errors can also deviate noticeably from the baseline values (eig, and svd) when input errors are significant. In these cases, all three UP methods show similar deviations from the baseline although there are small differences between UP-MC and the other two approaches. Deviations for UPDA-numDiff and UP-DA-closedForm arise from the inaccuracies introduced by Differential Analysis. Deviations for UP-MC arise from the fact that UP-MapReduce performs the Monte Carlo computation independently for each computation node in the DAG, as opposed to executing the entire DAG multiple times as in the\nBaseline experiments. As previously mentioned, our current implementation does not account for all covariances and does not consider input covariances when drawing input samples in UP-MC, all of which also contribute to the observed deviations.\nTo verify that the computed norms are not obfuscating large differences between the UP-MapReduce estimates and baseline results, we also study the differences for each output in the multioutput applications. For example, Figure 10 plots CDFs of relative errors produced by the Baseline-Normal and UP-DA-numDiff when running linsolve for a 50 \u00d7 50 linear system. Observe that UP accurately estimates the entire relative error CDF of multivariate outputs for 1% input relative errors (Figure 10(a)), while for larger relative errors of 15% UP precisely estimates a significant portion of errors (79%), with significant deviations for only a very few outputs. We observe similar trends in the remaining multivariate applications (svd, kmeans, linreg, eig and latency).\nInterestingly, UP-DA-closedForm adds very little overhead to the precise version. This is because the derivatives for all functions being evaluated in our applications are simple functions. For example, the partial derivative with respect to xi of the inner product \u27e8x, y\u27e9 = \u2211ni=1 xiyi is simply yi , an O(1) computation. Thus, even though the number of evaluations of the derivative functions grows linearly with the number of inputs, each evaluation is extremely cheap and so the computations adds little overhead overall. Out of the eight applications, the maximum overhead (compared to the same application without UP) is 11.4% (kmeans) while the average across them is 6.0%.\nIt is important to note that the overhead for UP-DA-closedForm in general depends on the complexity of the derivatives; however, in all applications under consideration it was less expensive to evaluate derivatives of a function than the function itself. Thus, we expect the overheads of UP-DA-closedForm to be routinely lower than the ones for the other two UP techniques."
                },
                {
                    "heading": "7.4 Scalability",
                    "text": "We finally explore the scalability of UP-MapReduce by running applications 3-11 on a cluster of 512 servers.We also run the original precise applications. We choose the following input sizes: linreg (16\u00b7106), kmeans (107), kNN (16\u00b7106), linsolve (9\u00b7106), eig (9\u00b7106), svd (9 \u00b7 106), filter (16 \u00b7 106), latency (16 \u00b7 106 from 150 locations). We illustrate our results (speedups) vs. increasing number of servers from four representative applications in Figure 11. The rest follow similar trends. We draw the following conclusions.\nFirst, we observe that in all evaluated applications UP-DA-closed Form achieves similar (on average 1.6% difference) scalability as the precise version due to it\u2019s low additive overhead. Thus, uncertainty propagation does not deteriorate scalability (which as shown in Figure 11 may be poor) of the original application. Second, UP-DAclosedForm and UP-MC-monteCarlo show better scalability in all applications (except kmeans) due to the increased work per task (map and/or reducer) which amortizes the framework overheads. We expect this improvement to hold in applications where UP does not cause heavy execution imbalance (following observation).\nThird, when evaluating kmeans with UP-DA-closedForm or UP-MC-monteCarlo we observe lower scalability (in contrary with the previous point). A study of executions shows that this is a\nresult of straggler reducers which are caused from data imbalance between different intermediate keys (centroids) and amplified either by the numerical differentiation or the Monte Carlo simulation. The imbalance is not noticeable in the precise and UP-DA-closedForm versions but the other UP methods increase the running times causing reduced scalability. These effects are even more noticeable on UP-DA-numDiff without straggler parallelization which attains a maximum speedup of just 10 when we utilize all our available servers (not shown in Figure 11)."
                },
                {
                    "heading": "8 CONCLUSIONS",
                    "text": "In this paper, we proposed an approach for propagating data uncertainties through DAG computations. Specifically, we showed how Differential Analysis can be used to propagate uncertainties through DAG nodes implementing continuous (and semi-continuous under certain conditions) and differentiable functions. Our approach falls back to Monte Carlo simulation of nodes otherwise, but uses statistical bounds to minimize overheads while achieving a target error bounds. Our approach also allows the inter-mixing of Differential Analysis and Monte Carlo simulation for different nodes within a DAG, offering flexibility in the operations supported and minimizing performance overheads\nWe have shown how our UP approach can be applied to general DAG frameworks. We have also implemented it in the UPMapReduce system. Experimentation with ten common data analytic applications revealed that UP-MapReduce is highly accurate in many cases, while its performance overheads are very low \u2013 an average of 6% performance degradation \u2013 when closed-form derivatives are provided. When numerical differentiation or Monte Carlo simulation must be used, overheads can become much more significant as input size increases. Fortunately, the impact of these overheads on overall execution time can be reduced by allocating additional computing resources. Our scalability results show that UP-MapReduce scales well to a cluster with 512 servers. Finally, using two workflows that couple approximation with UP, we show that significant reductions in execution time can be achieved with approximation, despite the need for UP which propagates estimated uncertainties to the final output."
                },
                {
                    "heading": "9 ACKNOWLEDGMENTS",
                    "text": "This work was partially supported by NSF grant CCF-1319755."
                }
            ],
            "year": 2018,
            "references": [
                {
                    "title": "Knowing when you\u2019re wrong: building fast and reliable approximate query processing systems",
                    "authors": [
                        "Sameer Agarwal",
                        "Henry Milner",
                        "Ariel Kleiner",
                        "Ameet Talwalkar",
                        "Michael Jordan",
                        "Samuel Madden",
                        "Barzan Mozafari",
                        "Ion Stoica"
                    ],
                    "venue": "In Proceedings of the 2014 ACM SIGMOD international conference on Management of data. ACM,",
                    "year": 2014
                },
                {
                    "title": "BlinkDB: queries with bounded errors and bounded response times on very large data",
                    "authors": [
                        "Sameer Agarwal",
                        "Barzan Mozafari",
                        "Aurojit Panda",
                        "Henry Milner",
                        "Samuel Madden",
                        "Ion Stoica"
                    ],
                    "venue": "In Proceedings of the 8th ACM European Conference on Computer Systems",
                    "year": 2013
                },
                {
                    "title": "An Introduction To Error Propagation: Derivation, Meaning and Examples of Equation Cy=",
                    "authors": [
                        "Kai O Arras"
                    ],
                    "venue": "Fx Cx FxT. Technical Report EPFL-ASL-TR-98-01",
                    "year": 1998
                },
                {
                    "title": "Uncertain< T>: A first-order type for uncertain data",
                    "authors": [
                        "James Bornholt",
                        "Todd Mytkowicz",
                        "Kathryn S McKinley"
                    ],
                    "venue": "ACM SIGPLAN Notices 49,",
                    "year": 2014
                },
                {
                    "title": "Probability type inference for flexible approximate programming",
                    "authors": [
                        "Brett Boston",
                        "Adrian Sampson",
                        "Dan Grossman",
                        "Luis Ceze"
                    ],
                    "venue": "In ACM SIGPLAN Notices,",
                    "year": 2015
                },
                {
                    "title": "Proving acceptability properties of relaxed nondeterministic approximate programs",
                    "authors": [
                        "Michael Carbin",
                        "Deokhwan Kim",
                        "Sasa Misailovic",
                        "Martin C Rinard"
                    ],
                    "venue": "ACM SIGPLAN Notices 47,",
                    "year": 2012
                },
                {
                    "title": "SCOPE: easy and efficient parallel processing of massive data sets",
                    "authors": [
                        "Ronnie Chaiken",
                        "Bob Jenkins",
                        "Per-\u00c5ke Larson",
                        "Bill Ramsey",
                        "Darren Shakib",
                        "Simon Weaver",
                        "Jingren Zhou"
                    ],
                    "venue": "Proceedings of the VLDB Endowment 1,",
                    "year": 2008
                },
                {
                    "title": "MapReduce: simplified data processing on large clusters",
                    "authors": [
                        "Jeffrey Dean",
                        "Sanjay Ghemawat"
                    ],
                    "venue": "Commun. ACM 51,",
                    "year": 2008
                },
                {
                    "title": "Architecture support for disciplined approximate programming",
                    "authors": [
                        "Hadi Esmaeilzadeh",
                        "Adrian Sampson",
                        "Luis Ceze",
                        "Doug Burger"
                    ],
                    "venue": "In ACM SIGPLAN Notices,",
                    "year": 2012
                },
                {
                    "title": "Multivariate statistical modelling based on generalized linear models",
                    "authors": [
                        "Ludwig Fahrmeir",
                        "Gerhard Tutz"
                    ],
                    "year": 2013
                },
                {
                    "title": "ApproxHadoop: Bringing Approximations to MapReduce Frameworks",
                    "authors": [
                        "\u00cd\u00f1igo Goiri",
                        "Ricardo Bianchini",
                        "Santosh Nagarakatte",
                        "Thu D Nguyen"
                    ],
                    "venue": "In Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems",
                    "year": 2015
                },
                {
                    "title": "Latin hypercube sampling and the propagation of uncertainty in analyses of complex systems",
                    "authors": [
                        "Jon C Helton",
                        "Freddie Joe Davis"
                    ],
                    "venue": "Reliability Engineering & System Safety 81,",
                    "year": 2003
                },
                {
                    "title": "JouleGuard: energy guarantees for approximate applications",
                    "authors": [
                        "Henry Hoffmann"
                    ],
                    "venue": "In Proceedings of the 25th Symposium on Operating Systems Principles",
                    "year": 2015
                },
                {
                    "title": "Data clustering: 50 years beyond K-means",
                    "authors": [
                        "Anil K Jain"
                    ],
                    "venue": "Pattern recognition letters 31,",
                    "year": 2010
                },
                {
                    "title": "Sampling-based estimators for subset-based queries",
                    "authors": [
                        "Shantanu Joshi",
                        "Christopher Jermaine"
                    ],
                    "venue": "The VLDB Journal\u2014The International Journal on Very Large Data Bases 18,",
                    "year": 2009
                },
                {
                    "title": "Tchebycheff systems: With applications in analysis and statistics",
                    "authors": [],
                    "year": 1966
                },
                {
                    "title": "Eigenvoice modeling with sparse training data",
                    "authors": [
                        "Patrick Kenny",
                        "Gilles Boulianne",
                        "Pierre Dumouchel"
                    ],
                    "venue": "Speech and Audio Processing, IEEE Transactions on 13,",
                    "year": 2005
                },
                {
                    "title": "A comparative study of uncertainty propagation methods for black-box-type problems",
                    "authors": [
                        "Sang Hoon Lee",
                        "Wei Chen"
                    ],
                    "venue": "Structural and Multidisciplinary Optimization 37,",
                    "year": 2009
                },
                {
                    "title": "iPlane Nano: Path Prediction for Peer-to-Peer Applications",
                    "authors": [
                        "Harsha V Madhyastha",
                        "Ethan Katz-Bassett",
                        "Thomas E Anderson",
                        "Arvind Krishnamurthy",
                        "Arun Venkataramani"
                    ],
                    "venue": "In NSDI,",
                    "year": 2009
                },
                {
                    "title": "The tight constant in the Dvoretzky-Kiefer-Wolfowitz inequality",
                    "authors": [
                        "Pascal Massart"
                    ],
                    "venue": "The Annals of Probability",
                    "year": 1990
                },
                {
                    "title": "Discovering Social Circles in Ego Networks",
                    "authors": [
                        "Julian Mcauley",
                        "Jure Leskovec"
                    ],
                    "venue": "ACM Trans. Knowl. Discov. Data",
                    "year": 2014
                },
                {
                    "title": "Load value approximation",
                    "authors": [
                        "Joshua San Miguel",
                        "Mario Badr",
                        "Natalie Enright Jerger"
                    ],
                    "venue": "In Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture. IEEE Computer Society,",
                    "year": 2014
                },
                {
                    "title": "Chisel: Reliability-and accuracy-aware optimization of approximate computational kernels",
                    "authors": [
                        "Sasa Misailovic",
                        "Michael Carbin",
                        "Sara Achour",
                        "Zichao Qi",
                        "Martin C Rinard"
                    ],
                    "venue": "In ACM SIGPLAN Notices,",
                    "year": 2014
                },
                {
                    "title": "Probabilistically accurate program transformations",
                    "authors": [
                        "Sasa Misailovic",
                        "Daniel M Roy",
                        "Martin C Rinard"
                    ],
                    "venue": "In International Static Analysis Symposium",
                    "year": 2011
                },
                {
                    "title": "Machine learning: a probabilistic perspective",
                    "authors": [
                        "Kevin P Murphy"
                    ],
                    "year": 2012
                },
                {
                    "title": "Iterative methods for sparse linear systems. Siam",
                    "authors": [
                        "Yousef Saad"
                    ],
                    "year": 2003
                },
                {
                    "title": "Paraprox: Pattern-based approximation for data parallel applications",
                    "authors": [
                        "Mehrzad Samadi",
                        "Davoud Anoushe Jamshidi",
                        "Janghaeng Lee",
                        "Scott Mahlke"
                    ],
                    "venue": "In ACM SIGARCH Computer Architecture News,",
                    "year": 2014
                },
                {
                    "title": "Approximate Storage in Solid-State Memories",
                    "authors": [
                        "Adrian Sampson",
                        "Jacob Nelson",
                        "Karin Strauss",
                        "Luis Ceze"
                    ],
                    "venue": "ACM Trans. Comput. Syst",
                    "year": 2014
                },
                {
                    "title": "Expressing and verifying probabilistic assertions",
                    "authors": [
                        "Adrian Sampson",
                        "Pavel Panchekha",
                        "Todd Mytkowicz",
                        "Kathryn S McKinley",
                        "Dan Grossman",
                        "Luis Ceze"
                    ],
                    "venue": "ACM SIGPLAN Notices 49,",
                    "year": 2014
                },
                {
                    "title": "Linear regression analysis",
                    "authors": [
                        "George AF Seber",
                        "Alan J Lee"
                    ],
                    "year": 2012
                },
                {
                    "title": "Face recognition using eigenfaces",
                    "authors": [
                        "M.A. Turk",
                        "A.P. Pentland"
                    ],
                    "venue": "In Proceedings",
                    "year": 1991
                },
                {
                    "title": "Cache in the air: exploiting content caching and delivery techniques for 5G systems",
                    "authors": [
                        "Xiaofei Wang",
                        "Min Chen",
                        "Tarik Taleb",
                        "Adlen Ksentini",
                        "Victor Leung"
                    ],
                    "venue": "IEEE Communications Magazine 52,",
                    "year": 2014
                },
                {
                    "title": "ECG data compression using truncated singular value decomposition",
                    "authors": [
                        "Jyh-Jong Wei",
                        "Chuang-Jan Chang",
                        "Nai-Kuan Chou",
                        "Gwo-Jen Jan"
                    ],
                    "venue": "Information Technology in Biomedicine, IEEE Transactions on 5,",
                    "year": 2001
                },
                {
                    "title": "Hadoop: The definitive guide",
                    "authors": [
                        "Tom White"
                    ],
                    "year": 2012
                },
                {
                    "title": "Continuous sampling for online aggregation over multiple queries",
                    "authors": [
                        "Sai Wu",
                        "Beng Chin Ooi",
                        "Kian-Lee Tan"
                    ],
                    "venue": "In Proceedings of the 2010 ACM SIGMOD International Conference on Management of data",
                    "year": 2010
                },
                {
                    "title": "Spark: cluster computing with working sets",
                    "authors": [
                        "Matei Zaharia",
                        "Mosharaf Chowdhury",
                        "Michael J Franklin",
                        "Scott Shenker",
                        "Ion Stoica"
                    ],
                    "venue": "HotCloud",
                    "year": 2010
                },
                {
                    "title": "Representative clustering of uncertain data",
                    "authors": [
                        "Andreas Z\u00fcfle",
                        "Tobias Emrich",
                        "Klaus Arthur Schmid",
                        "Nikos Mamoulis",
                        "Arthur Zimek",
                        "Matthias Renz"
                    ],
                    "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM,",
                    "year": 2014
                }
            ],
            "id": "SP:248a12009b7b61f5a310f066ba85b8b5f9417b41",
            "authors": [
                {
                    "name": "Ioannis Manousakis",
                    "affiliations": []
                },
                {
                    "name": "\u00cd\u00f1igo Goiri",
                    "affiliations": []
                },
                {
                    "name": "Ricardo Bianchini",
                    "affiliations": []
                },
                {
                    "name": "Sandro Rigo",
                    "affiliations": []
                },
                {
                    "name": "Thu D. Nguyen",
                    "affiliations": []
                }
            ],
            "abstractText": "We are seeing an explosion of uncertain data\u2014i.e., data that is more properly represented by probability distributions or estimated values with error bounds rather than exact values\u2014from sensors in IoT, sampling-based approximate computations and machine learning algorithms. In many cases, performing computations on uncertain data as if it were exact leads to incorrect results. Unfortunately, developing applications for processing uncertain data is a major challenge from both the mathematical and performance perspectives. This paper proposes and evaluates an approach for tackling this challenge in DAG-based data processing systems. We present a framework for uncertainty propagation (UP) that allows developers to modify precise implementations of DAG nodes to process uncertain inputs with modest effort. We implement this framework in a system called UP-MapReduce, and use it to modify ten applications, including AI/ML, image processing and trend analysis applications to process uncertain data. Our evaluation shows that UP-MapReduce propagates uncertainties with high accuracy and, in many cases, low performance overheads. For example, a social network trend analysis application that combines data sampling with UP can reduce execution time by 2.3x when the user can tolerate a maximum relative error of 5% in the final answer. These results demonstrate that our UP framework presents a compelling approach for handling uncertain data in DAG processing.",
            "title": "Uncertainty Propagation in Data Processing Systems"
        },
        "Y": {
            "blog_id": "uncertainty-propagation-in-data-processing-systems",
            "summary": [
                "Uncertainty propagation in data processing systems Manousakis et al., SoCC\u201918  When I\u2019m writing an edition of The Morning Paper, I often imagine a conversation with a hypothetical reader sat in a coffee shop somewhere at the start of their day.",
                "There are three levels of takeaway from today\u2019s paper choice:  If you\u2019re downing a quick espresso, then it\u2019s good to know that uncertainty can creep into our data in lots of different ways, and if you compute with those uncertain values as if they were precise, errors can compound quickly leading to incorrect results or false confidence.",
                "If you\u2019re savouring a cortado, then you might also want to dip into the techniques we can use to propagate uncertainty through a computation.",
                "If you\u2019re lingering over a latte, then the UP (Uncertainty Propagation) framework additionally shows how to integrate these techniques into a dataflow framework.",
                "We implement this framework in a system called UP-MapReduce, and use it to modify ten applications, including AI/ML, image processing, and trend analysis applications to process uncertain data.",
                "Our evaluation shows that UP-MapReduce propagates uncertainties with high accuracy and, in many cases, low performance overheads.",
                "Are you sure?",
                "Uncertainty can arise from a number of different sources including probabilistic modelling, machine learning, approximate computing, imprecise sensor data, and such like.",
                "For many applications, uncertain data should be represented as probability distributions or estimated values with error bounds rather than exact values.",
                "Failure to properly account for this uncertainty may lead to incorrect results.",
                "For example, Bornholt et al. have shown that computing speeds from recorded GPS positions can lead to absurd values (e.g., walking speeds above 30mph) when ignoring uncertainties in the recordings.",
                "If you have a dataflow system with computation based on a DAG, then uncertainty in upstream data values needs to flow through the computation.",
                "For example, consider a simple 2-node DAG where an approximate query is used to produce an approximate count of the number of customers in different age groups (e.g., using BlinkDB ), and then we take a weighted average of those groups.",
                "The second node will by default produce a single value, but in reality it should result in a distribution.",
                "There may be meaningful parts of that distribution where the outcome would be disadvantageous (for example), but the probability of this is completely lost when reporting a single value.",
                "Uncertainty propagation  Our method offers, to the best of our knowledge, the only known computationally tractable (and as our evaluation will show, potentially with low overheads) large-scale uncertainty propagation.",
                "Consider a function  , where  is an arbitrary function without side-effects representing the computation at a node in a dataflow,  is a set of random variables representing inputs with uncertainties, and  is a set of random variables representing outputs with uncertainties.",
                "Depending on the nature of  , we can use different statistical methods to approximate the mean and variance of each variable in the output.",
                "When  is a continuous differentiable function we can use first-order Differential Analysis:  The general strategy is to compute  by approximating  using its first-order Taylor series at the expected value of  .",
                "This approximation is accurate if  is roughly linear around the support (in other words, neighborhood) of  \u2026  When there are multiple inputs and multiple outputs, the calculation also needs to take into account the covariances between the outputs.",
                "When  is a semi-continuous function we have two possibilities.",
                "If the support of each input mostly or entirely falls within a continuous differentiable part of the function then we can use Differential Analysis (DA) as before.",
                "If it spans a discontinuity then we have to use Monte Carlo simulation.",
                "For example, consider the function  when  , and  otherwise.",
                "If each input is greater than  then we can use DA.",
                "We use Monte Carlo simulation to approximate  for functions  that do not meet (or the developers do not know whether they meet) the requirements for DA.",
                "is evaluated on  randomly drawn samples of the input, and the outputs are used as an approximation of  .",
                "To generate accurate samples, one must know the joint density of  and pay the heavy computational cost of any rejection-sampling algorithm.",
                "Unfortunately that cost grows exponentially with an increasing size of  and thus we resort to two approximations:  Given input distributions, generate samples accordingly and ignore covariances  In the absence of full distributional information, assume that each input is normally distributed with the same mean and covariance matrix as the unknown distribution.",
                "(This approximation works because the mean and variance estimation of Y depends solely on the mean and variance of  ).",
                "Uncertainty propagation in dataflows  As stated earlier, in a dataflow graph we need to perform uncertainty propagation at all nodes downstream of uncertain data.",
                "For Monte Carlo simulation-based uncertainty propagation (UP-MC) we can just treat a node as a black box, dynamically generate samples from the input set, and compute the mean and variance for each output using empirically derived distributions (or assume normal distributions in the absence of this information).",
                "The implementation of Differential Analysis (henceforth called UP-DA) is more challenging.",
                "Specifically, when a DAG node produces multiple outputs, we view it as being implemented by multiple sub-functions, each producing one of the outputs\u2026 input covariances can require additional data flow to be added to the DAG for computing output variances and covariances.",
                "If the programmer can provide a partial derivative function, then using this often gives better performance than resorting to numerical differentiation.",
                "Observe that we might make a saving early in the dataflow by introducing uncertainty (e.g. by computing an approximate result), but then we have to pay more later for the resulting uncertainty propagation.",
                "The evaluation explores this trade-off.",
                "UP-MapReduce is an implementation of the above ideas in the in MapReduce.",
                "The UP-MapReduce extension includes three Mapper and three Reducer classes that implement UP-MC, UP-DA for continuous functions, and UP-DA for semi-continuous functions.",
                "The extension also introduce the uncertain type PV (Probabilistic Value) which contains one or more random variables, each described by a mean, a variance-covariance matrix, and possibly an entire empirical distribution.",
                "The UP-DA Continuous Reducer class for example provides an abstract derivative method that a developer can implement to provide a closed-form derivative function.",
                "Uncertainty propagation in practice  We have built a toolbox of common operations (e.g., sum) and modified ten common data processing applications using UP-MapReduce to process uncertain data.",
                "Baselines for the evaluation are established by running a large Monte Carlo experiment over a precise version of each application.",
                "When input errors are small (e.g. below 3%) then UP-MapReduce estimates means with very low error.",
                "The following figure shows the relative errors and execution times for the three variants of UP-MC as compared to the baseline.",
                "Enlarge  For six of the applications UP-MapReduce is highly accurate, but when input errors are significant (e.g. eig, svd) its estimated relative errors can deviate noticeably from baseline values.",
                "The best performance is obtained when using closed-form (user provided) derivatives.",
                "tsocial and latency are both multi-stage approximate workflows.",
                "The following chart shows the execution times and maximum relative errors for sampling rates from 0.1% to 100% (precise).",
                "For tsocial, a sampling rate of 80% or less is required before the overheads of uncertainty propagation are outweighed by the sampling benefits.",
                "Experimentation with ten common data analytic applications revealed that UP-MapReduce is highly accurate in many cases, while its performance overheads are very low\u2014 an average of 6% performance degradation\u2014 when closed-form derivatives are provided.",
                "When numerical differentiation or Monte Carlo simulation must be used, overheads can become much more significant as input size increases.",
                "Fortunately, the impact of these overheads on overall execution time can be reduced by allocating additional computational resources."
            ],
            "author_id": "ACOLYER",
            "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3267809.3267833?download=true",
            "author_full_name": "Adrian Colyer",
            "source_website": "https://blog.acolyer.org/about/",
            "id": 8984509
        }
    },
    "4830217": {
        "X": {
            "sections": [
                {
                    "text": "Categories and Subject Descriptors H.2.8 [Database Management]: Database Applications\u2014 data mining ; I.2.6 [Artificial Intelligence]: Learning\u2014 concept learning ; I.5.2 [Pattern Recognition]: Design Methodology\u2014classifier design and evaluation\nGeneral Terms Decision trees, Hoeffding bounds, incremental learning, diskbased algorithms, subsampling"
                },
                {
                    "heading": "1. INTRODUCTION",
                    "text": "Knowledge discovery systems are constrained by three main limited resources: time, memory and sample size. In traditional applications of machine learning and statistics, sample size tends to be the dominant limitation: the computational resources for a massive search are available, but carrying out such a search over the small samples available (typically less than 10,000 examples) often leads to overfitting or \u201cdata dredging\u201d (e.g., [22, 16]). Thus overfitting avoidance becomes the main concern, and only a fraction of the available computational power is used [3]. In contrast, in many (if not\nmost) present-day data mining applications, the bottleneck is time and memory, not examples. The latter are typically in over-supply, in the sense that it is impossible with current KDD systems to make use of all of them within the available computational resources. As a result, most of the available examples go unused, and underfitting may result: enough data to model very complex phenomena is available, but inappropriately simple models are produced because we are unable to take full advantage of the data. Thus the development of highly efficient algorithms becomes a priority.\nCurrently, the most efficient algorithms available (e.g., [17]) concentrate on making it possible to mine databases that do not fit in main memory by only requiring sequential scans of the disk. But even these algorithms have only been tested on up to a few million examples. In many applications this is less than a day\u2019s worth of data. For example, every day retail chains record millions of transactions, telecommunications companies connect millions of calls, large banks process millions of ATM and credit card operations, and popular Web sites log millions of hits. As the expansion of the Internet continues and ubiquitous computing becomes a reality, we can expect that such data volumes will become the rule rather than the exception. Current data mining systems are not equipped to cope with them. When new examples arrive at a higher rate than they can be mined, the quantity of unused data grows without bounds as time progresses. Even simply preserving the examples for future use can be a problem when they need to be sent to tertiary storage, are easily lost or corrupted, or become unusable when the relevant contextual information is no longer available. When the source of examples is an open-ended data stream, the notion of mining a database of fixed size itself becomes questionable.\nIdeally, we would like to have KDD systems that operate continuously and indefinitely, incorporating examples as they arrive, and never losing potentially valuable information. Such desiderata are fulfilled by incremental learning methods (also known as online, successive or sequential methods), on which a substantial literature exists. However, the available algorithms of this type (e.g., [20]) have significant shortcomings from the KDD point of view. Some are reasonably efficient, but do not guarantee that the model learned will be similar to the one obtained by learning on the same data in batch mode. They are highly sensitive to example ordering, potentially never recovering from an unfavorable set of early examples. Others produce the same model as\nthe batch version, but at a high cost in efficiency, often to the point of being slower than the batch algorithm.\nThis paper proposes Hoeffding trees, a decision-tree learning method that overcomes this trade-off. Hoeffding trees can be learned in constant time per example (more precisely, in time that is worst-case proportional to the number of attributes), while being nearly identical to the trees a conventional batch learner would produce, given enough examples. The probability that the Hoeffding and conventional tree learners will choose different tests at any given node decreases exponentially with the number of examples. We also describe and evaluate VFDT, a decision-tree learning system based on Hoeffding trees. VFDT is I/O bound in the sense that it mines examples in less time than it takes to input them from disk. It does not store any examples (or parts thereof) in main memory, requiring only space proportional to the size of the tree and associated sufficient statistics. It can learn by seeing each example only once, and therefore does not require examples from an online stream to ever be stored. It is an anytime algorithm in the sense that a ready-to-use model is available at any time after the first few examples are seen, and its quality increases smoothly with time.\nThe next section introduces Hoeffding trees and studies their properties. We then describe the VFDT system and its empirical evaluation. The paper concludes with a discussion of related and future work."
                },
                {
                    "heading": "2. HOEFFDING TREES",
                    "text": "The classification problem is generally defined as follows. A set of N training examples of the form (x, y) is given, where y is a discrete class label and x is a vector of d attributes, each of which may be symbolic or numeric. The goal is to produce from these examples a model y = f(x) that will predict the classes y of future examples x with high accuracy. For example, x could be a description of a client\u2019s recent purchases, and y the decision to send that customer a catalog or not; or x could be a record of a cellular-telephone call, and y the decision whether it is fraudulent or not. One of the most effective and widely-used classification methods is decision tree learning [1, 15]. Learners of this type induce models in the form of decision trees, where each node contains a test on an attribute, each branch from a node corresponds to a possible outcome of the test, and each leaf contains a class prediction. The label y = DT (x) for an example x is obtained by passing the example down from the root to a leaf, testing the appropriate attribute at each node and following the branch corresponding to the attribute\u2019s value in the example. A decision tree is learned by recursively replacing leaves by test nodes, starting at the root. The attribute to test at a node is chosen by comparing all the available attributes and choosing the best one according to some heuristic measure. Classic decision tree learners like ID3, C4.5 and CART assume that all training examples can be stored simultaneously in main memory, and are thus severely limited in the number of examples they can learn from. Disk-based decision tree learners like SLIQ [10] and SPRINT [17] assume the examples are stored on disk, and learn by repeatedly reading them in sequentially (effectively once per level in the tree). While this greatly increases the size of usable training sets, it can become prohibitively ex-\npensive when learning complex trees (i.e., trees with many levels), and fails when datasets are too large to fit in the available disk space.\nOur goal is to design a decision tree learner for extremely large (potentially infinite) datasets. This learner should require each example to be read at most once, and only a small constant time to process it. This will make it possible to directly mine online data sources (i.e., without ever storing the examples), and to build potentially very complex trees with acceptable computational cost. We achieve this by noting with Catlett [2] and others that, in order to find the best attribute to test at a given node, it may be sufficient to consider only a small subset of the training examples that pass through that node. Thus, given a stream of examples, the first ones will be used to choose the root test; once the root attribute is chosen, the succeeding examples will be passed down to the corresponding leaves and used to choose the appropriate attributes there, and so on recursively.1 We solve the difficult problem of deciding exactly how many examples are necessary at each node by using a statistical result known as the Hoeffding bound (or additive Chernoff bound) [7, 9]. Consider a real-valued random variable r whose range is R (e.g., for a probability the range is one, and for an information gain the range is log c, where c is the number of classes). Suppose we have made n independent observations of this variable, and computed their mean r. The Hoeffding bound states that, with probability 1 \u2212 \u03b4, the true mean of the variable is at least r \u2212 , where\n=\nR2 ln(1/\u03b4)\n2n (1)\nThe Hoeffding bound has the very attractive property that it is independent of the probability distribution generating the observations. The price of this generality is that the bound is more conservative than distribution-dependent ones (i.e., it will take more observations to reach the same \u03b4 and ). Let G(Xi) be the heuristic measure used to choose test attributes (e.g., the measure could be information gain as in C4.5, or the Gini index as in CART). Our goal is to ensure that, with high probability, the attribute chosen using n examples (where n is as small as possible) is the same that would be chosen using infinite examples. Assume G is to be maximized, and let Xa be the attribute with highest observed G after seeing n examples, and Xb be the second-best attribute. Let \u2206G = G(Xa) \u2212 G(Xb) \u2265 0 be the difference between their observed heuristic values. Then, given a desired \u03b4, the Hoeffding bound guarantees that Xa is the correct choice with probability 1 \u2212 \u03b4 if n examples have been seen at this node and \u2206G > .2 In other words, if the ob-\n1We assume the examples are generated by a stationary stochastic process (i.e., their distribution does not change over time). If the examples are being read from disk, we assume that they are in random order. If this is not the case, they should be randomized, for example by creating a random index and sorting on it. 2In this paper we assume that the third-best and lower attributes have sufficiently smaller gains that their probability of being the true best choice is negligible. We plan to lift this assumption in future work. If the attributes at a given node are (pessimistically) assumed independent, it simply involves a Bonferroni correction to \u03b4 [11].\nserved \u2206G > then the Hoeffding bound guarantees that the true \u2206G \u2265 \u2206G \u2212 > 0 with probability 1 \u2212 \u03b4, and therefore that Xa is indeed the best attribute with probability 1 \u2212 \u03b4. This is valid as long as the G value for a node can be viewed as an average of G values for the examples at that node, as is the case for the measures typically used. Thus a node needs to accumulate examples from the stream until becomes smaller than \u2206G. (Notice that is a monotonically decreasing function of n.) At this point the node can be split using the current best attribute, and succeeding examples will be passed to the new leaves. This leads to the Hoeffding tree algorithm, shown in pseudo-code in Table 1.\nThe counts nijk are the sufficient statistics needed to compute most heuristic measures; if other quantities are required, they can be similarly maintained. Pre-pruning is carried out by considering at each node a \u201cnull\u201d attribute X\u2205 that consists of not splitting the node. Thus a split will only be made if, with confidence 1\u2212\u03b4, the best split found is better according to G than not splitting. The pseudo-code shown is only for discrete attributes, but its extension to numeric ones is immediate, following the usual method of allowing tests of the form \u201c(Xi < xij)?,\u201d and computing G for each allowed threshold xij . The sequence of examples S may be infinite, in which case the procedure never terminates, and at any point in time a parallel procedure can use the current tree HT to make class predictions. If d is the number of attributes, v is the maximum number of values per attribute, and c is the number of classes, the Hoeffding tree algorithm requires O(dvc) memory to store the necessary counts at each leaf. If l is the number of leaves in the tree, the total memory required is O(ldvc). This is independent of the number of examples seen, if the size of the tree depends only on the \u201ctrue\u201d concept and is independent of the size of the training set. (Although this is a common assumption in the analysis of decision-tree and related algorithms, it often fails in practice. Section 3 describes a refinement to the algorithm to cope with this.)\nA key property of the Hoeffding tree algorithm is that it is possible to guarantee under realistic assumptions that the trees it produces are asymptotically arbitrarily close to the ones produced by a batch learner (i.e., a learner that uses all the examples to choose a test at each node). In other words, the incremental nature of the Hoeffding tree algorithm does not significantly affect the quality of the trees it produces. In order to make this statement precise, we need to define the notion of disagreement between two decision trees. Let P (x) be the probability that the attribute vector (loosely, example) x will be observed, and let I(.) be the indicator function, which returns 1 if its argument is true and 0 otherwise.\nDefinition 1. The extensional disagreement \u2206e between two decision trees DT1 and DT2 is the probability that they will produce different class predictions for an example:\n\u2206e(DT1, DT2) = x P (x)I[DT1(x) 6= DT2(x)]\nConsider that two internal nodes are different if they contain different tests, two leaves are different if they contain different class predictions, and an internal node is different\nTable 1: The Hoeffding tree algorithm.\nInputs: S is a sequence of examples, X is a set of discrete attributes, G(.) is a split evaluation function, \u03b4 is one minus the desired probability of\nchoosing the correct attribute at any given node.\nOutput: HT is a decision tree.\nProcedure HoeffdingTree (S, X, G, \u03b4) Let HT be a tree with a single leaf l1 (the root). Let X1 = X \u222a {X\u2205}. Let G1(X\u2205) be the G obtained by predicting the most\nfrequent class in S. For each class yk\nFor each value xij of each attribute Xi \u2208 X Let nijk(l1) = 0.\nFor each example (x, yk) in S Sort (x, y) into a leaf l using HT . For each xij in x such that Xi \u2208 Xl\nIncrement nijk(l). Label l with the majority class among the examples\nseen so far at l. If the examples seen so far at l are not all of the same\nclass, then Compute Gl(Xi) for each attribute Xi \u2208 Xl \u2212 {X\u2205}\nusing the counts nijk(l). Let Xa be the attribute with highest Gl. Let Xb be the attribute with second-highest Gl. Compute using Equation 1. If Gl(Xa) \u2212 Gl(Xb) > and Xa 6= X\u2205, then\nReplace l by an internal node that splits on Xa. For each branch of the split\nAdd a new leaf lm, and let Xm = X \u2212 {Xa}. Let Gm(X\u2205) be the G obtained by predicting\nthe most frequent class at lm. For each class yk and each value xij of each\nattribute Xi \u2208 Xm \u2212 {X\u2205} Let nijk(lm) = 0.\nReturn HT .\nfrom a leaf. Consider also that two paths through trees are different if they differ in length or in at least one node.\nDefinition 2. The intensional disagreement \u2206i between two decision trees DT1 and DT2 is the probability that the path of an example through DT1 will differ from its path through DT2:\n\u2206i(DT1, DT2) = x P (x)I[Path1(x) 6= Path2(x)]\nwhere Pathi(x) is the path of example x through tree DTi.\nTwo decision trees agree intensionally on an example iff they are indistinguishable for that example: the example is passed down exactly the same sequence of nodes, and receives an identical class prediction. Intensional disagreement is a stronger notion than extensional disagreement, in the sense that \u2200DT1,DT2 \u2206i(DT1, DT2) \u2265 \u2206e(DT1, DT2).\nLet pl be the probability that an example that reaches level l in a decision tree falls into a leaf at that level. To simplify, we will assume that this probability is constant, i.e., \u2200l pl = p, where p will be termed the leaf probability. This is a realistic assumption, in the sense that it is typically approximately true for the decision trees that are generated in practice. Let HT\u03b4 be the tree produced by the Hoeffding tree algorithm with desired probability \u03b4 given an infinite sequence of examples S, and DT\u2217 be the asymptotic batch decision tree induced by choosing at each node the attribute with true greatest G (i.e., by using infinite examples at each node). Let E[\u2206i(HT\u03b4, DT\u2217)] be the expected value of \u2206i(HT\u03b4, DT\u2217), taken over all possible infinite training sequences. We can then state the following result.\nTheorem 1. If HT\u03b4 is the tree produced by the Hoeffding tree algorithm with desired probability \u03b4 given infinite examples (Table 1), DT\u2217 is the asymptotic batch tree, and p is the leaf probability, then E[\u2206i(HT\u03b4, DT\u2217)] \u2264 \u03b4/p.\nProof. For brevity, we will refer to intensional disagreement simply as disagreement. Consider an example x that falls into a leaf at level lh in HT\u03b4, and into a leaf at level ld in DT\u2217. Let l = min{lh, ld}. Let PathH(x) = (N H 1 (x), N H 2 (x), . . . , NHl (x)) be x\u2019s path through HT\u03b4 up to level l, where NHi (x) is the node that x goes through at level i in HT\u03b4, and similarly for PathD(x), x\u2019s path through DT\u2217. If l = lh then NHl (x) is a leaf with a class prediction, and similarly for NDl (x) if l = ld. Let Ii represent the proposition \u201cPathH(x) = PathD(x) up to and including level i,\u201d with I0 = True. Notice that P (lh 6= ld) is included in P (NHl (x) 6= N D l (x)|Il\u22121), because if the two paths have different lengths then one tree must have a leaf where the other has an internal node. Then, omitting the dependency of the nodes on x for brevity,\nP (PathH(x) 6= PathD(x))\n= P (NH1 6= N D 1 \u2228 N H 2 6= N D 2 \u2228 . . . \u2228 N H l 6= N D l ) = P (NH1 6= N D 1 |I0) + P (N H 2 6= N D 2 |I1) + . . .\n+P (NHl 6= N D l |Il\u22121)\n=\nl\ni=1\nP (NHi 6= N D i |Ii\u22121) \u2264\nl\ni=1\n\u03b4 = \u03b4l (2)\nLet HT\u03b4(S) be the Hoeffding tree generated from training sequence S. Then E[\u2206i(HT\u03b4, DT\u2217)] is the average over all infinite training sequences S of the probability that an example\u2019s path through HT\u03b4(S) will differ from its path through DT\u2217:\nE[\u2206i(HT\u03b4, DT\u2217)]\n= S P (S) x P (x) I[PathH(x) 6= PathD(x)]\n= x P (x) P (PathH(x) 6= PathD(x))\n=\n\u221e\ni=1 x\u2208Li\nP (x) P (PathH(x) 6= PathD(x)) (3)\nwhere Li is the set of examples that fall into a leaf of DT\u2217 at level i. According to Equation 2, the probability that\nan example\u2019s path through HT\u03b4(S) will differ from its path through DT\u2217, given that the latter is of length i, is at most \u03b4i (since i \u2265 l). Thus\nE[\u2206i(HT\u03b4, DT\u2217)] \u2264 \u221e\ni=1 x\u2208Li\nP (x)(\u03b4i)\n=\n\u221e\ni=1 (\u03b4i) x\u2208Li P (x) (4)\nThe sum x\u2208Li P (x) is the probability that an example x will fall into a leaf of DT\u2217 at level i, and is equal to (1 \u2212 p)i\u22121p, where p is the leaf probability. Therefore\nE[\u2206i(HT\u03b4, DT\u2217)]\n\u2264 \u221e\ni=1\n(\u03b4i)(1 \u2212 p)i\u22121p = \u03b4p \u221e\ni=1\ni(1 \u2212 p)i\u22121\n= \u03b4p\n\u221e\ni=1\n(1 \u2212 p)i\u22121 + \u221e\ni=2\n(1 \u2212 p)i\u22121 + \u00b7 \u00b7 \u00b7\n+ \u221e\ni=k (1 \u2212 p)i\u22121 + \u00b7 \u00b7 \u00b7 = \u03b4p 1\np +\n1 \u2212 p\np + \u00b7 \u00b7 \u00b7 +\n(1 \u2212 p)k\u22121\np + \u00b7 \u00b7 \u00b7\n= \u03b4 1 + (1 \u2212 p) + \u00b7 \u00b7 \u00b7 + (1 \u2212 p)k\u22121 + \u00b7 \u00b7 \u00b7 = \u03b4 \u221e\ni=0\n(1 \u2212 p)i = \u03b4\np (5)\nThis completes the demonstration of Theorem 1. An immediate corollary of Theorem 1 is that the expected extensional disagreement between HT\u03b4 and DT\u2217 is also asymptotically at most \u03b4/p (although in this case the bound is much looser). Another corollary (whose proof we omit here in the interests of space) is that there exists a subtree of the asymptotic batch tree such that the expected disagreement between it and the Hoeffding tree learned on finite data is at most \u03b4/p. In other words, if \u03b4/p is small then the Hoeffding tree learned on finite data is very similar to a subtree of the asymptotic batch tree. A useful application of Theorem 1 is that, instead of \u03b4, users can now specify as input to the Hoeffding tree algorithm the maximum expected disagreement they are willing to accept, given enough examples for the tree to settle. The latter is much more meaningful, and can be intuitively specified without understanding the workings of the algorithm or the Hoeffding bound. The algorithm will also need an estimate of p, which can easily be obtained (for example) by running a conventional decision tree learner on a manageable subset of the data. How practical are these bounds? Suppose that the best and second-best attribute differ by 10% (i.e., /R = 0.1). Then, according to Equation 1, ensuring \u03b4 = 0.1% requires 380 examples, and ensuring \u03b4 = 0.0001% requires only 345 additional examples. An exponential improvement in \u03b4, and therefore in expected disagreement, can be obtained with a linear increase in the number of examples. Thus, even with very small leaf probabilities (i.e., very large trees), very good agreements can be obtained with a relatively small number of examples per\nnode. For example, if p = 0.01%, an expected disagreement of at most 1% can be guaranteed with 725 examples per node. If p = 1%, the same number of examples guarantees a disagreement of at most 0.01%."
                },
                {
                    "heading": "3. THE VFDT SYSTEM",
                    "text": "We have implemented a decision-tree learning system based on the Hoeffding tree algorithm, which we call VFDT (Very Fast Decision Tree learner). VFDT allows the use of either information gain or the Gini index as the attribute evaluation measure. It includes a number of refinements to the algorithm in Table 1:\nTies. When two or more attributes have very similar G\u2019s, potentially many examples will be required to decide between them with high confidence. This is presumably wasteful, because in this case it makes little difference which attribute is chosen. Thus VFDT can optionally decide that there is effectively a tie and split on the current best attribute if \u2206G < < \u03c4 , where \u03c4 is a user-specified threshold.\nG computation. The most significant part of the time cost per example is recomputing G. It is inefficient to recompute G for every new example, because it is unlikely that the decision to split will be made at that specific point. Thus VFDT allows the user to specify a minimum number of new examples nmin that must be accumulated at a leaf before G is recomputed. This effectively reduces the global time spent on G computations by a factor of nmin, and can make learning with VFDT nearly as fast as simply classifying the training examples. Notice, however, that it will have the effect of implementing a smaller \u03b4 than the one specified by the user, because examples will be accumulated beyond the strict minimum required to choose the correct attribute with confidence 1 \u2212 \u03b4. (This increases the time required to build a node, but our experiments show that the net effect is still a large speedup.) Because \u03b4 shrinks exponentially fast with the number of examples, the difference could be large, and the \u03b4 input to VFDT should be correspondingly larger than the target.\nMemory. As long as VFDT processes examples faster than they arrive, which will be the case in all but the most demanding applications, the sole obstacle to learning arbitrarily complex models will be the finite RAM available. VFDT\u2019s memory use is dominated by the memory required to keep counts for all growing leaves. If the maximum available memory is ever reached, VFDT deactivates the least promising leaves in order to make room for new ones. If pl is the probability that an arbitrary example will fall into leaf l, and el is the observed error rate at that leaf, then plel is an upper bound on the error reduction achievable by refining the leaf. plel for a new leaf is estimated using the counts at the parent for the corresponding attribute value. The least promising leaves are considered to be the ones with the lowest values of plel. When a leaf is deactivated, its memory is freed, except for a single number required to keep track of plel. A leaf can then be reactivated if it becomes more promising than currently active leaves.\nThis is accomplished by, at regular intervals, scanning through all the active and inactive leaves, and replacing the least promising active leaves with the inactive ones that dominate them.\nPoor attributes. Memory usage is also minimized by dropping early on attributes that do not look promising. As soon as the difference between an attribute\u2019s G and the best one\u2019s becomes greater than , the attribute can be dropped from consideration, and the memory used to store the corresponding counts can be freed.\nInitialization. VFDT can be initialized with the tree produced by a conventional RAM-based learner on a small subset of the data. This tree can either be input as is, or over-pruned to contain only those nodes that VFDT would have accepted given the number of examples at them. This can give VFDT a \u201chead start\u201d that will allow it to reach the same accuracies at smaller numbers of examples throughout the learning curve.\nRescans. VFDT can rescan previously-seen examples. This option can be activated if either the data arrives slowly enough that there is time for it, or if the dataset is finite and small enough that it is feasible to scan it multiple times. This means that VFDT need never grow a smaller (and potentially less accurate) tree than other algorithms because of using each example only once.\nThe next section describes an empirical study of VFDT, where the utility of these refinements is evaluated."
                },
                {
                    "heading": "4. EMPIRICAL STUDY",
                    "text": ""
                },
                {
                    "heading": "4.1 Synthetic data",
                    "text": "A system like VFDT is only useful if it is able to learn more accurate trees than a conventional system, given similar computational resources. In particular, it should be able to use to advantage the examples that are beyond a conventional system\u2019s ability to process. In this section we test this empirically by comparing VFDT with C4.5 release 8 [15] on a series of synthetic datasets. Using these allows us to freely vary the relevant parameters of the learning process. In order to ensure a fair comparison, we restricted the two systems to using the same amount of RAM. This was done by setting VFDT\u2019s \u201cavailable memory\u201d parameter to 40MB, and giving C4.5 the maximum number of examples that would fit in the same memory (100k examples).3 VFDT used information gain as the G function. Fourteen concepts were used for comparison, all with two classes and 100 binary attributes. The concepts were created by randomly generating decision trees as follows. At each level after the first three, a fraction f of the nodes was replaced by leaves; the rest became splits on a random attribute (that had not been used yet on a path from the root to the node being considered). When the decision tree reached a depth of 18, all the remaining growing nodes were replaced with leaves. Each leaf was randomly assigned a class. The size of the resulting concepts ranged from 2.2k leaves to 61k leaves with a median of 12.6k. A stream of training examples was then\n3VFDT occasionally grew slightly beyond 40MB because the limit was only enforced on heap-allocated memory. C4.5 always exceeded 40MB by the size of the unpruned tree.\ngenerated by sampling uniformly from the instance space, and assigning classes according to the target tree. We added various levels of class and attribute noise to the training examples, from 0 to 30%.4 (A noise level of n% means that each class/attribute value has a probability of n% of being reassigned at random, with equal probability for all values, including the original one.) In each run, 50k separate examples were used for testing. C4.5 was run with all default settings. We ran our experiments on two Pentium 6/200 MHz, one Pentium II/400 MHz, and one Pentium III/500 MHz machine, all running Linux.\nFigure 1 shows the accuracy of the learners averaged over all the runs. VFDT was run with \u03b4 = 10\u22127, \u03c4 = 5%, nmin = 200, no leaf reactivation, and no rescans. VFDTboot is VFDT bootstrapped with an over-pruned version of the tree produced by C4.5. C4.5 is more accurate than VFDT up to 25k examples, and the accuracies of the two systems are similar in the range from 25k to 100k examples (at which point C4.5 is unable to consider further examples). Most significantly, VFDT is able to take advantage of the examples after 100k to greatly improve accuracy (88.7% for VFDT and 88.8% for VFDT-boot, vs. 76.5% for C4.5). C4.5\u2019s early advantage comes from the fact it reuses examples to make decisions on multiple levels of the tree it is inducing, while VFDT uses each example only once. As expected, VFDT-boot\u2019s initialization lets it achieve high accuracy more quickly than without it. However, VFDT-boot\u2019s performance is surprising in that its accuracy is much higher than C4.5\u2019s at 100k examples, when VFDT-boot has not seen any examples that C4.5 did not. An explanation for this is that many of the experiments reported in Figure 1 contained noise, and, as Catlett [2] showed, over-pruning can be very effective at reducing overfitting in noisy domains.\nFigure 2 shows the average number of nodes in the trees induced by each of the learners. Notice that VFDT and VFDT-boot induce trees with similar numbers of nodes, and that both achieve greater accuracy with far fewer nodes than C4.5. This suggests that using VFDT can substantially increase the comprehensibility of the trees induced relative to C4.5. It also suggests that VFDT is less prone than C4.5 to overfitting noisy data.\nFigure 3 shows how the algorithms respond to noise. It compares four runs on the same concept (with 12.6k leaves), but with increasing levels of noise added to the training examples. C4.5\u2019s accuracy reports are for training sets with 100k examples, and VFDT and VFDT-boot\u2019s are for training sets of 20 million examples. VFDT\u2019s advantage compared to C4.5 increases with the noise level. This is further evidence that use of the Hoeffding bound is an effective pruning method.\n4The exact concepts used were, in the form (f , noise level, #nodes, #leaves): (0.15, 0.10, 74449, 37225), (0.15, 0.10, 13389, 6695), (0.17, 0.10, 78891, 39446), (0.17, 0.10, 93391, 46696), (0.25, 0.00, 25209, 12605), (0.25, 0.20, 25209, 12605), (0.25, 0.30, 25209, 12605), (0.25, 0.00, 15917, 7959), (0.25, 0.10, 31223, 15612), (0.25, 0.15, 16781, 8391), (0.25, 0.20, 4483, 2242), (0.28, 0.10, 122391, 61196), (0.28, 0.10, 6611, 3306), (0.25, 0.10, 25209, 12605). The last set of parameters was also used as the basis for the lesion studies reported below.\nFigure 4 shows how the algorithms compare on six concepts of varying size.5 All the training sets had 10% noise. As before, C4.5\u2019s results are for learning on 100k examples, while VFDT and VFDT-boot\u2019s are for 20 million. Both versions of VFDT do better than C4.5 on every concept size considered. However, contrary to what we would expect, as concept size increases the relative benefit seems to remain approximately constant for VFDT and VFDT-boot. Looking deeper, we find that with 20 million examples VFDT and VFDT-boot induce trees with approximately 9k nodes regardless of the size of the underlying concept. This suggests that they would take good advantage of even more training examples.\nWe carried out all runs without ever writing VFDT\u2019s training examples to disk (i.e., generating them on the fly and passing them directly to VFDT). For time comparison purposes, however, we measured the time it takes VFDT to read examples from the (0.25, 0.10, 25209, 12605) data set from disk on the Pentium III/500 MHz machine. VFDT takes 5752 seconds to read the 20 million examples, and 625 seconds to process them. In other words, learning time is about an order of magnitude less than input time. On the same runs, C4.5 takes 36 seconds to read and process 100k examples, and VFDT takes 47 seconds.\nFinally, we generated 160 million examples from the (0.25, 0.10, 25209, 12605) concept. Figure 5 compares VFDT and C4.5 on this data set. VFDT makes progress over the entire data set, but begins to asymptote after 10 million examples; the final 150 million examples contribute 0.58% to accuracy. VFDT took 9501 seconds to process the examples (excluding I/O) and induced 21.9k leaves. In the near future we plan to carry out similar runs with more complex concepts and billions of examples."
                },
                {
                    "heading": "4.2 Lesion studies",
                    "text": "We conducted a series of lesion studies to evaluate the effectiveness of some of the components and parameters of the VFDT system. Figure 6 shows the accuracy of the learners on the (0.25, 0.00, 25209, 12605) data set. It also shows a slight modification to the VFDT-boot algorithm, where the tree produced by C4.5 is used without first over-pruning it. All versions of VFDT were run with \u03b4 = 10\u22127, \u03c4 = 5%, nmin = 200, no leaf reactivation, and no rescans. C4.5 does better without noise than with it, but VFDT is still able to use additional data to significantly improve accuracy. VFDT-boot with the \u201cno over-prune\u201d setting is initially better than the over-pruning version, but does not make much progress and is eventually overtaken. We hypothesize that this is because it has difficulty overcoming the poor low-confidence decisions C4.5 made near its leaves.\nIn the remainder of the lesion studies VFDT was run on the (0.25, 0.10, 25209, 12605) data set with \u03b4 = 10\u22127, \u03c4 = 5%, nmin = 200, no leaf reactivation, and no rescans. We evaluated the effect of disabling ties, so that VFDT does not make any splits until it is able to identify a clear winner.\n5The concept (0.15, 0.10, 74449, 37225) turned out to be atypically easy, and is not included in the graph to avoid obscuring the trend. The observed accuracies for this concept were: C4.5 \u2013 83.1%; VFDT \u2013 89.0%; VFDT-boot \u2013 89.7%.\nWe conducted two runs, holding all parameters constant except that the second run never split with a tie. Without ties VFDT induced a tree with only 65 nodes and 72.9% accuracy, compared to 8k nodes and 86.9% accuracy with ties. VFDT-boot without ties produced 805 nodes and 83.3% accuracy, compared to 8k nodes and 88.5% accuracy with ties. We also carried out two runs holding all parameters constant except nmin, the number of new examples that must be seen at a node before G\u2019s are recomputed. The first run recomputed G every 200 examples (nmin = 200), and the second did it for every example (nmin = 1). Doing the G computations for every example, VFDT gained 1.1% accuracy and took 3.8 times longer to run. VFDT-boot lost 0.9% accuracy and took 3.7 times longer. Both learners induced about 5% more nodes with the more frequent G computations. We then carried out two runs holding all parameters but VFDT\u2019s memory limit constant. The first run was allowed 40 MB of memory; the second was allowed 80 MB. VFDT and VFDT-boot both induced 7.8k more nodes with the additional memory, which improved VFDT\u2019s accuracy by 3.0% and VFDT-boot\u2019s by 3.2%. Finally, we carried out two runs holding all parameters but \u03b4 constant. The first run had a delta of 10\u22122, and the second had a delta of 10\u22127. With the lower \u03b4, VFDT and VFDT-boot both induced about 30% fewer nodes than with the higher one. VFDT\u2019s accuracy was 2.3% higher and VFDT-boot\u2019s accuracy was 1.0% higher with the lower \u03b4."
                },
                {
                    "heading": "4.3 Web data",
                    "text": "We are currently applying VFDT to mining the stream of Web page requests emanating from the whole University of Washington main campus. The nature of the data is described in detail in [23]. In our experiments so far we have used a one-week anonymized trace of all the external web accesses made from the university campus. There were 23,000 active clients during this one-week trace period, and the entire university population is estimated at 50,000 people (students, faculty and staff). The trace contains 82.8 million requests, which arrive at a peak rate of 17,400 per minute. The size of the compressed trace file is about 20 GB.6 Each request is tagged with an anonymized organization ID that associates the request with one of the 170 organizations (colleges, departments, etc.) within the university. One purpose this data can be used for is to improve Web caching. The key to this is predicting as accurately as possible which hosts and pages will be requested in the near future, given recent requests. We applied decisiontree learning to this problem in the following manner. We split the campus-wide request log into a series of equal time slices T0, T1, . . . , Tt, . . . ; in the experiments we report, each time slice is an hour. For each organization O1, O2, . . . , Oi, . . . , O170 and each of the 244k hosts appearing in the logs H1, . . . , Hj , . . . , H244k, we maintain a count of how many times the organization accessed the host in the time slice, Cijt. We discretize these counts into four buckets, representing \u201cno requests,\u201d \u201c1 \u2013 12 requests,\u201d \u201c13 \u2013 25 requests\u201d and \u201c26 or more requests.\u201d Then for each time slice and host accessed in that time slice (Tt, Hj) we generate an example with attributes t mod 24, C1,jt, . . . , Cijt, . . . C170,jt\n6This log is from May 1999. Traffic in May 2000 was double this size; a one-week log was approximately 50 GB compressed.\nand class 1 if Hj is requested in time slice Tt+1 and 0 if it is not. This can be carried out in real time using modest resources by keeping statistics on the last and current time slices Ct\u22121 and Ct in memory, only keeping counts for hosts that actually appear in a time slice (we never needed more than 30k counts), and outputting the examples for Ct\u22121 as soon as Ct is complete. Using this procedure we obtained a dataset containing 1.89 million examples, 61.1% of which were labeled with the most common class (that the host did not appear again in the next time slice).\nTesting was carried out on the examples from the last day (276,230 examples). VFDT was run with \u03b4 = 10\u22127, \u03c4 = 5%, and nmin = 200. All runs were carried out on a 400 MHz Pentium machine. A decision stump (a decision tree with only one node) obtains 64.2% accuracy on this data. The decision stump took 1277 seconds to learn, and VFDT took 1450 seconds to do one pass over the training data (after being initialized with C4.5\u2019s over-pruned tree). The majority of this time (983 seconds) was spent reading data from disk. The bootstrap run of C4.5 took 2975 seconds to learn on a subsample of 74.5k examples (as many as would fit in 40 MB of RAM) and achieved 73.3% accuracy. Thus VFDT learned faster on 1.61 million examples than C4.5 did on 75k. We also used a machine with 1 GB of RAM to run C4.5 on the entire 1.61 million training examples; the run took 24 hours and the resulting tree was 75% accurate. Figure 7 shows VFDT-boot\u2019s performance on this dataset, using 1 GB of RAM. We extended VFDT\u2019s run out to 4 million examples by rescanning. The x axis shows the number of examples presented to VFDT after the C4.5 bootstrap phase was complete. Accuracy improves steadily as more examples are seen. VFDT is able to achieve accuracy similar to C4.5\u2019s in a small fraction of the time. Further, C4.5\u2019s memory requirements and batch nature will not allow it to scale to traces much larger than a week, while VFDT can easily incorporate data indefinitely. The next step is to apply VFDT to predicting page requests from a given host. We also plan to address issues related to time-changing behavior and then set VFDT running permanently, learning and relearning as dictated by the data stream."
                },
                {
                    "heading": "5. RELATED WORK",
                    "text": "Previous work on mining large databases using subsampling methods includes the following. Catlett [2] proposed several heuristic methods for extending RAM-based batch decisiontree learners to datasets with up to hundreds of thousands of examples. Musick, Catlett and Russell [13] proposed and tested (but did not implement in a learner) a theoretical model for choosing the size of subsamples to use in comparing attributes. Maron and Moore [9] used Hoeffding bounds to speed selection of instance-based regression models via cross-validation (see also [12]). Gratch\u2019s Sequential ID3 [6] used a statistical method to minimize the number of examples needed to choose each split in a decision tree. (Sequential ID3\u2019s guarantees of similarity to the batch tree were much looser than those derived here for Hoeffding trees, and it was only tested on repeatedly sampled small datasets.) Gehrke et al.\u2019s BOAT [5] learned an approximate tree using a fixed-size subsample, and then refined it by scanning the full database. Provost et al. [14] studied different strategies for mining larger and larger subsamples until accuracy (apparently) asymptotes. In contrast to systems that learn in main memory by subsampling, systems like SLIQ [10] and SPRINT [17] use all the data, and concentrate on optimizing access to disk by always reading examples (more precisely, attribute lists) sequentially. VFDT combines the best of both worlds, accessing data sequentially and using subsampling to potentially require much less than one scan, as opposed to many. This allows it to scale to larger databases than either method alone. VFDT has the additional advantages of being incremental and anytime: new examples can be quickly incorporated as they arrive, and a usable model is available after the first few examples and then progressively refined.\nAs mentioned previously, there is a large literature on incremental learning, which space limitations preclude reviewing here. The system most closely related to ours is Utgoff\u2019s [20] ID5R (extended in [21]). ID5R learns the same tree as ID3 (a batch method), by restructuring subtrees as needed. While its learning time is linear in the number of examples, it is worst-case exponential in the number of attributes. On the simple, noise-free problems it was tested on, it was much slower than ID3; noise would presumably aggravate this. Thus ID5R does not appear viable for learning from high-speed data streams.\nA number of efficient incremental or single-pass algorithms for KDD tasks other than supervised learning have appeared in recent years (e.g., clustering [4] and association rule mining [19]). A substantial theoretical literature on online algorithms exists (e.g., [8]), but it focuses on weak learners (e.g., linear separators), because little can be proved about strong ones like decision trees."
                },
                {
                    "heading": "6. FUTURE WORK",
                    "text": "We plan to shortly compare VFDT with SPRINT/SLIQ. VFDT may outperform these even in fully disk-resident datasets, because it can learn in less than one scan while the latter require multiple scans, and the dominant component of their cost is often the time required to read examples from disk multiple times. VFDT\u2019s speed and anytime character make it ideal for interactive data mining; we plan to\nalso study its application in this context (see [18]). Other directions for future work include: further developing the application of VFDT to Web log data; studying other applications of VFDT (e.g., intrusion detection); using nondiscretized numeric attributes in VFDT; studying the use of post-pruning in VFDT; further optimizing VFDT\u2019s computations (e.g., by recomputing G\u2019s exactly when we can tell that the current example may cause the Hoeffding bound to be reached); using adaptive \u03b4\u2019s; studying the use of an example cache in main memory to speed induction by reusing examples at multiple levels; comparing VFDT to ID5R and other incremental algorithms; adapting VFDT to learn evolving concepts in time-changing domains; adapting VFDT to learning with imbalanced classes and asymmetric misclassification costs; adapting VFDT to the extreme case where even the final decision tree (without any stored sufficient statistics) does not fit in main memory; parallelizing VFDT; applying the ideas described here to other types of learning (e.g., rule induction, clustering); etc."
                },
                {
                    "heading": "7. CONCLUSION",
                    "text": "This paper introduced Hoeffding trees, a method for learning online from the high-volume data streams that are increasingly common. Hoeffding trees allow learning in very small constant time per example, and have strong guarantees of high asymptotic similarity to the corresponding batch trees. VFDT is a high-performance data mining system based on Hoeffding trees. Empirical studies show its effectiveness in taking advantage of massive numbers of examples. VFDT\u2019s application to a high-speed stream of Web log data is under way."
                },
                {
                    "heading": "Acknowledgments",
                    "text": "This research was partly funded by an NSF CAREER award to the first author."
                },
                {
                    "heading": "8. REFERENCES",
                    "text": "[1] L. Breiman, J. H. Friedman, R. A. Olshen, and C. J.\nStone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984.\n[2] J. Catlett. Megainduction: Machine Learning on Very Large Databases. PhD thesis, Basser Department of Computer Science, University of Sydney, Sydney, Australia, 1991.\n[3] T. G. Dietterich. Overfitting and undercomputing in machine learning. Computing Surveys, 27:326\u2013327, 1995.\n[4] M. Ester, H.-P. Kriegel, J. Sander, M. Wimmer, and X. Xu. Incremental clustering for mining in a data warehousing environment. In Proceedings of the Twenty-Fourth International Conference on Very Large Data Bases, pages 323\u2013333, New York, NY, 1998. Morgan Kaufmann.\n[5] J. Gehrke, V. Ganti, R. Ramakrishnan, and W.-L. Loh. BOAT: optimistic decision tree construction. In Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data, pages 169\u2013180, Philadelphia, PA, 1999. ACM Press.\n[6] J. Gratch. Sequential inductive learning. In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 779\u2013786, Portland, OR, 1996. AAAI Press.\n[7] W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American Statistical Association, 58:13\u201330, 1963.\n[8] N. Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. Machine Learning, 2:285\u2013318, 1997.\n[9] O. Maron and A. Moore. Hoeffding races: Accelerating model selection search for classification and function approximation. In J. D. Cowan, G. Tesauro, and J. Alspector, editors, Advances in Neural Information Processing Systems 6. Morgan Kaufmann, San Mateo, CA, 1994.\n[10] M. Mehta, A. Agrawal, and J. Rissanen. SLIQ: A fast scalable classifier for data mining. In Proceedings of the Fifth International Conference on Extending Database Technology, pages 18\u201332, Avignon, France, 1996. Springer.\n[11] R. G. Miller, Jr. Simultaneous Statistical Inference. Springer, New York, NY, 2nd edition, 1981.\n[12] A. W. Moore and M. S. Lee. Efficient algorithms for minimizing cross validation error. In Proceedings of the Eleventh International Conference on Machine Learning, pages 190\u2013198, New Brunswick, NJ, 1994. Morgan Kaufmann.\n[13] R. Musick, J. Catlett, and S. Russell. Decision theoretic subsampling for induction on large databases. In Proceedings of the Tenth International Conference on Machine Learning, pages 212\u2013219, Amherst, MA, 1993. Morgan Kaufmann.\n[14] F. Provost, D. Jensen, and T. Oates. Efficient progressive sampling. In Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 23\u201332, San Diego, CA, 1999. ACM Press.\n[15] J. R. Quinlan. C4.5: Programs for Machine Learning. Morgan Kaufmann, San Mateo, CA, 1993.\n[16] J. R. Quinlan and R. M. Cameron-Jones. Oversearching and layered search in empirical learning. In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, pages 1019\u20131024, Montre\u0301al, Canada, 1995. Morgan Kaufmann.\n[17] J. C. Shafer, R. Agrawal, and M. Mehta. SPRINT: A scalable parallel classifier for data mining. In Proceedings of the Twenty-Second International Conference on Very Large Databases, pages 544\u2013555, Mumbai, India, 1996. Morgan Kaufmann.\n[18] P. Smyth and D. Wolpert. Anytime exploratory data analysis for massive data sets. In Proceedings of the Third International Conference on Knowledge Discovery and Data Mining, pages 54\u201360, Newport Beach, CA, 1997. AAAI Press.\n[19] H. Toivonen. Sampling large databases for association rules. In Proceedings of the Twenty-Second International Conference on Very Large Data Bases, pages 134\u2013145, Mumbai, India, 1996. Morgan Kaufmann.\n[20] P. E. Utgoff. Incremental induction of decision trees. Machine Learning, 4:161\u2013186, 1989.\n[21] P. E. Utgoff. An improved algorithm for incremental induction of decision trees. In Proceedings of the Eleventh International Conference on Machine Learning, pages 318\u2013325, New Brunswick, NJ, 1994. Morgan Kaufmann.\n[22] G. I. Webb. OPUS: An efficient admissible algorithm for unordered search. Journal of Artificial Intelligence Research, 3:431\u2013465, 1995.\n[23] A. Wolman, G. Voelker, N. Sharma, N. Cardwell, M. Brown, T. Landray, D. Pinnel, A. Karlin, and H. Levy. Organization-based analysis of Web-object sharing and caching. In Proceedings of the Second USENIX Conference on Internet Technologies and Systems, pages 25\u201336, Boulder, CO, 1999."
                }
            ],
            "references": [
                {
                    "title": "Classification and Regression Trees",
                    "authors": [
                        "L. Breiman",
                        "J.H. Friedman",
                        "R.A. Olshen",
                        "C.J. Stone"
                    ],
                    "year": 1984
                },
                {
                    "title": "Megainduction: Machine Learning on Very Large Databases",
                    "authors": [
                        "J. Catlett"
                    ],
                    "venue": "PhD thesis, Basser Department of Computer Science,",
                    "year": 1991
                },
                {
                    "title": "Overfitting and undercomputing in machine learning",
                    "authors": [
                        "T.G. Dietterich"
                    ],
                    "venue": "Computing Surveys,",
                    "year": 1995
                },
                {
                    "title": "Incremental clustering for mining in a data warehousing environment",
                    "authors": [
                        "M. Ester",
                        "H.-P. Kriegel",
                        "J. Sander",
                        "M. Wimmer",
                        "X. Xu"
                    ],
                    "venue": "In Proceedings of the Twenty-Fourth International Conference on Very Large Data Bases,",
                    "year": 1998
                },
                {
                    "title": "BOAT: optimistic decision tree construction",
                    "authors": [
                        "J. Gehrke",
                        "V. Ganti",
                        "R. Ramakrishnan",
                        "W.-L. Loh"
                    ],
                    "venue": "In Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data,",
                    "year": 1999
                },
                {
                    "title": "Sequential inductive learning",
                    "authors": [
                        "J. Gratch"
                    ],
                    "venue": "In Proceedings of the Thirteenth National Conference on Artificial Intelligence,",
                    "year": 1996
                },
                {
                    "title": "Probability inequalities for sums of bounded random variables",
                    "authors": [
                        "W. Hoeffding"
                    ],
                    "venue": "Journal of the American Statistical Association,",
                    "year": 1963
                },
                {
                    "title": "Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm",
                    "authors": [
                        "N. Littlestone"
                    ],
                    "venue": "Machine Learning,",
                    "year": 1997
                },
                {
                    "title": "Hoeffding races: Accelerating model selection search for classification and function approximation",
                    "authors": [
                        "O. Maron",
                        "A. Moore"
                    ],
                    "venue": "Advances in Neural Information Processing Systems",
                    "year": 1994
                },
                {
                    "title": "SLIQ: A fast scalable classifier for data mining",
                    "authors": [
                        "M. Mehta",
                        "A. Agrawal",
                        "J. Rissanen"
                    ],
                    "venue": "In Proceedings of the Fifth International Conference on Extending Database Technology,",
                    "year": 1996
                },
                {
                    "title": "Simultaneous Statistical Inference",
                    "authors": [
                        "R.G. Miller",
                        "Jr."
                    ],
                    "year": 1981
                },
                {
                    "title": "Efficient algorithms for minimizing cross validation error",
                    "authors": [
                        "A.W. Moore",
                        "M.S. Lee"
                    ],
                    "venue": "In Proceedings of the Eleventh International Conference on Machine Learning,",
                    "year": 1994
                },
                {
                    "title": "Decision theoretic subsampling for induction on large databases",
                    "authors": [
                        "R. Musick",
                        "J. Catlett",
                        "S. Russell"
                    ],
                    "venue": "In Proceedings of the Tenth International Conference on Machine Learning,",
                    "year": 1993
                },
                {
                    "title": "Efficient progressive sampling",
                    "authors": [
                        "F. Provost",
                        "D. Jensen",
                        "T. Oates"
                    ],
                    "venue": "In Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
                    "year": 1999
                },
                {
                    "title": "Programs for Machine Learning",
                    "authors": [
                        "J.R. Quinlan. C"
                    ],
                    "year": 1993
                },
                {
                    "title": "Oversearching and layered search in empirical learning",
                    "authors": [
                        "J.R. Quinlan",
                        "R.M. Cameron-Jones"
                    ],
                    "venue": "In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence,",
                    "year": 1995
                },
                {
                    "title": "SPRINT: A scalable parallel classifier for data mining",
                    "authors": [
                        "J.C. Shafer",
                        "R. Agrawal",
                        "M. Mehta"
                    ],
                    "venue": "In Proceedings of the Twenty-Second International Conference on Very Large Databases,",
                    "year": 1996
                },
                {
                    "title": "Anytime exploratory data analysis for massive data sets",
                    "authors": [
                        "P. Smyth",
                        "D. Wolpert"
                    ],
                    "venue": "In Proceedings of the Third International Conference on Knowledge Discovery and Data Mining,",
                    "year": 1997
                },
                {
                    "title": "Sampling large databases for association rules",
                    "authors": [
                        "H. Toivonen"
                    ],
                    "venue": "In Proceedings of the Twenty-Second International Conference on Very Large Data Bases,",
                    "year": 1996
                },
                {
                    "title": "Incremental induction of decision trees",
                    "authors": [
                        "P.E. Utgoff"
                    ],
                    "venue": "Machine Learning,",
                    "year": 1989
                },
                {
                    "title": "An improved algorithm for incremental induction of decision trees",
                    "authors": [
                        "P.E. Utgoff"
                    ],
                    "venue": "In Proceedings of the Eleventh International Conference on Machine Learning,",
                    "year": 1994
                },
                {
                    "title": "OPUS: An efficient admissible algorithm for unordered search",
                    "authors": [
                        "G.I. Webb"
                    ],
                    "venue": "Journal of Artificial Intelligence Research,",
                    "year": 1995
                },
                {
                    "title": "Organization-based analysis of Web-object sharing and caching",
                    "authors": [
                        "A. Wolman",
                        "G. Voelker",
                        "N. Sharma",
                        "N. Cardwell",
                        "M. Brown",
                        "T. Landray",
                        "D. Pinnel",
                        "A. Karlin",
                        "H. Levy"
                    ],
                    "venue": "In Proceedings of the Second USENIX Conference on Internet Technologies and Systems,",
                    "year": 1999
                }
            ],
            "id": "SP:737af878ae6881ca54920d202cbb87f593c31d95",
            "authors": [
                {
                    "name": "Pedro Domingos",
                    "affiliations": []
                },
                {
                    "name": "Geoff Hulten",
                    "affiliations": []
                }
            ],
            "abstractText": "Many organizations today have more than very large databases; they have databases that grow without limit at a rate of several million records per day. Mining these continuous data streams brings unique opportunities, but also new challenges. This paper describes and evaluates VFDT, an anytime system that builds decision trees using constant memory and constant time per example. VFDT can incorporate tens of thousands of examples per second using off-the-shelf hardware. It uses Hoeffding bounds to guarantee that its output is asymptotically nearly identical to that of a conventional learner. We study VFDT\u2019s properties and demonstrate its utility through an extensive set of experiments on synthetic data. We apply VFDT to mining the continuous stream of Web access data from the whole University of Washington main campus.",
            "title": "Mining High-Speed Data Streams"
        },
        "Y": {
            "blog_id": "mining-high-speed-data-streams",
            "summary": [
                "Mining High-Speed Data Streams \u2013 Domingos & Hulten 2000  This paper won a \u2018test of time\u2019 award at KDD\u201915 as an \u2018outstanding paper from a past KDD Conference beyond the last decade that has had an important impact on the data mining community.\u2019  Here\u2019s what the test-of-time committee have to say about it:  This paper proposes a decision tree learner for data streams, the Hoeffding Tree algorithm, which comes with the guarantee that the learned decision tree is asymptotically nearly identical to that of a non-incremental learner using infinitely many examples.",
                "This work constitutes a significant step in developing methodology suitable for modern \u2018big data\u2019 challenges and has initiated a lot of follow-up research.",
                "The Hoeffding Tree algorithm has been covered in various textbooks and is available in several public domain tools, including the WEKA Data Mining platform.",
                "The goal is to create a knowledge discovery system that can cope with large volumes of data (perhaps an unbounded stream) without needing to fit everything in memory (40MB was the allotted amount used in their evaluation tests \u2013 remember this was 2000).",
                "Ideally, we would like to have KDD systems that operate continuously and indefinitely, incorporating examples as they arrive, and never losing potentially valuable information.",
                "Such desiderata are fulfilled by incremental learning methods (also known as online, successive or sequential methods), on which a substantial literature exists.",
                "However, the available algorithms of this type have significant shortcomings from the KDD point of view.",
                "Some are reasonably efficient, but do not guarantee that the model learned will be similar to the one obtained by learning on the same data in batch mode.",
                "They are highly sensitive to example ordering, potentially never recovering from an unfavorable set of early examples.",
                "Others produce the same model as the batch version, but at a high cost in efficiency, often to the point of being slower than the batch algorithm.",
                "Based on a statistical result known as the Hoeffding bound, the authors show how to create Hoeffding (decision) trees and build a Very Fast Decision Tree (VFDT) system based on them.",
                "A key property of the Hoeffding tree algorithm is that it is possible to guarantee under realistic assumptions that the trees it produces are asymptotically arbitrarily close to the ones produced by a batch learner (i.e., a learner that uses all the examples to choose a test at each node).",
                "In other words, the incremental nature of the Hoeffding tree algorithm does not significantly affect the quality of the trees it produces.",
                "In a classification problem, a set of N training examples of the form (x\u20d7,y) is given, where y is a discrete class label and x\u20d7 is a vector of d attributes.",
                "From these examples we need to produce a model y = f(x\u20d7) that will predict the class of future examples x\u20d7 with high accuracy.",
                "Decision tree learners create models in the form of decision trees, where each node contains a test on an attribute, each branch corresponds to a possible outcome of the test, and each leaf contains a class prediction.",
                "To learn a decision tree you recursively replace leaves by test nodes, starting at the root.",
                "Our goal is to design a decision tree learner for extremely large (potentially infinite) datasets.",
                "This learner should require each example to be read at most once, and only a small constant time to process it.",
                "This will make it possible to directly mine online data sources (i.e., without ever storing the examples), and to build potentially very complex trees with acceptable computational cost.",
                "In Hoeffding trees, in order to find the best attribute to test at a given node, only a small subset of the training examples that pass through that node are used.",
                "The key of course, is to determine how small that subset can be, and what guarantees we can give concerning it.",
                "Thus, given a stream of examples, the first ones will be used to choose the root test; once the root attribute is chosen, the succeeding examples will be passed down to the corresponding leaves and used to choose the appropriate attributes there, and so on recursively.",
                "We solve the difficult problem of deciding exactly how many examples are necessary at each node by using a statistical result known as the Hoeffding bound (or additive Chernoff bound).",
                "Given a real-valued random variable r with range R (e.g. 0-1 for a probability), and n independent observations of the variable, we can compute the mean of those observations, r\u0304.",
                "The Hoeffding bound tells us that with probability 1 \u2013 \u03b4, the true mean of the variable is at least r\u0304 \u2013 \u03b5, where:  The Hoeffding bound has the very attractive property that it is independent of the probability distribution generating the observations.",
                "The price of this generality is that the bound is more conservative than distribution-dependent ones (i.e., it will take more observations to reach the same \u03b4 and \u03b5).",
                "If G(Xi) is the heuristic used to choose test attributes, then we want to ensure with high probability the attribute chosen using n examples (where n is as small as possible) is the same that would be chosen using infinite examples.",
                "Suppose that we\u2019ve seen n examples so far, and the best attribute predicted by G is Xa and the second best is Xb.",
                "Call the difference between the observed heuristic values of Xa and Xb \u0394G\u0304  Now, given a desired \u03b4, the Hoeffding bound tells us that Xa is the correct choice with probability 1 \u2013 \u03b4 if n examples have been seen at this node and \u0394G\u0304 > \u03b52.",
                "Thus a node needs to accumulate examples from the streamuntil \u03b5 becomes smaller than \u2206G.",
                "(Notice that \u03b5 is a monotonically decreasing function of n.) At this point the node can be split using the current best attribute, and succeeding examples will be passed to the new leaves.",
                "Pseudo-code for a Hoeffding tree algorithm based on this is given in table 1 of the paper.",
                "The VFDT system was built using this algorithm, and included a number of additional optimisations:  When two or more attributes have very similar scores, lots of examples may be needed to decide between them with confidence.",
                "But if they are very similar, it probably doesn\u2019t matter too much which one we choose, so let\u2019s just pick one after we reach some user-defined threshold and move on\u2026  We don\u2019t need to recompute G after every example since it is unlikely the decision to split will be made at that specific point.",
                "So we can micro-batch and accept a minimum number of new examples before recomputing G.  Under memory pressure, VFDT deactivates the least promising leaves in order to make room for new ones.",
                "Likewise VFDT can also drop early on attributes that do not look promising.",
                "VFDT can be initialised with a tree produced offline by a traditional batch learner.",
                "(Trained on a subset of the overall data).",
                "VFDT can rescan previously seen examples if desired.",
                "This option can be activated if either the data arrives slowly enough that there is time for it, or if the dataset is finite and small enough that it is feasible to scan it multiple times.",
                "This means that VFDT need never grow a smaller (and potentially less accurate) tree than other algorithms because of using each example only once."
            ],
            "author_id": "ACOLYER",
            "pdf_url": "http://homes.cs.washington.edu/~pedrod/papers/kdd00.pdf",
            "author_full_name": "Adrian Colyer",
            "source_website": "https://blog.acolyer.org/about/",
            "id": 4830217
        }
    },
    "6726365": {
        "X": {
            "sections": [
                {
                    "heading": "1. Introduction",
                    "text": "Due to advantages such as computational speed, precise manipulation, and exact timing, computers and robots are often superior to humans at performing tasks with well-defined goals and objectives. However, it can be difficult, even for experts, to design reward functions and objectives that lead to desired behaviors when designing autonomous agents (Ng et al., 1999; Amodei et al., 2016). When goals or rewards are difficult for a human to specify, inverse reinforcement learn-\n*Equal contribution 1Department of Computer Science, University of Texas at Austin, USA 2Preferred Networks, Japan. Correspondence to: Daniel S. Brown <dsbrown@cs.utexas.edu>, Wonjoon Goo <wonjoon@cs.utexas.edu>.\nProceedings of the 36 th International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s).\nFigure 1. T-REX takes a sequence of ranked demonstrations and learns a reward function from these rankings that allows policy improvement over the demonstrator via reinforcement learning.\ning (IRL) (Abbeel & Ng, 2004) techniques can be applied to infer the intrinsic reward function of a user from demonstrations. Unfortunately, high-quality demonstrations are difficult to provide for many tasks\u2014for instance, consider a non-expert user attempting to give kinesthetic demonstrations of a household chore to a robot. Even for relative experts, tasks such as high-frequency stock trading or playing complex video games can be difficult to perform optimally.\nIf a demonstrator is suboptimal, but their intentions can be ascertained, then a learning agent ought to be able to exceed the demonstrator\u2019s performance in principle. However, existing IRL algorithms fail to do this, typically searching for a reward function that makes the demonstrations appear near-optimal (Ramachandran & Amir, 2007; Ziebart et al., 2008; Finn et al., 2016; Henderson et al., 2018). Thus, when the demonstrator is suboptimal, IRL results in suboptimal behavior as well. Imitation learning approaches (Argall et al., 2009) that mimic behavior directly without reward inference, such as behavioral cloning (Torabi et al., 2018a), also suffer from the same shortcoming.\nTo overcome this critical flaw in current imitation learning methods, we propose a novel IRL algorithm, Trajectoryranked Reward EXtrapolation (T-REX)1 that utilizes ranked demonstrations to extrapolate a user\u2019s underlying intent be-\n1Code available at https://github.com/hiwonjoon/ ICML2019-TREX\nar X\niv :1\n90 4.\n06 38\n7v 5\n[ cs\n.L G\n] 9\nJ ul\n2 01\n9\nyond the best demonstration, even when all demonstrations are highly suboptimal. This, in turn, enables a reinforcement learning agent to exceed the performance of the demonstrator by learning to optimize this extrapolated reward function. Specifically, we use ranked demonstrations to learn a statebased reward function that assigns greater total return to higher-ranked trajectories. Thus, while standard inverse reinforcement learning approaches seek a reward function that justifies the demonstrations, we instead seek a reward function that explains the ranking over demonstrations, allowing for potentially better-than-demonstrator performance.\nUtilizing ranking in this way has several advantages. First, rather than imitating suboptimal demonstrations, it allows us to identify features that are correlated with rankings, in a manner that can be extrapolated beyond the demonstrations. Although the learned reward function could potentially overfit to the provided rankings, we demonstrate empirically that it extrapolates well, successfully predicting returns of trajectories that are significantly better than any observed demonstration, likely due to the powerful regularizing effect of having many pairwise ranking constraints between trajectories. For example, the degenerate all-zero reward function (the agent always receives a reward of 0) makes any given set of demonstrations appear optimal. However, such a reward function is eliminated from consideration by any pair of (non-equally) ranked demonstrations. Second, when learning features directly from high-dimensional data, this regularizing effect can also help to prevent overfitting to the small fraction of state space visited by the demonstrator. By utilizing a set of suboptimal, but ranked demonstrations, we provide the neural network with diverse data from multiple areas of the state space, allowing an agent to better learn both what to do and what not to do in a variety of situations.\nWe evaluate T-REX on a variety of standard Atari and MuJoCo benchmark tasks. Our experiments show that T-REX can extrapolate well, achieving performance that is often more than twice as high as the best-performing demonstration, as well as outperforming state-of-the-art imitation learning algorithms. We also show that T-REX performs well even in the presence of significant ranking noise, and provide results showing that T-REX can learn good policies simply by observing a novice demonstrator that noisily improves over time."
                },
                {
                    "heading": "2. Related Work",
                    "text": "The goal of our work is to achieve improvements over a suboptimal demonstrator in high-dimensional reinforcement learning tasks without requiring a hand-specified reward function or supervision during policy learning. While there is a large body of research on learning from demonstrations (Argall et al., 2009; Gao et al., 2012; Osa et al., 2018; Arora & Doshi, 2018), most work assumes access to action labels,\nwhile we learn only from observations. Additionally, little work has addressed the problem of learning from ranked demonstrations, especially when they are significantly suboptimal. To the best of our knowledge, our work is the first to show better-than-demonstrator performance in highdimensional tasks such as Atari, without requiring active human supervision or access to ground-truth rewards."
                },
                {
                    "heading": "2.1. Learning from demonstrations",
                    "text": "Early work on learning from demonstration focused on behavioral cloning (Pomerleau, 1991), in which the goal is to learn a policy that imitates the actions taken by the demonstrator; however, without substantial human feedback and correction, this method is known to have large generalization error (Ross et al., 2011). Recent deep learning approaches to imitation learning (Ho & Ermon, 2016) have used Generative Adversarial Networks (Goodfellow et al., 2014) to model the distribution of actions taken by the demonstrator.\nRather than directly learn to mimic the demonstrator, inverse reinforcement learning (IRL) (Gao et al., 2012; Arora & Doshi, 2018) seeks to find a reward function that models the intention of the demonstrator, thereby allowing generalization to states that were unvisited during demonstration. Given such a reward function, reinforcement learning (Sutton & Barto, 1998) techniques can be applied to learn an optimal policy. Maximum entropy IRL seeks to find a reward function that makes the demonstrations appear near-optimal, while further disambiguating inference by also maximizing the entropy of the resulting policy (Ziebart et al., 2008; Boularias et al., 2011; Wulfmeier et al., 2015; Finn et al., 2016). While maximum entropy approaches are robust to limited and occasional suboptimality in the demonstrations, they still fundamentally seek a reward function that justifies the demonstrations, resulting in performance that is explicitly tied to the performance of the demonstrator.\nSyed & Schapire (2008) proved that, given prior knowledge about which features contribute positively or negatively to the true reward, an apprenticeship policy can be found that is guaranteed to outperform the demonstrator. However, their approach requires hand-crafted, linear features, knowledge of the true signs of the rewards features, and also requires repeatedly solving a Markov decision process (MDP). Our proposed method uses deep learning and ranked demonstrations to automatically learn complex features that are positively and negatively correlated with performance, and is able to generate a policy that can outperform the demonstrator via the solution to a single RL problem.\nOur work can be seen as a form of preference-based policy learning (Akrour et al., 2011) and preference-based IRL (PBIRL) (Wirth et al., 2016; Sugiyama et al., 2012) which both seek to optimize a policy based on preference rankings over demonstrations. However, existing approaches only\nconsider reward functions that are linear in hand-crafted features and have not studied extrapolation capabilities. For a more complete overview survey of preference-based reinforcement learning, see the survey by Wirth et al. (2017). Other methods (Burchfiel et al., 2016; El Asri et al., 2016) have proposed the use of quantitatively scored trajectories as opposed to qualitative pairwise preferences over demonstrations. However, none of the aforementioned methods have been applied to the types of high-dimensional deep inverse reinforcement learning tasks considered in this paper."
                },
                {
                    "heading": "2.2. Learning from observation",
                    "text": "Recently there has been a shift towards learning from observations, in which the actions taken by the demonstrator are unknown. Torabi et al. (2018a) propose a state-of-the-art model-based approach to perform behavioral cloning from observation. Sermanet et al. (2018) and Liu et al. (2018) propose methods to learn directly from a large corpus of videos containing multiple view points of the same task. Yu et al. (2018) and Goo & Niekum (2019) propose metalearning-from-observation approaches that can learn from a single demonstration, but require training on a wide variety of similar tasks. Henderson et al. (2018) and Torabi et al. (2018b) extend Generative Adversarial Imitation Learning (Ho & Ermon, 2016) to remove the need for action labels. However, inverse reinforcement learning methods based on Generative Adversarial Networks (Goodfellow et al., 2014) are notoriously difficult to train and have been shown to fail to scale to high-dimensional imitation learning tasks such as Atari (Tucker et al., 2018)."
                },
                {
                    "heading": "2.3. Learning from suboptimal demonstrations",
                    "text": "Very little work has tried to learn good policies from highly suboptimal demonstrations. Grollman & Billard (2011) propose a method that learns from failed demonstrations where a human attempts, but is unable, to perform a task; however, demonstrations must be labeled as failures and manually clustered into two sets of demonstrations: those that overshoot and those that undershoot the goal. Shiarlis et al. (2016) demonstrate that if successful and failed demonstrations are labeled and the reward function is a linear combination of known features, then maximum entropy IRL can be used to optimize a policy to match the expected feature counts of successful demonstrations while not matching the feature counts of failed demonstrations. Zheng et al. (2014) and Choi et al. (2019) propose methods that are robust to small numbers of unlabeled suboptimal demonstrations, but require a majority of expert demonstrations in order to correctly identify which demonstrations are anomalous.\nIn reinforcement learning, it is common to initialize a policy from suboptimal demonstrations and then improve this policy using the ground truth reward signal (Kober & Peters,\n2009; Taylor et al., 2011; Hester et al., 2017; Gao et al., 2018). However, it is often still difficult to perform significantly better than the demonstrator (Hester et al., 2017) and designing reward functions for reinforcement learning can be extremely difficult for non-experts and can easily lead to unintended behaviors (Ng et al., 1999; Amodei et al., 2016)."
                },
                {
                    "heading": "2.4. Reward learning for video games",
                    "text": "Most deep learning-based methods for reward learning require access to demonstrator actions and do not scale to high-dimensional tasks such as video games (e.g. Atari) (Ho & Ermon, 2016; Finn et al., 2016; Fu et al., 2017; Qureshi & Yip, 2018). Tucker et al. (2018) tested state-of-the-art IRL methods on the Atari domain and showed that they are unsuccessful, even with near-optimal demonstrations and extensive parameter tuning.\nOur work builds on the work of Christiano et al. (2017), who proposed an algorithm that learns to play Atari games via pairwise preferences over trajectories that are actively collected during policy learning. However, this approach requires obtaining thousands of labels through constant human supervision during policy learning. In contrast, our method only requires an initial set of (approximately) ranked demonstrations as input and can learn a better-than-demonstrator policy without any supervision during policy learning. Ibarz et al. (2018) combine deep Q-learning from demonstrations (DQfD) (Hester et al., 2017) and active preference learning (Christiano et al., 2017) to learn to play Atari games using both demonstrations and active queries. However, Ibarz et al. (2018) require access to the demonstrator\u2019s actions in order to optimize an action-based, large-margin loss (Hester et al., 2017) and to optimize the state-action Q-value function using (s, a, s\u2032)-tuples from the demonstrations. Additionally, the large-margin loss encourages Q-values that make the demonstrator\u2019s actions better than alternative actions, resulting in performance that is often significantly worse than the demonstrator despite using thousands of active queries during policy learning.\nAytar et al. (2018) use video demonstrations of experts to learn good policies for the Atari domains of Montezuma\u2019s Revenge, Pitfall, and Private Eye. Their method first learns a state-embedding and then selects a set of checkpoints from a demonstration. During policy learning, the agent is rewarded only when it reaches these checkpoints. This approach relies on high-performance demonstrations, which their method is unable to outperform. Furthermore, while Aytar et al. (2018) do learn a reward function purely from observations, their method is inherently different from ours in that their learned reward function is designed to only imitate the demonstrations, rather than extrapolate beyond the capabilities of the demonstrator.\nTo the best of our knowledge, our work is the first to sig-\nnificantly outperform a demonstrator without using ground truth rewards or active preference queries. Furthermore, our approach does not require demonstrator actions and is able to learn a reward function that matches the demonstrator\u2019s intention without any environmental interactions\u2014given rankings, reward learning becomes a binary classification problem and does not require access to an MDP."
                },
                {
                    "heading": "3. Problem Definition",
                    "text": "We model the environment as a Markov decision process (MDP) consisting of a set of states S, actions A, transition probabilities P , reward function r : S \u2192 R, and discount factor \u03b3 (Puterman, 2014). A policy \u03c0 is a mapping from states to probabilities over actions, \u03c0(a|s) \u2208 [0, 1]. Given a policy and an MDP, the expected discounted return of the policy is given by J(\u03c0) = E[ \u2211\u221e t=0 \u03b3 trt|\u03c0].\nIn this work we are concerned with the problem of inverse reinforcement learning from observation, where we do not have access to the reward function of the MDP nor the actions taken by the demonstrator. Given a sequence of m ranked trajectories \u03c4t for t = 1, . . . ,m, where \u03c4i \u227a \u03c4j if i < j, we wish to find a parameterized reward function r\u0302\u03b8 that approximates the true reward function r that the demonstrator is attempting to optimize. Given r\u0302\u03b8, we then seek to optimize a policy \u03c0\u0302 that can outperform the demonstrations.\nWe only assume access to a qualitative ranking over demonstrations. Thus, we only require the demonstrator to have an internal goal or intrinsic reward. The demonstrator can rank trajectories using any method, such as giving pairwise preferences over demonstrations or by rating each demonstration on a scale. Note that even if the relative scores of the demonstrations are used for ranking, it is still necessary to infer why some trajectories are better than others, which is what our proposed method does."
                },
                {
                    "heading": "4. Method",
                    "text": "We now describe Trajectory-ranked Reward EXtrapolation (T-REX), an algorithm for using ranked suboptimal demonstrations to extrapolate a user\u2019s underlying intent beyond the best demonstration. Given a sequence of m demonstrations ranked from worst to best, \u03c41, . . . , \u03c4m, T-REX has two steps: (1) reward inference and (2) policy optimization.\nGiven the ranked demonstrations, T-REX performs reward inference by approximating the reward at state s using a neural network, r\u0302\u03b8(s), such that \u2211 s\u2208\u03c4i r\u0302\u03b8(s) < \u2211 s\u2208\u03c4j r\u0302\u03b8(s) when \u03c4i \u227a \u03c4j . The parameterized reward function r\u0302\u03b8 can be trained with ranked demonstrations using the generalized loss function:\nL(\u03b8) = E\u03c4i,\u03c4j\u223c\u03a0 [ \u03be ( P ( J\u0302\u03b8(\u03c4i) < J\u0302\u03b8(\u03c4j) ) , \u03c4i \u227a \u03c4j )] , (1)\nwhere \u03a0 is a distribution over demonstrations, \u03be is a binary classification loss function, J\u0302 is a (discounted) return defined by a parameterized reward function r\u0302\u03b8, and \u227a is an indication of the preference between the demonstrated trajectories.\nWe represent the probability P as a softmax-normalized distribution and we instantiate \u03be using a cross entropy loss:\nP ( J\u0302\u03b8(\u03c4i) < J\u0302\u03b8(\u03c4j) ) \u2248\nexp \u2211 s\u2208\u03c4j r\u0302\u03b8(s)\nexp \u2211 s\u2208\u03c4i r\u0302\u03b8(s) + exp \u2211 s\u2208\u03c4j r\u0302\u03b8(s) ,\n(2)\nL(\u03b8) = \u2212 \u2211 \u03c4i\u227a\u03c4j log\nexp \u2211 s\u2208\u03c4j r\u0302\u03b8(s)\nexp \u2211 s\u2208\u03c4i r\u0302\u03b8(s) + exp \u2211 s\u2208\u03c4j r\u0302\u03b8(s) . (3)\nThis loss function trains a classifier that can predict whether one trajectory is preferable to another based on the predicted returns of each trajectory. This form of loss function follows from the classic Bradley-Terry and Luce-Shephard models of preferences (Bradley & Terry, 1952; Luce, 2012) and has been shown to be effective for training neural networks from preferences (Christiano et al., 2017; Ibarz et al., 2018).\nTo increase the number of training examples, T-REX trains on partial trajectory pairs rather than full trajectory pairs. This results in noisy preference labels that are only weakly supervised; however, using data augmentation to obtain pairwise preferences over many partial trajectories allows T-REX to learn expressive neural network reward functions from only a small number of ranked demonstrations. During training we randomly select pairs of trajectories, \u03c4i and \u03c4j . We then randomly select partial trajectories \u03c4\u0303i and \u03c4\u0303j of length L. For each partial trajectory, we take each observation and perform a forward pass through the network r\u0302\u03b8 and sum the predicted rewards to compute the cumulative return. We then use the predicted cumulative returns as the logit values in the cross-entropy loss with the label corresponding to the higher ranked demonstration.\nGiven the learned reward function r\u0302\u03b8(s), T-REX then seeks to optimize a policy \u03c0\u0302 with better-than-demonstrator performance through reinforcement learning using r\u0302\u03b8."
                },
                {
                    "heading": "5. Experiments and Results",
                    "text": ""
                },
                {
                    "heading": "5.1. Mujoco",
                    "text": "We first evaluated our proposed method on three robotic locomotion tasks using the Mujoco simulator (Todorov et al., 2012) within OpenAI Gym (Brockman et al., 2016), namely HalfCheetah, Hopper, and Ant. In all three tasks, the goal of the robot agent is to move forward as fast as possible without falling to the ground."
                },
                {
                    "heading": "5.1.1. DEMONSTRATIONS",
                    "text": "To generate demonstrations, we trained a Proximal Policy Optimization (PPO) (Schulman et al., 2017) agent with the ground-truth reward for 500 training steps (64,000 simulation steps) and checkpointed its policy after every 5 training steps. For each checkpoint, we generated a trajectory of length 1,000. This provides us with different demonstrations of varying quality which are then ranked based on the ground truth returns. To evaluate the effect of different levels of suboptimality, we divided the trajectories into different overlapping stages. We used 3 stages for HalfCheetah and Hopper. For HalfCheetah, we used the worst 9, 12, and 24 trajectories, respectively. For Hopper, we used the worst 9, 12, and 18 trajectories. For Ant, we used two stages consisting of the worst 12 and 40 trajectories. We used the PPO implementation from OpenAI Baselines (Dhariwal et al., 2017) with the given default hyperparameters."
                },
                {
                    "heading": "5.1.2. EXPERIMENTAL SETUP",
                    "text": "We trained the reward network using 5,000 random pairs of partial trajectories of length 50, with preference labels based on the trajectory rankings, not the ground-truth returns. To prevent overfitting, we represented the reward function using an ensemble of five deep neural networks, trained separately with different random pairs. Each network has 3 fully connected layers of 256 units with ReLU nonlinearities. We train the reward network using the Adam optimizer (Kingma & Ba, 2014) with a learning rate of 1e-4 and a minibatch size of 64 for 10,000 timesteps.\nTo evaluate the quality of our learned reward, we then trained a policy to maximize the inferred reward function via PPO. The outputs of each the five reward networks in our ensemble, r\u0302(s), are normalized by their standard deviation to compensate for any scale differences amongst the models. The reinforcement learning agent receives the average of the ensemble as the reward, plus the control penalty used in OpenAI Gym (Brockman et al., 2016). This control penalty represents a standard safety prior over reward functions for robotics tasks, namely to minimize joint torques. We found that optimizing a policy based solely on this control penalty does not lead to forward locomotion, thus learning a reward function from demonstrations is still necessary."
                },
                {
                    "heading": "5.1.3. RESULTS",
                    "text": "Learned Policy Performance We measured the performance of the policy learned by T-REX by measuring the forward distance traveled. We also compared against Behavior Cloning from Observations (BCO) (Torabi et al., 2018a), a state-of-the-art learning-from-observation method, and Generative Adversarial Imitation Learning (GAIL) (Ho & Ermon, 2016), a state-of-the-art inverse reinforcement learning algorithm. BCO trains a policy via supervised learning,\nand has been shown to be competitive with state-of-the-art IRL (Ho & Ermon, 2016) on MuJoCo tasks without requiring action labels, making it one of the strongest baselines when learning from observations. We trained BCO using only the best demonstration among the available suboptimal demonstrations. We trained GAIL with all of the demonstrations. GAIL uses demonstrator actions, while T-REX and BCO do not.\nWe compared against three different levels of suboptimality (Stage 1, 2, and 3), corresponding to increasingly better demonstrations. The results are shown in Figure 2 (see the appendix for full details). The policies learned by T-REX perform significantly better than the provided suboptimal trajectories in all the stages of HalfCheetah and Hopper. This provides evidence that T-REX can discover reward functions that extrapolate beyond the performance of the demonstrator. T-REX also outperforms BCO and GAIL on all tasks and stages except for Stage 2 for Hopper and Ant. BCO and GAIL usually fail to perform better than the average demonstration performance because they explicitly seek to imitate the demonstrator rather than infer the demonstrator\u2019s intention.\nReward Extrapolation We next investigated the ability of T-REX to accurately extrapolate beyond the demonstrator. To do so, we compared ground-truth return and T-REXinferred return across trajectories from a range of performance qualities, including trajectories much better than the best demonstration given to T-REX. The extrapolation of the reward function learned by T-REX is shown in Figure 3. The plots in Figure 3 give insight into the performance of T-REX. When T-REX learns a reward function that has a strong positive correlation with the ground-truth reward function, then it is able to surpass the performance of the\nsuboptimal demonstrations. However, in Ant the correlation is not as strong, resulting in worse-than-demonstrator performance in Stage 2."
                },
                {
                    "heading": "5.2. Atari",
                    "text": ""
                },
                {
                    "heading": "5.2.1. DEMONSTRATIONS",
                    "text": "We next evaluated T-REX on eight Atari games shown in Table 1. To obtain a variety of suboptimal demonstrations, we generated 12 full-episode trajectories using PPO policies checkpointed every 50 training updates for all games except for Seaquest and Enduro. For Seaquest, we used every 5th training update due to the ability of PPO to quickly find a good policy. For Enduro, we used every 50th training update starting from step 3,100 since PPO obtained 0 return until after 3,000 steps. We used the OpenAI Baselines implementation of PPO with the default hyperparameters."
                },
                {
                    "heading": "5.2.2. EXPERIMENTAL SETUP",
                    "text": "We used an architecture for reward learning similar to the one proposed in (Ibarz et al., 2018), with four convolutional layers with sizes 7x7, 5x5, 3x3, and 3x3, with strides 3, 2, 1, and 1. Each convolutional layer used 16 filters and LeakyReLU non-linearities. We then used a fully connected layer with 64 hidden units and a single scalar output. We fed in stacks of 4 frames with pixel values normalized between 0 and 1 and masked the game score and number of lives.\nFor all games except Enduro, we subsampled 6,000 trajectory pairs between 50 and 100 observations long. We optimized the reward functions using Adam with a learning rate of 5e-5 for 30,000 steps. Given two full trajectories \u03c4i and \u03c4j such that \u03c4i \u227a \u03c4j , we first randomly sample a subtrajectory from \u03c4i. Let ti be the starting timestep for this subtrajectory. We then sample an equal length subtrajectory from \u03c4j such that ti \u2264 tj , where tj is the starting time step of the subtrajectory from \u03c4j . We found that this resulted in\nbetter performance than comparing randomly chosen subtrajectories, likely due to the fact that (1) it eliminates pairings that compare a later part of a worse trajectory with an earlier part of a better trajectory and (2) it encourages reward functions that are monotonically increasing as progress is made in the game. For Enduro, training on short partial trajectories was not sufficient to score any points and instead we used 2,000 pairs of down-sampled full trajectories (see appendix for details).\nWe optimized a policy by training a PPO agent on the learned reward function. To reduce reward scaling issues, we normalized predicted rewards by feeding the output of r\u0302\u03b8(s) through a sigmoid function before passing it to PPO. We trained PPO on the learned reward function for 50 million frames to obtain our final policy. We also compare against Behavioral Cloning from Observation (BCO) (Torabi et al., 2018a) and the state-of-the-art Generative Adversarial Imitation Learning (GAIL) (Ho & Ermon, 2016). Note that we give action labels to GAIL, but not to BCO or T-REX. We tuned the hyperparameters for GAIL to maximize performance when using expert demonstrations on Breakout and Pong. We gave the same demonstrations to both BCO and T-REX; however, we found that GAIL was very sensitive to poor demonstrations so we trained GAIL on 10 demonstrations using the policy checkpoint that generated the best demonstration given to T-REX."
                },
                {
                    "heading": "5.2.3. RESULTS",
                    "text": "Learned Policy Performance The average performance of T-REX under the ground-truth reward function and the best and average performance of the demonstrator are shown in Table 1. Table 1 shows that T-REX outperformed both BCO and GAIL in 7 out of 8 games. T-REX also outperformed the best demonstration in 7 out of 8 games. On four games (Beam Rider, Breakout, Enduro, and Q*bert) T-REX achieved score that is more than double the score of the best\ndemonstration. In comparison, BCO performed worse than the average performance of the demonstrator in all games, and GAIL only performed better than the average demonstration on Space Invaders. Despite using better training data, GAIL was unable to learn good policies on any of the Atari tasks. These results are consistent with those of Tucker et al. (2018) that show that current GAN-based IRL methods do not perform well on Atari. In the appendix, we compare our results against prior work (Ibarz et al., 2018) that uses demonstrations plus active feedback during policy training to learn control policies for the Atari domain.\nReward Extrapolation We also examined the extrapolation of the reward function learned using T-REX. Results are shown in Figure 4. We observed accurate extrapolation for Beam Rider, Breakout, Enduro, Seaquest, and Space Invaders\u2014five games where T-REX significantly outperform the demonstrator. The learned rewards for Pong, Q*bert, and Hero show less correlation. On Pong, T-REX overfits to the suboptimal demonstrations and ends up preferring longer games which do not result in a significant win or loss. T-REX is unable to score any points on Hero, likely due to poor extrapolation and the higher complexity of the game. Surprisingly, the learned reward function for Q*bert shows poor extrapolation, yet T-REX is able to outperform the demonstrator. We analyzed the resulting policy for Q*bert and found that PPO learns a repeatable way to score points by inducing Coily to jump off the edge. This behavior was not seen in the demonstrations. In the appendix, we plot the maximum and minimum predicted observations from the trajectories used to create Figure 4 along with attention maps for the learned reward functions."
                },
                {
                    "heading": "5.2.4. HUMAN DEMONSTRATIONS",
                    "text": "The above results used synthetic demonstrations generated from an RL agent. We also tested T-REX when given ground-truth rankings over human demonstrations. We used\nnovice human demonstrations from the Atari Grand Challenge Dataset (Kurin et al., 2017) for five Atari tasks. TREX was able to significantly outperform the best human demonstration in Q*bert, Space Invaders, and Video Pinball, but was unable to outperform the human in Montezuma\u2019s Revenge and Ms Pacman (see the appendix for details)."
                },
                {
                    "heading": "5.3. Robustness to Noisy Rankings",
                    "text": "All experiments described thus far have had access to ground-truth rankings. To explore the effects of noisy rankings we first examined the stage 1 Hopper task. We synthetically generated ranking noise by starting with a list of trajectories sorted by ground-truth returns and randomly swapping adjacent trajectories. By varying the number of swaps, we were able to generate different noise levels. Given n trajectories in a ranked list provides ( n 2 ) pairwise preferences over trajectories. The noise level is measured as a total order correctness: the fraction of trajectory pairs whose pairwise ranking after random swapping matches the original ground-truth pairwise preferences. The results of this experiment, averaged over 9 runs per noise level, are shown in Figure 5. We found that T-REX is relatively robust to noise of up to around 15% pairwise errors.\nTo examine the effect of noisy human rankings, we used the synthetic PPO demonstrations that were used in the previous Atari experiments and used Amazon Mechanical Turk to collect human rankings. We presented videos of the demonstrations in pairs along with a brief text description of the goal of the game and asked workers to select which demonstration had better performance, with an option for selecting \u201cNot Sure\u201d. We collected six labels per demonstration pair and used the most-common label as the label for training the reward function. We removed from the training data any pairings where there was a tie for the most-common label or where \u201cNot Sure\u201d was the most common label. We found that despite this preprocessing step, human labels added a\nsignificant amount of noise and resulted in pair-wise rankings with accuracy between 63% and 88% when compared to ground-truth labels. However, despite significant ranking noise, T-REX outperformed the demonstrator on 5 of the 8 Atari games (see the appendix for full details)."
                },
                {
                    "heading": "5.3.1. LEARNING FROM TIME-BASED RANKINGS",
                    "text": "Finally, we tested whether T-REX has the potential to work without explicit rankings. We took the same demonstrations used for the Mujoco tasks, and rather than sorting them based on ground-truth rankings, we used the order in which they were generated by PPO to produce a ranked list of trajectories, ordered by timestamp from earliest to latest.\nThis provides ranked demonstrations without any need for demonstrator labels, and enables us to test whether simply observing an agent learn over time allows us to extrapolate intention by assuming that later trajectories are preferable to trajectories produced earlier in learning. The results for Hopper are shown in Figure 5 and other task results are shown in the appendix. We found that T-REX is able to infer a meaningful reward function even when noisy, time-based rankings are provided. All the trained policies produced comparable results on most stages to the groundtruth rankings, and those policies outperform BCO and GAIL on all tasks and stages except for Ant Stage 2."
                },
                {
                    "heading": "6. Conclusion",
                    "text": "In this paper, we introduced T-REX, a reward learning technique for high-dimensional tasks that can learn to extrapolate intent from suboptimal ranked demonstrations. To the best of our knowledge, this is the first IRL algorithm that is able to significantly outperform the demonstrator without additional external knowledge (e.g. signs of feature contributions to reward) and that scales to high-dimensional Atari games. When combined with deep reinforcement learning, we showed that this approach achieves better-thandemonstrator performance as well as outperforming stateof-the-art behavioral cloning and IRL methods. We also demonstrated that T-REX is robust to modest amounts of ranking noise, and can learn from automatically generated labels, obtained by watching a learner noisily improve at a task over time."
                },
                {
                    "heading": "Acknowledgments",
                    "text": "This work has taken place in the Personal AutonomousRobotics Lab (PeARL) at The University of Texas at Austin. PeARL research is supported in part by the NSF (IIS1724157, IIS-1638107, IIS-1617639, IIS-1749204) and ONR(N00014-18-2243)."
                },
                {
                    "heading": "A. Code and Videos",
                    "text": "Code as well as supplemental videos are available at https://github.com/hiwonjoon/ ICML2019-TREX."
                },
                {
                    "heading": "B. T-REX Results on the MuJoCo Domain",
                    "text": "B.1. Policy performance\nTable 1 shows the full results for the MuJoCo experiments. The T-REX (time-ordered) row shows the resulting performance of T-REX when demonstrations come from observing a learning agent and are ranked based on timestamps rather than using explicit preference rankings.\nB.2. Policy visualization\nWe visualized the T-REX-learned policy for HalfCheetah in Figure 1. Visualizing the demonstrations from different stages shows the specific way the policy evolves over time; an agent learns to crawl first and then begins to attempt to walk in an upright position. The T-REX policy learned from the highly suboptimal Stage 1 demonstrations results in a similar-style crawling gait; however, T-REX captures some of the intent behind the demonstration and is able to optimize a gait that resembles the demonstrator but with increased speed, resulting in a better-than-demonstrator policy. Similarly, given demonstrations from Stage 2, which are still highly suboptimal, T-REX learns a policy that resembles the gait of the best demonstration, but is able to optimize and partially stabilize this gait. Finally, given demonstrations from Stage 3, which are still suboptimal, T-REX is able to learn a near-optimal gait."
                },
                {
                    "heading": "C. Behavioral Cloning from Observation",
                    "text": "To build the inverse transition models used by BCO (Torabi et al., 2018a) we used 20,000 steps of a random policy to collect transitions with labeled states. We used the Adam optimizer with learning rate 0.0001 and L2 regularization of 0.0001. We used the DQN architecture (Mnih et al., 2015) for the classification network, using the same architecture to predict actions given state transitions as well as predict actions given states. When predicting P (a|st, st+1), we concatenate the state vectors obtaining an 8x84x84 input consisting of two 4x84x84 frames representing st and st+1.\nWe give both T-REX and BCO the full set of demonstrations. We tried to improve the performance of BCO by running behavioral cloning only on the bestX% of the demonstrations, but were unable to find a parameter setting that performed better than X = 100, likely due to a lack of training data when using very few demonstrations."
                },
                {
                    "heading": "D. Atari reward learning details",
                    "text": "We used the OpenAI Baselines implementation of PPO with default hyperparameters. We ran all of our experiments on an NVIDIA TITAN V GPU. We used 9 parallel workers when running PPO.\nWhen learning and predicting rewards, we mask the score and number of lives left for all games. We did this to avoid having the network learn to only look at the score and recognize, say, the number of significant digits, etc. We additionally masked the sector number and number of enemy ships left on Beam Rider. We masked the bottom half of the dashboard for Enduro to mask the position of the car in the race. We masked the number of divers found and the oxygen meter for Seaquest. We masked the power level and inventory for Hero.\nTo train the reward network for Enduro, we randomly downsampled full trajectories. To create a training set we repeatedly randomly select two full demonstrations, then randomly cropped between 0 and 5 of the initial frames from each trajectory and then downsampled both trajectories by only keeping every xth frame where x is randomly chosen between 3 and 6. We selected 2,000 randomly downsampled demonstrations and trained the reward network for 10,000 steps of Adam with a learning rate of 5e-5."
                },
                {
                    "heading": "E. Comparison to active reward learning",
                    "text": "In this section, we examine the ability of prior work on active preference learning to exceed the performance of the demonstrator. In Table 2, we denote the results that surpass the best demonstration with an asterisk (*). DQfD+A only surpasses the demonstrator in 3 out of 9 games tested, even with thousands of active queries. Note that DQfD+A extends the original DQfD algorithm (Hester et al., 2017), which uses demonstrations combined with RL on groundtruth rewards, yet is only able to surpass the best demonstration in 14 out of 41 Atari games. In contrast, we are able to leverage only 12 ranked demos to achieve betterthan-demonstrator performance on 7 out of 8 games tested, without requiring access to true rewards or access to thousands of active queries from an oracle.\nIbarz et al. (2018) combine Deep Q-learning from demonstrations and active preference queries (DQfD+A). DQfD+A uses demonstrations consisting of (st, at, st+1)-tuples to initialize a policy using DQfD (Hester et al., 2017). The algorithm then uses the active preference learning algorithm of Christiano et al. (2017) to refine the inferred reward function and initial policy learned from demonstrations. The first two columns of Table 2 compare the demonstration quality given to DQfD+A and T-REX. While our results make use of more demonstrations (12 for T-REX versus 4\u20137 for DQfD+A), our demonstrations are typically orders of magnitude worse than the demonstrations used by DQfD+A: on average the demonstrations given to DQfD+A are 38\ntimes better than those used by T-REX. However, despite this large gap in the performance of the demonstrations, TREX surpasses the performance of DQfD+A on Q*Bert, and Seaquest. We achieve these results using 12 ranked demonstrations. This requires only 66 comparisons (n \u00b7 (n\u2212 1)/2) by the demonstrator. In comparison, the DQfD+A results used 3,400 preference labels obtained during policy training using ground-truth rewards."
                },
                {
                    "heading": "F. Human Demonstrations and Rankings",
                    "text": "F.1. Human demonstrations\nWe used the Atari Grand Challenge data set (Kurin et al., 2017) to collect actual human demonstrations for five Atari games. We used the ground truth returns in the Atari Grand Challenge data set to rank demonstrations. To generate demonstrations we removed duplicate demonstrations (human demonstrations that achieved the same score). We then sorted the remaining demonstrations based on ground truth return and selected 12 of these demonstrations to form our training set. We ran T-REX using the same hyperparameters as described above.\nThe resulting performance of T-REX is shown in Table 3. T-REX is able to outperform the best human demonstration on Q*bert, Space Invaders, and Video Pinball; however, it is not able to learn a good control policy for Montezuma\u2019s Revenge or Ms Pacman. These games require maze navigation and balancing different objectives, such as collecting objects and avoiding enemies. This matches our results in the main text that show that T-REX is unable to learn a policy for playing Hero, a similar maze navigation task with multiple objectives such as blowing up walls, rescuing people, and destroying enemies. Extending T-REX to work in these types of settings is an interesting area of future work.\nF.2. Human rankings\nTo measure the effects of human ranking noise, we took the same 12 PPO demonstrations described above in the main text and had humans rank the demonstrations. We used Amazon Mechanical Turk and showed the workers two sideby-side demonstrations and asked them to classify whether video A or video B had better performance or whether they were unsure.\nWe took all 132 possible sequences of two videos across the 12 demonstrations and collected 6 labels for each pair of demonstrations. Because the workers are not actually giving the demonstrations and because some workers may exploit the task by simply selecting choices at random, we expect these labels to be a worst-case lower bound on the accuracy. To ameliorate the noise in the labels we take all 6 labels per pair and use the majority vote as the human label. If there is no majority or if the majority selects the \u201cNot Sure\u201d label, then we do not include this pair in our training data for T-REX.\nThe resulting accuracy and number of labels that had a majority preference are shown in Table 4. We ran T-REX using the same hyperparameters described in the main text. We ran PPO with 3 different seeds and report the performance of the best final policy averaged over 30 trials. We found that surprisingly, T-REX is able to optimize good policies for many of the games, despite noisy labels. However, we\ndid find cases such as Enduro, where the labels were too noisy to allow successful policy learning."
                },
                {
                    "heading": "G. Atari Reward Visualizations",
                    "text": "We generated attention maps for the learned rewards for the Atari domains. We use the method proposed by Greydanus et al. (2018), which takes a stack of 4 frames and passes a 3x3 mask over each of the frames with a stride of 1. The mask is set to be the default background color for each game. For each masked 3x3 region, we compute the absolute difference in predicted reward when the 3x3 region is not masked and when it is masked. This allows us to measure the influence of different regions of the image on the predicted reward. The sum total of absolute changes in reward for each pixel is used to generate an attention heatmap. We used the trajectories shown in the extrapolation plots in Figure 4 of the main text and performed a search using the learned reward function to find the observations with minimum and maximum predicted reward. We show the minimum and maximum observations (stacks of four frames) along with the attention heatmaps across all four stacked frames for the learned reward functions in figures 2\u20139. The reward function visualizations suggest that our networks are learning relevant features of the reward function."
                }
            ],
            "year": 2019,
            "references": [
                {
                    "title": "Apprenticeship learning via inverse reinforcement learning",
                    "authors": [
                        "P. Abbeel",
                        "A.Y. Ng"
                    ],
                    "venue": "In Proceedings of the 21st international conference on Machine learning,",
                    "year": 2004
                },
                {
                    "title": "Preference-based policy learning",
                    "authors": [
                        "R. Akrour",
                        "M. Schoenauer",
                        "M. Sebag"
                    ],
                    "venue": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,",
                    "year": 2011
                },
                {
                    "title": "Concrete problems in ai safety",
                    "authors": [
                        "D. Amodei",
                        "C. Olah",
                        "J. Steinhardt",
                        "P. Christiano",
                        "J. Schulman",
                        "D. Man\u00e9"
                    ],
                    "venue": "arXiv preprint arXiv:1606.06565,",
                    "year": 2016
                },
                {
                    "title": "A survey of robot learning from demonstration",
                    "authors": [
                        "B.D. Argall",
                        "S. Chernova",
                        "M. Veloso",
                        "B. Browning"
                    ],
                    "venue": "Robotics and autonomous systems,",
                    "year": 2009
                },
                {
                    "title": "A survey of inverse reinforcement learning: Challenges, methods and progress",
                    "authors": [
                        "S. Arora",
                        "P. Doshi"
                    ],
                    "venue": "arXiv preprint arXiv:1806.06877,",
                    "year": 2018
                },
                {
                    "title": "Playing hard exploration games by watching youtube",
                    "authors": [
                        "Y. Aytar",
                        "T. Pfaff",
                        "D. Budden",
                        "T.L. Paine",
                        "Z. Wang",
                        "N. de Freitas"
                    ],
                    "venue": "arXiv preprint arXiv:1805.11592,",
                    "year": 2018
                },
                {
                    "title": "Relative entropy inverse reinforcement learning",
                    "authors": [
                        "A. Boularias",
                        "J. Kober",
                        "J. Peters"
                    ],
                    "venue": "In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,",
                    "year": 2011
                },
                {
                    "title": "Rank analysis of incomplete block designs: I. the method of paired comparisons",
                    "authors": [
                        "R.A. Bradley",
                        "M.E. Terry"
                    ],
                    "year": 1952
                },
                {
                    "title": "Distance minimization for reward learning from scored trajectories",
                    "authors": [
                        "B. Burchfiel",
                        "C. Tomasi",
                        "R. Parr"
                    ],
                    "venue": "In AAAI,",
                    "year": 2016
                },
                {
                    "title": "Robust learning from demonstrations with mixed qualities using leveraged gaussian processes",
                    "authors": [
                        "S. Choi",
                        "K. Lee",
                        "S. Oh"
                    ],
                    "venue": "IEEE Transactions on Robotics,",
                    "year": 2019
                },
                {
                    "title": "Deep reinforcement learning from human preferences",
                    "authors": [
                        "P.F. Christiano",
                        "J. Leike",
                        "T. Brown",
                        "M. Martic",
                        "S. Legg",
                        "D. Amodei"
                    ],
                    "venue": "In Advances in Neural Information Processing Systems,",
                    "year": 2017
                },
                {
                    "title": "Score-based inverse reinforcement learning",
                    "authors": [
                        "L. El Asri",
                        "B. Piot",
                        "M. Geist",
                        "R. Laroche",
                        "O. Pietquin"
                    ],
                    "venue": "In Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems,",
                    "year": 2016
                },
                {
                    "title": "Guided cost learning: Deep inverse optimal control via policy optimization",
                    "authors": [
                        "C. Finn",
                        "S. Levine",
                        "P. Abbeel"
                    ],
                    "venue": "In International Conference on Machine Learning,",
                    "year": 2016
                },
                {
                    "title": "Learning robust rewards with adversarial inverse reinforcement learning",
                    "authors": [
                        "J. Fu",
                        "K. Luo",
                        "S. Levine"
                    ],
                    "venue": "arXiv preprint arXiv:1710.11248,",
                    "year": 2017
                },
                {
                    "title": "A survey of inverse reinforcement learning techniques",
                    "authors": [
                        "Y. Gao",
                        "J. Peters",
                        "A. Tsourdos",
                        "S. Zhifei",
                        "E. Meng Joo"
                    ],
                    "venue": "International Journal of Intelligent Computing and Cybernetics,",
                    "year": 2012
                },
                {
                    "title": "Reinforcement learning from imperfect demonstrations",
                    "authors": [
                        "Y. Gao",
                        "J. Lin",
                        "F. Yu",
                        "S. Levine",
                        "T Darrell"
                    ],
                    "venue": "arXiv preprint arXiv:1802.05313,",
                    "year": 2018
                },
                {
                    "title": "One-shot learning of multi-step tasks from observation via activity localization in auxiliary video",
                    "authors": [
                        "W. Goo",
                        "S. Niekum"
                    ],
                    "venue": "IEEE International Conference on Robotics and Automation (ICRA),",
                    "year": 2019
                },
                {
                    "title": "Generative adversarial nets",
                    "authors": [
                        "I. Goodfellow",
                        "J. Pouget-Abadie",
                        "M. Mirza",
                        "B. Xu",
                        "D. WardeFarley",
                        "S. Ozair",
                        "A. Courville",
                        "Y. Bengio"
                    ],
                    "venue": "In Advances in neural information processing systems,",
                    "year": 2014
                },
                {
                    "title": "Visualizing and understanding atari agents",
                    "authors": [
                        "S. Greydanus",
                        "A. Koul",
                        "J. Dodge",
                        "A. Fern"
                    ],
                    "venue": "In International Conference on Machine Learning,",
                    "year": 2018
                },
                {
                    "title": "Donut as i do: Learning from failed demonstrations",
                    "authors": [
                        "D.H. Grollman",
                        "A. Billard"
                    ],
                    "venue": "In Robotics and Automation (ICRA),",
                    "year": 2011
                },
                {
                    "title": "Optiongan: Learning joint reward-policy options using generative adversarial inverse reinforcement learning",
                    "authors": [
                        "P. Henderson",
                        "Chang",
                        "W.-D",
                        "Bacon",
                        "P.-L",
                        "D. Meger",
                        "J. Pineau",
                        "D. Precup"
                    ],
                    "venue": "In Thirty-Second AAAI Conference on Artificial Intelligence,",
                    "year": 2018
                },
                {
                    "title": "Deep q-learning from demonstrations",
                    "authors": [
                        "T. Hester",
                        "M. Vecerik",
                        "O. Pietquin",
                        "M. Lanctot",
                        "T. Schaul",
                        "B. Piot",
                        "D. Horgan",
                        "J. Quan",
                        "A. Sendonaris",
                        "G Dulac-Arnold"
                    ],
                    "venue": "arXiv preprint arXiv:1704.03732,",
                    "year": 2017
                },
                {
                    "title": "Generative adversarial imitation learning",
                    "authors": [
                        "J. Ho",
                        "S. Ermon"
                    ],
                    "venue": "In Advances in Neural Information Processing Systems,",
                    "year": 2016
                },
                {
                    "title": "Reward learning from human preferences and demonstrations in atari",
                    "authors": [
                        "B. Ibarz",
                        "J. Leike",
                        "T. Pohlen",
                        "G. Irving",
                        "S. Legg",
                        "D. Amodei"
                    ],
                    "venue": "In Advances in Neural Information Processing Systems,",
                    "year": 2018
                },
                {
                    "title": "Adam: A method for stochastic optimization",
                    "authors": [
                        "D.P. Kingma",
                        "J. Ba"
                    ],
                    "venue": "arXiv preprint arXiv:1412.6980,",
                    "year": 2014
                },
                {
                    "title": "Policy search for motor primitives in robotics",
                    "authors": [
                        "J. Kober",
                        "J.R. Peters"
                    ],
                    "venue": "In Advances in neural information processing systems,",
                    "year": 2009
                },
                {
                    "title": "Imitation from observation: Learning to imitate behaviors from raw video via context translation",
                    "authors": [
                        "Y. Liu",
                        "A. Gupta",
                        "P. Abbeel",
                        "S. Levine"
                    ],
                    "venue": "IEEE International Conference on Robotics and Automation (ICRA),",
                    "year": 2018
                },
                {
                    "title": "Individual choice behavior: A theoretical analysis",
                    "authors": [
                        "R.D. Luce"
                    ],
                    "venue": "Courier Corporation,",
                    "year": 2012
                },
                {
                    "title": "Human-level control through deep reinforcement learning",
                    "authors": [
                        "V. Mnih",
                        "K. Kavukcuoglu",
                        "D. Silver",
                        "A.A. Rusu",
                        "J. Veness",
                        "M.G. Bellemare",
                        "A. Graves",
                        "M. Riedmiller",
                        "A.K. Fidjeland",
                        "G Ostrovski"
                    ],
                    "year": 2015
                },
                {
                    "title": "Policy invariance under reward transformations: Theory and application to reward shaping",
                    "authors": [
                        "A.Y. Ng",
                        "D. Harada",
                        "S. Russell"
                    ],
                    "venue": "In ICML,",
                    "year": 1999
                },
                {
                    "title": "Efficient training of artificial neural networks for autonomous navigation",
                    "authors": [
                        "D.A. Pomerleau"
                    ],
                    "venue": "Neural Computation,",
                    "year": 1991
                },
                {
                    "title": "Markov decision processes: discrete stochastic dynamic programming",
                    "authors": [
                        "M.L. Puterman"
                    ],
                    "year": 2014
                },
                {
                    "title": "Adversarial imitation via variational inverse reinforcement learning",
                    "authors": [
                        "A.H. Qureshi",
                        "M.C. Yip"
                    ],
                    "venue": "arXiv preprint arXiv:1809.06404,",
                    "year": 2018
                },
                {
                    "title": "Bayesian inverse reinforcement learning",
                    "authors": [
                        "D. Ramachandran",
                        "E. Amir"
                    ],
                    "venue": "In Proceedings of the 20th International Joint Conference on Artifical intelligence,",
                    "year": 2007
                },
                {
                    "title": "A reduction of imitation learning and structured prediction to no-regret online learning",
                    "authors": [
                        "S. Ross",
                        "G. Gordon",
                        "D. Bagnell"
                    ],
                    "venue": "In Proceedings of the fourteenth international conference on artificial intelligence and statistics,",
                    "year": 2011
                },
                {
                    "title": "Proximal policy optimization algorithms",
                    "authors": [
                        "J. Schulman",
                        "F. Wolski",
                        "P. Dhariwal",
                        "A. Radford",
                        "O. Klimov"
                    ],
                    "venue": "arXiv preprint arXiv:1707.06347,",
                    "year": 2017
                },
                {
                    "title": "Time-contrastive networks: Selfsupervised learning from video",
                    "authors": [
                        "P. Sermanet",
                        "C. Lynch",
                        "Y. Chebotar",
                        "J. Hsu",
                        "E. Jang",
                        "S. Schaal",
                        "S. Levine",
                        "G. Brain"
                    ],
                    "venue": "IEEE International Conference on Robotics and Automation (ICRA),",
                    "year": 2018
                },
                {
                    "title": "Inverse reinforcement learning from failure",
                    "authors": [
                        "K. Shiarlis",
                        "J. Messias",
                        "S. Whiteson"
                    ],
                    "venue": "In Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems, pp. 1060\u20131068. International Foundation for Autonomous Agents and Multiagent Systems,",
                    "year": 2016
                },
                {
                    "title": "Preference-learning based inverse reinforcement learning for dialog control",
                    "authors": [
                        "H. Sugiyama",
                        "T. Meguro",
                        "Y. Minami"
                    ],
                    "venue": "In INTERSPEECH, pp",
                    "year": 2012
                },
                {
                    "title": "Introduction to reinforcement learning, volume 135",
                    "authors": [
                        "R.S. Sutton",
                        "A.G. Barto"
                    ],
                    "venue": "MIT press Cambridge,",
                    "year": 1998
                },
                {
                    "title": "A game-theoretic approach to apprenticeship learning",
                    "authors": [
                        "U. Syed",
                        "R.E. Schapire"
                    ],
                    "venue": "In Advances in neural information processing systems,",
                    "year": 2008
                },
                {
                    "title": "Mujoco: A physics engine for model-based control",
                    "authors": [
                        "E. Todorov",
                        "T. Erez",
                        "Y. Tassa"
                    ],
                    "venue": "In Intelligent Robots and Systems (IROS),",
                    "year": 2012
                },
                {
                    "title": "Behavioral cloning from observation",
                    "authors": [
                        "F. Torabi",
                        "G. Warnell",
                        "P. Stone"
                    ],
                    "venue": "In Proceedings of the 27th International Joint Conference on Artificial Intelligence (IJCAI),",
                    "year": 2018
                },
                {
                    "title": "Generative adversarial imitation from observation",
                    "authors": [
                        "F. Torabi",
                        "G. Warnell",
                        "P. Stone"
                    ],
                    "venue": "arXiv preprint arXiv:1807.06158,",
                    "year": 2018
                },
                {
                    "title": "Inverse reinforcement learning for video games",
                    "authors": [
                        "A. Tucker",
                        "A. Gleave",
                        "S. Russell"
                    ],
                    "venue": "In Proceedings of the Workshop on Deep Reinforcement Learning at NeurIPS,",
                    "year": 2018
                },
                {
                    "title": "Model-free preferencebased reinforcement learning",
                    "authors": [
                        "C. Wirth",
                        "J. F\u00fcrnkranz",
                        "G. Neumann"
                    ],
                    "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,",
                    "year": 2016
                },
                {
                    "title": "A survey of preference-based reinforcement learning methods",
                    "authors": [
                        "C. Wirth",
                        "R. Akrour",
                        "G. Neumann",
                        "J. F\u00fcrnkranz"
                    ],
                    "venue": "Journal of Machine Learning Research,",
                    "year": 2017
                },
                {
                    "title": "Maximum entropy deep inverse reinforcement learning",
                    "authors": [
                        "M. Wulfmeier",
                        "P. Ondruska",
                        "I. Posner"
                    ],
                    "venue": "arXiv preprint arXiv:1507.04888,",
                    "year": 2015
                },
                {
                    "title": "One-shot imitation from observing humans via domain-adaptive meta-learning",
                    "authors": [
                        "T. Yu",
                        "C. Finn",
                        "A. Xie",
                        "S. Dasari",
                        "T. Zhang",
                        "P. Abbeel",
                        "S. Levine"
                    ],
                    "venue": "arXiv preprint arXiv:1802.01557,",
                    "year": 2018
                },
                {
                    "title": "Robust bayesian inverse reinforcement learning with sparse behavior noise",
                    "authors": [
                        "J. Zheng",
                        "S. Liu",
                        "L.M. Ni"
                    ],
                    "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
                    "year": 2014
                },
                {
                    "title": "Maximum entropy inverse reinforcement learning",
                    "authors": [
                        "B.D. Ziebart",
                        "A.L. Maas",
                        "J.A. Bagnell",
                        "A.K. Dey"
                    ],
                    "venue": "In Proceedings of the 23rd AAAI Conference on Artificial Intelligence,",
                    "year": 2008
                },
                {
                    "title": "2018) combine Deep Q-learning from demonstrations and active preference queries (DQfD+A). DQfD+A uses demonstrations consisting of (st, at, st+1)-tuples to initialize a policy using DQfD (Hester et al., 2017)",
                    "authors": [
                        "Ibarz"
                    ],
                    "year": 2017
                },
                {
                    "title": "Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations Table 2. Best demonstrations and average performance of learned policies for T-REX (ours) and DQfD with active preference learning (DQfD+A",
                    "authors": [
                        "Ibarz"
                    ],
                    "year": 2018
                }
            ],
            "id": "SP:4f984188c6891666ea11841872ce1e26cc5912ba",
            "authors": [
                {
                    "name": "Daniel S. Brown",
                    "affiliations": []
                },
                {
                    "name": "Wonjoon Goo",
                    "affiliations": []
                },
                {
                    "name": "Prabhat Nagarajan",
                    "affiliations": []
                },
                {
                    "name": "Scott Niekum",
                    "affiliations": []
                }
            ],
            "abstractText": "A critical flaw of existing inverse reinforcement learning (IRL) methods is their inability to significantly outperform the demonstrator. This is because IRL typically seeks a reward function that makes the demonstrator appear near-optimal, rather than inferring the underlying intentions of the demonstrator that may have been poorly executed in practice. In this paper, we introduce a novel reward-learning-from-observation algorithm, Trajectory-ranked Reward EXtrapolation (T-REX), that extrapolates beyond a set of (approximately) ranked demonstrations in order to infer high-quality reward functions from a set of potentially poor demonstrations. When combined with deep reinforcement learning, T-REX outperforms state-of-the-art imitation learning and IRL methods on multiple Atari and MuJoCo benchmark tasks and achieves performance that is often more than twice the performance of the best demonstration. We also demonstrate that T-REX is robust to ranking noise and can accurately extrapolate intention by simply watching a learner noisily improve at a task over time.",
            "title": "Extrapolating Beyond Suboptimal Demonstrations via  Inverse Reinforcement Learning from Observations"
        },
        "Y": {
            "blog_id": "meta-reinforcement-learning-of-structured-exploration-strategies",
            "summary": [
                "The paper looks at the problem of learning structured exploration policies for training RL agents.",
                "Structured Exploration  Consider a stochastic, parameterized policy \u03c0\u03b8(a|s) where \u03b8 represents the policy-parameters.",
                "To encourage exploration, noise can be added to the policy at each time step t. But the noise added in such a manner does not have any notion of temporal coherence.",
                "Another issue is that if the policy is represented by a simple distribution (say parameterized unimodal Gaussian), it can not model complex time-correlated stochastic processes.",
                "The paper proposes to condition the policy on per-episode random variables (z) which are sampled from a learned latent distribution.",
                "Consider a distibution over the tasks p(T).",
                "At the start of any episode of the ith task, a latent variable zi is sampled from the distribution N(\u03bci, \u03c3i) where \u03bci and \u03c3i are the learned parameters of the distribution and are referred to as the variation parameters.",
                "Once sampled, the same zi is used to condition the policy for as long as the current episode lasts and the action is sampled from then distribution \u03c0\u03b8(a|s, zi).",
                "The intuition is that the latent variable zi would encode the notion of a task or goal that does not change arbitrarily during the episode.",
                "Model Agnostic Exploration with Structured Noise  The paper focuses on the setting where the structured exploration policies are to be learned while leveraging the learning from prior tasks.",
                "A meta-learning approach, called as model agnostic exploration with structured noise (MAESN) is proposed to learn a good initialization of the policy-parameters and to learn a latent space (for sampling the z from) that can inject structured stochasticity in the policy.",
                "General meta-RL approaches have two limitations when it comes to \u201clearning to explore\u201d:  Casting meta-RL problems as RL problems lead to policies that do not exhibit sufficient variability to explore effectively.",
                "Many current approaches try to meta-learn the entire learning algorithm which limits the asymptotic performance of the model.",
                "Idea behind MAESN is to meta-train policy-parameters so that they learn to use the task-specific latent variables for exploration and can quickly adapt to a new task.",
                "An important detail is that the parameters are optimized to maximize the expected rewards after one step of gradient update to ensure that the policy uses the latent variables for exploration.",
                "For every iteration of meta-training, an \u201cinner\u201d gradient update is performed on the variational parameters and the post-inner-update parameters are used to perform the meta-update.",
                "The authors report that performing the \u201cinner\u201d gradient update on the policy-parameters does not help the overall learning objective and that the step size for each parameter had to be meta-learned.",
                "The variation parameters have the usual KL divergence loss which encourages them to be close to the prior distribution (unit Gaussian in this case).",
                "After training, the variational parameters for each task are quite close to the prior probably because the training objective optimizes for the expected reward after one step of gradient descent on the variational parameters.",
                "Another implementation detail is that reward shaping is used to ensure that the policy gets useful signal during meta-training.",
                "To be fair to the baselines, reward shaping is used while training baselines as well.",
                "Moreover, the policies trained with reward shaping generalizes to sparse reward setup as well (during meta-test time).",
                "Experiments  Three tasks distributions: Robotic Manipulation, Wheeled Locomotion, and Legged Locomotion.",
                "Each task distribution has 100 meta-training tasks.",
                "In the Manipulation task distribution, the learner has to push different blocks from different positions to different goal positions.",
                "In the Locomotion task distributions, the different tasks correspond to the different goal positions.",
                "The experiments show that the proposed approach can adapt to new tasks quickly and the learn coherent exploration strategy.",
                "\u2022 In some cases, learning from scratch also provides a strong asymptotic performance although learning from scratch takes much longer."
            ],
            "author_id": "shugan",
            "pdf_url": "https://arxiv.org/pdf/1904.06387",
            "author_full_name": "Shagun Sodhani",
            "source_website": "https://github.com/shagunsodhani/papers-I-read",
            "id": 6726365
        }
    },
    "16057781": {
        "X": {
            "sections": [
                {
                    "text": "KEYWORDS Reinforcement learning; Task decomposition; Transfer; Lifelong learning ACM Reference Format: Bohan Wu, Jayesh K. Gupta, and Mykel J. Kochenderfer. 2019. Model Primitive Hierarchical Lifelong Reinforcement Learning . In Proc. of the 18th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2019), Montreal, Canada, May 13\u201317, 2019, IFAAMAS, 9 pages."
                },
                {
                    "heading": "1 INTRODUCTION",
                    "text": "In the lifelong learning setting, we want our agent to solve a series of related tasks drawn from some task distribution rather than a single, isolated task. Agents must be able to transfer knowledge gained in previous tasks to improve performance on future tasks. This setting is different from multi-task reinforcement learning [25, 27, 31] and variousmeta-reinforcement learning settings [7, 8], where the agent jointly trains on multiple task environments. Not only do such nonincremental settings make the problem of discovering common structures between tasks easier, they allow the methods to ignore the problem of catastrophic forgetting [16], which is the inability to solve previous tasks after learning to solve new tasks in a sequential learning setting.\nOur work takes a step towards solutions for such incremental settings. We draw on the idea of modularity [17]. While learning to perform a complex task, we force the agent to break its solution down into simpler subpolicies instead of learning a single monolithic policy. This decomposition allows our agent to rapidly learn another related task by transferring these subpolicies. We\nProc. of the 18th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2019), N. Agmon, M. E. Taylor, E. Elkind, M. Veloso (eds.), May 13\u201317, 2019, Montreal, Canada. \u00a9 2019 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.\nhypothesize that many complex tasks are heavily structured and hierarchical in nature. The likelihood of transfer of an agent\u2019s solution increases if it can capture such shared structure.\nA key ingredient of our proposal is the idea of world models [10, 12, 14] \u2014 transition models that can predict future sensory data given the agent\u2019s current actions. The world however is complex, and learning models that are consistent enough to plan with is not only hard [24], but planning with such one-step models is suboptimal [11]. We posit that the requirement that these world models be good predictors of the world state is unnecessary, provided we have a multiplicity of such models. We use the termmodel primitives to refer to these suboptimal world models. Since each model primitive is only relatively better at predicting the next states within a certain region of the environment space, we call this area the model primitive\u2019s region of specialization.\nModel primitives allow the agent to decompose the task being performed into subtasks according to their regions of specialization and learn a specialized subpolicy for each subtask. The same model primitives are used to learn a gating controller to select, improve, adapt, and sequence the various subpolicies to solve a given task in a manner very similar to a mixture of experts framework [15].\nOur framework assumes that at least a subset of model primitives are useful across a range of tasks and environments. This assumption is less restrictive than that of successor representations [3, 5]. Even though successor representations decouple the state transitions from the rewards (representing the task or goals), the transitions learned are policy dependent and can only transfer across tasks with the same environment dynamics.\nThere are alternative approaches to learning hierarchical spatiotemporal decompositions from the rewards seen while interacting with the environment. These approaches include meta-learning algorithms like Meta-learning Shared Hierarchies (MLSH) [8], which require a multiplicity of pretrained subpolicies and joint training on related tasks. Other approaches include the option-critic architecture [1] that allows learning such decompositions in a single task environment. However, this method requires regularization hyperparameters that are tricky to set. As observed by Vezhnevets et al. [30], its learning often collapses to a single subpolicy. Moreover, we posit that capturing the shared structure across task-environments can be more useful in the context of transfer for lifelong learning than reward-based task specific structures.\nTo summarize our contributions:\n\u2022 Given diverse suboptimalworldmodels, we propose amethod to leverage them for task decomposition. \u2022 We propose an architecture to jointly train decomposed subpolicies and a gating controller to solve a given task. \u2022 We demonstrate the effectiveness of this approach at both single-task and lifelong learning in complex domains with high-dimensional observations and continuous actions.\nar X\niv :1\n90 3.\n01 56\n7v 1\n[ cs\n.L G\n] 4\nM ar\n2 01\n9"
                },
                {
                    "heading": "2 PRELIMINARIES",
                    "text": "We assume the standard reinforcement learning (RL) formulation: an agent interacts with an environment to maximize the expected reward [23]. The environment is modeled as a Markov decision process (MDP), which is defined by \u27e8S,A,R,T ,\u03b3 \u27e9 with a state space S, an action space A, a reward function R : S \u00d7 A \u2192 R, a dynamics modelT : S\u00d7A \u2192 \u03a0(S), and a discount factor\u03b3 \u2208 [0, 1). Here, \u03a0(\u00b7) defines a probability distribution over a set. The agent acts according to stationary stochastic policies \u03c0 : S \u2192 \u03a0(A), which specify action choice probabilities for each state. Each policy \u03c0 has a corresponding Q\u03c0 : S \u00d7 A \u2192 R function that defines the expected discounted cumulative reward for taking an action a from state s and following the policy \u03c0 from that point onward.\nLifelong Reinforcement Learning: In a lifelong learning setting, the agent must interact with multiple tasks and successfully solve each of them. Adopting the framework from Brunskill and Li [4], in lifelong RL, the agent receives S,A, initial state distribution \u03c10 \u2208 \u03a0(S), horizon H , discount factor \u03b3 , and an unknown distribution over reward-transition function pairs, D. The agent samples (Ri ,Ti ) \u223c D and interacts with the MDP \u27e8S,A,Ri ,Ti ,\u03b3 \u27e9 for a maximum ofH timesteps, starting according to the initial state distribution \u03c10. After solving the given MDP or after H timesteps, whichever occurs first, the agent resamples from D and repeats.\nThe fundamental question in lifelong learning is to determine what knowledge should be captured by the agent from the tasks it has already solved so that it can improve its performance on future tasks. When learning with functional approximation, this translates to learning the right representation \u2014 the one with the right inductive bias for the tasks in the distribution. Given the assumption that the set of related tasks for lifelong learning share a lot of structure, the ideal representation should be able to capture this shared structure.\nThrun and Pratt [28] summarized various representation decomposition methods into two major categories. Modern approaches to avoiding catastrophic forgetting during transfer tend to fall into either category. The first category partitions the parameter space into task-specific parameters and general parameters [19]. The second category learns constraints that can be superimposed when learning a new function [13].\nA popular approachwithin the first category is to usewhat Thrun and Pratt [28] term as recursive functional decomposition. This approach assumes that solution to tasks can be decomposed into a function of the form fi = hi \u25e6 \u0434, where hi is task-specific whereas \u0434 is the same for all fi . This scheme has been particularly effective in computer vision where early convolutional layers in deep convolutional networks trained on ImageNet [6, 22] become a very effective \u0434 for a variety of tasks. However, this approach to decomposition often fails in DeepRL because of two main reasons. First, the gradients used to train such networks are noisier as a result of Monte Carlo sampling. Second, the i.i.d. assumption for training data often fails.\nWe instead focus on devising an effective piecewise functional decomposition of the parameter space, as defined by Thrun and Pratt [28]. The assumption behind this decomposition is that each function fi can be represented by a collection of functions h1, . . . ,hm ,\nwherem \u226a N , and N is the number of tasks to learn. Our hypothesis is that this decomposition is much more effective and easier to learn in RL."
                },
                {
                    "heading": "3 MODEL PRIMITIVE HIERARCHICAL RL",
                    "text": "This section outlines the Model Primitive Hierarchical Reinforcement Learning (MPHRL) framework (Figure 1) to address the problem of effective piecewise functional decomposition for transfer across a distribution of tasks."
                },
                {
                    "heading": "3.1 Model Primitives and Gating",
                    "text": "The key assumption in MPHRL is access to several diverse world models of the environment dynamics. These models can be seen as instances of learned approximations to the true environment dynamics T . In reality, these dynamics can even be non-stationary. Therefore, the task of learning a complete model of the environment dynamics might be too difficult. Instead, it can be much easier to train multiple approximate models that specialize in different parts of the environment. We use the term model primitives to refer to these approximate world models.\nSuppose we have access to K model primitives: T\u0302k : S \u00d7 A \u2192 \u03a0(S). For simplicity, we can assign a labelMk to each T\u0302k , such that their predictions of the environment\u2019s transition probabilities can be denoted by T\u0302 (st+1 | st ,at ,Mk ).\n3.1.1 Subpolicies. The goal of the MPHRL framework is to use these suboptimal predictions from different model primitives to decompose the task space into their regions of specialization, and learn different subpolicies \u03c0k : S \u2192 \u03a0(A) that can focus on these regions. In the function approximation regime, each subpolicy \u03c0k belongs to a fixed class of smoothly parameterized stochastic policies {\u03c0\u03b8k | \u03b8k \u2208 \u0398}, where \u0398 is a set of valid parameter vectors.\nModel primitives are suboptimal and make incorrect predictions about the next state. Therefore we do not use them for planning or model-based learning of subpolicies directly. Instead, model primitives give rise to useful functional decompositions and allow subpolicies to be learned in a model-free way.\n3.1.2 Gating Controller. Taking inspiration from the mixture-ofexperts literature [15], where the output from multiple experts can be combined using probabilistic gating functions, MPHRL decomposes the solution for a given task into multiple \u201cexpert\u201d subpolicies and a gating controller that can compose them to solve the task. We\nwant this switching behavior to be probabilistic and continuous to avoid abrupt transitions. During learning, we want this controller to help assign the reward signal to the correct blend of subpolicies to ensure effective learning as well as decomposition.\nSince the gating controller\u2019s goal is to choose the subpolicy whose corresponding model primitive makes the best prediction for a given transition, using Bayes\u2019 rule we can write:\nP(Mk | st ,at , st+1) \u221d P(Mk | st )\u03c0k (at | st )T\u0302 (st+1 | st ,at ,Mk ) (1)\nbecause \u03c0k (at | st ) = \u03c0 (at | st ,Mk ). The agent only has access to the current state st during execution. Therefore, the agent needs to marginalize out st+1 and at such that the model choice only depends on the current state st :\nP(Mk | st ) = \u222b\nst+1\u2208S \u222b at \u2208A P(Mk | st ,at , st+1)\nP(st+1,at )datdst+1 (2) This is equivalent to:\nP(Mk | st ) = Est+1,at\u223cP (st+1,at ) [P(Mk | st ,at , st+1)] (3) Unfortunately, computing these integrals requires expensive Monte Carlomethods. However, we can use an approximatemethod to achieve the same objective with discriminative learning [18].\nWe parameterize the gating controller (GC) as a categorical distribution P\u03d5 (Mk | st ) = P(Mk | st ;\u03d5) and minimize the conditional cross entropy loss between Est+1,at\u223cP (st+1,at ) [P(Mk | st ,at , st+1)] and P\u03d5 (Mk | st ) for all sampled transitions (st ,at , st+1) in a rollout:\nminimize \u03d5\nLGC (4)\nwhere LGC = \u2211 st \u2211 k \u2212 (\u2211 st+1 \u2211 at P(Mk | st ,at , st+1) ) \u00d7 log P(Mk | st ;\u03d5) (5) This is equivalent to an implicit Monte Carlo integration to compute the marginal if st+1,at \u223c P(st+1,at ). Although we cannot query or sample from P(st+1,at ) directly, st ,at , and st+1 can be sampled according to their respective distributionswhilewe perform rollouts in the environment. Despite the introduced bias in our estimates, we find Eq. 4 sufficient for achieving task decomposition.\n3.1.3 Subpolicy Composition. Taking inspiration from mixtureof-experts, the gating controller composes the subpolicies into a mixture policy:\n\u03c0 (at | st ) = K\u2211 k=1 P\u03d5 (Mk | st )\u03c0k (at | st ) (6)\n3.1.4 Decoupling Cross Entropy from Action Distribution. During a rollout, the agent samples as follows:\nat \u223c \u03c0 (at | st ) (7) st+1 \u223c T(st+1 | st ,at ) (8)\nThe \u03c0k from Eq. 1 gets coupled with this sampling distribution, making the target distribution in Eq. 5 no longer stationary and the\napproximation process difficult. We alleviate this issue by ignoring \u03c0k , effectively treating it as a distribution independent of k . This transforms Eq. 1 into:\nP\u0302(Mk | st ,at , st+1) \u221d P(Mk | st )T\u0302 (st+1 | st ,at ,Mk ) (9)"
                },
                {
                    "heading": "3.2 Learning",
                    "text": "Since the focus of this work is on difficult continuous action problems, we mostly concentrate on the issue of policy optimization and how it integrates with the gating controller. The standard policy (SP) optimization objective is:\nmaximize \u03b8\nLSP = E\u03c10,\u03c0\u03b8 [\u03c0\u03b8 (at | st )Q\u03c0\u03b8 (st ,at )] (10)\nWith baseline subtraction for variance reduction, this turns into [20]:\nmaximize \u03b8\nLPG = E\u03c10,\u03c0\u03b8 [\u03c0\u03b8 (at | st )A\u0302t ] (11)\nwhere A\u0302t is an estimator of the advantage function [2]. In MPHRL, we directly use the mixture policy as defined by Eq. 6. The standard policy gradients (PG) get weighted by the probability outputs of the gating controller, enforcing the required specialization by factorizing into:\n\u0434\u0302k = E\u03c10,\u03c0\u03b8k [ P\u03d5 (Mk | st )\u2207\u03b8k log\u03c0\u03b8k (at | st )A\u0302t ] (12)\nIn practice, we use the Clipped PPO objective [21] instead to perform stable updates by limiting the step size. This includes adding a baseline estimator (BL) parameterized by \u03c8 for value prediction and variance reduction. We optimize\u03c8 according to the following loss:\nLBL = E [ V\u03c8 \u2212V\u03c0\u03b8 2] (13)\nWe summarize this single-task learning algorithm in Algorithm 1, which results in a set of decomposed subpolicies, \u03c0\u03b81 , . . . \u03c0\u03b8K , and a gating controller P\u03d5 that can modulate between them to solve the task under consideration.\nAlgorithm 1 MPHRL: single-task learning\n1: Initialize P\u03d5 ,\u03c0\u03b8 = {\u03c0\u03b81 , . . . ,\u03c0\u03b8K }, V\u03c8 2: while not converged do 3: Rollout trajectories \u03c4 \u223c \u03c0\u03b8,\u03d5 4: Compute advantage estimates A\u0302\u03c4 5: Optimize LPG wrt \u03b81, . . . ,\u03b8K with expectations taken over \u03c4 6: Optimize LBL wrt\u03c8 with expectations taken over \u03c4 7: Optimize LGC wrt \u03d5\nwith expectations taken over \u03c4\nLifelong learning: We have shown how MPHRL can decompose a single complex task solution into different functional components. Complex tasks often share structure and can be decomposed into similar sets of subtasks. Different tasks however require different recomposition of similar subtasks. Therefore, we transfer the subpolicies to learn target tasks, but not the gating controller or the baseline estimator. We summarize the lifelong learning algorithm in Algorithm 2, with the global variable RESET set to true.\nAlgorithm 2MPHRL: lifelong learning\n1: Initialize P\u03d5 , \u03c0\u03b8 = {\u03c0\u03b81 , . . . ,\u03c0\u03b8K },V\u03c8 2: for Tasks (Ri ,Ti ) \u223c D do 3: if RESET then 4: Initialize P\u03d5 , V\u03c8 5: while not converged do 6: Rollout trajectories \u03c4 \u223c \u03c0\u03b8,\u03d5 7: Compute advantage estimates A\u0302\u03c4 8: Optimize LPG wrt \u03b81, . . . ,\u03b8K with expectations taken over \u03c4 9: Optimize LBL wrt\u03c8\nwith expectations taken over \u03c4 10: Optimize LGC wrt \u03d5\nwith expectations taken over \u03c4"
                },
                {
                    "heading": "4 EXPERIMENTS",
                    "text": "Our experiments aim to answer two questions: (a) can model primitives ensure task decomposition? (b) does such decomposition improve transfer for lifelong learning?\nWe evaluate our approach in two challenging domains: a MuJoCo [29] ant navigating different mazes and a Stacker [26] arm picking up and placing different boxes. In our experiments, we use subpolicies that have Gaussian action distributions, with mean given by a multi-layer perceptron taking observations as input and standard deviations given by a different set of parameters. MPHRL\u2019s gating controller outputs a categorical distribution and is parameterized by another multi-layer perceptron. We also use a separate multi-layer perceptron for the baseline estimator. We use the standard PPO algorithm as a baseline to compare against MPHRL. Transferring network weights empirically led to worse performance for standard PPO. Hence, we re-initialize its weights for every task. For fair comparison, we also shrink the hidden layer size of MPHRL\u2019s subpolicy networks from 64 to 16. We conduct each experiment across 5 different seeds. Error bars represent the standard deviation from the mean.\nThe focus of this work is on understanding the usefulness of model primitives for task decomposition and the resulting improvement in sample efficiency from transfer. To conduct controlled experiments with interpretable results, we hand-designed model primitives using the true next state provided by the environment simulator. Concretely, we apply distinct multivariate Gaussian noise models with covariance \u03c3\u03a3 to the true next state. We then sample from this distribution to obtain the mean of the probability distribution of a model primitive\u2019s next state prediction, using \u03a3 as its covariance. Here, \u03c3 is the noise scaling factor that distinguishes model primitives, while \u03a3 refers to the empirical covariance of the sampled next states:\n\u00b5 \u223c N(st+1,\u03c3k\u03a3) (14) T\u0302 (st+1 | st ,at ,Mk ) = N(\u00b5,\u03c3k\u03a3) (15)\nUsing \u03a3 as opposed to a constant covariance is essential for controlled experiments because different elements of the observation space have different orders of magnitude. Sampling \u00b5 from a distribution effectively adds random bias to the model primitive\u2019s next state probability distribution.\nHyperparameter details are in Table 1, and our code is freely available at http://github.com/sisl/MPHRL."
                },
                {
                    "heading": "4.1 Single-task Learning",
                    "text": "First, we focus on two single-task learning experiments where MPHRL learns a number of interpretable subpolicies to solve a single task. Both the L-Maze and D-Maze (Figure 2a) tasks require the ant to learn to walk and reach the green goal within a finite 18-Pickup&Place. 2Single task refers to L-Maze and D-Maze; source and target tasks refer to the first task and all subsequent tasks in a lifelong learning taskset, respectively. 3Baseline network hyperparameters apply to both MPHRL and baseline PPO; model primitive networks are for experiments with learned model primitives only. 4The baseline PPO has no subpolicies, so the subpolicy network is the policy network. 5Baseline and subpolicy networks only.\nhorizon. For both tasks, both the goal and the initial ant locations are fixed. For the L-Maze, the agent has access to two model primitives, one specializing in the horizontal (E, W) corridor and the other specializing in the vertical (N, S) corridor of the maze. Similarly for the D-Maze, the agent has access to four model primitives, one specializing in each N, S, E, W corridor of the maze. In their specialized corridors, the noise scaling factor \u03c3 = 0. Outside of their regions of specialization, \u03c3 = 0.5. The observation space includes the standard joint angles and positions, lidar information tracking distances from walls on each side, and the Manhattan distance to the goal. Figure 2b shows the experimental results on these environments. Notice that using model primitives can make the learning problem more difficult and increase the sample complexity on a single task. This is expected, since we are forcing the agent to decompose the solution, which could be unnecessary for easy tasks. However, we will observe in the following section that this decomposition can lead to remarkable improvements in transfer performance during lifelong learning."
                },
                {
                    "heading": "4.2 Lifelong Learning",
                    "text": "To evaluate our framework\u2019s performance at lifelong learning, we introduce two tasksets.\n4.2.1 10-Maze. To evaluate MPHRL\u2019s performance in lifelong learning, we generate a family of 10 random mazes for the MuJoCo Ant environment, referred to as the 10-Maze taskset (Figure 4) hereafter. The goal, the observation space, the Gaussian noise models, and the model primitives remain the same as in D-Maze. The agent has a maximum of 3 \u00d7 107 timesteps to reach 80% success rate in each of the 10 tasks. As shown in Figure 3a, MPHRL requires nearly double the number of timesteps to learn the decomposed subpolicies in the first task. However, this cost gets heavily amortized over the entire taskset, with MPHRL taking half the total number of timesteps of the baseline PPO, exhibiting strong subpolicy transfer.\n4.2.2 8-Pickup&Place. Wemodify the Stacker task [26] to create the 8-Pickup&Place taskset. As shown in Figure 5, a robotic arm is tasked to bring 2 boxes to their respective goal locations in a certain\norder. Marked by colors red, green, and blue, the goal locations reside within two short walls forming a \u201cstack\u201d.\nEach of the 8 tasks has a maximum of 3 goal locations. The observation space of the agent includes joint angles and positions, box and goal locations, their relative distances to each other, and the current stage of the task encoded as one-hot vectors. The agent has access to six model primitives for each box that specialize in reaching above, lowering to, grasping, picking up, carrying, and dropping a certain box. Similar to 10-Maze, model primitives have \u03c3 of 0 within their specialized stages and \u03c3 of 0.5 otherwise. Figure 3b shows MPHRL\u2019s experimental performance by learning twelve useful subpolicies for this taskset. We notice again the strong transfer performance due to the decomposition forced by the model primitives. Note that this taskset is much more complex than 10-Maze such that MPHRL even accelerates the learning of the first task."
                },
                {
                    "heading": "4.3 Ablation",
                    "text": "We conduct ablation experiments to answer the following questions: (1) How much gain in sample efficiency is achieved by transfer-\nring subpolicies? (2) Can MPHRL learn the task decomposition even when the\nmodel primitives are quite noisy or when the source task does not cover all \u201ccases\u201d? (3) When does MPHRL fail to decompose the solution? (4) What kind of diversity in the model primitives is essential for\nperformance? (5) When does MPHRL lead to negative transfer? (6) Is MPHRL\u2019s gain in sample efficiency a result of hand-crafted\nmodel primitives and how does it perform with actual learned model primitives?\n4.3.1 Model Noise. MPHRL has the ability to decompose the solution even given bad model primitives. Since the learning is done model-free, these suboptimal model primitives should not strongly affect the learning performance so long as they remain sufficiently distinct. To investigate the limitations to this claim, we conduct five experiments using various sets of noisy model primitives. Below, the first value corresponds to the noise scaling factor \u03c3 within their individual regions of specialization, while the second value corresponds to \u03c3 outside of their regions of specialization. (a) 0.4 and 0.5: good models with limited distinction (b) 0.5 and 1.0: good models with reasonable distinction\n(c) 5.0 and 10.0: bad models with reasonable distinction (d) 9.0 and 10.0: bad models with limited distinction (e) 0.5 and 0.5: good models with no distinction\nShown in Figure 6a, while (a), (b), (c), and (d) exhibit limited degradation in performance, (d) experiences the most performance degradation on average. On the other hand, in (e) MPHRL took 22.0\u00b14.6million timesteps to solve the first task and 2.8 \u00b1 1.6 million timesteps to solve the second task, but failed to solve the third task within 30 million timesteps. This is because the model primitives are identical and provide no information about task decomposition. In summary, MPHRL is robust against bad model primitives so long as the they maintain some relative distinction. Similar observations hold true for the 8-Pickup&Place taskset where noise models with distinctive models with large noise of \u03c3 = 5 and \u03c3 = 20 show little deterioration in performance, taking 15.8 \u00b1 5.5 million timesteps to reach 75% average success rate.\n4.3.2 Overlapping Model Primitives. We next test the condition when there is substantial overlap in regions of specialization between different model primitives. For the 10-Maze taskset, the most plausible region for this confusion is at the corners. In this experiment, within each corner, the two model primitives whose specialized corridors share the corner have \u03c3 = 0 while the other two have \u03c3 = 0.5. Figure 6b shows the performance for model primitive confusion against the standard set of model primitives with no confusion. We observe that despite some performance degradation, MPHRL continues to outperform the PPO baseline.\n4.3.3 Model Diversity. Having tested MPHRL against noises, we experimented with undesirable model primitives for 10-Maze: (a) Extra: a fifth model primitive that specializes in states where\nthe ant is moving horizontally; (b) H-V corridors: 2 model primitives specializing in horizontal (E,\nW) and vertical (N, S) corridors respectively; (c) Velocity: 2 model primitives specializing in states where the\nant is moving horizontally or vertically; and for 8-Pickup&Place: (a) Box-only: 2 model primitives for all actions on 2 boxes; (b) Action-only: 6 model primitives for 6 actions performed on\nboxes: reach above, lower to, grasp, pick up, carry, and drop. Table 2 shows MPHRL is susceptible to performance degradation given undesirable sets of model primitives. However, MPHRL still outperforms baseline PPO when given an extra, undesirable model primitive. This indicates that for best transfer, the model primitives need to approximately capture the structure present in the taskset.\n4.3.4 Negative Transfer and Catastrophic Forgetting. Lifelong learning agents with neural network function approximators face the problem of negative transfer and catastrophic forgetting. Ideally, they should find the solution quickly if the task has already been seen. More generally, given two sets of tasks T and T \u2032 such that T \u2282 T \u2032, after being exposed to T \u2032 the agent should perform no worse, and preferably better, than had it been exposed to T only.\nIn this experiment, we restore the subpolicy checkpoints after solving the 10 tasks and evaluate MPHRL\u2019s learning performance for the first 9 tasks. Similarly, we restore the subpolicy checkpoints after solving 6 tasks and evaluate MPHRL\u2019s performance on the\nfirst 5 tasks. The gating controller is reset for each task as in earlier experiments. We summarize the results in Table 3. Subpolicies trained sequentially on 6 or 10 tasks quickly relearn the required behavior for all previously seen tasks, implying no catastrophic forgetting. Moreover, if we compare the 10-task result to the 6-task result, we see remarkable improvements at transfer. This implies negative transfer is limited with this approach.\n4.3.5 Oracle Gating Controller. One might suspect that all gains in sample efficiency come from hand-crafted model primitives because they allow the agent to learn a perfect gating controller. However, Figure 7 shows the reward curves for an experiment where the gating controller is already perfectly known. This setup is unable to\nlearn any 10-Maze task. Since the 10-Maze taskset is composed of sequential subtasks, only one subpolicy will be learned in the first corridor when the gating controller is perfect. When transitioning to the second corridor, the second policy needs to be learned from scratch, making the ant\u2019s survival rate very low. This discourages the first subpolicy from entering the second corridor and activating the second subpolicy. Eventually, the ant stops moving forward close to the intersection between the first two corridors. In contrast, MPHRL\u2019s natural curriculum for gradual specialization allows multiple subpolicies to learn the basic skills for survival initially.\n4.3.6 Partial Decomposition. To confirm that the ordering of tasks does not significantly affect MPHRL\u2019s performance, we modified 10-Maze to create the 10-Maze-v2 taskset (Figure 9), in which the source task does not allow for complete decomposition into all useful subpolicies for the subsequent tasks. Again, we observe large improvement in sample efficiency over standard PPO (Figure 8).\n4.3.7 Learned Model Primitives. This paper focuses on evaluating suboptimal models for task decomposition in controlled experiments using hand-designed model primitives. Here, we show one way to obtain each model primitive for 10-Maze-v2 using three corridor environments demonstrated in Figure 10. Concretely, we parameterize each model primitive using a multivariate Gaussian distribution. We learn the mean of this distribution via a multilayer perceptron using a weighted mean square error in dynamics prediction as the loss. The standard deviation is still derived from the empirical covariance \u03a3 as described earlier. Even though the diversity in these learned model primitives is much more difficult to quantify and control, their sample efficiency substantially outperforms standard PPO and slightly underperforms hand-designed model primitives with 0 and 0.5 model noises (Figure 8).\n4.3.8 Gating Controller Transfer. To explore factors that lead to negative transfer, we tested MPHRL without re-initializing the gating controller in target tasks, as shown in Figure 6c. Although the mean sample efficiency remains stable, its standard deviation increases dramatically, indicating volatility due to negative transfer.\n4.3.9 Subpolicy Transfer. To measure how much gain in sample efficiency MPHRL has achieved by transferring subpolicies alone, we conducted a 10-Maze experiment by re-initializing all network\nweights for every new task. As shown in Figure 6c, sample complexity more than quintuples when subpolicies are re-initialized (in green).\n4.3.10 Coupling between Cross Entropy and Action Distribution. To validate using P\u0302(Mk | st ,at , st+1) in Eq. 9 as opposed to P(Mk | st ,at , st+1) from Eq. 1, we tested MPHRL with Eq. 1 on 10-Maze. All runs with different seeds failed to solve the first 5 tasks (Table 4). As the gating controller is re-initialized during transfer, most actions were chosen incorrectly. The gating controller is thus presented with the incorrect cross entropy target, which worsens the action distribution. The resulting vicious cycle forces the gating controller to converge to a suboptimal equilibrium against the incorrect target."
                },
                {
                    "heading": "5 CONCLUSIONS",
                    "text": "We showed how imperfect world models can be used to decompose a complex task into simpler ones. We introduced a framework that uses these model primitives to learn piecewise functional decompositions of solutions to complex tasks. The learned decomposed subpolicies can then be used to transfer to a variety of related tasks, reducing the overall sample complexity required to learn complex behaviors. Our experiments showed that such structured decomposition avoids negative transfer and catastrophic interference, a major concern for lifelong learning systems.\nOur approach does not require access to accurate world models. Neither does it need a well-designed task distribution or the incremental introduction of individual tasks. So long as the set of model primitives are useful across the task distribution, MPHRL is robust to other imperfections.\nNevertheless, learning useful and diverse model primitives, subpolicies and task decomposition all simultaneously is left for future work. The recently introduced Neural Processes [9] can potentially be an efficient approach to build upon."
                },
                {
                    "heading": "ACKNOWLEDGMENTS",
                    "text": "We are thankful to Kunal Menda and everyone at SISL for useful comments and suggestions. This work is supported in part by DARPA under agreement number D17AP00032. The content is solely the responsibility of the authors and does not necessarily represent the official views of DARPA. We are also grateful for the support from Google Cloud in scaling our experiments."
                }
            ],
            "year": 2019,
            "references": [
                {
                    "title": "The Option-critic Architecture",
                    "authors": [
                        "Pierre-Luc Bacon",
                        "Jean Harb",
                        "Doina Precup"
                    ],
                    "venue": "In AAAI Conference on Artificial Intelligence (AAAI)",
                    "year": 2017
                },
                {
                    "title": "Reinforcement Learning in Continuous Time: Advantage Updating",
                    "authors": [
                        "L C Baird"
                    ],
                    "venue": "In IEEE International Conference on Neural Networks (ICNN),",
                    "year": 1994
                },
                {
                    "title": "Successor Features for Transfer in Reinforcement Learning",
                    "authors": [
                        "Andr\u00e9 Barreto",
                        "Will Dabney",
                        "R\u00e9mi Munos",
                        "Jonathan J Hunt",
                        "Tom Schaul",
                        "Hado P van Hasselt",
                        "David Silver"
                    ],
                    "venue": "In Advances in Neural Information Processing Systems (NIPS)",
                    "year": 2017
                },
                {
                    "title": "PAC-inspired Option Discovery in Lifelong Reinforcement Learning",
                    "authors": [
                        "Emma Brunskill",
                        "Lihong Li"
                    ],
                    "venue": "In International Conference on Machine Learning (ICML)",
                    "year": 2014
                },
                {
                    "title": "Improving Generalization for Temporal Difference Learning: The Successor Representation",
                    "authors": [
                        "Peter Dayan"
                    ],
                    "venue": "Neural Computation",
                    "year": 1993
                },
                {
                    "title": "Imagenet: A Large-scale Hierarchical Image Database",
                    "authors": [
                        "Jia Deng",
                        "Wei Dong",
                        "Richard Socher",
                        "Li-Jia Li",
                        "Kai Li",
                        "Li Fei-Fei"
                    ],
                    "venue": "In IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
                    "year": 2009
                },
                {
                    "title": "Model-Agnostic Meta- Learning for Fast Adaptation of Deep Networks",
                    "authors": [
                        "Chelsea Finn",
                        "Pieter Abbeel",
                        "Sergey Levine"
                    ],
                    "venue": "In International Conference on Machine Learning (ICML)",
                    "year": 2017
                },
                {
                    "title": "Meta Learning Shared Hierarchies",
                    "authors": [
                        "Kevin Frans",
                        "Jonathan Ho",
                        "Xi Chen",
                        "Pieter Abbeel",
                        "John Schulman"
                    ],
                    "venue": "In International Conference on Learning Representations (ICLR)",
                    "year": 2018
                },
                {
                    "title": "2018",
                    "authors": [
                        "D. Ha",
                        "J. Schmidhuber"
                    ],
                    "venue": "World Models. CoRR abs/1803.10122 ",
                    "year": 2018
                },
                {
                    "title": "Erik Talvitie",
                    "authors": [
                        "G. Zacharias Holland"
                    ],
                    "venue": "and Michael Bowling. 2018. The Effect of Planning Shape on Dyna-style Planning in High-dimensional State Spaces. CoRR abs/1806.01825 ",
                    "year": 2018
                },
                {
                    "title": "Sensorimotor Mismatch Signals in Primary Visual Cortex of the Behaving Mouse",
                    "authors": [
                        "Georg B. Keller",
                        "Tobias Bonhoeffer",
                        "Mark H\u00c3ijbener"
                    ],
                    "venue": "Neuron 74,",
                    "year": 2012
                },
                {
                    "title": "Overcoming Catastrophic Forgetting in Neural Networks",
                    "authors": [
                        "James Kirkpatrick",
                        "Razvan Pascanu",
                        "Neil Rabinowitz",
                        "Joel Veness",
                        "Guillaume Desjardins",
                        "Andrei A Rusu",
                        "Kieran Milan",
                        "John Quan",
                        "Tiago Ramalho",
                        "Agnieszka Grabska-Barwinska"
                    ],
                    "venue": "Proceedings of the National Academy of Sciences 114,",
                    "year": 2017
                },
                {
                    "title": "A Sensorimotor Circuit in Mouse Cortex for Visual Flow Predictions",
                    "authors": [
                        "Marcus Leinweber",
                        "Daniel R. Ward",
                        "Jan M. Sobczak",
                        "Alexander Attinger",
                        "Georg B. Keller"
                    ],
                    "venue": "Neuron 95,",
                    "year": 2017
                },
                {
                    "title": "Mixture of Experts: A Literature Survey",
                    "authors": [
                        "Saeed Masoudnia",
                        "Reza Ebrahimpour"
                    ],
                    "venue": "Artificial Intelligence Review 42,",
                    "year": 2014
                },
                {
                    "title": "Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem",
                    "authors": [
                        "Michael McCloskey",
                        "Neal J Cohen"
                    ],
                    "venue": "In Psychology of Learning and Motivation, Gordon H Bower (Ed.)",
                    "year": 1989
                },
                {
                    "title": "Learning Modular Policies for Robotics",
                    "authors": [
                        "Gerhard Neumann",
                        "Christian Daniel",
                        "Alexandros Paraschos",
                        "Andras Kupcsik",
                        "Jan Peters"
                    ],
                    "venue": "Frontiers of Computational Neuroscience 8,",
                    "year": 2014
                },
                {
                    "title": "The Return of the Gating Network: Combining Generative Models and Discriminative Training in Natural Image Priors",
                    "authors": [
                        "Dan Rosenbaum",
                        "Yair Weiss"
                    ],
                    "venue": "In Advances in Neural Information Processing Systems",
                    "year": 2015
                },
                {
                    "title": "High-dimensional Continuous Control Using Generalized Advantage Estimation",
                    "authors": [
                        "John Schulman",
                        "Philipp Moritz",
                        "Sergey Levine",
                        "Michael Jordan",
                        "Pieter Abbeel"
                    ],
                    "venue": "CoRR abs/1506.02438",
                    "year": 2015
                },
                {
                    "title": "Proximal Policy Optimization Algorithms",
                    "authors": [
                        "John Schulman",
                        "Filip Wolski",
                        "Prafulla Dhariwal",
                        "Alec Radford",
                        "Oleg Klimov"
                    ],
                    "year": 2017
                },
                {
                    "title": "Revisiting Unreasonable Effectiveness of Data inDeep Learning Era",
                    "authors": [
                        "Chen Sun",
                        "Abhinav Shrivastava",
                        "Saurabh Singh",
                        "Abhinav Gupta"
                    ],
                    "venue": "In IEEE International Conference on Computer Vision (ICCV)",
                    "year": 2017
                },
                {
                    "title": "Reinforcement learning: An Introduction",
                    "authors": [
                        "Richard S Sutton",
                        "Andrew G Barto"
                    ],
                    "year": 1998
                },
                {
                    "title": "Self-Correcting Models for Model-Based Reinforcement Learning",
                    "authors": [
                        "Eric Talvitie"
                    ],
                    "venue": "In AAAI Conference on Artificial Intelligence (AAAI)",
                    "year": 2017
                },
                {
                    "title": "Multitask Reinforcement Learning on the Distribution of MDPs",
                    "authors": [
                        "Fumihide Tanaka",
                        "Masayuki Yamamura"
                    ],
                    "venue": "In IEEE International Symposium on Computational Intelligence in Robotics and Automation,",
                    "year": 2003
                },
                {
                    "title": "DeepMind Control Suite",
                    "authors": [
                        "Yuval Tassa",
                        "Yotam Doron",
                        "Alistair Muldal",
                        "Tom Erez",
                        "Yazhe Li",
                        "Diego de Las Casas",
                        "David Budden",
                        "Abbas Abdolmaleki",
                        "Josh Merel",
                        "Andrew Lefrancq",
                        "Timothy P. Lillicrap",
                        "Martin A. Riedmiller"
                    ],
                    "year": 2018
                },
                {
                    "title": "Distral: Robust Multitask Reinforcement Learning",
                    "authors": [
                        "Yee Teh",
                        "Victor Bapst",
                        "Wojciech M Czarnecki",
                        "John Quan",
                        "James Kirkpatrick",
                        "Raia Hadsell",
                        "Nicolas Heess",
                        "Razvan Pascanu"
                    ],
                    "venue": "In Advances in Neural Information Processing Systems",
                    "year": 2017
                },
                {
                    "title": "Learning to Learn: Introduction and Overview",
                    "authors": [
                        "Sebastian Thrun",
                        "Lorien Pratt"
                    ],
                    "venue": "In Learning to Learn, Sebastian Thrun and Lorien Pratt (Eds.)",
                    "year": 1998
                },
                {
                    "title": "MuJoCo: A Physics Engine for Model-based Control",
                    "authors": [
                        "Emanuel Todorov",
                        "Tom Erez",
                        "Yuval Tassa"
                    ],
                    "venue": "In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
                    "year": 2012
                },
                {
                    "title": "FeUdal Networks for Hierarchical Reinforcement Learning",
                    "authors": [
                        "Alexander Sasha Vezhnevets",
                        "Simon Osindero",
                        "Tom Schaul",
                        "Nicolas Heess",
                        "Max Jaderberg",
                        "David Silver",
                        "Koray Kavukcuoglu"
                    ],
                    "venue": "In International Conference on Machine Learning (ICML)",
                    "year": 2017
                },
                {
                    "title": "Multi-task Reinforcement Learning: A Hierarchical Bayesian Approach",
                    "authors": [
                        "Aaron Wilson",
                        "Alan Fern",
                        "Soumya Ray",
                        "Prasad Tadepalli"
                    ],
                    "venue": "In International Conference on Machine Learning (ICML)",
                    "year": 2007
                }
            ],
            "id": "SP:4a74e7a54bd5f31f20c78d8553cc39fd88834b49",
            "authors": [
                {
                    "name": "Bohan Wu",
                    "affiliations": []
                },
                {
                    "name": "Jayesh K. Gupta",
                    "affiliations": []
                },
                {
                    "name": "Mykel J. Kochenderfer",
                    "affiliations": []
                }
            ],
            "abstractText": "Learning interpretable and transferable subpolicies and performing task decomposition from a single, complex task is difficult. Some traditional hierarchical reinforcement learning techniques enforce this decomposition in a top-down manner, while meta-learning techniques require a task distribution at hand to learn such decompositions. This paper presents a framework for using diverse suboptimal world models to decompose complex task solutions into simpler modular subpolicies. This framework performs automatic decomposition of a single source task in a bottom up manner, concurrently learning the required modular subpolicies as well as a controller to coordinate them. We perform a series of experiments on high dimensional continuous action control tasks to demonstrate the effectiveness of this approach at both complex single task learning and lifelong learning. Finally, we perform ablation studies to understand the importance and robustness of different elements in the framework and limitations to this approach.",
            "title": "Model Primitive Hierarchical Lifelong Reinforcement Learning "
        },
        "Y": {
            "blog_id": "model-primitive-hierarchical-lifelong-reinforcement-learning",
            "summary": [
                "The paper presents a framework that uses diverse suboptimal world models that can be used to break complex policies into simpler and modular sub-policies.",
                "Given a task, both the sub-policies and the controller are simultaneously learned in a bottom-up manner.",
                "The framework is called as Model Primitive Hierarchical Reinforcement Learning (MPHRL).",
                "Idea  Instead of learning a single transition model of the environment (aka world model) that can model the transitions very well, it is sufficient to learn several (say k) suboptimal models (aka model primitives).",
                "Each model primitive will be good in only a small part of the state space (aka region of specialization).",
                "These model primitives can then be used to train a gating mechanism for selecting sub-policies to solve a given task.",
                "Since these model primitives are sub-optimal, they are not directly used with model-based RL but are used to obtain useful functional decompositions and sub-policies are trained with model-free approaches.",
                "Single Task Learning  A gating controller is trained to choose the sub-policy whose model primitive makes the best prediction.",
                "This requires modeling p(Mk | st, at, st+1) where p(Mk) denotes the probability of selecting the kth model primitive.",
                "This is hard to compute as the system does not have access to st+1 and at at time t before it has choosen the sub-policy.",
                "Properly marginalizing st+1 and at would require expensive MC sampling.",
                "Hence an approximation is used and the gating controller is modeled as a categorical distribution - to produce p(Mk | st).",
                "This is trained via a conditional cross entropy loss where the ground truth distribution is obtained from transitions sampled in a rollout.",
                "The paper notes that technique is biased but reports that it still works for the downstream tasks.",
                "The gating controller composes the sub-policies as a mixture of Gaussians.",
                "For learning, PPO algorithm is used with each model primitives gradient weighted by the probability from the gating controller.",
                "Lifelong Learning  Different tasks could share common subtasks but may require a different composition of subtasks.",
                "Hence, the learned sub-policies are transferred across tasks but not the gating controller or the baseline estimator (from PPO).",
                "Experiments  Domains:  Mujoco ant navigating different mazes.",
                "Stacker arm picking up and placing different boxes.",
                "Implementation Details:  Gaussian subpolicies  PPO as the baseline  Model primitives are hand-crafted using the true next state provided by the environment simulator.",
                "Single Task  Only maze task is considered with the start position (of the ant) and the goal position is fixed.",
                "Observation includes distance from the goal.",
                "Forcing the agent to decompose the problem, when a more direct solution may be available, causes the sample complexity to increase on one task.",
                "Lifelong Learning  Maze  10 random Mujoco ant mazes used as the task distribution.",
                "MPHRL takes almost twice the number of steps (as compared to PPO baseline) to solve the first task but this cost gets amortized over the distribution and the model takes half the number of steps as compared to the baseline (summed over the 10 tasks).",
                "Pick and Place  8 Pick and Place tasks are created with max 3 goal locations.",
                "Observation includes the position of the goal.",
                "Ablations  Overlapping model primitives can degrade the performance (to some extent).",
                "Similarly, the performance suffers when redundant primitives are introduced indicating that the gating mechanism is not very robust.",
                "Sub-policies could quickly adapt to the previous tasks (on which they were trained initially) despite being finetuned on subsequent tasks.",
                "The order of tasks (in the 10-Mazz task) does not degrage the performance.",
                "Transfering the gating controller leads to negative transfer.",
                "Notes  I think the biggest strength of the work is that accurate dynamics model are not needed (which are hard to train anyways!)",
                "through the experimental results are not conclusive given the limited number of domains on which the approach is tested."
            ],
            "author_id": "shugan",
            "pdf_url": "https://arxiv.org/pdf/1903.01567",
            "author_full_name": "Shagun Sodhani",
            "source_website": "https://github.com/shagunsodhani/papers-I-read",
            "id": 16057781
        }
    },
    "51450104": {
        "X": {
            "sections": [
                {
                    "text": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 12 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples."
                },
                {
                    "heading": "1 Introduction",
                    "text": "The promise of deep learning is to discover rich, hierarchical models [2] that represent probability distributions over the kinds of data encountered in artificial intelligence applications, such as natural images, audio waveforms containing speech, and symbols in natural language corpora. So far, the most striking successes in deep learning have involved discriminative models, usually those that map a high-dimensional, rich sensory input to a class label [14, 22]. These striking successes have primarily been based on the backpropagation and dropout algorithms, using piecewise linear units [19, 9, 10] which have a particularly well-behaved gradient . Deep generative models have had less of an impact, due to the difficulty of approximating many intractable probabilistic computations that arise in maximum likelihood estimation and related strategies, and due to difficulty of leveraging the benefits of piecewise linear units in the generative context. We propose a new generative model estimation procedure that sidesteps these difficulties. 1\nIn the proposed adversarial nets framework, the generative model is pitted against an adversary: a discriminative model that learns to determine whether a sample is from the model distribution or the data distribution. The generative model can be thought of as analogous to a team of counterfeiters, trying to produce fake currency and use it without detection, while the discriminative model is analogous to the police, trying to detect the counterfeit currency. Competition in this game drives both teams to improve their methods until the counterfeits are indistiguishable from the genuine articles.\n\u2217Jean Pouget-Abadie is visiting Universite\u0301 de Montre\u0301al from Ecole Polytechnique. \u2020Sherjil Ozair is visiting Universite\u0301 de Montre\u0301al from Indian Institute of Technology Delhi \u2021Yoshua Bengio is a CIFAR Senior Fellow. 1All code and hyperparameters available at http://www.github.com/goodfeli/adversarial\nar X\niv :1\n40 6.\n26 61\nv1 [\nst at\n.M L\n] 1\n0 Ju\nn 20\nThis framework can yield specific training algorithms for many kinds of model and optimization algorithm. In this article, we explore the special case when the generative model generates samples by passing random noise through a multilayer perceptron, and the discriminative model is also a multilayer perceptron. We refer to this special case as adversarial nets. In this case, we can train both models using only the highly successful backpropagation and dropout algorithms [17] and sample from the generative model using only forward propagation. No approximate inference or Markov chains are necessary."
                },
                {
                    "heading": "2 Related work",
                    "text": "An alternative to directed graphical models with latent variables are undirected graphical models with latent variables, such as restricted Boltzmann machines (RBMs) [27, 16], deep Boltzmann machines (DBMs) [26] and their numerous variants. The interactions within such models are represented as the product of unnormalized potential functions, normalized by a global summation/integration over all states of the random variables. This quantity (the partition function) and its gradient are intractable for all but the most trivial instances, although they can be estimated by Markov chain Monte Carlo (MCMC) methods. Mixing poses a significant problem for learning algorithms that rely on MCMC [3, 5].\nDeep belief networks (DBNs) [16] are hybrid models containing a single undirected layer and several directed layers. While a fast approximate layer-wise training criterion exists, DBNs incur the computational difficulties associated with both undirected and directed models.\nAlternative criteria that do not approximate or bound the log-likelihood have also been proposed, such as score matching [18] and noise-contrastive estimation (NCE) [13]. Both of these require the learned probability density to be analytically specified up to a normalization constant. Note that in many interesting generative models with several layers of latent variables (such as DBNs and DBMs), it is not even possible to derive a tractable unnormalized probability density. Some models such as denoising auto-encoders [30] and contractive autoencoders have learning rules very similar to score matching applied to RBMs. In NCE, as in this work, a discriminative training criterion is employed to fit a generative model. However, rather than fitting a separate discriminative model, the generative model itself is used to discriminate generated data from samples a fixed noise distribution. Because NCE uses a fixed noise distribution, learning slows dramatically after the model has learned even an approximately correct distribution over a small subset of the observed variables.\nFinally, some techniques do not involve defining a probability distribution explicitly, but rather train a generative machine to draw samples from the desired distribution. This approach has the advantage that such machines can be designed to be trained by back-propagation. Prominent recent work in this area includes the generative stochastic network (GSN) framework [5], which extends generalized denoising auto-encoders [4]: both can be seen as defining a parameterized Markov chain, i.e., one learns the parameters of a machine that performs one step of a generative Markov chain. Compared to GSNs, the adversarial nets framework does not require a Markov chain for sampling. Because adversarial nets do not require feedback loops during generation, they are better able to leverage piecewise linear units [19, 9, 10], which improve the performance of backpropagation but have problems with unbounded activation when used ina feedback loop. More recent examples of training a generative machine by back-propagating into it include recent work on auto-encoding variational Bayes [20] and stochastic backpropagation [24]."
                },
                {
                    "heading": "3 Adversarial nets",
                    "text": "The adversarial modeling framework is most straightforward to apply when the models are both multilayer perceptrons. To learn the generator\u2019s distribution pg over data x, we define a prior on input noise variables pz(z), then represent a mapping to data space as G(z; \u03b8g), where G is a differentiable function represented by a multilayer perceptron with parameters \u03b8g . We also define a second multilayer perceptron D(x; \u03b8d) that outputs a single scalar. D(x) represents the probability that x came from the data rather than pg . We train D to maximize the probability of assigning the correct label to both training examples and samples fromG. We simultaneously trainG to minimize log(1\u2212D(G(z))):\nIn other words,D andG play the following two-player minimax game with value function V (G,D):\nmin G max D V (D,G) = Ex\u223cpdata(x)[logD(x)] + Ez\u223cpz(z)[log(1\u2212D(G(z)))]. (1)\nIn the next section, we present a theoretical analysis of adversarial nets, essentially showing that the training criterion allows one to recover the data generating distribution as G and D are given enough capacity, i.e., in the non-parametric limit. See Figure 1 for a less formal, more pedagogical explanation of the approach. In practice, we must implement the game using an iterative, numerical approach. Optimizing D to completion in the inner loop of training is computationally prohibitive, and on finite datasets would result in overfitting. Instead, we alternate between k steps of optimizing D and one step of optimizing G. This results in D being maintained near its optimal solution, so long as G changes slowly enough. This strategy is analogous to the way that SML/PCD [31, 29] training maintains samples from a Markov chain from one learning step to the next in order to avoid burning in a Markov chain as part of the inner loop of learning. The procedure is formally presented in Algorithm 1.\nIn practice, equation 1 may not provide sufficient gradient for G to learn well. Early in learning, when G is poor, D can reject samples with high confidence because they are clearly different from the training data. In this case, log(1 \u2212 D(G(z))) saturates. Rather than training G to minimize log(1\u2212D(G(z))) we can train G to maximize logD(G(z)). This objective function results in the same fixed point of the dynamics ofG andD but provides much stronger gradients early in learning.\npdata(x) pdata(x)+pg(x) . (c) After an update to G, gradient of D has guided G(z) to flow to regions that are more likely to be classified as data. (d) After several steps of training, if G and D have enough capacity, they will reach a point at which both cannot improve because pg = pdata. The discriminator is unable to differentiate between the two distributions, i.e. D(x) = 1\n2 ."
                },
                {
                    "heading": "4 Theoretical Results",
                    "text": "The generator G implicitly defines a probability distribution pg as the distribution of the samples G(z) obtained when z \u223c pz . Therefore, we would like Algorithm 1 to converge to a good estimator of pdata, if given enough capacity and training time. The results of this section are done in a nonparametric setting, e.g. we represent a model with infinite capacity by studying convergence in the space of probability density functions.\nWe will show in section 4.1 that this minimax game has a global optimum for pg = pdata. We will then show in section 4.2 that Algorithm 1 optimizes Eq 1, thus obtaining the desired result.\nAlgorithm 1 Minibatch stochastic gradient descent training of generative adversarial nets. The number of steps to apply to the discriminator, k, is a hyperparameter. We used k = 1, the least expensive option, in our experiments.\nfor number of training iterations do for k steps do \u2022 Sample minibatch of m noise samples {z(1), . . . ,z(m)} from noise prior pg(z). \u2022 Sample minibatch of m examples {x(1), . . . ,x(m)} from data generating distribution pdata(x). \u2022 Update the discriminator by ascending its stochastic gradient:\n\u2207\u03b8d 1\nm m\u2211 i=1 [ logD ( x(i) ) + log ( 1\u2212D ( G ( z(i) )))] .\nend for \u2022 Sample minibatch of m noise samples {z(1), . . . ,z(m)} from noise prior pg(z). \u2022 Update the generator by descending its stochastic gradient:\n\u2207\u03b8g 1\nm m\u2211 i=1 log ( 1\u2212D ( G ( z(i) ))) .\nend for The gradient-based updates can use any standard gradient-based learning rule. We used momentum in our experiments.\n4.1 Global Optimality of pg = pdata\nWe first consider the optimal discriminator D for any given generator G.\nProposition 1. For G fixed, the optimal discriminator D is\nD\u2217G(x) = pdata(x)\npdata(x) + pg(x) (2)\nProof. The training criterion for the discriminator D, given any generator G, is to maximize the quantity V (G,D)\nV (G,D) = \u222b x pdata(x) log(D(x))dx+ \u222b z pz(z) log(1\u2212D(g(z)))dz\n= \u222b x pdata(x) log(D(x)) + pg(x) log(1\u2212D(x))dx (3)\nFor any (a, b) \u2208 R2 \\ {0, 0}, the function y \u2192 a log(y) + b log(1 \u2212 y) achieves its maximum in [0, 1] at aa+b . The discriminator does not need to be defined outside of Supp(pdata) \u222a Supp(pg), concluding the proof.\nNote that the training objective for D can be interpreted as maximizing the log-likelihood for estimating the conditional probability P (Y = y|x), where Y indicates whether x comes from pdata (with y = 1) or from pg (with y = 0). The minimax game in Eq. 1 can now be reformulated as:\nC(G) =max D V (G,D)\n=Ex\u223cpdata [logD\u2217G(x)] + Ez\u223cpz [log(1\u2212D\u2217G(G(z)))] (4) =Ex\u223cpdata [logD\u2217G(x)] + Ex\u223cpg [log(1\u2212D\u2217G(x))]\n=Ex\u223cpdata [ log\npdata(x)\nPdata(x) + pg(x)\n] + Ex\u223cpg [ log\npg(x)\npdata(x) + pg(x)\n]\nTheorem 1. The global minimum of the virtual training criterion C(G) is achieved if and only if pg = pdata. At that point, C(G) achieves the value \u2212 log 4.\nProof. For pg = pdata, D\u2217G(x) = 1 2 , (consider Eq. 2). Hence, by inspecting Eq. 4 atD \u2217 G(x) = 1 2 , we find C(G) = log 12 + log 1 2 = \u2212 log 4. To see that this is the best possible value of C(G), reached only for pg = pdata, observe that\nEx\u223cpdata [\u2212 log 2] + Ex\u223cpg [\u2212 log 2] = \u2212 log 4\nand that by subtracting this expression from C(G) = V (D\u2217G, G), we obtain:\nC(G) = \u2212 log(4) +KL ( pdata \u2225\u2225\u2225\u2225pdata + pg2 ) +KL ( pg \u2225\u2225\u2225\u2225pdata + pg2 )\n(5)\nwhere KL is the Kullback\u2013Leibler divergence. We recognize in the previous expression the Jensen\u2013 Shannon divergence between the model\u2019s distribution and the data generating process:\nC(G) = \u2212 log(4) + 2 \u00b7 JSD (pdata \u2016pg ) (6)\nSince the Jensen\u2013Shannon divergence between two distributions is always non-negative and zero only when they are equal, we have shown that C\u2217 = \u2212 log(4) is the global minimum of C(G) and that the only solution is pg = pdata, i.e., the generative model perfectly replicating the data generating process."
                },
                {
                    "heading": "4.2 Convergence of Algorithm 1",
                    "text": "Proposition 2. IfG andD have enough capacity, and at each step of Algorithm 1, the discriminator is allowed to reach its optimum given G, and pg is updated so as to improve the criterion\nEx\u223cpdata [logD\u2217G(x)] + Ex\u223cpg [log(1\u2212D\u2217G(x))]\nthen pg converges to pdata\nProof. Consider V (G,D) = U(pg, D) as a function of pg as done in the above criterion. Note that U(pg, D) is convex in pg . The subderivatives of a supremum of convex functions include the derivative of the function at the point where the maximum is attained. In other words, if f(x) = sup\u03b1\u2208A f\u03b1(x) and f\u03b1(x) is convex in x for every \u03b1, then \u2202f\u03b2(x) \u2208 \u2202f if \u03b2 = arg sup\u03b1\u2208A f\u03b1(x). This is equivalent to computing a gradient descent update for pg at the optimal D given the corresponding G. supD U(pg, D) is convex in pg with a unique global optima as proven in Thm 1, therefore with sufficiently small updates of pg , pg converges to px, concluding the proof.\nIn practice, adversarial nets represent a limited family of pg distributions via the function G(z; \u03b8g), and we optimize \u03b8g rather than pg itself. Using a multilayer perceptron to define G introduces multiple critical points in parameter space. However, the excellent performance of multilayer perceptrons in practice suggests that they are a reasonable model to use despite their lack of theoretical guarantees."
                },
                {
                    "heading": "5 Experiments",
                    "text": "We trained adversarial nets an a range of datasets including MNIST[23], the Toronto Face Database (TFD) [28], and CIFAR-10 [21]. The generator nets used a mixture of rectifier linear activations [19, 9] and sigmoid activations, while the discriminator net used maxout [10] activations. Dropout [17] was applied in training the discriminator net. While our theoretical framework permits the use of dropout and other noise at intermediate layers of the generator, we used noise as the input to only the bottommost layer of the generator network.\nWe estimate probability of the test set data under pg by fitting a Gaussian Parzen window to the samples generated with G and reporting the log-likelihood under this distribution. The \u03c3 parameter\nof the Gaussians was obtained by cross validation on the validation set. This procedure was introduced in Breuleux et al. [8] and used for various generative models for which the exact likelihood is not tractable [25, 3, 5]. Results are reported in Table 1. This method of estimating the likelihood has somewhat high variance and does not perform well in high dimensional spaces but it is the best method available to our knowledge. Advances in generative models that can sample but not estimate likelihood directly motivate further research into how to evaluate such models.\nIn Figures 2 and 3 we show samples drawn from the generator net after training. While we make no claim that these samples are better than samples generated by existing methods, we believe that these samples are at least competitive with the better generative models in the literature and highlight the potential of the adversarial framework."
                },
                {
                    "heading": "6 Advantages and disadvantages",
                    "text": "This new framework comes with advantages and disadvantages relative to previous modeling frameworks. The disadvantages are primarily that there is no explicit representation of pg(x), and that D must be synchronized well with G during training (in particular, G must not be trained too much without updatingD, in order to avoid \u201cthe Helvetica scenario\u201d in whichG collapses too many values of z to the same value of x to have enough diversity to model pdata), much as the negative chains of a Boltzmann machine must be kept up to date between learning steps. The advantages are that Markov chains are never needed, only backprop is used to obtain gradients, no inference is needed during learning, and a wide variety of functions can be incorporated into the model. Table 2 summarizes the comparison of generative adversarial nets with other generative modeling approaches.\nThe aforementioned advantages are primarily computational. Adversarial models may also gain some statistical advantage from the generator network not being updated directly with data examples, but only with gradients flowing through the discriminator. This means that components of the input are not copied directly into the generator\u2019s parameters. Another advantage of adversarial networks is that they can represent very sharp, even degenerate distributions, while methods based on Markov chains require that the distribution be somewhat blurry in order for the chains to be able to mix between modes."
                },
                {
                    "heading": "7 Conclusions and future work",
                    "text": "This framework admits many straightforward extensions:\n1. A conditional generative model p(x | c) can be obtained by adding c as input to both G and D. 2. Learned approximate inference can be performed by training an auxiliary network to predict z\ngiven x. This is similar to the inference net trained by the wake-sleep algorithm [15] but with the advantage that the inference net may be trained for a fixed generator net after the generator net has finished training.\n3. One can approximately model all conditionals p(xS | x 6S) where S is a subset of the indices of x by training a family of conditional models that share parameters. Essentially, one can use adversarial nets to implement a stochastic extension of the deterministic MP-DBM [11]. 4. Semi-supervised learning: features from the discriminator or inference net could improve performance of classifiers when limited labeled data is available. 5. Efficiency improvements: training could be accelerated greatly by divising better methods for coordinating G and D or determining better distributions to sample z from during training.\nThis paper has demonstrated the viability of the adversarial modeling framework, suggesting that these research directions could prove useful."
                },
                {
                    "heading": "Acknowledgments",
                    "text": "We would like to acknowledge Patrice Marcotte, Olivier Delalleau, Kyunghyun Cho, Guillaume Alain and Jason Yosinski for helpful discussions. Yann Dauphin shared his Parzen window evaluation code with us. We would like to thank the developers of Pylearn2 [12] and Theano [7, 1], particularly Fre\u0301de\u0301ric Bastien who rushed a Theano feature specifically to benefit this project. Arnaud Bergeron provided much-needed support with LATEX typesetting. We would also like to thank CIFAR, and Canada Research Chairs for funding, and Compute Canada, and Calcul Que\u0301bec for providing computational resources. Ian Goodfellow is supported by the 2013 Google Fellowship in Deep Learning. Finally, we would like to thank Les Trois Brasseurs for stimulating our creativity."
                }
            ],
            "year": 2014,
            "references": [
                {
                    "title": "Theano: new features and speed improvements",
                    "authors": [
                        "F. Bastien",
                        "P. Lamblin",
                        "R. Pascanu",
                        "J. Bergstra",
                        "I.J. Goodfellow",
                        "A. Bergeron",
                        "N. Bouchard",
                        "Y. Bengio"
                    ],
                    "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop",
                    "year": 2012
                },
                {
                    "title": "Learning deep architectures for AI",
                    "authors": [
                        "Y. Bengio"
                    ],
                    "venue": "Now Publishers",
                    "year": 2009
                },
                {
                    "title": "Better mixing via deep representations",
                    "authors": [
                        "Y. Bengio",
                        "G. Mesnil",
                        "Y. Dauphin",
                        "S. Rifai"
                    ],
                    "year": 2013
                },
                {
                    "title": "Generalized denoising auto-encoders as generative models",
                    "authors": [
                        "Y. Bengio",
                        "L. Yao",
                        "G. Alain",
                        "P. Vincent"
                    ],
                    "venue": "In NIPS26. Nips Foundation",
                    "year": 2013
                },
                {
                    "title": "Deep generative stochastic networks trainable by backprop",
                    "authors": [
                        "Y. Bengio",
                        "E. Thibodeau-Laufer",
                        "J. Yosinski"
                    ],
                    "year": 2014
                },
                {
                    "title": "Deep generative stochastic networks trainable by backprop",
                    "authors": [
                        "Y. Bengio",
                        "E. Thibodeau-Laufer",
                        "G. Alain",
                        "J. Yosinski"
                    ],
                    "venue": "In Proceedings of the 30th International Conference on Machine Learning (ICML\u201914)",
                    "year": 2014
                },
                {
                    "title": "Theano: a CPU and GPU math expression compiler",
                    "authors": [
                        "J. Bergstra",
                        "O. Breuleux",
                        "F. Bastien",
                        "P. Lamblin",
                        "R. Pascanu",
                        "G. Desjardins",
                        "J. Turian",
                        "D. Warde-Farley",
                        "Y. Bengio"
                    ],
                    "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy). Oral Presentation",
                    "year": 2010
                },
                {
                    "title": "Quickly generating representative samples from an RBM-derived process",
                    "authors": [
                        "O. Breuleux",
                        "Y. Bengio",
                        "P. Vincent"
                    ],
                    "venue": "Neural Computation,",
                    "year": 2011
                },
                {
                    "title": "Deep sparse rectifier neural networks. In AISTATS\u20192011",
                    "authors": [
                        "X. Glorot",
                        "A. Bordes",
                        "Y. Bengio"
                    ],
                    "year": 2011
                },
                {
                    "title": "Multi-prediction deep Boltzmann machines",
                    "authors": [
                        "I.J. Goodfellow",
                        "M. Mirza",
                        "A. Courville",
                        "Y. Bengio"
                    ],
                    "year": 2013
                },
                {
                    "title": "Pylearn2: a machine learning research",
                    "authors": [
                        "I.J. Goodfellow",
                        "D. Warde-Farley",
                        "P. Lamblin",
                        "V. Dumoulin",
                        "M. Mirza",
                        "R. Pascanu",
                        "J. Bergstra",
                        "F. Bastien",
                        "Y. Bengio"
                    ],
                    "year": 2013
                },
                {
                    "title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models",
                    "authors": [
                        "M. Gutmann",
                        "A. Hyvarinen"
                    ],
                    "year": 2010
                },
                {
                    "title": "Deep neural networks for acoustic modeling in speech recognition",
                    "authors": [
                        "G. Hinton",
                        "L. Deng",
                        "G.E. Dahl",
                        "A. Mohamed",
                        "N. Jaitly",
                        "A. Senior",
                        "V. Vanhoucke",
                        "P. Nguyen",
                        "T. Sainath",
                        "B. Kingsbury"
                    ],
                    "venue": "IEEE Signal Processing Magazine,",
                    "year": 2012
                },
                {
                    "title": "The wake-sleep algorithm for unsupervised neural networks",
                    "authors": [
                        "G.E. Hinton",
                        "P. Dayan",
                        "B.J. Frey",
                        "R.M. Neal"
                    ],
                    "venue": "Science, 268,",
                    "year": 1995
                },
                {
                    "title": "A fast learning algorithm for deep belief nets",
                    "authors": [
                        "G.E. Hinton",
                        "S. Osindero",
                        "Y. Teh"
                    ],
                    "venue": "Neural Computation,",
                    "year": 2006
                },
                {
                    "title": "Improving neural networks by preventing co-adaptation of feature detectors",
                    "authors": [
                        "G.E. Hinton",
                        "N. Srivastava",
                        "A. Krizhevsky",
                        "I. Sutskever",
                        "R. Salakhutdinov"
                    ],
                    "venue": "Technical report,",
                    "year": 2012
                },
                {
                    "title": "Estimation of non-normalized statistical models using score matching",
                    "authors": [
                        "A. Hyv\u00e4rinen"
                    ],
                    "venue": "J. Machine Learning Res.,",
                    "year": 2005
                },
                {
                    "title": "What is the best multi-stage architecture for object recognition",
                    "authors": [
                        "K. Jarrett",
                        "K. Kavukcuoglu",
                        "M. Ranzato",
                        "Y. LeCun"
                    ],
                    "venue": "In Proc. International Conference on Computer Vision",
                    "year": 2009
                },
                {
                    "title": "Auto-encoding variational bayes",
                    "authors": [
                        "D.P. Kingma",
                        "M. Welling"
                    ],
                    "venue": "In Proceedings of the International Conference on Learning Representations (ICLR)",
                    "year": 2014
                },
                {
                    "title": "Learning multiple layers of features from tiny images",
                    "authors": [
                        "A. Krizhevsky",
                        "G. Hinton"
                    ],
                    "year": 2009
                },
                {
                    "title": "ImageNet classification with deep convolutional neural networks. In NIPS\u20192012",
                    "authors": [
                        "A. Krizhevsky",
                        "I. Sutskever",
                        "G. Hinton"
                    ],
                    "year": 2012
                },
                {
                    "title": "Gradient-based learning applied to document recognition",
                    "authors": [
                        "Y. LeCun",
                        "L. Bottou",
                        "Y. Bengio",
                        "P. Haffner"
                    ],
                    "venue": "Proceedings of the IEEE,",
                    "year": 1998
                },
                {
                    "title": "Stochastic backpropagation and approximate inference in deep generative models",
                    "authors": [
                        "D.J. Rezende",
                        "S. Mohamed",
                        "D. Wierstra"
                    ],
                    "venue": "Technical report,",
                    "year": 2014
                },
                {
                    "title": "A generative process for sampling contractive auto-encoders",
                    "authors": [
                        "S. Rifai",
                        "Y. Bengio",
                        "Y. Dauphin",
                        "P. Vincent"
                    ],
                    "year": 2012
                },
                {
                    "title": "Deep Boltzmann machines",
                    "authors": [
                        "R. Salakhutdinov",
                        "G.E. Hinton"
                    ],
                    "venue": "In AISTATS\u20192009,",
                    "year": 2009
                },
                {
                    "title": "Information processing in dynamical systems: Foundations of harmony theory",
                    "authors": [
                        "P. Smolensky"
                    ],
                    "venue": "Parallel Distributed Processing,",
                    "year": 1986
                },
                {
                    "title": "The Toronto face dataset",
                    "authors": [
                        "J. Susskind",
                        "A. Anderson",
                        "G.E. Hinton"
                    ],
                    "venue": "Technical Report UTML TR 2010-001,",
                    "year": 2010
                },
                {
                    "title": "Training restricted Boltzmann machines using approximations to the likelihood gradient",
                    "authors": [
                        "T. Tieleman"
                    ],
                    "year": 2008
                },
                {
                    "title": "Extracting and composing robust features with denoising autoencoders",
                    "authors": [
                        "P. Vincent",
                        "H. Larochelle",
                        "Y. Bengio",
                        "Manzagol",
                        "P.-A"
                    ],
                    "year": 2008
                },
                {
                    "title": "On the convergence of Markovian stochastic algorithms with rapidly decreasing ergodicity rates",
                    "authors": [
                        "L. Younes"
                    ],
                    "venue": "Stochastics and Stochastic Reports,",
                    "year": 1999
                }
            ],
            "id": "SP:86ee1835a56722b76564119437070782fc90eb19",
            "authors": [
                {
                    "name": "Ian J. Goodfellow",
                    "affiliations": []
                },
                {
                    "name": "Jean Pouget-Abadie",
                    "affiliations": []
                },
                {
                    "name": "Mehdi Mirza",
                    "affiliations": []
                },
                {
                    "name": "Bing Xu",
                    "affiliations": []
                },
                {
                    "name": "David Warde-Farley",
                    "affiliations": []
                },
                {
                    "name": "Sherjil Ozair",
                    "affiliations": []
                },
                {
                    "name": "Aaron Courville",
                    "affiliations": []
                },
                {
                    "name": "Yoshua Bengio",
                    "affiliations": []
                }
            ],
            "abstractText": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1 2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",
            "title": "Generative Adversarial Nets"
        },
        "Y": {
            "blog_id": "9dc0444142be8bd8a7404a226880eb",
            "summary": [
                "The paper proposes an adversarial approach for estimating generative models where one model (generative model) tries to learn a data distribution and another model (discriminative model) tries to distinguish between samples from the generative model and original data distribution.",
                "Adversarial Net  Two models - Generative Model(G) and Discriminative Model(D)  Both are multi-layer perceptrons.",
                "G takes as input a noise variable z and outputs data sample x(=G(z)).",
                "D takes as input a data sample x and predicts whether it came from true data or from G.  G tries to minimise log(1-D(G(z))) while D tries to maximise the probability of correct classification.",
                "Think of it as a minimax game between 2 players and the global optimum would be when G generates perfect samples and D can not distinguish between the samples (thereby always returning 0.5 as the probability of sample coming from true data).",
                "Alternate between k steps of training D and 1 step of training G so that D is maintained near its optimal solution.",
                "When starting training, the loss log(1-D(G(z))) would saturate as G would be weak.",
                "Instead maximise log(D(G(z)))  The paper contains the theoretical proof for global optimum of the minimax game.",
                "Experiments  Datasets  MNIST, Toronto Face Database, CIFAR-10  Generator model uses RELU and sigmoid activations.",
                "Discriminator model uses maxout and dropout.",
                "Evaluation Metric  Fit Gaussian Parzen window to samples obtained from G and compare log-likelihood.",
                "Strengths  Computational advantages  Backprop is sufficient for training with no need for Markov chains or performing inference.",
                "A variety of functions can be used in the model.",
                "Since G is trained only using the gradients from D, fewer chances of directly copying features from the true data.",
                "Can represent sharp (even degenerate) distributions.",
                "Weakness  D must be well synchronised with G.  While G may learn to sample data points that are indistinguishable from true data, no explicit representation can be obtained.",
                "Possible Extensions  Conditional generative models.",
                "Inference network to predict z given x.",
                "Implement a stochastic extension of the deterministic Multi-Prediction Deep Boltzmann Machines  Using discriminator net or inference net for feature selection.",
                "Accelerating training by ensuring better coordination between G and D or by determining better distributions to sample z from during training."
            ],
            "author_id": "shugan",
            "pdf_url": "https://arxiv.org/pdf/1406.2661",
            "author_full_name": "Shagun Sodhani",
            "source_website": "https://github.com/shagunsodhani/papers-I-read",
            "id": 51450104
        }
    },
    "36679338": {
        "X": {
            "sections": [
                {
                    "text": "cies derived from Bitcoin, Zcash is often touted as the one with the strongest anonymity guarantees, due to its basis in well-regarded cryptographic research. In this paper, we examine the extent to which anonymity is achieved in the deployed version of Zcash. We investigate all facets of anonymity in Zcash\u2019s transactions, ranging from its transparent transactions to the interactions with and within its main privacy feature, a shielded pool that acts as the anonymity set for users wishing to spend coins privately. We conclude that while it is possible to use Zcash in a private way, it is also possible to shrink its anonymity set considerably by developing simple heuristics based on identifiable patterns of usage."
                },
                {
                    "heading": "1 Introduction",
                    "text": "Since the introduction of Bitcoin in 2008 [34], cryptocurrencies have become increasingly popular to the point of reaching a near-mania, with thousands of deployed cryptocurrencies now collectively attracting trillions of dollars in investment. While the broader positive potential of \u201cblockchain\u201d (i.e., the public decentralized ledger underlying almost all cryptocurrencies) is still unclear, despite the growing number of legitimate users there are today still many people using these cryptocurrencies for less legitimate purposes. These range from the purchase of drugs or other illicit goods on so-called dark markets such as Dream Market, to the payments from victims in ransomware attacks such as WannaCry, with many other crimes in between. Criminals engaged in these activities may be drawn to Bitcoin due to the relatively low friction of making international payments using only pseudonyms as identifiers, but the public nature of its ledger of transactions raises the question of how much anonymity is actually being achieved.\nIndeed, a long line of research [37, 38, 12, 27, 40] has by now demonstrated that the use of pseudonymous ad-\ndresses in Bitcoin does not provide any meaningful level of anonymity. Beyond academic research, companies now provide analysis of the Bitcoin blockchain as a business [19]. This type of analysis was used in several arrests associated with the takedown of Silk Road [20], and to identify the attempts of the WannaCry hackers to move their ransom earnings from Bitcoin into Monero [17].\nPerhaps in response to this growing awareness that most cryptocurrencies do not have strong anonymity guarantees, a number of alternative cryptocurrencies or other privacy-enhancing techniques have been deployed with the goal of improving on these guarantees. The most notable cryptocurrencies that fall into this former category are Dash [2] (launched in January 2014), Monero [3] (April 2014), and Zcash [7] (October 2016). At the time of this writing all have a market capitalization of over 1 billion USD [1], although this figure is notoriously volatile, so should be taken with a grain of salt.\nEven within this category of privacy-enhanced cryptocurrencies, and despite its relative youth, Zcash stands somewhat on its own. From an academic perspective, Zcash is backed by highly regarded research [28, 13], and thus comes with seemingly strong anonymity guarantees. Indeed, the original papers cryptographically prove the security of the main privacy feature of Zcash (known as the shielded pool), in which users can spend shielded coins without revealing which coins they have spent. These strong guarantees have attracted at least some criminal attention to Zcash: the underground marketplace AlphaBay was on the verge of accepting it before their shutdown in July 2017 [11], and the Shadow Brokers hacking group started accepting Zcash in May 2017 (and in fact for their monthly dumps accepted exclusively Zcash in September 2017) [16].\nDespite these theoretical privacy guarantees, the deployed version of Zcash does not require all transactions to take place within the shielded pool itself: it also supports so-called transparent transactions, which are essentially the same as transactions in Bitcoin in\nUSENIX Association 27th USENIX Security Symposium 463\nthat they reveal the pseudonymous addresses of both the senders and recipients, and the amount being sent. It does require, however, that all newly generated coins pass through the shielded pool before being spent further, thus ensuring that all coins have been shielded at least once. This requirement led the Zcash developers to conclude that the anonymity set for users spending shielded coins is in fact all generated coins, and thus that \u201cthe mixing strategies that other cryptocurrencies use for anonymity provide a rather small [anonymity set] in comparison to Zcash\u201d and that \u201cZcash has a distinct advantage in terms of transaction privacy\u201d [9].\nIn this paper, we provide the first in-depth empirical analysis of anonymity in Zcash, in order to examine these claims and more generally provide a longitudinal study of how Zcash has evolved and who its main participants are. We begin in Section 4 by providing a general examination of the Zcash blockchain, from which we observe that the vast majority of Zcash activity is in the transparent part of the blockchain, meaning it does not engage with the shielded pool at all. In Section 5, we explore this aspect of Zcash by adapting the analysis that has already been developed for Bitcoin, and find that exchanges typically dominate this part of the blockchain.\nWe then move in Section 6 to examining interactions with the shielded pool. We find that, unsurprisingly, the main actors doing so are the founders and miners, who are required to put all newly generated coins directly into it. Using newly developed heuristics for attributing transactions to founders and miners, we find that 65.6% of the value withdrawn from the pool can be linked back to deposits made by either founders or miners. We also implement a general heuristic for linking together other types of transactions, and capture an additional 3.5% of the value using this. Our relatively simple heuristics thus reduce the size of the overall anonymity set by 69.1%.\nIn Section 7, we then look at the relatively small percentage of transactions that have taken place within the shielded pool. Here, we find (perhaps unsurprisingly) that relatively little information can be inferred, although we do identify certain patterns that may warrant further investigation. Finally, we perform a small case study of the activities of the Shadow Brokers within Zcash in Section 8, and in Section 9 we conclude.\nAll of our results have been disclosed, at the time of the paper\u2019s submission, to the creators of Zcash, and discussed extensively with them since. This has resulted in changes to both their public communication about Zcash\u2019s anonymity as well as the transactional behavior of the founders. Additionally, all the code for our analysis is available as an open-source repository.1\n1https://github.com/manganese/zcash-empirical-analysis"
                },
                {
                    "heading": "2 Related work",
                    "text": "We consider as related all work that has focused on the anonymity of cryptocurrencies, either by building solutions to achieve stronger anonymity guarantees or by demonstrating its limits.\nIn terms of the former, there has been a significant volume of research in providing solutions for existing cryptocurrencies that allow interested users to mix their coins in a way that achieves better anonymity than regular transactions [15, 41, 21, 24, 39, 14, 22, 25]. Another line of research has focused on producing alternative privacy-enhanced cryptocurrencies. Most notably, Dash [2] incorporates the techniques of CoinJoin [24] in its PrivateSpend transactions; Monero [3, 35] uses ring signatures to allow users to create \u201cmix-ins\u201d (i.e., include the keys of other users in their own transactions as a way of providing a larger anonymity set); and Zcash [7, 13] uses zero-knowledge proofs to allow users to spend coins without revealing which coins are being spent.\nIn terms of the latter, there has also been a significant volume of research on de-anonymizing Bitcoin [37, 38, 12, 27, 40]. Almost all of these attacks follow the same pattern: they first apply so-called clustering heuristics that associate multiple different addresses with one single entity, based on some evidence of shared ownership. The most common assumption is that all input addresses in a transaction belong to the same entity, with some papers [12, 27] also incorporating an additional heuristic in which output addresses receiving change are also linked. Once these clusters are formed, a \u201cre-identification attack\u201d [27] then tags specific addresses and thus the clusters in which they are contained. These techniques have also been applied to alternative cryptocurrencies with similar types of transactions, such as Ripple [30].\nThe work that is perhaps closest to our own focuses on de-anonymizing the privacy solutions described above, rather than just on Bitcoin. Here, several papers have focused on analyzing so-called privacy overlays or mixing services for Bitcoin [33, 26, 31, 32], and considered both their level of anonymity and the extent to which participants must trust each other. Some of this analysis [32, 26] also has implications for anonymity in Dash, due to its focus on CoinJoin. More recently, Miller et al. [29] and Kumar et al. [23] looked at Monero. They both found that it was possible to link together transactions based on temporal patterns, and also based on certain patterns of usage, such as users who choose to do transactions with 0 mix-ins (in which case their ring signature provides no anonymity, which in turns affects other users who may have included their key in their own mix-ins). Finally, we are aware of one effort to de-anonymize Zcash, by Quesnelle [36]. This article focuses on linking together the transactions used to shield\n464 27th USENIX Security Symposium USENIX Association\nand deshield coins, based on their timing and the amount sent in the transactions. In comparison, our paper implements this heuristic but also provides a broader perspective on the entire Zcash ecosystem, as well as a more in-depth analysis of all interactions with (and within) the shielded pool."
                },
                {
                    "heading": "3 Background",
                    "text": ""
                },
                {
                    "heading": "3.1 How Zcash works",
                    "text": "Zcash (ZEC) is an alternative cryptocurrency developed as a (code) fork of Bitcoin that aims to break the link between senders and recipients in a transaction. In Bitcoin, recipients receive funds into addresses (referred to as the vOut in a transaction), and when they spend them they do so from these addresses (referred to as the vIn in a transaction). The act of spending bitcoins thus creates a link between the sender and recipient, and these links can be followed as bitcoins continue to change hands. It is thus possible to track any given bitcoin from its creation to its current owner.\nAny transaction which interacts with the so-called shielded pool in Zcash does so through the inclusion of a vJoinSplit, which specifies where the coins are coming from and where they are going. To receive funds, users can provide either a transparent address (t-address) or a shielded address (z-address). Coins that are held in z-addresses are said to be in the shielded pool.\nTo specify where the funds are going, a vJoinSplit contains (1) a list of output t-addresses with funds assigned to them (called zOut), (2) two shielded outputs, and (3) an encrypted memo field. The zOut can be empty, in which case the transaction is either shielded (tto-z) or private (z-to-z), depending on the inputs. If the zOut list contains a quantity of ZEC not assigned to any address, then we still consider it to be empty (as this is simply the allocation of the miner\u2019s fee). Each shielded\noutput contains an unknown quantity of ZEC as well as a hidden double-spending token. The shielded output can be a dummy output (i.e., it contains zero ZEC) to hide the fact that there is no shielded output. The encrypted memo field can be used to send private messages to the recipients of the shielded outputs.\nTo specify where the funds are coming from, a vJoinSplit also contains (1) a list of input t-addresses (called zIn), (2) two double-spending tokens, and (3) a zeroknowledge proof. The zIn can be empty, in which case the transaction is either deshielded (z-to-t) if zOut is not empty, or private (z-to-z) if it is. Each double-spending token is either a unique token belonging to some previous shielded output, or a dummy value used to hide the fact that there is no shielded input. The doublespending token does not reveal to which shielded output it belongs. The zero-knowledge proof guarantees two things. First, it proves that the double-spending token genuinely belongs to some previous shielded output. Second, it proves that the sum of (1) the values in the addresses in zIn plus (2) the values represented by the double-spending tokens is equal to the sum of (1) the values assigned to the addresses in zOut plus (2) the values in the shielded outputs plus (3) the miner\u2019s fee. A summary of the different types of transactions is in Figure 1."
                },
                {
                    "heading": "3.2 Participants in the Zcash ecosystem",
                    "text": "In this section we describe four types of participants who interact in the Zcash network.\nFounders took part in the initial creation and release of Zcash, and will receive 20% of all newly generated coins (currently 2.5 ZEC out of the 12.5 ZEC block reward). The founder addresses are specified in the Zcash chain parameters [8].\nMiners take part in the maintenance of the ledger, and in doing so receive newly generated coins (10 out of the 12.5 ZEC block reward), as well as any fees from the transactions included in the blocks they mine. Many miners choose not to mine on their own, but join a mining pool; a list of mining pools can be found in Table 4. One or many miners win each block, and the first transaction in the block is a coin generation (coingen) that assigns newly generated coins to their address(es), as well as to the address(es) of the founders.\nServices are entities that accept ZEC as some form of payment. These include exchanges like Bitfinex, which allow users to trade fiat currencies and other cryptocurrencies for ZEC (and vice versa), and platforms like ShapeShift [4], which allow users to trade within cryptocurrencies and other digital assets without requiring registration.\nFinally, users are participants who hold and transact in ZEC at a more individual level. In addition to regu-\nUSENIX Association 27th USENIX Security Symposium 465\nlar individuals, this category includes charities and other organizations that may choose to accept donations in Zcash. A notable user is the Shadow Brokers, a hacker group who have published several leaks containing hacking tools from the NSA and accept payment in Zcash. We explore their usage of Zcash in Section 8."
                },
                {
                    "heading": "4 General Blockchain Statistics",
                    "text": "We used the zcashd client to download the Zcash blockchain, and loaded a database representation of it into Apache Spark. We then performed our analysis using a custom set of Python scripts equipped with PySpark. We last parsed the block chain on January 21 2018, at which point 258,472 blocks had been mined. Overall, 3,106,643 ZEC had been generated since the genesis block, out of which 2,485,461 ZEC went to the miners and the rest (621,182 ZEC) went to the founders."
                },
                {
                    "heading": "4.1 Transactions",
                    "text": "Across all blocks, there were 2,242,847 transactions. A complete breakdown of the transaction types is in Table 1, and graphs depicting the growth of each transaction type over time are in Figures 2 and 3.2 The vast majority of transactions are public (i.e., either transparent or a coin generation). Of the transactions that do interact with the pool (335,630, or 14.96%, in total), only a very small percentage are private transactions; i.e., transactions within the pool. Looking at the types of transactions over time in Figure 2, we can see that the number of coingen, shielded, and deshielded transactions all grow in an approximately linear fashion. As we explore in Section 6.2, this correlation is due largely to the habits of the miners. Looking at both this figure and Figure 3, we can see that while the number of transactions interacting with the pool has grown in a relatively linear fashion, the value they carry has over time become a very small percentage of all blocks, as more mainstream (and thus transparent) usage of Zcash has increased.\n2We use the term \u2018mixed\u2019 to mean transactions that have both a vIn and a vOut, and a vJoinSplit."
                },
                {
                    "heading": "4.2 Addresses",
                    "text": "Across all transactions, there have been 1,740,378 distinct t-addresses used. Of these, 8,727 have ever acted as inputs in a t-to-z transaction and 330,780 have ever acted as outputs in a z-to-t transaction. As we explore in Section 6.2, much of this asymmetry is due to the behavior of mining pools, which use a small number of addresses to collect the block reward, but a large number of addresses (representing all the individual miners) to pay out of the pool. Given the nature of the shielded pool, it is not possible to know the total number of z-addresses used.\nFigure 4 shows the total value in the pool over time. Although the overall value is increasing over time, there are certain shielding and de-shielding patterns that create spikes. As we explore in Section 6, these spikes are due largely to the habits of the miners and founders. At the time of writing, there are 112,235 ZEC in the pool, or 3.6% of the total monetary supply.\nIf we rank addresses by their wealth, we first observe that only 25% of all t-addresses have a non-zero bal-\n466 27th USENIX Security Symposium USENIX Association\nance. Of these, the top 1% hold 78% of all ZEC. The address with the highest balance had 118,257.75 ZEC, which means the richest address has a higher balance than the entire shielded pool."
                },
                {
                    "heading": "5 T-Address Clustering",
                    "text": "As discussed in Section 4, a large proportion of the activity on Zcash does not use the shielded pool. This means it is essentially identical to Bitcoin, and thus can be deanonymized using the same techniques discussed for Bitcoin in Section 2."
                },
                {
                    "heading": "5.1 Clustering addresses",
                    "text": "To identify the usage of transparent addresses, we begin by recalling the \u201cmulti-input\u201d heuristic for clustering Bitcoin addresses. In this heuristic, addresses that are used as inputs to the same transaction are assigned to the same cluster. In Bitcoin, this heuristic can be applied to all transactions, as they are all transparent. In Zcash, we perform this clustering as long as there are multiple input t-addresses.\nHeuristic 1. If two or more t-addresses are inputs in the same transaction (whether that transaction is transparent, shielded, or mixed), then they are controlled by the same entity.\nIn terms of false positives, we believe that these are at least as unlikely for Zcash as they are for Bitcoin, as Zcash is a direct fork of Bitcoin and the standard client has the same behavior. In fact, we are not aware of any input-mixing techniques like CoinJoin [24] for Zcash, so could argue that the risk of false positives is even lower than it is for Bitcoin. As this heuristic has already been used extensively in Bitcoin, we thus believe it to be realistic for use in Zcash.\nWe implemented this heuristic by defining each taddress as a node in a graph, and adding an (undirected)\nedge in the graph between addresses that had been input to the same transaction. The connected components of the graph then formed the clusters, which represent distinct entities controlling potentially many addresses. The result was a set of 560,319 clusters, of which 97,539 contained more than a single address.\nAs in Bitcoin, using just this one heuristic is already quite effective but does not capture the common usage of change addresses, in which a transaction sends coins to the actual recipient but then also sends any coins left over in the input back to the sender. Meiklejohn et al. [27] use in their analysis a heuristic based on this behavior, but warn that it is somewhat fragile. Indeed, their heuristic seems largely dependent on the specific behavior of several large Bitcoin services, so we chose not to implement it in its full form. Nevertheless, we did use a related Zcash-specific heuristic in our case study of the Shadow Brokers in Section 8.\nHeuristic 2. If one (or more) address is an input taddress in a vJoinSplit transaction and a second address is an output t-address in the same vJoinSplit transaction, then if the size of zOut is 1 (i.e., this is the only transparent output address), the second address belongs to the same user who controls the input addresses.\nTo justify this heuristic, we observe that users may not want to deposit all of the coins in their address when putting coins into the pool, in which case they will have to make change. The only risk of a false positive is if users are instead sending money to two separate individuals, one using a z-address and one using a t-address. One notable exception to this rule is users of the zcash4win wallet. Here, the address of the wallet operator is an output t-address if the user decides to pay the developer fee, so would produce exactly this type of transaction for users putting money into the shielded pool. This address is identifiable, however, so these types of transactions can be omitted from our analysis. Nevertheless, due to concerns about the safety of this heuristic (i.e., its ability to avoid false positives), we chose not to incorporate it into our general analysis below."
                },
                {
                    "heading": "5.2 Tagging addresses",
                    "text": "Having now obtained a set of clusters, we next sought to assign names to them. To accomplish this, we performed a scaled-down version of the techniques used by Meiklejohn et al. [27]. In particular, given that Zcash is still relatively new, there are not many different types of services that accept Zcash. We thus restricted ourselves to interacting with exchanges.\nWe first identified the top ten Zcash exchanges according to volume traded [1]. We then created an account with each exchange and deposited a small quantity of\nUSENIX Association 27th USENIX Security Symposium 467\nZEC into it, tagging as we did the output t-addresses in the resulting transaction as belonging to the exchange. We then withdrew this amount to our own wallet, and again tagged the t-addresses (this time on the sender side) as belonging to the exchange. We occasionally did several deposit transactions if it seemed likely that doing so would tag more addresses. Finally, we also interacted with ShapeShift, which as mentioned in Section 3.2 allows users to move amongst cryptocurrencies without the need to create an account. Here we did a single \u201cshift\u201d into Zcash and a single shift out. A summary of our interactions with all the different exchanges is in Table 2.\nFinally, we collected the publicized addresses of the founders [8], as well as addresses from known mining pools. For the latter we started by scraping the tags of these addresses from the Zchain explorer [10]. We then validated them against the blocks advertised on some of the websites of the mining pools themselves (which we also scraped) to ensure that they were the correct tags; i.e., if the recipient of the coingen transaction in a given block was tagged as belonging to a given mining pool, then we checked to see that the block had been advertised on the website of that mining pool. We then augmented these sets of addresses with the addresses tagged as belonging to founders and miners according to the heuristics developed in Section 6. We present these heuristics in significantly more detail there, but they resulted in us tagging 123 founder addresses and 110,918 miner addresses (belonging to a variety of different pools)."
                },
                {
                    "heading": "5.3 Results",
                    "text": "As mentioned in Section 5.1, running Heuristic 1 resulted in 560,319 clusters, of which 97,539 contained more than a single address. We assigned each cluster\na unique identifier, ordered by the number of addresses in the cluster, so that the biggest cluster had identifier 0."
                },
                {
                    "heading": "5.3.1 Exchanges and wallets",
                    "text": "As can be seen in Table 2, many of the exchanges are associated with some of the biggest clusters, with four out of the top five clusters belonging to popular exchanges. In general, we found that the top five clusters accounted for 11.21% of all transactions. Identifying exchanges is important, as it makes it possible to discover where individual users may have purchased their ZEC. Given existing and emerging regulations, they are also the one type of participant in the Zcash ecosystem that might know the real-world identity of users.\nIn many of the exchange clusters, we also identified large fractions of addresses that had been tagged as miners. This implies that individual miners use the addresses of their exchange accounts to receive their mining reward, which might be expected if their goal is to cash out directly. We found some, but far fewer, founder addresses at some of the exchanges as well.\nOur clustering also reveals that ShapeShift (Cluster 2) is fairly heavily used: it had received over 1.1M ZEC in total and sent roughly the same. Unlike the exchanges, its cluster contained a relatively small number of miner addresses (54), which fits with its usage as a way to shift money, rather than hold it in a wallet."
                },
                {
                    "heading": "5.3.2 Mining pools and founders",
                    "text": "Although mining pools and founders account for a large proportion of the activity in Zcash (as we explore in Section 6), many re-use the same small set of addresses frequently, so do not belong to large clusters. For example, Flypool had three single-address clusters while Coinotron, coinmine.pl, Slushpool and Nanopool each had two single-address clusters. (A list of mining pools can be found in Table 4 in Section 6.2). Of the coins that we saw sent from clusters associated with mining pools, 99.8% of it went into the shielded pool, which further validates both our clustering and tagging techniques."
                },
                {
                    "heading": "5.3.3 Philanthropists",
                    "text": "Via manual inspection, we identified three large organizations that accept Zcash donations: the Internet Archive, torservers.net, and Wikileaks. Of these, torservers.net accepts payment only via a z-address, so we cannot identify their transactions (Wikileaks accepts payment via a z-address too, but also via a taddress). Of the 31 donations to the Internet Archive that we were able to identify, which totaled 17.3 ZEC, 9 of them were made anonymously (i.e., as z-to-t transactions). On the other hand, all of the 20 donations to Wik-\n468 27th USENIX Security Symposium USENIX Association\nileak\u2019s t-address were made as t-to-t transactions. None of these belong to clusters, as they have never sent a transaction."
                },
                {
                    "heading": "6 Interactions with the Shielded Pool",
                    "text": "What makes Zcash unique is of course not its t-addresses (since these essentially replicate the functionality of Bitcoin), but its shielded pool. To that end, this section explores interactions with the pool at its endpoints, meaning the deposits into (t-to-z) and withdrawals out of the pool (z-to-t). We then explore interactions within the pool (z-to-z transactions) in Section 7.\nTo begin, we consider just the amounts put into and taken out of the pool. Over time, 3,901,124 ZEC have been deposited into the pool,3 and 3,788,889 have been withdrawn. Figure 5 plots both deposits and withdrawals over time.\nThis figure shows a near-perfect reflection of deposits and withdrawals, demonstrating that most users not only withdraw the exact number of ZEC they deposit into the pool, but do so very quickly after the initial deposit. As we see in Sections 6.1 and 6.2, this phenomenon is accounted for almost fully by the founders and miners. Looking further at the figure, we can see that the symmetry is broken occasionally, and most notably in four \u201cspikes\u201d: two large withdrawals, and two large deposits. Some manual investigation revealed the following:\n\u201cThe early birds\u201d The first withdrawal spike took place at block height 30,900, which was created in December 2016. The cause of the spike was a single transaction in which 7,135 ZEC was taken out of the pool; given the exchange rate at that time of 34 USD per ZEC, this was equivalent to 242,590 USD. The coins were distributed across 15 t-addresses, which initially\n3This is greater than the total number of generated coins, as all coins must be deposited into the pool at least once, by the miners or founders, but may then go into and out of the pool multiple times.\nwe had not tagged as belonging to any named user. After running the heuristic described in Section 6.1, however, we tagged all of these addresses as belonging to founders. In fact, this was the very first withdrawal that we identified as being associated with founders.\n\u201cSecret Santa\u201d The second withdrawal spike took place on December 25 2017, at block height 242,642. In it, 10,000 ZEC was distributed among 10 different t-addresses, each receiving 1,000 ZEC. None of these t-addresses had done a transaction before then, and none have been involved in one since (i.e., the coins received in this transaction have not yet been spent).\n\u201cOne-man wolf packs\u201d Both of the deposit spikes in the graph correspond to single large deposits from unknown t-addresses that, using our analysis from Section 5, we identified as residing in single-address clusters. For the first spike, however, many of the deposited amounts came directly from a founder address identified by our heuristics (Heuristic 3), so given our analysis in Section 6.1 we believe this may also be associated with the founders.\nWhile this figure already provides some information about how the pool is used (namely that most of the money put into it is withdrawn almost immediately afterwards), it does not tell us who is actually using the pool. For this, we attempt to associate addresses with the types of participants identified in Section 3.2: founders, miners, and \u2018other\u2019 (encompassing both services and individual users).\nWhen considering deposits into the shielded pool, it is easy to associate addresses with founders and miners, as the consensus rules dictate that they must put their block rewards into the shielded pool before spending them further. As described in Section 5.2, we tagged founders according to the Zcash parameters, and tagged as miners all recipients of coingen transactions that were not founders. We then used these tags to identify a founder deposit as any t-to-z transaction using one or more founder addresses as input, and a miner deposit as any t-to-z transaction using one or more miner addresses as input. The results are in Figure 6.\nLooking at this figure, it is clear that miners are the main participants putting money into the pool. This is not particularly surprising, given that all the coins they receive must be deposited into the pool at least once, so if we divide that number of coins by the total number deposited we would expect at least 63.7% of the deposits to come from miners. (The actual number is 76.7%.) Founders, on the other hand, don\u2019t put as much money into the pool (since they don\u2019t have as much to begin with), but when they do they put in large amounts that cause visible step-like fluctuations to the overall line.\nUSENIX Association 27th USENIX Security Symposium 469\nIn terms of the heaviest users, we looked at the individual addresses that had put more than 10,000 ZEC into the pool. The results are in Figure 7.\nIn fact, this figure incorporates the heuristics we develop in Sections 6.1 and 6.2, although it looked very similar when we ran it before applying our heuristics (which makes sense, since our heuristics mainly act to link z-to-t transactions). Nevertheless, it demonstrates again that most of the heavy users of the pool are miners, with founders also depositing large amounts but spreading them over a wider variety of addresses. Of the four \u2018other\u2019 addresses, one of them belonged to ShapeShift, and the others belong to untagged clusters.\nWhile it is interesting to look at t-to-z transactions on their own, the main intention of the shielded pool is to provide an anonymity set, so that when users withdraw their coins it is not clear whose coins they are. In that sense, it is much more interesting to link together t-to-z and z-to-t transactions, which acts to reduce the anonymity set. More concretely, if a t-to-z transaction can be linked to a z-to-t transaction, then those coins can\nbe \u201cruled out\u201d of the anonymity set of future users withdrawing coins from the pool. We thus devote our attention to this type of analysis for the rest of the section.\nThe most na\u0131\u0308ve way to link together these transactions would be to see if the same addresses are used across them; i.e., if a miner uses the same address to withdraw their coins as it did to deposit them. By running this simple form of linking, we see the results in Figure 8a. This figure shows that we are not able to identify any withdrawals as being associated with founders, and only a fairly small number as associated with miners: 49,280 transactions in total, which account for 13.3% of the total value in the pool.\nNevertheless, using heuristics that we develop for identifying founders (as detailed in Section 6.1) and miners (Section 6.2), we are able to positively link most of the z-to-t activity with one of these two categories, as seen in Figures 8b and 8c. In the end, of the 177,009 zto-t transactions, we were able to tag 120,629 (or 68%) of them as being associated with miners, capturing 52.1% of the value coming out of the pool, and 2,103 of them as being associated with founders (capturing 13.5% of the value). We then examine the remaining 30-35% of the activity surrounding the shielded pool in Section 6.3."
                },
                {
                    "heading": "6.1 Founders",
                    "text": "After comparing the list of founder addresses against the outputs of all coingen transactions, we found that 14 of them had been used. Using these addresses, we were able to identify founder deposits into the pool, as already shown in Figure 6. Table 3 provides a closer inspection of the usage of each of these addresses.\nThis table shows some quite obvious patterns in the behavior of the founders. At any given time, only one address is \u201cactive,\u201d meaning it receives rewards and deposits them into the pool. Once it reaches the limit of 44,272.5 ZEC, the next address takes its place and it is not used again. This pattern has held from the third address onwards. What\u2019s more, the amount deposited was often the same: exactly 249.9999 ZEC, which is roughly the reward for 100 blocks. This was true of 74.9% of all founder deposits, and 96.2% of all deposits from the third address onwards. There were only ever five other deposits into the pool carrying value between 249 and 251 ZEC (i.e., carrying a value close but not equal to 249.9999 ZEC).\nThus, while we were initially unable to identify any withdrawals associated with the founders (as seen in Figure 8a), these patterns indicated an automated use of the shielded pool that might also carry into the withdrawals. Upon examining the withdrawals from the pool, we did not find any with a value exactly equal to 249.9999 ZEC. We did, however, find 1,953 withdrawals\n470 27th USENIX Security Symposium USENIX Association\nof exactly 250.0001 ZEC (and 1,969 carrying a value between 249 and 251 ZEC, although we excluded the extra ones from our analysis).\nThe value alone of these withdrawals thus provides some correlation with the deposits, but to further explore it we also looked at the timing of the transactions. When we examined the intervals between consecutive deposits of 249.9999 ZEC, we found that 85% happened within 6-10 blocks of the previous one. Similarly, when examining the intervals between consecutive withdrawals of 250.0001 ZEC, we found that 1,943 of the 1,953 withdrawals also had a proximity of 6-10 blocks. Indeed, both the deposits and the withdrawals proceeded in step-like patterns, in which many transactions were made within a very small number of blocks (resulting in the step up), at which point there would be a pause while more block rewards were accumulated (the step across). This pattern is visible in Figure 9, which shows the deposit and withdrawal transactions associated with the founders. Deposits are typically made in few large\nsteps, whereas withdrawals take many smaller ones.\nHeuristic 3. Any z-to-t transaction carrying 250.0001 ZEC in value is done by the founders.\nIn terms of false positives, we cannot truly know how risky this heuristic is, short of asking the founders. This is in contrast to the t-address clustering heuristics presented in Section 5, in which we were not attempting to assign addresses to a specific owner, so could validate the heuristics in other ways. Nevertheless, the high correlation between both the value and timing of the transactions led us to believe in the reliability of this heuristic.\nAs a result of running this heuristic, we added 75 more addresses to our initial list of 48 founder addresses (of which, again, only 14 had been used). Aside from the correlation showed in Figure 9, the difference in terms of our ability to tag founder withdrawals is seen in Figure 8b."
                },
                {
                    "heading": "6.2 Miners",
                    "text": "The Zcash protocol specifies that all newly generated coins are required to be put into the shielded pool before they can be spent further. As a result, we expect that a large quantity of the ZEC being deposited into the pool are from addresses associated with miners.\nUSENIX Association 27th USENIX Security Symposium 471"
                },
                {
                    "heading": "6.2.1 Deposits",
                    "text": "As discussed earlier and seen in Figure 6, it is easy to identify miner deposits into the pool due to the fact that they immediately follow a coin generation. Before going further, we split the category of miners into individual miners, who operate on their own, and mining pools, which represent collectives of potentially many individuals. In total, we gathered 19 t-addresses associated with Zcash mining pools, using the scraping methods described in Section 5.2. Table 4 lists these mining pools, as well as the number of addresses they control and the number of t-to-z transactions we associated with them. Figure 10 plots the value of their deposits into the shielded pool over time.\nIn this figure, we can clearly see that the two dominant mining pools are Flypool and F2Pool. Flypool consistently deposits the same (or similar) amounts, which we can see in their linear representation. F2Pool, on the\nother hand, has bursts of large deposits mixed with periods during which it is not very active, which we can also see reflected in the graph. Despite their different behaviors, the amount deposited between the two pools is similar."
                },
                {
                    "heading": "6.2.2 Withdrawals",
                    "text": "While the withdrawals from the pool do not solely re-use the small number of mining addresses identified using deposits (as we saw in our na\u0131\u0308ve attempt to link miner z-to-t transactions in Figure 8a), they do typically re-use some of them, so can frequently be identified anyway.\nIn particular, mining pool payouts in Zcash are similar to how many of them are in Bitcoin [27, 18]. The block reward is often paid into a single address, controlled by the operator of the pool, and the pool operator then deposits some set of aggregated block rewards into the shielded pool. They then pay the individual reward to each of the individual miners as a way of \u201csharing the pie,\u201d which results in z-to-t transactions with many outputs. (In Bitcoin, some pools opt for this approach while some form a \u201cpeeling chain\u201d in which they pay each individual miner in a separate transaction, sending the change back to themselves each time.) In the payouts for some of the mining pools, the list of output t-addresses sometimes includes one of the t-addresses known to be associated with the mining pool already. We thus tag these types of payouts as belonging to the mining pool, according to the following heuristic:\nHeuristic 4. If a z-to-t transaction has over 100 output taddresses, one of which belongs to a known mining pool, then we label the transaction as a mining withdrawal (associated with that pool), and label all non-pool output t-addresses as belonging to miners.\nAs with Heuristic 3, short of asking the mining pool operators directly it is impossible to validate this heuristic. Nevertheless, given the known operating structure of Bitcoin mining pools and the way this closely mirrors that structure, we again believe it to be relatively safe.\nAs a result of running this heuristic, we tagged 110,918 addresses as belonging to miners, and linked a much more significant portion of the z-to-t transactions, as seen in Figure 8c. As the last column in Table 4 shows, however, this heuristic captured the activity of only a small number of the mining pools, and the large jump in linked activity is mostly due to the high coverage with F2Pool (one of the two richest pools). This implies that further heuristics developed specifically for other pools, such as Flypool, would increase the linkability even more. Furthermore, a more active strategy in which we mined with the pools to receive payouts would reveal their structure, at which point (according to the\n472 27th USENIX Security Symposium USENIX Association\n1.1M deposited by Flypool shown in Figure 10 and the remaining value of 1.2M attributed to the \u2018other\u2019 category shown in Figure 8c) we would shrink the anonymity set even further.4"
                },
                {
                    "heading": "6.3 Other Entities",
                    "text": "Once the miners and founders have been identified, we can assume the remaining transactions belong to more general entities. In this section we look into different means of categorizing these entities in order to identify how the shielded pool is being used.\nIn particular, we ran the heuristic due to Quesnelle [36], which said that if a unique value (i.e., a value never seen in the blockchain before or since) is deposited into the pool and then, after some short period of time, the exact same value is withdrawn from the pool, the deposit and the withdrawal are linked in what he calls a round-trip transaction.\nHeuristic 5. [36] For a value v, if there exists exactly one t-to-z transaction carrying value v and one z-to-t transaction carrying value v, where the z-to-t transaction happened after the t-to-z one and within some small number of blocks, then these transactions are linked.\nIn terms of false positives, the fact that the value is unique in the blockchain means that the only possibility of a false positive is if some of the z-to-z transactions split or aggregated coins in such a way that another deposit (or several other deposits) of a different amount were altered within the pool to yield an amount identical to the initial deposit. While this is possible in theory, we observe that of the 12,841 unique values we identified, 9,487 of them had eight decimal places (the maximum number in Zcash), and 98.9% of them had more than three decimal places. We thus view it as highly unlikely that these exact values were achieved via manipulations in z-to-z transactions.\nBy running this heuristic, we identified 12,841 unique values, which means we linked 12,841 transactions. The values total 1,094,513.23684 ZEC and represent 28.5% of all coins ever deposited in the pool. Interestingly, most (87%) of the linked coins were in transactions attributed to the founders and miners, so had already been linked by our previous heuristics. We believe this lends further credence to their soundness. In terms of the block interval, we ran Heuristic 5 for every interval between 1 and 100 blocks; the results are in Figure 11.\nAs this figure shows, even if we assume a conservative block interval of 10 (meaning the withdrawal took place\n4It is possible that we have already captured some of the Flypool activity, as many of the miners receive payouts from multiple pools. We thus are not claiming that all remaining activity could be attributed to Flypool, but potentially some substantial portion.\n25 minutes after the deposit), we still capture 70% of the total value, or over 700K ZEC. If we require the withdrawal to have taken place within an hour of the deposit, we get 83%."
                },
                {
                    "heading": "7 Interactions within the Shielded Pool",
                    "text": "In this section we consider private transactions; i.e., z-toz transactions that interact solely with the shielded pool. As seen in Section 4.1, these transactions form a small percentage of the overall transactions. However, z-to-z transactions form a crucial part of the anonymity core of Zcash. In particular, they make it difficult to identify the round-trip transactions from Heuristic 5.\nOur analysis identified 6,934 z-to-z transactions, with 8,444 vJoinSplits. As discussed in Section 3.1, the only information revealed by z-to-z transactions is the miner\u2019s fee, the time of the transaction, and the number of vJoinSplits used as input. Of these, we looked at the time of transactions and the number of vJoinSplits in order to gain some insight as to the use of these operations.\nWe found that 93% of z-to-z transactions took just one vJoinSplit as input. Since each vJoinSplit can have at most two shielded outputs as its input, the majority of z-to-z transactions thus take no more than two shielded outputs as their input. This increases the difficulty of categorizing z-to-z transactions, because we cannot know if a small number of users are making many transactions, or many users are making one transaction.\nIn looking at the timing of z-to-z transactions, however, we conclude that it is likely that a small number of users were making many transactions. Figure 12 plots the cumulative number of vJoinSplits over time. The occurrences of vJoinSplits are somewhat irregular, with 17% of all vJoinSplits occurring in January 2017. There are four other occasions when a sufficient number of vJoinSplits occur within a sufficiently short period of time as to be visibly noticeable. It seems likely that these\nUSENIX Association 27th USENIX Security Symposium 473\nOct-2016 Jan-2017 Apr-2017 Jul-2017 Oct-2017 Jan-2018 Date\n1\n2\n3\n4\n5\n6\n7 8 Nu m be r o f I np ut s (In th ou sa nd s)\nFigure 12: The number of z-to-z vJoinSplits over time.\noccurrences belong to the same group of users, or at least by users interacting with the same service.\nFinally, looking back at the number of t-to-z and zto-t transactions identified with mining pools in Table 4, it is possible that BitClub Pool is responsible for up to 1,300 of the z-to-z transactions, as it had 196 deposits into the pool and 1,516 withdrawals. This can happen only because either (1) the pool made extra z-to-z transactions, or (2) it sent change from its z-to-t transactions back into the shielded pool. As most of BitClub Pool\u2019s z-to-t transactions had over 200 output t-addresses, however, we conclude that the former explanation is more likely."
                },
                {
                    "heading": "8 Case Study: The Shadow Brokers",
                    "text": "The Shadow Brokers (TSB) are a hacker collective that has been active since the summer of 2016, and that leaks tools supposedly created by the NSA. Some of these leaks are released as free samples, but many are sold via auctions and as monthly bundles. Initially, TSB accepted payment only using Bitcoin. Later, however, they began to accept Zcash for their monthly dump service. In this section we discuss how we identified t-to-z transactions that could represent payments to TSB. We identified twenty-four clusters (created using our analysis in Section 5) matching our criteria for potential TSB customers, one of which could be a regular customer."
                },
                {
                    "heading": "8.1 Techniques",
                    "text": "In order to identify the transactions that are most likely to be associated with TSB, we started by looking at their blog [5]. In May 2017, TSB announced that they would be accepting Zcash for their monthly dump service. Throughout the summer (June through August) they accepted both Zcash and Monero, but in September they announced that they would accept only Zcash. Table 5 summarizes the amount they were requesting in\neach of these months. The last blog post was made in October 2017, when they stated that all subsequent dumps would cost 500 ZEC.\nTo identify potential TSB transactions, we thus looked at all t-to-z transactions not associated with miners or founders that deposited either 100, 200, 400, or 500 ZEC \u00b1 5 ZEC. Our assumption was that users paying TSB were not likely to be regular Zcash users, but rather were using it with the main purpose of making the payment. On this basis, addresses making t-to-z transactions of the above values were flagged as a potential TSB customer if the following conditions held:\n1. They did not get their funds from the pool; i.e., there were no z-to-t transactions with this address as an output. Again, if this were a user mainly engaging with Zcash as a way to pay TSB, they would need to to buy their funds from an exchange, which engage only with t-addresses.\n2. They were not a frequent user, in the sense that they had not made or received more than 250 transactions (ever).\n3. In the larger cluster in which this address belonged, the total amount deposited by the entire cluster into the pool within one month was within 1 ZEC of the amounts requested by TSB. Here, because the resulting clusters were small enough to treat manually, we applied not only Heuristic 1 but also Heuristic 2 (clustering by change), making sure to weed out false positives. Again, the idea was that suspected TSB customers would not be frequent users of the pool.\nAs with our previous heuristics, there is no way to quantify the false-positive risks associated with this set of criteria, although we see below that many of the transactions matching it did occur in the time period associated with TSB acceptance of Zcash. Regardless, given this limitation we are not claiming that our results are definitive, but do believe this to be a realistic set of criteria that might be applied in the context of a law enforcement investigation attempting to narrow down potential suspects.\n474 27th USENIX Security Symposium USENIX Association"
                },
                {
                    "heading": "8.2 Results",
                    "text": "Our results, in terms of the number of transactions matching our requirements above up until 17 January 2018, are summarized in Table 6. Before the first TSB blog post in May, we found only a single matching transaction. This is very likely a false positive, but demonstrates that the types of transactions we were seeking were not common before TSB went live with Zcash. After the blog post, we flagged five clusters in May and June for the requested amount of 100 ZEC. There were only two clusters that was flagged for 500 ZEC, one of which was from August. No transactions of any of the required quantities were flagged in September, despite the fact that TSB switched to accepting only Zcash in September. This is possible for a number of reasons: our criteria may have caused us to miss transactions, or maybe there were no takers. From October onwards we flagged between 1-6 transactions per month. It is hard to know if these represent users paying for old data dumps or are simply false positives.\nFour out of the 24 transactions in Table 6 are highly likely to be false positives. First, there is the deposit of 100 ZEC into the pool in January, before TSB announced their first blog post. This cluster put an additional 252 ZEC into the pool in March, so is likely just some user of the pool. Second and third, there are two deposits of 200 ZEC into the pool in June, before TSB announced that one of the July dump prices would cost 200 ZEC. Finally, there is a deposit of 400 ZEC into the pool in June before TSB announced that one of the July dump prices would cost 400 ZEC.\nOf the remaining clusters, there is one whose activ-\nity is worth discussing. From this cluster, there was one deposit into the pool in June for 100 ZEC, one in July for 200 ZEC, and one in August for 500 ZEC, matching TSB prices exactly. The cluster belonged to a new user, and most of the money in this user\u2019s cluster came directly from Bitfinex (Cluster 3)."
                },
                {
                    "heading": "9 Conclusions",
                    "text": "This paper has provided the first in-depth exploration of Zcash, with a particular focus on its anonymity guarantees. To achieve this, we applied both well-known clustering heuristics that have been developed for Bitcoin and attribution heuristics we developed ourselves that take into account Zcash\u2019s shielded pool and its unique cast of characters. As with previous empirical analyses of other cryptocurrencies, our study has shown that most users are not taking advantage of the main privacy feature of Zcash at all. Furthermore, the participants who do engage with the shielded pool do so in a way that is identifiable, which has the effect of significantly eroding the anonymity of other users by shrinking the overall anonymity set.\nFuture work\nOur study was an initial exploration, and thus left many avenues open for further exploration. For example, it may be possible to classify more z-to-z transactions by analyzing the time intervals between the transactions in more detail, or by examining other metadata such as the miner\u2019s fee or even the size (in bytes) of the transaction. Additionally, the behavior of mining pools could be further identified by a study that actively interacts with them.\nSuggestions for improvement\nOur heuristics would have been significantly less effective if the founders interacting with the pool behaved in a less regular fashion. In particular, by always withdrawing the same amount in the same time intervals, it became possible to distinguish founders withdrawing funds from other users. Given that the founders are both highly invested in the currency and knowledgeable about how to use it in a secure fashion, they are in the best place to ensure the anonymity set is large.\nUltimately, the only way for Zcash to truly ensure the size of its anonymity set is to require all transactions to take place within the shielded pool, or otherwise significantly expand the usage of it. This may soon be computationally feasible given emerging advances in the underlying cryptographic techniques [6], or even if more mainstream wallet providers like Jaxx roll out support for z-\nUSENIX Association 27th USENIX Security Symposium 475\naddresses. More broadly, we view it as an interesting regulatory question whether or not mainstream exchanges would continue to transact with Zcash if it switched to supporting only z-addresses."
                },
                {
                    "heading": "Acknowledgments",
                    "text": "We would like to thank Lustro, the maintainer of the Zchain explorer, for answering specific questions we asked about the service. The authors are supported in part by EPSRC Grant EP/N028104/1, and in part by the EU H2020 TITANIUM project under grant agreement number 740558. Mary Maller is also supported by a scholarship from Microsoft Research."
                }
            ],
            "year": 2018,
            "references": [
                {
                    "title": "Evaluating user privacy in Bitcoin",
                    "authors": [
                        "E. Androulaki",
                        "G. Karame",
                        "M. Roeschlin",
                        "T. Scherer",
                        "S. Capkun"
                    ],
                    "venue": "A.-R. Sadeghi, editor, FC 2013, volume 7859 of LNCS, pages 34\u201351, Okinawa, Japan, Apr. 1\u20135",
                    "year": 2013
                },
                {
                    "title": "Zerocash: Decentralized anonymous payments from Bitcoin",
                    "authors": [
                        "E. Ben-Sasson",
                        "A. Chiesa",
                        "C. Garman",
                        "M. Green",
                        "I. Miers",
                        "E. Tromer",
                        "M. Virza"
                    ],
                    "venue": "2014 IEEE Symposium on Security and Privacy, pages 459\u2013474, Berkeley, CA, USA, May 18\u201321",
                    "year": 2014
                },
                {
                    "title": "Sybilresistant mixing for Bitcoin",
                    "authors": [
                        "G. Bissias",
                        "A.P. Ozisik",
                        "B.N. Levine",
                        "M. Liberatore"
                    ],
                    "venue": "Proceedings of the 13th Workshop on Privacy in the Electronic Society (WEIS), pages 149\u2013158",
                    "year": 2014
                },
                {
                    "title": "Mixcoin: Anonymity for Bitcoin with accountable mixes",
                    "authors": [
                        "J. Bonneau",
                        "A. Narayanan",
                        "A. Miller",
                        "J. Clark",
                        "J.A. Kroll",
                        "E.W. Felten"
                    ],
                    "venue": "N. Christin and R. Safavi-Naini, editors, FC 2014, volume 8437 of LNCS, pages 486\u2013504, Christ Church, Barbados, Mar. 3\u20137",
                    "year": 2014
                },
                {
                    "title": "The Shadow Brokers only accept ZCash payments for their monthly dump service, May 2017",
                    "authors": [
                        "J. Buntinx"
                    ],
                    "venue": "https: //themerkle.com/the-shadow-brokers-only-acceptzcash-payments-for-their-monthly-dump-service/",
                    "year": 2017
                },
                {
                    "title": "The Imperfect Crime: How the WannaCry Hackers Could Get Nabbed, Aug. 2017",
                    "authors": [
                        "J. Dunietz"
                    ],
                    "venue": "https: //www.scientificamerican.com/article/theimperfect-crime-how-the-wannacry-hackers-couldget-nabbed/",
                    "year": 2017
                },
                {
                    "title": "The miner\u2019s dilemma",
                    "authors": [
                        "I. Eyal"
                    ],
                    "venue": "2015 IEEE Symposium on Security and Privacy, pages 89\u2013103, San Jose, CA, USA, May 17\u201321",
                    "year": 2015
                },
                {
                    "title": "Bitcoin laundering: An analysis of illicit flows into digital currency services, Jan",
                    "authors": [
                        "Y.J. Fanusie",
                        "T. Robinson"
                    ],
                    "year": 2018
                },
                {
                    "title": "Stealing bitcoins with badges: How Silk Road\u2019s dirty cops got caught, Aug. 2016",
                    "authors": [
                        "C. Farivar",
                        "J. Mullin"
                    ],
                    "year": 2016
                },
                {
                    "title": "TumbleBit: an untrusted Bitcoin-compatible anonymous payment hub",
                    "authors": [
                        "E. Heilman",
                        "L. Alshenibr",
                        "F. Baldimtsi",
                        "A. Scafuro",
                        "S. Goldberg"
                    ],
                    "venue": "Proceedings of NDSS 2017",
                    "year": 2017
                },
                {
                    "title": "Hawk: The blockchain model of cryptography and privacypreserving smart contracts",
                    "authors": [
                        "A.E. Kosba",
                        "A. Miller",
                        "E. Shi",
                        "Z. Wen",
                        "C. Papamanthou"
                    ],
                    "venue": "2016 IEEE Symposium on Security and Privacy, pages 839\u2013858, San Jose, CA, USA, May 22\u201326",
                    "year": 2016
                },
                {
                    "title": "A traceability analysis of Monero\u2019s blockchain",
                    "authors": [
                        "A. Kumar",
                        "C. Fischer",
                        "S. Tople",
                        "P. Saxena"
                    ],
                    "venue": "Proceedings of ESORICS 2017, pages 153\u2013173",
                    "year": 2017
                },
                {
                    "title": "CoinJoin: Bitcoin privacy for the real world",
                    "authors": [
                        "G. Maxwell"
                    ],
                    "venue": "bitcointalk.org/index.php?topic=279249, Aug",
                    "year": 2013
                },
                {
                    "title": "M\u00f6bius: Trustless tumbling for transaction privacy",
                    "authors": [
                        "S. Meiklejohn",
                        "R. Mercer"
                    ],
                    "venue": "Proceedings on Privacy Enhancing Technologies",
                    "year": 2018
                },
                {
                    "title": "Privacy-enhancing overlays in Bitcoin",
                    "authors": [
                        "S. Meiklejohn",
                        "C. Orlandi"
                    ],
                    "venue": "M. Brenner, N. Christin, B. Johnson, and K. Rohloff, editors, FC 2015 Workshops, volume 8976 of LNCS, pages 127\u2013 141, San Juan, Puerto Rico, Jan. 30",
                    "year": 2015
                },
                {
                    "title": "A fistful of bitcoins: characterizing payments among men with no names",
                    "authors": [
                        "S. Meiklejohn",
                        "M. Pomarole",
                        "G. Jordan",
                        "K. Levchenko",
                        "D. Mc- Coy",
                        "G.M. Voelker",
                        "S. Savage"
                    ],
                    "venue": "Proceedings of the 2013 Internet Measurement Conference (IMC), pages 127\u2013 140",
                    "year": 2013
                },
                {
                    "title": "Zerocoin: Anonymous distributed E-cash from Bitcoin",
                    "authors": [
                        "I. Miers",
                        "C. Garman",
                        "M. Green",
                        "A.D. Rubin"
                    ],
                    "venue": "2013 IEEE Symposium on Security and Privacy, pages 397\u2013411, Berkeley, CA, USA, May 19\u201322",
                    "year": 2013
                },
                {
                    "title": "An empirical analysis of linkability in the Monero blockchain",
                    "authors": [
                        "A. Miller",
                        "M. M\u00f6ser",
                        "K. Lee",
                        "A. Narayanan"
                    ],
                    "venue": "arXiv:1704.04299",
                    "year": 2017
                },
                {
                    "title": "Listening to whispers of Ripple: Linking wallets and deanonymizing transactions in the Ripple network",
                    "authors": [
                        "P. Moreno-Sanchez",
                        "M.B. Zafar",
                        "A. Kate"
                    ],
                    "venue": "Proceedings on Privacy Enhancing Technologies, 2016(4):436\u2013453",
                    "year": 2016
                },
                {
                    "title": "Join me on a market for anonymity",
                    "authors": [
                        "M. M\u00f6ser",
                        "R. B\u00f6hme"
                    ],
                    "venue": "Proceedings of the 15th Workshop on the Economics of Information Security (WEIS)",
                    "year": 2016
                },
                {
                    "title": "Anonymous alone? measuring Bitcoin\u2019s second-generation anonymization techniques",
                    "authors": [
                        "M. M\u00f6ser",
                        "R. B\u00f6hme"
                    ],
                    "venue": "Proceedings of IEEE Security & Privacy on the Blockchain",
                    "year": 2017
                },
                {
                    "title": "An inquiry into money laundering tools in the Bitcoin ecosystem",
                    "authors": [
                        "M. M\u00f6ser",
                        "R. B\u00f6hme",
                        "D. Breuker"
                    ],
                    "venue": "Proceedings of the APWG E-Crime Researchers Summit",
                    "year": 2013
                },
                {
                    "title": "Bitcoin: A Peer-to-Peer",
                    "authors": [
                        "S. Nakamoto"
                    ],
                    "venue": "Electronic Cash System,",
                    "year": 2008
                },
                {
                    "title": "and the Monero Research Lab",
                    "authors": [
                        "S. Noether",
                        "A. Mackenzie"
                    ],
                    "venue": "Ring confidential transactions. Ledger, 1:1\u201318",
                    "year": 2016
                },
                {
                    "title": "On the linkability of Zcash transactions",
                    "authors": [
                        "J. Quesnelle"
                    ],
                    "venue": "arXiv:1712.01210",
                    "year": 2017
                },
                {
                    "title": "An analysis of anonymity in the Bitcoin system",
                    "authors": [
                        "F. Reid",
                        "M. Harrigan"
                    ],
                    "venue": "Security and privacy in social networks, pages 197\u2013 223. Springer",
                    "year": 2013
                },
                {
                    "title": "Quantitative analysis of the full Bitcoin transaction graph",
                    "authors": [
                        "D. Ron",
                        "A. Shamir"
                    ],
                    "venue": "A.-R. Sadeghi, editor, FC 2013, volume 7859 of LNCS, pages 6\u201324, Okinawa, Japan, Apr. 1\u20135",
                    "year": 2013
                },
                {
                    "title": "CoinShuffle: Practical decentralized coin mixing for Bitcoin",
                    "authors": [
                        "T. Ruffing",
                        "P. Moreno-Sanchez",
                        "A. Kate"
                    ],
                    "venue": "M. Kutylowski and J. Vaidya, editors, ESORICS 2014, Part II, volume 8713 of LNCS, pages 345\u2013364, Wroclaw, Poland, Sept. 7\u201311",
                    "year": 2014
                },
                {
                    "title": "BitIodine: Extracting intelligence from the Bitcoin network",
                    "authors": [
                        "M. Spagnuolo",
                        "F. Maggi",
                        "S. Zanero"
                    ],
                    "venue": "N. Christin and R. Safavi- Naini, editors, FC 2014, volume 8437 of LNCS, pages 457\u2013468, Christ Church, Barbados, Mar. 3\u20137",
                    "year": 2014
                },
                {
                    "title": "Blindcoin: Blinded",
                    "authors": [
                        "L. Valenta",
                        "B. Rowan"
                    ],
                    "venue": "accountable mixes for Bitcoin. In M. Brenner, N. Christin, B. Johnson, and K. Rohloff, editors, FC 2015 Workshops, volume 8976 of LNCS, pages 112\u2013126, San Juan, Puerto Rico, Jan. 30",
                    "year": 2015
                }
            ],
            "id": "SP:ab4da6d37196c4fbec6bcb3b7508e31506fe30bc",
            "authors": [
                {
                    "name": "George Kappos",
                    "affiliations": []
                },
                {
                    "name": "Haaroon Yousaf",
                    "affiliations": []
                },
                {
                    "name": "Mary Maller",
                    "affiliations": []
                }
            ],
            "abstractText": "Among the now numerous alternative cryptocurrencies derived from Bitcoin, Zcash is often touted as the one with the strongest anonymity guarantees, due to its basis in well-regarded cryptographic research. In this paper, we examine the extent to which anonymity is achieved in the deployed version of Zcash. We investigate all facets of anonymity in Zcash\u2019s transactions, ranging from its transparent transactions to the interactions with and within its main privacy feature, a shielded pool that acts as the anonymity set for users wishing to spend coins privately. We conclude that while it is possible to use Zcash in a private way, it is also possible to shrink its anonymity set considerably by developing simple heuristics based on identifiable patterns of usage.",
            "title": "An Empirical Analysis of Anonymity in Zcash"
        },
        "Y": {
            "blog_id": "an-empirical-analysis-of-anonymity-in-zcash",
            "summary": [
                "An empirical analysis of anonymity in Zcash Kappos et al., USENIX Security\u201918  As we\u2019ve seen before, in practice Bitcoin offers little in the way of anonymity .",
                "Zcash on the other hand was carefully designed with privacy in mind.",
                "It offers strong theoretical guarantees concerning privacy.",
                "So in theory users of Zcash can remain anonymous.",
                "In practice though it depends on the way those users interact with Zcash.",
                "Today\u2019s paper choice, \u2018An empirical analysis of anonymity in Zcash\u2019 studies how identifiable transaction participants are in practice based on the 2,242,847 transactions in the blockchain at the time of the study.",
                "We conclude that while it is possible to use Zcash in a private way, it is also possible to shrink its anonymity set considerably by developing simple heuristics based on identifiable patterns of usage.",
                "The analysis also provides some interesting insights into who is using Zcash and for what as well.",
                "Founders and miners combined account for around 66% of the value drawn from the shielded pool.",
                "The code for the analysis is available online at  [url]"
            ],
            "author_id": "ACOLYER",
            "pdf_url": "https://www.usenix.org/system/files/conference/usenixsecurity18/sec18-kappos.pdf",
            "author_full_name": "Adrian Colyer",
            "source_website": "https://blog.acolyer.org/about/",
            "id": 36679338
        }
    },
    "10587146": {
        "X": {
            "sections": [
                {
                    "heading": "1. Introduction",
                    "text": "One of the most basic ideas underlying democracy is that complicated decisions can be made by asking a group of people to vote on the alternatives at hand. As a decision-making framework, this paradigm is versatile, because people can express a sensible opinion about a wide range of issues. One of its seemingly inherent shortcomings, though, is that voters must take the time to cast a vote \u2014 hopefully an informed one \u2014 every time a new dilemma arises.\nBut what if we could predict the preferences of voters \u2014 instead of explicitly asking them each time \u2014 and then aggregate those predicted preferences to arrive at a decision? This is exactly the idea behind the work of Noothigattu et al. (2018), who are motivated by the challenge of automating ethical decisions. Specifically, their approach consists of three1 steps: first, collect preferences from voters on exam-\n1School of Computer Science, Carnegie Mellon University, Pittsburgh, USA. Correspondence to: Anson Kahng <akahng@cs.cmu.edu>.\nProceedings of the 36 th International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s).\n1Technically four, see Section 1.2.\nple dilemmas; second, learn models of their preferences, which generalize to any (previously unseen) dilemma; and third, at runtime, use those models to predict the voters\u2019 preferences on the current dilemma, and aggregate the predicted preferences to reach a decision. The idea is that we would ideally like to consult the voters on each decision, but in order to automate those decisions we instead use the models that we have learned as a proxy for the flesh and blood voters. In other words, the models serve as virtual voters, which is why we refer to this paradigm as virtual democracy.\nSince 2017, we have been building on this approach in a collaboration with a Pittsburgh-based non-profit, 412 Food Rescue, that provides on-demand food donation distribution services. The goal is to design and deploy an algorithm that would automatically make the decisions they most frequently face: given an incoming food donation, which recipient organization (such as a housing authority or food pantry) should receive it? The voters in our implementation are stakeholders: donors, recipients, volunteers (who pick up the food from the donor and deliver it to the recipient), and employees. We have collected roughly 100 pairwise comparisons from each voter, where in each comparison, the voter is provided information about the type of donation, as well as seven relevant features of the two alternatives that are being compared, e.g., the distance between donor and recipient, and when the recipient last received a donation. Using this data, we have learned a model of the preferences of each voter, which allows us to predict the voter\u2019s preference ranking over hundreds of recipients. And given a predicted ranking for each voter, we map them into a ranking over the alternatives by applying a voting rule.\nWhile this implementation sounds simple enough, the choice of voting rule can have a major impact on the efficacy of the system. In fact, the question of which voting rule to employ is one of the central questions in computational social choice (Brandt et al., 2016), and in social choice theory more broadly. A long tradition of impossibility results establishes that there are no perfect voting rules (Arrow, 1951), so the answer, such as it is, is often context-dependent.\nThe central premise of this paper is that, in the context of virtual democracy, certain statistical considerations should guide the choice of voting rule. Indeed, the voting rule\ninherently operates on noisy predictions of the voters\u2019 true preferences, yet one might hope that it would still output the same ranking as it would in the \u2018real\u2019 election based on the voters\u2019 true preferences (after all, this is the ideal that virtual democracy is trying to approximate). Our research question, therefore, is\n... which voting rules have the property that their output on the true preferences is likely to coincide with their output on noisy estimates thereof?"
                },
                {
                    "heading": "1.1. Our Approach and Results",
                    "text": "Our technical approach relies on the observation that the classic Mallows (1957) model is an unusually good fit with our problem. Typically the Mallows model describes situations where there is a true ranking of the alternatives \u03c3\u2217. The probability that voter i would be associated with a given ranking \u03c3i decreases exponentially with the number of pairs of alternatives on which \u03c3i and \u03c3\u2217 disagree (formally known as the Kendall tau distance). The model is parameterized by a parameter \u03c6 \u2208 (0, 1], which is directly related to the probability that \u03c3i agrees with \u03c3\u2217 on any particular pair of alternatives. This model is very well studied (see Section 1.2), but, even in situations where there is a ground-truth ranking, the Mallows model may not be an accurate representation of reality (Mao et al., 2013). This observation has motivated a body of work on generalized (Caragiannis et al., 2016; 2014) and adversarial (Procaccia et al., 2016; Benade et al., 2017) noise models.\nIn our setting each voter has a (possibly different) true ranking \u03c3\u2217i , and the voter\u2019s predicted ranking \u03c3i is drawn from a Mallows distribution around \u03c3\u2217i . Crucially, since the learning algorithm is, in fact, trying to predict pairwise comparisons (which make up the training set), the accuracy of the predictor can be directly mapped to the Mallows parameter \u03c6. In other words, instead of making the classic assumption that voters may fail to identify the ordering of some pairs of alternatives with some probability, we are essentially observing that the machine learning algorithm fails to accurately predict some of the pairwise comparisons, and mapping that to a separate Mallows model for each voter. To drive the point home, although the Mallows model is widely believed to be a tenuous fit with previously studied applications (as discussed earlier), it is intuitively the correct way of reasoning about the errors that arise when machine learning algorithms predict rankings based on pairwise comparisons. This insight is a key part of our conceptual contribution.\nOur main positive result (Theorem 1) is that the classic Borda count rule is robust to random noise, that is, it satisfies the property stated earlier, in a precise sense. Specifically, we establish an upper bound on the probability that two alternatives are ranked differently when Borda count is applied to the true preferences and to their noisy estimates.\nThe bound depends on the parameters of the model, as well as on the difference between the scores of the two alternatives in the true profile. On a high level, the theorem implies that if one alternative is stronger than another by a moderate margin under the true profile, Borda count is highly unlikely to swap the two when given noisy preferences.\nBy contrast, we show that voting rules belonging to the wide family of pairwise-majority consistent rules are not robust (Theorem 2). We do this by constructing an instance where there are significant margins between alternatives, yet any voting rule belonging to this family is likely to flip a pair of alternatives.\nFinally, we provide empirical results that further strengthen our case for the robustness of Borda count. Specifically, these results suggest that the probability of making a mistake on a pair of alternatives decreases very quickly with their average Borda score difference, independently of the distribution used to generate the underlying true preferences."
                },
                {
                    "heading": "1.2. Related Work",
                    "text": "A number of recent papers have explored the idea of automating ethical decisions via machine learning and social choice (Conitzer et al., 2017; Freedman et al., 2018; Noothigattu et al., 2018). As mentioned above, our work builds on the framework proposed by Noothigattu et al. (2018). However, it is important to clarify why the questions we explore here do not arise in their work. Since they deal with 1.3 million voters, and split-second decisions (what should a self-driving car do in an emergency?), they cannot afford to consult the individual voter models at runtime. Hence, they have added an additional summarization step, whereby the individual voter models are summarized as a single, concise model of societal preferences (with possibly significant loss to accuracy). The structure of the summary model is such that, for any given set of alternatives, almost all reasonable voting rules agree on the outcome (this is their main theoretical result), hence the choice of voting rule is a nonissue under that particular implementation. By contrast, our work is motivated by the food bank application of the virtual democracy framework, where the number of voters is small and speed is not of the essence, hence we predict the preferences of individual voters at runtime.\nIt is worth mentioning that another prominent approach to the allocation of food donations is based on (online) fair division (Aleksandrov et al., 2015). That said, it is important to emphasize that we study a general question about the foundations of the virtual democracy paradigm, that is, our work is not technically tied to any particular application.\nFurthermore, the Mallows model underlies a large body of work in computational social choice (Conitzer & Sandholm, 2005; Conitzer et al., 2009; Elkind et al., 2010; Elkind &\nShah, 2014; Xia et al., 2010; Xia & Conitzer, 2011; Lu & Boutilier, 2011; Procaccia et al., 2012; Jiang et al., 2014; Azari Soufiani et al., 2012; 2013; 2014; Mao et al., 2013; Caragiannis et al., 2014; 2016; Xia, 2016). Our model is loosely related to that of Jiang et al. (2014), where individual rankings are derived from a single ground truth ranking via a Mallows model, and then a second Mallows model is applied to obtain a noisy version of each voter\u2019s ranking. Our technical question is completely different from theirs.\nFinally, there is a large body of work in social choice on finding aggregation rules that satisfy axiomatic properties that formally capture notions of fairness or efficiency (Arrow, 1951; Tennenholtz & Zohar, 2016). However, many common axiomatic properties in social choice do not apply to standard applications of virtual democracy, including the autonomous vehicle domain of Noothigattu et al. (2018) and our setting of food rescue, although they may be relevant in other differently-constrained domains."
                },
                {
                    "heading": "2. Preliminaries",
                    "text": "We deal with a set of alternatives A such that |A| = m. Preferences over A are represented via a ranking \u03c3 \u2208 L, where L = L(A) is the set of rankings (or permutations) overA. We denote by \u03c3(j) the alternative ranked in position j in \u03c3, where position 1 is the highest, and m the lowest. We denote by \u03c3\u22121(x) the position in which x \u2208 A is ranked. We use x \u03c3 y to denote that x is preferred to y according to \u03c3, i.e., that \u03c3\u22121(x) < \u03c3\u22121(y).\nThe setting also includes a set of voters N = {1, . . . , n}. Each voter i \u2208 N is associated with a ranking \u03c3i \u2208 L. The preferences of N are represented as a preference profile \u03c3 = (\u03c31, . . . , \u03c3n) \u2208 Ln.\nGiven a preference profile \u03c3 \u2208 Ln, we say that x \u2208 A beats y \u2208 A in a pairwise comparison if a majority of voters prefer x to y, that is,\n|{i \u2208 N : x \u03c3i y}| > n/2.\nThe profile \u03c3 induces a weighted pairwise majority graph \u0393(\u03c3), where we have a vertex for each alternative in A. For each x \u2208 A and y \u2208 A \\ {x}, there is an edge from x to y if x beats y in a pairwise comparison; the weight on this edge is\nw(x,y)(\u03c3) , |{i \u2208 N : x \u03c3i y}| \u2212 |{i \u2208 N : y \u03c3i x}|."
                },
                {
                    "heading": "2.1. Voting Rules",
                    "text": "A voting rule (formally known as a social welfare function) is a function f : Ln \u2192 L, which receives a preference profile as input, and returns a \u2018consensus\u2019 ranking of the alternatives. We are especially interested in two families of voting rules.\n\u2022 Positional scoring rules. Each such rule is defined by a score vector (\u03b11, . . . , \u03b1m). Given a preference profile \u03c3, the score of alternative x is\nn\u2211 i=1 \u03b1\u03c3\u22121i (x) .\nIn words, each voter who ranks x in position p gives \u03b1p points to x. The positional scoring rule returns a ranking of the alternatives by non-increasing score, with ties broken arbitrarily.\nOur main positive result pertains to the classic Borda count voting rule, which is the positional scoring rule defined by the score vector (m \u2212 1,m \u2212 2, . . . , 0). Denote the Borda count score of x \u2208 A in \u03c3 \u2208 Ln by\nB(x,\u03c3) , n\u2211 i=1 ( m\u2212 \u03c3\u22121i (x) ) .\n\u2022 Pairwise-majority consistent (PMC) rules (Caragiannis et al., 2016): These rules satisfy a fairly weak requirement that extends the classic notion of Condorcet consistent social choice functions: Given a profile \u03c3, if the pairwise majority graph \u0393(\u03c3) = (A,E) is such that for all x \u2208 A, y \u2208 A\\{x}, either (x, y) \u2208 E or (y, x) \u2208 E (i.e., it is a tournament), and, moreover, \u0393 is acyclic, then f(\u03c3) = \u03c4 for the unique ranking \u03c4 induced by \u0393(\u03c3). Caragiannis et al. (2016) give many examples of prominent voting rules that are PMC, including the Kemeny rule, the Slater rule, the ranked pairs method, Copeland\u2019s method, and Schulze\u2019s method."
                },
                {
                    "heading": "2.2. The Mallows Model",
                    "text": "Let the Kendall tau distance between two rankings \u03c3, \u03c3\u2032 \u2208 L be\ndKT(\u03c3, \u03c3 \u2032) , |{(x, y) \u2208 A2 : x \u03c3 y \u2227 y \u03c3\u2032 x}|.\nIn words, it is the number of pairs of alternatives on which \u03c3 and \u03c3\u2032 disagree. For example, if \u03c3 = (a, b, c, d), and \u03c3\u2032 = (a, c, d, b), then dKT(\u03c3, \u03c3\u2032) = 2.\nIn the Mallows (1957) model, there is a ground truth ranking \u03c3?, which induces a probability distribution over perceived rankings. Specifically, the probability of a ranking \u03c3, given the ground truth ranking \u03c3?, is given by\nPr[\u03c3 | \u03c3?] , \u03c6 dKT(\u03c3,\u03c3\n?)\nZ ,\nwhere \u03c6 \u2208 (0, 1] is a parameter, and\nZ , \u2211 \u03c3\u2032\u2208L \u03c6dKT(\u03c3 \u2032,\u03c3?)\nis a normalization constant. Note that for \u03c6 = 1 this is a uniform distribution, whereas the probability of \u03c3? goes to 1 as \u03c6 goes to 0. In the rest of the paper we assume that \u03c6 < 1 for ease of exposition.\nThe repeated insertion model (Doignon et al., 2004) provides a convenient alternative way of reasoning about the Mallows model. In the former model, alternatives are sequentially inserted into a partial ranking, until all alternatives have been ranked. Specifically, after alternatives \u03c3?(1), . . . , \u03c3?(` \u2212 1) have been inserted, the alternative \u03c3?(`) is inserted into the first position with probability P `1 , into the second with probability P `2 , and so on until P ` ` . The following lemma connects the parameters of the random insertion model with the parameter \u03c6 of the Mallows model.\nLemma 1 (Doignon et al. 2004). The Mallows model with parameter \u03c6 \u2208 (0, 1) induces the same distribution over rankings as the random insertion model with parameters\nP `i = \u03c6 `\u2212i \u00b7 1\u2212 \u03c6\n1\u2212 \u03c6` .\nWe also require a lemma that gives bounds on the probability that the position of an element x in a ranking sampled from the Mallows model with parameter \u03c6 is far from its position in the true ranking.\nLemma 2 (Braverman & Mossel 2009). Let \u03c3 be sampled from a Mallows model with parameter \u03c6 and true ranking \u03c3?. Then for all alternatives x \u2208 A and all s \u2265 0,\nPr[\u03c3\u22121(x) \u2264 (\u03c3?)\u22121(x)\u2212 s] \u2264 \u03c6 s\n1\u2212 \u03c6\nPr[\u03c3\u22121(x) \u2265 (\u03c3?)\u22121(x) + s] \u2264 \u03c6 s\n1\u2212 \u03c6 ."
                },
                {
                    "heading": "3. From Predictions to Mallows",
                    "text": "In the virtual democracy framework, we are faced at runtime with a dilemma that induces a set of alternatives A. For example, when a food bank receives a donation, the set of alternatives is the current set of recipient organizations, each associated with information specific to the current donation, such as the distance between the donor and the recipient. Each voter i \u2208 N has a ranking \u03c3?i \u2208 L over the given set of alternatives; together these rankings comprise the true preference profile \u03c3?.\nOne of the novel components of this paper is the assumption that, for each voter i \u2208 N , we obtain a predicted ranking \u03c3i drawn from a Mallows distribution with parameter \u03c6 and true ranking \u03c3?i . We emphasize that, in contrast to almost all work on the Mallows Model, in our setting each voter has her own true ranking.\nWhy is the Mallows Model a good choice here? Recall that we are building preference models using pairwise compar-\nisons as training data. When validating a model, we therefore test its accuracy on pairwise comparisons. And the Mallows model itself, because it is defined via the Kendall tau distance, is essentially determined by pairwise comparisons. In fact, the Mallows model (with parameter \u03c6 and true ranking \u03c3?) is equivalent to the following generative process: for each pair of alternatives x and y such that x \u03c3? y, x is preferred to y with probability 1/(1 + \u03c6), and y is preferred to x with probability \u03c6/(1 + \u03c6); if this preference relation corresponds to a ranking (i.e., it is transitive), return that ranking, otherwise restart.\nIn more detail, let \u03b2 be the average probability that we predict a pairwise comparison correctly; in our food bank implementation, \u03b2 \u2248 0.9. Based on the preceding discussion, one might be tempted to set \u03b2 = 1/(1 + \u03c6), i.e., set \u03b2 to be the probability of getting the relative ordering of two adjacent alternatives correctly. While this is not unreasonable (and would have been very convenient for us), for \u03b2 \u2248 0.9 it would lead to extremely high probability of correctly ranking alternatives that are, say, 30 positions apart in the ground truth ranking. In order to moderate this effect, we define another parameter \u03ba \u2208 {2, . . . ,m}, and assume that our observed pairwise comparisons are between \u03c3?i (1) (the top-ranked alternative in the true ranking of i) and \u03c3?i (\u03ba) (the alternative ranked in position \u03ba). Formally, the parameters \u03b2 and \u03ba are such that, for the ranking \u03c3i sampled from a Mallows Model with \u03c6 and \u03c3?i ,\nPr [\u03c3?i (1) \u03c3i \u03c3?i (\u03ba)] = \u03b2. (1)\nIt is worth noting that the implicit assumption that we are observing comparisons between \u03c3?i (1) and \u03c3 ? i (\u03ba) specifically is not meant to be realistic. Rather, the idea is that there is some appropriate value of \u03ba such that the observed accuracy \u03b2 can be related to the underlying Mallows model through Equation (1), and, if we can establish results that are general with respect to the choice of \u03ba, they would carry over to the real world.\nMoving from conceptual issues to novel technical results, we start with the following lemma, which expresses the probability on the right hand side of Equation (1) in terms of the Mallows parameter \u03c6.\nLemma 3. Let \u03c3i be sampled from a Mallows Model with parameter \u03c6 and true ranking \u03c3?i . Then\nPr [\u03c3?i (1) \u03c3i \u03c3?i (\u03ba)] = \u03ba 1\u2212 \u03c6\u03ba \u2212 \u03ba\u2212 1 1\u2212 \u03c6\u03ba\u22121 .\nEquation (1) and Lemma 3 imply that\n\u03b2 = \u03ba 1\u2212 \u03c6\u03ba \u2212 \u03ba\u2212 1 1\u2212 \u03c6\u03ba\u22121 ,\nbut for subsequent results we need to express \u03c6 in terms of \u03b2 and \u03ba, and it is unclear whether this can be done in closed\nform. Nevertheless, we are are able to derive a bound that suffices for our purposes.\nLemma 4. For \u03b2 and \u03ba defined as in Equation (1), it holds that\n\u03c6 \u2264 (\n1\u2212 \u03b2 \u03b2\n) 1 2\u03ba\u22121\n.\nWe relegate the proofs of both lemmas to the full version of the paper. Note that Lemma 3 can be proved via a theorem of De\u0301sir et al. (2018). Their theorem gives a closed form for the probability that an alternative x is ranked first out of a subset of alternatives S. This closed form is complex, and requires quite a bit of additional notation, so we instead derive the probability we are interested in, i.e., the probability that \u03c3?i (\u03ba) is ranked above \u03c3 ? i (1), from scratch."
                },
                {
                    "heading": "4. Robustness of Borda Count",
                    "text": "In this section, we rigorously establish the robustness of Borda count to prediction error by showing that it satisfies a formal version of the desired property stated in Section 1. We do this by building on the machinery developed in Section 3, as well as additional lemmas that we will state and prove momentarily.\nAs we have already discussed, we do not have access to the Mallows parameter \u03c6. Instead, we can measure \u03b2, the probability that we correctly predict a pairwise comparison of alternatives that are \u03ba positions apart. On a very high level, the theorem bounds the probability that the noisy Borda ranking (based on the sampled profile) would disagree with the true Borda ranking (based on the true profile) on a given pair of alternatives.\nTheorem 1. For any \u03b2 > 1/2 and > 0 there exists a universal constant T = T (\u03b2, ) such that for all n,m, \u03ba \u2208 N such that n,m \u2265 2, for all s \u2265 T\u03ba log \u03ba, for all \u03c3? \u2208 Ln, and for all x, x\u2032 \u2208 A such that 1nB(x,\u03c3 ?) \u2265 1nB(x \u2032,\u03c3?)+ 2s, it holds that\nPr\n[ 1\nn B(x,\u03c3) >\n1 n B(x\u2032,\u03c3)\n] \u2265 1\u2212 n,\nwhere the probability is taken over the sampling of \u03c3.\nLet us discuss the statement of the theorem. First, note that the probability of mistake, n, converges to 0 exponentially fast as n grows, so the theorem immediately implies a \u201cwith high probability\u201d statement. Moreover, one can easily derive such a statement with respect to all pairs of alternatives (whose Borda scores are sufficiently separated) simultaneously, using a direct application of the union bound. Second, it is intuitive that the separation in Borda scores has to depend on \u03ba, but it is encouraging (and, to us, surprising) that this dependence is almost linear. In particular, even if \u03ba is almost linear in m, i.e., \u03ba \u2208 o(m/ logm), the theorem\nimplies that our noisy Borda ranking is highly unlikely to make mistakes on pairs of alternatives whose average score difference is linear in m.\nTurning to the proof, we start by bounding the probability that the Borda count score B(x,\u03c3) of an alternative x \u2208 A in the observed profile \u03c3 is far from the Borda count score B(x,\u03c3?) in the true profile \u03c3?. The proof of the following lemma adapts that of a lemma of (Braverman & Mossel, 2009), which deals with average rank (instead of average Borda count score), but in the case of a single true ranking, i.e., \u03c3?i = \u03c3 ? j , for all i, j.\nLemma 5. For all alternatives x \u2208 A, and all s \u2265 0\nPr\n[ 1\nn B(x,\u03c3) \u2264 1 n B(x,\u03c3?)\u2212 s ] \u2264 (\n2e(n+ ns\u2212 1) n\u2212 1 \u00b7 \u03c6 s 1\u2212 \u03c6\n)n ,\nPr\n[ 1\nn B(x,\u03c3) \u2265 1 n B(x,\u03c3?) + s ] \u2264 (\n2e(n+ ns\u2212 1) n\u2212 1 \u00b7 \u03c6 s 1\u2212 \u03c6\n)n .\nProof. We prove the first inequality; the proof of the second is analogous. Given a subset of voters S \u2286 N and a nonnegative vector b = (bi)i\u2208S \u2208 N|S|, let ES,b be the event that\nB(x, \u03c3i) \u2264 B(x, \u03c3?i )\u2212 bi\nfor all voters i \u2208 S, where we abuse notation by using\nB(x, \u03c3i) , m\u2212 \u03c3\u22121i (x)\nto denote the Borda count score of alternative x in the ranking \u03c3i. Lemma 2 implies that for all s \u2265 0,\nPr[B(x, \u03c3i) \u2264 B(x, \u03c3?i )\u2212 s] \u2264 \u03c6s\n1\u2212 \u03c6 . (2)\nTherefore, Pr[ES,b] = \u220f i\u2208S Pr[B(x, \u03c3i) \u2264 B(x, \u03c3?i )\u2212 bi]\n\u2264 \u220f i\u2208S \u03c6bi 1\u2212 \u03c6 = \u03c6 \u2211 i\u2208S bi (1\u2212 \u03c6)|S| ,\nwhere the inequality follows from Equation (2).\nLet E be the event that 1nB(x,\u03c3) \u2264 1 nB(x,\u03c3 ?)\u2212s. Notice that\nE \u2282 \u22c3\nS\u2286N,b\u2208N|S|: \u2211 i\u2208S bi=ns ES,b,\nas there must exist a subset of voters who contribute sufficiently to the difference in Borda scores. Moreover, for\na fixed S, the number of vectors b \u2208 N|S| such that\u2211 i\u2208S bi = ns is exactly (|S|+ns\u22121 |S|\u22121 ) . Therefore,\nPr[E ] \u2264 \u2211 S\u2286N \u2223\u2223\u2223\u2223\u2223 { b \u2208 N|S| : n\u2211 i=1 bi = ns }\u2223\u2223\u2223\u2223\u2223 \u00b7 \u03c6ns(1\u2212 \u03c6)|S| \u2264 2n \u00b7 ( n+ ns\u2212 1 n\u2212 1 ) \u00b7 \u03c6 ns (1\u2212 \u03c6)n\n\u2264 2n \u00b7 ( e(n+ ns\u2212 1)\nn\u2212 1\n)n\u22121 \u00b7 ( \u03c6s\n1\u2212 \u03c6 )n \u2264 (\n2e(n+ ns\u2212 1) n\u2212 1 \u00b7 \u03c6 s 1\u2212 \u03c6\n)n ,\nwhere we used the fact that ( n t ) \u2264 ( ent ) t.\nUsing Lemma 5 we can bound, given the Mallows parameter \u03c6, the probability that two alternatives, whose Borda count scores in the true profile \u03c3? are sufficiently far apart, are ranked by the Borda count voting rule in the correct order (in the sampled profile \u03c3).\nLemma 6. Let x, x\u2032 \u2208 A such that 1nB(x,\u03c3 ?) \u2265 1 nB(x \u2032,\u03c3?) + 2s. Then\nPr\n[ 1\nn B(x,\u03c3) >\n1 n B(x\u2032,\u03c3) ] \u2265 1\u2212 2 ( 2e(n+ ns\u2212 1)\nn\u2212 1 \u00b7 \u03c6\ns\n1\u2212 \u03c6\n)n .\nProof. Let E1 be the event that\n1 n B(x,\u03c3) \u2264 1 n B(x,\u03c3?)\u2212 s,\nand E2 be the event that\n1 n B(x\u2032,\u03c3) \u2265 1 n B(x\u2032,\u03c3?) + s.\nBy Lemma 5 and a union bound we have that Pr [E1 \u222a E2] \u2264 2 (\n2e(n+ ns\u2212 1) n\u2212 1 \u00b7 \u03c6 s 1\u2212 \u03c6\n)n .\nNext, notice that every time the Borda count scores of x and x\u2032 in the sampled preference profile are in the wrong order (or tied), then at least one of E1, E2 occurred, i.e.,\nPr\n[ 1\nn B(x,\u03c3) \u2264 1 n B(x\u2032,\u03c3)\n] \u2264 Pr[E1 \u222a E2].\nThe lemma directly follows.\nRecall that Lemma 4 gives an upper bound on \u03c6 as a function of \u03b2 and \u03ba. Combining with Lemma 6, we can bound the probability of getting the correct ranking as a function of \u03b2 and \u03ba, and prove our main result.\nProof of Theorem 1. By Lemma 6,\nPr\n[ 1\nn B(x,\u03c3) >\n1 n B(x\u2032,\u03c3) ] \u2265 1\u2212 2 ( 2e(n+ ns\u2212 1)\nn\u2212 1 \u00b7 \u03c6\ns\n1\u2212 \u03c6 )n \u2265 1\u2212 2 ( 4en\nn\u2212 1 \u00b7 s\u03c6\ns\n1\u2212 \u03c6 )n \u2265 1\u2212 2 ( 8e \u00b7 s\u03c6 s\n1\u2212 \u03c6\n)n .\nIt suffices to give a bound on s such that\ns\u03c6s\n1\u2212 \u03c6 \u2264 16e . (3)\nBy Lemma 4,\n\u03c6 \u2264 (\n1\u2212 \u03b2 \u03b2\n) 1 2\u03ba\u22121\n.\nSince \u03b2 > 1/2, there is a universal constant c > 1 such that 1\u2212\u03b2 \u03b2 = 1 c . Therefore,\ns\u03c6s\n1\u2212 \u03c6 \u2264 s \u00b7\n( 1\u2212\u03b2 \u03b2 ) s 2\u03ba\u22121\n1\u2212 (\n1\u2212\u03b2 \u03b2\n) 1 2\u03ba\u22121 = s \u00b7 c \u2212 s2\u03ba\u22121\n1\u2212 c\u2212 1 2\u03ba\u22121\n= s\nc s 2\u03ba\u22121 \u2212 c s\u22121 2\u03ba\u22121\n= s\nc s\u22121 2\u03ba\u22121 ( c 1 2\u03ba\u22121 \u2212 1 ) \u2264 s\nc s\u22121 2\u03ba\u22121 \u00b7 c 1 2\u03ba\u22121 (c\u22121) c(2\u03ba\u22121)\n\u2264 c c\u2212 1 \u00b7 s(2\u03ba\u2212 1) c s 2\u03ba\u22121 ,\nwhere for the penultimate inequality we use the inequality\nrz(z1/r \u2212 1) > z1/r(z \u2212 1),\nwhich holds for all z, r \u2265 1,2 with z = c and r = 2\u03ba\u2212 1. It is now easy to verify that there is a universal constant T > 0 such that if s \u2265 T\u03ba log \u03ba then Equation (3) holds.\n2To see this, let\nf(z, r) , rz(z1/r \u2212 1)\u2212 z1/r(z \u2212 1)\nz\n= (r \u2212 1)z1/r + z1/r\u22121 \u2212 r.\nTaking the partial derivative with respect to z, we have\n\u2202\n\u2202z f(z, r) =\n(r \u2212 1)(z \u2212 1)z1/r\u22122\nr ,\nwhich is clearly non-negative for z, r \u2265 1. Also, f(1, r) = 0. So, we have shown that f(z, r) \u2265 0 for all z, r \u2265 1, which implies the claim.\nIt is important to note that it should be possible to extend Theorem 1 to other positional scoring rules defined by a score vector (\u03b11, . . . , \u03b1m) where \u03b1j > \u03b1j+1 for all j = 1, . . . ,m\u2212 1. However, Borda count is especially practical and easy to explain (see Section 7 for more on this), which is why we focus on it for our positive result."
                },
                {
                    "heading": "5. Non-Robustness of PMC Rules",
                    "text": "Theorem 1 shows that Borda count is robust against noisy perturbations of the preference profile. It is natural to ask whether \u2018many\u2019 voting rules satisfy a similar property. In this section we answer this question in the negative, by proving that any voting rule that belongs to the important family of PMC rules is not robust in a similar sense.\nSpecifically, recall that under a PMC rule, when the weighted pairwise majority graph is acyclic, the output ranking is the topological ordering of the pairwise majority graph. We show that there exist profiles in which the pairwise majority graph is acyclic and all edge weights are large, but, with high probability, the noisy profile also has an acyclic pairwise majority graph which induces a different ranking. This means that any PMC rule would return different rankings when applied to the true profile and the noisy profile. Theorem 2. For all \u03b4 > 0, \u03c6 \u2208 (0, 1), and m \u2208 N such that m \u2265 3, there exists n0 \u2208 N such that for all n \u2265 n0, there exists a profile \u03c3? \u2208 Ln such that \u0393(\u03c3?) is acyclic and all edges have weight \u2126(n), but with probability at least 1\u2212\u03b4 \u0393(\u03c3) is acyclic and there is a pair of alternatives on which the unique rankings induced by \u0393(\u03c3?) and \u0393(\u03c3) disagree, where the probability is taken over the sampling of \u03c3.\nIt is instructive to contrast our positive result, Theorem 1, with this negative result. On a very high level, the former result asserts that \u201cif Borda count says that the gaps between alternatives are significant, then the alternatives will not flip under Borda count,\u201d whereas the latter says \u201ceven if a PMC rule says that the gaps between alternatives are very significant, some alternatives are likely to flip under that rule.\u201d On a technical level, a subtle difference is that Theorem 1 is stated for \u03b2 and \u03ba, whereas Theorem 2 is stated directly for \u03c6. This actually strengthens the negative result, because a constant \u03b2 and \u03ba \u2208 \u03c9(1) lead to \u03c6 = 1 \u2212 o(1), i.e., very noisy distributions \u2014 and still the positive result of Theorem 1 holds. By contrast, the negative result of Theorem 2 is true even when \u03c6 is constant, i.e., for settings that are not nearly as noisy. That said, the two results are not directly comparable, as Borda count and PMC rules deal with very different notions of score or weight. Nevertheless, the takehome message is that the notion of score that defines Borda count is inherently more robust to random perturbations of the preference profile.\nThe proof of Theorem 2 is rather technical, and appears in the full version of the paper. In a nutshell, we construct a preference profile \u03c3? with \u03b1n voters whose preferences are x? x1 \u00b7 \u00b7 \u00b7 , and (1 \u2212 \u03b1)n voters whose preferences are x1 \u00b7 \u00b7 \u00b7 x?, for \u03b1 > 1/2. This profile induces a ranking where x? is first and x1 is second. However, it can be seen that, in the sampled profile \u03c3, many voters from the first group would flip x? and x1, leading to a majority who prefer x1 to x?. Furthermore, we prove the nontrivial claim that \u0393(\u03c3) is likely to be acyclic (\u2018nontrivial\u2019 because it is unclear there would not be a cycle involving x?), which completes the argument."
                },
                {
                    "heading": "6. Empirical Results",
                    "text": "In Section 4 we have established that Borda count is robust to prediction error. However, our positive theoretical result, Theorem 1, only provides asymptotic guarantees. In this section, we evaluate the performance of Borda count on profiles of size that is more representative of real-world instances. For our evaluation metric, we consider the probability of the rule flipping alternatives when aggregating noisy rankings against their difference in Borda score in the underlying true profile.\nAll of our code is open-source and can be found at https://github.com/akahng/VirtualDemocracy-ICML2019."
                },
                {
                    "heading": "6.1. Methodology",
                    "text": "Given n voters, m alternatives, a Mallows parameter \u03c6 \u2208 (0, 1), and a probability p \u2208 [0, 1], we generate a true profile \u03c3? = (\u03c3?1 , . . . , \u03c3 ? n) from a mixture of Mallows models. Specifically, each ranking is drawn with probability p from a Mallows model with base ranking x1 x2 \u00b7 \u00b7 \u00b7 xm and parameter \u03c6, and with probability 1\u2212p from a Mallows model with base ranking xm xm\u22121 \u00b7 \u00b7 \u00b7 x1 and parameter \u03c6.\nWe then repeatedly generate noisy profiles \u03c3 = (\u03c31, . . . , \u03c3n) where each \u03c3i is generated by a Mallows model centered at \u03c3?i with parameter \u03c6. For every pair of alternatives (xi, xj) such that B(xi,\u03c3?) > B(xj ,\u03c3?) \u2014 that is, xi beat xj when Borda count was applied to the true profile \u2014 we calculate the percentage of noisy profiles that flipped the order of xi and xj , i.e., those where B(xj ,\u03c3) > B(xi,\u03c3). Based on the true difference in Borda scores B(xi,\u03c3?) \u2212 B(xj ,\u03c3?), we place this data point in the appropriate bucket, where the width of each bucket corresponds to an average Borda score difference of 1. This way we can relate the Borda score difference to the probability of making a pairwise prediction error. Note that starting from a mixture of \u2018opposite\u2019 ranking models allows us to vary the distribution over score differences in \u03c3? by varying p."
                },
                {
                    "heading": "6.2. Results",
                    "text": "Throughout our experiments, we let n = 100, m = 40, \u03c6 \u2208 {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9}, and p \u2208 {1, 0.7, 0.5}. Our results for p = 1, shown in Figure 1, plot the average probability of flipping the order of alternatives as a function of the difference in average Borda scores of the alternatives, where comparisons are bucketed by the difference in average Borda score. For \u03c6 \u2208 {0.1, 0.2, 0.3}, the observed probability of flipping any two alternatives, regardless of average Borda score difference, is 0; i.e., there are no mistakes.\nAt a high level, error rate decreases with true average Borda score distance in all experiments. Note that the maximum observed error rate increases with the Mallows parameter \u03c6, which is intuitive because higher values of \u03c6 imply noisier (more uniformly random) rankings, so the probability of swapping alternatives should increase. However, for all values of \u03c6 and under all methods of generating profiles, the probability of making errors quickly decreases with average Borda score difference in the true profile.\nSimilar plots for p = 0.7 and p = 0.5 are included in the full version of the paper; these plots support the observation that the probability of making a mistake depends on the average Borda score difference, and not on the particular methods used to sample the underlying true profile."
                },
                {
                    "heading": "7. Discussion",
                    "text": "Our theoretical and empirical results identify Borda count as an especially attractive voting rule for virtual democracy, from a statistical viewpoint. However, Borda count is also compelling in terms of usability and explainability.\nIn more detail, in our implemented donor-recipient matching system, clicking on a recommended alternative displays an explanation for why it was ranked highly by Borda count, which consists of two components. First, we show the alternative\u2019s average position in the predicted preferences of each of the four stakeholder groups. Note that this information determines the Borda score of the alternative, given the weight of each stakeholder group.3 Second \u2014 this is the more novel component \u2014 we show specific features in which the recommended alternative stands out. This is interesting because classic social choice theory does not have features for alternatives, and we are able to give this type of explanation precisely because our alternatives are represented as vectors of features (which is crucial for the application of learning-to-rank algorithms).\nBased on the results presented in this paper, as well as these additional insights, we use Borda count in our implemented virtual-democracy-based system.\n3These weights were decided by the stakeholders themselves."
                }
            ],
            "year": 2019,
            "references": [
                {
                    "title": "Online fair division: Analysing a food bank problem",
                    "authors": [
                        "M. Aleksandrov",
                        "H. Aziz",
                        "S. Gaspers",
                        "T. Walsh"
                    ],
                    "venue": "In Proceedings of the 24th International Joint Conference on Artificial Intelligence (IJCAI),",
                    "year": 2015
                },
                {
                    "title": "Social Choice and Individual Values",
                    "authors": [
                        "K. Arrow"
                    ],
                    "year": 1951
                },
                {
                    "title": "Random utility theory for social choice",
                    "authors": [
                        "H. Azari Soufiani",
                        "D.C. Parkes",
                        "L. Xia"
                    ],
                    "venue": "In Proceedings of the 26th Annual Conference on Neural Information Processing Systems (NIPS),",
                    "year": 2012
                },
                {
                    "title": "Preference elicitation for general random utility models",
                    "authors": [
                        "H. Azari Soufiani",
                        "D.C. Parkes",
                        "L. Xia"
                    ],
                    "venue": "In Proceedings of the 29th Annual Conference on Uncertainty in Artificial Intelligence (UAI),",
                    "year": 2013
                },
                {
                    "title": "Computing parametric ranking models via rank-breaking",
                    "authors": [
                        "H. Azari Soufiani",
                        "D.C. Parkes",
                        "L. Xia"
                    ],
                    "venue": "In Proceedings of the 31st International Conference on Machine Learning (ICML),",
                    "year": 2014
                },
                {
                    "title": "Making right decisions based on wrong opinions",
                    "authors": [
                        "G. Benade",
                        "A. Kahng",
                        "A.D. Procaccia"
                    ],
                    "venue": "In Proceedings of the 18th ACM Conference on Economics and Computation (EC),",
                    "year": 2017
                },
                {
                    "title": "Handbook of Computational Social Choice",
                    "authors": [
                        "F. Brandt",
                        "V. Conitzer",
                        "U. Endriss",
                        "J. Lang",
                        "Procaccia",
                        "A.D. (eds"
                    ],
                    "year": 2016
                },
                {
                    "title": "Sorting from noisy information",
                    "authors": [
                        "M. Braverman",
                        "E. Mossel"
                    ],
                    "venue": "arXiv preprint arXiv:0910.1191,",
                    "year": 2009
                },
                {
                    "title": "Modal ranking: A uniquely robust voting rule",
                    "authors": [
                        "I. Caragiannis",
                        "A.D. Procaccia",
                        "N. Shah"
                    ],
                    "venue": "In Proceedings of the 28th AAAI Conference on Artificial Intelligence (AAAI),",
                    "year": 2014
                },
                {
                    "title": "When do noisy votes reveal the truth",
                    "authors": [
                        "I. Caragiannis",
                        "A.D. Procaccia",
                        "N. Shah"
                    ],
                    "venue": "ACM Transactions on Economics and Computation,",
                    "year": 2016
                },
                {
                    "title": "Common voting rules as maximum likelihood estimators",
                    "authors": [
                        "V. Conitzer",
                        "T. Sandholm"
                    ],
                    "venue": "In Proceedings of the 21st Annual Conference on Uncertainty in Artificial Intelligence (UAI),",
                    "year": 2005
                },
                {
                    "title": "Preference functions that score rankings and maximum likelihood estimation",
                    "authors": [
                        "V. Conitzer",
                        "M. Rognlie",
                        "L. Xia"
                    ],
                    "venue": "In Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI),",
                    "year": 2009
                },
                {
                    "title": "Mallowssmoothed distribution over rankings approach for modeling choice",
                    "authors": [
                        "A. D\u00e9sir",
                        "V. Goyal",
                        "S. Jagabathula",
                        "D. Segev"
                    ],
                    "venue": "SSRN preprint,",
                    "year": 2018
                },
                {
                    "title": "The repeated insertion model for rankings: Missing link between two subset choice models",
                    "authors": [
                        "Doignon",
                        "J.-P",
                        "A. Peke\u010d",
                        "M. Regenwetter"
                    ],
                    "year": 2004
                },
                {
                    "title": "Electing the most probable without eliminating the irrational: Voting over intransitive domains",
                    "authors": [
                        "E. Elkind",
                        "N. Shah"
                    ],
                    "venue": "In Proceedings of the 30th Annual Conference on Uncertainty in Artificial Intelligence (UAI),",
                    "year": 2014
                },
                {
                    "title": "Adapting a kidney exchange algorithm to align with human values",
                    "authors": [
                        "R. Freedman",
                        "J. Schaich Borg",
                        "W. Sinnott-Armstrong",
                        "J.P. Dickerson",
                        "V. Conitzer"
                    ],
                    "venue": "In Proceedings of the 32nd AAAI Conference on Artificial Intelligence (AAAI),",
                    "year": 2018
                },
                {
                    "title": "Diverse randomized agents vote to win",
                    "authors": [
                        "A.X. Jiang",
                        "L.S. Marcolino",
                        "A.D. Procaccia",
                        "T. Sandholm",
                        "N. Shah",
                        "M. Tambe"
                    ],
                    "venue": "In Proceedings of the 28th Annual Conference on Neural Information Processing Systems (NIPS),",
                    "year": 2014
                },
                {
                    "title": "Robust approximation and incremental elicitation in voting protocols",
                    "authors": [
                        "T. Lu",
                        "C. Boutilier"
                    ],
                    "venue": "In Proceedings of the 22nd International Joint Conference on Artificial Intelligence (IJCAI),",
                    "year": 2011
                },
                {
                    "title": "Better human computation through principled voting",
                    "authors": [
                        "A. Mao",
                        "A.D. Procaccia",
                        "Y. Chen"
                    ],
                    "venue": "In Proceedings of the 27th AAAI Conference on Artificial Intelligence (AAAI),",
                    "year": 2013
                },
                {
                    "title": "A votingbased system for ethical decision making",
                    "authors": [
                        "R. Noothigattu",
                        "S.S. Gaikwad",
                        "E. Awad",
                        "S. Dsouza",
                        "I. Rahwan",
                        "P. Ravikumar",
                        "A.D. Procaccia"
                    ],
                    "venue": "In Proceedings of the 32nd AAAI Conference on Artificial Intelligence (AAAI),",
                    "year": 2018
                },
                {
                    "title": "A maximum likelihood approach for selecting sets of alternatives",
                    "authors": [
                        "A.D. Procaccia",
                        "S.J. Reddi",
                        "N. Shah"
                    ],
                    "venue": "In Proceedings of the 28th Annual Conference on Uncertainty in Artificial Intelligence (UAI),",
                    "year": 2012
                },
                {
                    "title": "Voting rules as error-correcting codes",
                    "authors": [
                        "A.D. Procaccia",
                        "N. Shah",
                        "Y. Zick"
                    ],
                    "venue": "Artificial Intelligence,",
                    "year": 2016
                },
                {
                    "title": "The axiomatic approach and the Internet",
                    "authors": [
                        "M. Tennenholtz",
                        "A. Zohar"
                    ],
                    "venue": "Handbook of Computational Social Choice,",
                    "year": 2016
                },
                {
                    "title": "Bayesian estimators as voting rules",
                    "authors": [
                        "L. Xia"
                    ],
                    "venue": "In Proceedings of the 32nd Annual Conference on Uncertainty in Artificial Intelligence (UAI),",
                    "year": 2016
                },
                {
                    "title": "A maximum likelihood approach towards aggregating partial orders",
                    "authors": [
                        "L. Xia",
                        "V. Conitzer"
                    ],
                    "venue": "In Proceedings of the 22nd International Joint Conference on Artificial Intelligence (IJCAI),",
                    "year": 2011
                },
                {
                    "title": "Aggregating preferences in multi-issue domains by using maximum likelihood estimators",
                    "authors": [
                        "L. Xia",
                        "V. Conitzer",
                        "J. Lang"
                    ],
                    "venue": "In Proceedings of the 9th International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS),",
                    "year": 2010
                }
            ],
            "id": "SP:c630c7c327f09dc30d2564c963e42403126b95a2",
            "authors": [
                {
                    "name": "Anson Kahng",
                    "affiliations": []
                },
                {
                    "name": "Min Kyung Lee",
                    "affiliations": []
                },
                {
                    "name": "Ritesh Noothigattu",
                    "affiliations": []
                },
                {
                    "name": "Ariel D. Procaccia",
                    "affiliations": []
                },
                {
                    "name": "Alexandros Psomas",
                    "affiliations": []
                }
            ],
            "abstractText": "Virtual democracy is an approach to automating decisions, by learning models of the preferences of individual people, and, at runtime, aggregating the predicted preferences of those people on the dilemma at hand. One of the key questions is which aggregation method \u2014 or voting rule \u2014 to use; we offer a novel statistical viewpoint that provides guidance. Specifically, we seek voting rules that are robust to prediction errors, in that their output on people\u2019s true preferences is likely to coincide with their output on noisy estimates thereof. We prove that the classic Borda count rule is robust in this sense, whereas any voting rule belonging to the wide family of pairwisemajority consistent rules is not. Our empirical results further support, and more precisely measure, the robustness of Borda count.",
            "title": "Statistical Foundations of Virtual Democracy"
        },
        "Y": {
            "blog_id": "statistical-foundations-of-virtual-democracy",
            "summary": [
                "Statiscal foundations of virtual democracy Kahng et al., ICML\u201919  This is another paper on the theme of combining information and making decisions in the face of noise and uncertainty \u2013 but the setting is quite different to those we\u2019ve been looking at recently.",
                "Consider a food bank that receives donations of food and distributes it to those in need.",
                "The goal is to implement an automated decision making system such that when a food donation is received, the system outputs the organisation (e.g. housing authority or food pantry) that should receive it.",
                "We could hard code a set of rules, but what should they be?",
                "And who gets to decide?",
                "A democratic solution to this would be to give each of the stakeholders a vote on every decision.",
                "In the food bank setting, identified classes of stakeholders include the donors, the recipients, the volunteers (who pick up food from the donor and deliver it to the recipient), and employees.",
                "Their votes encode their own preferences and biases, perhaps in a way that even the voters themselves couldn\u2019t neatly codify in a set of explicit rules.",
                "It\u2019s not really practical to have an actual vote with all stakeholders participating every time a food donation is made though!",
                "One of the most basic ideas underlying democracy is that complicated decisions can be made by asking a group of people to vote on the alternatives at hand.",
                "As a decision-making framework, this paradigm is versatile, because people can express a sensible opinion about a wide range of issues.",
                "One of its seemingly inherent shortcomings, though, is that voters must take the time to cast a vote\u2014 hopefully an informed one\u2014 every time a new dilemma arises.",
                "The big idea behind virtual democracy is that we learn the voting preferences of each stakeholder, essentially creating an agent which is able to vote in their place, a virtual voter.",
                "Then when we need to make a decision we ask those virtual voters to cast their votes (in the form of a preference ranking).",
                "The central question in this paper is this: given a set of preference rankings, how should we combine them to produce an actual decision?",
                "The procedure for doing this is known as the voting rule.",
                "\u2026 the choice of voting rule can have a major impact on the efficacy of the system.",
                "In fact, the question of which voting rule to employ is one of the central questions in computational social choice.",
                "It\u2019s one thing to come up with a voting rule that works well when we have the actual true preference rankings of all of the stakeholders.",
                "In a virtual democracy setting though, where we have learned approximations to those preference rankings, a highly desirable feature of a voting rule is that it is robust to noise.",
                "I.e., we want a voting rule whereby\u2026  \u2026 the output on the true preferences is likely to coincide with the output on noisy estimates thereof.",
                "Learning preferences  To learn voter preferences, voters are asked to make a set of pairwise comparisons (about 100) between alternatives.",
                "I.e., given this donation, should it be sent to recipient A or recipient B?",
                "Each alternative is presented as a set of pre-determined features.",
                "In the case of the food bank question voters are given information about the type of donation, and seven additional features such as distance between the donor and recipient, and when the recipient last received a donation.",
                "At the end of this process, the training data is used to learn a model of the preferences of the voter.",
                "This model is then used to predict the voter\u2019s preference ranking over many hundreds of recipients for a given donation.",
                "The Mallows model  To be able to compare the efficacy of various voting rules, we\u2019re going to need a way to compare how good their outputs are.",
                "The Kendall tau (KT) distance between two rankings (permutations) of a set is defined as the number of pairs of alternatives on which the rankings disagree.",
                "By disagree we mean that given a pair  one ranks  ahead of  , and the other ranks  ahead of  .",
                "For example, the KT distance between  and  is 2.",
                "The Mallows (1957) model was originally designed for use in situations where there is true ranking of the alternatives, and assigns a probability that a given voter is associated with a given alternative ranking.",
                "The probability decreases exponentially with the number of pairs of alternatives on which the true and alternative ranking disagree, i.e., their KT distance.",
                "A Mallows model is parameterised by a  parameter  .",
                "Our technical approach relies on the observation that the classic Mallows (1957).",
                "model is an unusually good fit with our problem.",
                "In the problem at hand, instead of a single true ranking, each voter has their own true ranking.",
                "When validating a learned  model, the test for accuracy is done using pairwise comparisons, just like in Mallows.",
                "Given an observed prediction accuracy  , we can relate this accuracy to an underlying Mallows model through a parameter  , where pairwise comparisons are drawn from within the top  ranked items in the true ranking.",
                "(See \u00a73 in the paper).",
                "Voting rules and the Borda count  The next piece of the puzzle is the selection of a voting rule to combine rankings and produce a final decision.",
                "The main result in the paper concerns the Borda count voting rule.",
                "Borda count is a positional scoring rule.",
                "Positional scoring rules give a score vector that assigns points to each position in a ranking.",
                "E.g. 5 points for being ranked first, 3 points for being ranked second, and so on.",
                "The score of an alternative is the sum of its ranking points across all of the voters.",
                "The alternative with the biggest score wins (break ties via random selection).",
                "The Borda count uses a very straightforward score vector: if there are  alternatives in the ranking, the score vector is defined as  .",
                "The heart of the paper is \u00a74, where drawing on the properties of the Mallows model, it\u2019s relationship to the predicted accuracy, and the Borda count rule, the authors show the Borda count is surprisingly robust to noise.",
                "I\u2019m going to happily skip over the proofs here and leave you to follow up on those if you\u2019re interested!",
                "\u2026 it is intuitive that the separation in Borda scores has to depend on  , but it is encouraging (and, to us, surprising) that his dependence is almost linear\u2026 the theorem implies that our noisy Borda ranking is highly unlikely to make mistakes on pairs of alternatives whose average score difference is linear in  .",
                "Other rules  So far so good, but what about other voting rules?",
                "Are they also robust to noise or is there something special about the Borda count?",
                "The main alternative to positional scoring rules are pairwise-majority consistent (PMC) rules, of which there are many examples (e.g., the ranked pairs method).",
                "The key result in \u00a75 of the paper is that all rules in this class are not robust to noise.",
                "It is instructive to contrast our positive result, Theorem 1, with this negative result.",
                "On a very high level, the former result asserts that \u201cif Borda count says that the gaps between alternatives are signi\ufb01cant, then the alternatives will not \ufb02ip under Borda count,\u201d whereas the latter says \u201ceven if a PMC rule says that the gaps between alternatives are very signi\ufb01cant, some alternatives are likely to \ufb02ip under that rule.\u201d  Borda count FTW  So there you have it: if you need to robustly combine noisy rankings of alternatives to make a decision, use the Borda count!",
                "Our theoretical and empirical results identify Borda count as an especially attractive voting rule for virtual democracy, from a statistical viewpoint.",
                "Another important feature of the Borda count rule is that the decisions it takes can be easily explained.",
                "An explanation consists of two elements: first the average position in the predicted preferences of each of the stakeholder groups, and second the features that were most important in achieving that ranking position (possible since alternatives are presented as vectors of features)."
            ],
            "author_id": "ACOLYER",
            "pdf_url": "http://proceedings.mlr.press/v97/kahng19a/kahng19a.pdf",
            "author_full_name": "Adrian Colyer",
            "source_website": "https://blog.acolyer.org/about/",
            "id": 10587146
        }
    }
}
{"article_lines": ["A Unigram Orientation Model For Statistical Machine Translation", "Lebanese violate warplanes Israeli airspace A l T A r A t A l H r b y P y l y P A l A s r A t n t h k A l m j A l A l j w y A l l b n A n y Abstract In this paper , we present a unigram segmentation model for statistical machine translation where the segmentation units are blocks pairs of phrases without internal structure .", "The segmentation model uses a novel orientation component to handle swapping of neighbor blocks .", "During training , we collect block uncounts with we count how often a block occurs to the left or to the right of some predecessor block .", "The orientation model is shown to improve translation performance over two models 1 no block re ordering is used , and 2 the block swapping is controlled only by a language model .", "We show experimental results on a standard Arabic English translation task .", "In recent years , phrase based systems for statistical machine translation Och et al . , 1999 ; Koehn et al . , 2003 ; Venugopal et al . , 2003 have delivered state of the art performance on standard translation tasks .", "In this paper , we present a phrase based unigram system similar to the one in Tillmann and Xia , 2003 , which is extended by an unigram orientation model .", "The units of translation are blocks , pairs of phrases without internal structure .", "Fig .", "1 shows an example block translation using five Arabic English blocks .", "The unigram orientation model is trained from word aligned training data .", "During decoding , we view translation as a block segmentation process , where the input sentence is segmented from left to right and the target sentence is generated from bottom to top , one block at a time .", "A monotone block sequence is generated except for the possibility to swap a pair of neighbor blocks .", "The novel orientation model is used to assist the block swapping as shown in section 3 , block swapping where only a trigram language model is used to compute probabilities between neighbor blocks fails to improve translation performance .", "Wu , 1996 ; Zens and Ney , 2003 present re ordering models that make use of a straight inverted orientation model that is related to our work .", "Here , we investigate in detail the effect of restricting the word re ordering to neighbor block swapping only .", "In this paper , we assume a block generation process that generates block sequences from bottom to top , one block at a time .", "The score of a successor block depends on its predecessor block and on its orientation relative to the block .", "In Fig .", "1 for example , block is the predecessor of block , and block is the predecessor of block .", "The target clump of a predecessor block is adjacent to the target clump of a successor block .", "A right adjacent predecessor block is a block where additionally the source clumps are adjacent and the source clump of occurs to the right of the source clump of .", "A left adjacent predecessor block is defined accordingly .", "During decoding , we compute the score of a block sequence with orientation as a product of block bigram scores where is a block and is a three valued orientation component linked to the block the orientation of the predecessor block is ignored . .", "A block has right orientation if it has a left adjacent predecessor .", "Accordingly , a block has left orientation if it has a right adjacent predecessor .", "If a block has neither a left or right adjacent predecessor , its orientation is neutral .", "The neutral orientation is not modeled explicitly in this paper , rather it is handled as a default case as explained below .", "In Fig .", "1 , the orientation sequence is , i . e . block and block are generated using left orientation .", "During decoding most blocks have right orientation , since the block translations are mostly monotone .", "We try to find a block sequence with orientation that maximizes .", "The following three types of parameters are used to model the block bigram score in Eq .", "1 Two unigram count based models and .", "We compute the unigram probability of a block based on its occurrence count .", "The blocks are counted from word aligned training data .", "We also collect unigram counts with orientation a left count and a right count .", "These counts are defined via an enumeration process and are used to define the orientation model Trigram language model The block language model score is computed as the probability of the first target word in the target clump of given the final two words of the target clump of .", "The three models are combined in a log linear way , as shown in the following section .", "The basic idea of the orientation model can be illustrated as follows In the example translation in Fig .", "1 , block occurs to the left of block .", "Although the joint block consisting of the two smaller blocks and has not been seen in the training data , we can still profit from the fact that block occurs more frequently with left than with right orientation .", "In our Arabic English training data , block has been seen times with left orientation , and with right orientation , i . e . it is always involved in swapping .", "This intuition is formalized using unigram counts with orientation .", "The orientation model is related to the distortion model in Brown et al . , 1993 , but we do not compute a block alignment during training .", "We rather enumerate all relevant blocks in some order .", "Enumeration does not allow us to capture position dependent distortion probabilities , but we can compute statistics about adjacent block predecessors .", "Our baseline model is the unigram monotone model described in Tillmann and Xia , 2003 .", "Here , we select blocks from word aligned training data and unigram block occurrence counts are computed all blocks for a training sentence pair are enumerated in some order and we count how often a given block occurs in the parallel training data 1 .", "The training algorithm yields a list of about blocks per training sentence pair .", "In this paper , we make extended use of the baseline enumeration procedure for each block , we additionally enumerate all its left and right predecessors .", "No optimal block segmentation is needed to compute the predecessors for each block , we check for adjacent predecessor blocks that also occur in the enumeration list .", "We compute left orientation counts as follows Here , we enumerate all adjacent predecessors of block over all training sentence pairs .", "The identity of is ignored . is the number of times the block succeeds some right adjacent predecessor block .", "The right orientation count is defined accordingly .", "Note , that in general the unigram count during enumeration , a block might have both left and right adjacent predecessors , either a left or a right adjacent predecessor , or no adjacent predecessors at all .", "The orientation count collection is illustrated in Fig .", "2 each time a block has a left or right adjacent predecessor in the parallel training data , the orientation counts are incremented accordingly .", "The decoding orientation restrictions are illustrated in Fig 3 a monotone block sequence with right 'We keep all blocks for which and the phrase length is less or equal .", "No other selection criteria are applied .", "For the model , we keep all blocks for which . order for each block , we look for left and right adjacent predecessors . orientation is generated .", "If a block is skipped e . g . block in Fig 3 by first generating block then block , the block is generated using left orientation .", "Since the block translation is generated from bottom to top , the blocks and do not have adjacent predecessors below them they are generated by a default model without orientation component .", "The orientation model is given in Eq .", "2 , the default model is given in Eq .", "The block bigram model in Eq .", "1 is defined as where and the orientation of the predecessor is ignored .", "The are chosen to be optimal on the devtest set the optimal parameter setting is shown in Table .", "Only two parameters have to be optimized due to the constraint that the have to sum to .", "The default model is defined as .", "Straightforward normalization over all successor blocks in Eq .", "2 and in Eq .", "3 is not feasible there are tens of millions of possible successor blocks .", "In future work , normalization over a restricted successor set , e . g . for a given source input sentence , all blocks that match this sentence might be useful for both training and decoding .", "The segmentation model in Eq .", "1 naturally prefers translations that make use of a smaller number of blocks which leads to a smaller number of factors in Eq .", "Using fewer bigger blocks to carry out the translation generally seems to improve translation performance .", "Since normalization does not influence the number of blocks used to carry out the translation , it might be less important for our segmentation model .", "We use a DP based beam search procedure similar to the one presented in Tillmann and Xia , 2003 .", "We maximize over all block segmentations with orientation for which the source phrases yield a segmentation of the input sentence .", "Swapping involves only blocks for which for the successor block , e . g . the blocks and in Fig 1 .", "We tried several thresholds for , and performance is reduced significantly only if .", "No other parameters are used to control the block swapping .", "In particular the orientation of the predecessor block is ignored in future work , we might take into account that a certain predecessor block typically precedes other blocks .", "The translation system is tested on an Arabic to English translation task .", "The training data comes from the UN news sources million Arabic and million English words .", "The training data is sentence aligned yielding million training sentence pairs .", "The Arabic data is romanized , some punctuation tokenization and some number classing are carried out on the English and the Arabic training data .", "As devtest set , we use testing data provided by LDC , which consists of sentences with Arabic words with reference translations .", "As a blind test set , we use MT 03 Arabic English DARPA evaluation test set consisting of sentences with Arabic words .", "Three systems are evaluated in our experiments is the baseline block unigram model without re ordering .", "Here , monotone block alignments are generated the blocks have only left predecessors no blocks are swapped .", "This is the model presented in Tillmann and Xia , 2003 .", "For the model , the sentence is translated mostly monotonously , and only neighbor blocks are allowed to be swapped at most block is skipped .", "The model allows for the same block swapping as the model , but additionally uses the orientation component described in Section 2 the block swapping is controlled where .", "The are not optimized separately , rather we define by the unigram orientation counts .", "The and models use the block bigram model in Eq .", "3 all blocks are generated with neutral orientation , and only two components , the block unigram model and the block bigram score are used .", "Experimental results are reported in Table 1 three BLEU results are presented for both devtest set and blind test set .", "Two scaling parameters are set on the devtest set and copied for use on the blind test set .", "The second column shows the model name , the third column presents the optimal weighting as obtained from the devtest set by carrying out an exhaustive grid search .", "The fourth column shows BLEU results together with confidence intervals Here , the word casing is ignored .", "The block swapping model obtains a statistical significant improvement over the baseline model .", "Interestingly , the swapping model without orientation performs worse than the baseline model the word based trigram language model alone is too weak to control the block swapping the model is too unrestrictive to handle the block swapping reliably .", "Additionally , Table 2 presents devtest set example blocks that have actually been swapped .", "The training data is unsegmented , as can be seen from the first two blocks .", "The block in the first line has been seen times more often with left than with right orientation .", "Blocks for which the ratio is bigger than are likely candidates for swapping in our Arabic English experiments .", "The ratio itself is not currently used in the orientation model .", "The orientation model mostly effects blocks where the Arabic and English words are verbs or nouns .", "As shown in Fig .", "1 , the orientation model uses the orientation probability for the noun block , and only the default model for the adjective block .", "Although the noun block might occur by itself without adjective , the swapping is not controlled by the occurrence of the adjective block which does not have adjacent predecessors .", "We rather model the fact that a noun block is typically preceded by some block .", "This situation seems typical for the block swapping that occurs on the evaluation test set .", "This work was partially supported by DARPA and monitored by SPAWAR under contract No .", "N66001 99 28916 .", "The paper has greatly profited from discussion with Kishore Papineni and Fei Xia ."], "summary_lines": ["A Unigram Orientation Model For Statistical Machine Translation\n", "In this paper, we present a unigram segmentation model for statistical machine translation where the segmentation units are blocks: pairs of phrases without internal structure.\n", "The segmentation model uses a novel orientation component to handle swapping of neighbor blocks.\n", "During training, we collect block unigram counts with orientation: we count how often a block occurs to the left or to the right of some predecessor block.\n", "The orientation model is shown to improve translation performance over two models: 1) no block re-ordering is used, and 2) the block swapping is controlled only by a language model.\n", "We show experimental results on a standard Arabic-English translation task.\n", "This work introduces lexical features for distortion modeling.\n"]}
{"article_lines": ["Disambiguation Of Proper Names In Text", "trieving information from full text using linguisknowledge , In of the Fifteenth Online Meeting , New York , May .", "Text processing applications , such as machine translation systems , information retrieval systems or natural language understanding systems , need to identify multi word expressions that refer to proper names of people , organizations , places , laws and other entities .", "When encountering Mrs . Candy Hill in input text , for example , a machine translation system should not attempt to look up the translation of candy and hill , but should translate Mrs . to the appropriate personal title in the target language and preserve the rest of the name intact .", "Similarly , an information retrieval system should not attempt to expand Candy to all of its morphological variants or suggest synonyms Wacholder et al . 1994 .", "The need to identify proper names has two aspects the recognition of known names and the discovery of new names .", "Since obtaining and maintaining a name database requires significant effort , many applications need to operate in the absence of such a resource .", "Without a database , names need to be discovered in the text and linked to entities they refer to .", "Even where name databases exist , text needs to be scanned for new names that are formed when entities , such as countries or commercial companies , are created , or for unknown names which become important when the entities they refer to become topical .", "This situation is the norm for dynamic applications such as news providing services or Internet information indexing .", "The next Section describes the different types of proper name ambiguities we have observed .", "Section 3 discusses the role of context and world knowledge in their disambiguation ; Section 4 describes the process of name discovery as implemented in Nominator , a module for proper name recognition developed at the IBM T . J . Watson Research Center .", "Sections 5 7 elaborate on Nominator's disambiguation heuristics .", "Name identification requires resolution of a subset of the types of structural and semantic ambiguities encountered in the analysis of nouns and noun phrases NPs in natural language processing .", "Like common nouns , Jensen and Binot 1987 , Hindle and Rooth 1993 and Brill and Resnick 1994 , proper names exhibit structural ambiguity in prepositional phrase PP attachment and in conjunction scope .", "A PP may be attached to the preceding NP and form part of a single large name , as in NP Midwest Center PP for NP Computer Research .", "Alternatively it may be independent of the preceding NP , as in NP Carnegie Hall PP for NP Irwin Berlin , where for separates two distinct names , Carnegie Hall and Irwin Berlin .", "As with PP attachment of common noun phrases , the ambiguity is not always resolved , even in human sentence parsing cf . the famous example I saw the girl in the park with the telescope .", "The location of an organization , for instance , could be part of its name City University of New York or an phrases .", "The components of Victoria and Albert Museum and IBM and Bell Laboratories look identical ; however , and is part of the name of the museum in the first example , but a conjunction joining two computer company names in the second .", "Although this problem is well known , a search of the computational literature shows that few solutions have been proposed , perhaps because the conjunct ambiguity problem is harder than PP attachment though see Agarwal and Boggess 1992 for a method of conjunct identification that relies on syntactic category and semantic label .", "Similar structural ambiguity exists with respect to the possessive pronoun , which may indicate a relationship between two names e . g . , Israel's Shimon Peres or may constitute a component of a single name e . g . , Donoghue's Money Fund Report .", "The resolution of structural ambiguity such as PP attachment and conjunction scope is required in order to automatically establish the exact boundaries of proper names .", "Once these boundaries have been established , there is another type of well known structural ambiguity , involving the internal structure of the proper name .", "For example , Professor of Far Eastern Art John Blake is parsed as Professor of Far Eastern Art John Blake whereas Professor Art Klein is Professor Art Klein .", "Proper names also display semantic ambiguity .", "Identification of the type of proper nouns resembles the problem of sense disambiguation for common nouns where , for instance , state taken out of context may refer either to a government body or the condition of a person or entity .", "A name variant taken out of context may be one of many types , e . g . , Ford by itself could be a person Gerald Ford , an organization Ford Motors , a make of car Ford , or a place Ford , Michigan .", "Entity type ambiguity is quite common , as places are named after famous people and companies are named after their owners or locations .", "In addition , naming conventions are sometimes disregarded by people who enjoy creating novel and unconventional names .", "A store named Mr .", "Tall and a woman named April Wednesday McDonald 1993 come to mind .", "Like common nouns , proper nouns exhibit systematic metonymy United States refers either to a geographical area or to the political body which governs this area ; Wall Street Journal refers to the printed object , its content , and the commercial entity that produces it .", "In addition , proper names resemble definite noun phrases in that their intended referent may be ambiguous .", "The man may refer to more than one male individual previously mentioned in the discourse or present in the non linguistic context ; J . Smith may similarly refer to more than one individual named Joseph Smith , John Smith , Jane Smith , etc .", "Semantic ambiguity of names is very common because of the standard practice of using shorter names to stand for longer ones .", "Shared knowledge and context are crucial disambiguation factors .", "Paris , usually refers to the capital of France , rather than a city in Texas or the Trojan prince , but in a particular context , such as a discussion of Greek mythology , the presumed referent changes .", "Beyond the ambiguities that proper names share with common nouns , some ambiguities are particular to names noun phrases may be ambiguous between a name reading and a common noun phrase , as in Candy , the person's name , versus candy the food , or The House as an organization versus a house referring to a building .", "In English , capitalization usually disambiguates the two , though not at sentence beginnings at the beginning of a sentence , the components and capitalization patterns of New Coke and New Sears are identical ; only world knowledge informs us that New Coke is a product and Sears is a company .", "Furthermore , capitalization does not always disambiguate names from non names because what constitutes a name as opposed to a non name is not always clear .", "According to Quirk et al . 1972 names , which consist of proper nouns classified into personal names like Shakespeare , temporal names like Monday , or geographical names like Australia have 'unique' reference .", "Proper nouns differ in their linguistic behavior from common nouns in that they mostly do not take determiners or have a plural form .", "However , some names do take determiners , as in The New York Times ; in this case , they quot ; are perfectly regular in taking the definite article since they are basically premodified count nouns . . .", "The difference between an ordinary common noun and an ordinary common noun turned name is that the unique reference of the name has been institutionalized , as is made overt in writing by initial capital letter . quot ; Quirk et al . 's description of names seems to indicate that capitalized words like Egyptian an adjective or Frenchmen a noun referring to a set of individuals are not names .", "It leaves capitalized sequences like Minimum Alternative Tax , Annual Report , and Chairman undetermined as to whether or not they are names .", "All of these ambiguities must be dealt with if proper names are to be identified correctly .", "In the rest of the paper we describe the resources and heuristics we have designed and implemented in Nominator and the extent to which they resolve these ambiguities .", "In general , two types of resources are available for disambiguation context and world knowledge .", "Each of these can be exploited along a continuum , from 'cheaper' to computationally and manually more expensive usage .", "'Cheaper' models , which include no context or world knowledge , do very little disambiguation .", "More 'expensive' models , which use full syntactic parsing , discourse models , inference and reasoning , require computational and human resources that may not always be available , as when massive amounts of text have to be rapidly processed on a regular basis .", "In addition , given the current state of the art , full parsing and extensive world knowledge would still not yield complete automatic ambiguity resolution .", "In designing Nominator , we have tried to achieve a balance between high accuracy and speed by adopting a model which uses minimal context and world knowledge .", "Nominator uses no syntactic contextual information .", "It applies a set of heuristics to a list of multi word strings , based on patterns of capitalization , punctuation and location within the sentence and the document .", "This design choice differentiates our approach from that of several similar projects .", "Most proper name recognizers that have been reported on in print either take as input text tagged by part of speech e . g . , the systems of Paik et al . 1993 and Mani et al .", "1993 or perform syntactic and or morphological analysis on all words , including capitalized ones , that are part of candidate proper names e . g . , Coates Stephens 1993 and McDonald 1993 .", "Several e . g . , McDonald 1993 , Mani et al . 1993 , Paik et al .", "1993 and Cowie et al . 1992 look in the local context of the candidate proper name for external information such as appositives e . g . , in a sequence such as Robin Clark , president of Clark Co . or for human subject verbs e . g . , say , plan in order to determine the category of the candidate proper name .", "Nominator does not use this type of external context .", "Instead , Nominator makes use of a different kind of contextual information proper names cooccuring in the document .", "It is a fairly standard convention in an edited document for one of the first references to an entity excluding a reference in the title to include a relatively full form of its name .", "In a kind of discourse anaphora , other references to the entity take the form of shorter , more ambiguous variants .", "Nominator identifies the referent of the full form see below and then takes advantage of the discourse context provided by the list of names to associate shorter more ambiguous name occurrences with their intended referents .", "In terms of world knowledge , the most obvious resource is a database of known names .", "In fact , this is what many commercially available name identification applications use e . g . , Hayes 1994 .", "A reliable database provides both accuracy and efficiency , if fast look up methods are incorporated .", "A database also has the potential to resolve structural ambiguity ; for example , if IBM and Apple Computers are listed individually in the database but IBM and Apple Computers is not , it may indicate a conjunction of two distinct names .", "A database may also contain default world knowledge information e . g . , with no other over riding information , it may be safe to assume that the string McDonald's refers to an organization .", "But even if an existing database is reliable , names that are not yet in it must be discovered and information in the database must be over ridden when appropriate .", "For example , if a new name such as IBM Credit Corp . occurs in the text but not in the database , while IBM exists in the database , automatic identification of IBM should be blocked in favor of the new name IBM Credit Corp .", "If a name database exists , Nominator can take advantage of it .", "However , our goal has been to design Nominator to function optimally in the absence of such a resource .", "In this case , Nominator consults a small authority file which contains information on about 3000 special 'name words' and their relevant lexical features .", "Listed are personal titles e . g . , Mr . , King , organizational identifiers including strong identifiers such as Inc . and weaker domain identifiers such as Arts and names of large places e . g . , Los Angeles , California , but not Scarsdale , N . Y . .", "Also listed are exception words , such as upper case lexical items that are unlikely to be single word proper names e . g . , Very , I or TV and lower case lexical items e . g . , and and van that can be parts of proper names .", "In addition , the authority file contains about 20 , 000 first names .", "Our choice of disambiguation resources makes Nominator fast and robust .", "The precision and recall of Nominator , operating without a database of pre existing proper names , is in the 90's while the processing rate is over 40Mg of text per hour on a RISC 6000 machine .", "See Ravin and Wacholder 1996 for details .", "This efficient processing has been achieved at the cost of limiting the extent to which the program can 'understand' the text being analyzed and resolve potential ambiguity .", "Many wordsequences that are easily recognized by human readers as names are ambiguous for Nominator , given the restricted set of tools available to it .", "In cases where Nominator cannot resolve an ambiguity with relatively high confidence , we follow the principle that 'noisy information' is to be preferred to data omitted , so that no information is lost .", "In ambiguous cases , the module is designed to make conservative decisions , such as including non names or non name parts in otherwise valid name sequences .", "It assigns weak types such as ? HUMAN or fails to assign a type if the available information is not sufficient .", "In this section , we give an overview of the process by which Nominator identifies and classifies proper names .", "Nominator's first step is to build a list of candidate names for a document .", "Next , 'splitting' heuristics are applied to all candidate names for the purpose of breaking up complex names into smaller ones .", "Finally , Nominator groups together name vanants that refer to the same entity .", "After information about names and their referents has been extracted from individual documents , an aggregation process combines the names collected from all the documents into a dictionary , or database of names , representative of the document collection .", "For more details on the process , see Ravin and Wacholder 1996 .", "We illustrate the process of name discovery with an excerpt taken from a Wall Street Journal article in the TIPSTER CD ROM collection NIST 1993 .", "Paragraph breaks are omitted to conserve space .", "The professional conduct of lawyers in other jurisdictions is guided by American Bar Association rules or by state bar ethics codes , none of which permit non lawyers to be partners in law firms .", "The ABA has steadfastly reserved the title of partner and partnership perks which include getting a stake of the firm's profit for those with law degrees .", "But Robert Jordan , a partner at Steptoe Johnson who took the lead in drafting the new district bar code , said the ABA's rules were viewed as quot ; too restrictive quot ; by lawyers here .", "quot ; The practice of law in Washington is very different from what it is in Dubuque , quot ; he said .", "Some of these non lawyer employees are paid at partners' levels .", "Yet , not having the partner title quot ; makes non lawyers working in law firms second class citizens , quot ; said Mr . Jordan of Steptoe Johnson .", ". . . Before the text is processed by Nominator , it is analyzed into tokens sentences , words , tags , and punctuation elements .", "Nominator forms a candidate name list by scanning the tokenized document and collecting sequences of capitalized tokens or words as well as some special lower case tokens , such as conjunctions and prepositions .", "The list of candidate names extracted from the sample document contains Each candidate name is examined for the presence of conjunctions , prepositions or possessive 's .", "A set of heuristics is applied to determine whether each candidate name should be split into smaller independent names .", "For example , Mr . Jordan of Steptoe Johnson is split into Mr . Jordan and Steptoe Johnson .", "Finally , Nominator links together variants that refer to the same entity .", "Because of standard English language naming conventions , Mr . Jordan is grouped with Robert Jordan .", "ABA is grouped with American Bar Association as a possible abbreviation of the longer name .", "Each linked group is categorized by an entity type and assigned a 'canonical name' as its identifier .", "The canonical name is the fullest , least ambiguous label that can be used to refer to the entity .", "It may be one of the variants found in the document or it may be constructed from components of different ones As the links are formed , each group is assigned a type .", "In the sample output shown below , each canonical name is followed by its entity type and by the variants linked to it .", "After the whole document collection has been processed , linked groups are merged across documents and their variants combined .", "Thus , if in one document President Clinton was a variant of William Clinton , while in another document Governor Clinton was a variant of William Clinton , both are treated as variants of an aggregated William Clinton group .", "In this minimal sense , Nominator uses the larger context of the document collection to 'learn' more variants for a given name .", "In the following sections we describe how ambiguity is resolved as part of the name discovery process .", "We identify three indicators of potential structural ambiguity , prepositions , conjunctions and possessive pronouns , which we refer to as 'ambiguous operators' .", "In order to determine whether 'splitting' should occur , a name sequence containing an ambiguous operator is divided into three segments the operator , the substring to its left and the substring to its right .", "The splitting process applies a set of heuristics based on patterns of capitalization , lexical features and the relative 'scope' of operators see below to name sequences containing these operators to determine whether or not they should be split into smaller names .", "We can describe the splitting heuristics as determining the scope of ambiguous operators , by analogy to the standard linguistic treatment of quantifiers .", "From Nominator's point of view , all three operator types behave in similar ways and often interact when they co occur in the same name sequence , as in New York's MOMA and the Victoria and Albert Museum in London .", "The scope of ambiguous operators also interacts with the 'scope' of NP heads , if we define the scope of NP heads as the constituents they dominate .", "For example , in Victoria and Albert Museum , the conjunction is within the scope of the lexical head Museum because Museum is a noun that can take PP modification Museum of Natural History and hence pre modification Natural History Museum .", "Since pre modifiers can contain conjunctions Japanis within the scope of the noun , and so the name is not split .", "Although the same relationship holds between the lexical head Laboratories and the conjunction and in IBM and Bell Laboratories , another heuristic takes precedence , one whose condition requires splitting a string if it contains an acronym immediately to the left or to the right of the ambiguous operator .", "It is not possible to determine relative scope strength for all the combinations of different operators .", "Contradictory examples abound Gates of Microsoft and Gerstner of IBMsuggests stronger scope of and over of , The Department of German Languages and Literature suggests the opposite .", "Since it is usually the case that a right hand operator has stronger scope over a left hand one , we evaluate strings containing operators from right to left .", "To illustrate , New York's MOMA and the Victoria and Albert Museum in London is first evaluated for splitting on in .", "Since the left and right substrings do not satisfy any conditions , we proceed to the next operator on the left and .", "Because of the strong scope of Museum , as mentioned above , no splitting occurs .", "Next , the second and from the right is evaluated .", "It causes a split because it is immediately preceded by an all capitalized word .", "We have found this simple typographical heuristic to be powerful and surprisingly accurate .", "Ambiguous operators form recursive structures and so the splitting heuristics apply recursively to name sequences until no more splitting conditions hold .", "New York's MOMA is further split at 's because of a heuristic that checks for place names on the left of a possessive pronoun or a comma .", "Victoria and Albert Museum in London remains intact .", "Nominator's other heuristics resemble those discussed above in that they check for typographical patterns or for the presence of particular name types to the left or right of certain operators .", "Some heuristics weigh the relative scope strength in the substrings on either side of the operator .", "If the scope strength is similar , the string is split .", "We have observed that this type of heuristic works quite well .", "Thus , the string The Natural History Museum and The Board of Education is split at and because each of its substrings contains a strong scope NP head as we define it with modifiers within its scope .", "These two substrings are better balanced than the substrings of The Food and Drug Administration where the left substring does not contain a strong scope NP head while the right one does Administration .", "Because of the principle that noisy data is preferable to loss of information , Nominator does not split names if relative strength cannot be determined .", "As a result , there occur in Nominator's output certain 'names' such as American Television 6 Commu", "Special treatment is required for words in sentenceinitial position , which may be capitalized because they are part of a proper name or simply because they are sentence initial .", "While the heuristics for splitting names are linguistically motivated and rule governed , the heuristics for handling sentence initial names are based on patterns of word occurrence in the document .", "When all the names have been collected and split , names containing sentence initial words are compared to other names on the list .", "If the sentence initial candidate name also occurs as a non sentence initial name or as a substring of it , the candidate name is assumed to be valid and is retained .", "Otherwise , it is removed from the list .", "For example , if White occurs at sentence initial position and also as a substring of another name e . g . , Mr . White it is kept .", "If it is found only in sentence initial position e . g . , White paint is . . . , White is discarded .", "A more difficult situation arises when a sentenceinitial candidate name contains a valid name that begins at the second word of the string .", "If the preceding word is an adverb , a pronoun , a verb or a preposition , it can safely be discarded .", "Thus a sentence beginning with Yesterday Columbia yields Columbia as a name .", "But cases involving other parts of speech remain unresolved .", "If they are sentenceinitial , Nominator accepts as names both New Sears and New Coke ; it also accepts sentence initial Five Reagan as a variant of President Reagan , if the two co occur in a document .", "In a typical document , a single entity may be referred to by many name variants which differ in their degree of potential ambiguity .", "As noted above , Paris and Washington are highly ambiguous out of context but in well edited text they are often disambiguated by the occurrence of a single unambiguous variant in the same document .", "Thus , Washington is likely to co occur with either President Washington or Washington , D . C . , but not with both .", "Indeed , we have observed that if several unambiguous variants do co occur , as in documents that mention both the owner of a company and the company named after the owner , the editors refrain from using a variant that is ambiguous with respect to both .", "To disambiguate highly ambiguous variants then , we link them to unambiguous ones occurring within the same document .", "Nominator cycles through the list of names , identifying 'anchors' , or variant names that unambiguously refer to certain entity types .", "When an anchor is identified , the list of name candidates is scanned for ambiguous variants that could refer to the same entity .", "They are linked to the anchor .", "Our measure of ambiguity is very pragmatic .", "It is based on the confidence scores yielded by heuristics that analyze a name and determine the entity types it can refer to .", "If the heuristic for a certain entity type a person , for example results in a high condifence score highly confident that this is a person name , we determine that the name unambiguously refers to this type .", "Otherwise , we choose the highest score obtained by the various heuristics .", "A few simple indicators can unambiguously determine the entity type of a name , such as Mr . for a person or Inc . for an organization .", "More commonly , however , several pieces of positive and negative evidence are accumulated in order to make this judgement .", "We have defined a set of obligatory and optional components for each entity type .", "For a human name , these components include a professional title e . g . , Attorney General , a personal title e . g . , Dr . , a first name , middle name , nickname , last name , and suffix e . g . , Jr . .", "The combination of the various components is inspected .", "Some combinations may result in a high negative score highly confident that this cannot be a person name .", "For example , if the name lacks a personal title and a first name , and its last name is listed as an organization word e . g . , Department in the authority list , it receives a high negative score .", "This is the case with Justice Department or Frank Sinatra Building .", "The same combination but with a last name that is not a listed organization word results in a low positive score , as for Justice Johnson or Frank Sinatra .", "The presence or absence of a personal title is also important for determining confidence If present , the result is a high confidence score e . g . , Mrs . Ruth Lake ; No personal title with a known first name results in a low positive confidence score e . g . , Ruth Lake , Beverly Hills ; and no personal title with an unknown first name results in a zero score e . g . , Panorama Lake .", "By the end of the analysis process , Justice Department has a high negative score for person and a low positive score for organization , resulting in its classification as an organization .", "Beverly Hills , by contrast , has low positive scores both for place and for person .", "Names with low or zero scores are first tested as possible variants of names with high positive scores .", "However , if they are incompatible with any , they are assigned a weak entity type .", "Thus in the absence of any other evidence in the document , Beverly Hills is classified as a ? PERSON .", "? PERSON is preferred over ? PLACE as it tends to be the correct choice most of the time .", "This analysis of course can be over ridden by a name database listing Beverly Hills as a place .", "Further disambiguation may be possible during aggregation across documents .", "As mentioned before , during aggregation , linked groups from different documents are merged if their canonical forms are identical .", "As a rule , their entity types should be identical as well , to prevent a merge of Boston PLACE and Boston ORG .", "Weak entity types , however , are allowed to merge with stronger entity types .", "Thus , Jordan Hills ? PERSON from one document is aggregated with Jordan Hills PERSON from another , where there was sufficient evidence , such as Mr . Hills , to make a firmer decision .", "An evaluation of an earlier version of Nominator , was performed on 88 Wall Street Journal documents NIST 1993 that had been set aside for testing .", "We chose the Wall Street Journal corpus because it follows standard stylistic conventions , especially capitalization , which is essential for Nominator to work .", "Nominator's performance deteriorates if other conventions are not consistently followed .", "A linguist manually identified 2426 occurrences of proper names , which reduced to 1354 unique tokens .", "Of these , Nominator correctly identified the boundaries of 91 1230 1354 .", "The precision rate was 92 for the 1409 names Nominator identified 1230 1409 .", "In terms of semantic disambiguation , Nominator failed to assign an entity type to 21 of the names it identified .", "This high percentage is due to a decision not to assign a type if the confidence measure is too low .", "The payoff of this choice is a very high precision rate 99 for the assignment of semantic type to those names that were disambiguated .", "See Ravin and Wacholder 1996 for details .", "The main reason that names remain untyped is insufficent evidence in the document .", "If IBM , for example , occurs in a document without International Business Machines , Nominator does not type it ; rather , it lets later processes inspect the local context for further clues .", "These processess form part of the Talent tool set under development at the T . J . Watson Research Center .", "They take as their input text processed by Nominator and further disambiguate untyped names appearing in certain contexts , such as an appositive , e . g . , president of CitiBank Corp . Other untyped names , such as Star Bellied Sneetches or George Melloan's Business World , are neither people , places , organizations nor any of the other legal or financial entities we categorize into .", "Many of these uncategorized names are titles of articles , books and other works of art that we currently do not handle .", "Ambiguity remains one of the main challenges in the processing of natural language text .", "Efforts to resolve it have traditionally focussed on the development of full coverage parsers , extensive lexicons , and vast repositories of world knowledge .", "For some natural language applications , the tremendous effort involved in developing these tools is still required , but in other applications , such as information extraction , there has been a recent trend towards favoring minimal parsing and shallow knowledge Cowie and Lehnert 1996 .", "In its minimal use of resources , Nominator follows this trend it relies on no syntactic information and on a small semantic lexicon an authority list which could easily be modified to include information about new domains .", "Other advantages of using limited resources are robustness and execution speed , which are important in processing large amounts of text .", "In another sense , however , development of a module like Nominator still requires considerable human effort to discover reliable heuristics , particularly when only minimal information is used .", "These heuristics are somewhat domain dependent different generalizations hold for names of drugs and chemicals than those identified for names of people or organizations .", "In addition , as the heuristics depend on linguistic conventions , they are language dependent , and need updating when stylistic conventions change .", "Note , for example , the recent popularity of software names which include exclamation points as part of the name .", "Because of these difficulties , we believe that for the forseeable future , practical applications to discover new names in text will continue to require the sort of human effort invested in Nominator ."], "summary_lines": ["Disambiguation Of Proper Names In Text\n", "Identifying the occurrences of proper names in text and the entities they refer to can be a difficult task because of the many-to-many mapping between names and their referents.\n", "We analyze the types of ambiguity -- structural and semantic -- that make the discovery of proper names difficult in text, and describe the heuristics used to disambiguate names in Nominator, a fully-implemented module for proper name recognition developed at the IBM T.J. Watson Research Center.\n", "We use hand-written rules and knowledge bases to classify proper names into broad categories.\n"]}
{"article_lines": ["Models Of Translational Equivalence Among Words", "Parallel texts bitexts have properties that distinguish them from other kinds of parallel data .", "First , most words translate to only one other word .", "Second , bitext correspondence is typically only partial many words in each text have no clear equivalent in the other text .", "This article presents methods for biasing statistical translation models to reflect these properties .", "Evaluation with respect to independent human judgments has confirmed that translation models biased in this fashion are significantly more accurate than a baseline knowledge free model .", "This article also shows how a statistical translation model can take advantage of preexisting knowledge that might be available about particular language pairs .", "Even the simplest kinds of languagespecific knowledge , such as the distinction between content words and function words , are shown to reliably boost translation model performance on some tasks .", "Statistical models that reflect knowledge about the model domain combine the best of both the rationalist and empiricist paradigms .", "The idea of a computer system for translating from one language to another is almost as old as the idea of computer systems .", "Warren Weaver wrote about mechanical translation as early as 1949 .", "More recently , Brown et al . 1988 suggested that it may be possible to construct machine translation systems automatically .", "Instead of codifying the human translation process from introspection , Brown and his colleagues proposed machine learning techniques to induce models of the process from examples of its input and output .", "The proposal generated much excitement , because it held the promise of automating a task that forty years of research have proven very labor intensive and error prone .", "Yet very few other researchers have taken up the cause , partly because Brown et al . 's 1988 approach was quite a departure from the paradigm in vogue at the time .", "Brown et al . 1988 built statistical models of equivalence models' , short .", "In the context of computational linguistics , translational equivalence is a relation that holds between two expressions with the same meaning , where the two expressions are in different languages .", "Empirical estimation statistical translation models is typically based on texts of texts that are translations of each other .", "As with all statistical models , the best translation models are those whose parameters correspond best with the sources of variance in the data .", "Probabilistic translation models whose parameters reflect universal properties of translational equivalence and or existing knowledge about particular D1 66F , 610 Opperman Drive , Eagan , MN 55123 .", "E mail dan . melamed twestgroup . com 1 The term translation model , which is standard in the literature , refers to a mathematical relationship two data sets . hi this context , the term implies nothing about the translation between natural languages , automated or otherwise .", "2000 Association for Computational Linguistics Computational Linguistics Volume 26 , Number 2 languages and language pairs benefit from the best of both the empiricist and rationalist traditions .", "This article presents three such models , along with methods for efficiently estimating their parameters .", "Each new method is designed to account for an additional universal property of translational equivalence in bitexts 1 .", "Most word tokens translate to only one word token .", "I approximate this tendency with a one to one assumption .", "Most text segments are not translated word for word .", "I build an explicit noise model .", "Different linguistic objects have statistically different behavior in translation .", "I show a way to condition translation models on different word classes to help account for the variety Quantitative evaluation with respect to independent human judgments has shown that each of these three estimation biases significantly improves translation model accuracy over a baseline knowledge free model .", "However , these biases will not produce the best possible translation models by themselves .", "Anyone attempting to build an optimal translation model should infuse it with all available knowledge sources , including syntactic , dictionary and cognate information .", "My goal here is only to demonstrate the value of some previously unused kinds of information that are always available for translation modeling , and to show how these information sources can be integrated with others .", "Parallel texts bitexts have properties that distinguish them from other kinds of parallel data .", "First , most words translate to only one other word .", "Second , bitext correspondence is typically only partial many words in each text have no clear equivalent in the other text .", "This article presents methods for biasing statistical translation models to reflect these properties .", "Evaluation with respect to independent human judgments has confirmed that translation models biased in this fashion are significantly more accurate than a baseline knowledge free model .", "This article also shows how a statistical translation model can take advantage of preexisting knowledge that might be available about particular language pairs .", "Even the simplest kinds of languagespecific knowledge , such as the distinction between content words and function words , are shown to reliably boost translation model performance on some tasks .", "Statistical models that reflect knowledge about the model domain combine the best of both the rationalist and empiricist paradigms .", "The idea of a computer system for translating from one language to another is almost as old as the idea of computer systems .", "Warren Weaver wrote about mechanical translation as early as 1949 .", "More recently , Brown et al . 1988 suggested that it may be possible to construct machine translation systems automatically .", "Instead of codifying the human translation process from introspection , Brown and his colleagues proposed machine learning techniques to induce models of the process from examples of its input and output .", "The proposal generated much excitement , because it held the promise of automating a task that forty years of research have proven very labor intensive and error prone .", "Yet very few other researchers have taken up the cause , partly because Brown et al . 's 1988 approach was quite a departure from the paradigm in vogue at the time .", "Formally , Brown et al . 1988 built statistical models of translational equivalence or translation models' , for short .", "In the context of computational linguistics , translational equivalence is a relation that holds between two expressions with the same meaning , where the two expressions are in different languages .", "Empirical estimation of statistical translation models is typically based on parallel texts or bitexts pairs of texts that are translations of each other .", "As with all statistical models , the best translation models are those whose parameters correspond best with the sources of variance in the data .", "Probabilistic translation models whose parameters reflect universal properties of translational equivalence and or existing knowledge about particular languages and language pairs benefit from the best of both the empiricist and rationalist traditions .", "This article presents three such models , along with methods for efficiently estimating their parameters .", "Each new method is designed to account for an additional universal property of translational equivalence in bitexts Quantitative evaluation with respect to independent human judgments has shown that each of these three estimation biases significantly improves translation model accuracy over a baseline knowledge free model .", "However , these biases will not produce the best possible translation models by themselves .", "Anyone attempting to build an optimal translation model should infuse it with all available knowledge sources , including syntactic , dictionary and cognate information .", "My goal here is only to demonstrate the value of some previously unused kinds of information that are always available for translation modeling , and to show how these information sources can be integrated with others .", "A review of some previously published translation models follows an introduction to translation model taxonomy The core of the article is a presentation of the model estimation biases described above .", "The last section reports the results of experiments designed to evaluate these innovations .", "Throughout this article , I shall use CAGEIg'RAPHIC letters to denote entire text corpora and other sets of sets , CAPITAL letters to denote collections , including sequences and bags , and italics for scalar variables .", "I shall also distinguish between types and tokens by using bold font for the former and plain font for the latter .", "There are two kinds of applications of translation models those where word order plays a crucial role and those where it doesn't .", "Empirically estimated models of translational equivalence among word types can play a central role in both kinds of applications .", "Applications where word order is not essential include For these applications , empirically estimated models have a number of advantages over handcrafted models such as on line versions of bilingual dictionaries .", "Two of the advantages are the possibility of better coverage and the possibility of frequent updates by nonexpert users to keep up with rapidly evolving vocabularies .", "A third advantage is that statistical models can provide more accurate information about the relative importance of different translations .", "Such information is crucial for applications such as cross language information retrieval CUR .", "In the vector space approach to CUR , the query vector Q' is in a different language a different vector space from the document vectors D . A word to word translation model T can map Q' into a vector Q in the vector space of D . In order for the mapping to be accurate , T must be able to encode many levels of relative importance among the possible translations of each element of Q' .", "A typical bilingual dictionary says only what the possible translations are , which is equivalent to positing a uniform translational distribution .", "The performance of cross language information retrieval with a uniform T is likely to be limited in the same way as the performance of conventional information retrieval without term frequency information , i . e . , where the system knows which terms occur in which documents , but not how often Buckley 1993 .", "Applications where word order is crucial include speech transcription for translation Brousseau et al . 1995 , bootstrapping of OCR systems for new languages Philip Resnik and Tapas Kanungo , personal communication , interactive translation Foster , Isabelle , and Plamondon 1996 , and fully automatic high quality machine translation e . g . , Al Onaizan et al .", "In such applications , a word to word translation model can serve as an independent module in a more complex sequence tosequence translation model . '", "The independence of such a module is desirable for two reasons , one practical and one philosophical .", "The practical reason is illustrated in this article Order independent translation models can be accurately estimated more efficiently in isolation .", "The philosophical reason is that words are an important epistemological category in our naive mental representations of language .", "We have many intuitions and even some testable theories about what words are and how they behave .", "We can bring these intuitions to bear on our translation models without being distracted by other facets of language , such as phrase structure .", "For example , the translation models presented in the last two chapters of Melamed to appear capture the intuitions that words can have multiple senses and that spaces in text do not necessarily delimit words .", "The independence of a word to word translation module in a sequence to sequence translation model can be effected by a two stage decomposition .", "The first stage is based on the observation that every sequence L is just an ordered bag , and that the bag B can be modeled independently of its order 0 .", "For example , the sequence abc consists of the bag c , a , b and the ordering relation b , 2 , a , 1 , c , 3 1 .", "If we represent each sequence L as a pair B , 0 , then 2 quot ; Sentence to sentence quot ; might be a more transparent term than quot ; sequence to sequence , quot ; but all the models that I'm aware of apply equally well to sequences of words that are not sentences .", "Now , let Li and L2 be two sequences and let A be a one to one mapping between the elements of L1 and the elements of L2 .", "Borrowing a term from the operations research literature , I shall refer to such mappings as assignments . '", "Let A be the set of all possible assignments between L1 and L2 .", "Using assignments , we can decompose conditional and joint probabilities over sequences The second stage of decomposition takes us from bags of words to the words that they contain .", "The following bag pair generation process illustrates how a wordto word translation model can be embedded in a bag to bag translation model for languages Li and 2 from LI x according to the distribution trans ti , V , to lexicalize the concept in the two languages . '", "Some concepts are not lexicalized in some languages , so one of it , and V , may be empty .", "A pair of bags containing m and n nonempty word sequences can be generated by a process where is anywhere between 1 and m n . For notational convenience , the elements of the two bags can be labeled so that Bi , 10 and B2 Irlt , VI , where some of the ti's and V's may be empty .", "The elements of an assignment , then , are pairs of bag element labels A ii , ii , . , where each i ranges over di , , each ranges over VI , , V . 1 , each i is distinct , and each j is distinct .", "The label pairs in a given assignment can be generated in any order , so there are 1 ! ways to generate an assignment of size 1 . 6 It follows that the probability of generating a pair of bags B1 , B2 with a particular assignment A of size 1 is The above equation holds regardless of how we represent concepts .", "There are many plausible representations , such as pairs of trees from synchronous tree adjoining grammars Abeille et al . 1990 ; Shieber 1994 ; Candito 1998 , lexical conceptual structures Dorr 1992 and WordNet synsets Fellbaum 1998 ; Vossen 1998 .", "Of course , for a representation to be used , a method must exist for estimating its distribution in data .", "A useful representation will reduce the entropy of the trans distribution , which is conditioned on the concept distribution as shown in Equation 10 .", "This topic is beyond the scope of this article , however .", "I mention it only to show how the models presented here may be used as building blocks for models that are more psycholinguistically sophisticated .", "To make the translation model estimation methods presented here as general as possible , I shall assume a totally uninformative concept representation the trans distribution itself .", "In other words , I shall assume that each different pair of word sequence types is deterministically generated from a different concept , so that trans Ili , ViIC is zero for all concepts except one .", "Now , a bag to bag translation model can be fully specified by the distributions of 1 and trans .", "The probability distribution trans ii , it' is a word to word translation model .", "Unlike the models proposed by Brown et al . 1993b , this model is symmetric , because both word bags are generated together from a joint probability distribution .", "Brown and his colleagues' models , reviewed in Section 4 . 3 , generate one half of the bitext given the other half , so they are represented by conditional probability distributions .", "A sequenceto sequence translation model can be obtained from a word to word translation model by combining Equation 11 with order information as in Equation 8 .", "The most general word to word translation model trans ii , V , where ii and range over sequences in Li and 2 , has an infinite number of parameters .", "This model can be constrained in various ways to make it more practical .", "The models presented in this article are based on the one to one assumption Each word is translated to at most one other word .", "In these models , \u00fc and ii may consist of at most one word each .", "As before , one of the two sequences but not both may be empty .", "I shall describe empty sequences as consisting of a special NULL word , so that each word sequence will contain exactly one word and can be treated as a scalar .", "Henceforth , I shall write u and v instead of \u00fc and Under the one to one assumption , a pair of bags containing m and n nonempty words can be generated by a process where the bag size is anywhere between max m , n and m n . The one to one assumption is not as restrictive as it may appear The explanatory power of a model based on this assumption may be raised to an arbitrary level by extending Western notions of what words are to include words that contain spaces e . g . , in English or several characters e . g . , in Chinese .", "For example , I have shown elsewhere how to estimate word to word translation models where a word can be a noncompositional compound consisting of several space delimited tokens Melamed , to appear .", "For the purposes of this article , however , words are the tokens generated by my tokenizers and stemmers for the languages in question .", "Therefore , the models in this article are only a first approximation to the vast complexities of translational equivalence between natural languages .", "They are intended mainly as stepping stones towards better models .", "Most methods for estimating translation models from bitexts start with the following intuition Words that are translations of each other are more likely to appear in corresponding bitext regions than other pairs of words .", "Following this intuition , most authors begin by counting the number of times that word types in one half of the bitext co occur with word types in the other half .", "Different co occurrence counting methods stem from different models of co occurrence .", "A model of co occurrence is a Boolean predicate , which indicates whether a given pair of word tokens co occur in corresponding regions of the bitext space .", "Different models of co occurrence are possible , depending on the kind of bitext map that is available , the language specific information that is available , and the assumptions made about the nature of translational equivalence .", "All the translation models reviewed and introduced in this article can be based on any of the co occurrence models described by Melamed 1998a .", "For expository purposes , however , I shall assume a boundarybased model of co occurrence throughout this article .", "A boundary based model of co occurrence assumes that both halves of the bitext have been segmented into s segments , so that segment U , in one half of the bitext and segment V , in the other half are mutual translations , 1 i s . Under the boundary based model of co occurrence , there are several ways to compute co occurrence counts cooc u , v between word types u and v . In the models of Brown , Della Pietra , Della Pietra , and Mercer 1993 , reviewed in Section 4 . 3 , where e , and f , are the urtigram frequencies of u and v , respectively , in each aligned text segment i .", "For most translation models , this method produces suboptimal results , however , when e , u 1 and J v 1 .", "I argue elsewhere Melamed 1998a that nods and hoche often co occur , as do nods and head .", "The direct association between nods and hoche , and the direct association between nods and head give rise to an indirect association between hoche and head .", "Many researchers have proposed greedy algorithms for estimating nonprobabilistic word to word translation models , also known as translation lexicons e . g . , Catizone , Russell , and Warwick 1989 ; Gale and Church 1991 ; Fung 1995 ; Kumano and Hirakawa The remaining word pairs become the entries in the translation lexicon .", "The various proposals differ mainly in their choice of similarity function .", "Almost all the similarity functions in the literature are based on a model of co occurrence with some linguistically motivated filtering see Fung 1995 for a notable exception .", "Given a reasonable similarity function , the greedy algorithm works remarkably well , considering how simple it is .", "However , the association scores in Step 2 are typically computed independently of each other .", "The problem with this independence assumption is illustrated in Figure 1 .", "The two word sequences represent corresponding regions of an English French bitext .", "If nods and hoche co occur much more often than expected by chance , then any reasonable similarity metric will deem them likely to be mutual translations .", "Nods and hoche are indeed mutual translations , so their tendency to co occur is called a direct association .", "Now , suppose that nods and head often co occur in English .", "Then hoche and head will also co occur more often than expected by chance .", "The dashed arrow between hoche and head in Figure 1 represents an indirect association , since the association between hoche and head arises only by virtue of the association between each of them and nods .", "Models of translational equivalence that are ignorant of indirect associations have quot ; a tendency . . . to be confused by collocates quot ; Dagan , Church , and Gale 1993 , 5 .", "Paradoxically , the irregularities noise in text and in translation mitigate the problem .", "If noise in the data reduces the strength of a direct association , then the same noise will reduce the strengths of any indirect associations that are based on this direct the two halves of the bitext a pair of aligned text segments in U , V the unigram frequency of u in U the unigram frequency of v in V the number of times that u and v co occur the probability that a token of u will be translated as a token of v association .", "On the other hand , noise can reduce the strength of an indirect association without affecting any direct associations .", "Therefore , direct associations are usually stronger than indirect associations .", "If all the entries in a translation lexicon are sorted by their association scores , the direct associations will be very dense near the top of the list , and sparser towards the bottom .", "Gale and Church 1991 have shown that entries at the very top of the list can be over 98 correct .", "Their algorithm gleaned lexicon entries for about 61 of the word tokens in a sample of 800 English sentences .", "To obtain 98 precision , their algorithm selected only entries for which it had high confidence that the association score was high .", "These would be the word pairs that co occur most frequently .", "A random sample of 800 sentences from the same corpus showed that 61 of the word tokens , where the tokens are of the most frequent types , represent 4 . 5 of all the word types .", "A similar strategy was employed by Wu and Xia 1994 and by Fung 1995 .", "Fung skimmed off the top 23 . 8 of the noun noun entries in her lexicon to achieve a precision of 71 . 6 .", "Wu and Xia have reported automatic acquisition of 6 , 517 lexicon entries from a 3 . 3 million word corpus , with a precision of 86 .", "The first 3 . 3 million word tokens in an English corpus from a similar genre contained 33 , 490 different word types , suggesting a recall of roughly 19 .", "Note , however , that Wu and Xia chose to weight their precision estimates by the probabilities attached to each entry For example , if the translation set for English word detect has the two correct Chinese candidates with 0 . 533 probability and with 0 . 277 probability , and the incorrect translation with 0 . 190 probability , then we count this as 0 . 810 correct translations and 0 . 190 incorrect translations .", "Wu and Xia 1994 , 211 This is a reasonable evaluation method , but it is not comparable to methods that simply count each lexicon entry as either right or wrong e . g . , Daille , Gaussier , and Lange 1994 ; Melamed 1996b .", "A weighted precision estimate pays more attention to entries that are more frequent and hence easier to estimate .", "Therefore , weighted precision estimates are generally higher than unweighted ones .", "Most probabilistic translation model reestimation algorithms published to date are variations on the theme proposed by Brown et al . 1993b .", "These models involve conditional probabilities , but they can be compared to symmetric models if the latter are normalized by the appropriate marginal distribution .", "I shall review these models using the notation in Table 1 . ploy the expectation maximization EM algorithm Dempster , Laird , and Rubin 1977 to estimate the parameters of their Model 1 .", "On iteration i , the EM algorithm reestimates the model parameters transi vju based on their estimates from iteration i 1 .", "In Model 1 , the relationship between the new parameter estimates and the old ones is where z is a normalizing factor . '", "It is instructive to consider the form of Equation 14 when all the translation probabilities trans v1u for a particular u are initialized to the same constant p , as Brown et al . 1993b , 273 actually do The initial translation probability trans1 v1u is set proportional to the co occurrence count of u and v and inversely proportional to the length of each segment U in which u occurs .", "The intuition behind the numerator is central to most bitext based translation models The more often two words co occur , the more likely they are to be mutual translations .", "The intuition behind the denominator is that the co occurrence count of u and v should be discounted to the degree that v also co occurs with other words in the same segment pair .", "Now consider how Equation 16 would behave if all the text segments on each side were of the same length , ' so that each token of v co occurs with exactly c words where c is constant The normalizing coefficient is constant over all words .", "The only difference between Equations 16 and 18 is that the former discounts co occurrences proportionally to the segment lengths .", "When information about segment lengths is not available , the only information available to initialize Model 1 is the co occurrence counts .", "This property makes Model 1 an appropriate baseline for comparison to more sophisticated models that use other information sources , both in the work of Brown and his colleagues and in the work described here . the true bitext map correlate with the positions of their translations .", "The correlation is stronger for language pairs with more similar word order .", "Brown et al . 1988 introduced the idea that this correlation can be encoded in translation model parameters .", "Dagan , Church , and Gale 1993 expanded on this idea by replacing Brown et al . 's 1988 word alignment parameters , which were based on absolute word positions in aligned segments , with a much smaller set of relative offset parameters .", "The much smaller number of parameters allowed Dagan , Church , and Gale's model to be effectively trained on much smaller bitexts .", "Vogel , Ney , and Tillmann 1996 have shown how some additional assumptions can turn this model into a hidden Markov model , enabling even more efficient parameter estimation .", "It cannot be overemphasized that the word order correlation bias is just knowledge about the problem domain , which can be used to guide the search for the optimum model parameters .", "Translational equivalence can be empirically modeled for any pair of languages , but some models and model biases work better for some language pairs than for others .", "The word order correlation bias is most useful when it has high predictive power , i . e . , when the distribution of alignments or offsets has low entropy .", "The entropy of this distribution is indeed relatively low for the language pair that both Brown and his colleagues and Dagan , Church , and Gale were working with French and English have very similar word order .", "A word order correlation bias , as well as the phrase structure biases in Brown et al . 's 1993b Models 4 and 5 , would be less beneficial with noisier training bitexts or for language pairs with less similar word order .", "Nevertheless , one should use all available information sources , if one wants to build the best possible translation model .", "Section 5 . 3 suggests a way to add the word order correlation bias to the models presented in this article .", "At about the same time that I developed the models in this article , Hiemstra 1996 independently developed his own bag to bag model of translational equivalence .", "His model is also based on a one to one assumption , but it differs from my models in that it allows empty words in only one of the two bags , the one representing the shorter sentence .", "Thus , Hiemstra's model is similar to the first model in Section 5 , but it has a little less explanatory power .", "Hiemstra's approach also differs from mine in his use of the Iterative Proportional Fitting Procedure IPFP Deming and Stephan 1940 for parameter estimation .", "The IPFP is quite sensitive to initial conditions , so Hiemstra investigated a number of initialization options .", "Choosing the most advantageous , Hiemstra has published parts of the translational distributions of certain words , induced using both his method and Brown et al . 's 1993b Model 1 from the same training bitext .", "Subjective comparison of these examples suggests that Hiemstra's method is more accurate .", "Hiemstra 1998 has also evaluated the recall and precision of his method and of Model 1 on a small hand constructed set of link tokens in a particular bitext .", "Model 1 fared worse , on average .", "This section describes my methods for estimating the parameters of a symmetric wordto word translation model from a bitext .", "For most applications , we are interested in estimating the probability trans u , v of jointly generating the pair of words u , v .", "Unfortunately , these parameters cannot be directly inferred from a training bitext , because we don't know which words in one half of the bitext were generated together with which words in the other half .", "The observable features of the bitext are only the co occurrence counts cooc u , v see Section 4 . 1 .", "Methods for estimating translation parameters from co occurrence counts typically involve link counts links u , v , which represent hypotheses about the number of times that u and v were generated together , for each u and v in the bitext .", "A link token is an ordered pair of word tokens , one from each half of the bitext .", "A link type is an ordered pair of word types .", "The link counts links u , v range over link types .", "We can always estimate trans u , v by normalizing link counts so that E trans u , v 1 For estimation purposes , it is convenient to also employ a separate set of nonprobabilistic parameters score u , v , which represent the chances that u and v can ever be mutual translations , i . e . , that there exists some context where tokens u and v are generated from the same concept .", "The relationship between score u , v and trans u , v can be more or less direct , depending on the model and its estimation method .", "Each of the models presented below uses a different score formulation .", "All my methods for estimating the translation parameters trans u , v share the following general outline Under certain conditions , a parameter estimation process of this sort is an instance of the expectation maximization EM algorithm Dempster , Laird , and Rubin 1977 .", "As explained below , meeting these conditions is computationally too expensive for my models . '", "Therefore , I employ some approximations , which lack the EM algorithm's convergence guarantee .", "The maximum likelihood approach to estimating the unknown parameters is to find the set of parameters 6 that maximize the probability of the training bitext U , V .", "The probability of the bitext is a sum over the distribution A of possible assignments The number of possible assignments grows exponentially with the size of aligned text segments in the bitext .", "Due to the parameter interdependencies introduced by the one to one assumption , we are unlikely to find a method for decomposing the assignments into parameters that can be estimated independently of each other as in Brown et al . 1993b , Equation 26 .", "Barring such a decomposition method , the MLE approach is infeasible .", "This is why we must make do with approximations to the EM algorithm .", "In this situation , Brown et al . 1993b , 293 recommend quot ; evaluating the expectations using only a single , probable alignment . quot ; The single most probable assignment Amax is the maximum a posteriori MAP assignment If we represent the bitext as a bipartite graph and weight the edges by log trans u , v , then the right hand side of Equation 26 is an instance of the weighted maximum matching problem and Amax is its solution .", "For a bipartite graph G V1 U V2 , E , with v 1V1 U V21 and e El , the lowest currently known upper bound on the computational complexity of this problem is 0 ve v2 log v Ahuja , Magnati , and Orlin 1993 , 500 .", "Although this upper bound is polynomial , it is still too expensive for typical bitexts . 1 Subsection 5 . 1 . 2 describes a greedy approximation to the MAP approximation .", "5 . 1 . 1 Step 1 Initialization .", "Almost every translation model estimation algorithm exploits the well known correlation between translation probabilities and co occurrence counts .", "Many algorithms also normalize the co occurrence counts cooc u , v by the marginal frequencies of u and v . However , these quantities account for only the three shaded cells in Table 2 .", "The statistical interdependence between two word types can be estimated more robustly by considering the whole table .", "For example , Gale and Church 1991 , 154 suggest that quot ; 02 , a x2 like statistic , seems to be a particularly good choice because it makes good use of the off diagonal cells quot ; in the contingency table .", "In informal experiments described elsewhere Melamed 1995 , I found that the G2 statistic suggested by Dunning 1993 slightly outperforms 02 .", "Let the cells of the contingency table be named as follows where B kin , p pk 1 p n k are binomial probabilities .", "The statistic uses maximum likelihood estimates for the probability parameters p1 bf P2 c cd' P a ab cc Fd' G2 is easy to compute because the binomial coefficients in the numerator and in the denominator cancel each other out .", "All my methods initialize the parameters score u , v to G2 u , v , except that any pairing with NULL is initialized to an infinitesimal value .", "I have also found it useful to smooth the co occurrence counts , e . g . , using the Simple Good Turing smoothing method Gale and Sampson 1995 , before computing G2 .", "5 . 1 . 2 Step 2 Estimation of Link Counts .", "To further reduce the complexity of estimating link counts , I employ the competitive linking algorithm , which is a greedy approximation to the MAP approximation bitext linked to NULL .", "Otherwise , link all co occurring token pairs u , v in the bitext .", "D The one to one assumption implies that linked words cannot be linked again .", "Therefore , remove all linked word tokens from their respective halves of the bitext .", "The competitive linking algorithm can be viewed as a heuristic search for the most likely assignment in the space of all possible assignments .", "The heuristic is that the most likely assignments contain links that are individually the most likely .", "The search proceeds by a process of elimination .", "In the first search iteration , all the assignments that do not contain the most likely link are discarded .", "In the second iteration , all the assignments that do not contain the second most likely link are discarded , and so on until only one assignment remains . quot ; The algorithm greedily selects the most likely links first , and then selects less likely links only if they don't conflict with previous selections .", "The probability of a link being rejected increases with the number of links that are selected before it , and thus decreases with the link's score .", "In this problem domain , the competitive linking algorithm usually finds one of the most likely assignments , as I will show in Section 6 .", "Under an appropriate hashing scheme , the expected running time of the competitive linking algorithm is linear in the size of the input bitext .", "The competitive linking algorithm and its one to one assumption are potent weapons against the ever present sparse data problem .", "They enable accurate estimation of translational distributions even for words that occur only once , as long as the surrounding words are more frequent .", "In most translation models , link scores are correlated with co occurrence frequency .", "So , links between tokens u and v for which score u , v is highest are the ones for which there is the most evidence , and thus also the ones that are easiest to predict correctly .", "Winner take all link assignment methods , such as the competitive linking algorithm , can prevent links based on indirect associations see Section 4 . 2 , thereby leveraging their accuracy on the more confident links to raise the accuracy of the less confident links .", "For example , suppose that u1 and u2 co occur with v1 and v2 in the training data , and the model estimates score ui , vi . 05 , score u1 , v2 . 02 , and score u2 , v2 . 01 .", "According to the one to one assumption , ui , v2 is an indirect association and the correct translation of 02 is u2 .", "To the extent that the one to one assumption is valid , it reduces the probability of spurious links for the rarer words .", "The more incorrect candidate translations can be eliminated for a given rare word , the more likely the correct translation is to be found .", "So , the probability of a correct match for a rare word is proportional to the fraction of words around it that can be linked with higher confidence .", "This fraction is largely determined by two bitext properties the distribution of word frequencies , and the distribution of co occurrence counts .", "Melamed to appear explores these properties in greater depth . parameters as the logarithm of the trans parameters .", "The competitive linking algorithm only cares about the relative magnitudes of the various score u , v .", "However , Equation 26 is a sum rather than a product , so I scale the trans parameters logarithmically , to be consistent with its probabilistic interpretation Yarowsky 1993 , 271 has shown that quot ; for several definitions of sense and collocation , an ambiguous word has only one sense in a given collocation with a probability of 90 99 . quot ; In other words , a single contextual clue can be a highly reliable indicator of a word's sense .", "One of the definitions of quot ; sense quot ; studied by Yarowsky was a word token's translation in the other half of a bitext .", "For example , the English word sentence may be considered to have two senses , corresponding to its French translations peine judicial sentence and phrase grammatical sentence .", "If a token of sentence occurs in the vicinity of a word like jury or prison , then it is far more likely to be translated as peine than as phrase .", "quot ; In the vicinity of quot ; is one kind of collocation .", "Co occurrence The ratio links u , v I cooc u , v , for several values of cooc u , v . in bitext space is another kind of collocation .", "If each word's translation is treated as a sense tag Resnik and Yarowsky 1997 , then quot ; translational quot ; collocations have the unique property that the collocate and the word sense are one and the same !", "Method B exploits this property under the hypothesis that quot ; one sense per collocation quot ; holds for translational collocations .", "This hypothesis implies that if u and v are possible mutual translations , and a token u co occurs with a token v in the bitext , then with very high probability the pair u , v was generated from the same concept and should be linked .", "To test this hypothesis , I ran one iteration of Method A on 300 , 000 aligned sentence pairs from the Canadian Hansards bitext .", "I then plotted the of the competitive linking process , because in the first iteration , linking decisions are based only on the initial similarity metric .", "Information about how often words co occur without being linked can be used to bias the estimation of translation model parameters .", "The smaller the ratio icionokcsuu quot ; vv ? , the more likely it is that u and v are not mutual translations , and that links posited between tokens of u and v are noise .", "The bias can be implemented via auxiliary parameters that model the curve illustrated in Figure 2 .", "The competitive linking algorithm creates all the links of a given type independently of each other . '", "So , the distribution of the number links u , v of links connecting word types u and v can be modeled by a binomial distribution with parameters cooc u , v and p u , v . p u , v is the probability that u and v will be linked when they co occur .", "There is never enough data to robustly estimate each p parameter separately .", "Instead , I shall model all the p's with just two parameters .", "For u and v that are mutual translations , p u , v will average to a relatively high probability , which I will call A . for u and v that are not mutual translations , p u , v will average to a relatively low probability , which I will call A . .", "A and Acorrespond to the two peaks of the distribution , which is illustrated in Figure 2 .", "The two parameters can also be interpreted as the rates of true and false positives .", "If the translation in the bitext is consistent and the translation model is accurate , then A will be close to one and A will be close to zero .", "To find the most likely values of the auxiliary parameters A and A , I adopt the standard method of maximum likelihood estimation , and find the values that maximize the probability of the link frequency distributions , under the usual independence assumptions Table 3 summarizes the variables involved in this auxiliary estimation process .", "The factors on the right hand side of Equation 29 can be written explicitly with the help of a mixture coefficient .", "Let T be the probability that an arbitrary co occurring pair of word types are mutual translations .", "Let B kIn , p denote the probability that k links are observed out of n co occurrences , where k has a binomial distribution with parameters n and p . Then the probability that word types u and v will be linked links u , v times out of cooc u , v co occurrences is a mixture of two binomials One more variable allows us to express T in terms of A and A Let A be the probability that an arbitrary co occuring pair of word tokens will be linked , regardless of whether they are mutual translations .", "Since T is constant over all word types , it also represents the probability that an arbitrary co occurring pair of word tokens are mutual translations .", "Therefore , Pr linkslmodel , as given in Equation 29 , has only one global maximum in the region of interest , where 1 A A 0 . and let N be the total number of word token pair co occurrences Equating the right hand sides of Equations 31 and 34 and rearranging the terms , we get In the preceding equations , either u or v can be NULL .", "However , the number of times that a word co occurs with NULL is not an observable feature of bitexts .", "To make sense of co occurrences with NULL , we can view co occurrences as potential links and cooc u , v as the maximum number of times that tokens of u and v might be linked .", "From this point of view , cooc u , NULL should be set to the unigram frequency of u , since each token of u represents one potential link to NULL .", "Similarly for cooc NULL , v .", "These co occurrence counts should be summed together with all the others in Equation 33 .", "The probability function expressed by Equations 29 and 30 may have many local maxima .", "In practice , these local maxima are like pebbles on a mountain , invisible at low resolution .", "I computed Equation 29 over various combinations of A and A after one iteration of Method A over 300 , 000 aligned sentence pairs from the Canadian Hansard bitext .", "Figure 3 illustrates that the region of interest in the parameter space , where 1 A A A 0 , has only one dominant global maximum .", "This global maximum can be found by standard hill climbing methods , as long as the step size is large enough to avoid getting stuck on the pebbles .", "Given estimates for A and A , we can compute B links u , v lcooc u , v , A and B links u , v icooc u , v , A for each occurring combination of links and cooc values .", "These are the probabilities that links u , v links were generated out of cooc u , v possible links by a process that generates correct links and by a process that generates incorrect links , respectively .", "The ratio of these probabilities is the likelihood ratio in favor of the types u and v being possible mutual translations , for all u and v Method B differs from Method A only in its redefinition of the score function in Equation 36 .", "The auxiliary parameters A and A and the noise model that they represent can be employed the same way in translation models that are not based on the one to one assumption .", "In Method B , the estimation of the auxiliary parameters A and A depends only on the overall distribution of co occurrence counts and link frequencies .", "All word pairs that co occur the same number of times and are linked the same number of times are assigned the same score .", "More accurate models can be induced by taking into account various features of the linked tokens .", "For example , frequent words are translated less consistently than rare words Catizone , Russell , and Warwick 1989 .", "To account for these differences , we can estimate separate values of A and A for different ranges of cooc u , v .", "Similarly , the auxiliary parameters can be conditioned on the linked parts of speech .", "A kind of word order correlation bias can be effected by conditioning the auxiliary parameters on the relative positions of linked word tokens in their respective texts .", "Just as easily , we can model link types that coincide with entries in an on line bilingual dictionary separately from those that do not cf .", "Brown et al . 1993 .", "When the auxiliary parameters are conditioned on different link classes , their optimization is carried out separately for each class B links u , v Icooc u , v , A zF 37 scorec u , viz class u , v log B links u , v lcooc u , v , Section 6 . 1 . 1 describes the link classes used in the experiments below .", "This section compares translation model estimation methods A , B , and C to each other and to Brown et al . 's 1993b Model 1 .", "To reiterate , Model 1 is based on co occurrence information only ; Method A is based on the one to one assumption ; Method B adds the quot ; one sense per collocation quot ; hypothesis to Method A ; Method C conditions the auxiliary parameters of Method B on various word classes .", "Whereas Methods A and B and Model 1 were fully specified in Section 4 . 3 . 1 and Section 5 , the latter section described a variety of features on which Method C might classify links .", "For the purposes of the experiments described in this article , Method C employed the simple classification in Table 4 for both languages in the bitext .", "All classification was performed by table lookup ; no context aware part of speech tagger was used .", "In particular , words that were ambiguous between open classes and closed classes were always deemed to be in the closed class .", "The only language specific knowledge involved in this classification method is the list of function words in class F . Certainly , more sophisticated word classification methods could produce better models , but even the simple classification in Table 4 should suffice to demonstrate the method's potential .", "6 . 1 . 1 Experiment 1 .", "Until now , translation models have been evaluated either subjectively e . g .", "White and O'Connell 1993 or using relative metrics , such as perplexity with respect to other models Brown et al . 1993b .", "Objective and more accurate tests can be carried out using a quot ; gold standard . quot ; I hired bilingual annotators to link roughly 16 , 000 corresponding words between on line versions of the Bible in French and English .", "This bitext was selected to facilitate widespread use and standardization see Melamed 1998c for details .", "The entire Bible bitext comprised 29 , 614 verse pairs , of which 250 verse pairs were hand linked using a specially developed annotation tool .", "The annotation style guide Melamed 1998b was based on the intuitions of the annotators , so it was not biased towards any particular translation model .", "The annotation was replicated five times by seven different annotators .", "Each of the four methods was used to estimate a word to word translation model from the 29 , 614 verse pairs in the Bible bitext .", "All methods were deemed to have converged when less than . 0001 of the translational probability distribution changed from one iteration to the next .", "The links assigned by each of methods A , B , and C in the last iteration were normalized into joint probability distributions using Equation 19 .", "I shall refer to these joint distributions as Model A , Model B , and Model C , respectively .", "Each of the joint probability distributions was further normalized into two conditional probability distributions , one in each direction .", "Since Model 1 is inherently directional , its conditional probability distributions were estimated separately in each direction , instead of being derived from a joint distribution .", "The four models' predictions were compared to the gold standard annotations .", "Each model guessed one translation either stochastically or deterministically , depending on the task for each word on one side of the gold standard bitext .", "Therefore , precision recall here , and I shall refer to the results simply as quot ; percent correct . quot ; The accuracy of each model was averaged over the two directions of translation English to French and French to English .", "The five fold replication of annotations in the test data enabled computation of the statistical significance of the differences in model accuracy .", "The statistical significance of all results in this section was measured at the a . 05 level , using the Wilcoxon signed ranks test .", "Although the models were evaluated on part of the same bitext on which they were trained , the evaluations were with respect to the translational equivalence relation hidden in this bitext , not with respect to any of the bitext's visible features .", "Such testing on training data is standard practice for unsupervised learning algorithms , where the objective is to compare several methods .", "Of course , performance would degrade on previously unseen data .", "In addition to the different translation models , there were two other independent variables in the experiment method of translation and whether function words were included .", "Some applications , such as query translation for CUR , don't care about function words .", "To get a sense of the relative effectiveness of the different translation model estimation methods when function words are taken out of the equation , I removed from the gold standard all link tokens where one or both of the linked words were closed class words .", "Then , I removed all closed class words including nonalphabetic symbols from the models and renormalized the conditional probabilities .", "The method of translation was either single best or whole distribution .", "Singlebest translation is the kind that somebody might use to get the gist of a foreignlanguage document .", "The input to the task was one side of the gold standard bitext .", "The output was the model's single best guess about the translation of each word in the input , together with the input word .", "In other words , each model produced link tokens consisting of input words and their translations .", "For some applications , it is insufficient to guess only the single most likely translation of each word in the input .", "The model is expected to output the whole distribution of possible translations for each input word .", "This distribution is then combined with other distributions that are relevant to the application .", "For example , for cross language information retrieval , the translational distribution can be combined with the distribution of term frequencies .", "For statistical machine translation , the translational distribution can be decoded with a source language model Brown et al . 1988 ; Al Onaizan et al .", "To predict how the different models might perform in such applications , the whole distribution task was to generate a whole set of links from each input word , weighted according to the probability assigned by the model to each of the input word's translations .", "Each model was tested on this task with and without function words .", "The mean results are plotted in Figures 4 and 5 with 95 confidence intervals .", "All four graphs in these figures are on the same scale to facilitate comparison .", "On both tasks involving the entire vocabulary , each of the biases presented in this article improves the efficiency of modeling the available training data .", "When closed class words were ignored , Model 1 performed better than Method A , because open class words are more likely to violate the one to one assumption .", "However , the explicit noise model in Methods B and C boosted their scores significantly higher than Model 1 and Method A .", "Method B was better than Method C at choosing the single best open class links , and the situation was reversed for the whole distribution of open class links .", "However , the differences in performance between these two methods were tiny on the open class tasks , because they left only two classes for Method C to distinguish content words and NULLS .", "Most of the scores on the whole distribution task were lower than their counterparts on the single best translation task , because it is more difficult for any statistical method to correctly model the less common translations .", "The quot ; best quot ; translations are usually the most common .", "6 . 1 . 2 Experiment 2 .", "To study how the benefits of the various biases vary with training corpus size , I evaluated Models A , B , C , and 1 on the whole distribution translation task , after training them on three different size subsets of the Bible bitext .", "The first subset consisted of only the 250 verse pairs in the gold standard .", "The second subset included these 250 plus another random sample of 2 , 250 for a total of 2 , 500 , an order of magnitude larger than the first subset .", "The third subset contained all 29 , 614 verse pairs in the Bible bitext , roughly an order of magnitude larger than the second subset .", "All models were compared to the five gold standard annotations , and the scores were averaged over the two directions of translation , as before .", "Again , because the total probability assigned to all translations for each source word was one , precision recall percent correct on this task .", "The mean scores over the five gold standard annotations are graphed in Figure 6 , where the right edge of the figure corresponds to the means of Figure 5 a .", "The figure supports the hypothesis in Melamed to appear , Chapter 7 that the biases presented in this article are even more valuable when the training data are more sparse .", "The one to one assumption is useful , even though it forces us to use a greedy approximation to maximum likelihood .", "In relative terms , the advantage of the one to one assumption is much more pronounced on smaller training sets .", "For example , Model A is 102 more accurate than Model 1 when trained on only 250 verse pairs .", "The explicit noise model buys a considerable gain in accuracy across all sizes of training data , as do the link classes of Model C . In concert , when trained and tested only on the gold standard test set , the three biases outperformed Model 1 by up to 125 .", "This difference is even more significant given the absolute performance ceiling of 82 established by the interannotator agreement rates on the gold standard .", "6 . 2 Evaluation at the Type Level An important application of statistical translation models is to help lexicographers compile bilingual dictionaries .", "Dictionaries are written to answer the question , quot ; What are the possible translations of X ? quot ; This is a question about link types , rather than about link tokens .", "Evaluation by link type is a thorny issue .", "Human judges often disagree about the degree to which context should play a role in judgments of translational equivalence .", "For example , the Harper Collins French Dictionary Cousin et al . 1990 gives the following French translations for English appoint nommer , engager , fixer , designer .", "Likewise , most Distribution of link type scores .", "The long plateaus correspond to the most common combinations of links u'v 1 1 , 2 2 , and 3 3 . cooc u , v lay judges would not consider instituer a correct French translation of appoint .", "In actual translations , however , when the object of the verb is commission , task force , panel , etc . , English appoint is usually translated into French as instituer .", "To account for this kind of context dependent translational equivalence , link types must be evaluated with respect to the bitext whence they were induced .", "I performed a post hoc evaluation of the link types produced by an earlier version of Method B Melamed 1996b .", "The bitext used for this evaluation was the same aligned Hansards bitext used by Gale and Church 1991 , except that I used only 300 , 000 aligned segment pairs to save time .", "The bitext was automatically pretokenized to delimit punctuation , English possessive pronouns , and French elisions .", "Morphological variants in both halves of the bitext were stemmed to a canonical form .", "The link types assigned by the converged model were sorted by the scores in Equation 36 .", "Figure 7 shows the distribution of these scores on a log scale .", "The log scale helps to illustrate the plateaus in the curve .", "The longest plateau represents the set of word pairs that were linked once out of one co occurrence 1 1 in the bitext .", "All these word pairs were equally likely to be correct .", "The second longest plateau resulted from word pairs that were linked twice out of two co occurrences 2 2 and the third longest plateau is from word pairs that were linked three times out of three co occurrences 3 3 .", "As usual , the entries with higher scores were more likely to be correct .", "By discarding entries with lower scores , coverage could be traded for accuracy .", "This trade off was measured at three points , representing cutoffs at the end of each of the three longest plateaus .", "The traditional method of measuring coverage requires knowledge of the correct link types , which is impossible to determine without a gold standard .", "An approximate coverage measure can be based on the number of different words in the corpus .", "For lexicons extracted from corpora , perfect coverage implies at least one entry containing each word in the corpus .", "One sided variants , which consider only source words , have also been used Gale and Church 1991 .", "Table 5 shows both the marginal one sided and the combined coverage at each of the three cutoff points .", "It also shows the absolute number of non NULL entries in each of the three lexicons .", "Of course , the size of automatically induced lexicons depends on the size of the training bitext .", "Table 5 shows that , given a sufficiently large bitext , the method can automatically construct translation lexicons with as many entries as published bilingual dictionaries .", "The next task was to measure accuracy .", "It would have taken too long to evaluate every lexicon entry manually .", "Instead , I took five random samples with replacement of 100 entries each from each of the three lexicons .", "Each of the samples was first compared to a translation lexicon extracted from a machine readable bilingual dictionary Cousin et al . 1991 .", "All the entries in the sample that appeared in the dictionary were assumed to be correct .", "I checked the remaining entries in all the samples by hand .", "To account for context dependent translational equivalence , I evaluated the accuracy of the translation lexicons in the context of the bitext whence they were extracted , using a simple bilingual concordancer .", "A lexicon entry u , v was considered correct if u and v ever appeared as direct translations of each other in an aligned segment pair .", "That is , a link type was considered correct if any of its tokens were correct .", "Direct translations come in different flavors .", "Most entries that I checked by hand were of the plain vanilla variety that you might find in a bilingual dictionary entry type V .", "However , a significant number of words translated into a different part of speech entry type P .", "For instance , in the entry protection , prot\u00e9g\u00e9 , the English word is a noun but the French word is an adjective .", "This entry appeared because to have protection is often translated as etre prot\u00e9g\u00e9 'to be protected' in the bitext .", "The entry will never occur in a bilingual dictionary , but users of translation lexicons , be they human or machine , will want to know that translations often happen this way .", "The evaluation of translation models at the word type level is complicated by the possibility of phrasal translations , such as immediatement 4 right away .", "All the methods being evaluated here produce models of translational equivalence between individual words only .", "How can we decide whether a single word translation quot ; matches quot ; a phrasal translation ?", "The answer lies in the observation that corpus based lexicography usually involves a lexicographer .", "Bilingual lexicographers can work with bilingual concordancing software that can point them to instances of any link type induced from a bitext and display these instances sorted by their contexts e . g .", "Simard , Foster , and Perrault 1993 .", "Given an incomplete link type , the lexicographer can usually reconstruct the complete link type from the contexts in the concordance .", "For example , if the model proposes an equivalence between irnmediatement and right , a bilingual concordance can show the lexicographer that the model was really trying to capture the equivalence between immediatement and right away or between immediatement and right now .", "I counted incomplete entries in a third category entry type I .", "Whether links in this category should be considered correct depends on the application .", "Table 6 shows the distribution of correct lexicon entries among the types V . P and I .", "Figure 8 graphs the accuracy of the method against coverage , with 95 confidence intervals .", "The upper curve represents accuracy when incomplete links are considered correct , and the lower when they are considered incorrect .", "On the former metric , the method can generate translation lexicons with accuracy and coverage both exceeding 90 , as well as dictionary size translation lexicons that are over 99 correct .", "There are many ways to model translational equivalence and many ways to estimate translation models .", "quot ; The mathematics of statistical machine translation quot ; proposed by Brown et al . 1993b are just one kind of mathematics for one kind of statistical translation .", "In this article , I have proposed and evaluated new kinds of translation model biases , alternative parameter estimation strategies , and techniques for exploiting preexisting knowledge that may be available about particular languages and language pairs .", "On a variety of evaluation metrics , each infusion of knowledge about the problem domain resulted in better translation models .", "Each innovation presented here opens the way for more research .", "Model biases can be mixed and matched with each other , with previously published biases like the word order correlation bias , and with other biases yet to be invented .", "The competitive linking algorithm can be generalized in various ways .", "New kinds of preexisting knowledge can be exploited to improve accuracy for particular language pairs or even just for particular bitexts .", "It is difficult to say where the greatest advances will come from .", "Yet , one thing is clear from our current vantage point Research on empirical methods for modeling translational equivalence has not run out of steam , as some have claimed , but has only just begun .", "Much of this research was performed at the Department of Computer and Information Science at the University of Pennsylvania , where it was supported by an equipment grant from Sun MicroSystems Laboratories and by ARPA Contract N66001 94C 6043 .", "Many thanks to my former colleagues at UPenn and to the anonymous reviewers for their insightful suggestions for improvement ."], "summary_lines": ["Models Of Translational Equivalence Among Words\n", "Parallel texts (bitexts) have properties that distinguish them from other kinds of parallel data.\n", "First, most words translate to only one other word.\n", "Second, bitext correspondence is typically only partial - many words in each text have no clear equivalent in the other text.\n", "This article presents methods for biasing statistical translation models to reflect these properties.\n", "Evaluation with respect to independent human judgments has confirmed that translation models biased in this fashion are significantly more accurate than a baseline knowledge-free model.\n", "This article also shows how a statistical translation model can take advantage of preexisting knowledge that might be available about particular language pairs.\n", "Even the simplest kinds of language-specific knowledge, such as the distinction between content words and function words, are shown to reliably boost translation model performance on some tasks.\n", "Statistical models that reflect knowledge about the model domain combine the best of both the rationalist and empiricist paradigms.\n", "We measure the orthographic similarity using longest common subsequence ratio (LCSR).\n", "We define a direct association as an association between two words where the two words are indeed mutual translations.\n", "We propose Competitive Linking Algorithm (CLA) to align the words to construct confusion network.\n", "We use competitive linking to greedily construct matchings where the pair score is a measure of word-to-word association.\n", "We argue that there are ways to determine the boundaries of some multi-words phrases, allowing to treat several words as a single token.\n"]}
{"article_lines": ["Comlex Syntax Bu i ld ing a Computat iona l Lex icon Ra lph Gr i shm m , Cather ine Mac leod , and Adam Mcyers Computer Science Depar tment , New York Un ivers i ty 715 Broadw , y , 7th F loor , New York , NY 10003 , U . S . A . gr i s lnnan , mac leod , me . yers cs . nyu . e ht Abstract We des tile tile design of Comlex Syntax , a co , nputa tional lexicon providing detailed syntactic iuformation ff r approximately 38 , 000 English headwords .", "We con sider the types of errors which arise in creating such a lexicon , and how such errors can be measured and controlled .", "1 Goal The goal of the omlex Syntax project is to create a moderately broad coverage lexicon recording the syn tactic features of gnglist ; words for purposes of cou putational anguage analysis .", "This dictionary is be ing developed at New York University and is to he distributed by the Linguistic Data Consortimn , to be freely usable for both research and commercial pur poses by members of the Consortium .", "In order to ineet the needs of a wide range of an lyzers , we have inchlded a rich set of syntactic features and haw aimed to characterize these Datures in a rela tively theory neutral way .", "In l articnlar , the feature set is more detailed than those of the major commercial dictionaries , such ; us the Oxford Adwmced Learners Dictionary OALI d and the Longnum Dictionary of Contemporary English LDOCE 8 , which haw I een widely used as a source o lexical i , , for , , lal , ioil ill ; lll guage analyzers .", "1 In addil . ion , we have ahned to be irio , e cOrrlpreheiisive ill capturhig featt , res hi partic . u ar , stibcategorization eatures than co , iI , llercial dic tlonaries .", "2 Structure Ti le word list was derived fion , the file prepared by Prof . Roger Mitten from the Oxford Adwn , ced Learners Dictionary , and contains about 38 , 000 head forms , although some purely British terms have been omitted , loach entry is organized as a nested set of typed feature vahle ists .", "We currently use a Lisp like parenthesized list notation , although the lexicon couhl ITo facilii ate the transit ion to COMLEX by currenl , users of these dictionaries , we have i reparcd mappings froln COMI , EX classes to those of several other dictionaries .", "be readily mapped into other hwn , s , such as SC , MI , marked text , if desired .", "SOllie sauil le dicticl l , ary entries are shown ilt F igure 1 .", "The first syml ol gives the part of speech ; a word with several parts of speech will have several dictionary entries , one for each part of speech .", "Each e , itry has all orth foati lre , g iv ing the base fO , lfl of ti le word , No , ins , verbs , and adjectiw s with irregular Inorphology will liave featt , res for the irregular fo , . iris plural , past , past part , etc .", "Words which take con , i leirients will have a subcatego , ization sube eat , ire .", "For exaniple the verb ai andon eali occur with a IlOllri phrase followed by a prepositional phrase with tim preposition to e . g . , 1 abandoned hii , i to the linguists .", "or with just a , lOll , , phrase comple i f ient aballdone l the shill . .", "Other syntactic features are recorded under features .", "For example , the noun abandon is marked as count able pval wlth , indicating that it must appear in the singular with a deter , niner unless it is preceded by the preposZion with .", "2 . 1 Subcategor i za t ion We have paid p uticular attention to providing detailed subcategorization information information about complement s ructure , both for verbs and for tllose nouns and adjectives which do take cmnl lements .", "In order to insure the COml leteness of our codes , we studied the codiug e ul loyed by s weral other u , ajor texicous , includh , g , he Ihandeis Verh Lexlcolt 2 , the A JQIJII , EX Prc , ject 10 , the NYU Linguistic String lroject 9 , the OALI , and IA OCI , a , nd , whenever feasiMe , haw sought to incorporate distinctions made in any of these all tie , tortes .", "ur resu l t ing feature sys ten , includes 92 subcategorization features Ibr w rbs , 14 for adjectives , and 9 for llO , , ns .", "These features record dilforences in grammatical functional structure as well as constituent structure .", "In particular , tl , ey Calfl . ure four different ypes of control subject control , object control , variable control , and arbitrary control .", "Fur thermore , the notation allows us to indicate that verl Irlay haw dill rent control features for different comlflement structmes or ewm for dilrerent preposi tions within the complement .", "We record , for example , that blame . . . on involves arbitrary control lie 2 l ewdoped by J .", "ihin ; sha . w and I . . lackendoff .", "268 verb noun prep adverb adjective verb verb noun orth abandon subc np pp pval to np orth abandon features countable pval with orth above orth above orth above features ainrn apreq orth abstain subc intrans pp pva from p ing sc pua fro , l orth accept subc np that s np as np orth acceptance Figure 1 . qai , lph X M I , I , , X . qyntax diction uy en . ri s . IAarned the countrys health i roblems . m eating tc , o much chocolate .", ", whereas blanle for involw , s ol .", "ject control l ie blamed John for going too fast . .", "The names fl r the ditferent complmnent types are b sed on the conventions used ill the Ih ancleis wwb lexicon , where each COml Mneut is designated by tl , , names of its constituents , together with a few tags to indicate things such as control phenonleua .", "Earh corn plement ype is formally defined by n fr ; uue see Fig .", "Tile frame includes the constituellt structure , cs , tile grammatical structure , gs , one cu , nlme fea tures , and one or more ex unples , ex .", "Tile constit . uent structure lists the constituents in sequence ; the gram marital structure indicates the functional role played by e , ch c mstituent .", "The elemenl . s of the constitueut structure are indexed , and these indices are referenced in the grammatical structure field in up . frames , I . he index 1 in the grammatical structures always refers to tile surface subject of tile verb .", "Three verb frames are shown ill Figure 2 .", "The fhst , s , is for flail sententiM complements with ; m optional that eo , nplementizer .", "Tim second aim third frames I oth represent infinitiwd conq lemel , ts , aim dillre only in their filnctiona structure .", "The to ingsc frame iv f r subject cm trol verbs , verbs for which the surface subject is the flmctional subject of both the nlatrix ; tad embedded chmses .", "The notation subject 1 in the cs tleld indicates that the surface subject is the sub ject of tile enlbedded clause , while the subject 1 ill the gs Iield indicates that it is the subject of the matrix clause .", "The indication features control subject pro vides this nforlnation redundantly ; we include I oth indications in case one is more collvelliellt for i ; trticu ltu dictionary users .", "The to ingrs flatne is for raising to subject verbs verbs for which the surface subject is tile functional subject only of the embedded c ; tuso .", "The functional subject position in the matrix clause is unlilled , as indicated by the notation gs subject corap 2 .", "3 Methods Our basic aplm acll has been to create an initial lexicon lll llIUtl y a , lld , h ll , list !", "; t vtHi ty of resolllces both commercial aml corpus deriwd , to reline rids lexicon .", "Alth ugh methods haw been dew ped . ww tile last few years for autovual , ically ideutifyi , g sore , subcat i , gorizati ll consl , r tillts I , llrotlgh corpus ; tllulysis 2 , 5 , these methods are sl , ill lhuited iu the range cf disthlc l , ions they can identify and their Mfility to deal with w frequency words .", "hmsequently ve have chosen , o use manual entry for creaticm of our initial dictio , mry .", "The entry of lexical information is being performed by flmr gll ; tdllllte liuguistics studcllts , relerled I . o as elves elf euterer , , f lexical features .", "Tile elw s are provided with a memMmsed interl ce c ded in C lu mort 1 , isp using the Garnet GI I package , aim runuiug on Sun workst . atimls .", "Tiffs iuterfa . ce also p , c . vides ac tess t , o a huge text corpus ; as wcwd is being , eutered , instances .", "f t , he word e ; m be viewed in one of tim win dows .", "I , lves rely on cited , ions from the corpus , dellni ti ? ms and citations from any of several printed dictio naries and their own linguistic intuitions in assigninp ; features I , o words .", "I ictiouary entry began ill April 19 ! 3 .", "Au initial dicti mary contahlhut ewtries for all the u u . us , verbs and adjvci , ives ill tile AI , I was coluldetml iu M . y , 1 !", "91 . 3 We expect t .", "tiffs dicti , mary ; tg ; tillSt sevela SOIIrC ! S , VVe hltelld to C ?", "lill ale the IilaAlll ; t s l lbcate gorizations for verbs aF . ainsl , I , hose in the A , I , and would be pleased to make COllI a , r isous ; I . l . , ; a . illst other broad c werage dictiouarios if those Cttll be m ! tde avail able tbr this purpose .", "We also hltend to mMw COml ar is ms against sewnal corpus deriw d lists at the very least , with w ! rb l reptMthm and w rb partMe pairs wit . h high mutual inf , rmation 3 mid , if possible , wil . h the results of recently developed procedures for ex tractinF , subcai , egorlzal , iou tYames from corpor ; t 2 , . ti .", "While tiffs corpus derived information may not be de tailed or accurate e lough for fu ly autonl tted l xicon 3No fl ! gtlllres ; ire being assigned to adwM s in the initial eXi OII 269 vp frame s vp frame to ingsc vp frame to inf rs cs s 2 that comp optional gs subject 1 comp 2 ex they thought that he was always late cs vp 2 mood to infinitive subject 1 features control subject gs subject 1 comp 2 ex 1 wanted to come .", "cs vp 2 mood to infinitlve subject 1 features raising subject gs subject comp 2 ex they seemed to wilt .", "Figure 2 Sample O OMI , I X . Syntax subcategorization Dames .", "creation , it sliould be most wduable as a basis for com parisons .", "4 Types and Sources of Er ror As art of the process of refining the dictionary and as suring its quality , we have spent considerable r sources on reviewing dictionary entries and on occasion have had sections coded by two or even four of the elves .", "This process has allowed us to make some analysis of the sources and types of error in the lexicon , and how these errors might be reduced .", "can divide the sources of error and inconsistency into four classes 1 . errors of class i f icat ion where an instance of a word is improperly analyzed , and in particular where the words following a verb are not properly identified with regard to complement type .", "SI e eific types of problems include misclassifying ad juncts as arguments or vice versa and identifying the wrong control features .", "Our primary defenses against such errors have been a steady refinement of tile feature deseril tions in ollr nlanlla and rel ; ular grou I review sessions with all the elves .", "Ill particular , we have developed detailed criteria for making adjunct argument distinctiolis O .", "A 1 reliminary study , conducted on examples drawn at random from a corpus not used for our concordance of verbs beginning with j , in dicated that elves were consistent 93 to 94 of the time in labeling argument adjunct distinc tions following our criteria and , in these eases , rarely disagreed on the subcategorization .", "I more than half of the cases where there was disagree then , the elves separately flagged these as drill cult , ambiguous , or figurative uses of the verbs and therefore would probably not use them its the basis for assigning lexical features .", "The agree ment rate for examl les whicti were not flagged was 96 to 98 .", "2 . on i i t ted features where an ell omits a Dature because it is not suggested by an example in the concordance , a citation ill the dictionary , or the elfs introspection .", "In order to get an est . ilnate of the niag , itude of this problem we decided to es tablish a measure of coverage or recall for the subcategorization Dal . ures assigned by our elves .", "lb do this , we tagged the first 150 j verbs from a randomly selected corpus from a part of the San Diego Mercury which was not inchlded in our concordance and then compared the dictio nary entries created by our lexicographers against the tagged eorptis .", "The restllts of this colnparison are sliown in Figure 3 .", "Phe omplements only is the percentage of in stances in the corpus covered by the subcatego rization tags assigned by the elves and does not include the identification of i rly l rel ositions or adverbs .", "llie oinl lements only would corre spond rougllly to the type of inforinal , ion provided by OALI and l , I .", "The COlllpielnc nl , s q l relmsitions l articles colliirin inehides eli the fl , al . ures tllal , is it , eonsidelS the correct idenl , ill cation of the conip einent l ilS the sp , cilie prepo sil . ions aiid adverbs it !", "lllile by eert thi comple illonl . s .", "Ttie two COlllliiliS of igiiies iUlder Ci ni l lenients t I rel ositions lari . icles , show tim re suits with and without the enumeration of dhoe tional l reposltlons .", "We have recently changed ollr approach to i , he classification of verbs like riin send , jog , wall , juml ; wliieh take a long list of direc tional l rel ositlons , by l roviding our entering pro grain with a P D I option on the preposition llst .", "lhis option will automatically assign a list of di rectional prepositions to the verb and thus will saw .", "tirne and eliminate errors of rriissing prepo sitions .", "In some eases this apl roaeli will provide 4 I I OCI does provide some preposltloiis and particles .", "270 elf JOml lenlenl . s lily Conq lemeuts Irepositions lhtrtich s without I I IIL using ILl IlL l 96 89 90 2 82 63 79 3 95 83 92 4 87 69 81 elf av 90 76 8 , 1 elf union 10 1 93 9 , 1 Figure 3 Numl er of subcategorization features assigned 1 . o j verbs by lifferenl , elves .", "elf Coml len , mts only mpl menls F I repositions I artMes tvithoul .", "using ILl IlL _ _ m 1 2 100 1 3 97 1 4 96 2 3 99 2 4 95 3 4 97 2 elf av I !", "85 9 1 92 91 9O 86 92 . 97 ss7 , , 91 Figure 4 Numl er of subcategorization features assigned t j glories by pairs of elw ! s .", "a prel osition list that is a little rich for a given verb I ut we have decided to err on the side of a slight overgeneration rather thall risk missing ally prel ositions which actually occur .", "As you can see , the removal of the ILl IlLs from consideration im proves the in lividual elf scores .", "The elf union score is the union , of the lexical en tries for all fcmr elves .", "These are certainly nuln bets to be proud of , but realistically , having the verbs clone four sel arate times is not I ractical .", "llowew r , in our original proposal we stated that because of the complexity of the verb entries we wouhl like to have them done twice .", "As can be seeil in l igure 5 , with two passes we su ce , , d hi raising individual percentages in all cases .", "We would like to make clear that evell in tim two cases where our individuM lexicographers miss 18 and 13 of the complements , there was only one instance in which this might have resulted in the inability to parse a sentence .", "This was a miss ing intransitNe .", "Otherwise , the missed cOnll le rnents wouhl have been analyzed as adjuncts ince they were a combination of prepositional phrases and adverbials with one case of a suhordinal . e ccm j line ion as .", "We endeavored to make a comparison with LDOCE on the measurement .", "This was a bit dif ficult since LDOCE lacks some con , plements we have and combines others , not always consistently .", "For instance , our PP roughly corresponds to either 1 , 9 our P l a l V l or l rep adv T1 e . g .", "TI our II IAI ; F NP but in some cases a I relmsit ion is ment ioned but the verb is classified as iIltr tllSii , iVe .", "H ie stra , ight l Fw ; ud colnparisor has I , I O E illdhlg 7 ; t of f , he Lagged COml le menl . s hut a softer measure eliminating comple ments that I , I OC , E seems to 1 e lacking IAILT NP IP , ILPOSSIN I , PP Pl aM Mlowing for a 1 P coral lenient for joke , although it is no .", "spec itied , results ill a lmlcelfl . ag ; of 79 .", "We haw adOld . ed tw .", "lines of defense against the prohh ! m of omitted features , l irsg , critical en tries particularly high fre luency wM s have been done independently by two or more elves .", "Second , we are dewq . pinp ; a IIIO1 balanced C HplIH for t lo dv , , s to , , , , i l t .", "i c , . , , , , st , , di , s e4 ; . , 1 co , lh . , , , our serv ; ttions I , h d , I d , ures SllCh as sub atego rizati n patter l l s luay di il ! r sui stantiatly betweell corpora .", "We began with a corpus f , ol , a single newspaper S m . lose Mercury News , but lutve since added the Ihown corpus , several literary works from the l , ibrary of America , scientific ab stracts floul the U . S .", "I epartment of Energy , aml ; ill additional newspal er the Wall treet Jnur Iiztl .", "In , xl , ending the corpus , we h ve limited mlrselves to texts which would lie readily awdlable I , o nlenlliers of the l , inguistic I ata Consortium .", "exl e . ss ft atules when ; m elf assigns a spurious feature throu . gh incorrect extrapolation or analo . gy from available xamples or introspection .", "Because of our desire Io obtain relatively complete foatllre sets , even figr infrequent verl s , we have pernfit 271 ted elves to extrapolate from the citations fotmd .", "Such a process is bound to be less certain than the assignment of features from extant examples .", "Ilowever , this problem does not appear to be very severe .", "A review of tile j verb entries produced by all four elves indicates that the fraction of spu rious entries ranges from 2 to 6 .", "d . fl zzy features feature assignment is defined in terms of the acceptability of words in particular syntactic frames .", "Acceptability , however , is often not absolute but a matter of degree .", "A verb may occur primarily with particular complements , but will be acceptable with others .", "This problem is eompmmded by words which take on particular features only in special contexts .", "Thus , we dont ordinarily think of dead as be ing gradable Fred is more dead than Mary .", ", but we do say deader than a door nail .", "It is also compounded by our decision not to make sense distinctions initially .", "For examl le , many words which are countable require a determiner before the singular form also have a generic sense in which the determiner is not required Fred bought apple .", "but Apple is a wonderflfl fla vor . .", "For each such problematic feature we have prepared gnidelines for the elves , but these still require considerable discretion on their part .", "Fhese problems have emphasized for ns tbe impof tanee of developing a tagged corpus in conjunction with the dictionary , so that frequency of occurrence of a feature and frequency by text type will be avail able .", "We have done stone preliminary tagging in par aim with the completion of our initial dictionary .", "We expect to start tagging in earnest in early summmer .", "Our plan is to begin by tagging verbs in the Brown corpus , in order to be able to correlate our tagging with the word sense tagging being done by tim Word Net group on the same corpus 7 .", "We expect to tag at least 25 instances of each verb .", "If there are not enough occurrences in tim Brown Corlms , we will use examples from the same sources as our extended cor pus s e above .", "5 Acknowledgements Design and preparation of COMLEX Syntax has been supported by tile Advanced Research Projects Agency through the Office of Naval Research under Awards No .", "MDA972 92 J 1016 and N00014 90 J 1851 , and The IYustees of the University of Pennsylwmia .", "41 5 2 Michael Brent .", "From grammar to lexicon Unsu pervised learning oflexical syntax .", "Computalional Linguisties , 19 2 243 262 , 1993 .", "3 l onald ltindle and Mats l . ooth .", "Structural ambi guity and lexieal relations .", "In Proceeding . s of the 29th Annual MeetiT g of the Assn .", "flrr Computa lional Ling . uislics , pages 229 25 ; 6 , Berkeley , CA , June 1991 .", "A . S . Ilornby , edito , .", "O , ford Advanced Learners Diclionary of Current English .", "Christol ller Manning .", "Automatie acquisition of a large subeategorization dictionary fiom eo , pora .", "In Proceedings of the , lst Annual Meeting of the Assn .", "fl r Compulalional Linguistics , pages 22 5 242 , Columbus , OI1 , June 1993 .", "6 Adanl Meyers , Catherine Macleod , and l . alph Grisl , man .", "Standardization of tim complement ad . iunct distinction .", "Proteus Project Memoran dum 64 , Computer Science l epartment , New York University , 1994 .", "7 George Miller , Clm , dia Leaeoek , l andee Tengl , and Ross Ihmker .", "A semantic concordance .", "In Proceedings of the Human Language Technology Workshop , pages 303 308 , Princeton , NJ , March 1993 .", "Morgan l aufmaml .", "8 P . Proctor , editor .", "Longman Dictionary of Con lemporary English .", "Long , nan , 1978 .", "9 Naomi Sager .", "Natural Language Information Pro cessing .", "Addison Wesley , I . eading , MA , 1981 .", "10 Antonio Sanlilippo .", "LI B encoding of lexi eal knowledge .", "In q . Briscoe , A . Copestake , and V . de Iavia , editors , l cfaull Inheritance in Unification Based Approaches to Ihe Lexicom Cambridge University Press , 1 ! 92 .", "References 1 Douglas Biber .", "Using reglster diversified corpora for general angnage studies .", "Computational Lin guistics , 19 2 219 242 , 1993 ."], "summary_lines": ["Comlex Syntax: Building A Computational Lexicon\n", "We describe the design of Complex Syntax, a computational lexicon providing detailed syntactic information for approximately 38,000 English headwords.\n", "We consider the types of errors which arise in creating such a lexicon, and how such errors can be measured and controlled.\n", "Our COMLEX syntax dictionary provides verb subategorization information and syntactic paraphrases, but they are indexed by words thus not suitable to use in generation directly.\n"]}
{"article_lines": ["Japanese Dependency Structure Analysis Based On Support Vector Machines", "This paper presents a method of Japanese dependency structure analysis based on Support Vector Machines SVMs .", "Conventional parsing techniques based on Machine Learning framework , such as Decision Trees and Maximum Entropy Models , have difficulty in selecting useful features as well as finding appropriate combination of selected features .", "On the other hand , it is well known that SVMs achieve high generalization performance even with input data of very high dimensional feature space .", "Furthermore , by introducing the Kernel principle , SVMs can carry out the training in high dimensional spaces with a smaller computational cost independent of their dimensionality .", "We apply SVMs to Japanese dependency structure identification problem .", "Experimental results on Kyoto University corpus show that our sysachieves the 89 . 09 even with small training data 7958 sentences .", "Dependency structure analysis has been recognized as a basic technique in Japanese sentence analysis , and a number of studies have been proposed for years .", "Japanese dependency structure is usually defined in terms of the relationship between phrasal units called 'bunsetsu' segments hereafter quot ; chunks quot ; .", "Generally , dependency structure analysis consists of two steps .", "In the first step , dependency matrix is constructed , in which each element corresponds to a pair of chunks and represents the probability of a dependency relation between them .", "The second step is to find the optimal combination of dependencies to form the entire sentence .", "In previous approaches , these probabilites of dependencies are given by manually constructed rules .", "However , rule based approaches have problems in coverage and consistency , since there are a number of features that affect the accuracy of the final results , and these features usually relate to one another .", "On the other hand , as large scale tagged corpora have become available these days , a number of statistical parsing techniques which estimate the dependency probabilities using such tagged corpora have been developed Collins , 1996 ; Fujio and Matsumoto , 1998 .", "These approaches have overcome the systems based on the rule based approaches .", "Decision Trees Haruno et al . , 1998 and Maximum Entropy models Ratnaparkhi , 1997 ; Uchimoto et al . , 1999 ; Charniak , 2000 have been applied to dependency or syntactic structure analysis .", "However , these models require an appropriate feature selection in order to achieve a high performance .", "In addition , acquisition of an efficient combination of features is difficult in these models .", "In recent years , new statistical learning techniques such as Support Vector Machines SVMs Cortes and Vapnik , 1995 ; Vapnik , 1998 and Boosting Freund and Schapire , 1996 are proposed .", "These techniques take a strategy that maximize the margin between critical examples and the separating hyperplane .", "In particular , compared with other conventional statistical learning algorithms , SVMs achieve high generalization even with training data of a very high dimension .", "Furthermore , by optimizing the Kernel function , SVMs can handle non linear feature spaces , and carry out the training with considering combinations of more than one feature .", "Thanks to such predominant nature , SVMs deliver state of the art performance in realworld applications such as recognition of hand written letters , or of three dimensional images .", "In the field of natural language processing , SVMs are also applied to text categorization , and are reported to have achieved To maximize this margin , we should minimize In other words , this problem becomes equivalent to solving the following optimization problem Furthermore , this optimization problem can be rewritten into the dual form problem Find the Lagrange multipliers ai 0 i 1 , , so that In this dual form problem , xi with non zero ai is called a Support Vector .", "For the Support Vectors , w and b can thus be expressed as follows w E aiyi xi b w xi yi .", "; xiEsvs The elements of the set SVs are the Support Vectors that lie on the separating hyperplanes .", "Finally , the decision function f 1 can be written as high accuracy without falling into over fitting even with a large number of words taken as the features Joachims , 1998 ; Taira and Haruno , 1999 .", "In this paper , we propose an application of SVMs to Japanese dependency structure analysis .", "We use the features that have been studied in conventional statistical dependency analysis with a little modification on them .", "Let us define the training data which belong either to positive or negative class as follows . xi is a feature vector of i th sample , which is represented by an n dimensional vector xi f1 , fn E Rn . yi is a scalar value that specifies the class positive 1 or negative 1 class of i th data .", "Formally , we can define the pattern recognition problem as a learning and building process of the decision function In basic SVMs framework , we try to separate the positive and negative examples in the training data by a linear hyperplane written as w x b 0 wERn , bert .", "1 It is supposed that the farther the positive and negative examples are separated by the discrimination function , the more accurately we could separate unseen test examples with high generalization performance .", "Let us consider two hyperplanes called separating hyperplanes Distance from the separating hyperplane to the point xi can be written as In the case where we cannot separate training examples linearly , quot ; Soft Margin quot ; method forgives some classification errors that may be caused by some noise in the training examples .", "First , we introduce non negative slack variables , and 2 , 3 are rewritten as In this case , we minimize the following value instead of 111w112 The first term in 7 specifies the size of margin and the second term evaluates how far the training data are away from the optimal separating hyperpla , ne .", "C is the parameter that defines the balance of two quantities .", "If we make C larger , the more classification errors are neglected .", "Though we omit the details here , minimization of 7 is reduced to the problem to minimize the objective function 5 under the following constraints .", "Usually , the value of C is estimated experimentally .", "In general classification problems , there are cases in which it is unable to separate the training data linearly .", "In such cases , the training data could be separated linearly by expanding all combinations of features as new ones , and projecting them onto a higherdimensional space .", "However , such a naive approach requires enormous computational overhead .", "Let us consider the case where we project the training data x onto a higher dimensional space by using projection function cio 1 .", "As we pay attention to the objective function 5 and the decision function 6 , these functions depend only on the dot products of the input training vectors .", "If we could calculate the dot products from xi and x2 directly without considering the vectors I xi and I , x2 projected onto the higher dimensional space , we can reduce the computational complexity considerably .", "Namely , we can reduce the computational overhead if we could find the function K that satisfies 4 x2 K xi , x2 .", "8 On the other hand , since we do not need itself for actual learning and classification , 'In general , 1 , x is a mapping into Hilbert space . all we have to do is to prove the existence of cl that satisfies 8 provided the function K is selected properly .", "It is known that 8 holds if and only if the function K satisfies the Mercer condition Vapnik , 1998 .", "In this way , instead of projecting the training data onto the high dimensional space , we can decrease the computational overhead by replacing the dot products , which is calculated in optimization and classification steps , with the function K . Such a function K is called a Kernel function .", "Among the many kinds of Kernel functions available , we will focus on the d th polynomial kernel Use of d th polynomial kernel function allows us to build an optimal separating hyperplane which takes into account all combination of features up to d . Using a Kernel function , we can rewrite the decision function as", "This section describes a general formulation of the probability model and parsing techniques for Japanese statistical dependency analysis .", "First of all , we let a sequence of chunks be b1 , b2 , bni by B , and the sequence dependency pattern be Dep 1 , Dep 2 , , Dep rn .", "1 by D , where Dep i j means that the chunk bi depends on modifies the chunk bi .", "In this framework , we suppose that the dependency sequence D satisfies the following constraints .", "Statistical dependency structure analysis is defined as a searching problem for the dependency pattern D that maximizes the conditional probability P D1 . 3 of the input sequence under the above mentioned constraints .", "If we assume that the dependency probabilities are mutually independent , P DIB could be rewritten as that bi depends on modifies bi . fij is an n dimensional feature vector that represents various kinds of linguistic features related with the chunks bi and bj .", "We obtain Db t taking into all the combination of these probabilities .", "Generally , the optimal solution Db t can be identified by using bottom up algorithm such as CYK algorithm .", "Sekine suggests an efficient parsing technique for Japanese sentences that parses from the end of a sentence Sekine et al . , 2000 .", "We apply Sekine's technique in our experiments .", "In order to use SVMs for dependency analysis , we need to prepare positive and negative examples since SVMs is a binary classifier .", "We adopt a simple and effective method for our purpose Out of all combination of two chunks in the training data , we take a pair of chunks that are in a dependency relation as a positive example , and two chunks that appear in a sentence but are not in a dependency relation as a negative example .", "tank E aklYkIK Via , flij b 11 k , t ; fkiEsvs 11 shows that the distance between test data and the separating hyperplane is put into the sigmoid function , assuming it represents the probability value of the dependency relation .", "We adopt this method in our experiment to transform the distance measure obtained in SVMs into a probability function and analyze dependency structure with a framework of conventional probability model 2 .", "Features that are supposed to be effective in Japanese dependency analysis are head words and their parts of speech , particles and inflection forms of the words that appear at the end of chunks , distance between two chunks , existence of punctuation marks .", "As those are solely defined by the pair of chunks , we refer to them as static features .", "Japanese dependency relations are heavily constrained by such static features since the inflection forms and postpositional particles constrain the dependency relation .", "However , when a sentence is long and there are more than one possible dependents , static features , by themselves cannot determine the correct dependency .", "Let us look at the following example . watashi ha kono hon wo motteiru josei wo sagasiteiru I top , this book acc , have , lady acc , be looking for In this example , quot ; kono hon wo this bookacc quot ; may modify either of quot ; motteiru have quot ; or quot ; sagasiteiru be looking for quot ; and cannot be determined only with the static features .", "However , quot ; josei wo lady am quot ; can modify the only the verb quot ; sagasiteiru , quot ; .", "Knowing such information is quite useful for resolving syntactic ambiguity , since two accusative noun phrses hardly modify the same verb .", "It is possible to use such information if we add new features related to other modifiers .", "In the above case , the chunk quot ; sagasiteiru quot ; can receive a new feature of accusative modification by quot ; josei wo quot ; during the parsing process , which precludes the chunk quot ; kono honwo quot ; from modifying quot ; sagasiteiru quot ; since there is a strict constraint about double accusative modification that will be learned from training examples .", "We decided to take into consideration all such modification information by using functional words or inflection forms of modifiers .", "Using such information about modifiers in the training phase has no difficulty since they are clearly available in a tree bank .", "On the other hand , they are not known in the parsing phase of the test data .", "This problem can be easily solved if we adopt a bottom up parsing algorithm and attach the modification information dynamically to the newly constructed phrases the chunks that become the head of the phrases .", "As we describe later we apply a beam search for parsing , and it is possible to keep several intermediate solutions while suppressing the combinatorial explosion .", "We refer to the features that are added incrementally during the parsing process as dynamic features .", "We use Kyoto University text corpus Version 2 . 0 consisting of articles of Mainichi newspaper annotated with dependency structure Kurohashi and Nagao , 1997 .", "7 , 958 sentences from the articles on January 1st to January 7th are used for the training data , and 1 , 246 sentences from the articles on January 9th are used for the test data .", "For the kernel function , we used the polynomial function 9 .", "We set the soft margin parameter C to be 1 .", "The feature set used in the experiments are shown in Table 1 .", "The static features are basically taken from Uchimoto's list Uchimoto et al . , 1999 with little modification .", "In Table 1 , 'Head' means the rightmost content word in a chunk whose part of speech is not a functional category .", "'Type' mewls the rightmost functional word or the inflectional form of the rightmost predicate if there is no functional word in the chunk .", "The static features include the information on existence of brackets , question marks and punctuation marks etc .", "Besides , there are features that show the relative relation of two chunks , such as distance , and existence of brackets , quotation marks and punctuation marks between them .", "For dynamic features , we selected functional words or inflection forms of the rightmost predicates in the chunks that appear between two chunks and depend on the modifiee .", "Considering data sparseness problem , we apply a simple filtering based on the part ofspeech of functional words We use the lexical form if the word's POS is particle , adverb , adnominal or conjunction .", "We use the inflection form if the word has inflection .", "We use the POS tags for others .", "Table 2 shows the result of passing accuracy under the condition k 5 beam width , and d 3 dimension of the polynomial functions used for the kernel function .", "This table shows two types of dependency accuracy , A and B .", "The training data size is measured by the number of sentences .", "The accuracy A means the accuracy of the entire dependency relations .", "Since Japanese is a headfinal language , the second chunk from the end of a sentence always modifies the last chunk .", "The accuracy B is calculated by excluding this dependency relation .", "Hereafter , we use the accuracy A , if it is not explicitly specified , since this measure is usually used in other literature .", "Table3 shows the accuracy when only static features are used .", "Generally , the results with dynamic feature set is better than the results without them .", "The results with dynamic features constantly outperform that with static features only .", "In most of cases , the improvements is significant .", "In the experiments , we restrict the features only from the chunks that appear between two chunks being in consideration , however , dynamic features could be also taken from the chunks that appear not between the two chunks .", "For example , we could also take into consideration the chunk that is modified by the right chunk , or the chunks that modify the left chunk .", "We leave experiment in such a setting for the future work .", "Figure 1 shows the relationship between the size of the training data and the parsing accuracy .", "This figure shows the accuracy of with and without the dynamic features .", "The parser achieves 86 . 52 accuracy for test data even with small training data 1172 sentences .", "This is due to a good characteristic of SVMs to cope with the data sparseness problem .", "Furthermore , it achieves almost 100 accuracy for the training data , showing that the training data are completely separated by appropriate combination of features .", "Generally , selecting those specific features of the training data tends to cause overfitting , and accuracy for test data may fall .", "However , the SVMs method achieve a high accuracy not only on the training data but also on the test data .", "We claim that this is due to the high generalization ability of SVMs .", "In addition , observing at the learning curve , further improvement will be possible if we increase the size of the training data .", "Table 4 shows the relationship between the dimension of the kernel function and the parsing accuracy under the condition k 5 .", "As a result , the case of d 4 gives the best accuracy .", "We could not carry out the training in realistic time for the case of d 1 .", "This result supports our intuition that we need a combination of at least two features .", "In other words , it will be hard to confirm a dependency relation with only the features of the modifier or the modfiee .", "It is natural that a dependency relation is decided by at least the information from both of two chunks .", "In addition , further improvement has been possible by considering combinations of three or more features .", "Sekine Sekine et al . , 2000 gives an interesting report about the relationship between the beam width and the parsing accuracy .", "Generally , high parsing accuracy is expected when a large beam width is employed in the dependency structure analysis .", "However , the result is against our intuition .", "They report that a beam width between 3 and 10 gives the best parsing accuracy , and parsing accuracy falls down with a width larger than 10 .", "This result suggests that Japanese dependency structures may consist of a series of local optimization processes .", "We evaluate the relationship between the beam width and the parsing accuracy .", "Table 5 shows their relationships under the condition d 3 , along with the changes of the beam width from k 1 to 15 .", "The best parsing accuracy is achieved at k 5 and the best sentence accuracy is achieved at k 5 and k 7 .", "We have to consider how we should set the beam width that gives the best parsing accuracy .", "We believe that the beam width that gives the best passing accuracy is related not only with the length of the sentence , but also with the lexical entries and parts of speech that comprise the chunks .", "Instead of learning a single classifier using all training data , we can make n classifiers dividing all training data by n , and the final result is decided by their voting .", "This approach would reduce computational overhead .", "The use of multi processing computer would help to reduce their training time considerably since all individual training can be carried out in parallel .", "To investigate the effectiveness of this method , we perform a simple experiment Dividing all training data 7958 sentences by 4 , the final dependency score is given by a weighted average of each scores .", "This simple voting approach is shown to achieve the accuracy of 88 . 66 , which is nearly the same accuracy achieved 5540 training sentences .", "In this experiment , we simply give an equal weight to each classifier .", "However , if we optimized the voting weight more carefully , the further improvements would be achieved Inui and Inui , 2000 .", "Uchimoto Uchimoto et al . , 1999 and Sekine Sekine et al . , 2000 report that using Kyoto University Corpus for their training and testing , they achieve around 87 . 2 accuracy by building statistical model based on Maximum Entropy framework .", "For the training data , we used exactly the same data that they used in order to make a fair comparison .", "In our experiments , the accuracy of 89 . 09 is achieved using same training data .", "Our model outperforms Uchimoto's model as far as the accuracies are compared .", "Although Uchimoto suggests that the importance of considering combination of features , in ME framework we must expand these combination by introducing new feature set .", "Uchimoto heuristically selects quot ; effective quot ; combination of features .", "However , such a manual selection does not always cover all relevant combinations that are important in the determination of dependency relation .", "We believe that our model is better than others from the viewpoints of coverage and consistency , since our model learns the combination of features without increasing the computational complexity .", "If we want to reconsider them , all we have to do is just to change the Kernel function .", "The computational complexity depends on the number of support vectors not on the dimension of the Kernel function .", "The simplest and most effective way to achieve better accuracy is to increase the training data .", "However , the proposed method that uses all candidates that form dependency relation requires a great amount of time to compute the separating hyperplane as the size of the training data increases .", "The experiments given in this paper have actually taken long training time 3 .", "To handle large size of training data , we have to select only the related portion of examples that are effective for the analysis .", "This will reduce the training overhead as well as the analysis time .", "The committee based approach discussed section 4 . 7 is one method of coping with this problem .", "For future research , to reduce the computational overhead , we will work on methods for sample selection as follows Some pairs of chunks need not consider since there is no possibility of dependency between them from grammatical constraints .", "Such pairs of chunks are not necessary to use as negative examples in the training phase .", "For example , a chunk within quotation marks may not modify a chunk that locates outside of the quotation marks .", "Of course , we have to be careful in introducing such constraints , and they should be learned from existing corpus .", "Integration with other simple models Suppose that a computationally light and moderately accuracy learning model is obtainable there are actually such systems based on probabilistic parsing models .", "We can use the system to output some redundant parsing results and use only those results for the positive and negative examples .", "This is another way to reduce the size of training data .", "We can start with a small size of training data with a small size of feature set .", "Then , by analyzing held out training data and selecting the features that affect the passing accuracy .", "This kind of gradual increase of training data and feature set will be another method for reducing the computational overhead .", "This paper proposes Japanese dependency analysis based on Support Vector Machines .", "Through the experiments with Japanese bracketed corpus , the proposed method achieves a high accuracy even with a small 3With AlphaServer 8400 617Mhz , it took 15 days to train with 7958 sentences . training data and outperforms existing methods based on Maximum Entropy Models .", "The result shows that Japanese dependency analysis can be effectively performed by use of SVMs due to its good generalization and nonoverfitting characteristics ."], "summary_lines": ["Japanese Dependency Structure Analysis Based On Support Vector Machines\n", "This paper presents a method of Japanese dependency structure analysis based on Support Vector Machines (SVMs).\n", "Conventional parsing techniques based on Machine Learning framework, such as Decision Trees and Maximum Entropy Models, have difficulty in selecting useful features as well as finding appropriate combination of selected features.\n", "On the other hand, it is well-known that SVMs achieve high generalization performance even with input data of very high dimensional feature space.\n", "Furthermore, by introducing the Kernel principle, SVMs can carry out the training in high-dimensional spaces with a smaller computational cost independent of their dimensionality.\n", "We apply SVMs to Japanese dependency structure identification problem.\n", "Experimental results on Kyoto University corpus show that our system achieves the accuracy of 89.09% even with small training data (7958 sentences).\n", "We introduce a new type of feature called dynamic features which are created dynamically during the parsing process.\n"]}
{"article_lines": ["Towards Terascale Semantic Acquisition", "Although vast amounts of textual data are freely available , many NLP algorithms exploit only a minute percentage of it .", "In this paper , we study the challenges of working at the terascale .", "We present an algorithm , designed for the terascale , for mining is a relations that achieves similar performance to a state of the art linguistically rich method .", "We focus on the accuracy of these two systems as a func tion of processing time and corpus size .", "The Natural Language Processing NLP com munity has recently seen a growth in corpus based methods .", "Algorithms light in linguistic theories but rich in available training data have been successfully applied to several applications such as ma chine translation Och and Ney 2002 , information extraction Etzioni et al 2004 , and question an swering Brill et al 2001 .", "In the last decade , we have seen an explosion in the amount of available digital text resources .", "It is estimated that the Internet contains hundreds of terabytes of text data , most of which is in an unstructured format .", "Yet , many NLP algorithms tap into only megabytes or gigabytes of this information .", "In this paper , we make a step towards acquiring semantic knowledge from terabytes of data .", "We present an algorithm for extracting is a relations , designed for the terascale , and compare it to a state of the art method that employs deep analysis of text Pantel and Ravichandran 2004 .", "We show that by simply utilizing more data on this task , we can achieve similar performance to a linguisticallyrich approach .", "The current state of the art co occurrence model requires an estimated 10 years just to parse a 1TB corpus see Table 1 .", "Instead of using a syntactically motivated co occurrence ap proach as above , our system uses lexico syntactic rules .", "In particular , it finds lexico POS patterns by making modifications to the basic edit distance algorithm .", "Once these patterns have been learnt , the algorithm for finding new is a relations runs in O n , where n is the number of sentences .", "In semantic hierarchies such as WordNet Miller 1990 , an is a relation between two words x and y represents a subordinate relationship i . e . x is more specific than y .", "Many algorithms have recently been proposed to automatically mine is a hypo nym hypernym relations between words .", "Here , we focus on is a relations that are characterized by the questions ? What Who is X ? ?", "For example , Table 2 shows a sample of 10 is a relations discovered by the algorithms presented in this paper .", "In this table , we call azalea , tiramisu , and Winona Ryder in stances of the respective concepts flower , dessert and actress .", "These kinds of is a relations would be useful for various purposes such as ontology con struction , semantic information retrieval , question answering , etc . The main contribution of this paper is a comparison of the quality of our pattern based and co occurrence models as a function of processing time and corpus size .", "Also , the paper lays a foundation for terascale acquisition of knowledge .", "We will show that , for very small or very large corpora or for situations where recall is valued over precision , the pattern based approach is best .", "Previous approaches to extracting is a relations fall under two categories pattern based and co occurrence based approaches .", "2 . 1 Pattern based approaches .", "Marti Hearst 1992 was the first to use a pat tern based approach to extract hyponym relations from a raw corpus .", "She used an iterative process to semi automatically learn patterns .", "However , a corpus of 20MB words yielded only 400 examples .", "Our pattern based algorithm is very similar to the one used by Hearst .", "She uses seed examples to manually discover her patterns whearas we use a minimal edit distance algorithm to automatically discover the patterns .", "771Riloff and Shepherd 1997 used a semi automatic method for discovering similar words using a few seed examples by using pattern based techniques and human supervision .", "Berland and Charniak 1999 used similar pattern based tech niques and other heuristics to extract meronymy part whole relations .", "They reported an accuracy of about 55 precision on a corpus of 100 , 000 words .", "Girju et al 2003 improved upon Berland and Charniak's work using a machine learning filter .", "Mann 2002 and Fleischman et al 2003 used part of speech patterns to extract a subset of hyponym relations involving proper nouns .", "Our pattern based algorithm differs from these approaches in two ways .", "We learn lexico POS patterns in an automatic way .", "Also , the patterns are learned with the specific goal of scaling to the terascale see Table 2 .", "2 . 2 Co occurrence based approaches .", "The second class of algorithms uses co occurrence statistics Hindle 1990 , Lin 1998 .", "These systems mostly employ clustering algo rithms to group words according to their meanings in text .", "Assuming the distributional hypothesis Harris 1985 , words that occur in similar gram matical contexts are similar in meaning .", "Curran and Moens 2002 experimented with corpus size and complexity of proximity features in building automatic thesauri .", "CBC Clustering by Commit tee proposed by Pantel and Lin 2002 achieves high recall and precision in generating similarity lists of words discriminated by their meaning and senses .", "However , such clustering algorithms fail to name their classes .", "Caraballo 1999 was the first to use clustering for labeling is a relations using conjunction and apposition features to build noun clusters .", "Re cently , Pantel and Ravichandran 2004 extended this approach by making use of all syntactic de pendency features for each noun .", "Much of the research discussed above takes a similar approach of searching text for simple sur face or lexico syntactic patterns in a bottom up approach .", "Our co occurrence model Pantel and Ravichandran 2004 makes use of semantic classes like those generated by CBC .", "Hyponyms are gen erated in a top down approach by naming each group of words and assigning that name as a hypo nym of each word in the group i . e . , one hyponym per instance group label pair .", "The input to the extraction algorithm is a list of semantic classes , in the form of clusters of words , which may be generated from any source .", "For example , following are two semantic classes discov ered by CBC A peach , pear , pineapple , apricot , mango , raspberry , lemon , cherry , strawberry , melon , blueberry , fig , apple , plum , nectarine , avocado , grapefruit , papaya , banana , cantaloupe , cranberry , blackberry , lime , orange , tangerine , . . .", "B Phil Donahue , Pat Sajak , Arsenio Hall , Geraldo Rivera , Don Imus , Larry King , David Letterman , Conan O'Brien , Rosie O'Donnell , Jenny Jones , Sally Jessy Raph ael , Oprah Winfrey , Jerry Springer , Howard Stern , Jay Leno , Johnny Carson , . . .", "The extraction algorithm first labels concepts A and B with fruit and host respectively .", "Then , is a relationships are extracted , such as apple is a fruit , pear is a fruit , and David Letterman is a host .", "An instance such as pear is assigned a hypernym fruit not because it necessarily occurs in any par ticular syntactic relationship with the word fruit , but because it belongs to the class of instances that does .", "The labeling of semantic classes is performed in three phases , as outlined below .", "3 . 1 Phase I . In the first phase of the algorithm , feature vec tors are extracted for each word that occurs in a semantic class .", "Each feature corresponds to a grammatical context in which the word occurs .", "For example , ? catch __ ?", "is a verb object context .", "If the word wave occurred in this context , then the con text is a feature of wave .", "We then construct a mutual information vector MI e mie1 , mie2 , ? , miem for each word e , where mief is the pointwise mutual information between word e and context f , which is defined as N c N c N c ef m j ej n i if ef mi ? ?", "? 11 log Table 2 .", "Sample of 10 is a relationships discovered by our co occurrence and pattern based systems .", "CO OCCURRENCE SYSTEM PATTERN BASED SYSTEM Word Hypernym Word Hypernym azalea flower American airline bipolar disorder disease Bobby Bonds coach Bordeaux wine radiation therapy cancer treatment Flintstones television show tiramisu dessert salmon fish Winona Ryder actress Table 1 .", "Approximate processing time on a single Pentium 4 2 . 5 GHz machine .", "TOOL 15 GB ORPUS 1 TB CORPUS POS Tagger 2 days 125 days NP Chunker 3 days 214 days Dependency Parser 56 days 10 . 2 years Syntactic Parser 5 . 8 years 388 . 4 years 772 where n is the number of elements to be clustered , cef is the frequency count of word e in grammatical context f , and N is the total frequency count of all features of all words .", "3 . 2 Phase II .", "Following Pantel and Lin 2002 , a committee for each semantic class is constructed .", "A committee is a set of representative elements that unambi guously describe the members of a possible class .", "For example , in one of our experiments , the committees for semantic classes A and B from Sec tion 3 were A peach , pear , pineapple , apricot , mango , raspberry , lemon , blueberry B Phil Donahue , Pat Sajak , Arsenio Hall , Geraldo Rivera , Don Imus , Larry King , David Letterman 3 . 3 Phase III .", "By averaging the feature vectors of the commit tee members of a particular semantic class , we obtain a grammatical template , or signature , for that class .", "For example , Figure 1 shows an excerpt of the grammatical signature for semantic class B .", "The vector is obtained by averaging the fea ture vectors of the words in the committee of this class .", "The ? V subj N joke ?", "feature indicates a sub ject verb relationship between the class and the verb joke while ? N appo N host ?", "indicates an ap position relationship between the class and the noun host .", "The two columns of numbers indicate the frequency and mutual information scores .", "To name a class , we search its signature for cer tain relationships known to identify class labels .", "These relationships , automatically learned in Pantel and Ravichandran 2004 , include apposi tions , nominal subjects , such as relationships , and like relationships .", "We sum up the mutual information scores for each term that occurs in these rela tionships with a committee of a class .", "The highest scoring term is the name of the class .", "The syntactical co occurrence approach has worst case time complexity O n2k , where n is the number of words in the corpus and k is the feature space Pantel and Ravichandran 2004 .", "Just to parse a 1 TB corpus , this approach requires ap proximately 10 . 2 years see Table 2 .", "We propose an algorithm for learning highly scalable lexico POS patterns .", "Given two sentences with their surface form and part of speech tags , the algorithm finds the optimal lexico POS alignment .", "For example , consider the following 2 sentences 1 Platinum is a precious metal .", "2 Molybdenum is a metal .", "Applying a POS tagger Brill 1995 gives the following output Surface Platinum is a precious metal . POS NNP VBZ DT JJ NN . Surface Molybdenum is a metal . POS NNP VBZ DT NN . A very good pattern to generalize from the alignment of these two strings would be Surface is a metal . POS NNP . We use the following notation to denote this alignment ? _NNP is a s metal . ? , where ? _NNP represents the POS tag NNP ? .", "To perform such alignments we introduce two wildcard operators , skip s and wildcard g .", "The skip operator represents 0 or 1 instance of any word similar to the w pattern in Perl , while the wildcard operator represents exactly 1 instance of any word similar to the w pattern in Perl .", "4 . 1 Algorithm .", "We present an algorithm for learning patterns at multiple levels .", "Multilevel representation is de fined as the different levels of a sentence such as the lexical level and POS level .", "Consider two strings a 1 , n and b 1 , m of lengths n and m re spectively .", "Let a1 1 , n and a2 1 , n be the level 1 lexical level and level 2 POS level representa tions for the string a 1 , n .", "Similarly , let b1 1 , m and b2 1 , m be the level 1 and level 2 representa tions for the string b 1 , m .", "The algorithm consists of two parts calculation of the minimal edit dis tance and retrieval of an optimal pattern .", "The minimal edit distance algorithm calculates the number of edit operations insertions , deletions and replacements required to change one string to another string .", "The optimal pattern is retrieved by Phil Donahue , Pat Sajak , Arsenio Hall N gen N talk show 93 11 . 77 television show 24 11 . 30 TV show 25 10 . 45 show 255 9 . 98 audience 23 7 . 80 joke 5 7 . 37 V subj N joke 39 7 . 11 tape 10 7 . 09 poke 15 6 . 87 host 40 6 . 47 co host 4 6 . 14 banter 3 6 . 00 interview 20 5 . 89 N appo N host 127 12 . 46 comedian 12 11 . 02 King 13 9 . 49 star 6 7 . 47 Figure 1 .", "Excerpt of the grammatical signature for the television host class .", "773 keeping track of the edit operations which is the second part of the algorithm .", "Algorithm for calculating the minimal edit distance between two strings D 0 , 0 0 for i 1 to n do D i , 0 D i 1 , 0 cost insertion for j 1 to m do D 0 , j D 0 , j 1 cost deletion for i 1 to n do for j 1 to m do D i , j min D i 1 , j 1 cost substitution , D i 1 , j cost insertion , D i , j 1 cost deletion Print D n , m Algorithm for optimal pattern retrieval i n , j m ; while i ? 0 and j ? 0 if D i , j D i 1 , j cost insertion print s , i i 1 else if D i , j D i , j 1 cost deletion print s , j j 1 else if a1i b1j print a1i , i i 1 , j j 1 else if a2i b2j print a2i , i i 1 , j j 1 else print g , i i 1 , j j 1 We experimentally set by trial and error cost insertion 3 cost deletion 3 cost substitution 0 if a1i b1j 1 if a1i ? b1j , a2i b2j 2 if a1i ? b1j , a2i ? b2j 4 . 2 Implementation and filtering .", "The above algorithm takes O y2 time for every pair of strings of length at most y . Hence , if there are x strings in the collection , each string having at most length y , the algorithm has time complexity O x2y2 to extract all the patterns in the collection .", "Applying the above algorithm on a corpus of 3GB with 50 is a relationship seeds , we obtain a set of 600 lexico POS .", "Following are two of them 1 X_JJ NN JJ NN NN NN _CC Y_JJ JJ NN JJ NNS NN JJ NNS NN NN JJ NN JJ NN NN e . g . ? caldera or lava lake ?", "2 X_NNP NNP NNP NNP NNP NNP NNP CC NNP NNP VBN NN NN VBG NN NN , _ , _DT Y_NN IN NN JJ JJ NN JJ NN NN IN NNP NNP NNP NN NN JJ NN JJ NN NN e . g . ? leukemia , the cancer of . . .", "Note that we store different POS variations of the anchors X and Y . As shown in example 1 , the POS variations of the anchor X are JJ NN , JJ NN NN , NN .", "The variations for anchor Y are JJ JJ NN , JJ , etc . .", "The reason is quite straightforward we need to determine the boundary of the anchors X and Y and a reasonable way to delimit them would be to use POS information .", "All the patterns produced by the multi level pattern learning algo rithm were generated from positive examples .", "From amongst these patterns , we need to find the most important ones .", "This is a critical step because frequently occurring patterns have low precision whereas rarely occurring patterns have high preci sion .", "From the Information Extraction point of view neither of these patterns is very useful .", "We need to find patterns with relatively high occurrence and high precision .", "We apply the log likeli hood principle Dunning 1993 to compute this score .", "The top 15 patterns according to this metric are listed in Table 3 we omit the POS variations for visibility .", "Some of these patterns are similar to the ones discovered by Hearst 1992 while other patterns are similar to the ones used by Fleischman et al 2003 .", "4 . 3 Time complexity .", "To extract hyponym relations , we use a fixed number of patterns across a corpus .", "Since we treat each sentences independently from others , the algorithm runs in linear time O n over the corpus size , where n is number of sentences in the corpus .", "In this section , we empirically compare the pattern based and co occurrence based models pre sented in Section 3 and Section 4 .", "The focus is on the precision and recall of the systems as a func tion of the corpus size .", "5 . 1 Experimental Setup .", "We use a 15GB newspaper corpus consisting of TREC9 , TREC 2002 , Yahoo !", "News 0 . 5GB , AP newswire 2GB , New York Times 2GB , Reuters 0 . 8GB , Wall Street Journal 1 . 2GB , and various online news website 1 . 5GB .", "For our experiments , we extract from this corpus six data sets of differ ent sizes 1 . 5MB , 15 MB , 150 MB , 1 . 5GB , 6GB and 15GB .", "For the co occurrence model , we used Minipar Lin 1994 , a broad coverage parser , to parse each data set .", "We collected the frequency counts of the grammatical relationships contexts output by Minipar and used them to compute the pointwise mutual information vectors described in Section 3 . 1 .", "For the pattern based approach , we use Brill ? s . POS tagger 1995 to tag each data set .", "5 . 2 Precision .", "We performed a manual evaluation to estimate the precision of both systems on each dataset .", "For each dataset , both systems extracted a set of is a Table 3 .", "Top 15 lexico syntactic patterns discovered by our system .", "X , or Y X , _DT Y _ WDT IN Y like X and X , a an Y X , _RB known as Y _NN , X and other Y X , Y X Y Y , including X , Y , or X Y such as X Y , such as X X is a Y X , _RB called Y Y , especially X 774relationships .", "Six sets were extracted for the pattern based approach and five sets for the co occurrence approach the 15GB corpus was too large to process using the co occurrence model ? see dependency parsing time estimates in Table 2 .", "From each resulting set , we then randomly se lected 50 words along with their top 3 highest ranking is a relationships .", "For example , Table 4 shows three randomly selected names for the pat tern based system on the 15GB dataset .", "For each word , we added to the list of hypernyms a human generated hypernym obtained from an annotator looking at the word without any system or Word Net hyponym .", "We also appended the WordNet hypernyms for each word only for the top 3 senses .", "Each of the 11 random samples contained a maximum of 350 is a relationships to manually evaluate 50 random words with top 3 system , top 3 WordNet , and human generated relationship . .", "We presented each of the 11 random samples to two human judges .", "The 50 randomly selected words , together with the system , human , and WordNet generated is a relationships , were ran domly ordered .", "That way , there was no way for a judge to know the source of a relationship nor each system ? s ranking of the relationships .", "For each relationship , we asked the judges to assign a score of correct , partially correct , or incorrect .", "We then computed the average precision of the system , human , and WordNet on each dataset .", "We also computed the percentage of times a correct rela tionship was found in the top 3 is a relationships of a word and the mean reciprocal rank MRR .", "For each word , a system receives an MRR score of 1 M , where M is the rank of the first name judged correct .", "Table 5 shows the results comparing the two automatic systems .", "Table 6 shows similar results for a more lenient evaluation where both correct and partially correct are judged correct .", "For small datasets below 150MB , the pattern based method achieves higher precision since the co occurrence method requires a certain critical mass of statistics before it can extract useful class signatures see Section 3 .", "On the other hand , the pattern based approach has relatively constant precision since most of the is a relationships se lected by it are fired by a single pattern .", "Once the co occurrence system reaches its critical mass at 150MB , it generates much more precise hypo nyms .", "The Kappa statistics for our experiments were all in the range 0 . 78 ? 0 . 85 .", "Table 7 and Table 8 compare the precision of the pattern based and co occurrence based methods with the human and WordNet hyponyms .", "The variation between the human and WordNet scores across both systems is mostly due to the relative cleanliness of the tokens in the co occurrencebased system due to the parser used in the ap proach .", "WordNet consistently generated higher precision relationships although both algorithms approach WordNet quality on 6GB the pattern based algorithm even surpasses WordNet precision on 15GB .", "Furthermore , WordNet only generated a hyponym 40 of the time .", "This is mostly due to the lack of proper noun coverage in WordNet .", "On the 6 GB corpus , the co occurrence approach took approximately 47 single Pentium 4 2 . 5 GHz processor days to complete , whereas it took the pattern based approach only four days to complete on 6 GB and 10 days on 15 GB .", "5 . 3 Recall .", "The co occurrence model has higher precision than the pattern based algorithm on most datasets .", "Table 4 .", "Is a relationships assigned to three randomly selected words using pattern based system on 15GB dataset .", "RANDOM WORD HUMAN WORDNET PATTERN BASED SYSTEM RANKED Sanwa Bank bank none subsidiary lender bank MCI Worldcom Inc . telecommunications company none phone company competitor company cappuccino beverage none item food beverage Table 5 .", "Average precision , top 3 precision , and MRR for both systems on each dataset .", "PATTERN SYSTEM CO OCCURRENCE SYSTEM Prec Top 3 MRR Prec Top 3 MRR 1 . 5MB 38 . 7 41 . 0 41 . 0 4 . 3 8 . 0 7 . 3 15MB 39 . 1 43 . 0 41 . 5 14 . 6 32 . 0 24 . 3 150MB 40 . 6 46 . 0 45 . 5 51 . 1 73 . 0 67 . 0 1 . 5GB 40 . 4 39 . 0 39 . 0 56 . 7 88 . 0 77 . 7 6GB 46 . 3 52 . 0 49 . 7 64 . 9 90 . 0 78 . 8 15GB 55 . 9 54 . 0 52 . 0 Too large to process Table 6 .", "Lenient average precision , top 3 precision , and MRR for both systems on each dataset .", "PATTERN SYSTEM CO OCCURRENCE SYSTEM Prec Top 3 MRR Prec Top 3 MRR 1 . 5MB 56 . 6 60 . 0 60 . 0 12 . 4 20 . 0 15 . 2 15MB 57 . 3 63 . 0 61 . 0 23 . 2 50 . 0 37 . 3 150MB 50 . 7 56 . 0 55 . 0 60 . 6 78 . 0 73 . 2 1 . 5GB 52 . 6 51 . 0 51 . 0 69 . 7 93 . 0 85 . 8 6GB 61 . 8 69 . 0 67 . 5 78 . 7 92 . 0 86 . 2 15GB 67 . 8 67 . 0 65 . 0 Too large to process 775 However , Figure 2 shows that the pattern based approach extracts many more relationships .", "Semantic extraction tasks are notoriously diffi cult to evaluate for recall .", "To approximate recall , we defined a relative recall measure and conducted a question answering QA task of answering defi nition questions .", "5 . 3 . 1 Relative recall Although it is impossible to know the number of is a relationships in any non trivial corpus , it is possible to compute the recall of a system relative to another system ? s recall .", "The recall of a system A , RA , is given by the following formula C C R AA where CA is the number of correct is a relation ships extracted by A and C is the total number of correct is a relationships in the corpus .", "We define relative recall of system A given system B , RA , B , as B A B A BA C C R R R , Using the precision estimates , PA , from the pre vious section , we can estimate CA ? PA ? A , where A is the total number of is a relationships discov ered by system A . Hence , BP AP R B A BA ? ?", ", Figure 3 shows the relative recall of A pattern based approach relative to B co occurrence model .", "Because of sparse data , the pattern based approach has much higher precision and recall six times than the co occurrence approach on the small 15MB dataset .", "In fact , only on the 150MB dataset did the co occurrence system have higher recall .", "With datasets larger than 150MB , the co occurrence algorithm reduces its running time by filtering out grammatical relationships for words that occurred fewer than k 40 times and hence recall is affected in contrast , the pattern based approach may generate a hyponym for a word that it only sees once .", "5 . 3 . 2 Definition questions Following Fleischman et al 2003 , we select the 50 definition questions from the TREC2003 Voorhees 2003 question set .", "These questions are of the form ? Who is X ? ?", "and ? What is X ? ?", "For each question e . g . , ? Who is Niels Bohr ? ? , ? What is feng shui ? ?", "we extract its respective instance e . g . , ? Neils Bohr ?", "and ? feng shui ? , look up their corresponding hyponyms from our is a table , and present the corresponding hyponym as the answer .", "We compare the results of both our systems with WordNet .", "We extract at most the top 5 hyponyms provided by each system .", "We manually evaluate the three systems and assign 3 classes ? Correct C ? , ? Partially Correct P ?", "or ? Incorrect I ?", "to each answer .", "This evaluation is different from the evaluation performed by the TREC organizers for definition questions .", "However , by being consistent across all Total Number of Is A Relationships vs . Dataset 0 200000 400000 600000 800000 1000000 1200000 1400000 1 . 5MB 15MB 150MB 1 . 5GB 6GB 15GB Datasets To ta l Is A Re la tio n s hi ps s Pattern based System Co occurrence based System Figure 2 .", "Number of is a relationships extracted by the pattern based and co occurrence based approaches .", "Table 7 .", "Average precision of the pattern based sys tem vs . WordNet and human hyponyms .", "PRECISION MRR Pat .", "WNet Human Pat .", "WNet Human 1 . 5MB 38 . 7 45 . 8 83 . 0 41 . 0 84 . 4 83 . 0 15MB 39 . 1 52 . 4 81 . 0 41 . 5 95 . 0 91 . 0 150MB 40 . 6 49 . 4 84 . 0 45 . 5 88 . 9 94 . 0 1 . 5GB 40 . 4 43 . 4 79 . 0 39 . 0 93 . 3 89 . 0 6GB 46 . 3 46 . 5 76 . 0 49 . 7 75 . 0 76 . 0 15GB 55 . 9 45 . 6 79 . 0 52 . 0 78 . 0 79 . 0 Table 8 .", "Average precision of the co occurrence based system vs . WordNet and human hyponyms .", "PRECISION MRR Co occ WNet Human Co occ WNet Human 1 . 5MB 4 . 3 42 . 7 52 . 7 7 . 3 87 . 7 95 . 0 15MB 14 . 6 38 . 1 48 . 7 24 . 3 86 . 6 95 . 0 150MB 51 . 1 57 . 5 65 . 8 67 . 0 85 . 1 98 . 0 1 . 5GB 56 . 7 62 . 8 70 . 3 77 . 7 93 . 0 98 . 0 6GB 64 . 9 68 . 9 75 . 2 78 . 8 94 . 3 98 . 0 Relative Recall Pattern based vs . Co occurrence based 0 . 00 1 . 00 2 . 00 3 . 00 4 . 00 5 . 00 6 . 00 7 . 00 1 . 5MB 15MB 150MB 1 . 5GB 6GB 15GB projected Datesets Re la tiv e Re ca ll Figure 3 .", "Relative recall of the pattern based approach relative to the co occurrence approach .", "776 systems during the process , these evaluations give an indication of the recall of the knowledge base .", "We measure the performance on the top 1 and the top 5 answers returned by each system .", "Table 9 and Table 10 show the results .", "The corresponding scores for WordNet are 38 accuracy in both the top 1 and top 5 categories for both strict and lenient .", "As seen in this experiment , the results for both the pattern based and cooccurrence based systems report very poor per formance for data sets up to 150 MB .", "However , there is an increase in performance for both systems on the 1 . 5 GB and larger datasets .", "The per formance of the system in the top 5 category is much better than that of WordNet 38 .", "There is promise for increasing our system accuracy by re ranking the outputs of the top 5 hypernyms .", "There is a long standing need for higher quality performance in NLP systems .", "It is possible that semantic resources richer than WordNet will en able them to break the current quality ceilings .", "Both statistical and symbolic NLP systems can make use of such semantic knowledge .", "With the increased size of the Web , more and more training data is becoming available , and as Banko and Brill 2001 showed , even rather simple learning algo rithms can perform well when given enough data .", "In this light , we see an interesting need to de velop fast , robust , and scalable methods to mine semantic information from the Web .", "This paper compares and contrasts two methods for extracting is a relations from corpora .", "We presented a novel pattern based algorithm , scalable to the terascale , which outperforms its more informed syntactical co occurrence counterpart on very small and very large data .", "Albeit possible to successfully apply linguisti cally light but data rich approaches to some NLP applications , merely reporting these results often fails to yield insights into the underlying theories of language at play .", "Our biggest challenge as we venture to the terascale is to use our new found wealth not only to build better systems , but to im prove our understanding of language ."], "summary_lines": ["Towards Terascale Semantic Acquisition\n", "Although vast amounts of textual data are freely available, many NLP algorithms exploit only a minute percentage of it.\n", "In this paper, we study the challenges of working at the terascale.\n", "We present an algorithm, designed for the terascale, for mining is-a relations that achieves similar performance to a state-of-the-art linguistically-rich method.\n", "We focus on the accuracy of these two systems as a function of processing time and corpus size.\n", "We propose a similar, highly scalable approach, based on an edit-distance technique, to learn lexico-POS patterns, showing both good performance and efficiency.\n", "We extend is-a relation acquisition towards terascale, and automatically identified hypernym patterns by minimal edit distance.\n", "We propose, in the scenario of extracting is-a relations, one pattern-based approach and compared it with a baseline syntactic distributional similarity method (called syntactic co-occurrence in their paper).\n"]}
{"article_lines": ["On Coreference Resolution Performance Metrics", "The paper proposes a Constrained Entity Alignment F Measure CEAF for evaluatingcoreference resolution .", "The metric is com puted by aligning reference and system entities or coreference chains with the constraint that a system reference entity is aligned with at most one reference system entity .", "We show that the best alignment is a maximum bipartite matching problem which can be solved by theKuhn Munkres algorithm .", "Comparative experiments are conducted to show that the widely known MUC F measure has serious flaws in evaluating a coreference system .", "The proposed metric is also compared with the ACE Value , the official evaluation metric in the AutomaticContent Extraction ACE task , and we con clude that the proposed metric possesses someproperties such as symmetry and better inter pretability missing in the ACE Value .", "A working definition of coreference resolution is partitioning the noun phrases we are interested in into equiv alence classes , each of which refers to a physical entity . We adopt the terminologies used in the Automatic Con tent Extraction ACE task NIST , 2003a and call eachindividual phrase a mention and equivalence class an en tity .", "For example , in the following text segment , 1 ? The American Medical Association voted yesterday to install the heir apparent as its president elect , rejecting a strong , upstart challenge by a district doctor who argued that the nation ? s largest physicians ?", "group needs stronger ethics and new leadership . ? mentions are underlined , ? American Medical Associa tion ? , ? its ?", "and ? group ?", "refer to the same organization object and they form an entity .", "Similarly , ? the heir ap parent ?", "and ? president elect ?", "refer to the same person and they form another entity .", "It is worth pointing out that the entity definition here is different from what used in the Message Understanding Conference MUC task MUC , 1995 ; MUC , 1998 ? ACE entity is called coreference chain or equivalence class in MUC , and ACE mention is called entity in MUC .", "An important problem in coreference resolution is how to evaluate a system ? s performance .", "A good performance metric should have the following two properties"], "summary_lines": ["On Coreference Resolution Performance Metrics\n", "The paper proposes a Constrained Entity-Alignment F-Measure (CEAF) for evaluating coreference resolution.\n", "The metric is computed by aligning reference and system entities (or coreference chains) with the constraint that a system (reference) entity is aligned with at most one reference (system) entity.\n", "We show that the best alignment is a maximum bipartite matching problem which can be solved by the Kuhn-Munkres algorithm.\n", "Comparative experiments are conducted to show that the widely-known MUC F-measure has serious flaws in evaluating a coreference system.\n", "The proposed metric is also compared with the ACE-Value, the official evaluation metric in the Automatic Content Extraction (ACE) task, and we conclude that the proposed metric possesses some properties such as symmetry and better interpretability missing in the ACE-Value.\n", "We use a Bell tree to score and store the searching path.\n"]}
{"article_lines": ["SemEval 2007 Task 02 Evaluating Word Sense Induction and Discrimination Systems", "The goal of this task is to allow for comparison across sense induction and discrim ination systems , and also to compare thesesystems to other supervised and knowledgebased systems .", "In total there were 6 participating systems .", "We reused the SemEval 2007 English lexical sample subtask of task17 , and set up both clustering style unsuper vised evaluation using OntoNotes senses as gold standard and a supervised evaluation using the part of the dataset for mapping .", "We provide a comparison to the results ofthe systems participating in the lexical sam ple subtask of task 17 .", "Word Sense Disambiguation WSD is a key enabling technology .", "Supervised WSD techniques are the best performing in public evaluations , butneed large amounts of hand tagging data .", "Exist ing hand annotated corpora like SemCor Miller et al , 1993 , which is annotated with WordNetsenses Fellbaum , 1998 allow for a small improve ment over the simple most frequent sense heuristic , as attested in the all words track of the last Senseval competition Snyder and Palmer , 2004 .", "In the ory , larger amounts of training data SemCor hasapprox .", "500M words would improve the perfor mance of supervised WSD , but no current projectexists to provide such an expensive resource .", "An other problem of the supervised approach is that theinventory and distribution of senses changes dra matically from one domain to the other , requiring additional hand tagging of corpora Mart ? ? nez and Agirre , 2000 ; Koeling et al , 2005 .", "Supervised WSD is based on the ? fixed list of senses ?", "paradigm , where the senses for a target wordare a closed list coming from a dictionary or lex icon .", "Lexicographers and semanticists have long warned about the problems of such an approach , where senses are listed separately as discrete entities , and have argued in favor of more complex rep resentations , where , for instance , senses are dense regions in a continuum Cruse , 2000 . Unsupervised Word Sense Induction and Discrimination WSID , also known as corpus based unsupervised systems has followed this line of think ing , and tries to induce word senses directly fromthe corpus .", "Typical WSID systems involve clustering techniques , which group together similar examples .", "Given a set of induced clusters which repre sent word uses or senses1 , each new occurrence of the target word will be compared to the clusters and the most similar cluster will be selected as its sense .", "One of the problems of unsupervised systems isthat of managing to do a fair evaluation .", "Most of cur rent unsupervised systems are evaluated in house , with a brief comparison to a re implementation of aformer system , leading to a proliferation of unsuper vised systems with little ground to compare amongthem .", "The goal of this task is to allow for comparison across sense induction and discrimination systems , and also to compare these systems to other su pervised and knowledge based systems .", "The paper is organized as follows .", "Section 2 presents the evaluation framework used in this task .", "Section 3 presents the systems that participated in 1WSID approaches prefer the term ? word uses ?", "to ? word senses ? .", "In this paper we use them interchangeably to refer to both the induced clusters , and to the word senses from some reference lexicon .", "7 the task , and the official results .", "Finally , Section 5 draws the conclusions .", "All WSID algorithms need some addition in orderto be evaluated .", "One alternative is to manually de cide the correctness of the clusters assigned to each occurrence of the words .", "This approach has twomain disadvantages .", "First , it is expensive to manually verify each occurrence of the word , and dif ferent runs of the algorithm need to be evaluatedin turn .", "Second , it is not an easy task to manu ally decide if an occurrence of a word effectively corresponds with the use of the word the assignedcluster refers to , especially considering that the person is given a short list of words linked to the clus ter .", "We also think that instead of judging whether the cluster returned by the algorithm is correct , theperson should have independently tagged the occur rence with his own senses , which should have been then compared to the cluster returned by the system .", "This is paramount to compare a corpus which has been hand tagged with some reference senses alsoknown as the gold standard with the clustering result .", "The gold standard tags are taken to be the def inition of the classes , and standard measures from the clustering literature can be used to evaluate the clusters against the classes .", "A second alternative would be to devise a method to map the clusters returned by the systems to thesenses in a lexicon .", "Pantel and Lin 2002 automatically map the senses to WordNet , and then mea sure the quality of the mapping .", "More recently , themapping has been used to test the system on publicly available benchmarks Purandare and Peder sen , 2004 ; Niu et al , 2005 . A third alternative is to evaluate the systems ac cording to some performance in an application , e . g . information retrieval Schu ? tze , 1998 .", "This is a veryattractive idea , but requires expensive system devel opment and it is sometimes difficult to separate the reasons for the good or bad performance . In this task we decided to adopt the first two alternatives , since they allow for comparison over pub licly available systems of any kind .", "With this goal onmind we gave all the participants an unlabeled cor pus , and asked them to induce the senses and create a clustering solution on it .", "We evaluate the results according to the following types of evaluation 1 .", "Evaluate the induced senses as clusters of ex .", "amples .", "The induced clusters are compared to the sets of examples tagged with the given gold standard word senses classes , and evaluated using the FScore measure for clusters .", "We will call this evaluation unsupervised .", "Map the induced senses to gold standard .", "senses , and use the mapping to tag the test cor pus with gold standard tags .", "The mapping is automatically produced by the organizers , and the resulting results evaluated according to theusual precision and recall measures for super vised word sense disambiguation systems .", "We call this evaluation supervised .", "We will see each of them in turn .", "2 . 1 Unsupervised evaluation .", "In this setting the results of the systems are treated as clusters of examples and gold standard senses are classes .", "In order to compare the clusters with the classes , hand annotated corpora is needed .", "The testset is first tagged with the induced senses .", "A per fect clustering solution will be the one where each cluster has exactly the same examples as one of the classes , and vice versa . Following standard cluster evaluation practice Zhao and Karypis , 2005 , we consider the FS core measure for measuring the performance of the systems .", "The FScore is used in a similar fashion to Information Retrieval exercises , with precisionand recall defined as the percentage of correctly ? retrieved ?", "examples for a cluster divided by total clus ter size , and recall as the percentage of correctly ? retrieved ?", "examples for a cluster divided by total class size .", "Given a particular class sr of size nr and a cluster hi of size ni , suppose nir examples in the class sr belong to hi .", "The F value of this class and cluster is defined to be f sr , hi 2P sr , hi R sr , hi P sr , hi R sr , hi where P sr , hi nir nr is the precision value and R sr , hi nir ni is the recall value defined for classsr and cluster hi .", "The FScore of class sr is the max imum F value attained at any cluster , that is , 8 F sr max hi f sr , hi and the FScore of the entire clustering solution is FScore c ?", "r 1 nr n F sr where q is the number of classes and n is the size of the clustering solution .", "If the clustering is theidentical to the original classes in the datasets , FS core will be equal to one which means that the higher the FScore , the better the clustering is . For the sake of completeness we also include thestandard entropy and purity measures in the unsupervised evaluation .", "The entropy measure consid ers how the various classes of objects are distributedwithin each cluster .", "In general , the smaller the entropy value , the better the clustering algorithm per forms .", "The purity measure considers the extent to which each cluster contained objects from primarilyone class .", "The larger the values of purity , the bet ter the clustering algorithm performs .", "For a formal definition refer to Zhao and Karypis , 2005 .", "2 . 2 Supervised evaluation .", "We have followed the supervised evaluation frame work for evaluating WSID systems as described in Agirre et al , 2006 .", "First , we split the corpus intoa train test part .", "Using the hand annotated sense in formation in the train part , we compute a mappingmatrix M that relates clusters and senses in the fol lowing way .", "Suppose there are m clusters and n senses for the target word .", "Then , M mij 1 ? i ? m , 1 ? j ? n , and each mij P sj hi , that is , mij is the probability of a word having sense jgiven that it has been assigned cluster i . This probability can be computed counting the times an occur rence with sense sj has been assigned cluster hi in the train corpus .", "The mapping matrix is used to transform any cluster score vector h ?", "h1 , . . .", ", hm returned by the WSID algorithm into a sense score vector s ?", "s1 , . . .", ", sn .", "It suffices to multiply the score vector by M , i . e . , s ?", "h ? M . We use the M mapping matrix in order to convert the cluster score vector of each test corpus instance into a sense score vector , and assign the sense with All Nouns Verbs train 22281 14746 9773 test 4851 2903 2427 all 27132 17649 12200 Table 1 Number of occurrences for the 100 target words in the corpus following the train test split . maximum score to that instance .", "Finally , the result ing test corpus is evaluated according to the usual precision and recall measures for supervised word sense disambiguation systems .", "In this section we will introduce the gold standard and corpus used , the description of the systems andthe results obtained .", "Finally we provide some mate rial for discussion .", "Gold StandardThe data used for the actual evaluation was bor rowed from the SemEval 2007 ? English lexical sample subtask ?", "of task 17 .", "The texts come from the Wall Street Journal corpus , and were hand annotated with OntoNotes senses Hovy et al , 2006 .", "Note that OntoNotes senses are coarser than WordNet senses , and thus the number of senses to be induced is smaller in this case .", "Participants were provided with information about 100 target words 65 verbs and 35 nouns , each target word having a set of contexts where the word appears .", "After removing the sense tags from the train corpus , the train and test parts were joined into the official corpus and given to the participants .", "Participants had to tag with the induced senses all the examples in this corpus .", "Table 1 summarizes the size of the corpus .", "Participant systems In total there were 6 participant systems .", "One of them UoFL was not a sense induction system , but rather a knowledge based WSD system .", "We include their data in the results section below for coherence with the official results submitted to participants , but we will not mention it here .", "I2R This team used a cluster validation method to estimate the number of senses of a target word in untagged data , and then grouped the instances of thistarget word into the estimated number of clusters us ing the sequential Information Bottleneck algorithm .", "9 UBC AS A two stage graph based clustering where a co occurrence graph is used to compute similarities against contexts .", "The context similarity matrix is pruned and the resulting associated graphis clustered by means of a random walk type al gorithm .", "The parameters of the system are tuned against the Senseval 3 lexical sample dataset , and some manual tuning is performed in order to reduce the overall number of induced senses .", "Note that thissystem was submitted by the organizers .", "The orga nizers took great care in order to participate under the same conditions as the rest of participants . UMND2 A system which clusters the second or der co occurrence vectors associated with each word in a context .", "Clustering is done using k means and the number of clusters was automatically discoveredusing the Adapted Gap Statistic .", "No parameter tun ing is performed .", "upv si A self term expansion method based onco ocurrence , where the terms of the corpus are ex panded by its best co ocurrence terms in the samecorpus .", "The clustering is done using one implemen tation of the KStar method where the stop criterionhas been modified .", "The trial data was used for de termining the corpus structure .", "No further tuning is performed . UOY A graph based system which creates a co occurrence hypergraph model .", "The hypergraph isfiltered and weighted according to some associa tion rules .", "The clustering is performed by selecting the nodes of higher degree until a stop criterion isreached .", "WSD is performed by assigning to each in duced cluster a score equal to the sum of weights of hyperedges found in the local context of the target word .", "The system was tested and tuned on 10 nouns of Senseval 3 lexical sample .", "Official Results Participants were required to induce the senses of the target words and cluster all target word contextsaccordingly2 .", "Table 2 summarizes the average num ber of induced senses as well as the real senses in the gold standard .", "2They were allowed to label each context with a weighted score vector , assigning a weight to each induced sense .", "In the unsupervised evaluation only the sense with maximum weightwas considered , but for the supervised one the whole score vector was used .", "However , none of the participating systems la beled any instance with more than one sense .", "system All nouns verbs I2R 3 . 08 3 . 11 3 . 06 UBC AS ?", "1 . 32 1 . 63 1 . 15 UMND2 1 . 36 1 . 71 1 . 17 upv si 5 . 57 7 . 2 4 . 69 UOY 9 . 28 11 . 28 8 . 2 Gold standard test 2 . 87 2 . 86 2 . 86 train 3 . 6 3 . 91 3 . 43 all 3 . 68 3 . 94 3 . 54Table 2 Average number of clusters as returned by the par ticipants , and number of classes in the gold standard .", "Note that UBC AS ?", "is the system submitted by the organizers of the task .", "System R . All Nouns Verbs FSc .", "Pur .", "Entr .", "FSc .", "FSc .", "1c1word 1 78 . 9 79 . 8 45 . 4 80 . 7 76 . 8 UBC AS ?", "2 78 . 7 80 . 5 43 . 8 80 . 8 76 . 3 upv si 3 66 . 3 83 . 8 33 . 2 69 . 9 62 . 2 UMND2 4 66 . 1 81 . 7 40 . 5 67 . 1 65 . 0 I2R 5 63 . 9 84 . 0 32 . 8 68 . 0 59 . 3 UofL ? ?", "6 61 . 5 82 . 2 37 . 8 62 . 3 60 . 5 UOY 7 56 . 1 86 . 1 27 . 1 65 . 8 45 . 1 Random 8 37 . 9 86 . 1 27 . 7 38 . 1 37 . 7 1c1inst 9 9 . 5 100 0 6 . 6 12 . 7 Table 3 Unsupervised evaluation on the test corpus FScore , including 3 baselines .", "Purity and entropy are also provided .", "UBC AS ?", "was submitted by the organizers .", "UofL ? ?", "is not a sense induction system .", "System Rank Supervised evaluation All Nouns Verbs I2R 1 81 . 6 86 . 8 75 . 7 UMND2 2 80 . 6 84 . 5 76 . 2 upv si 3 79 . 1 82 . 5 75 . 3 MFS 4 78 . 7 80 . 9 76 . 2 UBC AS ?", "5 78 . 5 80 . 7 76 . 0 UOY 6 77 . 7 81 . 6 73 . 3 UofL ? ?", "7 77 . 1 80 . 5 73 . 3Table 4 Supervised evaluation as recall .", "UBC AS ?", "was submitted by the organizers .", "UofL ? ?", "is not a sense induction sys tem .", "Table 3 shows the unsupervised evaluation of the systems on the test corpus .", "We also include three baselines the ? one cluster per word ?", "baseline 1c1word , which groups all instances of a word intoa single cluster , the ? one cluster per instance ?", "baseline 1c1inst , where each instance is a distinct clus ter , and a random baseline , where the induced wordsenses and their associated weights have been ran domly produced .", "The random baseline figures in this paper are averages over 10 runs .", "As shown in Table 3 , no system outperforms the 1c1word baseline , which indicates that this baseline 10is quite strong , perhaps due the relatively small num ber of classes in the gold standard .", "However , all systems outperform by far the random and 1c1instbaselines , meaning that the systems are able to in duce correct senses .", "Note that the purity and entropy measures are not very indicative in this setting .", "For completeness , we also computed the FScore usingthe complete corpus both train and test .", "The re sults are similar and the ranking is the same .", "We omit them for brevity .", "The results of the supervised evaluation can be seen in Table 4 .", "The evaluation is also performed over the test corpus .", "Apart from participants , we also show the most frequent sense MFS , which tags every test instance with the sense that occurredmost often in the training part .", "Note that the su pervised evaluation combines the information in theclustering solution implicitly with the MFS information via the mapping in the training part .", "Pre vious Senseval evaluation exercises have shown thatthe MFS baseline is very hard to beat by unsuper vised systems .", "In fact , only three of the participant systems are above the MFS baseline , which showsthat the clustering information carries over the map ping successfully for these systems .", "Note that the1c1word baseline is equivalent to MFS in this setting .", "We will review the random baseline in the dis cussion section below .", "Further Results Table 5 shows the results of the best systems from the lexical sample subtask of task 17 .", "The best sense induction system is only 6 . 9 percentage points belowthe best supervised , and 3 . 5 percentage points be low the best and only semi supervised system .", "If the sense induction system had participated , it would be deemed as semi supervised , as it uses , albeit in ashallow way , the training data for mapping the clusters into senses .", "In this sense , our supervised evalu ation does not seek to optimize the available training data . After the official evaluation , we realized that con trary to previous lexical sample evaluation exercises task 17 organizers did not follow a random train test split .", "We decided to produce a random train testsplit following the same 82 18 proportion as the of ficial split , and re evaluated the systems .", "The results are presented in Table 6 , where we can see that all System Supervised evaluation best supervised 88 . 7 best semi supervised 85 . 1 best induction semi sup .", "81 . 6 MFS 78 . 7 best unsupervised 53 . 8 Table 5 Comparing the best induction system in this task with those of task 17 .", "System Supervised evaluation I2R 82 . 2 UOY 81 . 3 UMND2 80 . 1 upv si 79 . 9 UBC AS 79 . 0 MFS 78 . 4 Table 6 Supervised evaluation as recall using a random train test split .", "participants are above the MFS baseline , showingthat all of them learned useful clustering informa tion .", "Note that UOY was specially affected by the original split .", "The distribution of senses in this split did not vary cf .", "Table 2 . Finally , we also studied the supervised evalua tion of several random clustering algorithms , which can attain performances close to MFS , thanks to the mapping information .", "This is due to the fact that therandom clusters would be mapped to the most fre quent senses .", "Table 7 shows the results of random solutions using varying numbers of clusters e . g . random2 is a random choice between two clusters .", "Random2 is only 0 . 1 below MFS , but as the number of clusters increases some clusters don ? t get mapped , and the recall of the random baselines decrease .", "The evaluation of clustering solutions is not straightforward .", "All measures have some bias towards cer tain clustering strategy , and this is one of the reasonsof adding the supervised evaluation as a complemen tary information to the more standard unsupervised evaluation . In our case , we noticed that the FScore penal ized the systems with a high number of clusters , and favored those that induce less senses .", "Given the fact that FScore tries to balance precision higher for large numbers of clusters and recall higher for small numbers of clusters , this was not expected .", "We were also surprised to see that no system could 11 System Supervised evaluation random2 78 . 6 random10 77 . 6 ramdom100 64 . 2 random1000 31 . 8 Table 7 Supervised evaluation of several random baselines . beat the ? one cluster one word ?", "baseline .", "An expla nation might lay in that the gold standard was based on the coarse grained OntoNotes senses .", "We also noticed that some words had hundreds of instancesand only a single sense .", "We suspect that the partic ipating systems would have beaten all baselines if a fine grained sense inventory like WordNet had been used , as was customary in previous WSD evaluation exercises .", "Supervised evaluation seems to be more neutral regarding the number of clusters , as the ranking of systems according to this measure include diverse cluster averages .", "Each of the induced clusters is mapped into a weighted vector of senses , and thus inducing a number of clusters similar to the number of senses is not a requirement for good results .", "With this measure some of the systems3 are able to beat all baselines .", "We have presented the design and results of theSemEval 2007 task 02 on evaluating word sense induction and discrimination systems .", "6 systems participated , but one of them was not a sense induction system .", "We reused the data from the SemEval 2007 English lexical sample subtask of task 17 , and .", "set up both clustering style unsupervised evaluation using OntoNotes senses as gold standard and a su pervised evaluation using the training part of thedataset for mapping .", "We also provide a compari son to the results of the systems participating in the lexical sample subtask of task 17 . Evaluating clustering solutions is not straightfor ward .", "The unsupervised evaluation seems to besensitive to the number of senses in the gold stan dard , and the coarse grained sense inventory usedin the gold standard had a great impact in the results .", "The supervised evaluation introduces a mapping step which interacts with the clustering solu tion .", "In fact , the ranking of the participating systems 3All systems in the case of a random train test split varies according to the evaluation method used .", "We think the two evaluation results should be taken to be complementary regarding the information learned by the clustering systems , and that the evaluation of word sense induction and discrimination systemsneeds further developments , perhaps linked to a cer tain application or purpose .", "Acknowledgments We want too thank the organizers of SemEval 2007 task 17 for kindly letting us use their corpus .", "We are also grateful to Ted Pedersen for his comments on the evaluation results .", "This work has been partially funded by the Spanish education ministry project KNOW and by the regional government of Gipuzkoa project DAHAD ."], "summary_lines": ["SemEval-2007 Task 02: Evaluating Word Sense Induction and Discrimination Systems\n", "The goal of this task is to allow for comparison across sense-induction and discrimination systems, and also to compare these systems to other supervised and knowledge-based systems.\n", "In total there were 6 participating systems.\n", "We reused the SemEval-2007 English lexical sample subtask of task 17, and set up both clustering-style unsupervised evaluation (using Onto Notes senses as gold-standard) and a supervised evaluation (using the part of the dataset for mapping).\n", "We provide a comparison to the results of the systems participating in the lexical sample subtask of task 17.\n", "The object of the sense induction task of SENSEVAL-4 is to cluster 27,132 instances of 100 different words (35 nouns and 65 verbs) into senses or classes.\n", "Graph-based methods have been employed for word sense induction.\n"]}
{"article_lines": ["Named Entity Transliteration With Comparable Corpora", "In this paper we investigate Chinesename transliteration using compacorpora where texts in the two languages deal in some of the same topics and therefore share references to named entities but are not translations of each other .", "We present two distinct methods for transliteration , one approach using phonetic transliteration , and the second using the temporal distribution of candidate pairs .", "Each of these approaches works quite well , but by combining the approaches one can achieve even better results .", "We then propose a novel score propagation method that utilizes the co occurrence of transliteration pairs within document pairs .", "This propagation method achieves further improvement over the best results from the previous step .", "As part of a more general project on multilingual named entity identification , we are interested in the problem of name transliteration across languages that use different scripts .", "One particular issue is the discovery of named entities in comparable texts in multiple languages , where by comparable we mean texts that are about the same topic , but are not in general translations of each other .", "For example , if one were to go through an English , Chinese and Arabic newspaper on the same day , it is likely that the more important international events in various topics such as politics , business , science and sports , would each be covered in each of the newspapers .", "Names of the same persons , locations and so forth which are often transliterated rather than translated would be found in comparable stories across the three papers . 1 We wish to use this expectation to leverage transliteration , and thus the identification of named entities across languages .", "Our idea is that the occurrence of a cluster of names in , say , an English text , should be useful if we find a cluster of what looks like the same names in a Chinese or Arabic text .", "An example of what we are referring to can be found in Figure 1 .", "These are fragments of two stories from the June 8 , 2001 Xinhua English and Chinese newswires , each covering an international women s badminton championship .", "Though these two stories are from the same newswire source , and cover the same event , they are not translations of each other .", "Still , not surprisingly , a lot of the names that occur in one , also occur in the other .", "Thus Camilla Martin shows up in the Chinese version asAi'1 ma er ting ; Judith Meulendijks is T V fL' A A JW yu mo lun di ke si ; and Mette Sorensen is fL' V mai su lun sen . Several other correspondences also occur .", "While some of the transliterations are standard thus Martin is conventionally transliterated asAi'1 ma erting many of them were clearly more novel , though all of them follow the standard Chinese conventions for transliterating foreign names .", "These sample documents illustrate an important point if a document in language L1 has a set of names , and one finds a document in L2 containing a set of names that look as if they could be transliterations of the names in the L1 document , then this should boost one s confidence that the two sets of names are indeed transliterations of each other .", "We will demonstrate that this intuition is correct .", "In previous work on Chinese named entity transliteration e . g .", "Meng et al . , 2001 ; Gao et al . , 2004 , the problem has been cast as the problem of producing , for a given Chinese name , an English equivalent such as one might need in a machine translation system .", "For example , for the nameI A wei wei lian mu si , one would like to arrive at the English name V enus Williams .", "Common approaches include sourcechannel methods , following Knight and Graehl , 1998 or maximum entropy models .", "Comparable corpora have been studied extensively in the literature e . g . , Fung , 1995 ; Rapp , 1995 ; Tanaka and Iwasaki , 1996 ; Franz et al . , 1998 ; Ballesteros and Croft , 1998 ; Masuichi et al . , 2000 ; Sadat et al . , 2003 , but transliteration in the context of comparable corpora has not been well addressed .", "The general idea of exploiting frequency correlations to acquire word translations from comparable corpora has been explored in several previous studies e . g . , Fung , 1995 ; Rapp , 1995 ; Tanaka and Iwasaki , 1996 . Recently , a method based on Pearson correlation was proposed to mine word pairs from comparable corpora Tao and Zhai , 2005 , an idea similar to the method used in Kay and Roscheisen , 1993 for sentence alignment .", "In our work , we adopt the method proposed in Tao and Zhai , 2005 and apply it to the problem of transliteration .", "We also study several variations of the similarity measures .", "Mining transliterations from multilingual web pages was studied in Zhang and Vines , 2004 ; Our work differs from this work in that we use comparable corpora in particular , news data and leverage the time correlation information naturally available in comparable corpora .", "We assume that we have comparable corpora , consisting of newspaper articles in English and Chinese from the same day , or almost the same day .", "In our experiments we use data from the English and Chinese stories from the Xinhua News agency for about 6 months of 2001 . 2 We assume that we have identified names for persons and locations two types that have a strong tendency to be transliterated wholly or mostly phonetically in the English text ; in this work we use the named entity recognizer described in Li et al . , 2004 , which is based on the SNoW machine learning toolkit Carlson et al . , 1999 .", "To perform the transliteration task , we propose the following general three step approach The intuition behind the third step is the following .", "Suppose several high confidence name transliteration pairs occur in a pair of English and Chinese documents .", "Intuitively , this would increase our confidence in the other plausible transliteration pairs in the same document pair .", "We thus propose a score propagation method to allow these high confidence pairs to propagate some of their scores to other co occurring transliteration pairs .", "As we will show later , such a propagation strategy can generally further improve the transliteration accuracy ; in particular , it can further improve the already high performance from combining the two scoring methods .", "The English named entity candidate selection process was already described above .", "Candidate Chinese transliterations are generated by consulting a list of characters that are frequently used for transliterating foreign names .", "As discussed elsewhere Sproat et al . , 1996 , a subset of a few hundred characters out of several thousand tends to be used overwhelmingly for transliterating foreign names into Chinese .", "We use a list of 495 such characters , derived from various online dictionaries .", "A sequence of three or more characters from the list is taken as a possible name .", "If the character occurs , which is frequently used to represent the space between parts of an English name , then at least one character to the left and right of this character will be collected , even if the character in question is not in the list of foreign characters .", "Armed with the English and Chinese candidate lists , we then consider the pairing of every English candidate with every Chinese candidate .", "Obviously it would be impractical to do this for all of the candidates generated for , say , an entire year we consider as plausible pairings those candidates that occur within a day of each other in the two corpora .", "We adopt a source channel model for scoring English Chinese transliteration pairs .", "In general , we seek to estimate P elc , where e is a word in Roman script , and c is a word in Chinese script .", "Since Chinese transliteration is mostly based on pronunciation , we estimate P e c , where e is the pronunciation of e and c is the pronunciation of c . Again following standard practice , we decompose the estimate of P e c as P e c _ Ili P e i c i .", "Here , e i is the ith subsequence of the English phone string , and c i is the ith subsequence of the Chinese phone string .", "Since Chinese transliteration attempts to match the syllablesized characters to equivalent sounding spans of the English language , we fix the c i to be syllables , and let the e i range over all possible subsequences of the English phone string .", "For training data we have a small list of 721 names in Roman script and their Chinese equivalent . 3 Pronunciations for English words are obtained using the Festival text tospeech system Taylor et al . , 1998 ; for Chinese , we use the standard pinyin transliteration of the characters .", "English Chinese pairs in our training dictionary were aligned using the alignment algorithm from Kruskal , 1999 , and a hand derived set of 21 rules of thumb for example , we have rules that encode the fact that Chinese l can correspond to English r , n or er ; and that Chinese w may be used to represent v .", "Given that there are over 400 syllables in Mandarin not counting tone and each of these syllables can match a large number of potential English phone spans , this is clearly not enough training data to cover all the parameters , and so we use Good Turing estimation to estimate probabilities for unseen correspondences .", "Since we would like to filter implausible transliteration pairs we are less lenient than standard estimation techniques in that we are willing to assign zero probability to some correspondences .", "Thus we set a hard rule that for an English phone span to correspond to a Chinese syllable , the initial phone of the English span must have been seen in the training data as corresponding to the initial of the Chinese syllable some minimum number of times .", "For consonant initial syllables we set the minimum to 4 .", "We omit further details of our estimation technique for lack of space .", "This phonetic correspondence model can then be used to score putative transliteration pairs .", "Names of the same entity that occur in different languages often have correlated frequency patterns due to common triggers such as a major event .", "Thus if we have comparable news articles over a sufficiently long time period , it is possible to exploit such correlations to learn the associations of names in different languages .", "The idea of exploiting frequency correlation has been well studied .", "See the previous work section .", "We adopt the method proposed in Tao and Zhai , 2005 , which works as follows We pool all documents in a single day to form a large pseudo document .", "Then , for each transliteration candidate both Chinese and English , we compute its frequency in each of those pseudo documents and obtain a raw frequency vector .", "We further normalize the raw frequency vector so that it becomes a frequency distribution over all the time points days .", "In order to compute the similarity between two distribution vectors , The Pearson correlation coefficient was used in Tao and Zhai , 2005 ; here we also considered two other commonly used measures cosine Salton and McGill , 1983 , and Jensen Shannon divergence Lin , 1991 , though our results show that Pearson correlation coefficient performs better than these two other methods .", "In both scoring methods described above , scoring of each candidate transliteration pair is independent of the other .", "As we have noted , document pairs that contain lots of plausible transliteration pairs should be viewed as more plausible document pairs ; at the same time , in such a situation we should also trust the putative transliteration pairs more .", "Thus these document pairs and transliteration pairs mutually reinforce each other , and this can be exploited to further optimize our transliteration scores by allowing transliteration pairs to propagate their scores to each other according to their co occurrence strengths .", "Formally , suppose the current generation of transliteration scores are ei , ci , wi i 1 , . . . , n , where ei , ci is a distinct pair of English and Chinese names .", "Note that although for any i 6 j , we have ei , ci 6 ej , cj , it is possible that ei ej or ci cj for some i 6 j . wi is the transliteration score of ei , ci .", "These pairs along with their co occurrence relation computed based on our comparable corpora can be formally represented by a graph as shown in Figure 2 .", "In such a graph , a node represents ei , ci , wi .", "An edge between ei , ci , wi and ej , cj , wj is constructed iff ei , ci and ej , cj co occur in a certain document pair Et , Ct , i . e . there exists a document pair Et , Ct , such that ei , ej Et and ci , cj Ct .", "Given a node ei , ci , wi , we refer to all its directly connected nodes as its neighbors .", "The documents do not appear explicitly in the graph , but they implicitly affect the graph s topology and the weight of each edge .", "Our idea of score propagation can now be formulated as the following recursive equation for updating the scores of all the transliteration pairs . where w k i is the new score of the pair ei , ci after an iteration , while w k 1 is its old score i before updating ; \u03b1 0 , 1 is a parameter to control the overall amount of propagation when \u03b1 1 , no propagation occurs ; P j i is the conditional probability of propagating a score from node ej , cj , wj to node ei , ci , wi .", "We estimate P j i in two different ways 1 The number of cooccurrences in the whole collection Denote as CO .", "P j i C i , j where MI i , j is the mutual information of ei , ci and ej , cj .", "As we will show , the CO method works better .", "Note that the transition probabilities between indirect neighbors are always 0 .", "Thus propagation only happens between direct neighbors .", "This formulation is very similar to PageRank , a link based ranking algorithm for Web retrieval Brin and Page , 1998 .", "However , our motivation is propagating scores to exploit cooccurrences , so we do not necessarily want the equation to converge .", "Indeed , our results show that although the initial iterations always help improve accuracy , too many iterations actually would decrease the performance .", "We use a comparable English Chinese corpus to evaluate our methods for Chinese transliteration .", "We take one day s worth of comparable news articles 234 Chinese stories and 322 English stories , generate about 600 English names with the entity recognizer Li et al . , 2004 as described above , and find potential Chinese transliterations also as previously described .", "We generated 627 Chinese candidates .", "In principle , all these 600 x 627 pairs are potential transliterations .", "We then apply the phonetic and time correlation methods to score and rank all the candidate Chinese English correspondences .", "To evaluate the proposed transliteration methods quantitatively , we measure the accuracy of the ranked list by Mean Reciprocal Rank MRR , a measure commonly used in information retrieval when there is precisely one correct answer Kantor and Voorhees , 2000 .", "The reciprocal rank is the reciprocal of the rank of the correct answer .", "For example , if the correct answer is ranked as the first , the reciprocal rank would be 1 . 0 , whereas if it is ranked the second , it would be 0 . 5 , and so forth .", "To evaluate the results for a set of English names , we take the mean of the reciprocal rank of each English name .", "We attempted to create a complete set of answers for all the English names in our test set , but a small number of English names do not seem to have any standard transliteration according to the resources that we consulted .", "We ended up with a list of about 490 out of the 600 English names judged .", "We further notice that some answers about 20 are not in our Chinese candidate set .", "This could be due to two reasons 1 The answer does not occur in the Chinese news articles we look at .", "2 The answer is there , but our candidate generation method has missed it .", "In order to see more clearly how accurate each method is for ranking the candidates , we also compute the MRR for the subset of English names whose transliteration answers are in our candidate list .", "We distinguish the MRRs computed on these two sets of English names as AllMRR and CoreMRR .", "Below we first discuss the results of each of the two methods .", "We then compare the two methods and discuss results from combining the two methods .", "We show sample results for the phonetic scoring method in Table 1 .", "This table shows the 10 highest scoring transliterations for each Chinese character sequence based on all texts in the Chinese and English Xinhua newswire for the 13th of August , 2001 .", "8 out of these 10 are correct .", "For all the English names the MRR is 0 . 3 , and for the hua corpus for 8 13 01 .", "The final column is the log P estimate for the transliteration .", "Starred entries are incorrect . core names it is 0 . 89 .", "Thus on average , the correct answer , if it is included in our candidate list , is ranked mostly as the first one .", "We proposed three similarity measures for the frequency correlation method , i . e . , the Cosine , Pearson coefficient , and Jensen Shannon divergence .", "In Table 2 , we show their MRRs .", "Given that the only resource the method needs is comparable text documents over a sufficiently long period , these results are quite encouraging .", "For example , with Pearson correlation , when the Chinese transliteration of an English name is included in our candidate list , the correct answer is , on average , ranked at the 3rd place or better .", "The results thus show that the idea of exploiting frequency correlation does work .", "We also see that among the three similarity measures , Pearson correlation performs the best ; it performs better than Cosine , which is better than JS divergence .", "Compared with the phonetic correspondence method , the performance of the frequency correlation method is in general much worse , which is not surprising , given the fact that terms may be correlated merely because they are topically related .", "Since the two methods exploit complementary resources , it is natural to see if we can improve performance by combining the two methods .", "Indeed , intuitively the best candidate is the one that has a good pronunciation alignment as well as a correlated frequency distribution with the English name .", "We evaluated two strategies for combining the two methods .", "The first strategy is to use the phonetic model to filter out clearly impossible candidates and then use the frequency correlation method to rank the candidates .", "The second is to combine the scores of these two methods .", "Since the correlation coefficient has a maximum value of 1 , we normalize the phonetic correspondence score by dividing all scores by the maximum score so that the maximum normalized value is also 1 .", "We then take the average of the two scores and rank the candidates based on their average scores .", "Note that the second strategy implies the application of the first strategy .", "The results of these two combination strategies are shown in Table 3 along with the results of the two individual methods .", "We see that both combination strategies are effective and the MRRs of the combined results are all better than those of the two individual methods .", "It is interesting to see that the benefit of applying the phonetic correspondence model as a filter is quite significant .", "Indeed , although the performance of the frequency correlation method alone is much worse than that of the phonetic correspondence method , when working on the subset of candidates passing the phonetic filter i . e . , those candidates that have a reasonable phonetic alignment with the English name , it can outperform the phonetic correspondence method .", "This once again indicates that exploiting the frequency correlation can be effective .", "When combining the scores of these two methods , we not only implicitly apply the phonetic filter , but also exploit the discriminative power provided by the phonetic correspondence scores and this is shown to bring in additional benefit , giving the best performance among all the methods .", "From the results above , we see that the MRRs for the core English names are substantially higher than those for all the English names .", "This means that our methods perform very well whenever we have the answer in our candidate list , but we have also missed the answers for many English names .", "The missing of an answer in the candidate list is thus a major source of errors .", "To further understand the upper bound of our method , we manually add the missing correct answers to our candidate set and apply all the methods to rank this augmented set of candidates .", "The performance is reported in Table 4 with the corresponding performance on the original candidate set .", "We see that , as expected , the performance on the augmented candidate list , which can be interpreted as an upper bound of our method , is indeed much better , suggesting that if we can somehow improve the candidate generation method to include the answers in the list , we can expect to significantly improve the performance for all the methods .", "This is clearly an interesting topic for further research .", "The relative performance of different methods on this augmented candidate list is roughly the same as on the original candidate list , except that the Freq PhoneticFilter is slightly worse than that of the phonetic method alone , though it is still much better than the performance of the frequency correlation alone .", "One possible explanation may be that since these names do not necessarily occur in our comparable corpora , we may not have sufficient frequency observations for some of the names .", "To demonstrate that score propagation can further help transliteration , we use the combination scores in Table 3 as the initial scores , and apply our propagation algorithm to iteratively update them .", "We remove the entries when they do not co occur with others .", "There are 25 such English name candidates .", "Thus , the initial scores are actually slightly different from the values in Table 3 .", "We show the new scores and the best propagation scores in Table 5 .", "In the table , init . refers to the initial scores . and CO and MI stand for best scores obtained using either the co occurrence or mutual information method .", "While both methods result in gains , CO very slightly outperforms the MI approach .", "In the score propagation process , we introduce two additional parameters the interpolation parameter \u03b1 and the number of iterations k . Figure 3 and Figure 4 show the effects of these parameters .", "Intuitively , we want to preserve the initial score of a pair , but add a slight boost from its neighbors .", "Thus , we set \u03b1 very close to 1 0 . 9 and 0 . 95 , and allow the system to perform 20 iterations .", "In both figures , the first few iterations certainly leverage the transliteration , demonstrating that the propagation method works .", "However , we observe that the performance drops when more iterations are used , presumably due to noise introduced from more distantly connected nodes .", "Thus , a relatively conservative approach is to choose a high \u03b1 value , and run only a few iterations .", "Note , finally , that the CO method seems to be more stable than the MI method .", "In this paper we have discussed the problem of Chinese English name transliteration as one component of a system to find matching names in comparable corpora .", "We have proposed two methods for transliteration , one that is more traditional and based on phonetic correspondences , and one that is based on word distributions and adopts methods from information retrieval .", "We have shown that both methods yield good results , and that even better results can be achieved by combining the methods .", "We have further showed that one can improve upon the combined model by using reinforcement via score propagation when transliteration pairs cluster together in document pairs .", "The work we report is ongoing .", "We are investigating transliterations among several language pairs , and are extending these methods to Korean , Arabic , Russian and Hindi see Tao et al . , 2006 .", "This work was funded by Dept . of the Interior contract NBCHC040176 REFLEX .", "We also thank three anonymous reviewers for ACL06 ."], "summary_lines": ["Named Entity Transliteration With Comparable Corpora\n", "In this paper we investigate Chinese-English name transliteration using comparable corpora, corpora where texts in the two languages deal in some of the same topics - and therefore share references to named entities - but are not translations of each other.\n", "We present two distinct methods for transliteration, one approach using phonetic transliteration, and the second using the temporal distribution of candidate pairs.\n", "Each of these approaches works quite well, but by combining the approaches one can achieve even better results.\n", "We then propose a novel score propagation method that utilizes the co-occurrence of transliteration pairs within document pairs.\n", "This propagation method achieves further improvement over the best results from the previous step.\n", "We compare names from comparable and contemporaneous English and Chinese texts, scoring matches by training a learning algorithm to compare the phonemic representations of the names in the pair, in addition to taking into account the frequency distribution of the pair over time.\n"]}
{"article_lines": ["A Study on Similarity and Relatedness Using Distributional and WordNet based Approaches", "This paper presents and compares WordNetbased and distributional similarity approaches .", "The strengths and weaknesses of each approach regarding similarity and relatedness tasks are discussed , and a combination is presented .", "Each of our methods independently provide the best results in their class on the RG and WordSim353 datasets , and a supervised combination of them yields the best published results on all datasets .", "Finally , we pioneer cross lingual similarity , showing that our methods are easily adapted for a cross lingual task with minor losses .", "Measuring semantic similarity and relatedness between terms is an important problem in lexical semantics .", "It has applications in many natural language processing tasks , such as Textual Entailment , Word Sense Disambiguation or Information Extraction , and other related areas like Information Retrieval .", "The techniques used to solve this problem can be roughly classified into two main categories those relying on pre existing knowledge resources thesauri , semantic networks , taxonomies or encyclopedias Alvarez and Lim , 2007 ; Yang and Powers , 2005 ; Hughes and Ramage , 2007 and those inducing distributional properties of words from corpora Sahami and Heilman , 2006 ; Chen et al . , 2006 ; Bollegala et al . , 2007 .", "In this paper , we explore both families .", "For the first one we apply graph based algorithms to WordNet , and for the second we induce distributional similarities collected from a 1 . 6 Terabyte Web corpus .", "Previous work suggests that distributional similarities suffer from certain limitations , which make them less useful than knowledge resources for semantic similarity .", "For example , Lin 1998b finds similar phrases like captive westerner which made sense only in the context of the corpus used , and Budanitsky and Hirst 2006 highlight other problems that stem from the imbalance and sparseness of the corpora .", "Comparatively , the experiments in this paper demonstrate that distributional similarities can perform as well as the knowledge based approaches , and a combination of the two can exceed the performance of results previously reported on the same datasets .", "An application to cross lingual CL similarity identification is also described , with applications such as CL Information Retrieval or CL sponsored search .", "A discussion on the differences between learning similarity and relatedness scores is provided .", "The paper is structured as follows .", "We first present the WordNet based method , followed by the distributional methods .", "Section 4 is devoted to the evaluation and results on the monolingual and crosslingual tasks .", "Section 5 presents some analysis , including learning curves for distributional methods , the use of distributional similarity to improve WordNet similarity , the contrast between similarity and relatedness , and the combination of methods .", "Section 6 presents related work , and finally , Section 7 draws the conclusions and mentions future work .", "WordNet Fellbaum , 1998 is a lexical database of English , which groups nouns , verbs , adjectives and adverbs into sets of synonyms synsets , each expressing a distinct concept .", "Synsets are interlinked with conceptual semantic and lexical relations , including hypernymy , meronymy , causality , etc .", "Given a pair of words and a graph based representation of WordNet , our method has basically two steps We first compute the personalized PageRank over WordNet separately for each of the words , producing a probability distribution over WordNet synsets .", "We then compare how similar these two discrete probability distributions are by encoding them as vectors and computing the cosine between the vectors .", "We represent WordNet as a graph G V , E as follows graph nodes represent WordNet concepts synsets and dictionary words ; relations among synsets are represented by undirected edges ; and dictionary words are linked to the synsets associated to them by directed edges .", "For each word in the pair we first compute a personalized PageRank vector of graph G Haveliwala , 2002 .", "Basically , personalized PageRank is computed by modifying the random jump distribution vector in the traditional PageRank equation .", "In our case , we concentrate all probability mass in the target word .", "Regarding PageRank implementation details , we chose a damping value of 0 . 85 and finish the calculation after 30 iterations .", "These are default values , and we did not optimize them .", "Our similarity method is similar , but simpler , to that used by Hughes and Ramage , 2007 , which report very good results on similarity datasets .", "More details of our algorithm can be found in Agirre and Soroa , 2009 .", "The algorithm and needed resouces are publicly available1 .", "The WordNet versions that we use in this work are the Multilingual Central Repository or MCR Atserias et al . , 2004 which includes English WordNet version 1 . 6 and wordnets for several other languages like Spanish , Italian , Catalan and Basque , and WordNet version 3 . 02 .", "We used all the relations in MCR except cooccurrence relations and selectional preference relations and in WordNet 3 . 0 .", "Given the recent availability of the disambiguated gloss relations for WordNet 3 . 03 , we also used a version which incorporates these relations .", "We will refer to the three versions as MCR16 , WN30 and WN30g , respectively .", "Our choice was mainly motivated by the fact that MCR contains tightly aligned wordnets of several languages see below .", "MCR follows the EuroWordNet design Vossen , 1998 , which specifies an InterLingual Index ILI that links the concepts across wordnets of different languages .", "The wordnets for other languages in MCR use the English WordNet synset numbers as ILIs .", "This design allows a decoupling of the relations between concepts which can be taken to be language independent and the links from each content word to its corresponding concepts which is language dependent .", "As our WordNet based method uses the graph of the concepts and relations , we can easily compute the similarity between words from different languages .", "For example , consider a English Spanish pair like car coche .", "Given that the Spanish WordNet is included in MCR we can use MCR as the common knowledge base for the relations .", "We can then compute the personalized PageRank for each of car and coche on the same underlying graph , and then compare the similarity between both probability distributions .", "As an alternative , we also tried to use publicly available mappings for wordnets Daude et al . , 2000 4 in order to create a 3 . 0 version of the Spanish WordNet .", "The mapping was used to link Spanish variants to 3 . 0 synsets .", "We used the English WordNet 3 . 0 , including glosses , to construct the graph .", "The two Spanish WordNet versions are referred to as MCR16 and WN30g .", "In this section , we describe the distributional methods used for calculating similarities between words , and profiting from the use of a large Web based corpus .", "This work is motivated by previous studies that make use of search engines in order to collect cooccurrence statistics between words .", "Turney 2001 uses the number of hits returned by a Web search engine to calculate the Pointwise Mutual Information PMI between terms , as an indicator of synonymy .", "Bollegala et al . 2007 calculate a number of popular relatedness metrics based on page counts , like PMI , the Jaccard coefficient , the Simpson coefficient and the Dice coefficient , which are combined with lexico syntactic patterns as model features .", "The model parameters are trained using Support Vector Machines SVM in order to later rank pairs of words .", "A different approach is the one taken by Sahami and Heilman 2006 , who collect snippets from the results of a search engine and represent each snippet as a vector , weighted with the tf idf score .", "The semantic similarity between two queries is calculated as the inner product between the centroids of the respective sets of vectors .", "To calculate the similarity of two words w1 and w2 , Ruiz Casado et al . 2005 collect snippets containing w1 from a Web search engine , extract a context around it , replace it with w2 and check for the existence of that modified context in the Web .", "Using a search engine to calculate similarities between words has the drawback that the data used will always be truncated .", "So , for example , the numbers of hits returned by search engines nowadays are always approximate and rounded up .", "The systems that rely on collecting snippets are also limited by the maximum number of documents returned per query , typically around a thousand .", "We hypothesize that by crawling a large corpus from the Web and doing standard corpus analysis to collect precise statistics for the terms we should improve over other unsupervised systems that are based on search engine results , and should yield results that are competitive even when compared to knowledge based approaches .", "In order to calculate the semantic similarity between the words in a set , we have used a vector space model , with the following three variations In the bag of words approach , for each word w in the dataset we collect every term t that appears in a window centered in w , and add them to the vector together with its frequency .", "In the context window approach , for each word w in the dataset we collect every window W centered in w removing the central word , and add it to the vector together with its frequency the total number of times we saw window W around w in the whole corpus .", "In this case , all punctuation symbols are replaced with a special token , to unify patterns like , the term said to and the term said to .", "Throughout the paper , when we mention a context window of size N it means N words at each side of the phrase of interest .", "In the syntactic dependency approach , we parse the entire corpus using an implementation of an Inductive Dependency parser as described in Nivre 2006 .", "For each word w we collect a template of the syntactic context .", "We consider sequences of governing words e . g . the parent , grand parent , etc . as well as collections of descendants e . g . , immediate children , grandchildren , etc . .", "This information is then encoded as a contextual template .", "For example , the context template cooks term delicious could be contexts for nouns such as food , meals , pasta , etc .", "This captures both syntactic preferences as well as selectional preferences .", "Contrary to Pado and Lapata 2007 , we do not use the labels of the syntactic dependencies .", "Once the vectors have been obtained , the frequency for each dimension in every vector is weighted using the other vectors as contrast set , with the k2 test , and finally the cosine similarity between vectors is used to calculate the similarity between each pair of terms .", "Except for the syntactic dependency approach , where closed class words are needed by the parser , in the other cases we have removed stopwords pronouns , prepositions , determiners and modal and auxiliary verbs .", "We have used a corpus of four billion documents , crawled from the Web in August 2008 .", "An HTML parser is used to extract text , the language of each document is identified , and non English documents are discarded .", "The final corpus remaining at the end of this process contains roughly 1 . 6 Terawords .", "All calculations are done in parallel sharding by dimension , and it is possible to calculate all pairwise similarities of the words in the test sets very quickly on this corpus using the MapReduce infrastructure .", "A complete run takes around 15 minutes on 2 , 000 cores .", "In order to calculate similarities in a cross lingual setting , where some of the words are in a language l other than English , the following algorithm is used models and distributional models .", "CW Context Windows , BoW bag of words , Syn syntactic vectors .", "For Syn , the window size is actually the tree depth for the governors and descendants .", "For examples , G1 indicates that the contexts include the parents and D2 indicates that both the children and grandchildren make up the contexts .", "The final grouping includes both contextual windows at width 4 and syntactic contexts in the template vectors .", "Max scores are bolded .", "We have used two standard datasets .", "The first one , RG , consists of 65 pairs of words collected by Rubenstein and Goodenough 1965 , who had them judged by 51 human subjects in a scale from 0 . 0 to 4 . 0 according to their similarity , but ignoring any other possible semantic relationships that might appear between the terms .", "The second dataset , WordSim3535 Finkelstein et al . , 2002 contains 353 word pairs , each associated with an average of 13 to 16 human judgements .", "In this case , both similarity and rell never forget the' on his face when grin , 2 , smile , 10 he had a giant' on his face and grin , 3 , smile , 2 room with a huge' on her face and grin , 2 , smile , 6 the state of every' will be updated every automobile , 2 , car , 3 repair or replace the' if it is stolen automobile , 2 , car , 2 located on the north' of the Bay of shore , 14 , coast , 2 areas on the eastern' of the Adriatic Sea shore , 3 , coast , 2 Thesaurus of Current English' The Oxford Pocket Thesaurus slave , 3 , boy , 5 , shore , 3 , string , 2 latedness are annotated without any distinction .", "Several studies indicate that the human scores consistently have very high correlations with each other Miller and Charles , 1991 ; Resnik , 1995 , thus validating the use of these datasets for evaluating semantic similarity .", "For the cross lingual evaluation , the two datasets were modified by translating the second word in each pair into Spanish .", "Two humans translated simultaneously both datasets , with an inter tagger agreement of 72 for RG and 84 for WordSim353 .", "Table 1 shows the Spearman correlation obtained on the RG and WordSim353 datasets , including the interval at 0 . 95 of confidence6 .", "Overall the distributional context window approach performs best in the RG , reaching 0 . 89 correlation , and both WN30g and the combination of context windows and syntactic context perform best on WordSim353 .", "Note that the confidence intervals are quite large in both RG and WordSim353 , and few of the pairwise differences are statistically significant .", "Regarding WordNet based approaches , the use of the glosses and WordNet 3 . 0 WN30g yields the best results in both datasets .", "While MCR16 is close to WN30g for the RG dataset , it lags well behind on WordSim353 .", "This discrepancy is further analyzed is Section 5 . 3 .", "Note that the performance of WordNet in the WordSim353 dataset suffers from unknown words .", "In fact , there are nine pairs which returned null similarity for this reason .", "The number in parenthesis in Table 1 for WordSim353 shows the results for the 344 remaining pairs .", "Section 5 . 2 shows a proposal to overcome this limitation .", "The bag of words approach tends to group together terms that can have a similar distribution of contextual terms .", "Therefore , terms that are topically related can appear in the same textual passages and will get high values using this model .", "We see this as an explanation why this model performed better than the context window approach for WordSim353 , where annotators were instructed to provide high ratings to related terms .", "On the contrary , the context window approach tends to group together words that are exchangeable in exactly the same context , preserving order .", "Table 2 illustrates a few examples of context collected .", "Therefore , true synonyms and hyponyms hyperonyms will receive high similarities , whereas terms related topically or based on any other semantic relation e . g . movie and star will have lower scores .", "This explains why this method performed better for the RG dataset .", "Section 5 . 3 confirms these observations .", "Table 3 shows the results for the English Spanish cross lingual datasets .", "For RG , MCR16 and the context windows methods drop only 5 percentage points , showing that cross lingual similarity is feasible , and that both cross lingual strategies are robust .", "The results for WordSim353 show that WN30g is the best for this dataset , with the rest of the methods falling over 10 percentage points relative to the monolingual experiment .", "A closer look at the WordNet results showed that most of the drop in performance was caused by out of vocabulary words , due to the smaller vocabulary of the Spanish WordNet .", "Though not totally comparable , if we compute the correlation over pairs covered in WordNet alone , the correlation would drop only 2 percentage points .", "In the case of the distributional approaches , the fall in performance was caused by the translations , as only 61 of the words were translated into the original word in the English datasets .", "In this section we present some analysis , including learning curves for distributional methods , the use of distributional similarity to improve WordNet similarity , the contrast between similarity and relatedness , and the combination of methods .", "Figure 1 shows that the correlation improves with the size of the corpus , as expected .", "For the results using the WordSim353 corpus , we show the results of the bag of words approach with context size 10 .", "Results improve from 0 . 5 Spearman correlation up to 0 . 65 when increasing the corpus size three orders of magnitude , although the effect decays at the end , which indicates that we might not get further gains going beyond the current size of the corpus .", "With respect to results for the RG dataset , we used a context window approach with context radius 4 .", "Here , results improve even more with data size , probably due to the sparse data problem collecting 8 word context windows if the corpus is not large enough .", "Correlation improves linearly right to the end , where results stabilize around 0 . 89 .", "Although the vocabulary of WordNet is very extensive , applications are bound to need the similarity between words which are not included in WordNet .", "This is exemplified in the WordSim353 dataset , where 9 pairs contain words which are unknown to WordNet .", "In order to overcome this shortcoming , we could use similar words instead , as provided by the distributional thesaurus .", "We used the distributional thesaurus defined in Section 3 , using context windows of width 4 , to provide three similar words for each of the unknown words in WordNet .", "Results improve for both WN30 and WN30g , as shown in Table 4 , attaining our best results for WordSim353 .", "We mentioned above that the annotation guidelines of WordSim353 did not distinguish between similar and related pairs .", "As the results in Section 4 show , different techniques are more appropriate to calculate either similarity or relatedness .", "In order to study this effect , ideally , we would have two versions of the dataset , where annotators were given precise instructions to distinguish similarity in one case , and relatedness in the other .", "Given the lack of such datasets , we devised a simpler approach in order to reuse the existing human judgements .", "We manually split the dataset in two parts , as follows .", "First , two humans classified all pairs as being synonyms of each other , antonyms , identical , hyperonym hyponym , hyponym hyperonym , holonym meronym , meronym holonym , and noneof the above .", "The inter tagger agreement rate was 0 . 80 , with a Kappa score of 0 . 77 .", "This annotation was used to group the pairs in three categories similar pairs those classified as synonyms , antonyms , identical , or hyponym hyperonym , related pairs those classified as meronym holonym , and pairs classified as none of the above , with a human average similarity greater than 5 , and unrelated pairs those classified as none of the above that had average similarity less than or equal to 5 .", "We then created two new gold standard datasets similarity the union of similar and unrelated pairs , and relatedness the union of related and unrelated 7 .", "Table 5 shows the results on the relatedness and similarity subsets of WordSim353 for the different methods .", "Regarding WordNet methods , both WN30 and WN30g perform similarly on the similarity subset , but WN30g obtains the best results by far on the relatedness data .", "These results are congruent with our expectations two words are similar if their synsets are in close places in the WordNet hierarchy , and two words are related if there is a connection between them .", "Most of the relations in WordNet are of hierarchical nature , and although other relations exist , they are far less numerous , thus explaining the good results for both WN30 and WN30g on similarity , but the bad results of WN30 on relatedness .", "The disambiguated glosses help find connections among related concepts , and allow our method to better model relatedness with respect to WN30 .", "The low results for MCR16 also deserve some comments .", "Given the fact that MCR16 performed very well on the RG dataset , it comes as a surprise that it performs so poorly for the similarity subset of WordSim353 .", "In an additional evaluation , we attested that MCR16 does indeed perform as well as MCR30g on the similar pairs subset .", "We believe that this deviation could be due to the method used to construct the similarity dataset , which includes some pairs of loosely related pairs labeled as unrelated .", "Concerning the techniques based on distributional similarities , the method based on context windows provides the best results for similarity , and the bagof words representation outperforms most of the other techniques for relatedness .", "In order to gain an insight on which would be the upper bound that we could obtain when combining our methods , we took the output of three systems bag of words with window size 10 , context window with size 4 , and the WN30g run .", "Each of these outputs is a ranking of word pairs , and we implemented an oracle that chooses , for each pair , the rank that is most similar to the rank of the pair in the gold standard .", "The outputs of the oracle have a Spearman correlation of 0 . 97 for RG and 0 . 92 for WordSim353 , which gives as an indication of the correlations that could be achieved by choosing for each pair the rank output by the best classifier for that pair .", "The previous results motivated the use of a supervised approach to combine the output of the different systems .", "We created a training corpus containing pairs of pairs of words from the datasets , having as features the similarity and rank of each pair involved as given by the different unsupervised systems .", "A classifier is trained to decide whether the first pair is more similar than the second one .", "For example , a training instance using two unsupervised classifiers is 0 . 001364 , 31 , 0 . 327515 , 64 , 0 . 084805 , 57 , 0 . 109061 , 59 , negative meaning that the similarities given by the first classifier to the two pairs were 0 . 001364 and 0 . 327515 respectively , which ranked them in positions 31 and 64 .", "The second classifier gave them similarities of 0 . 084805 and 0 . 109061 respectively , which ranked them in positions 57 and 59 .", "The class negative indicates that in the gold standard the first pair has a lower score than the second pair .", "We have trained a SVM to classify pairs of pairs , and use its output to rank the entries in both datasets .", "It uses a polynomial kernel with degree 4 .", "We did not have a held out set , so we used the standard settings of Weka , without trying to modify parameters , e . g .", "C . Each word pair is scored with the number of pairs that were considered to have less similarity using the SVM .", "The results using 10 fold crossvalidation are shown in Table 6 .", "A combination of all methods produces the best results reported so far for both datasets , statistically significant for RG .", "Contrary to the WordSim353 dataset , common practice with the RG dataset has been to perform the evaluation with Pearson correlation .", "In our believe Pearson is less informative , as the Pearson correlation suffers much when the scores of two systems are not linearly correlated , something which happens often given due to the different nature of the techniques applied .", "Some authors , e . g .", "Alvarez and Lim 2007 , use a non linear function to map the system outputs into new values distributed more similarly to the values in the gold standard .", "In their case , the mapping function was exp '4 , which was chosen empirically .", "Finding such a function is dependent on the dataset used , and involves an extra step in the similarity calculations .", "Alternatively , the Spearman correlation provides an evaluation metric that is independent of such data dependent transformations .", "Most similarity researchers have published their complete results on a smaller subset of the RG dataset containing 30 word pairs Miller and Charles , 1991 , usually referred to as MC , making it possible to compare different systems using different correlation .", "Table 7 shows the results of related work on MC that was available to us , including our own .", "For the authors that did not provide the detailed data we include only the Pearson correlation with no confidence intervals .", "Among the unsupervised methods introduced in this paper , the context window produced the best reported Spearman correlation , although the 0 . 95 confidence intervals are too large to allow us to accept the hypothesis that it is better than all others methods .", "The supervised combination produces the best results reported so far .", "For the benefit of future research , our results for the MC subset are displayed in Table 8 .", "Comparison on the WordSim353 dataset is easier , as all researchers have used Spearman .", "The figures in Table 9 show that our WordNet based method outperforms all previously published WordNet methods .", "We want to note that our WordNetbased method outperforms that of Hughes and Ramage 2007 , which uses a similar method .", "Although there are some differences in the method , we think that the main performance gain comes from the use of the disambiguated glosses , which they did not use .", "Our distributional methods also outperform all other corpus based methods .", "The most similar approach to our distributional technique is Finkelstein et al . 2002 , who combined distributional similarities from Web documents with a similarity from WordNet .", "Their results are probably worse due to the smaller data size they used 270 , 000 documents and the differences in the calculation of the similarities .", "The only method which outperforms our non supervised methods is that of Gabrilovich and Markovitch , 2007 when based on Wikipedia , probably because of the dense , manually distilled knowledge contained in Wikipedia .", "All in all , our supervised combination gets the best published results on this dataset .", "This paper has presented two state of the art distributional and WordNet based similarity measures , with a study of several parameters , including performance on similarity and relatedness data .", "We show that the use of disambiguated glosses allows for the best published results for WordNet based systems on the WordSim353 dataset , mainly due to the better modeling of relatedness as opposed to similarity .", "Distributional similarities have proven to be competitive when compared to knowledgebased methods , with context windows being better for similarity and bag of words for relatedness .", "Distributional similarity was effectively used to cover out of vocabulary items in the WordNet based measure providing our best unsupervised results .", "The complementarity of our methods was exploited by a supervised learner , producing the best results so far for RG and WordSim353 .", "Our results include confidence values , which , surprisingly , were not included in most previous work , and show that many results over RG and WordSim353 are indistinguishable .", "The algorithm for WordNet base similarity and the necessary resources are publicly available8 .", "This work pioneers cross lingual extension and evaluation of both distributional and WordNet based measures .", "We have shown that closely aligned wordnets provide a natural and effective way to compute cross lingual similarity with minor losses .", "A simple translation strategy also yields good results for distributional methods . automobile , car 3 . 92 62 journey , voyage 3 . 84 54 gem , jewel 3 . 84 61 boy , lad 3 . 76 57 coast , shore 3 . 7 53 asylum , madhouse 3 . 61 45 magician , wizard 3 . 5 49 midday , noon 3 . 42 61 furnace , stove 3 . 11 50 food , fruit 3 . 08 47 bird , cock 3 . 05 46 bird , crane 2 . 97 38 implement , tool 2 . 95 55 brother , monk 2 . 82 42 crane , implement 1 . 68 26 brother , lad 1 . 66 39 car , journey 1 . 16 37 monk , oracle 1 . 1 32 food , rooster 0 . 89 3 coast , hill 0 . 87 34 forest , graveyard 0 . 84 27 monk , slave 0 . 55 17 lad , wizard 0 . 42 13 coast , forest 0 . 42 18 cord , smile 0 . 13 5 glass , magician 0 . 11 10 rooster , voyage 0 . 08 1 noon , string 0 . 08 5"], "summary_lines": ["A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches\n", "This paper presents and compares WordNet-based and distributional similarity approaches.\n", "The strengths and weaknesses of each approach regarding similarity and relatedness tasks are discussed, and a combination is presented.\n", "Each of our methods independently provide the best results in their class on the RG and WordSim353 datasets, and a supervised combination of them yields the best published results on all datasets.\n", "Finally, we pioneer cross-lingual similarity, showing that our methods are easily adapted for a cross-lingual task with minor losses.\n", "We derive a WordNet-based measure using PageRank and combined it with several corpus based vector space models using SVMs.\n", "Examining the relations between the words in each pair, we further split this dataset into similar pairs (WS-sim) and related pairs (WS-rel), where the former contains synonyms, antonyms, identical words and hyponyms/hypernyms and the latter capture other word relations.\n"]}
{"article_lines": ["Global Thresholding And Multiple Pass Parsing", "We present a variation on classic beam thresholding techniques that is up to an order of magnitude faster than the traditional method , at the same performance level .", "We also present a new thresholding technique , global thresholding , which , combined with the new beam thresholding , gives an additional factor of two improvement , and a novel technique , multiple pass parsing , that can be combined with the others to yield yet another 50 improvement .", "We use a new search algorithm to simultaneously optimize the thresholding parameters of the various algorithms .", "In this paper , we examine thresholding techniques for statistical parsers .", "While there exist theoretically efficient 0 n3 algorithms for parsing Probabilistic Context Free Grammars PCFGs and related formalisms , practical parsing algorithms usually make use of pruning techniques , such as beam thresholding , for increased speed .", "We introduce two novel thresholding techniques , global thresholding and multiple pass parsing , and one significant variation on traditional beam thresholding .", "We examine the value of these techniques when used separately , and when combined .", "In order to examine the combined techniques , we also introduce an algorithm for optimizing the settings comments on earlier drafts , and the anonymous reviewers for their extensive comments . of multiple thresholds .", "When all three thresholding methods are used together , they yield very significant speedups over traditional beam thresholding , while achieving the same level of performance .", "We apply our techniques to CKY chart parsing , one of the most commonly used parsing methods in natural language processing .", "In a CKY chart parser , a two dimensional matrix of cells , the chart , is filled in .", "Each cell in the chart corresponds to a span of the sentence , and each cell of the chart contains the nonterminals that could generate that span .", "Cells covering shorter spans are filled in first , so we also refer to this kind of parser as a bottom up chart parser .", "The parser fills in a cell in the chart by examining the nonterminals in lower , shorter cells , and combining these nonterminals according to the rules of the grammar .", "The more nonterminals there are in the shorter cells , the more combinations of nonterminals the parser must consider .", "In some grammars , such as PCFGs , probabilities are associated with the grammar rules .", "This introduces problems , since in many PCFGs , almost any combination of nonterminals is possible , perhaps with some low probability .", "The large number of possibilities can greatly slow parsing .", "On the other hand , the probabilities also introduce new opportunities .", "For instance , if in a particular cell in the chart there is some nonterminal that generates the span with high probability , and another that generates that span with low probability , then we can remove the less likely nonterminal from the cell .", "The less likely nonterminal will probably not be part of either the correct parse or the tree returned by the parser , so removing it will do little harm .", "This technique is called beam thresholding .", "If we use a loose beam threshold , removing only those nonterminals that are much less probable than the best nonterminal in a cell , our parser will run only slightly faster than with no thresholding , while performance measures such as precision and recall will remain virtually unchanged .", "On the other hand , if we use a tight threshold , removing nonterminals that are almost as probable as the best nonterminal in a cell , then we can get a considerable speedup , but at a considerable cost .", "Figure 1 shows the tradeoff between accuracy and time .", "In this paper , we will consider three different kinds of thresholding .", "The first of these is a variation on traditional beam search .", "In traditional beam search , only the probability of a nonterminal generating the terminals of the cell's span is used .", "We have found that a minor variation , introduced in Section 2 , in which we also consider the prior probability that each nonterminal is part of the correct parse , can lead to nearly an order of magnitude improvement .", "The problem with beam search is that it only compares nonterminals to other nonterminals in the same cell .", "Consider the case in which a particular cell contains only bad nonterminals , all of roughly equal probability .", "We can't threshold out these nodes , because even though they are all bad , none is much worse than the best .", "Thus , what we want is a thresholding technique that uses some global information for thresholding , rather than just using information in a single cell .", "The second kind of thresholding we consider is a novel technique , global thresholding , described in Section 3 .", "Global thresholding makes use of the observation that for a nonterminal to be part of the correct parse , it must be part of a sequence of reasonably probable nonterminals covering the whole sentence .", "The last technique we consider , multiple pass parsing , is introduced in Section 4 .", "The basic idea is that we can use information from parsing with one grammar to speed parsing with another .", "We run two passes , the first of which is fast and simple , eliminating from consideration many unlikely potential constituents .", "The second pass is more complicated and slower , but also more accurate .", "Because we have already eliminated many nodes in our first pass , the second pass can run much faster , and , despite the fact that we have to run two passes , the added savings in the second pass can easily outweigh the cost of the first one .", "Experimental comparisons of these techniques show that they lead to considerable speedups over traditional thresholding , when used separately .", "We also wished to combine the thresholding techniques ; this is relatively difficult , since searching for the optimal thresholding parameters in a multi dimensional space is potentially very time consuming .", "We . designed a variant on a gradient descent search algorithm to find the optimal parameters .", "Using all three thresholding methods together , and the parameter search algorithm , we achieved our best results , running an estimated 30 times faster than traditional beam search , at the same performance level .", "The first , and simplest , technique we will examine is beam thresholding .", "While this technique is used as part of many search algorithms , beam thresholding with PCFGs is most similar to beam thresholding as used in speech recognition .", "Beam thresholding is often used in statistical parsers , such as that of Collins 1996 .", "Consider a nonterminal X in a cell covering the span of terminals ti . . . tk .", "We will refer to this as node INT ? , . since it corresponds to a potential node in the final parse tree .", "Recall that in beam thresholding , we compare nodes Nik and Nrk covering the same span .", "If one node is much more likely than the other , then it is unlikely that the less probable node will be part of the correct parse , and we can remove it from the chart , saving time later .", "There is a subtlety about what it means for a node Nxk to be more likely than some other node .", "Ac2 , cording to folk wisdom , the best way to measure the likelihood of a node NiCk is to use the probability that the nonterminal X generates the span ti . tk called the inside probability .", "Formally , we write this as P X ti . . . tk , and denote it by nk .", "However , this does not give information about the probability of the node in the context of the full parse tree .", "For instance , two nodes , one an NP and the other a FRAU fragment , may have equal inside probabilities , but since there are far more NPs than there are FRAU clauses , the NP node is more likely overall .", "Therefore , we must consider more information than just the inside probability .", "The outside probability of a node Nfk is the probability of that node given the surrounding terminals of the sentence , i . e .", "P S _k 1 . tn ; which we denote by a Nick .", "Ideally , we would multiply the inside probability by the outside probability , and normalize .", "This product would give us the overall probability that the node is part of the correct parse .", "Unfortunately , there is no good way to quickly compute the outside probability of a node during bottom up chart parsing although it can be efficiently computed afterwards .", "Thus , we instead multiply the inside probability simply by the prior probability of the nonterminal type , P X , which is an approximation to the outside probability .", "Our final thresholding measure is P X x 3 Nfk .", "In Section 7 . 4 , we will show experiments comparing insideprobability beam thresholding to beam thresholding using the inside probability times the prior .", "Using the prior can lead to a speedup of up to a factor of 10 , at the same performance level .", "To the best of our knowledge , using the prior probability in beam thresholding is new , although not particularly insightful on our part .", "Collins personal communication independently observed the usefulness of this modification , and Caraballo and Charniak 1996 used a related technique in a best first parser .", "We think that the main reason this technique was not used sooner is that beam thresholding for PCFGs is derived from beam thresholding in speech recognition using Hidden Markov Models HMMs .", "In an HMM , the forward probability of a given state corresponds to the probability of reaching that state from the start state .", "The probability of eventually reaching the final state from any state is always 1 .", "Thus , the forward probability is all that is needed .", "The same is true in some top down probabilistic parsing algorithms , such as stochastic versions of Earley's algorithm Stolcke , 1993 .", "However , in a bottom up algorithm , we need the extra factor that indicates the probability of getting from the start symbol to the nonterminal in question , which we approximate by the prior probability .", "As we noted , this can be very different for different nonterminals .", "As mentioned earlier , the problem with beam thresholding is that it can only threshold out the worst nodes of a cell .", "It cannot threshold out an entire cell , even if there are no good nodes in it .", "To remedy this problem , we introduce a novel thresholding technique , global thresholding .", "The key insight of global thresholding is due to Rayner and Carter 1996 .", "Rayner et al . noticed that a particular node cannot be part of the correct parse if there are no nodes in adjacent cells .", "In fact , it must be part of a sequence of nodes stretching from the start of the string to the end .", "In a probabilistic framework where almost every node will have some possibly very small probability , we can rephrase this requirement as being that the node must be part of a reasonably probable sequence .", "Figure 2 shows an example of this insight .", "Nodes A , B , and C will not be thresholded out , because each is part of a sequence from the beginning to the end of the chart .", "On the other hand , nodes X , Y , and Z will be thresholded out , because none is part of such a sequence .", "Rayner et al . used this insight for a hierarchical , non recursive grammar , and only used their technique to prune after the first level of the grammar .", "They computed a score for each sequence as the minimum of the scores of each node in the sequence , and computed a score for each node in the sequence as the minimum of three scores one based on statistics about nodes to the left , one based on nodes to the right , and one based on unigram statistics .", "We wanted to extend the work of Rayner et al . to general PCFGs , including those that were recursive .", "Our approach therefore differs from theirs in many ways .", "Rayner et al . ignore the inside probabilities of nodes ; while this may work after processing only the first level of a grammar , when the inside probabilities will be relatively homogeneous , it could cause problems after other levels , when the inside probability of a node will give important information about its usefulness .", "On the other hand , because long nodes will tend to have low inside probabilities , taking the minimum of all scores strongly favors sequences of short nodes .", "Furthermore , their algorithm requires time 0 n3 to run just once .", "This is acceptable if the algorithm is run only after the first level , but running it more often would lead to an overall run time of 0 0 .", "Finally , we hoped to find an algorithm that was somewhat less heuristic in nature .", "Our global thresholding technique thresholds out node N if the ratio between the most probable sequence of nodes including node N and the overall most probable sequence of nodes is less than some threshold , T . Formally , denoting sequences of nodes by L , we threshold node N if Now , the hard part is determining P L , the probability of a node sequence .", "Unfortunately , there is no way to do this efficiently as part of the intermediate computation of a bottom up chart parser . '", "We will approximate P L as follows 'Some other parsing techniques , such as stochastic versions of Earley parsers Stolcke , 1993 , efficiently compute related probabilities , but we won't explore these parsers here .", "We confess that our real interest is in more complicated grammars , such as those that use head words .", "Grammars such as these can best be parsed bottom up .", "That is , we assume independence between the elements of a sequence .", "The probability of node L , Nx is just its prior probability times its inside 3 , k probability , as before .", "The most important difference between global thresholding and beam thresholding is that global thresholding is global any node in the chart can help prune out any other node .", "In stark contrast , beam thresholding only compares nodes to other nodes covering the same span .", "Beam thresholding typically allows tighter thresholds since there are fewer approximations , but does not benefit from global information .", "Global thresholding is performed in a bottom up chart parser immediately after each length is completed .", "It thus runs n times during the course of parsing a sentence of length n . We use the simple dynamic programming algorithm in Figure 3 .", "There are 0 n2 nodes in the chart , and each node is examined exactly three times , so the run time of this algorithm is 0 n2 .", "The first section of the algorithm works forwards , computing , for each i , f i , which contains the score of the best sequence covering terminals ti . . . ti i Thus f n 1 contains the score of the best sequence covering the whole sentence , maxi , P L .", "The algorithm works analogously to the Viterbi algorithm for HMMs .", "The second section is analogous , but works backwards , computing b i , which contains the score of the best sequence covering terminals ti . tn Once we have computed the preceding arrays , computing maxi , liveL P L is straightforward .", "We simply want the score of the best sequence covering the nodes to the left of N , f Nstart , times the score of the node itself , times the score of the best sequence of nodes fromN isetnargt Niength to the end , which is just b Nstart N th .", "Using this expression , we can threshold each node quickly .", "Since this algorithm is run n times during the course of parsing , and requires time 0 n2 each time it runs , the algorithm requires time 0 n3 overall .", "Experiments will show that the time it saves easily outweighs the time it uses .", "In this section , we discuss a novel thresholding technique , multiple pass parsing .", "We show that multiple pass parsing techniques can yield large speedups .", "Multiple pass parsing is a variation on a new technique in speech recognition , multiple pass speech recognition Zavaliagkos et al . , 1994 , which we introduce first .", "In an idealized multiple pass speech recognizer , we first run a simple pass , computing the forward and backward probabilities .", "This first pass runs relatively quickly .", "We can use information from this simple , fast first pass to eliminate most states , and then run a more complicated , slower second pass that does not examine states that were deemed unlikely by the first pass .", "The extra time of running two passes is more than made up for by the time saved in the second pass .", "The mathematics of multiple pass recognition is fairly simple .", "In the first simple pass , we record the forward probabilities , oi S1 , and backward probabilities , 3 51 , of each state i at each time t . Now , a s xo sn gives the overall probability of being in state i at time t given the acoustics .", "Our second pass will use an HMM whose states are analogous to the first pass HMM's states .", "If a first pass state at some time is unlikely , then the analogous second pass state is probably also unlikely , so we can threshold it out .", "There are a few complications to multiple pass recognition .", "First , storing all the forward and backward probabilities can be expensive .", "Second , the second pass is more complicated than the first , typically meaning that it has more states .", "So the mapping between states in the first pass and states in the second pass may be non trivial .", "To solve both these problems , only states at word transitions are saved .", "That is , from pass to pass , only information about where words are likely to start and end is used for thresholding .", "We can use an analogous algorithm for multiple pass parsing .", "In particular , we can use two grammars , one fast and simple and the other slower , more complicated , and more accurate .", "Rather than using the forward and backward probabilities of speech recognition , we use the analogous inside and outside probabilities , 3 NA and a NA respectively .", "Rememthat ' s the probability that NA is in the correct parse given , as always , the model and the string .", "Thus , we run our first pass , computing this expression for each node .", "We can then eliminate from consideration in our later passes all nodes for which the probability of being in the correct parse was too small in the first pass .", "Of course , for our second pass to be more accurate , it will probably be more complicated , typically containing an increased number of nonterminals and productions .", "Thus , we create a mapping function for length 2 to n for start I to n length I such that P L R add P to Chart length start ; from each first pass nonterminal to a set of second pass nonterminals , and threshold out those second pass nonterminals that map from low scoring first pass nonterminals .", "We call this mapping function the descendants function . 2 There are many possible examples of first and second pass combinations .", "For instance , the first pass could use regular nonterminals , such as NP and VP and the second pass could use nonterminals augmented with head word information .", "The descendants function then appends the possible head words to the first pass nonterminals to get the second pass ones .", "Even though the correspondence between forward backward and inside outside probabilities is very close , there are important differences between speech recognition HMMs and natural language processing PCFGs .", "In particular , we have found that it is more important to threshold productions than nonterminals .", "That is , rather than just noticing that a particular nonterminal VP spanning the words quot ; killed the rabbit quot ; is very likely , we also note that the production VP 4 V NP and the relevant spans is likely .", "Both the first and second pass parsing algorithms are simple variations on CKY parsing .", "In the first pass , we now keep track of each production instance associated with a node , i . e .", "A N no , computing the inside and outside probabilities of each .", "The second pass requires more changes .", "Let us denote the descendants of nonterminal X by X1 . . . Xx .", "In the second pass , for each production 21n this paper , we will assume that each second pass nonterminal can descend from at most one first pass nonterminal in each cell .", "The grammars used here have this property .", "If this assumption is violated , multiple pass parsing is still possible , but some of the algorithms need to be changed . of the form N NIA NI 1 , 1 in the first pass that wasn't thresholded out by multi pass thresholding , beam thresholding , etc . , we consider every descendant production instance , that is , all those of the form NP Ni ; NkZ ; 1 , i , for appropriate values of p , q , r . This algorithm is given in Figure 4 , which uses a current pass matrix Chart to keep track of nonterminals in the current pass , and a previous pass matrix , Prey Chart to keep track of nonterminals in the previous pass .", "We use one additional optimization , keeping track of the descendants of each nonterminal in each cell in PrevChart which are in the corresponding cell of Chart .", "We tried multiple pass thresholding in two different ways .", "In the first technique we tried , productioninstance thresholding , we remove from consideration in the second pass the descendants of all production instances whose combined inside outside probability falls below a threshold .", "In the second technique , node thresholding , we remove from consideration the descendants of all nodes whose inside outside probability falls below a threshold .", "In our pilot experiments , we found that in some cases one technique works slightly better , and in some cases the other does .", "We therefore ran our experiments using both thresholds together .", "One nice feature of multiple pass parsing is that under special circumstances , it is an admissible search technique , meaning that we are guaranteed to find the best solution with it .", "In particular , if we parse using no thresholding , and our grammars have the property that for every non zero probability parse in the second pass , there is an analogous non zero probability parse in the first pass , then multiple pass search is admissible .", "Under these circumstances , no non zero probability parse will be thresholded out , but many zero probability parses may be removed from consideration .", "While we will almost always wish to parse using thresholds , it is nice to know that multiple pass parsing can be seen as an approximation to an admissible technique , where the degree of approximation is controlled by the thresholding parameter .", "The use of any one of these techniques does not exclude the use of the others .", "There is no reason that we cannot use beam thresholding , global thresholding , and multiple pass parsing all at the same time .", "In general , it wouldn't make sense to use a technique such as multiple pass parsing without other thresholding techniques ; our first pass would be overwhelmingly slow without some sort of thresholding .", "There are , however , some practical considerations .", "To optimize a single threshold , we could simply sweep our parameters over a one dimensional range , and pick the best speed versus performance tradeoff .", "In combining multiple techniques , we need to find optimal combinations of thresholding parameters .", "Rather than having to examine 10 values in a single dimensional space , we might have to examine 100 combinations in a two dimensional space .", "Later , we show experiments with up to six thresholds .", "Since we don't have time to parse with one million parameter combinations , we need a better search algorithm .", "Ideally , we would like to be able to pick a performance level in terms of either entropy or precision and recall and find the best set of thresholds for achieving that performance level as quickly as possible .", "If this is our goal , then a normal gradient descent technique won't work , since we can't use such a technique to optimize one function of a set of variables time as a function of thresholds while holding another one constant performance . 3 We wanted a metric of performance which would be sensitive to changes in threshold values .", "In particular , our ideal metric would be strictly increasing as our thresholds loosened , so that every loosening of threshold values would produce a measurable increase in performance .", "The closer we get to this ideal , the fewer sentences we need to test during parameter optimization .", "We tried an experiment in which we ran beam thresholding with a tight threshold , and then a loose threshold , on all sentences of section 0 of length 40 .", "For this experiment only , we discarded those sentences which could not be parsed with the specified setting of the threshold , rather than retrying with looser thresholds .", "We then computed for each of six metrics how often the metric decreased , stayed the same , or increased for each sentence between the two runs .", "Ideally , as we loosened the threshold , every sentence should improve on every metric , but in practice , that wasn't the case .", "As can be seen , the inside score was by far the most nearly strictly increasing metric .", "Therefore , we should use the inside probability as our metric of performance ; however inside probabilities can become very close to zero , so instead we measure entropy , the negative logarithm of the inside probability .", "Metric decrease same increase We implemented a variation on a steepest descent search technique .", "We denote the entropy of the sentence after thresholding by ET .", "Our search engine is given a target performance level ET to search for , 3We could use gradient descent to minimize a weighted sum of time and performance , but we wouldn't know at the beginning what performance we would have at the end .", "If our goal is to have the best performance we can while running in real time , or to achieve a minimum acceptable performance level with as little time as necessary , then a simple gradient descent function wouldn't work as well as our algorithm .", "Also , for this algorithm although not for most experiments , our measurement of time was the total number of productions searched , rather than cpu time ; we wanted the greater accuracy of measuring productions . and then tries to find the best combination of parameters that works at approximately this level of performance .", "At each point , it finds the threshold to change that gives the most quot ; bang for the buck . quot ; It then changes this parameter in the correct direction to move towards ET and possibly overshoot it .", "A simplified version of the algorithm is given in Figure 5 .", "Figure 6 shows graphically how the algorithm works .", "There are two cases .", "In the first case , if we are currently above the goal entropy , then we loosen our thresholds , leading to slower speed and lower entropy .", "We then wish to get as much entropy reduction as possible per time increase ; that is , we want the steepest slope possible .", "On the other hand , if we are trying to increase our entropy , we want as much time decrease as possible per entropy increase ; that is , we want the flattest slope possible .", "Because of this difference , we need to compute different ratios depending on which side of the goal we are on .", "There are several subtleties when thresholds are set very tightly .", "When we fail to parse a sentence because the thresholds are too tight , we retry the parse with lower thresholds .", "This can lead to conditions that are the opposite of what we expect ; for instance , loosening thresholds may lead to faster parsing , because we don't need to parse the sentence , fail , and then retry with looser thresholds .", "The full algorithm contains additional checks that our thresholding change had the effect we expected either increased time for decreased entropy or vice versa .", "If we get either a change in the wrong direction , or a change that makes everything worse , then we retry with the inverse change , hoping that that will have the intended effect .", "If we get a change that makes both time and entropy better , then we make that change regardless of the ratio .", "Also , we need to do checks that the denominator when computing Ratio isn't too small .", "If it is very small , then our estimate may be unreliable , and we don't consider changing this parameter .", "Finally , the actual algorithm we used also contained a simple quot ; annealing schedule quot ; , in which we slowly decreased the factor by which we changed thresholds .", "That is , we actually run the algorithm multiple times to termination , first changing thresholds by a factor of 16 .", "After a loop is reached at this factor , we lower the factor to 4 , then 2 . then 1 . 414 , then 1 . 15 .", "Note that this algorithm is fairly domain independent .", "It can be used for almost any statistical parsing formalism that uses thresholds , or even for speech recognition .", "Beam thresholding is a common approach .", "While we don't know of other systems that have used exactly our techniques , our techniques are certainly similar to those of others .", "For instance , Collins 1996 uses a form of beam thresholding that differs from ours only in that it doesn't use the prior probability of nonterminals as a factor , and Caraballo and Charniak 1996 use a version with the prior , but with other factors as well .", "Much of the previous related work on thresholding is in the similar area of priority functions for agenda based parsers .", "These parsers try to do quot ; best first quot ; parsing , with some function akin to a thresholding function determining what is best .", "The best comparison of these functions is due to Caraballo and Charniak 1996 ; 1997 , who tried various prioritization methods .", "Several of their techniques are similar to our beam thresholding technique , and one of their techniques , not yet published Caraballo and Charniak , 1997 , would probably work better .", "The only technique that Caraballo and Charniak 1996 give that took into account the scores of other nodes in the priority function , the quot ; prefix model , quot ; required 0 n5 time to compute , compared to our 0 n3 system .", "On the other hand , all nodes in the agenda parser were compared to all other nodes , so in some sense all the priority functions were global .", "Note that agenda based PCFG parsers in general require more than 0 n3 run time , because , when better derivations are discovered , they may be forced to propagate improvements to productions that they have previously considered .", "For instance , if an agenda based system first computes the probability for a production S NP VP , and then later computes some better probability for the NP , it must update the probability for the S as well .", "This could propagate through much of the chart .", "To remedy this , Caraballo et al . only propagated probabilities that caused a large enough change Caraballo and Charniak , 1997 .", "Also , the question of when an agenda based system should stop is a little discussed issue , and difficult since there is no obvious stopping criterion .", "Because of these issues , we chose not to implement an agenda based system for comparison .", "As mentioned earlier , Rayner and Carter 1996 describe a system that is the inspiration for global thresholding .", "Because of the limitation of their system to non recursive grammars , and the other differences discussed in Section 3 , global thresholding represents a significant improvement .", "Collins 1996 uses two thresholding techniques .", "The first of these is essentially beam thresholding for each rule P R if nonterminal L in left cell if nonterminal R in right cell add P to parent cell ; Algorithm One for each nonterminal L in left cell for each nonterminal R in right cell for each rule P L R add P to parent cell ; without a prior .", "In the second technique , there is a constant probability threshold .", "Any nodes with a probability below this threshold are pruned .", "If the parse fails , parsing is restarted with the constant lowered .", "We attempted to duplicate this technique , but achieved only negligible performance improvements .", "Collins personal communication reports a 38 speedup when this technique is combined with loose beam thresholding , compared to loose beam thresholding alone .", "Perhaps our lack of success is due to differences between our grammars , which are fairly different formalisms .", "When Collins began using a formalism somewhat closer to ours , he needed to change his beam thresholding to take into account the prior , so this is not unlikely .", "Hwa personal communication using a model similar to PCFGs , Stochastic Lexicalized Tree Insertion Grammars , also was not able to obtain a speedup using this technique .", "There is previous work in the speech recognition community on automatically optimizing some parameters Schwartz et al . , 1992 .", "However , this previous work differed significantly from ours both in the techniques used , and in the parameters optimized .", "In particular , previous work focused on optimizing weights for various components , such as the language model component .", "In contrast , we optimize thresholding parameters .", "Previous techniques could not be used or easily adapted to thresholding parameters .", "The inner loop of the CKY algorithm , which determines for every pair of cells what nodes must be added to the parent , can be written in several different ways .", "Which way this is done interacts with thresholding techniques .", "There are two possibilities , as shown in Figure 7 .", "We used the second technique , since the first technique gets no speedup from most thresholding systems .", "All experiments were trained on sections 2 18 of the Penn Treebank , version II .", "A few were tested , where noted , on the first 200 sentences of section 00 of length at most 40 words .", "In one experiment , we used the first 15 of length at most 40 , and in the remainder of our experiments , we used those sentences in the first 1001 of length at most 40 .", "Our parameter optimization algorithm always used the first 31 sentences of length at most 40 words from section 19 .", "We ran some experiments on more sentences , but there were three sentences in this larger test set that could not be parsed with beam thresholding , even with loose settings of the threshold ; we therefore chose to report the smaller test set , since it is difficult to compare techniques which did not parse exactly the same sentences .", "We needed several grammars for our experiments so that we could test the multiple pass parsing algorithm .", "The grammar rules , and their associated probabilities , were determined by reading them off of the training section of the treebank , in a manner very similar to that used by Charniak 1996 .", "The main grammar we chose was essentially of the following form That is , our grammar was binary branching except that we also allowed unary branching productions .", "There were never more than five subscripted symbols for any nonterminal , although there could be fewer than five if there were fewer than five symbols remaining on the right hand side .", "Thus , our grammar was a kind of 6 gram model on symbols in the grammar . 4 Figure 8 shows an example of how we converted trees to binary branching with our grammar .", "We refer to this grammar as the 6 gram grammar .", "The terminals of the grammar were the part of speech symbols in the treebank .", "Any experiments that don't mention which grammar we used were run with the 6 gram grammar .", "For a simple grammar , we wanted something that would be very fast .", "The fastest grammar we can think of we call the terminal grammar , because it has one nonterminal for each terminal symbol in the alphabet .", "The nonterminal symbol indicates the first terminal in its span .", "The parses are binary branching in the same way that the 6 gram grammar parses are .", "Figure 9 shows how to convert a parse tree to the terminal grammar .", "Since there is only one nonterminal possible for each cell of the chart , parsing is quick for this grammar .", "For technical and practical reasons , we actually wanted a marginally more complicated grammar , which included the quot ; prime quot ; symbol of the 6 gram grammar , indicating that a cell is part of the same constituent as its parent .", "Therefore , we doubled the size of the grammar so that there would be both primed and non primed versions of each terminal ; we call this the terminalprime grammar , and also show how to convert to it in Figure 9 .", "This is the grammar we actually used as the first pass in our multiple pass parsing experiments .", "The goal of a good thresholding algorithm is to trade off correctness for increased speed .", "We must thus measure both correctness and speed , and there are some subtleties to measuring each .", "First , the traditional way of measuring correctness is with metrics such as precision and recall .", "Unfortunately , there are two problems with these measures .", "First , they are two numbers , neither useful without the other .", "Second , they are subject to considerable noise .", "In pilot experiments , we found that as we changed our thresholding values monotonically , precision and recall changed non monotonically see Figure 11 .", "We attribute this to the fact that we must choose a single parse from our parse forest , and , as we tighten a thresholding parameter , we may threshold out either good or bad parses .", "Furthermore , rather than just changing precision or recall by a small amount , a single thresholded item may completely change the shape of the resulting tree .", "Thus , precision and recall are only smooth with very large sets of test data .", "However , because of the large number of experiments we wished to run , using a large set of test data was not feasible .", "Thus , we looked for a surrogate measure , and decided to use the total inside probability of all parses , which , with no thresholding , is just the probability of the sentence given the model .", "If we denote the total inside probability with no thresholding by I and the total inside probability with thresholding by IT , then IL is the probability that we did not threshold out the correct parse , given the model .", "Thus , maximizing IT should maximize correctness .", "Since probabilities can become very small , we instead minimize entropies , the negative logarithm of the probabilities .", "Figure 11 shows that with a large data set , entropy correlates well with precision and recall , and that with smaller sets , it is much smoother .", "Entropy is smoother because it is a function of many more variables in one experiment , there were about 16000 constituents which contributed to precision and recall measurements , versus 151 million productions potentially contributing to entropy .", "Thus , we choose entropy as our measure of correctness for most experiments .", "When we did measure precision and recall , we used the metric as defined by Collins 1996 .", "Note that the fact that entropy changes smoothly and monotonically is critical for the performance of the multiple parameter optimization algorithm .", "Furthermore , we may have to run quite a few iterations of that algorithm to get convergence , so the fact that entropy is smooth for relatively small numbers of sentences is a large help .", "Thus , the discovery that entropy is a good surrogate for precision and recall is non trivial .", "The same kinds of observations could be extended to speech recognition to optimize multiple thresholds there the typical modern speech system has quite a few thresholds , a topic for future research .", "Note that for some sentences , with too tight thresholding , the parser will fail to find any parse at all .", "We dealt with these cases by restarting the parser with all thresholds lowered by a factor of 5 , it , ffating this loosening until a parse could be found .", "This is why for some tight thresholds , the parser may be slower than with looser thresholds the sentence has to be parsed twice , once with tight thresholds , and once with loose ones .", "Next , we needed to choose a measure of time .", "There are two obvious measures amount of work done by the parser , and elapsed time .", "If we measure amount of work done by the parser in terms of the number of productions with non zero probability examined by the parser , wehave a fairly implementation independent , machine independent measure of speed .", "On the other hand , because we used many different thresholding algorithms , some with a fair amount of overhead , this measure seems inappropriate .", "Multiple pass parsing requires use of the outside algorithm ; global thresholding uses its own dynamic programming algorithm ; and even beam thresholding has some per node overhead .", "Thus , we will give most measurements in terms of elapsed time , not including loading the grammar and other 0 1 overhead .", "We did want to verify that elapsed time was a reasonable measure , so we did a beam thresholding experiment to make sure that elapsed time and number of productions examined were well correlated , using 200 sentences and an exponential sweep of the thresholding parameter .", "The results , shown in Figure 10 , clearly indicate that time is a good proxy for productions examined .", "Our first goal was to show that entropy is a good surrogate for precision and recall .", "We thus tried two experiments one with a relatively large test set of 200 sentences , and one with a relatively small test set of 15 sentences .", "Presumably , the 200 sentence test set should be much less noisy , and fairly indicative of performance .", "We graphed both precision and recall , and entropy , versus time , as we swept the thresholding parameter over a sequence of values .", "The results are in Figure 11 .", "As can be seen , entropy is significantly smoother than precision and recall for both size test corpora .", "Our second goal was to check that the prior probability is indeed helpful .", "We ran two experiments , one with the prior and one without .", "Since the experiments without the prior were much worse than those with it , all other beam thresholding experiments included the prior .", "The results , shown in Figure 12 , indicate that the prior is a critical component .", "This experiment was run on 200 sentences of test data .", "Notice that as the time increases , the data tends to approach an asymptote , as shown in the left hand graph of Figure 12 .", "In order to make these small asymptotic changes more clear , we wished to expand the scale towards the asymptote .", "The right hand graph was plotted with this expanded scale , based on log entropy asymptote , a slight variation on a normal log scale .", "We use this scale in all the remaining entropy graphs .", "A normal logarithmic scale is used for the time axis .", "The fact that the time axis is logarithmic is especially useful for determining how much more efficient one algorithm is than another at a given performance level .", "If one picks a performance level on the vertical axis , then the distance between the two curves at that level represents the ratio between their speeds .", "There is roughly a factor of 8 to 10 difference between using the prior and not using it at all graphed performance levels , with a slow trend towards smaller differences as the thresholds are loosened .", "We tried experiments comparing global thresholding to beam thresholding .", "Figure 13 shows the results of this experiment , and later experiments .", "In the best case , global thresholding works twice as well as beam thresholding , in the sense that to achieve the same level of performance requires only half as much time , although smaller improvements were more typical .", "We have found that , in general , global thresholding works better on simpler grammars .", "In some complicated grammars we explored in other work , there were systematic , strong correlations between nodes , which violated the independence approximation used in global thresholding .", "This prevented us from using global thresholding with these grammars .", "In the future , we may modify global thresholding to model some of these correlations .", "While global thresholding works better than beam thresholding in general , each has its own strengths .", "Global thresholding can threshold across cells , but because of the approximations used , the thresholds must generally be looser .", "Beam thresholding can only threshold within a cell , but can do so fairly tightly .", "Combining the two offers the potential to get the advantages of both .", "We ran a series of experiments using the thresholding optimization algorithm of Section 5 .", "Figure 13 gives the results .", "The combination of beam and global thresholding together is clearly better than either alone , in some cases running 40 faster than global thresholding alone , while achieving the same performance level .", "The combination generally runs twice as fast as beam thresholding alone , although up to a factor of three .", "Multiple pass parsing improves even further on our experiments combining beam and global thresholding .", "Note that we used both beam and global thresholding for both the first and second pass in these experiments .", "The first pass grammar was the very simple terminal prime grammar , and the second pass grammar was the usual 6 gram grammar .", "We evaluated multiple pass parsing slightly differently from the other thresholding techniques .", "In the experiments conducted here , our first and second pass grammars were very different from each other .", "For a given parse to be returned , it must be in the intersection of both grammars , and reasonably likely according to both .", "Since the first and second pass grammars capture different information , parses which are likely according to both are especially good .", "The entropy of a sentence measures its likelihood according to the second pass , but ignores the fact that the returned parse must also be likely according to the first pass .", "Thus , entropy , our measure in the previous experiments , which measures only likelihood according to the final pass , is not necessarily the right measure to use .", "We therefore give precision and recall results in this section .", "We still optimized our thresholding parameters using the same 31 sentence held out corpus , and minimizing entropy versus number of productions , as before .", "We should note that when we used a first pass grammar that captured a strict subset of the information in the second pass grammar , we have found that entropy is a very good measure of performance .", "As in our earlier experiments , it tends to be well correlated with precision and recall but less subject to noise .", "It is only because of the grammar mismatch that we have changed the evaluation .", "Figure 14 shows precision and recall curves for single pass versus multiple pass experiments .", "As in the entropy curves , we can determine the performance ratio by looking across horizontally .", "For instance , the multi pass recognizer achieves a 74 recall level using 2500 seconds , while the best single pass algorithm requires about 4500 seconds to reach that level .", "Due to the noise resulting from precision and recall measurements , it is hard to exactly quantify the advantage from multiple pass parsing , but it is generally about 50 .", "In this paper , we only considered applying multiplepass and global thresholding techniques to parsing probabilistic context free grammars .", "However , just about any probabilistic grammar formalism for which inside and outside probabilities can be computed can benefit from these techniques .", "For instance , Probabilistic Link Grammars Lafferty , Sleator , and Temperley , 1992 could benefit from our algorithms .", "We have however had trouble using global thresholding with grammars that strongly violated the independence assumptions of global thresholding .", "One especially interesting possibility is to apply multiple pass techniques to formalisms that require 0 n3 parsing time , such as Stochastic Bracketing Transduction Grammar SBTG Wu , 1996 and Stochastic Tree Adjoining Grammars STAG Resnik , 1992 ; Schabes , 1992 .", "SBTG is a contextfree like formalism designed for translation from one language to another ; it uses a four dimensional chart to index spans in both the source and target language simultaneously .", "It would be interesting to try speeding up an SBTG parser by running an 0 n3 first pass on the source language alone , and using this to prune parsing of the full SBTG .", "The STAG formalism is a mildly context sensitive formalism , requiring 0 n6 time to parse .", "Most STAG productions in practical grammars are actually context free .", "The traditional way to speed up STAG parsing is to use the context free subset of an STAG to form a Stochastic Tree Insertion Grammar STIG Schabes and Waters , 1994 , an 0 n3 formalism , but this method has problems , because the STIG undergenerates since it is missing some elementary trees .", "A different approach would be to use multiple pass parsing .", "We could first find a contextfree covering grammar for the STAG , and use this as a first pass , and then use the full STAG for the second pass .", "The grammars described here are fairly simple , presented for purposes of explication .", "In other work in preparation , in which we have used a significantly more complicated grammar , which we call the Probabilistic Feature Grammar PFG , the improvements from multiple pass parsing are even more dramatic single pass experiments are simply too slow to run at all .", "We have also found the automatic thresholding parameter optimization algorithm to be very useful .", "Before writing the parameter optimization algorithm , we developed the PFG grammar and the multiple pass parsing technique and ran a series of experiments using hand optimized parameters .", "We recently ran the optimization algorithm and reran the experiments , achieving a factor of two speedup with no performance loss .", "While we had not spent a great deal of time hand optimizing these parameters , we are very encouraged by the optimization algorithm's practical utility .", "This paper introduces four new techniques beam thresholding with priors , global thresholding , multiple pass parsing , and automatic search for thresholding parameters .", "Beam thresholding with priors can lead to almost an order of magnitude improvement over beam thresholding without priors .", "Global thresholding can be up to three times as efficient as the new beam thresholding technique , although the typical improvement is closer to 50 .", "When global thresholding and beam thresholding are combined , they are usually two to three times as fast as beam thresholding alone .", "Multiple pass parsing can lead to up to an additional 50 improvement with the grammars in this paper .", "We expect the parameter optimization algorithm to be broadly useful ."], "summary_lines": ["Global Thresholding And Multiple-Pass Parsing\n", "We present a variation on classic beam thresholding techniques that is up to an order of magnitude faster than the traditional method, at the same performance level.\n", "We also present a new thresholding technique, global thresholding, which, combined with the new beam thresholding, gives an additional factor of two improvement, and a novel technique, multiple pass parsing, that can be combined with the others to yield yet another 50% improvement.\n", "We use a new search algorithm to simultaneously optimize the thresholding parameters of the various algorithms.\n", "we describe a method for producing a simple but crude approximate grammar of a standard context-free grammar.\n"]}
{"article_lines": ["Minimum Cut Model For Spoken Lecture Segmentation", "We consider the task of unsupervised lecture segmentation .", "We formalize segmentation as a graph partitioning task that optimizes the normalized cut criterion .", "Our approach moves beyond localized comparisons and takes into account longrange cohesion dependencies .", "Our results demonstrate that global analysis improves the segmentation accuracy and is robust in the presence of speech recognition errors .", "The development of computational models of text structure is a central concern in natural language processing .", "Text segmentation is an important instance of such work .", "The task is to partition a text into a linear sequence of topically coherent segments and thereby induce a content structure of the text .", "The applications of the derived representation are broad , encompassing information retrieval , question answering and summarization .", "Not surprisingly , text segmentation has been extensively investigated over the last decade .", "Following the first unsupervised segmentation approach by Hearst 1994 , most algorithms assume that variations in lexical distribution indicate topic changes .", "When documents exhibit sharp variations in lexical distribution , these algorithms are likely to detect segment boundaries accurately .", "For example , most algorithms achieve high performance on synthetic collections , generated by concatenation of random text blocks Choi , 2000 .", "The difficulty arises , however , when transitions between topics are smooth and distributional variations are subtle .", "This is evident in the performance of existing unsupervised algorithms on less structured datasets , such as spoken meeting transcripts Galley et al . , 2003 .", "Therefore , a more refined analysis of lexical distribution is needed .", "Our work addresses this challenge by casting text segmentation in a graph theoretic framework .", "We abstract a text into a weighted undirected graph , where the nodes of the graph correspond to sentences and edge weights represent the pairwise sentence similarity .", "In this framework , text segmentation corresponds to a graph partitioning that optimizes the normalized cut criterion Shi and Malik , 2000 .", "This criterion measures both the similarity within each partition and the dissimilarity across different partitions .", "Thus , our approach moves beyond localized comparisons and takes into account long range changes in lexical distribution .", "Our key hypothesis is that global analysis yields more accurate segmentation results than local models .", "We tested our algorithm on a corpus of spoken lectures .", "Segmentation in this domain is challenging in several respects .", "Being less structured than written text , lecture material exhibits digressions , disfluencies , and other artifacts of spontaneous communication .", "In addition , the output of speech recognizers is fraught with high word error rates due to specialized technical vocabulary and lack of in domain spoken data for training .", "Finally , pedagogical considerations call for fluent transitions between different topics in a lecture , further complicating the segmentation task .", "Our experimental results confirm our hypothesis considering long distance lexical dependencies yields substantial gains in segmentation performance .", "Our graph theoretic approach compares favorably to state of the art segmentation algorithms and attains results close to the range of human agreement scores .", "Another attractive property of the algorithm is its robustness to noise the accuracy of our algorithm does not deteriorate significantly when applied to speech recognition output .", "Most unsupervised algorithms assume that fragments of text with homogeneous lexical distribution correspond to topically coherent segments .", "Previous research has analyzed various facets of lexical distribution , including lexical weighting , similarity computation , and smoothing Hearst , 1994 ; Utiyama and Isahara , 2001 ; Choi , 2000 ; Reynar , 1998 ; Kehagias et al . , 2003 ; Ji and Zha , 2003 .", "The focus of our work , however , is on an orthogonal yet fundamental aspect of this analysis the impact of long range cohesion dependencies on segmentation performance .", "In contrast to previous approaches , the homogeneity of a segment is determined not only by the similarity of its words , but also by their relation to words in other segments of the text .", "We show that optimizing our global objective enables us to detect subtle topical changes . mentation Our work is inspired by minimum cutbased segmentation algorithms developed for image analysis .", "Shi and Malik 2000 introduced the normalized cut criterion and demonstrated its practical benefits for segmenting static images .", "Our method , however , is not a simple application of the existing approach to a new task .", "First , in order to make it work in the new linguistic framework , we had to redefine the underlying representation and introduce a variety of smoothing and lexical weighting techniques .", "Second , the computational techniques for finding the optimal partitioning are also quite different .", "Since the minimization of the normalized cut is NP complete in the general case , researchers in vision have to approximate this computation .", "Fortunately , we can find an exact solution due to the linearity constraint on text segmentation .", "Linguistic research has shown that word repetition in a particular section of a text is a device for creating thematic cohesion Halliday and Hasan , 1976 , and that changes in the lexical distributions usually signal topic transitions .", "Figure 1 illustrates these properties in a lecture transcript from an undergraduate Physics class .", "We use the text Dotplotting representation by Church , 1993 and plot the cosine similarity scores between every pair of sentences in the text .", "The intensity of a point i , j on the plot indicates the degree to which the i th sentence in the text is similar to the j th sentence .", "The true segment boundaries are denoted by vertical lines .", "This similarity plot reveals a block structure where true boundaries delimit blocks of text with high inter sentential similarity .", "Sentences found in different blocks , on the other hand , tend to exhibit low similarity .", "Formalizing the Objective Whereas previous unsupervised approaches to segmentation rested on intuitive notions of similarity density , we formalize the objective of text segmentation through cuts on graphs .", "We aim to jointly maximize the intra segmental similarity and minimize the similarity between different segments .", "In other words , we want to find the segmentation with a maximally homogeneous set of segments that are also maximally different from each other .", "Let G V , E be an undirected , weighted graph , where V is the set of nodes corresponding to sentences in the text and E is the set of weighted edges See Figure 2 .", "The edge weights , w u , v , define a measure of similarity between pairs of nodes u and v , where higher scores indicate higher similarity .", "Section 4 provides more details on graph construction .", "We consider the problem of partitioning the graph into two disjoint sets of nodes A and B .", "We aim to minimize the cut , which is defined to be the sum of the crossing edges between the two sets of nodes .", "In other words , we want to split the sentences into two maximally dissimilar classes by choosing A and B to minimize Decoding Papadimitriou proved that the problem of minimizing normalized cuts on graphs is NP complete Shi and Malik , 2000 .", "However , in our case , the multi way cut is constrained to preserve the linearity of the segmentation .", "By segmentation linearity , we mean that all of the nodes between the leftmost and the rightmost nodes of a particular partition have to belong to that partition .", "With this constraint , we formulate a dynamic programming algorithm for exactly finding the minimum normalized multiway cut in polynomial time However , we need to ensure that the two partitions are not only maximally different from each other , but also that they are themselves homogeneous by accounting for intra partition node similarity .", "We formulate this requirement in the framework of normalized cuts Shi and Malik , 2000 , where the cut value is normalized by the volume of the corresponding partitions .", "The volume of the partition is the sum of its edges to the whole graph The normalized cut criterion Ncut is then defined as follows By minimizing this objective we simultaneously minimize the similarity across partitions and maximize the similarity within partitions .", "This formulation also allows us to decompose the objective into a sum of individual terms , and formulate a dynamic programming solution to the multiway cut problem .", "This criterion is naturally extended to a k way normalized cut where A1 . . . Ak form a partition of the graph , and V Ak is the set difference between the entire graph and partition k . C i , k is the normalized cut value of the optimal segmentation of the first k sentences into i segments .", "The i th segment , Aj , k , begins at node uj and ends at node uk .", "B i , k is the back pointer table from which we recover the optimal sequence of segment boundaries .", "Equations 3 and 4 capture respectively the condition that the normalized cut value of the trivial segmentation of an empty text into one segment is zero and the constraint that the first segment starts with the first node .", "The time complexity of the dynamic programming algorithm is O KN2 , where K is the number of partitions and N is the number of nodes in the graph or sentences in the transcript .", "Clearly , the performance of our model depends on the underlying representation , the definition of the pairwise similarity function , and various other model parameters .", "In this section we provide further details on the graph construction process .", "Preprocessing Before building the graph , we apply standard text preprocessing techniques to the text .", "We stem words with the Porter stemmer Porter , 1980 to alleviate the sparsity of word counts through stem equivalence classes .", "We also remove words matching a prespecified list of stop words .", "Graph Topology As we noted in the previous section , the normalized cut criterion considers long term similarity relationships between nodes .", "This effect is achieved by constructing a fullyconnected graph .", "However , considering all pairwise relations in a long text may be detrimental to segmentation accuracy .", "Therefore , we discard edges between sentences exceeding a certain threshold distance .", "This reduction in the graph size also provides us with computational savings .", "Similarity Computation In computing pairwise sentence similarities , sentences are represented as vectors of word counts .", "Cosine similarity is commonly used in text segmentation Hearst , 1994 .", "To avoid numerical precision issues when summing a series of very small scores , we compute exponentiated cosine similarity scores between pairs of sentence vectors We further refine our analysis by smoothing the similarity metric .", "When comparing two sentences , we also take into account similarity between their immediate neighborhoods .", "The smoothing is achieved by adding counts of words that occur in adjoining sentences to the current sentence feature vector .", "These counts are weighted in accordance to their distance from the current sentence e \u03b1 j i sj , where si are vectors of word counts , and \u03b1 is a parameter that controls the degree of smoothing .", "In the formulation above we use sentences as our nodes .", "However , we can also represent graph nodes with non overlapping blocks of words of fixed length .", "This is desirable , since the lecture transcripts lack sentence boundary markers , and short utterances can skew the cosine similarity scores .", "The optimal length of the block is tuned on a heldout development set .", "Lexical Weighting Previous research has shown that weighting schemes play an important role in segmentation performance Ji and Zha , 2003 ; Choi et al . , 2001 .", "Of particular concern are words that may not be common in general English discourse but that occur throughout the text for a particular lecture or subject .", "For example , in a lecture about support vector machines , the occurrence of the term SVM is not going to convey a lot of information about the distribution of sub topics , even though it is a fairly rare term in general English and bears much semantic content .", "The same words can convey varying degrees of information across different lectures , and term weighting specific to individual lectures becomes important in the similarity computation .", "In order to address this issue , we introduce a variation on the tf idf scoring scheme used in the information retrieval literature Salton and Buckley , 1988 .", "A transcript is split uniformly into N chunks ; each chunk serves as the equivalent of documents in the tf idf computation .", "The weights are computed separately for each transcript , since topic and word distributions vary across lectures .", "In this section we present the different corpora used to evaluate our model and provide a brief overview of the evaluation metrics .", "Next , we describe our human segmentation study on the corpus of spoken lecture data .", "A heldout development set of three lectures isused for estimating the optimal word block length for representing nodes , the threshold distances for discarding node edges , the number of uniform chunks for estimating tf idf lexical weights , the alpha parameter for smoothing , and the length of the smoothing window .", "We use a simple greedy search procedure for optimizing the parameters .", "We evaluate our segmentation algorithm on three sets of data .", "Two of the datasets we use are new segmentation collections that we have compiled for this study , 1 and the remaining set includes a standard collection previously used for evaluation of segmentation algorithms .", "Various corpus statistics for the new datasets are presented in Table 1 .", "Below we briefly describe each corpus .", "Physics Lectures Our first corpus consists of spoken lecture transcripts from an undergraduate Physics class .", "In contrast to other segmentation datasets , our corpus contains much longer texts .", "A typical lecture of 90 minutes has 500 to 700 sentences with 8500 words , which corresponds to about 15 pages of raw text .", "We have access both to manual transcriptions of these lectures and also output from an automatic speech recognition system .", "The word error rate for the latter is 19 . 4 , 2 which is representative of state of the art performance on lecture material Leeuwis et al . , 2003 .", "The Physics lecture transcript segmentations were produced by the teaching staff of the introductory Physics course at the Massachusetts Institute of Technology .", "Their objective was to facilitate access to lecture recordings available on the class website .", "This segmentation conveys the high level topical structure of the lectures .", "On average , a lecture was annotated with six segments , and a typical segment corresponds to two pages of a transcript .", "Artificial Intelligence Lectures Our second lecture corpus differs in subject matter , lecturing style , and segmentation granularity .", "The graduate Artificial Intelligence class has , on average , twelve segments per lecture , and a typical segment is about half of a page .", "One segment roughly corresponds to the content of a slide .", "This time the segmentation was obtained from the lecturer herself .", "The lecturer went through the transcripts of lecture recordings and segmented the lectures with the objective of making the segments correspond to presentation slides for the lectures .", "Due to the low recording quality , we were unable to obtain the ASR transcripts for this class .", "Therefore , we only use manual transcriptions of these lectures .", "Synthetic Corpus Also as part of our analysis , we used the synthetic corpus created by Choi 2000 which is commonly used in the evaluation of segmentation algorithms .", "This corpus consists of a set of concatenated segments randomly sampled from the Brown corpus .", "The length of the segments in this corpus ranges from three to eleven sentences .", "It is important to note that the lexical transitions in these concatenated texts are very sharp , since the segments come from texts written in widely varying language styles on completely different topics .", "We use the Pk and WindowDiff measures to evaluate our system Beeferman et al . , 1999 ; Pevzner and Hearst , 2002 .", "The Pk measure estimates the probability that a randomly chosen pair of words within a window of length k words is inconsistently classified .", "The WindowDiff metric is a variant of the Pk measure , which penalizes false positives on an equal basis with near misses .", "Both of these metrics are defined with respect to the average segment length of texts and exhibit high variability on real data .", "We follow Choi 2000 and compute the mean segment length used in determining the parameter k on each reference text separately .", "We also plot the Receiver Operating Characteristic ROC curve to gauge performance at a finer level of discrimination Swets , 1988 .", "The ROC plot is the plot of the true positive rate against the false positive rate for various settings of a decision criterion .", "In our case , the true positive rate is the fraction of boundaries correctly classified , and the false positive rate is the fraction of non boundary positions incorrectly classified as boundaries .", "In computing the true and false positive rates , we vary the threshold distance to the true boundary within which a hypothesized boundary is considered correct .", "Larger areas under the ROC curve of a classifier indicate better discriminative performance .", "Spoken lectures are very different in style from other corpora used in human segmentation studies Hearst , 1994 ; Galley et al . , 2003 .", "We are interested in analyzing human performance on a corpus of lecture transcripts with much longer texts and a less clear cut concept of a sub topic .", "We define a segment to be a sub topic that signals a prominent shift in subject matter .", "Disregarding this sub topic change would impair the high level understanding of the structure and the content of the lecture .", "As part of our human segmentation analysis , we asked three annotators to segment the Physics lecture corpus .", "These annotators had taken the class in the past and were familiar with the subject matter under consideration .", "We wrote a detailed instruction manual for the task , with annotation guidelines for the most part following the model used by Gruenstein et al . 2005 .", "The annotators were instructed to segment at a level of granularity that would identify most of the prominent topical transitions necessary for a summary of the lecture .", "The annotators used the NOMOS annotation software toolkit , developed for meeting segmentation Gruenstein et al . , 2005 .", "They were provided with recorded audio of the lectures and the corresponding text transcriptions .", "We intentionally did not provide the subjects with the target number of boundaries , since we wanted to see if the annotators would converge on a common segmentation granularity .", "Table 2 presents the annotator segmentation statistics .", "We see two classes of segmentation granularities .", "The original reference O and annotator A segmented at a coarse level with an average of 6 . 6 and 8 . 9 segments per lecture , respectively .", "Annotators B and C operated at much finer levels of discrimination with 18 . 4 and 13 . 8 segments per lecture on average .", "We conclude that multiple levels of granularity are acceptable in spoken lecture segmentation .", "This is expected given the length of the lectures and varying human judgments in selecting relevant topical content .", "Following previous studies , we quantify the level of annotator agreement with the Pk measure Gruenstein et al . , 2005 . 3 Table 3 shows the annotator agreement scores between different pairs of annotators .", "Pk measures ranged from 0 . 24 and 0 . 42 .", "We observe greater consistency at similar levels of granularity , and less so across the two classes .", "Note that annotator A operated at a level of granularity consistent with the original reference segmentation .", "Hence , the 0 . 24 Pk measure score serves as the benchmark with which we can compare the results attained by segmentation algorithms on the Physics lecture data .", "As an additional point of reference we note that the uniform and random baseline segmentations attain 0 . 469 and 0 . 493 Pk measure , respectively , on the Physics lecture set .", "Figure 3 ROC plot for the Minimum Cut Segmenter on thirty Physics Lectures , with edge cutoffs set at five and hundred sentences .", "Benefits of global analysis We first determine the impact of long range pairwise similarity dependencies on segmentation performance .", "Our rithms using the Pk and WindowDiff measures , with three lectures heldout for development . key hypothesis is that considering long distance lexical relations contributes to the effectiveness of the algorithm .", "To test this hypothesis , we discard edges between nodes that are more than a certain number of sentences apart .", "We test the system on a range of data sets , including the Physics and AI lectures and the synthetic corpus created by Choi 2000 .", "We also include segmentation results on Physics ASR transcripts .", "The results in Table 4 confirm our hypothesis taking into account non local lexical dependencies helps across different domains .", "On manually transcribed Physics lecture data , for example , the algorithm yields 0 . 394 Pk measure when taking into account edges separated by up to ten sentences .", "When dependencies up to a hundred sentences are considered , the algorithm yields a 25 reduction in Pk measure .", "Figure 3 shows the ROC plot for the segmentation of the Physics lecture data with different cutoff parameters , again demonstrating clear gains attained by employing longrange dependencies .", "As Table 4 shows , the improvement is consistent across all lecture datasets .", "We note , however , that after some point increasing the threshold degrades performance , because it introduces too many spurious dependencies see the last column of Table 4 .", "The speaker will occasionally return to a topic described at the beginning of the lecture , and this will bias the algorithm to put the segment boundary closer to the end of the lecture .", "Long range dependencies do not improve the performance on the synthetic dataset .", "This is expected since the segments in the synthetic dataset are randomly selected from widely varying documents in the Brown corpus , even spanning different genres of written language .", "So , effectively , there are no genuine long range dependencies that can be exploited by the algorithm .", "Comparison with local dependency models We compare our system with the state of the art similarity based segmentation system developed by Choi 2000 .", "We use the publicly available implementation of the system and optimize the system on a range of mask sizes and different parameter settings described in Choi , 2000 on a heldout development set of three lectures .", "To control for segmentation granularity , we specify the number of segments in the reference O segmentation for both our system and the baseline .", "Table 5 shows that the Minimum Cut algorithm consistently outperforms the similarity based baseline on all the lecture datasets .", "We attribute this gain to the presence of more attenuated topic transitions in spoken language .", "Since spoken language is more spontaneous and less structured than written language , the speaker needs to keep the listener abreast of the changes in topic content by introducing subtle cues and references to prior topics in the course of topical transitions .", "Non local dependencies help to elucidate shifts in focus , because the strength of a particular transition is measured with respect to other local and long distance contextual discourse relationships .", "Our system does not outperform Choi s algorithm on the synthetic data .", "This again can be attributed to the discrepancy in distributional properties of the synthetic corpus which lacks coherence in its thematic shifts and the lecture corpus of spontaneous speech with smooth distributional variations .", "We also note that we did not try to adjust our model to optimize its performance on the synthetic data .", "The smoothing method developed for lecture segmentation may not be appropriate for short segments ranging from three to eleven sentences that constitute the synthetic set .", "We also compared our method with another state of the art algorithm which does not explicitly rely on pairwise similarity analysis .", "This algorithm Utiyama and Isahara , 2001 UI computes the optimal segmentation by estimating changes in the language model predictions over different partitions .", "We used the publicly available implementation of the system that does not require parameter tuning on a heldout development set .", "Again , our method achieves favorable performance on a range of lecture data sets See Table 5 , and both algorithms attain results close to the range of human agreement scores .", "The attractive feature of our algorithm , however , is robustness to recognition errors testing it on the ASR transcripts caused only 7 . 8 relative increase in Pk measure from 0 . 298 to 0 . 322 , compared to a 13 . 5 relative increase for the UI system .", "We attribute this feature to the fact that the model is less dependent on individual recognition errors , which have a detrimental effect on the local segment language modeling probability estimates for the UI system .", "The block level similarity function is not as sensitive to individual word errors , because the partition volume normalization factor dampens their overall effect on the derived models .", "In this paper we studied the impact of long range dependencies on the accuracy of text segmentation .", "We modeled text segmentation as a graphpartitioning task aiming to simultaneously optimize the total similarity within each segment and dissimilarity across various segments .", "We showed that global analysis of lexical distribution improves the segmentation accuracy and is robust in the presence of recognition errors .", "Combining global analysis with advanced methods for smoothing Ji and Zha , 2003 and weighting could further boost the segmentation performance .", "Our current implementation does not automatically determine the granularity of a resulting segmentation .", "This issue has been explored in the past Ji and Zha , 2003 ; Utiyama and Isahara , 2001 , and we will explore the existing strategies in our framework .", "We believe that the algorithm has to produce segmentations for various levels of granularity , depending on the needs of the application that employs it .", "Our ultimate goal is to automatically generate tables of content for lectures .", "We plan to investigate strategies for generating titles that will succinctly describe the content of each segment .", "We will explore how the interaction between the generation and segmentation components can improve the performance of such a system as a whole .", "The authors acknowledge the support of the National Science Foundation CAREER grant IIS 0448168 , grant IIS0415865 , and the NSF Graduate Fellowship .", "Any opinions , findings , conclusions or recommendations expressed in this publication are those of the author s and do not necessarily reflect the views of the National Science Foundation .", "We would like to thank Masao Utiyama for providing us with an implementation of his segmentation system and Alex Gruenstein for assisting us with the NOMOS toolkit .", "We are grateful to David Karger for an illuminating discussion on the Minimum Cut algorithm .", "We also would like to acknowledge the MIT NLP and Speech Groups , the three annotators , and the three anonymous reviewers for valuable comments , suggestions , and help ."], "summary_lines": ["Minimum Cut Model For Spoken Lecture Segmentation\n", "We consider the task of unsupervised lecture segmentation.\n", "We formalize segmentation as a graph-partitioning task that optimizes the normalized cut criterion.\n", "Our approach moves beyond localized comparisons and takes into account long-range cohesion dependencies.\n", "Our results demonstrate that global analysis improves the segmentation accuracy and is robust in the presence of speech recognition errors.\n", "We optimize a normalized minimum-cut criteria based on a variation of the cosine similarity between sentences.\n", "Our problem is to find topical boundaries in transcripts of course lectures.\n", "We create a corpus of course lectures segmented by four annotators, noting that the annotators operated at different levels of granularity.\n"]}
{"article_lines": ["A Tale of Two Parsers Investigating and Combining Graph based and Transition based Dependency Parsing", "Graph based and transition based approaches to dependency parsing adopt very different views of the problem , each view having its own strengths and limitations .", "We study both approaches under the framework of beamsearch .", "By developing a graph based and a transition based dependency parser , we show that a beam search decoder is a competitive choice for both methods .", "More importantly , we propose a beam search based parser that combines both graph based and transitionbased parsing into a single system for training and decoding , showing that it outperforms both the pure graph based and the pure transition based parsers .", "Testing on the English and Chinese Penn Treebank data , the combined system gave state of the art accuraof respectively .", "Graph based McDonald et al . , 2005 ; McDonald and Pereira , 2006 ; Carreras et al . , 2006 and transition based Yamada and Matsumoto , 2003 ; Nivre et al . , 2006 parsing algorithms offer two different approaches to data driven dependency parsing .", "Given an input sentence , a graph based algorithm finds the highest scoring parse tree from all possible outputs , scoring each complete tree , while a transition based algorithm builds a parse by a sequence of actions , scoring each action individually .", "The terms graph based and transition based were used by McDonald and Nivre 2007 to describe the difference between MSTParser McDonald and Pereira , 2006 , which is a graph based parser with an exhaustive search decoder , and MaltParser Nivre et al . , 2006 , which is a transition based parser with a greedy search decoder .", "In this paper , we do not differentiate graph based and transitionbased parsers by their search algorithms a graphbased parser can use an approximate decoder while a transition based parser is not necessarily deterministic .", "To make the concepts clear , we classify the two types of parser by the following two criteria By this classification , beam search can be applied to both graph based and transition based parsers .", "Representative of each method , MSTParser and MaltParser gave comparable accuracies in the CoNLL X shared task Buchholz and Marsi , 2006 .", "However , they make different types of errors , which can be seen as a reflection of their theoretical differences McDonald and Nivre , 2007 .", "MSTParser has the strength of exact inference , but its choice of features is constrained by the requirement of efficient dynamic programming .", "MaltParser is deterministic , yet its comparatively larger feature range is an advantage .", "By comparing the two , three interesting research questions arise 1 how to increase the flexibility in defining features for graph based parsing ; 2 how to add search to transition based parsing ; and 3 how to combine the two parsing approaches so that the strengths of each are utilized .", "In this paper , we study these questions under one framework beam search .", "Beam search has been successful in many NLP tasks Koehn et al . , 2003 ; Collins and Roark , 2004 , and can achieve accuracy that is close to exact inference .", "Moreover , a beamsearch decoder does not impose restrictions on the search problem in the way that an exact inference decoder typically does , such as requiring the optimal subproblem property for dynamic programming , and therefore enables a comparatively wider range of features for a statistical system .", "We develop three parsers .", "Firstly , using the same features as MSTParser , we develop a graph based parser to examine the accuracy loss from beamsearch compared to exact search , and the accuracy gain from extra features that are hard to encode for exact inference .", "Our conclusion is that beamsearch is a competitive choice for graph based parsing .", "Secondly , using the transition actions from MaltParser , we build a transition based parser and show that search has a positive effect on its accuracy compared to deterministic parsing .", "Finally , we show that by using a beam search decoder , we are able to combine graph based and transition based parsing into a single system , with the combined system significantly outperforming each individual system .", "In experiments with the English and Chinese Penn Treebank data , the combined parser gave 92 . 1 and 86 . 2 accuracy , respectively , which are comparable to the best parsing results for these data sets , while the Chinese accuracy outperforms the previous best reported by 1 . 8 .", "In line with previous work on dependency parsing using the Penn Treebank , we focus on projective dependency parsing .", "Following MSTParser McDonald et al . , 2005 ; McDonald and Pereira , 2006 , we define the graphVariables agenda the beam for state items item partial parse tree output a set of output items index , prev word indexes Input x POS tagged input sentence .", "Initialization agenda put the best items from output to agenda Output the best item in agenda based parsing problem as finding the highest scoring tree y from all possible outputs given an input x where GEN x denotes the set of possible parses for the input x .", "To repeat our earlier comments , in this paper we do not consider the method of finding the arg max to be part of the definition of graph based parsing , only the fact that the dependency graph itself is being scored , and factored into scores attached to the dependency links .", "The score of an output parse y is given by a linear model where 4b y is the global feature vector from y and w is the weight vector of the model .", "We use the discriminative perceptron learning algorithm Collins , 2002 ; McDonald et al . , 2005 to train the values of w . The algorithm is shown in Figure 1 .", "Averaging parameters is a way to reduce overfitting for perceptron training Collins , 2002 , and is applied to all our experiments .", "While the MSTParser uses exact inference Eisner , 1996 , we apply beam search to decoding .", "This is done by extending the deterministic Covington algorithm for projective dependency parsing Covington , 2001 .", "As shown in Figure 2 , the decoder works incrementally , building a state item i . e . partial parse tree word by word .", "When each word is processed , links are added between the current word and its predecessors .", "Beam search is applied by keeping the B best items in the agenda at each processing stage , while partial candidates are compared by scores from the graph based model , according to partial graph up to the current word .", "Before decoding starts , the agenda contains an empty sentence .", "At each processing stage , existing partial candidates from the agenda are extended in all possible ways according to the Covington algorithm .", "The top B newly generated candidates are then put to the agenda .", "After all input words are processed , the best candidate output from the agenda is taken as the final output .", "The projectivity of the output dependency trees is guaranteed by the incremental Covington process .", "The time complexity of this algorithm is O n2 , where n is the length of the input sentence .", "During training , the early update strategy of Collins and Roark 2004 is used when the correct state item falls out of the beam at any stage , parsing is stopped immediately , and the model is updated using the current best partial item .", "The intuition is to improve learning by avoiding irrelevant information when all the items in the current agenda are incorrect , further parsing steps will be irrelevant because the correct partial output no longer exists in the candidate ranking .", "Table 1 shows the feature templates from the MSTParser McDonald and Pereira , 2006 , which are defined in terms of the context of a word , its parent and its sibling .", "To give more templates , features from templates 1 5 are also conjoined with the link direction and distance , while features from template 6 are also conjoined with the direction and distance between the child and its sibling .", "Here distance refers to the difference between word indexes .", "We apply all these feature templates to the graph based parser .", "In addition , we define two extra feature templates Table 2 that capture information about grandchildren and arity i . e . the number of children to the left or right .", "These features are not conjoined with information about direction and distance .", "They are difficult to include in an efficient dynamic programming decoder , but easy to include in a beam search decoder .", "We develop our transition based parser using the transition model of the MaltParser Nivre et al . , 2006 , which is characterized by the use of a stack and four transition actions Shift , ArcRight , ArcLeft and Reduce .", "An input sentence is processed from left to right , with an index maintained for the current word .", "Initially empty , the stack is used throughout the parsing process to store unfinished words , which are the words before the current word that may still be linked with the current or a future word .", "The Shift action pushes the current word to the stack and moves the current index to the next word .", "The ArcRight action adds a dependency link from the stack top to the current word i . e . the stack top becomes the parent of the current word , pushes the current word on to the stack , and moves the current index to the next word .", "The ArcLeft action adds a dependency link from the current word to the stack top , and pops the stack .", "The Reduce action pops the stack .", "Among the four transition actions , Shift and ArcRight push a word on to the stack while ArcLeft and Reduce pop the stack ; Shift and ArcRight read the next input word while ArcLeft and ArcRight add a link to the output .", "By repeated application of these actions , the parser reads through the input and builds a parse tree .", "The MaltParser works deterministically .", "At each step , it makes a single decision and chooses one of the four transition actions according to the current context , including the next input words , the stack and the existing links .", "As illustrated in Figure 3 , the contextual information consists of the top of stack ST , the parent STP of ST , the leftmost STLC and rightmost child STRC of ST , the current word N0 , the next three words from the input N1 , N2 , N3 and the leftmost child of N0 N0LC .", "Given the context s , the next action T is decided as follows where ACTION Shift , ArcRight , ArcLeft , Reduce .", "One drawback of deterministic parsing is error propagation , since once an incorrect action is made , the output parse will be incorrect regardless of the subsequent actions .", "To reduce such error propagation , a parser can keep track of multiple candidate outputs and avoid making decisions too early .", "Suppose that the parser builds a set of candidates GEN x for the input x , the best output F x can be decided by considering all actions Here T0 represents one action in the sequence act y by which y is built , and sT' represents the corresponding context when T0 is taken .", "Our transition based algorithm keeps B different sequences of actions in the agenda , and chooses the one having the overall best score as the final parse .", "Pseudo code for the decoding algorithm is shown in Figure 4 .", "Here each state item contains a partial parse tree as well as a stack configuration , and state items are built incrementally by transition actions .", "Initially the stack is empty , and the agenda contains an empty sentence .", "At each processing stage , one transition action is applied to existing state items as a step to build the final parse .", "Unlike the MaltParser , which makes a decision at each stage , our transitionbased parser applies all possible actions to each existing state item in the agenda to generate new items ; then from all the newly generated items , it takes the B with the highest overall score and puts them onto the agenda .", "In this way , some ambiguity is retained for future resolution .", "Note that the number of transition actions needed to build different parse trees can vary .", "For example , the three word sentence A B C can be parsed by the sequence of three actions Shift ArcRight ArcRight B modifies A ; C modifies B or the sequence of four actions Shift ArcLeft Shift ArcRight both A and C modifies B .", "To ensure that all final state items are built by the same number of transition actions , we require that the final state transfer the best items from output to agenda Output the best item in agenda items must 1 have fully built parse trees ; and 2 have only one root word left on the stack .", "In this way , popping actions should be made even after a complete parse tree is built , if the stack still contains more than one word .", "Now because each word excluding the root must be pushed to the stack once and popped off once during the parsing process , the number of actions Inputs training examples xi , yi Initialization set w 0 needed to parse a sentence is always 2n 1 , where n is the length of the sentence .", "Therefore , the decoder has linear time complexity , given a fixed beam size .", "Because the same transition actions as the MaltParser are used to build each item , the projectivity of the output dependency tree is ensured .", "We use a linear model to score each transition action , given a context N0t , but not STwt or STwN0w , we combine features manually .", "As with the graph based parser , we use the discriminative perceptron Collins , 2002 to train the transition based model see Figure 5 .", "It is worth noticing that , in contrast to MaltParser , which trains each action decision individually , our training algorithm globally optimizes all action decisions for a parse .", "Again , early update and averaging parameters are applied to the training process .", "The graph based and transition based approaches adopt very different views of dependency parsing .", "McDonald and Nivre 2007 showed that the MSTParser and MaltParser produce different errors .", "This observation suggests a combined approach by using both graph based information and transition based information , parsing accuracy can be improved .", "The beam search framework we have developed facilitates such a combination .", "Our graph based and transition based parsers share many similarities .", "Both build a parse tree incrementally , keeping an agenda of comparable state items .", "Both rank state items by their current scores , and use the averaged perceptron with early update for training .", "The key differences are the scoring models and incremental parsing processes they use , which must be addressed when combining the parsers .", "Firstly , we combine the graph based and the transition based score models simply by summation .", "This is possible because both models are global and linear .", "In particular , the transition based model can be written as If we take ET0 act y \u03a6 T0 , sT0 as the global feature vector \u03a6T y , we have which has the same form as the graph based model ScoreG y \u03a6G y wG We therefore combine the two models to give Concatenating the feature vectors \u03a6G y and \u03a6T y to give a global feature vector \u03a6C y , and the weight vectors wG and wT to give a weight vector wC , the combined model can be written as which is a linear model with exactly the same form as both sub models , and can be trained with the perceptron algorithm in Figure 1 .", "Because the global feature vectors from the sub models are concatenated , the feature set for the combined model is the union of the sub model feature sets .", "Second , the transition based decoder can be used for the combined system .", "Both the graph based decoder in Figure 2 and the transition based decoder in Figure 4 construct a parse tree incrementally .", "However , the graph based decoder works on a per word basis , adding links without using transition actions , and so is not appropriate for the combined model .", "The transition based algorithm , on the other hand , uses state items which contain partial parse trees , and so provides all the information needed by the graph based parser i . e . dependency graphs , and hence the combined system .", "In summary , we build the combined parser by using a global linear model , the union of feature templates and the decoder from the transition based parser .", "We evaluate the parsers using the English and Chinese Penn Treebank corpora .", "The English data is prepared by following McDonald et al . 2005 .", "Bracketed sentences from the Penn Treebank PTB 3 are split into training , development and test sets as shown in Table 4 , and then translated into dependency structures using the head finding rules from Yamada and Matsumoto 2003 .", "Before parsing , POS tags are assigned to the input sentence using our reimplementation of the POStagger from Collins 2002 .", "Like McDonald et al . 2005 , we evaluate the parsing accuracy by the precision of lexical heads the percentage of input words , excluding punctuation , that have been assigned the correct parent and by the percentage of complete matches , in which all words excluding punctuation have been assigned the correct parent .", "Since the beam size affects all three parsers , we study its influence first ; here we show the effect on the transition based parser .", "Figure 6 shows different accuracy curves using the development data , each with a different beam size B .", "The X axis represents the number of training iterations , and the Y axis the precision of lexical heads .", "The parsing accuracy generally increases as the beam size increases , while the quantity of increase becomes very small when B becomes large enough .", "The decoding times after the first training iteration are 10 . 2s , 27 . 3s , 45 . 5s , 79 . 0s , 145 . 4s , 261 . 3s and 469 . 5s , respectively , when B 1 , 2 , 4 , 8 , 16 , 32 , 64 .", "In the rest of the experiments , we set B 64 in order to obtain the highest possible accuracy .", "When B 1 , the transition based parser becomes a deterministic parser .", "By comparing the curves when B 1 and B 2 , we can see that , while the use of search reduces the parsing speed , it improves the quality of the output parses .", "Therefore , beam search is a reasonable choice for transitionbased parsing .", "The test accuracies are shown in Table 5 , where each row represents a parsing model .", "Rows MSTParser 1 2 show the first order using feature templates 1 5 from Table 1 McDonald et al . , 2005 and secondorder using all feature templates from Table 1 McDonald and Pereira , 2006 MSTParsers , as reported by the corresponding papers .", "Rows Graph M and Graph MA represent our graph based parser using features from Table 1 and Table 1 Table 2 , respectively ; row Transition represents our transition based parser ; and rows Combined TM and Combined TMA represent our combined parser using features from Table 3 Table 1 and Table 3 Table 1 Table 2 , respectively .", "Columns Word and Complete show the precision of lexical heads and complete matches , respectively .", "As can be seen from the table , beam search reduced the head word accuracy from 91 . 5 42 . 1 MSTParser 2 to 91 . 2 40 . 8 Graph M with the same features as exact inference .", "However , with only two extra feature templates from Table 2 , which are not conjoined with direction or distance information , the accuracy is improved to 91 . 4 42 . 5 Graph MA .", "This improvement can be seen as a benefit of beam search , which allows the definition of more global features .", "The combined parser is tested with various sets of features .", "Using only graph based features in Table 1 , it gave 88 . 6 accuracy , which is much lower than 91 . 2 from the graph based parser using the same features Graph M .", "This can be explained by the difference between the decoders .", "In particular , the graph based model is unable to score the actions Reduce and Shift , since they do not modify the parse tree .", "Nevertheless , the score serves as a reference for the effect of additional features in the combined parser .", "Using both transition based features and graphbased features from the MSTParser Combined TM , the combined parser achieved 92 . 0 perword accuracy , which is significantly higher than the pure graph based and transition based parsers .", "Additional graph based features further improved the accuracy to 92 . 1 45 . 5 , which is the best among all the parsers compared . 1 We use the Penn Chinese Treebank CTB 5 for experimental data .", "Following Duan et al . 2007 , we 1A recent paper , Koo et al .", "2008 reported parent prediction accuracy of 92 . 0 using a graph based parser with a different larger set of features Carreras , 2007 .", "By applying separate word cluster information , Koo et al . 2008 improved the accuracy to 93 . 2 , which is the best known accuracy on the PTB data .", "We excluded these from Table 5 because our work is not concerned with the use of such additional knowledge . split the corpus into training , development and test data as shown in Table 6 , and use the head finding rules in Table 8 in the Appendix to turn the bracketed sentences into dependency structures .", "Most of the head finding rules are from Sun and Jurafsky 2004 , while we added rules to handle NN and FRAG , and a default rule to use the rightmost node as the head for the constituent that are not listed .", "Like Duan et al . 2007 , we use gold standard POS tags for the input .", "The parsing accuracy is evaluated by the percentage of non root words that have been assigned the correct head , the percentage of correctly identified root words , and the percentage of complete matches , all excluding punctuation .", "The accuracies are shown in Table 7 .", "Rows Graph MA , Transition , Combined TM and Combined TMA show our models in the same way as for the English experiments from Section 5 . 2 .", "Row Duan 2007 represents the transition based model from Duan et al . 2007 , which applies beamsearch to the deterministic model from Yamada and Matsumoto 2003 , and achieved the previous best accuracy on the data .", "Our observations on parsing Chinese are essentially the same as for English .", "Our combined parser outperforms both the pure graph based and the pure transition based parsers .", "It gave the best accuracy we are aware of for dependency parsing using CTB .", "Our graph based parser is derived from the work of McDonald and Pereira 2006 .", "Instead of performing exact inference by dynamic programming , we incorporated the linear model and feature templates from McDonald and Pereira 2006 into our beam search framework , while adding new global features .", "Nakagawa 2007 and Hall 2007 also showed the effectiveness of global features in improving the accuracy of graph based parsing , using the approximate Gibbs sampling method and a reranking approach , respectively .", "Our transition based parser is derived from the deterministic parser of Nivre et al . 2006 .", "We incorporated the transition process into our beamsearch framework , in order to study the influence of search on this algorithm .", "Existing efforts to add search to deterministic parsing include Sagae and Lavie 2006b , which applied best first search to constituent parsing , and Johansson and Nugues 2006 and Duan et al . 2007 , which applied beamsearch to dependency parsing .", "All three methods estimate the probability of each transition action , and score a state item by the product of the probabilities of all its corresponding actions .", "But different from our transition based parser , which trains all transitions for a parse globally , these models train the probability of each action separately .", "Based on the work of Johansson and Nugues 2006 , Johansson and Nugues 2007 studied global training with an approximated large margin algorithm .", "This model is the most similar to our transition based model , while the differences include the choice of learning and decoding algorithms , the definition of feature templates and our application of the early update strategy .", "Our combined parser makes the biggest contribution of this paper .", "In contrast to the models above , it includes both graph based and transition based components .", "An existing method to combine multiple parsing algorithms is the ensemble approach Sagae and Lavie , 2006a , which was reported to be useful in improving dependency parsing Hall et al . , 2007 .", "A more recent approach Nivre and McDonald , 2008 combined MSTParser and MaltParser by using the output of one parser for features in the other .", "Both Hall et al . 2007 and Nivre and McDonald 2008 can be seen as methods to combine separately defined models .", "In contrast , our parser combines two components in a single model , in which all parameters are trained consistently .", "We developed a graph based and a transition based projective dependency parser using beam search , demonstrating that beam search is a competitive choice for both parsing approaches .", "We then combined the two parsers into a single system , using discriminative perceptron training and beam search decoding .", "The appealing aspect of the combined parser is the incorporation of two largely different views of the parsing problem , thus increasing the information available to a single statistical parser , and thereby significantly increasing the accuracy .", "When tested using both English and Chinese dependency data , the combined parser was highly competitive compared to the best systems in the literature .", "The idea of combining different approaches to the same problem using beam search and a global model could be applied to other parsing tasks , such as constituent parsing , and possibly other NLP tasks .", "This work is supported by the ORS and Clarendon Fund .", "We thank the anonymous reviewers for their detailed comments ."], "summary_lines": ["A Tale of Two Parsers: Investigating and Combining Graph-based and Transition-based Dependency Parsing\n", "Graph-based and transition-based approaches to dependency parsing adopt very different views of the problem, each view having its own strengths and limitations.\n", "We study both approaches under the framework of beam-search.\n", "By developing a graph-based and a transition-based dependency parser, we show that a beam-search decoder is a competitive choice for both methods.\n", "More importantly, we propose a beam-search-based parser that combines both graph-based and transition-based parsing into a single system for training and decoding, showing that it outperforms both the pure graph-based and the pure transition-based parsers.\n", "Testing on the English and Chinese Penn Treebank data, the combined system gave state-of-the-art accuracies of 92.1% and 86.2%, respectively.\n", "We define head rules to convert phrase structures into dependency structures.\n", "We combine beam search with a globally normalized discriminative model, using structured perceptron learning and the early update strategy of Collins and Roark (2004), and also explore the addition of graph based features to a transition-based parser.\n"]}
{"article_lines": ["Forest Reranking Discriminative Parsing with Non Local Features", "reranking techniques ofsuffer from the limited scope of the best list , which rules out many potentially alternatives .", "We instead propose a method that reranks a packed forest of exponentially many parses .", "Since exact inference is intractable with non local features , we present an approximate algorithm inspired by forest rescoring that makes discriminative training practical over the whole Treebank .", "Our final result , an F score of 91 . 7 , outperforms both 50 best and 100 best reranking baselines , and is better than any previously reported systems trained on the Treebank .", "Discriminative reranking has become a popular technique for many NLP problems , in particular , parsing Collins , 2000 and machine translation Shen et al . , 2005 .", "Typically , this method first generates a list of top n candidates from a baseline system , and then reranks this n best list with arbitrary features that are not computable or intractable to compute within the baseline system .", "But despite its apparent success , there remains a major drawback this method suffers from the limited scope of the nbest list , which rules out many potentially good alternatives .", "For example 41 of the correct parses were not in the candidates of 30 best parses in Collins , 2000 .", "This situation becomes worse with longer sentences because the number of possible interpretations usually grows exponentially with the sentence length .", "As a result , we often see very few variations among the n best trees , for example , 50best trees typically just represent a combination of 5 to 6 binary ambiguities since 25 50 26 .", "Alternatively , discriminative parsing is tractable with exact and efficient search based on dynamic programming DP if all features are restricted to be local , that is , only looking at a local window within the factored search space Taskar et al . , 2004 ; McDonald et al . , 2005 .", "However , we miss the benefits of non local features that are not representable here .", "Ideally , we would wish to combine the merits of both approaches , where an efficient inference algorithm could integrate both local and non local features .", "Unfortunately , exact search is intractable at least in theory for features with unbounded scope .", "So we propose forest reranking , a technique inspired by forest rescoring Huang and Chiang , 2007 that approximately reranks the packed forest of exponentially many parses .", "The key idea is to compute non local features incrementally from bottom up , so that we can rerank the n best subtrees at all internal nodes , instead of only at the root node as in conventional reranking see Table 1 .", "This method can thus be viewed as a step towards the integration of discriminative reranking with traditional chart parsing .", "Although previous work on discriminative parsing has mainly focused on short sentences 15 words Taskar et al . , 2004 ; Turian and Melamed , 2007 , our work scales to the whole Treebank , where only at the root exact N A we achieved an F score of 91 . 7 , which is a 19 error reduction from the 1 best baseline , and outperforms both 50 best and 100 best reranking .", "This result is also better than any previously reported systems trained on the Treebank .", "Informally , a packed parse forest , or forest in short , is a compact representation of all the derivations i . e . , parse trees for a given sentence under a context free grammar Billot and Lang , 1989 .", "For example , consider the following sentence 0 I 1 saw 2 him 3 with 4 a 5 mirror 6 where the numbers between words denote string positions .", "Shown in Figure 1 , this sentence has at least two derivations depending on the attachment of the prep . phrase PP3 , 6 with a mirror it can either be attached to the verb saw , or be attached to him , which will be further combined with the verb to form the same VP as above .", "These two derivations can be represented as a single forest by sharing common sub derivations .", "Such a forest has a structure of a hypergraph Klein and Manning , 2001 ; Huang and Chiang , 2005 , where items like PP3 , 6 are called nodes , and deductive steps like correspond to hyperedges .", "More formally , a forest is a pair V , E , where V is the set of nodes , and E the set of hyperedges .", "For a given sentence w1 l w1 . . . wl , each node v E V is in the form of Xz , j , which denotes the recognition of nonterminal X spanning the substring from positions i through j that is , wz 1 . . . wj .", "Each hyperedge e E E is a pair tails e , head e , where head e E V is the consequent node in the deductive step , and tails e E V is the list of antecedent nodes .", "For example , the hyperedge for deduction is notated We also denote IN v to be the set of incoming hyperedges of node v , which represent the different ways of deriving v . For example , in the forest in Figure 1 , IN VP1 , 6 is e1 , e2 , with e2 VBD1 , 2 , NP2 , 6 , VP1 , 6 .", "We call jej the arity of hyperedge e , which counts the number of tail nodes in e . The arity of a hypergraph is the maximum arity over all hyperedges .", "A CKY forest has an arity of 2 , since the input grammar is required to be binary branching cf .", "Chomsky Normal Form to ensure cubic time parsing complexity .", "However , in this work , we use forests from a Treebank parser Charniak , 2000 whose grammar is often flat in many productions .", "For example , the arity of the forest in Figure 1 is 3 .", "Such a Treebank style forest is easier to work with for reranking , since many features can be directly expressed in it .", "There is also a distinguished root node TOP in each forest , denoting the goal item in parsing , which is simply S0 , l where S is the start symbol and l is the sentence length .", "We first establish a unified framework for parse reranking with both n best lists and packed forests .", "For a given sentence s , a generic reranker selects the best parse y among the set of candidates cand s according to some scoring function In n best reranking , cand s is simply a set of n best parses from the baseline parser , that is , cand s y1 , y2 , . . . , yn .", "Whereas in forest reranking , cand s is a forest implicitly representing the set of exponentially many parses .", "As usual , we define the score of a parse y to be the dot product between a high dimensional feature representation and a weight vector w where the feature extractor f is a vector of d functions f f1 , . . . , fd , and each feature fj maps a parse y to a real number fj y .", "Following Charniak and Johnson , 2005 , the first feature f1 y log Pr y is the log probability of a parse from the baseline generative parser , while the remaining features are all integer valued , and each of them counts the number of times that a particular configuration occurs in parse y .", "For example , one such feature f2000 might be a question how many times is a VP of length 5 surrounded by the word has and the period ?", "which is an instance of the WordEdges feature see Figure 2 c and Section 3 . 2 for details .", "Using a machine learning algorithm , the weight vector w can be estimated from the training data where each sentence si is labelled with its correct gold standard parse y i .", "As for the learner , Collins 2000 uses the boosting algorithm and Charniak and Johnson 2005 use the maximum entropy estimator .", "In this work we use the averaged perceptron algorithm Collins , 2002 since it is an online algorithm much simpler and orders of magnitude faster than Boosting and MaxEnt methods .", "Shown in Pseudocode 1 , the perceptron algorithm makes several passes over the whole training data , and in each iteration , for each sentence si , it tries to predict a best parse yi among the candidates cand si using the current weight setting .", "Intuitively , we want the gold parse y i to be picked , but in general it is not guaranteed to be within cand si , because the grammar may fail to cover the gold parse , and because the gold parse may be pruned away due to the limited scope of cand si .", "So we define an oracle parse yz to be the candidate that has the highest Parseval F score with respect to the gold tree y i 1 where function F returns the F score .", "Now we train the reranker to pick the oracle parses as often as possible , and in case an error is made line 6 , perform an update on the weight vector line 7 , by adding the difference between two feature representations .", "1If one uses the gold y i for oracle yz , the perceptron will continue to make updates towards something unreachable even when the decoder has picked the best possible candidate .", "Pseudocode 1 Perceptron for Generic Reranking In n best reranking , since all parses are explicitly enumerated , it is trivial to compute the oracle tree . 2 However , it remains widely open how to identify the forest oracle .", "We will present a dynamic programming algorithm for this problem in Sec .", "We also use a refinement called averaged parameters where the final weight vector is the average of weight vectors after each sentence in each iteration over the training data .", "This averaging effect has been shown to reduce overfitting and produce much more stable results Collins , 2002 .", "A key difference between n best and forest reranking is the handling of features .", "In n best reranking , all features are treated equivalently by the decoder , which simply computes the value of each one on each candidate parse .", "However , for forest reranking , since the trees are not explicitly enumerated , many features can not be directly computed .", "So we first classify features into local and non local , which the decoder will process in very different fashions .", "We define a feature f to be local if and only if it can be factored among the local productions in a tree , and non local if otherwise .", "For example , the Rule feature in Fig .", "2 a is local , while the ParentRule feature in Fig .", "2 b is non local .", "It is worth noting that some features which seem complicated at the first sight are indeed local .", "For example , the WordEdges feature in Fig .", "2 c , which classifies a node by its label , span length , and surrounding words , is still local since all these information are encoded either in the node itself or in the input sentence .", "In contrast , it would become non local if we replace the surrounding words by surrounding POS 2In case multiple candidates get the same highest F score , we choose the parse with the highest log probability from the baseline parser to be the oracle parse Collins , 2000 . tags , which are generated dynamically .", "More formally , we split the feature extractor f f1 , . . . , fd into f fL ; fN where fL and fN are the local and non local features , respectively .", "For the former , we extend their domains from parses to hyperedges , where f e returns the value of a local feature f E fL on hyperedge e , and its value on a parsey factors across the hyperedges local productions , and we can pre compute fL e for each e in a forest .", "Non local features , however , can not be precomputed , but we still prefer to compute them as early as possible , which we call on the fly computation , so that our decoder can be sensitive to them at internal nodes .", "For instance , the NGramTree feature in Fig .", "2 d returns the minimum tree fragement spanning a bigram , in this case saw and the , and should thus be computed at the smallest common ancestor of the two , which is the VP node in this example .", "Similarly , the ParentRule feature in Fig .", "2 b can be computed when the S subtree is formed .", "In doing so , we essentially factor non local features across subtrees , where for each subtree y in a parse y , we define a unit feature f y to be the part of f y that are computable within y , but not computable in any proper subtree of y .", "Then we have Intuitively , we compute the unit non local features at each subtree from bottom up .", "For example , for the binary branching node Ai , k in Fig .", "3 , the unit NGramTree instance is for the pair wj 1 , wj on the boundary between the two subtrees , whose smallest common ancestor is the current node .", "Other unit NGramTree instances within this span have already been computed in the subtrees , except those for the boundary words of the whole node , wi and wk 1 , which will be computed when this node is further combined with other nodes in the future .", "Before moving on to approximate decoding with non local features , we first describe the algorithm for exact decoding when only local features are present , where many concepts and notations will be re used later .", "We will use D v to denote the top derivations of node v , where D1 v is its 1 best derivation .", "We also use the notation e , j to denote the derivation along hyperedge e , using the jith subderivation for tail ui , so e , 1 is the best derivation along e . The exact decoding algorithm , shown in Pseudocode 2 , is an instance of the bottom up Viterbi algorithm , which traverses the hypergraph in a topological order , and at each node v , calculates its 1 best derivation using each incoming hyperedge e E IN v .", "The cost of e , c e , is the score of its Pseudocode 2 Exact Decoding with Local Features Pseudocode 3 Cube Pruning for Non local Features pre computed local features w fL e .", "This algorithm has a time complexity of O E , and is almost identical to traditional chart parsing , except that the forest might be more than binary branching .", "For non local features , we adapt cube pruning from forest rescoring Chiang , 2007 ; Huang and Chiang , 2007 , since the situation here is analogous to machine translation decoding with integrated language models we can view the scores of unit nonlocal features as the language model cost , computed on the fly when combining sub constituents .", "Shown in Pseudocode 3 , cube pruning works bottom up on the forest , keeping a beam of at most k derivations at each node , and uses the k best parsing Algorithm 2 of Huang and Chiang 2005 to speed up the computation .", "When combining the subderivations along a hyperedge e to form a new subtree y e , j , we also compute its unit non local feature values fN e , j line 25 .", "A priority queue heap in Pseudocode 3 is used to hold the candidates for the next best derivation , which is initialized to the set of best derivations along each hyperedge lines 7 to 9 .", "Then at each iteration , we pop the best derivation lines 12 , and push its successors back into the priority queue line 14 .", "Analogous to the language model cost in forest rescoring , the unit feature cost here is a non monotonic score in the dynamic programming backbone , and the derivations may thus be extracted out of order .", "So a buffer buf is used to hold extracted derivations , which is sorted at the end line 15 to form the list of top k derivations D v of node v . The complexity of this algorithm is O E V k log kJV Huang and Chiang , 2005 , where O JV is the time for on the fly feature extraction for each subtree , which becomes the bottleneck in practice .", "Recall that the Parseval F score is the harmonic mean of labelled precision P and labelled recall R where y and y are the numbers of brackets in the test parse and gold parse , respectively , and y n y is the number of matched brackets .", "Since the harmonic mean is a non linear combination , we can not optimize the F scores on sub forests independently with a greedy algorithm .", "In other words , the optimal F score tree in a forest is not guaranteed to be composed of two optimal F score subtrees .", "We instead propose a dynamic programming algorithm which optimizes the number of matched brackets for a given number of test brackets .", "For example , our algorithm will ask questions like , when a test parse has 5 brackets , what is the maximum number of matched brackets ? More formally , at each node v , we compute an oracle function ora v N H N , which maps an integer t to ora v t , the max . number of matched brackets Pseudocode 4 Forest Oracle Algorithm When node v is combined with another node u along a hyperedge e v , u , w , we need to combine the two oracle functions ora v and ora u by distributing the test brackets of w between v and u , and optimize the number of matched bracktes .", "To do this we define a convolution operator between two functions f and g where 1 is the indicator function , returning 1 if node w is found in the gold tree y , in which case we increment the number of matched brackets .", "We can also express Eq .", "9 in a purely functional form where 1 is a translation operator which shifts a function along the axes Above we discussed the case of one hyperedge .", "If there is another hyperedge e deriving node w , we also need to combine the resulting oracle functions from both hyperedges , for which we define a pointwise addition operator Shown in Pseudocode 4 , we perform these computations in a bottom up topological order , and finally at the root node TOP , we can compute the best global F score by maximizing over different numbers of test brackets line 7 .", "The oracle tree y can be recursively restored by keeping backpointers for each ora v t , which we omit in the pseudocode .", "The time complexity of this algorithm for a sentence of l words is O JEJ l2 a 1 where a is the arity of the forest .", "For a CKY forest , this amounts to O l3 l2 O l5 , but for general forests like those in our experiments the complexities are much higher .", "In practice it takes on average 0 . 05 seconds for forests pruned by p 10 see Section 4 . 2 , but we can pre compute and store the oracle for each forest before training starts .", "Our forest pruning algorithm Jonathan Graehl , p . c . is very similar to the method based on marginal probability Charniak and Johnson , 2005 , except that ours prunes hyperedges as well as nodes .", "Basically , we use an Inside Outside algorithm to compute the Viterbi inside cost Q v and the Viterbi outside cost a v for each node v , and then compute the merit aQ e for each hyperedge Intuitively , this merit is the cost of the best derivation that traverses e , and the difference S e aQ e Q TOP can be seen as the distance away from the globally best derivation .", "We prune away all hyperedges that have S e p for a threshold p . Nodes with all incoming hyperedges pruned are also pruned .", "The key difference from Charniak and Johnson , 2005 is that in this algorithm , a node can partially survive the beam , with a subset of its hyperedges pruned .", "In practice , this method prunes on average 15 more hyperedges than their method .", "We compare the performance of our forest reranker against n best reranking on the Penn English Treebank Marcus et al . , 1993 .", "The baseline parser is the Charniak parser , which we modified to output a packed forest for each sentence . 3 We use the standard split of the Treebank sections 02 21 as the training data 39832 sentences , section 22 as the development set 1700 sentences , and section 23 as the test set 2416 sentences .", "Following Charniak and Johnson , 2005 , the training set is split into 20 folds , each containing about 1992 sentences , and is parsed by the Charniak parser with a model trained on sentences from the remaining 19 folds .", "The development set and the test set are parsed with a model trained on all 39832 training sentences .", "We implemented both n best and forest reranking systems in Python and ran our experiments on a 64bit Dual Core Intel Xeon with 3 . 0GHz CPUs .", "Our feature set is summarized in Table 2 , which closely follows Charniak and Johnson 2005 , except that we excluded the non local features Edges , NGram , and CoPar , and simplified Rule and NGramTree features , since they were too complicated to compute . 4 We also added four unlexicalized local features from Collins 2000 to cope with data sparsity .", "Following Charniak and Johnson 2005 , we extracted the features from the 50 best parses on the training set sec .", "02 21 , and used a cut off of 5 to prune away low count features .", "There are 0 . 8M features in our final set , considerably fewer than that of Charniak and Johnson which has about 1 . 3M features in the updated version . 5 However , our initial experiments show that , even with this much simpler feature set , our 50 best reranker performed equally well as theirs both with an F score of 91 . 4 , see Tables 3 and 4 .", "This result confirms that our feature set design is appropriate , and the averaged perceptron learner is a reasonable candidate for reranking .", "The forests dumped from the Charniak parser are huge in size , so we use the forest pruning algorithm in Section 4 . 2 to prune them down to a reasonable size .", "In the following experiments we use a threshold of p 10 , which results in forests with an average number of 123 . 1 hyperedges per forest .", "Then for each forest , we annotate its forest oracle , and on each hyperedge , pre compute its local features . 6 Shown in Figure 4 , these forests have an forest oracle of 97 . 8 , which is 1 . 1 higher than the 50 best oracle 96 . 7 , and are 8 times smaller in size .", "Table 3 compares the performance of forest reranking against standard n best reranking .", "For both systems , we first use only the local features , and then all the features .", "We use the development set to determine the optimal number of iterations for averaged perceptron , and report the F1 score on the test set .", "With only local features , our forest reranker achieves an F score of 91 . 25 , and with the addition of noning on sec .", "The pre comp . column is for feature extraction , and training column shows the number of perceptron iterations that achieved best results on the dev set , and average time per iteration . local features , the accuracy rises to 91 . 69 with beam size k 15 , which is a 0 . 26 absolute improvement over 50 best reranking . 7 This improvement might look relatively small , but it is much harder to make a similar progress with n best reranking .", "For example , even if we double the size of the n best list to 100 , the performance only goes up by 0 . 06 Table 3 .", "In fact , the 100best oracle is only 0 . 5 higher than the 50 best one see Fig .", "In addition , the feature extraction step in 100 best reranking produces huge data files and takes 44 hours in total , though this part can be parallelized . 8 On two CPUs , 100 best reranking takes 25 hours , while our forest reranker can also finish in 26 hours , with a much smaller disk space .", "Indeed , this demonstrates the severe redundancies as another disadvantage of n best lists , where many subtrees are repeated across different parses , while the packed forest reduces space dramatically by sharing common sub derivations see Fig .", "To put our results in perspective , we also compare them with other best performing systems in Table 4 .", "Our final result 91 . 7 is better than any previously reported system trained on the Treebank , although 7It is surprising that 50 best reranking with local features achieves an even higher F score of 91 . 28 , and we suspect this is due to the aggressive updates and instability of the perceptron , as we do observe the learning curves to be non monotonic .", "We leave the use of more stable learning algorithms to future work . best performing systems on the whole Section 23 .", "Types D , G , and S denote discriminative , generative , and semi supervised approaches , respectively .", "McClosky et al . 2006 achieved an even higher accuarcy 92 . 1 by leveraging on much larger unlabelled data .", "Moreover , their technique is orthogonal to ours , and we suspect that replacing their n best reranker by our forest reranker might get an even better performance .", "Plus , except for n best reranking , most discriminative methods require repeated parsing of the training set , which is generally impratical Petrov and Klein , 2008 .", "Therefore , previous work often resorts to extremely short sentences 15 words or only looked at local features Taskar et al . , 2004 ; Henderson , 2004 ; Turian and Melamed , 2007 .", "In comparison , thanks to the efficient decoding , our work not only scaled to the whole Treebank , but also successfully incorporated non local features , which showed an absolute improvement of 0 . 44 over that of local features alone .", "We have presented a framework for reranking on packed forests which compactly encodes many more candidates than n best lists .", "With efficient approximate decoding , perceptron training on the whole Treebank becomes practical , which can be done in about a day even with a Python implementation .", "Our final result outperforms both 50 best and 100 best reranking baselines , and is better than any previously reported systems trained on the Treebank .", "We also devised a dynamic programming algorithm for forest oracles , an interesting problem by itself .", "We believe this general framework could also be applied to other problems involving forests or lattices , such as sequence labeling and machine translation ."], "summary_lines": ["Forest Reranking: Discriminative Parsing with Non-Local Features\n", "Conventional n-best reranking techniques often suffer from the limited scope of the nbest list, which rules out many potentially good alternatives.\n", "We instead propose forest reranking, a method that reranks a packed forest of exponentially many parses.\n", "Since exact inference is intractable with non-local features, we present an approximate algorithm inspired by forest rescoring that makes discriminative training practical over the whole Treebank.\n", "Our final result, an F-score of 91.7, outperforms both 50-best and 100-best reranking baselines, and is better than any previously reported systems trained on the Treebank.\n", "We show that the use of non-local features does in fact contribute substantially to parser performance.\n", "To prune the packed forests, we use inside and outside probabilities to compute the distance of the best derivation that traverses a hyper edge away from the globally best derivation.\n"]}
{"article_lines": ["A Clustering Approach For Nearly Unsupervised Recognition Of Nonliteral Language", "In this paper we present TroFi Trope Finder , a system for automatically classifying literal and nonliteral usages of verbs through nearly unsupervised word sense disambiguation and clustering techniques .", "TroFi uses sentential context instead of selectional constraint violations or paths in semantic hierarchies .", "It also uses literal and nonliteral seed sets acquired and cleaned without human supervision in order to bootstrap learning .", "We adapt a word sense disambiguation algorithm to our task and augment it with multiple seed set learners , a voting schema , and additional features like SuperTags and extrasentential context .", "Detailed experiments on hand annotated data show that our enhanced algorithm outperforms the baseline by 24 . 4 .", "Using the TroFi algorithm , we also build the TroFi Example Base , an extensible resource of annotated literal nonliteral examples which is freely available to the NLP research community .", "In this paper , we propose TroFi Trope Finder , a nearly unsupervised clustering method for separating literal and nonliteral usages of verbs .", "For example , given the target verb pour , we would expect TroFi to cluster the sentence Custom demands that cognac be poured from a freshly opened bottle as literal , and the sentence Salsa and rap music pour out of the windows as nonliteral , which , indeed , it does .", "We call our method nearly unsupervised .", "See Section 3 . 1 for why we use this terminology .", "We reduce the problem of nonliteral language recognition to one of word sense disambiguation by redefining literal and nonliteral as two different senses of the same word , and we adapt an existing similarity based word sense disambiguation method to the task of separating usages of verbs into literal and nonliteral clusters .", "This paper focuses on the algorithmic enhancements necessary to facilitate this transformation from word sense disambiguation to nonliteral language recognition .", "The output of TroFi is an expandable example base of literal nonliteral clusters which is freely available to the research community .", "Many systems that use NLP methods such as dialogue systems , paraphrasing and summarization , language generation , information extraction , machine translation , etc .", "would benefit from being able to recognize nonliteral language .", "Consider an example based on a similar example from an automated medical claims processing system .", "We must determine that the sentence she hit the ceiling is meant literally before it can be marked up as an ACCIDENT claim .", "Note that the typical use of hit the ceiling stored in a list of idioms cannot help us .", "Only using the context , She broke her thumb while she was cheering for the Patriots and , in her excitement , she hit the ceiling , can we decide .", "We further motivate the usefulness of the ability to recognize literal vs . nonliteral usages using an example from the Recognizing Textual Entailment RTE 1 challenge of 2005 .", "This is just an example ; we do not compute entailments .", "In the challenge data , Pair 1959 was Kerry hit Bush hard on his conduct on the war in Iraq .", "Kerry shot Bush .", "The objective was to report FALSE since the second statement in this case is not entailed from the first one .", "In order to do this , it is crucial to know that hit is being used nonliterally in the first sentence .", "Ideally , we would like to look at TroFi as a first step towards an unsupervised , scalable , widely applicable approach to nonliteral language processing that works on real world data from any domain in any language .", "The foundations of TroFi lie in a rich collection of metaphor and metonymy processing systems everything from hand coded rule based systems to statistical systems trained on large corpora .", "Rule based systems some using a type of interlingua Russell , 1976 ; others using complicated networks and hierarchies often referred to as metaphor maps e . g .", "Fass , 1997 ; Martin , 1990 ; Martin , 1992 must be largely hand coded and generally work well on an enumerable set of metaphors or in limited domains .", "Dictionarybased systems use existing machine readable dictionaries and path lengths between words as one of their primary sources for metaphor processing information e . g .", "Dolan , 1995 .", "Corpus based systems primarily extract or learn the necessary metaphor processing information from large corpora , thus avoiding the need for manual annotation or metaphor map construction .", "Examples of such systems can be found in Murata et . al . , 2000 ; Nissim Markert , 2003 ; Mason , 2004 .", "The work on supervised metonymy resolution by Nissim Markert and the work on conceptual metaphors by Mason come closest to what we are trying to do with TroFi .", "Nissim Markert 2003 approach metonymy resolution with machine learning methods , which exploit the similarity between examples of conventional metonymy Nissim Markert , 2003 , p . 56 .", "They see metonymy resolution as a classification problem between the literal use of a word and a number of pre defined metonymy types .", "They use similarities between possibly metonymic words PMWs and known metonymies as well as context similarities to classify the PMWs .", "The main difference between the Nissim Markert algorithm and the TroFi algorithm besides the fact that Nissim Markert deal with specific types of metonymy and not a generalized category of nonliteral language is that Nissim Markert use a supervised machine learning algorithm , as opposed to the primarily unsupervised algorithm used by TroFi .", "Mason 2004 presents CorMet , a corpusbased system for discovering metaphorical mappings between concepts Mason , 2004 , p . 23 .", "His system finds the selectional restrictions of given verbs in particular domains by statistical means .", "It then finds metaphorical mappings between domains based on these selectional preferences .", "By finding semantic differences between the selectional preferences , it can articulate the higher order structure of conceptual metaphors Mason , 2004 , p . 24 , finding mappings like LIQUID MONEY .", "Like CorMet , TroFi uses contextual evidence taken from a large corpus and also uses WordNet as a primary knowledge source , but unlike CorMet , TroFi does not use selectional preferences .", "Metaphor processing has even been approached with connectionist systems storing world knowledge as probabilistic dependencies Narayanan , 1999 .", "TroFi is not a metaphor processing system .", "It does not claim to interpret metonymy and it will not tell you what a given idiom means .", "Rather , TroFi attempts to separate literal usages of verbs from nonliteral ones .", "For the purposes of this paper we will take the simplified view that literal is anything that falls within accepted selectional restrictions he was forced to eat his spinach vs . he was forced to eat his words or our knowledge of the world the sponge absorbed the water vs . the company absorbed the loss .", "Nonliteral is then anything that is not literal , including most tropes , such as metaphors , idioms , as well phrasal verbs and other anomalous expressions that cannot really be seen as literal .", "In terms of metonymy , TroFi may cluster a verb used in a metonymic expression such as I read Keats as nonliteral , but we make no strong claims about this .", "The TroFi algorithm requires a target set called original set in Karov Edelman , 1998 the set of sentences containing the verbs to be classified into literal or nonliteral and the seed sets the literal feedback set and the nonliteral feedback set .", "These sets contain feature lists consisting of the stemmed nouns and verbs in a sentence , with target or seed words and frequent words removed .", "The frequent word list 374 words consists of the 332 most frequent words in the British National Corpus plus contractions , single letters , and numbers from 0 10 .", "The target set is built using the 88 89 Wall Street Journal Corpus WSJ tagged using the Ratnaparkhi , 1996 tagger and the Bangalore Joshi , 1999 SuperTagger ; the feedback sets are built using WSJ sentences conAlgorithm 1 KE train Karov Edelman , 1998 algorithm adapted to literal nonliteral classification taining seed words extracted from WordNet and the databases of known metaphors , idioms , and expressions DoKMIE , namely Wayne Magnuson English Idioms Sayings Slang and George Lakoff s Conceptual Metaphor List , as well as example sentences from these sources .", "See Section 4 for the sizes of the target and feedback sets .", "One may ask why we need TroFi if we have databases like the DoKMIE .", "The reason is that the DoKMIE are unlikely to list all possible instances of nonliteral language and because knowing that an expression can be used nonliterally does not mean that you can tell when it is being used nonliterally .", "The target verbs may not , and typically do not , appear in the feedback sets .", "In addition , the feedback sets are noisy and not annotated by any human , which is why we call TroFi unsupervised .", "When we use WordNet as a source of example sentences , or of seed words for pulling sentences out of the WSJ , for building the literal feedback set , we cannot tell if the WordNet synsets , or the collected feature sets , are actually literal .", "We provide some automatic methods in Section 3 . 3 to ensure that the feedback set feature sets that will harm us in the clustering phase are removed .", "As a sideeffect , we may fill out sparse nonliteral sets .", "In the next section we look at the Core TroFi algorithm and its use of the above data sources .", "Since we are attempting to reduce the problem of literal nonliteral recognition to one of word sense disambiguation , TroFi makes use of an existing similarity based word sense disambiguation algorithm developed by Karov Edelman , 1998 , henceforth KE .", "The KE algorithm is based on the principle of attraction similarities are calculated between sentences containing the word we wish to disambiguate the target word and collections of seed sentences feedback sets see also Section 3 . 1 .", "A target set sentence is considered to be attracted to the feedback set containing the sentence to which it shows the highest similarity .", "Two sentences are similar if they contain similar words and two words are similar if they are contained in similar sentences .", "The resulting transitive similarity allows us to defeat the knowledge acquisition bottleneck i . e . the low likelihood of finding all possible usages of a word in a single corpus .", "Note that the KE algorithm concentrates on similarities in the way sentences use the target literal or nonliteral word , not on similarities in the meanings of the sentences themselves .", "Algorithms 1 and 2 summarize the basic TroFi version of the KE algorithm .", "Note that p w , s is the unigram probability of word w in sentence s , normalized by the total number of words in s . In practice , initializing s simI0 in line 2 of Algorithm 1 to 0 and then updating it from w simo means that each target sentence is still maximally similar to itself , but we also discover additional similarities between target sentences .", "We further enhance the algorithm by using Sum of Similarities .", "To implement this , in Algorithm 2 we change line 2 into Esy s simL sx , sy Esy s simN sx , sy Although it is appropriate for fine grained tasks like word sense disambiguation to use the single highest similarity score in order to minimize noise , summing across all the similarities of a target set sentence to the feedback set sentences is more appropriate for literal nonliteral clustering , where the usages could be spread across numerous sentences in the feedback sets .", "We make another modification to Algorithm 2 by checking that the maximum sentence similarity in line 2 is above a certain threshold for classification .", "If the similarity is above this threshold , we label a target word sentence as literal or nonliteral .", "Before continuing , let us look at an example .", "The features are shown in bold .", "N2 This idea is risky , but it looks like the director of the institute has comprehended the basic principles behind it .", "N3 Mrs . Fipps is having trouble comprehending the legal straits of the institute .", "N4 She had a hand in his fully comprehending the quandary .", "The target set consists of sentences from the corpus containing the target word .", "The feedback sets contain sentences from the corpus containing synonyms of the target word found in WordNet literal feedback set and the DoKMIE nonliteral feedback set .", "The feedback sets also contain example sentences provided in the target word entries of these datasets .", "TroFi attempts to cluster the target set sentences into literal and nonliteral by attracting them to the corresponding feature sets using Algorithms 1 2 .", "Using the basic KE algorithm , target sentence 2 is correctly attracted to the nonliteral set , and sentences 1 and 3 are equally attracted to both sets .", "When we apply our sum of similarities enhancement , sentence 1 is correctly attracted to the literal set , but sentence 3 is now incorrectly attracted to the literal set too .", "In the following sections we describe some enhancements Learners Voting , SuperTags , and Context that try to solve the problem of incorrect attractions .", "In this section we describe how we clean up the feedback sets to improve the performance of the Core algorithm .", "We also introduce the notion of Learners Voting .", "Recall that neither the raw data nor the collected feedback sets are manually annotated for training purposes .", "Since , in addition , the feedback sets are collected automatically , they are very noisy .", "For instance , in the example in Section 3 . 2 , the literal feedback set sentence L3 contains an idiom which was provided as an example sentence in WordNet as a synonym for grasp .", "In N4 , we have the side effect feature hand , which unfortunately overlaps with the feature hand that we might hope to find in the literal set e . g .", "grasp his hand .", "In order to remove sources of false attraction like these , we introduce the notion of scrubbing .", "Scrubbing is founded on a few basic principles .", "The first is that the contents of the DoKMIE come from third party human annotations and are thus trusted .", "Consequently we take them as primary and use them to scrub the WordNet synsets .", "The second is that phrasal and expression verbs , for example throw away , are often indicative of nonliteral uses of verbs i . e . they are not the sum of their parts so they can be used for scrubbing .", "The third is that content words appearing in both feedback sets for example the wind is blowing vs . the winds of war are blowing for the target word blow will lead to impure feedback sets , a situation we want to avoid .", "The fourth is that our scrubbing action can take a number of different forms we can choose to scrub just a word , a whole synset , or even an entire feature set .", "In addition , we can either move the offending item to the opposite feedback set or remove it altogether .", "Moving synsets or feature sets can add valuable content to one feedback set while removing noise from the other .", "However , it can also cause unforeseen contamination .", "We experimented with a number of these options to produce a whole complement of feedback set learners for classifying the target sentences .", "Ideally this will allow the different learners to correct each other .", "For Learner A , we use phrasal expression verbs and overlap as indicators to select whole WordNet synsets for moving over to the nonliteral feedback set .", "In our example , this causes L1 L3 to be moved to the nonliteral set .", "For Learner B , we use phrasal expression verbs and overlap as indicators to remove problematic synsets .", "Thus we avoid accidentally contaminating the nonliteral set .", "However , we do end up throwing away information that could have been used to pad out sparse nonliteral sets .", "In our example , this causes L1 L3 to be dropped .", "For Learner C , we remove feature sets from the final literal and nonliteral feedback sets based on overlapping words .", "In our example , this causes L2 and N4 to be dropped .", "Learner D is the baseline no scrubbing .", "We simply use the basic algorithm .", "Each learner has benefits and shortcomings .", "In order to maximize the former and minimize the latter , instead of choosing the single most successful learner , we introduce a voting system .", "We use a simple majority rules algorithm , with the strongest learners weighted more heavily .", "In our experiments we double the weights of Learners A and D . In our example , this results in sentence 3 now being correctly attracted to the nonliteral set .", "Even before voting , we attempt to improve the correctness of initial attractions through the use of SuperTags , which allows us to add internal structure information to the bag of words feature lists .", "SuperTags Bangalore Joshi , 1999 encode a great deal of syntactic information in a single tag each tag is an elementary tree from the XTAG English Tree Adjoining Grammar .", "In addition to a word s part of speech , they also encode information about its location in a syntactic tree i . e . we learn something about the surrounding words as well .", "We devised a SuperTag trigram composed of the SuperTag of the target word and the following two words and their SuperTags if they contain nouns , prepositions , particles , or adverbs .", "This is helpful in cases where the same set of features can be used as part of both literal and nonliteral expressions .", "For example , turning It s hard to kick a habit like drinking into habit drink kick B nx0Vpls1 habit A NXN , results in a higher attraction to sentences about kicking habits than to sentences like She has a habit of kicking me when she s been drinking . Note that the creation of Learners A and B changes if SuperTags are used .", "In the original version , we only move or remove synsets based on phrasal expression verbs and overlapping words .", "If SuperTags are used , we also move or remove feature sets whose SuperTag trigram indicates phrasal verbs verb particle expressions .", "A final enhancement involves extending the context to help with disambiguation .", "Sometimes critical disambiguation features are contained not in the sentence with the target word , but in an adjacent sentence .", "To add context , we simply group the sentence containing the target word with a specified number of surrounding sentences and turn the whole group into a single feature set .", "TroFi was evaluated on the 25 target words listed in Table 1 .", "The target sets contain from 1 to 115 manually annotated sentences for each verb .", "The first round of annotations was done by the first annotator .", "The second annotator was given no instructions besides a few examples of literal and nonliteral usage not covering all target verbs .", "The authors of this paper were the annotators .", "Our inter annotator agreement on the annotations used as test data in the experiments in this paper is quite high . n Cohen and n S C on a random sample of 200 annotated examples annotated by two different annotators was found to be 0 . 77 .", "As per Di Eugenio Glass , 2004 , cf . refs therein , the standard assessment for n values is that tentative conclusions on agreement exists when . 67 n . 8 , and a definite conclusion on agreement exists when n . 8 .", "In the case of a larger scale annotation effort , having the person leading the effort provide one or two examples of literal and nonliteral usages for each target verb to each annotator would almost certainly improve inter annotator agreement .", "Table 1 lists the total number of target sentences , plus the manually evaluated literal and nonliteral counts , for each target word .", "It also provides the feedback set sizes for each target word .", "The totals across all words are given at the bottom of the table .", "The algorithms were evaluated based on how accurately they clustered the hand annotated sentences .", "Sentences that were attracted to neither cluster or were equally attracted to both were put in the opposite set from their label , making a failure to cluster a sentence an incorrect clustering .", "Evaluation results were recorded as recall , precision , and f score values .", "Literal recall is defined as correct literals in literal cluster total correct literals .", "Literal precision is defined as correct literals in literal cluster size of literal cluster .", "If there are no literals , literal recall is 100 ; literal precision is 100 if there are no nonliterals in the literal cluster and 0 otherwise .", "The f score is defined as 2 precision recall precision recall .", "Nonliteral precision and recall are defined similarly .", "Average precision is the average of literal and nonliteral precision ; similarly for average recall .", "For overall performance , we take the f score of average precision and average recall .", "We calculated two baselines for each word .", "The first was a simple majority rules baseline .", "Due to the imbalance of literal and nonliteral examples , this baseline ranges from 60 . 9 to 66 . 7 with an average of 63 . 6 .", "Keep in mind though that using this baseline , the f score for the nonliteral set will always be 0 .", "We come back to this point at the end of this section .", "We calculated a second baseline using a simple attraction algorithm .", "Each target set sentence is attracted to the feedback set containing the sentence with which it has the most words in common .", "This corresponds well to the basic highest similarity TroFi algorithm .", "Sentences attracted to neither , or equally to both , sets are put in the opposite cluster to where they belong .", "Since this baseline actually attempts to distinguish between literal and nonliteral and uses all the data used by the TroFi algorithm , it is the one we will refer to in our discussion below .", "Experiments were conducted to first find the results of the core algorithm and then determine the effects of each enhancement .", "The results are shown in Figure 1 .", "The last column in the graph shows the average across all the target verbs .", "On average , the basic TroFi algorithm KE gives a 7 . 6 improvement over the baseline , with some words , like lend and touch , having higher results due to transitivity of similarity .", "For our sum of similarities enhancement , all the individual target word results except for examine sit above the baseline .", "The dip is due to the fact that while TroFi can generate some beneficial similarities between words related by context , it can also generate some detrimental ones .", "When we use sum of similarities , it is possible for the transitively discovered indirect similarities between a target nonliteral sentence and all the sentences in a feedback set to add up to more than a single direct similarity between the target sentence and a single feedback set sentence .", "This is not possible with highest similarity because a single sentence would have to show a higher similarity to the target sentence than that produced by sharing an identical word , which is unlikely since transitively discovered similarities generally do not add up to 1 .", "So , although highest similarity occasionally produces better results than using sum of similarities , on average we can expect to get better results with the latter .", "In this experiment alone , we get an average f score of 46 . 3 for the sum of similarities results a 9 . 4 improvement over the high similarity results 36 . 9 and a 16 . 9 improvement over the baseline 29 . 4 .", "In comparing the individual results of all our learners , we found that the results for Learners A and D 46 . 7 and 46 . 3 eclipsed Learners B and C by just over 2 . 5 .", "Using majority rules voting with Learners A and D doubled , we were able to obtain an average f score of 48 . 4 , showing that voting does to an extent balance out the learners varying results on different words .", "The addition of SuperTags caused improvements in some words like drag and stick .", "The overall gain was only 0 . 5 , likely due to an overgeneration of similarities .", "Future work may identify ways to use SuperTags more effectively .", "The use of additional context was responsible for our second largest leap in performance after sum of similarities .", "We gained 4 . 9 , bringing us to an average f score of 53 . 8 .", "Worth noting is that the target words exhibiting the most significant improvement , drown and grasp , had some of the smallest target and feedback set feature sets , supporting the theory that adding cogent features may improve performance .", "With an average of 53 . 8 , all words but one lie well above our simple attraction baseline , and some even achieve much higher results than the majority rules baseline .", "Note also that , using this latter baseline , TroFi boosts the nonliteral f score from 0 to 42 . 3 .", "In this section we discuss the TroFi Example Base .", "First , we examine iterative augmentation .", "Then we discuss the structure and contents of the example base and the potential for expansion .", "After an initial run for a particular target word , we have the cluster results plus a record of the feedback sets augmented with the newly clustered sentences .", "Each feedback set sentence is saved with a classifier weight , with newly clustered sentences receiving a weight of 1 . 0 .", "Subsequent runs may be done to augment the initial clusters .", "For these runs , we use the classifiers from our initial run as feedback sets .", "New sentences for clustering are treated like a regular target set .", "Running TroFi produces new clusters and re weighted classifiers augmented with newly clustered sentences .", "There can be as many runs as desired ; hence iterative augmentation .", "We used the iterative augmentation process to build a small example base consisting of the target words from Table 1 , as well as another 25 words drawn from the examples of scholars whose work was reviewed in Section 2 .", "It is important to note that in building the example base , we used TroFi with an Active Learning component see Birke , 2005 which improved our average f score from 53 . 8 to 64 . 9 on the original 25 target words .", "An excerpt from the example base is shown in Figure 2 .", "Each entry includes an ID number and a Nonliteral , Literal , or Unannotated tag .", "Annotations are from testing or from active learning during example base construction .", "The TroFi Example Base is available at http www . cs . sfu . ca anoop students jbirke .", "Further unsupervised expansion of the existing clusters as well as the production of additional clusters is a possibility .", "In this paper we presented TroFi , a system for separating literal and nonliteral usages of verbs through statistical word sense disambiguation and clustering techniques .", "We suggest that TroFi is applicable to all sorts of nonliteral language , and that , although it is currently focused on English verbs , it could be adapted to other parts of speech and other languages .", "We adapted an existing word sense disambiguation algorithm to literal nonliteral clustering through the redefinition of literal and nonliteral as word senses , the alteration of the similarity scores used , and the addition of learners and voting , SuperTags , and additional context .", "For all our models and algorithms , we carried out detailed experiments on hand annotated data , both to fully evaluate the system and to arrive at an optimal configuration .", "Through our enhancements we were able to produce results that are , on average , 16 . 9 higher than the core algorithm and 24 . 4 higher than the baseline .", "Finally , we used our optimal configuration of TroFi , together with active learning and iterative augmentation , to build the TroFi Example Base , a publicly available , expandable resource of literal nonliteral usage clusters that we hope will be useful not only for future research in the field of nonliteral language processing , but also as training data for other statistical NLP tasks ."], "summary_lines": ["A Clustering Approach For Nearly Unsupervised Recognition Of Nonliteral Language\n", "In this paper we present TroFi (Trope Finder), a system for automatically classifying literal and nonliteral usages of verbs through nearly unsupervised word-sense disambiguation and clustering techniques.\n", "TroFi uses sentential context instead of selectional constraint violations or paths in semantic hierarchies.\n", "It also uses literal and nonliteral seed sets acquired and cleaned without human supervision in order to bootstrap learning.\n", "We adapt a word-sense disambiguation algorithm to our task and augment it with multiple seed set learners, a voting schema, and additional features like SuperTags and extra-sentential context.\n", "Detailed experiments on hand-annotated data show that our enhanced algorithm outperforms the baseline by 24.4%.\n", "Using the TroFi algorithm, we also build the TroFi Example Base, an extensible resource of annotated literal/nonliteral examples which is freely available to the NLP research community.\n", "For scoring, Literal recall is defined as (correct literals in literal cluster/ total correct literals); Literal precision is defined as (correct literals in literal cluster/ size of literal cluster).\n", "We model literal vs. non-literal classification as a word sense disambiguation task and use a clustering algorithm which compares test instances to two automatically constructed seed sets (one with literal and one with non-literal expressions), assigning the label of the closest set.\n"]}
{"article_lines": ["Exploring Content Models for Multi Document Summarization", "We present an exploration of generative probabilistic models for multi document summarization .", "Beginning with a simple word frequency based model Nenkova and Vanderwende , 2005 , we construct a sequence of models each injecting more structure into the representation of document set content and exhibiting ROUGE gains along the way .", "Our model , utilizes a hierarchical LDA style model Blei et al . , 2004 to represent content specificity as a hierarchy of topic vocabulary distributions .", "At the task of producing generic DUC style summaries , state of the art ROUGE performance and in pairwise user evaluation strongly outperforms Toutanova et al . 2007 s state of the art discriminative system .", "We explore capacity to produce multiple topical summaries in order to facilitate content discovery and navigation .", "Over the past several years , there has been much interest in the task of multi document summarization .", "In the common Document Understanding Conference DUC formulation of the task , a system takes as input a document set as well as a short description of desired summary focus and outputs a word length limited summary . 1 To avoid the problem of generating cogent sentences , many systems opt for an extractive approach , selecting sentences from the document set which best reflect its core content . 2 There are several approaches to modeling document content simple word frequency based methods Luhn , 1958 ; Nenkova and Vanderwende , 2005 , graph based approaches Radev , 2004 ; Wan and Yang , 2006 , as well as more linguistically motivated techniques Mckeown et al . , 1999 ; Leskovec et al . , 2005 ; Harabagiu et al . , 2007 .", "Another strand of work Barzilay and Lee , 2004 ; Daum e III and Marcu , 2006 ; Eisenstein and Barzilay , 2008 , has explored the use of structured probabilistic topic models to represent document content .", "However , little has been done to directly compare the benefit of complex content models to simpler surface ones for generic multi document summarization .", "In this work we examine a series of content models for multi document summarization and argue that LDA style probabilistic topic models Blei et al . , 2003 can offer state of the art summarization quality as measured by automatic metrics see section 5 . 1 and manual user evaluation see section 5 . 2 .", "We also contend that they provide convenient building blocks for adding more structure to a summarization model .", "In particular , we utilize a variation of the hierarchical LDA topic model Blei et al . , 2004 to discover multiple specific subtopics within a document set .", "The resulting model , HIERSUM see section 3 . 4 , can produce general summaries as well as summaries for any of the learned sub topics .", "The task we will consider is extractive multidocument summarization .", "In this task we assume a document collection D consisting of documents Di , . . . , D , , , describing the same or closely related narrative Lapata , 2003 . set of events .", "Our task will be to propose a summary S consisting of sentences in D totaling at most L words . 3 Here as in much extractive summarization , we will view each sentence as a bag of words or more generally a bag of ngrams see section 5 . 1 .", "The most prevalent example of this data setting is document clusters found on news aggregator sites .", "For model development we will utilize the DUC 2006 evaluation set4 consisting of 50 document sets each with 25 documents ; final evaluation will utilize the DUC 2007 evaluation set section 5 .", "Automated evaluation will utilize the standard DUC evaluation metric ROUGE Lin , 2004 which represents recall over various n grams statistics from a system generated summary against a set of humangenerated peer summaries . 5 We compute ROUGE scores with and without stop words removed from peer and proposed summaries .", "In particular , we utilize R 1 recall against unigrams , R 2 recall against bigrams , and R SU4 recall against skip 4 bigrams 6 .", "We present R 2 without stop words in the running text , but full development results are presented in table 1 .", "Official DUC scoring utilizes the jackknife procedure and assesses significance using bootstrapping resampling Lin , 2004 .", "In addition to presenting automated results , we also present a user evaluation in section 5 . 2 .", "We present a progression of models for multidocument summarization .", "Inference details are given in section 4 .", "The SUMBASIC algorithm , introduced in Nenkova and Vanderwende 2005 , is a simple effective procedure for multi document extractive summarization .", "Its design is motivated by the observation that the relative frequency of a non stop word in a document set is a good predictor of a word appearing in a human summary .", "In SUMBASIC , each sentence where PD initially reflects the observed unigram probabilities obtained from the document collection D . A summary S is progressively built by adding the highest scoring sentence according to 1 . 7 In order to discourage redundancy , the words in the selected sentence are updated PDnew w a til the summary word limit has been reached .", "Despite its simplicity , SUMBASIC yields 5 . 3 R 2 without stop words on DUC 2006 see table 1 . 8 By comparison , the highest performing ROUGE system at the DUC 2006 evaluation , SUMFOCUS , was built on top of SUMBASIC and yielded a 6 . 0 , which is not a statistically significant improvement Vanderwende et al . , 2007 . 9 Intuitively , SUMBASIC is trying to select a summary which has sentences where most words have high likelihood under the document set unigram distribution .", "One conceptual problem with this objective is that it inherently favors repetition of frequent non stop words despite the squaring update .", "Ideally , a summarization criterion should be more recall oriented , penalizing summaries which omit moderately frequent document set words and quickly diminishing the reward for repeated use of word .", "Another more subtle shortcoming is the use of the raw empirical unigram distribution to represent content significance .", "For instance , there is no distinction between a word which occurs many times in the same document or the same number of times across several documents .", "Intuitively , the latter word is more indicative of significant document set content .", "The KLSUM algorithm introduces a criterion for selecting a summary S given document collection D , where PS is the empirical unigram distribution of the candidate summary S and KL P Q represents the Kullback Lieber KL divergence given by divergence between the true distribution P here the document set unigram distribution and the approximating distribution Q the summary distribution .", "This criterion casts summarization as finding a set of summary sentences which closely match the document set unigram distribution .", "Lin et al . 2006 propose a related criterion for robust summarization evaluation , but to our knowledge this criteria has been unexplored in summarization systems .", "We address optimizing equation 2 as well as summary sentence ordering in section 4 .", "KLSUM yields 6 . 0 R 2 without stop words , beating SUMBASIC but not with statistical significance .", "It is worth noting however that KLSUM s performance matches SUMFOCUS Vanderwende et al . , 2007 , the highest R 2 performing system at DUC 2006 .", "As mentioned in section 3 . 2 , the raw unigram distribution PD may not best reflect the content of D for the purpose of summary extraction .", "We propose TOPICSUM , which uses a simple LDA like topic model Blei et al . , 2003 similar to Daum e III and Marcu 2006 to estimate a content distribu10In order to ensure finite values of KL divergence we smoothe PS so that it has a small amount of mass on all document set words . tion for summary extraction . 11 We extract summary sentences as before using the KLSUM criterion see equation 2 , plugging in a learned content distribution in place of the raw unigram distribution .", "First , we describe our topic model see figure 1 which generates a collection of document sets .", "We assume a fixed vocabulary V 12 11A topic model is a probabilistic generative process that generates a collection of documents using a mixture of topic vocabulary distributions Steyvers and Griffiths , 2007 .", "Note this usage of topic is unrelated to the summary focus given for document collections ; this information is ignored by our models .", "12In contrast to previous models , stop words are not removed in pre processing .", "13DIRICHLET V , A represents the symmetric Dirichlet prior distribution over V each with a pseudo count of A .", "Concrete pseudo count values will be given in section 4 .", "For each sentence S of each document D , draw a distribution \u03c8T over topics CONTENT , DOCSPECIFIC , BACKGROUND from a Dirichlet prior with pseudo counts 1 . 0 , 5 . 0 , 10 . 0 . 14 For each word position in the sentence , we draw a topic Z from \u03c8T , and a word W from the topic distribution Z indicates .", "Our intent is that \u03c6C represents the core content of a document set .", "Intuitively , \u03c6C does not include words which are common amongst several document collections modeled with the BACKGROUND topic , or words which don t appear across many documents modeled with the DOCSPECIFIC topic .", "Also , because topics are tied together at the sentence level , words which frequently occur with other content words are more likely to be considered content words .", "We ran our topic model over the DUC 2006 document collections and estimated the distribution \u03c6C for each document set . 15 Then we extracted a summary using the KLSUM criterion with our estimated \u03c6C in place of the the raw unigram distribution .", "Doing so yielded 6 . 3 R 2 without stop words see TOPICSUM in table 1 ; while not a statistically significant improvement over KLSUM , it is our first model which outperforms SUMBASIC with statistical significance .", "Daum e III and Marcu 2006 explore a topic model similar to ours for query focused multidocument summarization . 16 Crucially however , Daum e III and Marcu 2006 selected sentences with the highest expected number of CONTENT words . 17 We found that in our model using this extraction criterion yielded 5 . 3 R 2 without stop words , significantly underperforming our TOPICSUM model .", "One reason for this may be that Daum e III and Marcu 2006 s criterion encourages selecting sentences which have words that are confidently generated by the CONTENT distribution , but not necessarily sentences which contain a plurality of it s mass .", "TENT distribution by analytically integrating over \u03c6C Blei et al . , 2003 , doing so gave no benefit .", "Previous sections have treated the content of a document set as a single perhaps learned unigram distribution .", "However , as Barzilay and Lee 2004 observe , the content of document collections is highly structured , consisting of several topical themes , each with its own vocabulary and ordering preferences .", "For concreteness consider the DUC 2006 document collection describing the opening of Star Wars Episode 1 see figure 2 a .", "While there are words which indicate the general content of this document collection e . g . star , wars , there are several sub stories with their own specific vocabulary .", "For instance , several documents in this collection spend a paragraph or two talking about the financial aspect of the film s opening and use a specific vocabulary there e . g .", ", million , record .", "A user may be interested in general content of a document collection or , depending on his or her interests , one or more of the sub stories .", "We choose to adapt our topic modeling approach to allow modeling this aspect of document set content .", "Rather than drawing a single CONTENT distribution 0C for a document collection , we now draw a general content distribution 0C0 from DIRICHLET V , AG as well as specific content distributions 0Ci for i 1 , . . . , K each from DIRICHLET V , AS . 18 Our intent is that 0C0 represents the 18We choose K 3 in our experiments , but one could flexibly general content of the document collection and each 0Ci represents specific sub stories .", "As with TOPICSUM , each sentence has a distribution \u03c8T over topics BACKGROUND , DOCSPECIFIC , CONTENT .", "When BACKGROUND or DOCSPECIFIC topics are chosen , the model works exactly as in TOPICSUM .", "However when the CONTENT topic is drawn , we must decide whether to emit a general content word from 0C0 or from one of the specific content distributions from one of 0Ci for i 1 , . . . , K .", "The generative story of TOPICSUM is altered as follows in this case General or Specific ?", "We must first decide whether to use a general or specific content word .", "Each sentence draws a binomial distribution \u03c8G determining whether a CONTENT word in the sentence will be drawn from the general or a specific topic distribution .", "Reflecting the intuition that the earlier sentences in a document19 describe the general content of a story , we bias \u03c8G to be drawn from BETA 5 , 2 , preferring general content words , and every later sentence from BETA 1 , 2 . 20 emitting a topic specific content word , we must decide which of 0Cl , . . . , 0CK to use .", "In order to ensure tight lexical cohesion amongst the specific topics , we assume that each sentence draws a single specific topic ZS used for every specific content word in that sentence .", "Reflecting intuition that adjacent sentences are likely to share specific content vocabulary , we utilize a sticky HMM as in Barzilay and Lee 2004 over the each sentences ZS .", "Concretely , ZS for the first sentence in a document is drawn uniformly from 1 , . . . , K , and each subsequent sentence s ZS will be identical to the previous sentence with probability Q , and with probability 1 Q we select a successor topic from a learned transition distribution amongst 1 , . . . , K . 21 Our intent is that the general content distribution 0C0 now prefers words which not only appear in many documents , but also words which appear consistently throughout a document rather than being concentrated in a small number of sentences .", "Each specific content distribution 0Ci is meant to model topics which are used in several documents but tend to be used in concentrated locations .", "HIERSUM can be used to extract several kinds of summaries .", "It can extract a general summary by plugging 0C0 into the KLSUM criterion .", "It can also produce topical summaries for the learned specific topics by extracting a summary over each 0Ci distribution ; this might be appropriate for a user who wants to know more about a particular substory .", "While we found the general content distribution from 0Co to produce the best single summary , we experimented with utilizing topical summaries for other summarization tasks see section 6 . 1 .", "The resulting system , HIERSUM yielded 6 . 4 R 2 without stop words .", "While not a statistically significant improvement in ROUGE over TOPICSUM , we found the summaries to be noticeably improved .", "Since globally optimizing the KLSUM criterion in equation equation 2 is exponential in the total number of sentences in a document collection , we 21We choose \u03c3 0 . 75 in our experiments . opted instead for a simple approximation where sentences are greedily added to a summary so long as they decrease KL divergence .", "We attempted more complex inference procedures such as McDonald 2007 , but these attempts only yielded negligible performance gains .", "All summary sentence ordering was determined as follows each sentence in the proposed summary was assigned a number in 0 , 1 reflecting its relative sentence position in its source document , and sorted by this quantity .", "All topic models utilize Gibbs sampling for inference Griffiths , 2002 ; Blei et al . , 2004 .", "In general for concentration parameters , the more specific a distribution is meant to be , the smaller its concentration parameter .", "Accordingly for TOPICSUM , AG AD 1 and AC 0 . 1 .", "For HIERSUM we used AG 0 . 1 and AS 0 . 01 .", "These parameters were minimally tuned without reference to ROUGE results in order to ensure that all topic distribution behaved as intended .", "We present formal experiments on the DUC 2007 data main summarization task , proposing a general summary of at most 250 words22 which will be evaluated automatically and manually in order to simulate as much as possible the DUC evaluation environment . 23 DUC 2007 consists of 45 document sets , each consisting of 25 documents and 4 human reference summaries .", "We primarily evaluate the HIERSUM model , extracting a single summary from the general content distribution using the KLSUM criterion see section 3 . 2 .", "Although the differences in ROUGE between HIERSUM and TOPICSUM were minimal , we found HIERSUM summary quality to be stronger .", "In order to provide a reference for ROUGE and manual evaluation results , we compare against PYTHY , a state of the art supervised sentence extraction summarization system .", "PYTHY uses humangenerated summaries in order to train a sentence ranking system which discriminatively maximizes ROUGE scores .", "PYTHY uses several features to rank sentences including several variations of the SUMBASIC score see section 3 . 1 .", "At DUC 2007 , PYTHY was ranked first overall in automatic ROUGE evaluation and fifth in manual content judgments .", "As PYTHY utilizes a sentence simplification component , which we do not , we also compare against PYTHY without sentence simplification .", "ROUGE results comparing variants of HIERSUM and PYTHY are given in table 3 .", "The HIERSUM system as described in section 3 . 4 yields 7 . 3 R 2 without stop words , falling significantly short of the 8 . 7 that PYTHY without simplification yields .", "Note that R 2 is a measure of bigram recall and HIERSUM does not represent bigrams whereas PYTHY includes several bigram and higher order n gram statistics .", "In order to put HIERSUM and PYTHY on equalfooting with respect to R 2 , we instead ran HIERSUM with each sentence consisting of a bag of bigrams instead of unigrams . 24 All the details of the model remain the same .", "Once a general content distribution over bigrams has been determined by hierarchical topic modeling , the KLSUM criterion is used as before to extract a summary .", "This system , labeled HIERSUM bigram in table 3 , yields 9 . 3 R 2 without stop words , significantly outperforming HIERSUM unigram .", "This model outperforms PYTHY with and without sentence simplification , but not with statistical significance .", "We conclude that both PYTHY variants and HIERSUM bigram are comparable with respect to ROUGE performance .", "24Note that by doing topic modeling in this way over bigrams , our model becomes degenerate as it can generate inconsistent bags of bigrams .", "Future work may look at topic models over n grams as suggested by Wang et al . 2007 .", "In order to obtain a more accurate measure of summary quality , we performed a simple user study .", "For each document set in the DUC 2007 collection , a user was given a reference summary , a PYTHY summary , and a HIERSUM summary ; 25 note that the original documents in the set were not provided to the user , only a reference summary .", "For this experiment we use the bigram variant of HIERSUM and compare it to PYTHY without simplification so both systems have the same set of possible output summaries .", "The reference summary for each document set was selected according to highest R 2 without stop words against the remaining peer summaries .", "Users were presented with 4 questions drawn from the DUC manual evaluation guidelines 26 1 Overall quality Which summary was better overall ?", "2 Non Redundancy Which summary was less redundant ?", "3 Coherence Which summary was more coherent ?", "4 Focus Which summary was more 25The system identifier was of course not visible to the user .", "The order of automatic summaries was determined randomly . focused in its content , not conveying irrelevant details ?", "The study had 16 users and each was asked to compare five summary pairs , although some did fewer .", "A total of 69 preferences were solicited .", "Document collections presented to users were randomly selected from those evaluated fewest .", "As seen in table 5 . 2 , HIERSUM outperforms PYTHY under all questions .", "All results are statistically significant as judged by a simple pairwise t test with 95 confidence .", "It is safe to conclude that users in this study strongly preferred the HIERSUM summaries over the PYTHY summaries .", "While it is difficult to qualitatively compare one summarization system over another , we can broadly characterize HIERSUM summaries compared to some of the other systems discussed .", "For example output from HIERSUM and PYTHY see table 2 .", "On the whole , HIERSUM summaries appear to be significantly less redundant than PYTHY and moderately less redundant than SUMBASIC .", "The reason for this might be that PYTHY is discriminatively trained to maximize ROUGE which does not directly penalize redundancy .", "Another tendency is for HIERSUM to select longer sentences typically chosen from an early sentence in a document .", "As discussed in section 3 . 4 , HIERSUM is biased to consider early sentences in documents have a higher proportion of general content words and so this tendency is to be expected .", "A common concern in multi document summarization is that without any indication of user interest or intent providing a single satisfactory summary to a user may not be feasible .", "While many variants of the general summarization task have been proposed which utilize such information Vanderwende et al . , 2007 ; Nastase , 2008 , this presupposes that a user knows enough of the content of a document collection in order to propose a query .", "As Leuski et al . 2003 and Branavan et al .", "2007 suggest , a document summarization system should facilitate content discovery and yield summaries relevant to a user s interests .", "We may use HIERSUM in order to facilitate content discovery via presenting a user with salient words or phrases from the specific content topics parametrized by 0C1 , . . . , 0CK for an example see figure 3 .", "While these topics are not adaptive to user interest , they typically reflect lexically coherent vocabularies .", "In this paper we have presented an exploration of content models for multi document summarization and demonstrated that the use of structured topic models can benefit summarization quality as measured by automatic and manual metrics .", "Acknowledgements The authors would like to thank Bob Moore , Chris Brockett , Chris Quirk , and Kristina Toutanova for their useful discussions as well as the reviewers for their helpful comments ."], "summary_lines": ["Exploring Content Models for Multi-Document Summarization\n", "We present an exploration of generative probabilistic models for multi-document summarization.\n", "Beginning with a simple word frequency based model (Nenkova and Vanderwende, 2005), we construct a sequence of models each injecting more structure into the representation of document set content and exhibiting ROUGE gains along the way.\n", "Our final model, HIERSUM, utilizes a hierarchical LDA-style model (Blei et al., 2004) to represent content specificity as a hierarchy of topic vocabulary distributions.\n", "At the task of producing generic DUC-style summaries, HIERSUM yields state-of-the-art ROUGE performance and in pairwise user evaluation strongly outperforms Toutanova et al. (2007)'s state-of-the-art discriminative system.\n", "We also explore HIERSUM's capacity to produce multiple 'topical summaries' in order to facilitate content discovery and navigation.\n", "In TOPICSUM, each word is generated by a single topic which can be a corpus-wide background distribution over common words, a distribution of document-specific words or a distribution of the core content of a given cluster.\n", "We build a summarization system based on topic models, where both topics at general document level as well as those at specific subtopic levels were learnt.\n"]}
{"article_lines": ["Better Evaluation for Grammatical Error Correction", "We present a novel method for evaluating grammatical error correction .", "The core of method , which we call is an algorithm for efficiently computing the sequence of phrase level edits between a source sentence and a system hypothesis that achieves the highest overlap with the goldstandard annotation .", "This optimal edit seis subsequently scored using mea We test our on the Helping Our Own HOO shared task data and show that our method results in more accurate evaluation for grammatical error correction .", "Progress in natural language processing NLP research is driven and measured by automatic evaluation methods .", "Automatic evaluation allows fast and inexpensive feedback during development , and objective and reproducible evaluation during testing time .", "Grammatical error correction is an important NLP task with useful applications for second language learning .", "Evaluation for error correction is typically done by computing F1 measure between a set of proposed system edits and a set of humanannotated gold standard edits Leacock et al . , 2010 .", "Unfortunately , evaluation is complicated by the fact that the set of edit operations for a given system hypothesis is ambiguous .", "This is due to two reasons .", "First , the set of edits that transforms one string into another is not necessarily unique , even at the token level .", "Second , edits can consist of longer phrases which introduce additional ambiguity .", "To see how this can affect evaluation , consider the following source sentence and system hypothesis from the recent Helping Our Own HOO shared task Dale and Kilgarriff , 2011 on grammatical error correction Source Our baseline system feeds word into PB SMT pipeline .", "Hypot .", "Our baseline system feeds a word into PB SMT pipeline .", "The HOO evaluation script extracts the system edit c a , i . e . , inserting the article a .", "Unfortunately , the gold standard annotation instead contains the edits word a word , words .", "Although the extracted system edit results in the same corrected sentence as the first gold standard edit option , the system hypothesis was considered to be invalid .", "In this work , we propose a method , called MaxMatch M2 , to overcome this problem .", "The key idea is that if there are multiple possible ways to arrive at the same correction , the system should be evaluated according to the set of edits that matches the gold standard as often as possible .", "To this end , we propose an algorithm for efficiently computing the set of phrase level edits with the maximum overlap with the gold standard .", "The edits are subsequently scored using F1 measure .", "We test our method in the context of the HOO shared task and show that our method results in a more accurate evaluation for error correction .", "The remainder of this paper is organized as follows Section 2 describes the proposed method ; Section 3 presents experimental results ; Section 4 discusses some details of grammar correction evaluation ; and Section 5 concludes the paper .", "We begin by establishing some notation .", "We consider a set of source sentences 5 s1 , . . . , sn together with a set of hypotheses H h1 , . . . , hn generated by an error correction system .", "Let G g1 , . . . , gn be the set of gold standard annotations for the same sentences .", "Each annotation gi g1i , . . . , gr i is a set of edits .", "An edit is a triple a , b , C , consisting of The remainder of this section describes a method for solving these two steps .", "We start by describing how to construct an edit lattice from a source hypothesis pair .", "Then , we show that finding the optimal sequence of edits is equivalent to solving a shortest path search through the lattice .", "Finally , we describe how to evaluate the edits using F1 measure .", "We start from the well established Levenshtein distance Levenshtein , 1966 , which is defined as the minimum number of insertions , deletions , and substitutions needed to transform one string into another .", "The Levenshtein distance between a source sentence si s1i , . . . , ski and a hypothesis hi h1i , . . . , hl i can be efficiently computed using a two dimensional matrix that is filled using a classic dynamic programming algorithm .", "We assume that both si and hi have been tokenized .", "The matrix for the example from Section 1 is shown in Figure 1 .", "By performing a simple breadth first search , similar to the Viterbi algorithm , we can extract the lattice of all shortest paths that lead from the top left corner to the bottom right corner of the Levenshtein matrix .", "Each vertex in the lattice corresponds to a cell in the Levenshtein matrix , and each edge in the lattice corresponds to an atomic edit operation inserting a token , deleting a token , substituting a token , or leaving a token unchanged .", "Each path through the lattice corresponds to a shortest sequence of edits that transform si into hi .", "We assign a unit cost to each edge in the lattice .", "We have seen that annotators can use longer phrases and that phrases can include unchanged words from the context , e . g . , the gold edit from the example in Section 1 is 4 , 5 , word , a word , words .", "However , it seems unrealistic to allow an arbitrary number of unchanged words in an edit .", "In particular , we want to avoid very large edits that cover complete sentences .", "Therefore , we limit the number of unchanged words by a parameter u .", "To allow for phrase level edits , we add transitive edges to the lattice as long as the number of unchanged words in the newly added edit is not greater than u and the edit changes at least one word .", "Let e1 a1 , b1 , C1 and e2 a2 , b2 , C2 be two edits corresponding to adjacent edges in the lattice , with the first end offset b1 being equal to the second start offset a2 .", "We can combine them into a new edit e3 a1 , b2 , C1 C2 , where C1 C2 is the concatenation of strings C1 and C2 .", "The cost of a transitive edge is the sum of the costs of its parts .", "The lattice extracted from the example sentence is shown in Figure 2 .", "Our goal is to find the sequence of edits ei with the maximum overlap with the gold standard .", "Let L V , E be the edit lattice graph from the last section .", "We change the cost of each edge whose corsystem feeds a word into PB SMT pipeline . responding edit has a match in the gold standard to u 1 E .", "An edit e matches a gold edit g iff they have the same offsets and e s correction is included in g Then , we perform a single source shortest path search with negative edge weights from the start to the end vertex1 .", "This can be done efficiently , for example with the Bellman Ford algorithm Cormen et al . , 2001 .", "As the lattice is acyclic , the algorithm is guaranteed to terminate and return a shortest path .", "Theorem 1 .", "The set of edits corresponding to the shortest path has the maximum overlap with the gold standard annotation .", "Proof .", "Let e e1 , . . . , ek be the edit sequence corresponding to the shortest path and let p be the number of matched edits .", "Assume that there exists another edit sequence e0 with higher total edge weights but p0 p matching edits .", "Then we have where q and q0 denote the combined cost of all nonmatching edits in the two paths , respectively .", "Because p0 p 1 , the right hand side is at most u 1 E .", "Because q and q0 are positive and bounded by u 1 E , the left hand side cannot be smaller than or equal to u 1 E .", "This is a contradiction .", "Therefore there cannot exist such an edit sequence e0 , and e is the sequence with the maximum overlap with the gold standard annotation .", "What is left to do is to evaluate the set of edits with respect to the gold standard .", "This is done by computing precision , recall , and F1 measure van Rijsbergen , 1979 between the set of system edits e1 , . . . , en and the set of gold edits g1 , . . . , gn for all sentences where we define the intersection between ei and gi as ei gi e ei g gi match e , g .", "We experimentally test our M2 method in the context of the HOO shared task .", "The HOO test data2 consists of text fragments from NLP papers together with manually created gold standard corrections see Dale and Kilgarriff , 2011 for details .", "We test our method by re scoring the best runs of the participating teams3 in the HOO shared task with our M2 scorer and comparing the scores with the official HOO scorer , which simply uses GNU wdiff4 to extract system edits .", "We obtain each system s output and segment it at the sentence level according to the gold standard sentence segmentation .", "The source sentences , system hypotheses , and corrections are tokenized using the Penn Treebank standard Marcus et al . , 1993 .", "The character edit offsets are automatically converted to token offsets .", "We set the parameter u to 2 , allowing up to two unchanged words per edit .", "The results are shown in Table 1 .", "Note that the M2 scorer and the HOO scorer adhere to the same score definition and only differ in the way the system edits are computed .", "We can see that the M2 scorer results in higher scores than the official scorer for all systems , showing that the official scorer missed some valid edits .", "For example , the M2 scorer finds 155 valid edits for the UI system compared to 141 found by the official scorer , and 83 valid edits for the NU system , compared to 78 by the official scorer .", "We manually inspect the output of the scorers and find that the M2 scorer indeed extracts the correct edits matching the gold standard where possible .", "Examples are shown in Table 2 .", "The evaluation framework proposed in this work differs slightly from the one in the HOO shared task .", "Sentence by sentence .", "We compute the edits between source hypothesis sentence pairs , while the HOO scorer computes edits at the document level .", "As the HOO data comes in a sentencesegmented format , both approaches are equivalent , while sentence by sentence is easier to work with .", "Token level offsets .", "In our work , the start and end of an edit are given as token offsets , while the HOO data uses character offsets .", "Character offsets make the evaluation procedure very brittle as a small change , e . g . , an additional whitespace character , will affect all subsequent edits .", "Character offsets also introduce ambiguities in the annotation , e . g . , whether a comma is part of the preceding token .", "Alternative scoring .", "The HOO shared task defines three different scores detection , recognition , and correction .", "Effectively , all three scores are F1 measures and only differ in the conditions on when an edit is counted as valid .", "Additionally , each score is reported under a with bonus alternative , where a system receives rewards for missed optional edits .", "The F1 measure defined in Section 2 . 3 is equivalent to correction without bonus .", "Our method can be used to compute detection and recognition scores and scores with bonus as well .", "We have presented a novel method , called MaxMatch M2 , for evaluating grammatical error correction .", "Our method computes the sequence of phrase level edits that achieves the highest overlap with the gold standard annotation .", "Experiments on the HOO data show that our method overcomes deficiencies in the current evaluation method .", "The M2 scorer is available for download at http nlp . comp . nus . edu . sg software .", "We thank Chang Liu for comments on an earlier draft .", "This research is supported by the Singapore National Research Foundation under its International Research Centre Singapore Funding Initiative and administered by the IDM Programme Office ."], "summary_lines": ["Better Evaluation for Grammatical Error Correction\n", "We present a novel method for evaluating grammatical error correction.\n", "The core of our method, which we call MaxMatch (M2), is an algorithm for efficiently computing the sequence of phrase-level edits between a source sentence and a system hypothesis that achieves the highest overlap with the gold-standard annotation.\n", "This optimal edit sequence is subsequently scored using F1 measure.\n", "We test our M2 scorer on the Helping Our Own (HOO) shared task data and show that our method results in more accurate evaluation for grammatical error correction.\n", "We propose an alternative evaluation scheme which operates in terms of tokens rather than character offsets.\n"]}
{"article_lines": ["A Joint Model of Text and Aspect Ratings for Sentiment Summarization", "Online reviews are often accompanied with numerical ratings provided by users for a set of service or product aspects .", "We propose a statistical model which is able to discover corresponding topics in text and extract textual evidence from reviews supporting each of these aspect ratings a fundamental problem in aspect based sentiment summarization Hu and Liu , 2004a .", "Our model achieves high accuracy , without any explicitly labeled data except the user provided opinion ratings .", "The proposed approach is general and can be used for segmentation in other applications where sequential data is accompanied with correlated signals .", "User generated content represents a unique source of information in which user interface tools have facilitated the creation of an abundance of labeled content , e . g . , topics in blogs , numerical product and service ratings in user reviews , and helpfulness rankings in online discussion forums .", "Many previous studies on user generated content have attempted to predict these labels automatically from the associated text .", "However , these labels are often present in the data already , which opens another interesting line of research designing models leveraging these labelings to improve a wide variety of applications .", "In this study , we look at the problem of aspectbased sentiment summarization Hu and Liu , 2004a ; Popescu and Etzioni , 2005 ; Gamon et al . , 2005 ; Carenini et al . , 2006 ; Zhuang et al . , 2006 . 1 An aspect based summarization system takes as input a set of user reviews for a specific product or service and produces a set of relevant aspects , the aggregated sentiment for each aspect , and supporting textual evidence .", "For example , figure 1 summarizes a restaurant using aspects food , decor , service , and value plus a numeric rating out of 5 .", "Standard aspect based summarization consists of two problems .", "The first is aspect identification and mention extraction .", "Here the goal is to find the set of relevant aspects for a rated entity and extract all textual mentions that are associated with each .", "Aspects can be fine grained , e . g . , fish , lamb , calamari , or coarse grained , e . g . , food , decor , service .", "Similarly , extracted text can range from a single word to phrases and sentences .", "The second problem is sentiment classification .", "Once all the relevant aspects and associated pieces of texts are extracted , the system should aggregate sentiment over each aspect to provide the user with an average numeric or symbolic rating .", "Sentiment classification is a well studied problem Wiebe , 2000 ; Pang et al . , 2002 ; Turney , 2002 and in many domains users explicitly provide ratings for each aspect making automated means unnecessary . 2 Aspect identification has also been thoroughly studied Hu and Liu , 2004b ; Gamon et al . , 2005 ; Titov and McDonald , 2008 , but again , ontologies and users often provide this information negating the need for automation .", "Though it may be reasonable to expect a user to provide a rating for each aspect , it is unlikely that a user will annotate every sentence and phrase in a review as being relevant to some aspect .", "Thus , it can be argued that the most pressing challenge in an aspect based summarization system is to extract all relevant mentions for each aspect , as illustrated in figure 2 .", "When labeled data exists , this problem can be solved effectively using a wide variety of methods available for text classification and information extraction Manning and Schutze , 1999 .", "However , labeled data is often hard to come by , especially when one considers all possible domains of products and services .", "Instead , we propose an unsupervised model that leverages aspect ratings that frequently accompany an online review .", "In order to construct such model , we make two assumptions .", "First , ratable aspects normally represent coherent topics which can be potentially discovered from co occurrence information in the text .", "Second , we hypothesize that the most predictive features of an aspect rating are features derived from the text segments discussing the corresponding aspect .", "Motivated by these observations , we construct a joint statistical model of text and sentiment ratings .", "The model is at heart a topic model in that it assigns words to a set of induced topics , each of which may represent one particular aspect .", "The model is extended through a set of maximum entropy classifiers , one per each rated aspect , that are used to pre2E . g . , http zagat . com and http tripadvisor . com . dict the sentiment rating towards each of the aspects .", "However , only the words assigned to an aspects corresponding topic are used in predicting the rating for that aspect .", "As a result , the model enforces that words assigned to an aspects topic are predictive of the associated rating .", "Our approach is more general than the particular statistical model we consider in this paper .", "For example , other topic models can be used as a part of our model and the proposed class of models can be employed in other tasks beyond sentiment summarization , e . g . , segmentation of blogs on the basis of topic labels provided by users , or topic discovery on the basis of tags given by users on social bookmarking sites . 3 The rest of the paper is structured as follows .", "Section 2 begins with a discussion of the joint textsentiment model approach .", "In Section 3 we provide both a qualitative and quantitative evaluation of the proposed method .", "We conclude in Section 4 with an examination of related work .", "In this section we describe a new statistical model called the Multi Aspect Sentiment model MAS , which consists of two parts .", "The first part is based on Multi Grain Latent Dirichlet Allocation Titov and McDonald , 2008 , which has been previously shown to build topics that are representative of ratable aspects .", "The second part is a set of sentiment predictors per aspect that are designed to force specific topics in the model to be directly correlated with a particular aspect .", "The Multi Grain Latent Dirichlet Allocation model MG LDA is an extension of Latent Dirichlet Allocation LDA Blei et al . , 2003 .", "As was demonstrated in Titov and McDonald 2008 , the topics produced by LDA do not correspond to ratable aspects of entities .", "In particular , these models tend to build topics that globally classify terms into product instances e . g . , Creative Labs Mp3 players versus iPods , or New York versus Paris Hotels .", "To combat this , MG LDA models two distinct types of topics global topics and local topics .", "As in LDA , the distribution of global topics is fixed for a document a user review .", "However , the distribution of local topics is allowed to vary across the document .", "A word in the document is sampled either from the mixture of global topics or from the mixture of local topics specific to the local context of the word .", "It was demonstrated in Titov and McDonald 2008 that ratable aspects will be captured by local topics and global topics will capture properties of reviewed items .", "For example , consider an extract from a review of a London hotel . . . public transport in London is straightforward , the tube station is about an 8 minute walk . . . or you can get a bus for 1 . 50 .", "It can be viewed as a mixture of topic London shared by the entire review words London , tube , , and the ratable aspect location , specific for the local context of the sentence words transport , walk , bus .", "Local topics are reused between very different types of items , whereas global topics correspond only to particular types of items .", "In MG LDA a document is represented as a set of sliding windows , each covering T adjacent sentences within a document . 4 Each window v in document d has an associated distribution over local topics loc d , v and a distribution defining preference for local topics versus global topics 7rd , v .", "A word can be sampled using any window covering its sentence s , where the window is chosen according to a categorical distribution Od , s .", "Importantly , the fact that windows overlap permits the model to exploit a larger co occurrence domain .", "These simple techniques are capable of modeling local topics without more expensive modeling of topic transitions used in Griffiths et al . , 2004 ; Wang and McCallum , 2005 ; Wallach , 2006 ; Gruber et al . , 2007 .", "Introduction of a symmetrical Dirichlet prior Dir \u03b3 for the distribution Od , s can control the smoothness of transitions .", "4Our particular implementation is over sentences , but sliding windows in theory can be over any sized fragment of text .", "The formal definition of the model with Kgl global and Kloc local topics is as follows First , draw Kgl word distributions for global topics cogl z from a Dirichlet prior Dir 0gl and Kloc word distributions for local topics coloc z from Dir 0loc .", "Then , for each document d if rd , i gl choose global topic zd , i gl d , if rd , i loc choose local topic zd , i loc d , vd , i , choose word wd , i from the word distribution rd , i zd , i .", "Beta \u03b1mix is a prior Beta distribution for choosing between local and global topics .", "In Figure 3a the corresponding graphical model is presented .", "MG LDA constructs a set of topics that ideally correspond to ratable aspects of an entity often in a many to one relationship of topics to aspects .", "A major shortcoming of this model and all other unsupervised models is that this correspondence is not explicit , i . e . , how does one say that topic X is really about aspect Y ?", "However , we can observe that numeric aspect ratings are often included in our data by users who left the reviews .", "We then make the assumption that the text of the review discussing an aspect is predictive of its rating .", "Thus , if we model the prediction of aspect ratings jointly with the construction of explicitly associated topics , then such a model should benefit from both higher quality topics and a direct assignment from topics to aspects .", "This is the basic idea behind the Multi Aspect Sentiment model MAS .", "In its simplest form , MAS introduces a classifier for each aspect , which is used to predict its rating .", "Each classifier is explicitly associated to a single topic in the model and only words assigned to that topic can participate in the prediction of the sentiment rating for the aspect .", "However , it has been observed that ratings for different aspects can be correlated Snyder and Barzilay , 2007 , e . g . , very negative opinion about room cleanliness is likely to result not only in a low rating for the aspect rooms , but also is very predictive of low ratings for the aspects service and dining .", "This complicates discovery of the corresponding topics , as in many reviews the most predictive features for an aspect rating might correspond to another aspect .", "Another problem with this overly simplistic model is the presence of opinions about an item in general without referring to any particular aspect .", "For example , this product is the worst I have ever purchased is a good predictor of low ratings for every aspect .", "In such cases , non aspect background words will appear to be the most predictive .", "Therefore , the use of the aspect sentiment classifiers based only on the words assigned to the corresponding topics is problematic .", "Such a model will not be able to discover coherent topics associated with each aspect , because in many cases the most predictive fragments for each aspect rating will not be the ones where this aspect is discussed .", "Our proposal is to estimate the distribution of possible values of an aspect rating on the basis of the overall sentiment rating and to use the words assigned to the corresponding topic to compute corrections for this aspect .", "An aspect rating is typically correlated to the overall sentiment rating5 and the fragments discussing this particular aspect will help to correct the overall sentiment in the appropriate direction .", "For example , if a review of a hotel is generally positive , but it includes a sentence the neighborhood is somewhat seedy then this sentence is predictive of rating for an aspect location being below other ratings .", "This rectifies the aforementioned 5In the dataset used in our experiments all three aspect ratings are equivalent for 5 , 250 reviews out of 10 , 000 . problems .", "First , aspect sentiment ratings can often be regarded as conditionally independent given the overall rating , therefore the model will not be forced to include in an aspect topic any words from other aspect topics .", "Secondly , the fragments discussing overall opinion will influence the aspect rating only through the overall sentiment rating .", "The overall sentiment is almost always present in the real data along with the aspect ratings , but it can be coarsely discretized and we preferred to use a latent overall sentiment .", "The MAS model is presented in Figure 3b .", "Note that for simplicity we decided to omit in the figure the components of the MG LDA model other than variables r , z and w , though they are present in the statistical model .", "MAS also allows for extra unassociated local topics in order to capture aspects not explicitly rated by the user .", "As in MG LDA , MAS has global topics which are expected to capture topics corresponding to particular types of items , such London hotels or seaside resorts for the hotel domain .", "In figure 3b we shaded the aspect ratings ya , assuming that every aspect rating is present in the data though in practice they might be available only for some reviews .", "In this model the distribution of the overall sentiment rating yo is based on all the n gram features of a review text .", "Then the distribution of ya , for every rated aspect a , can be computed from the distribution of yo and from any n gram feature where at least one word in the n gram is assigned to the associated aspect topic r loc , z a .", "Instead of having a latent variable yo , 6 we use a similar model which does not have an explicit notion of yo .", "The distribution of a sentiment rating ya for each rated aspect a is computed from two scores .", "The first score is computed on the basis of all the ngrams , but using a common set of weights independent of the aspect a .", "Another score is computed only using n grams associated with the related topic , but an aspect specific set of weights is used in this computation .", "More formally , we consider the log linear distribution where w , r , z are vectors of all the words in a document , assignments of context global or local and topics for all the words in the document , respectively . bay is the bias term which regulates the prior distribution P ya y , f iterates through all the n grams , Jy , f and Jay , f are common weights and aspect specific weights for n gram feature f . pa f , r , z is equal to a fraction of words in n gram feature f assigned to the aspect topic r loc , z a .", "Exact inference in the MAS model is intractable .", "Following Titov and McDonald 2008 we use a collapsed Gibbs sampling algorithm that was derived for the MG LDA model based on the Gibbs sampling method proposed for LDA in Griffiths and Steyvers , 2004 .", "Gibbs sampling is an example of a Markov Chain Monte Carlo algorithm Geman and Geman , 1984 .", "It is used to produce a sample from a joint distribution when only conditional distributions of each variable can be efficiently computed .", "In Gibbs sampling , variables are sequentially sampled from their distributions conditioned on all other variables in the model .", "Such a chain of model states converges to a sample from the joint distribution .", "A naive application of this technique to LDA would imply that both assignments of topics to words z and distributions \u03b8 and \u03d5 should be sampled .", "However , Griffiths and Steyvers , 2004 demonstrated that an efficient collapsed Gibbs sampler can be constructed , where only assignments z need to be sampled , whereas the dependency on distributions \u03b8 and \u03d5 can be integrated out analytically .", "In the case of MAS we also use maximum aposteriori estimates of the sentiment predictor parameters bay , Jy , f and Jay , f .", "The MAP estimates for parameters bay , Jy , f and Jay , f are obtained by using stochastic gradient ascent .", "The direction of the gradient is computed simultaneously with running a chain by generating several assignments at each step and averaging over the corresponding gradient estimates .", "For details on computing gradients for loglinear graphical models with Gibbs sampling we refer the reader to Neal , 1992 .", "Space constraints do not allow us to present either the derivation or a detailed description of the sampling algorithm .", "However , note that the conditional distribution used in sampling decomposes into two parts where v , r and z are vectors of assignments of sliding windows , context global or local and topics for all the words in the collection except for the considered word at position i in document d ; y is the vector of sentiment ratings .", "The first factor \u03b7d , i v , r , z is responsible for modeling co occurrences on the window and document level and coherence of the topics .", "This factor is proportional to the conditional distribution used in the Gibbs sampler of the MG LDA model Titov and McDonald , 2008 .", "The last factor quantifies the influence of the assignment of the word d , i on the probability of the sentiment ratings .", "It appears only if ratings are known observable and equals where the probability distribution is computed as defined in expression 1 , yda is the rating for the ath aspect of review d .", "In this section we present qualitative and quantitative experiments .", "For the qualitative analysis we show that topics inferred by the MAS model correspond directly to the associated aspects .", "For the quantitative analysis we show that the MAS model induces a distribution over the rated aspects which can be used to accurately predict whether a text fragment is relevant to an aspect or not .", "To perform qualitative experiments we used a set of reviews of hotels taken from TripAdvisor . com7 that contained 10 , 000 reviews 109 , 024 sentences , 2 , 145 , 313 words in total .", "Every review was rated with at least three aspects service , location and rooms .", "Each rating is an integer from 1 to 5 .", "The dataset was tokenized and sentence split automatically .", "We ran the sampling chain for 700 iterations to produce a sample .", "Distributions of words in each topic were estimated as the proportion of words assigned to each topic , taking into account topic model priors 0gl and 0loc .", "The sliding windows were chosen to cover 3 sentences for all the experiments .", "All the priors were chosen to be equal to 0 . 1 .", "We used 15 local topics and 30 global topics .", "In the model , the first three local topics were associated to the rating classifiers for each aspects .", "As a result , we would expect these topics to correspond to the service , location , and rooms aspects respectively .", "Unigram and bigram features were used in the sentiment predictors in the MAS model .", "Before applying the topic models we removed punctuation and also removed stop words using the standard list of stop words , 8 however , all the words and punctuation were used in the sentiment predictors .", "It does not take many chain iterations to discover initial topics .", "This happens considerably faster than the appropriate weights of the sentiment predictor being learned .", "This poses a problem , because , in the beginning , the sentiment predictors are not accurate enough to force the model to discover appropriate topics associated with each of the rated aspects .", "And as soon as topic are formed , aspect sentiment predictors cannot affect them anymore because they do not have access to the true words associated with their aspects .", "To combat this problem we first train the sentiment classifiers by assuming that paf , r , z is equal for all the local topics , which effectively ignores the topic model .", "Then we use the estimated parameters within the topic model . 9 Secondly , we modify the sampling algorithm .", "The conditional probability used in sampling , expression 2 , is proportional to the product of two factors .", "The first factor , d , i v , r , z , expresses a preference for topics likely from the co occurrence information , whereas the second d , i one , pr , z , favors the choice of topics which are predictive of the observable sentiment ratings .", "We used pd , i r , z 1 0 . sstq in the sampling distribution instead of pr , z , where t is the iteration number . q was chosen to be 4 , though the quality of the topics seemed to be indistinguishable with any q between 3 and 10 .", "This can be thought of as having 1 0 . 95tq ratings instead of a single vector assigned to each review , i . e . , focusing the model on prediction of the ratings rather than finding the topic labels which are good at explaining co occurrences of words .", "These heuristics influence sampling only during the first iterations of the chain .", "Top words for some of discovered local topics , in9Initial experiments suggested that instead of doing this pre training we could start with very large priors \u03b1loc and \u03b1m , and then reduce them through the course of training .", "However , this is significantly more computationally expensive . cluding the first 3 topics associated with the rated aspects , and also top words for some of global topics are presented in Table 1 .", "We can see that the model discovered as its first three topics the correct associated aspects service , location , and rooms .", "Other local topics , as for the MG LDA model , correspond to other aspects discussed in reviews breakfast , prices , noise , and as it was previously shown in Titov and McDonald 2008 , aspects for global topics correspond to the types of reviewed items hotels in Russia , Paris hotels or background words .", "Notice though , that the 3rd local topic induced for the rating rooms is slightly narrow .", "This can be explained by the fact that the aspect rooms is a central aspect of hotel reviews .", "A very significant fraction of text in every review can be thought of as a part of the aspect rooms .", "These portions of reviews discuss different coherent sub aspects related to the aspect rooms , e . g . , the previously discovered topic noise .", "Therefore , it is natural to associate several topics to such central aspects .", "To test this we varied the number of topics associated with the sentiment predictor for the aspect rooms .", "Top words for resulting topics are presented in Table 2 .", "It can be observed that the topic model discovered appropriate topics while the number of topics was below 4 .", "With 4 topics a semantically unrelated topic check in arrival is induced .", "Manual selection of the number of topics is undesirable , but this problem can be potentially tackled with Dirichlet Process priors or a topic split criterion based on the accuracy of the sentiment predictor in the MAS model .", "We found that both service and location did not benefit by the assignment of additional topics to their sentiment rating models .", "The experimental results suggest that the MAS model is reliable in the discovery of topics corresponding to the rated aspects .", "In the next section we will show that the induced topics can be used to accurately extract fragments for each aspect .", "A primary advantage of MAS over unsupervised models , such as MG LDA or clustering , is that topics are linked to a rated aspect , i . e . , we know exactly which topics model which aspects .", "As a result , these topics can be directly used to extract textual mentions that are relevant for an aspect .", "To test this , we hand labeled 779 random sentences from the dataset considered in the previous set of experiments .", "The sentences were labeled with one or more aspects .", "Among them , 164 , 176 and 263 sentences were labeled as related to aspects service , location and rooms , respectively .", "The remaining sentences were not relevant to any of the rated aspects .", "We compared two models .", "The first model uses the first three topics of MAS to extract relevant mentions based on the probability of that topic aspect being present in the sentence .", "To obtain these probabilities we used estimators based on the proportion of words in the sentence assigned to an aspects topic and normalized within local topics .", "To improve the reliability of the estimator we produced 100 samples for each document while keeping assignments of the topics to all other words in the collection fixed .", "The probability estimates were then obtained by averaging over these samples .", "We did not perform any model selection on the basis of the hand labeled data , and tested only a single model of each type .", "For the second model we trained a maximum entropy classifier , one per each aspect , using 10 fold cross validation and unigram bigram features .", "Note that this is a supervised system and as such represents an upper bound in performance one might expect when comparing an unsupervised model such as MAS .", "We chose this comparison to demonstrate that our model can find relevant text mentions with high accuracy relative to a supervised model .", "It is difficult to compare our model to other unsupervised systems such as MG LDA or LDA .", "Again , this is because those systems have no mechanism for directly correlating topics or clusters to corresponding aspects , highlighting the benefit of MAS .", "The resulting precision recall curves for the aspects service , location and rooms are presented in Figure 4 .", "In Figure 4c , we varied the number of topics associated with the aspect rooms . 10 The average precision we obtained the standard measure proportional to the area under the curve is 75 . 8 , 85 . 5 for aspects service and location , respectively .", "For the aspect rooms these scores are equal to 75 . 0 , 74 . 5 , 87 . 6 , 79 . 8 with 1 4 topics per aspect , respectively .", "The logistic regression models achieve 80 . 8 , 94 . 0 and 88 . 3 for the aspects service , location and rooms .", "We can observe that the topic model , which does not use any explicitly aspect labeled text , achieves accuracies lower than , but comparable to a supervised model .", "There is a growing body of work on summarizing sentiment by extracting and aggregating sentiment over ratable aspects and providing corresponding textual evidence .", "Text excerpts are usually extracted through string matching Hu and Liu , 2004a ; Popescu and Etzioni , 2005 , sentence clustering Gamon et al . , 2005 , or through topic models Mei et al . , 2007 ; Titov and McDonald , 2008 .", "String extraction methods are limited to fine grained aspects whereas clustering and topic model approaches must resort to ad hoc means of labeling clusters or topics .", "However , this is the first work we are aware of that uses a pre defined set of aspects plus an associated signal to learn a mapping from text to an aspect for the purpose of extraction .", "A closely related model to ours is that of Mei et al . 2007 which performs joint topic and sentiment modeling of collections .", "Our model differs from theirs in many respects Mei et al . only model sentiment predictions for the entire document and not on the aspect level ; They treat sentiment predictions as unobserved variables , whereas we treat them as observed signals that help to guide the creation of topics ; They model co occurrences solely on the document level , whereas our model is based on MG LDA and models both local and global contexts .", "Recently , Blei and McAuliffe 2008 proposed an approach for joint sentiment and topic modeling that can be viewed as a supervised LDA sLDA model that tries to infer topics appropriate for use in a given classification or regression problem .", "MAS and sLDA are similar in that both use sentiment predictions as an observed signal that is predicted by the model .", "However , Blei et al . do not consider multiaspect ranking or look at co occurrences beyond the document level , both of which are central to our model .", "Parallel to this study Branavan et al . 2008 also showed that joint models of text and user annotations benefit extractive summarization .", "In particular , they used signals from pros cons lists whereas our models use aspect rating signals .", "In this paper we presented a joint model of text and aspect ratings for extracting text to be displayed in sentiment summaries .", "The model uses aspect ratings to discover the corresponding topics and can thus extract fragments of text discussing these aspects without the need of annotated data .", "We demonstrated that the model indeed discovers corresponding coherent topics and achieves accuracy in sentence labeling comparable to a standard supervised model .", "The primary area of future work is to incorporate the model into an end to end sentiment summarization system in order to evaluate it at that level .", "This work benefited from discussions with Sasha Blair Goldensohn and Fernando Pereira ."], "summary_lines": ["A Joint Model of Text and Aspect Ratings for Sentiment Summarization\n", "Online reviews are often accompanied with numerical ratings provided by users for a set of service or product aspects.\n", "We propose a statistical model which is able to discover corresponding topics in text and extract textual evidence from reviews supporting each of these aspect ratings \u2013 a fundamental problem in aspect-based sentiment summarization (Hu and Liu, 2004a).\n", "Our model achieves high accuracy, without any explicitly labeled data except the user provided opinion ratings.\n", "The proposed approach is general and can be used for segmentation in other applications where sequential data is accompanied with correlated signals.\n", "In contrast, MLSLDA draws on techniques that view sentiment as a regression problem based on the topics used in a document, as in supervised latent Dirichlet allocation (SLDA) (Blei and McAuliffe, 2007) or in finer-grained parts of a document (Titov and McDonald, 2008).\n", "We propose a joint model of text and aspect ratings which utilizes a modified LDA topic model to build topics that are representative of ratable aspects, and builds a set of sentiment predictors.\n"]}
{"article_lines": ["Learning Dependency Based Compositional Semantics", "Compositional question answering begins by mapping questions to logical forms , but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms .", "In this paper , we learn to map questions to answers via latent logical forms , which are induced automatically from question answer pairs .", "In tackling this challenging learning problem , we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms .", "On two stansemantic parsing benchmarks our system obtains the highest published accuracies , despite requiring no annotated logical forms .", "What is the total population of the ten largest capitals in the US ?", "Answering these types of complex questions compositionally involves first mapping the questions into logical forms semantic parsing .", "Supervised semantic parsers Zelle and Mooney , 1996 ; Tang and Mooney , 2001 ; Ge and Mooney , 2005 ; Zettlemoyer and Collins , 2005 ; Kate and Mooney , 2007 ; Zettlemoyer and Collins , 2007 ; Wong and Mooney , 2007 ; Kwiatkowski et al . , 2010 rely on manual annotation of logical forms , which is expensive .", "On the other hand , existing unsupervised semantic parsers Poon and Domingos , 2009 do not handle deeper linguistic phenomena such as quantification , negation , and superlatives .", "As in Clarke et al . 2010 , we obviate the need for annotated logical forms by considering the endto end problem of mapping questions to answers .", "However , we still model the logical form now as a latent variable to capture the complexities of language .", "Figure 1 shows our probabilistic model with respect to a world w database of facts , producing an answer y .", "We represent logical forms z as labeled trees , induced automatically from x , y pairs .", "We want to induce latent logical forms z and parameters 0 given only question answer pairs x , y , which is much cheaper to obtain than x , z pairs .", "The core problem that arises in this setting is program induction finding a logical form z over an exponentially large space of possibilities that produces the target answer y .", "Unlike standard semantic parsing , our end goal is only to generate the correct y , so we are free to choose the representation for z .", "Which one should we use ?", "The dominant paradigm in compositional semantics is Montague semantics , which constructs lambda calculus forms in a bottom up manner .", "CCG is one instantiation Steedman , 2000 , which is used by many semantic parsers , e . g . , Zettlemoyer and Collins 2005 .", "However , the logical forms there can become quite complex , and in the context of program induction , this would lead to an unwieldy search space .", "At the same time , representations such as FunQL Kate et al . , 2005 , which was used in Clarke et al . 2010 , are simpler but lack the full expressive power of lambda calculus .", "The main technical contribution of this work is a new semantic representation , dependency based compositional semantics DCS , which is both simple and expressive Section 2 .", "The logical forms in this framework are trees , which is desirable for two reasons i they parallel syntactic dependency trees , which facilitates parsing and learning ; and ii evaluating them to obtain the answer is computationally efficient .", "We trained our model using an EM like algorithm Section 3 on two benchmarks , GEO and JOBS Section 4 .", "Our system outperforms all existing systems despite using no annotated logical forms .", "We first present a basic version Section 2 . 1 of dependency based compositional semantics DCS , which captures the core idea of using trees to represent formal semantics .", "We then introduce the full version Section 2 . 2 , which handles linguistic phenomena such as quantification , where syntactic and semantic scope diverge .", "We start with some definitions , using US geography as an example domain .", "Let V be the set of all values , which includes primitives e . g . , 3 , CA V as well as sets and tuples formed from other values e . g . , 3 , 3 , 4 , 7 , CA , 5 V .", "Let P be a set of predicates e . g . , state , count P , which are just symbols .", "A world w is mapping from each predicate p P to a set of tuples ; for example , w state CA , OR , . . . .", "Conceptually , a world is a relational database where each predicate is a relation possibly infinite .", "Define a special predicate \u00f8 with w \u00f8 V . We represent functions by a set of inputoutput pairs , e . g . , w count S , n n S .", "As another example , w average S , x We write a DCS tree z as hp ; r1 c1 ; . . . ; rm cmi .", "Figure 2 a shows an example of a DCS tree .", "Although a DCS tree is a logical form , note that it looks like a syntactic dependency tree with predicates in place of words .", "It is this transparency between syntax and semantics provided by DCS which leads to a simple and streamlined compositional semantics suitable for program induction .", "The basic version of DCS restricts R to join and aggregate relations see Table 1 .", "Let us start by considering a DCS tree z with only join relations .", "Such a z defines a constraint satisfaction problem CSP with nodes as variables .", "The CSP has two types of constraints i x w p for each node x labeled with predicate p P ; and ii xj yj0 the j th component of x must equal the j' th component of y for each edge x , y labeled with j0j R . A solution to the CSP is an assignment of nodes to values that satisfies all the constraints .", "We say a value v is consistent for a node x if there exists a solution that assigns v to x .", "The denotation JzKw z evaluated on w is the set of consistent values of the root node see Figure 2 for an example .", "Computation We can compute the denotation JzKw of a DCS tree z by exploiting dynamic programming on trees Dechter , 2003 .", "The recurrence is as follows At each node , we compute the set of tuples v consistent with the predicate at that node v w p , and S x , where a set of pairs S is treated as a set valued function S x y x , y S with domain S1 x x , y S .", "The logical forms in DCS are called DCS trees , where nodes are labeled with predicates , and edges are labeled with relations .", "Formally Definition 1 DCS trees Let Z be the set of DCS trees , where each z Z consists of i a predicate for each child i , the ji th component of v must equal the j'i th component of some t in the child s denotation t JciKw .", "This algorithm is linear in the number of nodes times the size of the denotations . 1 Now the dual importance of trees in DCS is clear We have seen that trees parallel syntactic dependency structure , which will facilitate parsing .", "In addition , trees enable efficient computation , thereby establishing a new connection between dependency syntax and efficient semantic evaluation .", "Aggregate relation DCS trees that only use join relations can represent arbitrarily complex compositional structures , but they cannot capture higherorder phenomena in language .", "For example , consider the phrase number of major cities , and suppose that number corresponds to the count predicate .", "It is impossible to represent the semantics of this phrase with just a CSP , so we introduce a new aggregate relation , notated E . Consider a tree hE ci , whose root is connected to a child c via E . If the denotation of c is a set of values s , the parent s denotation is then a singleton set containing s . Formally Figure 3 a shows the DCS tree for our running example .", "The denotation of the middle node is s , where s is all major cities .", "Having instantiated s as a value , everything above this node is an ordinary CSP s constrains the count node , which in turns constrains the root node to s .", "A DCS tree that contains only join and aggregate relations can be viewed as a collection of treestructured CSPs connected via aggregate relations .", "The tree structure still enables us to compute denotations efficiently based on 1 and 2 .", "The basic version of DCS described thus far handles a core subset of language .", "But consider Figure 4 a is headed by borders , but states needs to be extracted ; in b , the quantifier no is syntactically dominated by the head verb borders but needs to take wider scope .", "We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope .", "The key idea that allows us to give semanticallyscoped denotations to syntactically scoped trees is as follows We mark a node low in the tree with a mark relation one of E , Q , or C .", "Then higher up in the tree , we invoke it with an execute relation Xi to create the desired semantic scope . 2 This mark execute construct acts non locally , so to maintain compositionality , we must augment the denotation d JzKw to include any information about the marked nodes in z that can be accessed by an execute relation later on .", "In the basic version , d was simply the consistent assignments to the root .", "Now d contains the consistent joint assignments to the active nodes which include the root and all marked nodes , as well as information stored about each marked node .", "Think of d as consisting of n columns , one for each active node according to a pre order traversal of z .", "Column 1 always corresponds to the root node .", "Formally , a denotation is defined as follows see Figure 5 for an example Definition 2 Denotations Let D be the set of denotations , where each d E D consists of where each store a contains a mark relation a . r E E , Q , C , 0 , a base denotation a . b E D U 0 , and a child denotation a . c E D U 0 .", "We write d as A ; ri , bi , ci ; . . . ; rn , bn , cn .", "We use d ri x to mean d with d . ri d . ai . r x similar definitions apply for d ai x , d bi x , and d ci x .", "The denotation of a DCS tree can now be defined recursively The base case is defined in 3 if z is a single node with predicate p , then the denotation of z has one column with the tuples w p and an empty store .", "The other six cases handle different edge relations .", "These definitions depend on several operations mj , j0 , E , Xi , M which we will define shortly , but let us first get some intuition .", "Let z be a DCS tree .", "If the last child c of z s root is a join jj0 , aggregate E , or execute Xi relation 4 6 , then we simply recurse on z with c removed and join it with some transformation identity , E , or Xi of c s denotation .", "If the last or first child is connected via a mark relation E , C or Q , then we strip off that child and put the appropriate information in the store by invoking M . We now define the operations mj , j0 , E , Xi , M . Some helpful notation For a sequence v v1 , . . . , vn and indices i i1 , . . . , ik , let vi vi1 , . . . , vik be the projection of v onto i ; we write v i to mean v 1 i .", "Extending this notation to denotations , let hA ; \u03b1ii i hh ai a A ; \u03b1iii .", "Let d \u00f8 d i , where i are the columns with empty stores .", "For example , for d in Figure 5 , d 1 keeps column 1 , d \u00f8 keeps column 2 , and d 2 , 2 swaps the two columns .", "Join The join of two denotations d and d' with respect to components j and j' means all components is formed by concatenating all arrays a of d with all compatible arrays a' of d' , where compatibility means a1j a'1j0 .", "The stores are also concatenated \u03b1 \u03b1' .", "Non initial columns with empty stores are projected away by applying 1 , \u00f8 .", "The full definition of join is as follows Aggregate The aggregate operation takes a denotation and forms a set out of the tuples in the first column for each setting of the rest of the columns Now we turn to the mark M and execute Xi operations , which handles the divergence between syntactic and semantic scope .", "In some sense , this is the technical core of DCS .", "Marking is simple When a node e . g . , size in Figure 5 is marked e . g . , with relation C , we simply put the relation r , current denotation d and child c s denotation into the store of column 1 The execute operation Xi d processes columns i in reverse order .", "It suffices to define Xi d for a single column i .", "There are three cases Extraction d . ri E In the basic version , the denotation of a tree was always the set of consistent values of the root node .", "Extraction allows us to return the set of consistent values of a marked non root node .", "Formally , extraction simply moves the i th column to the front Xi d d i , i , \u00f8 \u03b11 \u00f8 .", "For example , in Figure 4 a , before execution , the denotation of the DCS tree is hh CA , OR , OR , . . . ; \u00f8 ; E , Qhstatei w , \u00f8 ii ; after applying X1 , we have hh OR , . . . ; \u00f8ii .", "Generalized Quantification d . ri Q Generalized quantifiers are predicates on two sets , a restrictor A and a nuclear scope B .", "For example , In a DCS tree , the quantifier appears as the child of a Q relation , and the restrictor is the parent see Figure 4 b for an example .", "This information is retrieved from the store when the quantifier in column i is executed .", "In particu using the exact same machinery as superlatives .", "Figlar , the restrictor is A E d . bi and the nu ure 4 g shows that we can naturally account for clear scope is B E d i , i , 0 .", "We then superlative ambiguity based on where the scopeapply d . ci to these two sets technically , denota determining execute relation is placed . tions and project away the first column Xi d 3 Semantic Parsing d . ci . 1 , 1 A . 2 , 1 B 1 .", "We now turn to the task of mapping natural language For the example in Figure 4 b , the de utterances to DCS trees .", "Our first question is given notation of the DCS tree before execution is an utterance x , what trees z Z are permissible ?", "To California cities , and it also allows us to underspecify L . In particular , our L will not include verbs or prepositions ; rather , we rely on the predicates corresponding to those words to be triggered by traces .", "The augmentation function A takes a set of trees and optionally attaches E and Xi relations to the root e . g . , A hcityi hcityi , hcity ; E \u00f8i .", "The filtering function F rules out improperly typed trees such as hcity ; 00 hstateii .", "To further reduce the search space , F imposes a few additional constraints , e . g . , limiting the number of marked nodes to 2 and only allowing trace predicates between arity 1 predicates .", "Model We now present our discriminative semantic parsing model , which places a log linear distribution over z ZL x given an utterance x .", "Formally , p\u03b8 z x e\u03c6 x , z T\u03b8 , where \u03b8 and \u03c6 x , z are parameter and feature vectors , respectively .", "As a running example , consider x city that is in California and z hcity ; 11 hloc ; 21 hCAiii , where city triggers city and California triggers CA .", "To define the features , we technically need to augment each tree z ZL x with alignment information namely , for each predicate in z , the span in x if any that triggered it .", "This extra information is already generated from the recursive definition in 13 .", "The feature vector \u03c6 x , z is defined by sums of five simple indicator feature templates F1 a word triggers a predicate e . g . , city , city ; F2 a word is under a relation e . g . , that , 11 ; F3 a word is under a trace predicate e . g . , in , loc ; F4 two predicates are linked via a relation in the left or right direction e . g . , city , 11 , loc , RIGHT ; and F5 a predicate has a child relation e . g . , city , 11 .", "Learning Given a training dataset D containing x , y pairs , we define the regularized marginal log likelihood objective O \u03b8 E x , y ED log p\u03b8 JzKw y x , z ZL x \u03bbk\u03b8k22 , which sums over all DCS trees z that evaluate to the target answer y .", "Our model is arc factored , so we can sum over all DCS trees in ZL x using dynamic programming .", "However , in order to learn , we need to sum over z ZL x JzKw y , and unfortunately , the additional constraint JzKw y does not factorize .", "We therefore resort to beam search .", "Specifically , we truncate each Ci , j to a maximum of K candidates sorted by decreasing score based on parameters \u03b8 .", "Let ZL , \u03b8 x be this approximation of ZL x .", "Our learning algorithm alternates between i using the current parameters \u03b8 to generate the K best set ZL , \u03b8 x for each training example x , and ii optimizing the parameters to put probability mass on the correct trees in these sets ; sets containing no correct answers are skipped .", "Formally , let O \u03b8 , \u03b8' be the objective function O \u03b8 with ZL x ZL , \u03b8I x .", "We optimize O \u03b8 , \u03b8' by setting \u03b8 0 0 and iteratively solving \u03b8 t 1 argmax\u03b8 O \u03b8 , \u03b8 t using L BFGS until t T . In all experiments , we set \u03bb 0 . 01 , T 5 , and K 100 .", "After training , given a new utterance x , our system outputs the most likely y , summing out the latent logical form z argmaxy p\u03b8 T y x , z ZL , \u03b8 T .", "We tested our system on two standard datasets , GEO and JOBS .", "In each dataset , each sentence x is annotated with a Prolog logical form , which we use only to evaluate and get an answer y .", "This evaluation is done with respect to a world w . Recall that a world w maps each predicate p P to a set of tuples w p .", "There are three types of predicates in P generic e . g . , argmax , data e . g . , city , and value e . g . , CA .", "GEO has 48 non value predicates and JOBS has 26 .", "For GEO , w is the standard US geography database that comes with the dataset .", "For JOBS , if we use the standard Jobs database , close to half the y s are empty , which makes it uninteresting .", "We therefore generated a random Jobs database instead as follows we created 100 job IDs .", "For each data predicate p e . g . , language , we add each possible tuple e . g . , job37 , Java to w p independently with probability 0 . 8 .", "We used the same training test splits as Zettlemoyer and Collins 2005 600 280 for GEO and 500 140 for JOBS .", "During development , we further held out a random 30 of the training sets for validation .", "Our lexical triggers L include the following i predicates for a small set of 20 function words e . g . , most , argmax , ii x , x for each value predicate x in w e . g . , Boston , Boston , and iii predicates for each POS tag in JJ , NN , NNS e . g . , JJ , size , JJ , area , etc .", ". 3 Predicates corresponding to verbs and prepositions e . g . , traverse are not included as overt lexical triggers , but rather in the trace predicates L E .", "We also define an augmented lexicon L which includes a prototype word x for each predicate appearing in iii above e . g . , large , size , which cancels the predicates triggered by x s POS tag .", "For GEO , there are 22 prototype words ; for JOBS , there are 5 .", "Specifying these triggers requires minimal domain specific supervision .", "Results We first compare our system with Clarke et al . 2010 henceforth , SEMRESP , which also learns a semantic parser from question answer pairs .", "Table 2 shows that our system using lexical triggers L henceforth , DCS outperforms SEMRESP 78 . 9 over 73 . 2 .", "In fact , although neither DCS nor SEMRESP uses logical forms , DCS uses even less supervision than SEMRESP .", "SEMRESP requires a lexicon of 1 . 42 words per non value predicate , WordNet features , and syntactic parse trees ; DCS requires only words for the domain independent predicates overall , around 0 . 5 words per non value predicate , POS tags , and very simple indicator features .", "In fact , DCS performs comparably to even the version of SEMRESP trained using logical forms .", "If we add prototype triggers use L , the resulting system DCS outperforms both versions of SEMRESP by a significant margin 87 . 2 over 73 . 2 and 80 . 4 .", "Next , we compared our systems DCS and DCS with the state of the art semantic parsers on the full dataset for both GEO and JOBS see Table 3 .", "All other systems require logical forms as training data , whereas ours does not .", "Table 3 shows that even DCS , which does not use prototypes , is comparable to the best previous system Kwiatkowski et al . , 2010 , and by adding a few prototypes , DCS offers a decisive edge 91 . 1 over 88 . 9 on GEO .", "Rather than using lexical triggers , several of the other systems use IBM word alignment models to produce an initial word predicate mapping .", "This option is not available to us since we do not have annotated logical forms , so we must instead rely on lexical triggers to define the search space .", "Note that having lexical triggers is a much weaker requirement than having a CCG lexicon , and far easier to obtain than logical forms .", "Intuitions How is our system learning ?", "Initially , the weights are zero , so the beam search is essentially unguided .", "We find that only for a small fraction of training examples do the K best sets contain any trees yielding the correct answer 29 for DCS on GEO .", "However , training on just these examples is enough to improve the parameters , and this 29 increases to 66 and then to 95 over the next few iterations .", "This bootstrapping behavior occurs naturally The easy examples are processed first , where easy is defined by the ability of the current model to generate the correct answer using any tree . with scope variation .", "Think of DCS as a higher level Our system learns lexical associations between programming language tailored to natural language , words and predicates .", "For example , area by virtue which results in programs DCS trees which are of being a noun triggers many predicates city , much simpler than the logically equivalent lambda state , area , etc .", "Inspecting the final parameters calculus formulae .", "DCS on GEO , we find that the feature area , area The idea of using CSPs to represent semantics is has a much higher weight than area , city .", "Trace inspired by Discourse Representation Theory DRT predicates can be inserted anywhere , but the fea Kamp and Reyle , 1993 ; Kamp et al . , 2005 , where tures favor some insertions depending on the words variables are discourse referents .", "The restriction to present for example , in , loc has high weight . trees is similar to economical DRT Bos , 2009 .", "The errors that the system makes stem from mul The other major focus of this work is program tiple sources , including errors in the POS tags e . g . , induction inferring logical forms from their denostates is sometimes tagged as a verb , which triggers tations .", "There has been a fair amount of past work on no predicates , confusion of Washington state with this topic Liang et al . 2010 induces combinatory Washington D . C . , learning the wrong lexical asso logic programs in a non linguistic setting .", "Eisenciations due to data sparsity , and having an insuffi stein et al . 2009 induces conjunctive formulae and ciently large K . uses them as features in another learning problem .", "5 Discussion Piantadosi et al . 2008 induces first order formuA major focus of this work is on our semantic rep lae using CCG in a small domain assuming observed resentation , DCS , which offers a new perspective lexical semantics .", "The closest work to ours is Clarke on compositional semantics .", "To contrast , consider et al . 2010 , which we discussed earlier .", "CCG Steedman , 2000 , in which semantic pars The integration of natural language with denotaing is driven from the lexicon .", "The lexicon en tions computed against a world grounding is becodes information about how each word can used in coming increasingly popular .", "Feedback from the context ; for example , the lexical entry for borders world has been used to guide both syntactic parsing is S NP NP Ay . Ax . border x , y , which means Schuler , 2003 and semantic parsing Popescu et borders looks right for the first argument and left al . , 2003 ; Clarke et al . , 2010 .", "Past work has also fofor the second .", "These rules are often too stringent , cused on aligning text to a world Liang et al . , 2009 , and for complex utterances , especially in free word using text in reinforcement learning Branavan et al . , order languages , either disharmonic combinators are 2009 ; Branavan et al . , 2010 , and many others .", "Our employed Zettlemoyer and Collins , 2007 or words work pushes the grounded language agenda towards are given multiple lexical entries Kwiatkowski et deeper representations of language think grounded al . , 2010 . compositional semantics .", "In DCS , we start with lexical triggers , which are 6 Conclusion more basic than CCG lexical entries .", "A trigger for We built a system that interprets natural language borders specifies only that border can be used , but utterances much more accurately than existing sysnot how .", "The combination rules are encoded in the tems , despite using no annotated logical forms .", "Our features as soft preferences .", "This yields a more system is based on a new semantic representation , factorized and flexible representation that is easier DCS , which offers a simple and expressive alterto search through and parametrize using features . native to lambda calculus .", "Free from the burden It also allows us to easily add new lexical triggers of annotating logical forms , we hope to use our without becoming mired in the semantic formalism . techniques in developing even more accurate and Quantifiers and superlatives significantly compli broader coverage language understanding systems . cate scoping in lambda calculus , and often type rais Acknowledgments We thank Luke Zettlemoyer ing needs to be employed .", "In DCS , the mark execute and Tom Kwiatkowski for providing us with data construct provides a flexible framework for dealing and answering questions ."], "summary_lines": ["Learning Dependency-Based Compositional Semantics\n", "Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.\n", "In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.\n", "In tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.\n", "On two standard semantic parsing benchmarks (GEO and JOBS), our system obtains the highest published accuracies, despite requiring no annotated logical forms.\n", "We DCS for dependency-based compositional semantics, which represents a semantic parse as a tree with nodes representing database elements and operations, and edges representing relational joins.\n", "Dependency-based Compositional Semantics (DCS) provides an intuitive way to model semantics of questions, by using simple dependency-like trees.\n"]}
{"article_lines": ["Learning Non Isomorphic Tree Mappings For Machine Translation", "Often one may wish to learn a tree to tree mapping , training it on unaligned pairs of trees , or on a mixture of trees and strings .", "Unlike previous statistical formalisms limited to isomorphic TSG local distortion of the tree topology .", "We reformulate it to permit dependency trees , and sketch EM Viterbi algorithms for alignment , training , and decoding .", "Statistical machine translation systems are trained on pairs of sentences that are mutual translations .", "For example , beaucoup d enfants donnent un baiser a Sam , kids kiss Sam quite often .", "This translation is somewhat free , as is common in naturally occurring data .", "The first sentence is literally Lots of children give a kiss to Sam .", "This short paper outlines natural formalisms and algorithms for training on pairs of trees .", "Our methods work on either dependency trees as shown or phrase structure trees .", "Note that the depicted trees are not isomorphic . enfants Our main concern is to develop models that can align and learn from these tree pairs despite the mismatches in tree structure .", "Many mismatches are characteristic of a language pair e . g . , preposition insertion of c , multiword locutions kiss H give a kiss to ; misinform H wrongly inform , and head swapping float down H descend byfloating .", "Such systematic mismatches should be learned by the model , and used during translation .", "It is even helpful to learn mismatches that merely tend to arise during free translation .", "Knowing that beaucoup d is often deleted will help in aligning the rest of the tree .", "When would learned tree to tree mappings be useful ?", "Obviously , in MT , when one has parsers for both the source and target language .", "Systems for deep analysis and generation might wish to learn mappings between deep and surface trees B ohmov a et al . , 2001 or between syntax and semantics Shieber and Schabes , 1990 .", "Systems for summarization or paraphrase could also be trained on tree pairs Knight and Marcu , 2000 .", "Non NLP applications might include comparing studentwritten programs to one another or to the correct solution .", "Our methods can naturally extend to train on pairs of forests including packed forests obtained by chart parsing .", "The correct tree is presumed to be an element of the forest .", "This makes it possible to train even when the correct parse is not fully known , or not known at all .", "We make the quite natural proposal of using a synchronous tree substitution grammar STSG .", "An STSG is a collection of ordered pairs of aligned elementary trees .", "These may be combined into a derived pair of trees .", "Both the elementary tree pairs and the operation to combine them will be formalized in later sections .", "As an example , the tree pair shown in the introduction might have been derived by vertically assembling the 6 elementary tree pairs below .", "The symbol denotes a frontier node of an elementary tree , which must be replaced by the circled root of another elementary tree .", "If two frontier nodes are linked by a dashed line labeled with the state X , then they must be replaced by two roots that are also linked by a dashed line labeled with X .", "The elementary trees represent idiomatic translation chunks . The frontier nodes represent unfilled roles in the chunks , and the states are effectively nonterminals that specify the type of filler that is required .", "Thus , donnent un baiser a give a kiss to corresponds to kiss , with the French subject matched to the English subject , and the French indirect object matched to the English direct object .", "The states could be more refined than those shown above the state for the subject , for example , should probably be not NP but a pair Npl , NP3s .", "STSG is simply a version of synchronous treeadjoining grammar or STAG Shieber and Schabes , 1990 that lacks the adjunction operation .", "It is also equivalent to top down tree transducers .", "What , then , is new here ?", "First , we know of no previous attempt to learn the chunk to chunk mappings .", "That is , we do not know at training time how the tree pair of section 1 was derived , or even what it was derived from .", "Our approach is to reconstruct all possible derivations , using dynamic programming to decompose the tree pair into aligned pairs of elementary trees in all possible ways .", "This produces a packed forest of derivations , some more probable than others .", "We use an efficient inside outside algorithm to do Expectation Maximization , reestimating the model by training on all derivations in proportion to their probabilities .", "The runtime is quite low when the training trees are fully specified and elementary trees are bounded in size . 1 Second , it is not a priori obvious that one can reasonably use STSG instead of the slower but more powerful STAG .", "TSG can be parsed as fast as CFG .", "But without an adjunction operation , 2 , one cannot break the training trees into linguistically minimal units .", "An elementary tree pair A elle est finalement partie , finally she left cannot be further decomposed into B elle est partie , she left and C finalement , finally .", "This appears to miss a generalization .", "Our perspective is that the generalization should be picked up by the statistical model that defines the probability of elementary tree pairs . p A can be defined using mainly the same parameters that define p B and p C , with the result that p A , p B p C .", "The balance between the STSG and the statistical model is summarized in the last paragraph of this paper .", "Third , our version of the STSG formalism is more flexible than previous versions .", "We carefully address the case of empty trees , which are needed to handle freetranslation mismatches . In the example , an STSG cannot replace beaucoup d lots of in the NP by quite often in the VP ; instead it must delete the former and insert the latter .", "Thus we have the alignments beaucoup d , e and e , quite often .", "These require innovations .", "The tree internal deletion of beaucoup d is handled by an empty elementary tree in which the root is itself a frontier node .", "The subject frontier node of kiss is replaced with this frontier node , which is then replaced with kids .", "The tree peripheral insertion of quite often requires an English frontier node that is paired with a French null .", "We also formulate STSGs flexibly enough that they can handle both phrase structure trees and dependency trees .", "The latter are small and simple Alshawi et al . , 2000 tree nodes are words , and there need be no other structure to recover or align .", "Selectional preferences and other interactions can be accommodated by enriching the states .", "Any STSG has a weakly equivalent SCFG that generates the same string pairs .", "So STSG unlike STAG has no real advantage for modeling string pairs . 3 But STSGs can generate a wider variety of tree pairs , e . g . , non isomorphic ones .", "So when actual trees are provided for training , STSG can be more flexible in aligning them .", "1Goodman 2002 presents efficient TSG parsing with unbounded elementary trees .", "Unfortunately , that clever method does not permit arbitrary models of elementary tree probabilities , nor does it appear to generalize to our synchronous case .", "It would need exponentially many nonterminals to keep track of an matching of unboundedly many frontier nodes .", "Most statistical MT derives from IBM style models Brown et al . , 1993 , which ignore syntax and allow arbitrary word to word translation .", "Hence they are able to align any sentence pair , however mismatched .", "However , they have a tendency to translate long sentences into word salad .", "Their alignment and translation accuracy improves when they are forced to translate shallow phrases as contiguous , potentially idiomatic units Och et al . , 1999 .", "Several researchers have tried putting more syntax into translation models like us , they use statistical versions of synchronous grammars , which generate source and target sentences in parallel and so describe their correspondence . 4 This approach offers four features absent from IBM style models 1 a recursive phrase based translation , 2 a syntax based language model , 3 the ability to condition a word s translation on the translation of syntactically related words , and 4 polynomial time optimal alignment and decoding Knight , 1999 .", "Previous work in statistical synchronous grammars has been limited to forms of synchronous context free grammar Wu , 1997 ; Alshawi et al . , 2000 ; Yamada and Knight , 2001 .", "This means that a sentence and its translation must have isomorphic syntax trees , although they may have different numbers of surface words if null words a are allowed in one or both languages .", "This rigidity does not fully describe real data .", "The one exception is the synchronous DOP approach of Poutsma , 2000 , which obtains an STSG by decomposing aligned training trees in all possible ways and using naive count based probability estimates .", "However , we would like to estimate a model from unaligned data .", "For expository reasons and to fill a gap in the literature , first we formally present non synchronous TSG .", "Let Q be a set of states .", "Let L be a set of labels that may decorate nodes or edges .", "Node labels might be words or nonterminals .", "Edge labels might include grammatical roles such as Subject .", "In many trees , each node s children have an order , recorded in labels on the node s outgoing edges .", "An elementary tree is a a tuple V , V i , E , , q , s where V is a set of nodes ; V i C_ V is the set of internal nodes , and we write V f V V i for the set of frontier nodes ; E C_ V i x V is a set of directed edges thus all frontier nodes are leaves .", "The graph V , E must be connected and acyclic , and there must be exactly one node r E V the root that has no incoming edges .", "The function V i U E L labels each internal node or edge ; q E Q is the root state , and s V f Q assigns a frontier state to each frontier node perhaps including r .", "4The joint probability model can be formulated , if desired , as a language model times a channel model .", "A TSG is a set of elementary trees .", "The generation process builds up a derived tree T that has the same form as an elementary tree , and for which V f 0 .", "Initially , T is chosen to be any elementary tree whose root state T . q Start .", "As long as T has any frontier nodes , T . V f , the process expands each frontier node d E T . V f by substituting at d an elementary tree t whose root state , t . q , equals d s frontier state , T . s d .", "This operation replaces T with T . V U t . V d , T . V i U t . V i , T . E' U t . E , T . B U t . , T . q , T . s U t . s d , t . q .", "Note that a function is regarded here as a set of input , output pairs .", "T . E' is a version of T . E in which d has been been replaced by t . r .", "A probabilistic TSG also includes a function p t q , which , for each state q , gives a conditional probability distribution over the elementary trees t with root state q .", "The generation process uses this distribution to randomly choose which tree t to substitute at a frontier node of T having state q .", "The initial value of T is chosen from p t Start .", "Thus , the probability of a given derivation is a product of p t q terms , one per chosen elementary tree .", "There is a natural analogy between probabilistic TSGs and probabilistic CFGs .", "An elementary tree t with root state q and frontier states ql . . . qk for k 0 is analogous to a CFG rule q t ql . . . qk .", "By including t as a terminal symbol in this rule , we ensure that distinct elementary trees t with the same states correspond to distinct rules .", "Indeed , an equivalent definition of the generation process first generates a derivation tree from this derivation CFG , and then combines its terminal nodes t which are elementary trees into the derived tree T .", "Given a a grammar G and a derived tree T , we may be interested in constructing the forest of T s possible derivation trees as defined above .", "We call this tree parsing , as it finds ways of decomposing T into elementary trees .", "Given a node c E T . v , we would like to find all the potential elementary subtrees t of T whose root t . r could have contributed c during the derivation of T . Such an elementary tree is said to fit c , in the sense that it is isomorphic to some subgraph of T rooted at c . The following procedure finds an elementary tree t that fits c . Freely choose a connected subgraph U of T such that U is rooted at c or is empty .", "Let t . V i be the vertex set of U .", "Let t . E be the set of outgoing edges from nodes in t . Vi to their children , that is , t . E T . E n t . V i x T . V .", "Let t . be the restriction of T . to t . Vi U t . E , that is , t . T . n t . V i U t . E x L .", "Let t . V be the set of nodes mentioned in t . E , or put t . V c if t . Vi t . E 0 .", "Finally , choose t . q freely from Q , and choose s t . Vf Q to associate states with the frontier nodes of t ; the free choice is because the nodes of the derived tree T do not specify the states used during the derivation .", "How many elementary trees can we find that fit c ?", "Let us impose an upper bound k on t . V i and hence on U .", "Then in an m ary tree T , the above procedure considers at most mk 1 m 1 connected subgraphs U of order k rooted at c . For dependency grammars , limiting to m 6 and k 3 is quite reasonable , leaving at most 43 subgraphs U rooted at each node c , of which the biggest contain only c , a child c' of c , and a child or sibling of c' .", "These will constitute the internal nodes of t , and their remaining children will be t s frontier nodes .", "However , for each of these 43 subgraphs , we must jointly hypothesize states for all frontier nodes and the root node .", "For Q 1 , there are exponentially many ways to do this .", "To avoid having exponentially many hypotheses , one may restrict the form of possible elementary trees so that the possible states of each node of t can be determined somehow from the labels on the corresponding nodes in T . As a simple but useful example , a node labeled NP might be required to have state NP .", "Rich labels on the derived tree essentially provide supervision as to what the states must have been during the derivation .", "The tree parsing algorithm resembles bottom up chart parsing under the derivation CFG .", "But the input is a tree rather than a string , and the chart is indexed by nodes of the input tree rather than spans of the input string 5 The \u03b2 values are inside probabilities .", "After running the algorithm , if r is the root of T , then \u03b2r Start is the probability that the grammar generates T . p t q in line 4 may be found by hash lookup if the grammar is stored explicitly , or else by some probabilistic model that analyzes the structure , labels , and states of the elementary tree t to compute its probability .", "One can mechanically transform this algorithm to compute outside probabilities , the Viterbi parse , the parse forest , and other quantities Goodman , 1999 .", "One can also apply agenda based parsing strategies .", "For a fixed grammar , the runtime and space are only O n for a tree of n nodes .", "The grammar constant is the number of possible fits to a node c of a fixed tree .", "As noted above , there usually not many of these unless the states are uncertain and they are simple to enumerate .", "As discussed above , an inside outside algorithm may be used to compute the expected number of times each elementary tree t appeared in the derivation of T . That is the E step of the EM algorithm .", "In the M step , these expected counts collected over a corpus of trees are used to reestimate the parameters \u03b8 of p t q .", "One alternates O converges to a local maximum .", "The prior p O can discourage overfitting .", "We are now prepared to discuss the synchronous case .", "A synchronous TSG consists of a set of elementary tree pairs .", "An elementary tree pair t is a tuple t1 , t2 , q , m , s .", "Here t1 and t2 are elementary trees without state labels we write tj Vj , Vji , Ej , j . q E Q is the root state as before . m C_ V1f x V2f is a matching between t1 s and t2 s frontier nodes , 6 .", "Let m denote m U d1 , null d1 is unmatched in m U null , d2 d2 is unmatched in m .", "Finally , s m Q assigns a state to each frontier node pair or unpaired frontier node .", "In the figure of section 2 , donnent un baiser a has 2 frontier nodes and kiss has 3 , yielding 13 possible matchings .", "Note that least one English node must remain unmatched ; it still generates a full subtree , aligned with null .", "As before , a derived tree pair T has the same form as an elementary tree pair .", "The generation process is similar to before .", "As long as T . m 0 , the process expands some node pair d1 , d2 E T . m .", "It chooses an elementary tree pair t such that t . q T . s d1 , d2 .", "Then for each j 1 , 2 , it substitutes tj at dj if non null .", "If dj is null , then t . q must guarantee that tj is the special null tree .", "In the probabilistic case , we have a distribution p t q just as before , but this time t is an elementary tree pair .", "Several natural algorithms are now available to us Training .", "Given an unaligned tree pair T1 , T2 , we can again find the forest of all possible derivations , with expected inside outside counts of the elementary tree pairs .", "This allows EM training of the p t q model .", "The algorithm is almost as before .", "The outer loop iterates bottom up over nodes c1 of T1 ; an inner loop iterates bottom up over c2 of T2 .", "Inside probabilities for example now have the form \u03b2c1 , c2 q .", "Although this brings the complexity up to O n2 , the real complication is that there can be many fits to c1 , c2 .", "There are still not too many elementary trees t1 and t2 rooted at c1 and c2 ; but each t1 , t2 pair may be used in many elementary tree pairs t , since there are exponentially many matchings of their frontier nodes .", "Fortunately , most pairs of frontier nodes have low \u03b2 values that indicate that their subtrees cannot be aligned well ; pairing such nodes in a matching would result in poor global probability .", "This observation can be used to prune the space of matchings greatly .", "1 best Alignment if desired .", "This is just like training , except that we use the Viterbi algorithm to find the single best derivation of the input tree pair .", "This derivation can be regarded as the optimal syntactic alignment . 7 We then extract the max probability synchronous derivation and return the T2 that it derives .", "This algorithm is essentially alignment to an unknown tree T2 ; we do not loop over its nodes c2 , but choose t2 freely ."], "summary_lines": ["Learning Non-Isomorphic Tree Mappings For Machine Translation\n", "Often one may wish to learn a tree-to-tree mapping, training it on unaligned pairs of trees, or on a mixture of trees and strings.\n", "Unlike previous statistical formalisms (limited to isomorphic trees), synchronous TSG allows local distortion of the tree topology.\n", "We reformulate it to permit dependency trees, and sketch EM/Viterbi algorithms for alignment, training, and decoding.\n", "We argue that if the parse tree of source sentence is provided, decoding (for tree-to-string and tree-to-tree models) can also be cast as a tree-parsing problem.\n", "We consider synchronous tree substitution grammar, a formalism that can account for structural mismatches, and is trained discriminatively.\n"]}
{"article_lines": ["NLTK The Natural Language Toolkit", "NLTK , the Natural Language Toolkit , is a suite of open source program modules , tutorials and problem sets , providing ready to use computational linguistics courseware .", "NLTK covers symbolic and statistical natural language processing , and is interfaced to annotated corpora .", "Students augment and replace existing components , learn structured programming by example , and manipulate sophisticated models from the outset .", "Teachers of introductory courses on computational linguistics are often faced with the challenge of setting up a practical programming component for student assignments and projects .", "This is a difficult task because different computational linguistics domains require a variety of different data structures and functions , and because a diverse range of topics may need to be included in the syllabus .", "A widespread practice is to employ multiple programming languages , where each language provides native data structures and functions that are a good fit for the task at hand .", "For example , a course might use Prolog for parsing , Perl for corpus processing , and a finite state toolkit for morphological analysis .", "By relying on the built in features of various languages , the teacher avoids having to develop a lot of software infrastructure .", "An unfortunate consequence is that a significant part of such courses must be devoted to teaching programming languages .", "Further , many interesting projects span a variety of domains , and would require that multiple languages be bridged .", "For example , a student project that involved syntactic parsing of corpus data from a morphologically rich language might involve all three of the languages mentioned above Perl for string processing ; a finite state toolkit for morphological analysis ; and Prolog for parsing .", "It is clear that these considerable overheads and shortcomings warrant a fresh approach .", "Apart from the practical component , computational linguistics courses may also depend on software for in class demonstrations .", "This context calls for highly interactive graphical user interfaces , making it possible to view program state e . g . the chart of a chart parser , observe program execution step by step e . g . execution of a finite state machine , and even make minor modifications to programs in response to what if questions from the class .", "Because of these difficulties it is common to avoid live demonstrations , and keep classes for theoretical presentations only .", "Apart from being dull , this approach leaves students to solve important practical problems on their own , or to deal with them less efficiently in office hours .", "In this paper we introduce a new approach to the above challenges , a streamlined and flexible way of organizing the practical component of an introductory computational linguistics course .", "We describe NLTK , the Natural Language Toolkit , which we have developed in conjunction with a course we have taught at the University of Pennsylvania .", "The Natural Language Toolkit is available under an open source license from http nltk . sf . net .", "NLTK runs on all platforms supported by Python , including Windows , OS X , Linux , and Unix .", "The most basic step in setting up a practical component is choosing a suitable programming language .", "A number of considerations influenced our choice .", "First , the language must have a shallow learning curve , so that novice programmers get immediate rewards for their efforts .", "Second , the language must support rapid prototyping and a short develop test cycle ; an obligatory compilation step is a serious detraction .", "Third , the code should be self documenting , with a transparent syntax and semantics .", "Fourth , it should be easy to write structured programs , ideally object oriented but without the burden associated with languages like C .", "Finally , the language must have an easy to use graphics library to support the development of graphical user interfaces .", "In surveying the available languages , we believe that Python offers an especially good fit to the above requirements .", "Python is an object oriented scripting language developed by Guido van Rossum and available on all platforms www . python . org .", "Python offers a shallow learning curve ; it was designed to be easily learnt by children van Rossum , 1999 .", "As an interpreted language , Python is suitable for rapid prototyping .", "Python code is exceptionally readable , and it has been praised as executable pseudocode . Python is an object oriented language , but not punitively so , and it is easy to encapsulate data and methods inside Python classes .", "Finally , Python has an interface to the Tk graphics toolkit Lundh , 1999 , and writing graphical interfaces is straightforward .", "Several criteria were considered in the design and implementation of the toolkit .", "These design criteria are listed in the order of their importance .", "It was also important to decide what goals the toolkit would not attempt to accomplish ; we therefore include an explicit set of nonrequirements , which the toolkit is not expected to satisfy .", "Ease of Use .", "The primary purpose of the toolkit is to allow students to concentrate on building natural language processing NLP systems .", "The more time students must spend learning to use the toolkit , the less useful it is .", "Consistency .", "The toolkit should use consistent data structures and interfaces .", "Extensibility .", "The toolkit should easily accommodate new components , whether those components replicate or extend the toolkit s existing functionality .", "The toolkit should be structured in such a way that it is obvious where new extensions would fit into the toolkit s infrastructure .", "Documentation .", "The toolkit , its data structures , and its implementation all need to be carefully and thoroughly documented .", "All nomenclature must be carefully chosen and consistently used .", "Simplicity .", "The toolkit should structure the complexities of building NLP systems , not hide them .", "Therefore , each class defined by the toolkit should be simple enough that a student could implement it by the time they finish an introductory course in computational linguistics .", "Modularity .", "The interaction between different components of the toolkit should be kept to a minimum , using simple , well defined interfaces .", "In particular , it should be possible to complete individual projects using small parts of the toolkit , without worrying about how they interact with the rest of the toolkit .", "This allows students to learn how to use the toolkit incrementally throughout a course .", "Modularity also makes it easier to change and extend the toolkit .", "Comprehensiveness .", "The toolkit is not intended to provide a comprehensive set of tools .", "Indeed , there should be a wide variety of ways in which students can extend the toolkit .", "Efficiency .", "The toolkit does not need to be highly optimized for runtime performance .", "However , it should be efficient enough that students can use their NLP systems to perform real tasks .", "Cleverness .", "Clear designs and implementations are far preferable to ingenious yet indecipherable ones .", "The toolkit is implemented as a collection of independent modules , each of which defines a specific data structure or task .", "A set of core modules defines basic data types and processing systems that are used throughout the toolkit .", "The token module provides basic classes for processing individual elements of text , such as words or sentences .", "The tree module defines data structures for representing tree structures over text , such as syntax trees and morphological trees .", "The probability module implements classes that encode frequency distributions and probability distributions , including a variety of statistical smoothing techniques .", "The remaining modules define data structures and interfaces for performing specific NLP tasks .", "This list of modules will grow over time , as we add new tasks and algorithms to the toolkit .", "The parser module defines a high level interface for producing trees that represent the structures of texts .", "The chunkparser module defines a sub interface for parsers that identify nonoverlapping linguistic groups such as base noun phrases in unrestricted text .", "Four modules provide implementations for these abstract interfaces .", "The srparser module implements a simple shift reduce parser .", "The chartparser module defines a flexible parser that uses a chart to record hypotheses about syntactic constituents .", "The pcfgparser module provides a variety of different parsers for probabilistic grammars .", "And the rechunkparser module defines a transformational regular expression based implementation of the chunk parser interface .", "The tagger module defines a standard interface for augmenting each token of a text with supplementary information , such as its part of speech or its WordNet synset tag ; and provides several different implementations for this interface .", "The fsa module defines a data type for encoding finite state automata ; and an interface for creating automata from regular expressions .", "Debugging time is an important factor in the toolkit s ease of use .", "To reduce the amount of time students must spend debugging their code , we provide a type checking module , which can be used to ensure that functions are given valid arguments .", "The type checking module is used by all of the basic data types and processing classes .", "Since type checking is done explicitly , it can slow the toolkit down .", "However , when efficiency is an issue , type checking can be easily turned off ; and with type checking is disabled , there is no performance penalty .", "Visualization modules define graphical interfaces for viewing and manipulating data structures , and graphical tools for experimenting with NLP tasks .", "The draw . tree module provides a simple graphical interface for displaying tree structures .", "The draw . tree edit module provides an interface for building and modifying tree structures .", "The draw . plot graph module can be used to graph mathematical functions .", "The draw . fsa module provides a graphical tool for displaying and simulating finite state automata .", "The draw . chart module provides an interactive graphical tool for experimenting with chart parsers .", "The visualization modules provide interfaces for interaction and experimentation ; they do not directly implement NLP data structures or tasks .", "Simplicity of implementation is therefore less of an issue for the visualization modules than it is for the rest of the toolkit .", "The classifier module defines a standard interface for classifying texts into categories .", "This interface is currently implemented by two modules .", "The classifier . naivebayes module defines a text classifier based on the Naive Bayes assumption .", "The classifier . maxent module defines the maximum entropy model for text classification , and implements two algorithms for training the model Generalized Iterative Scaling and Improved Iterative Scaling .", "The classifier . feature module provides a standard encoding for the information that is used to make decisions for a particular classification task .", "This standard encoding allows students to experiment with the differences between different text classification algorithms , using identical feature sets .", "The classifier . featureselection module defines a standard interface for choosing which features are relevant for a particular classification task .", "Good feature selection can significantly improve classification performance .", "The toolkit is accompanied by extensive documentation that explains the toolkit , and describes how to use and extend it .", "This documentation is divided into three primary categories Tutorials teach students how to use the toolkit , in the context of performing specific tasks .", "Each tutorial focuses on a single domain , such as tagging , probabilistic systems , or text classification .", "The tutorials include a high level discussion that explains and motivates the domain , followed by a detailed walk through that uses examples to show how NLTK can be used to perform specific tasks .", "Reference Documentation provides precise definitions for every module , interface , class , method , function , and variable in the toolkit .", "It is automatically extracted from docstring comments in the Python source code , using Epydoc Loper , 2002 .", "Technical Reports explain and justify the toolkit s design and implementation .", "They are used by the developers of the toolkit to guide and document the toolkit s construction .", "Students can also consult these reports if they would like further information about how the toolkit is designed , and why it is designed that way .", "NLTK can be used to create student assignments of varying difficulty and scope .", "In the simplest assignments , students experiment with an existing module .", "The wide variety of existing modules provide many opportunities for creating these simple assignments .", "Once students become more familiar with the toolkit , they can be asked to make minor changes or extensions to an existing module .", "A more challenging task is to develop a new module .", "Here , NLTK provides some useful starting points predefined interfaces and data structures , and existing modules that implement the same interface .", "As an example of a moderately difficult assignment , we asked students to construct a chunk parser that correctly identifies base noun phrase chunks in a given text , by defining a cascade of transformational chunking rules .", "The NLTK rechunkparser module provides a variety of regular expression based rule types , which the students can instantiate to construct complete rules .", "For example , ChunkRule NN . W builds chunks from sequences of consecutive nouns ; ChinkRule VB . excises verbs from existing chunks ; SplitRule NN , DT splits any existing chunk that contains a singular noun followed by determiner into two pieces ; and MergeRule JJ , JJ combines two adjacent chunks where the first chunk ends and the second chunk starts with adjectives .", "The chunking tutorial motivates chunk parsing , describes each rule type , and provides all the necessary code for the assignment .", "The provided code is responsible for loading the chunked , part of speech tagged text using an existing tokenizer , creating an unchunked version of the text , applying the chunk rules to the unchunked text , and scoring the result .", "Students focus on the NLP task only providing a rule set with the best coverage .", "In the remainder of this section we reproduce some of the cascades created by the students .", "The first example illustrates a combination of several rule types ChunkRule DT NN . VB . NN .", ", ChunkRule DT VB . NN .", ", ChunkRule .", ", UnChunkRule IN VB . CC MD RB .", ", UnChunkRule quot ; , . quot ; , MergeRule NN . DT JJ .", "CD , NN . DT JJ .", "CD , SplitRule NN .", ", DT JJ The next example illustrates a brute force statistical approach .", "The student calculated how often each part of speech tag was included in a noun phrase .", "They then constructed chunks from any sequence of tags that occurred in a noun phrase more than 50 of the time .", "In the third example , the student constructed a single chunk containing the entire text , and then excised all elements that did not belong .", "NLTK provides graphical tools that can be used in class demonstrations to help explain basic NLP concepts and algorithms .", "These interactive tools can be used to display relevant data structures and to show the step by step execution of algorithms .", "Both data structures and control flow can be easily modified during the demonstration , in response to questions from the class .", "Since these graphical tools are included with the toolkit , they can also be used by students .", "This allows students to experiment at home with the algorithms that they have seen presented in class .", "The chart parsing tool is an example of a graphical tool provided by NLTK .", "This tool can be used to explain the basic concepts behind chart parsing , and to show how the algorithm works .", "Chart parsing is a flexible parsing algorithm that uses a data structure called a chart to record hypotheses about syntactic constituents .", "Each hypothesis is represented by a single edge on the chart .", "A set of rules determine when new edges can be added to the chart .", "This set of rules controls the overall behavior of the parser e . g . , whether it parses top down or bottom up .", "The chart parsing tool demonstrates the process of parsing a single sentence , with a given grammar and lexicon .", "Its display is divided into three sections the bottom section displays the chart ; the middle section displays the sentence ; and the top section displays the partial syntax tree corresponding to the selected edge .", "Buttons along the bottom of the window are used to control the execution of the algorithm .", "The main display window for the chart parsing tool is shown in Figure 1 .", "This tool can be used to explain several different aspects of chart parsing .", "First , it can be used to explain the basic chart data structure , and to show how edges can represent hypotheses about syntactic constituents .", "It can then be used to demonstrate and explain the individual rules that the chart parser uses to create new edges .", "Finally , it can be used to show how these individual rules combine to find a complete parse for a given sentence .", "To reduce the overhead of setting up demonstrations during lecture , the user can define a list of preset charts .", "The tool can then be reset to any one of these charts at any time .", "The chart parsing tool allows for flexible control of the parsing algorithm .", "At each step of the algorithm , the user can select which rule or strategy they wish to apply .", "This allows the user to experiment with mixing different strategies e . g . , top down and bottom up .", "The user can exercise fine grained control over the algorithm by selecting which edge they wish to apply a rule to .", "This flexibility allows lecturers to use the tool to respond to a wide variety of questions ; and allows students to experiment with different variations on the chart parsing algorithm .", "NLTK provides students with a flexible framework for advanced projects .", "Typical projects involve the development of entirely new functionality for a previously unsupported NLP task , or the development of a complete system out of existing and new modules .", "The toolkit s broad coverage allows students to explore a wide variety of topics .", "In our introductory computational linguistics course , topics for student projects included text generation , word sense disambiguation , collocation analysis , and morphological analysis .", "NLTK eliminates the tedious infrastructurebuilding that is typically associated with advanced student projects by providing students with the basic data structures , tools , and interfaces that they need .", "This allows the students to concentrate on the problems that interest them .", "The collaborative , open source nature of the toolkit can provide students with a sense that their projects are meaningful contributions , and not just exercises .", "Several of the students in our course have expressed interest in incorporating their projects into the toolkit .", "Finally , many of the modules included in the toolkit provide students with good examples of what projects should look like , with well thought out interfaces , clean code structure , and thorough documentation .", "The probabilistic parsing module was created as a class project for a statistical NLP course .", "The toolkit provided the basic data types and interfaces for parsing .", "The project extended these , adding a new probabilistic parsing interface , and using subclasses to create a probabilistic version of the context free grammar data structure .", "These new components were used in conjunction with several existing components , such as the chart data structure , to define two implementations of the probabilistic parsing interface .", "Finally , a tutorial was written that explained the basic motivations and concepts behind probabilistic parsing , and described the new interfaces , data structures , and parsers .", "We used NLTK as a basis for the assignments and student projects in CIS 530 , an introductory computational linguistics class taught at the University of Pennsylvania .", "CIS 530 is a graduate level class , although some advanced undergraduates were also enrolled .", "Most students had a background in either computer science or linguistics and occasionally both .", "Students were required to complete five assignments , two exams , and a final project .", "All class materials are available from the course website http www . cis . upenn . edu cis530 .", "The experience of using NLTK was very positive , both for us and for the students .", "The students liked the fact that they could do interesting projects from the outset .", "They also liked being able to run everything on their computer at home .", "The students found the extensive documentation very helpful for learning to use the toolkit .", "They found the interfaces defined by NLTK intuitive , and appreciated the ease with which they could combine different components to create complete NLP systems .", "We did encounter a few difficulties during the semester .", "One problem was finding large clean corpora that the students could use for their assignments .", "Several of the students needed assistance finding suitable corpora for their final projects .", "Another issue was the fact that we were actively developing NLTK during the semester ; some modules were only completed one or two weeks before the students used them .", "As a result , students who worked at home needed to download new versions of the toolkit several times throughout the semester .", "Luckily , Python has extensive support for installation scripts , which made these upgrades simple .", "The students encountered a couple of bugs in the toolkit , but none were serious , and all were quickly corrected .", "The computational component of computational linguistics courses takes many forms .", "In this section we briefly review a selection of approaches , classified according to the original target audience .", "Linguistics Students .", "Various books introduce programming or computing to linguists .", "These are elementary on the computational side , providing a gentle introduction to students having no prior experience in computer science .", "Examples of such books are Using Computers in Linguistics Lawler and Dry , 1998 , and Programming for Linguistics Java Technology for Language Researchers Hammond , 2002 .", "Grammar Developers .", "Infrastructure for grammar development has a long history in unification based or constraint based grammar frameworks , from DCG Pereira and Warren , 1980 to HPSG Pollard and Sag , 1994 .", "Recent work includes Copestake , 2000 ; Baldridge et al . , 2002a .", "A concurrent development has been the finite state toolkits , such as the Xerox toolkit Beesley and Karttunen , 2002 .", "This work has found widespread pedagogical application .", "Other Researchers and Developers .", "A variety of toolkits have been created for research or R D purposes .", "Examples include the CMU Cambridge Statistical Language Modeling Toolkit Clarkson and Rosenfeld , 1997 , the EMU Speech Database System Harrington and Cassidy , 1999 , the General Architecture for Text Engineering Bontcheva et al . , 2002 , the Maxent Package for Maximum Entropy Models Baldridge et al . , 2002b , and the Annotation Graph Toolkit Maeda et al . , 2002 .", "Although not originally motivated by pedagogical needs , all of these toolkits have pedagogical applications and many have already been used in teaching .", "NLTK provides a simple , extensible , uniform framework for assignments , projects , and class demonstrations .", "It is well documented , easy to learn , and simple to use .", "We hope that NLTK will allow computational linguistics classes to include more hands on experience with using and building NLP components and systems .", "NLTK is unique in its combination of three factors .", "First , it was deliberately designed as courseware and gives pedagogical goals primary status .", "Second , its target audience consists of both linguists and computer scientists , and it is accessible and challenging at many levels of prior computational skill .", "Finally , it is based on an object oriented scripting language supporting rapid prototyping and literate programming .", "We plan to continue extending the breadth of materials covered by the toolkit .", "We are currently working on NLTK modules for Hidden Markov Models , language modeling , and tree adjoining grammars .", "We also plan to increase the number of algorithms implemented by some existing modules , such as the text classification module .", "Finding suitable corpora is a prerequisite for many student assignments and projects .", "We are therefore putting together a collection of corpora containing data appropriate for every module defined by the toolkit .", "NLTK is an open source project , and we welcome any contributions .", "Readers who are interested in contributing to NLTK , or who have suggestions for improvements , are encouraged to contact the authors .", "We are indebted to our students for feedback on the toolkit , and to anonymous reviewers , Jee Bang , and the workshop organizers for comments on an earlier version of this paper .", "We are grateful to Mitch Marcus and the Department of Computer and Information Science at the University of Pennsylvania for sponsoring the work reported here ."], "summary_lines": ["NLTK: The Natural Language Toolkit\n", "NLTK, the Natural Language Toolkit, is a suite of open source program modules, tutorials and problem sets, providing ready-to-use computational linguistics courseware.\n", "NLTK covers symbolic and statistical natural language processing, and is interfaced to annotated corpora.\n", "Students augment and replace existing components, learn structured programming by example, and manipulate sophisticated models from the outset.\n", "NLTK, the Natural Language Toolkit, is a suite of Python modules providing many NLP data types, processing tasks, corpus samples and readers, together with animated algorithms, tutorials and problem sets.\n"]}
{"article_lines": ["Automatically Labeling Semantic Classes", "Systems that automatically discover semantic classes have emerged in part to address the limitations of broad coverage lexical resources such as WordNet and Cyc .", "The current state of the art discovers many semantic classes but fails to label their concepts .", "We propose an algorithm labeling semantic and for leveraging them to extract relationships using a top down approach .", "The natural language literature is rich in theories of semantics Barwise and Perry 1985 ; Schank and Abelson 1977 .", "However , WordNet Miller 1990 and Cyc Lenat 1995 aside , the community has had little success in actually building large semantic repositories .", "Such broad coverage lexical resources are extremely useful in applications such as word sense disambiguation Leacock , Chodorow and Miller 1998 and question answering Pasca and Harabagiu 2001 .", "Current manually constructed ontologies such as WordNet and Cyc have important limitations .", "First , they often contain rare senses .", "For example , WordNet includes a rare sense of computer that means the person who computes' .", "Using WordNet to expand queries to an information retrieval system , the expansion of computer will include words like estimator and reckoner .", "Also , the words dog , computer and company all have a sense that is a hyponym of person .", "Such rare senses make it difficult for a coreference resolution system to use WordNet to enforce the constraint that personal pronouns e . g . he or she must refer to a person .", "The second problem with these lexicons is that they miss many domain specific senses .", "For example , WordNet misses the user interface object sense of the word dialog as often used in software manuals .", "WordNet also contains a very poor coverage of proper nouns .", "There is a need for semi automatic approaches to building and extending ontologies as well as for validating the structure and content of existing ones .", "With the advent of the Web , we have access to enormous amounts of text .", "The future of ontology growing lies in leveraging this data by harvesting it for concepts and semantic relationships .", "Moreover , once such knowledge is discovered , mechanisms must be in place to enrich current ontologies with this new knowledge .", "To address some of the coverage and specificity problems in WordNet and Cyc , Pantel and Lin 2002 proposed and algorithm , called CBC , for automatically extracting semantic classes .", "Their classes consist of clustered instances like the three shown below A limitation of these concepts is that CBC does not discover their actual names .", "That is , CBC discovers a semantic class of Canadian provinces such as Manitoba , Alberta , and Ontario , but stops short of labeling the concept as Canadian Provinces .", "Some applications such as question answering would benefit from class labels .", "For example , given the concept list B and a label goalie goaltender , a QA system could look for answers to the question quot ; Which goaltender won the most Hart Trophys ? quot ; in the concept .", "In this paper , we propose an algorithm for automatically inducing names for semantic classes and for finding instance concept is a relationships .", "Using concept signatures templates describing the prototypical syntactic behavior of instances of a concept , we extract concept names by searching for simple syntactic patterns such as quot ; concept apposition of instance quot ; .", "Searching concept signatures is more robust than searching the syntactic features of individual instances since many instances suffer from sparse features or multiple senses .", "Once labels are assigned to concepts , we can extract a hyponym relationship between each instance of a concept and its label .", "For example , once our system labels list C as color , we may extract relationships such as pink is a color , red is a color , turquoise is a color , etc .", "Our results show that of the 159 , 000 hyponyms we extract using this simple method , 68 are correct .", "Of the 65 , 000 proper name hyponyms we discover , 81 . 5 are correct .", "The remainder of this paper is organized as follows .", "In the next section , we review previous algorithms for extracting semantic classes and hyponym relationships .", "Section 3 describes our algorithm for labeling concepts and for extracting hyponym relationships .", "Experimental results are presented in Section 4 and finally , we conclude with a discussion and future work .", "There have been several approaches to automatically discovering lexico semantic information from text Hearst 1992 ; Riloff and Shepherd 1997 ; Riloff and Jones 1999 ; Berland and Charniak 1999 ; Pantel and Lin 2002 ; Fleischman et al . 2003 ; Girju et al .", "One approach constructs automatic thesauri by computing the similarity between words based on their distribution in a corpus Hindle 1990 ; Lin 1998 .", "The output of these programs is a ranked list of similar words to each word .", "For example , Lin's approach outputs the following top 20 similar words of orange D peach , grapefruit , yellow , lemon , pink , avocado , tangerine , banana , purple , Santa Ana , strawberry , tomato , red , pineapple , pear , Apricot , apple , green , citrus , mango A common problem of such lists is that they do not discriminate between the senses of polysemous words .", "For example , in D , the color and fruit senses of orange are mixed up .", "Lin and Pantel 2001 proposed a clustering algorithm , UNICON , which generates similar lists but discriminates between senses of words .", "Later , Pantel and Lin 2002 improved the precision and recall of UNICON clusters with CBC Clustering by Committee .", "Using sets of representative elements called committees , CBC discovers cluster centroids that unambiguously describe the members of a possible class .", "The algorithm initially discovers committees that are well scattered in the similarity space .", "It then proceeds by assigning elements to their most similar committees .", "After assigning an element to a cluster , CBC removes their overlapping features from the element before assigning it to another cluster .", "This allows CBC to discover the less frequent senses of a word and to avoid discovering duplicate senses .", "CBC discovered both the color sense of orange , as shown in list C of Section 1 , and the fruit sense shown below E peach , pear , apricot , strawberry , banana , mango , melon , apple , pineapple , cherry , plum , lemon , grapefruit , orange , berry , raspberry , blueberry , kiwi , . . .", "There have also been several approaches to discovering hyponym is a relationships from text .", "Hearst 1992 used seven lexico syntactic patterns , for example quot ; such NP as NP , or and NP quot ; and quot ; NP , NP , or other NP quot ; .", "Berland and Charniak 1999 used similar pattern based techniques and other heuristics to extract meronymy part whole relations .", "They reported an accuracy of about 55 precision on a corpus of 100 , 000 words .", "Girju , Badulescu and Moldovan 2003 improved upon this work by using a machine learning filter .", "Mann 2002 and Fleischman et al . 2003 used part of speech patterns to extract a subset of hyponym relations involing proper nouns .", "The research discussed above on discovering hyponym relationships all take a bottom up approach .", "That is , they use patterns to independently discover semantic relationships of words .", "However , for infrequent words , these patterns do not match or , worse yet , generate incorrect relationships .", "Ours is a top down approach .", "We make use of cooccurrence statistics of semantic classes discovered by algorithms like CBC to label their concepts .", "Hyponym relationships may then be extracted easily one hyponym per instance concept label pair .", "For example , if we labeled concept A from Section 1 with disease , then we could extract is a relationships such as diabetes is a disease , cancer is a disease , and lupus is a disease .", "A concept instance such as lupus is assigned a hypernym disease not because it necessarily occurs in any particular syntactic relationship with disease , but because it belongs to the class of instances that does .", "The input to our labeling algorithm is a list of semantic classes , in the form of clusters of words , which may be generated from any source .", "In our experiments , we used the clustering outputs of CBC Pantel and Lin 2002 .", "The output of the system is a ranked list of concept names for each semantic class .", "In the first phase of the algorithm , we extract feature vectors for each word that occurs in a semantic class .", "Phase II then uses these features to compute grammatical signatures of concepts using the CBC algorithm .", "Finally , we use simple syntactic patterns to discover class names from each class' signature .", "Below , we describe these phases in detail .", "We represent each word concept instance by a feature vector .", "Each feature corresponds to a context in which the word occurs .", "For example , quot ; catch _ quot ; is a verbobject context .", "If the word wave occurred in this context , then the context is a feature of wave .", "We first construct a frequency count vector C e ce1 , ce2 , , cem , where m is the total number of features and cef is the frequency count of feature f occurring in word e . Here , cef is the number of times word e occurred in a grammatical context f . For example , if the word wave occurred 217 times as the object of the verb catch , then the feature vector for wave will have value 217 for its quot ; object of catch quot ; feature .", "In Section 4 . 1 , we describe how we obtain these features .", "We then construct a mutual information vector MI e mie1 , mie2 , . . . , miem for each word e , where mief is the pointwise mutual information between word e and feature f , which is defined as Following Pantel and Lin 2002 , we construct a committee for each semantic class .", "A committee is a set of representative elements that unambiguously describe the members of a possible class .", "For each class c , we construct a matrix containing the similarity between each pair of words ei and ej in c using the cosine coefficient of their mutual information vectors Salton and McGill 1983 For each word e , we then cluster its most similar instances using group average clustering Han and Kamber 2001 and we store as a candidate committee the highest scoring cluster c' according to the following metric where c' is the number of elements in c' and avgsim c' is the average pairwise similarity between words in c' .", "The assumption is that the best representative for a concept is a large set of very similar instances .", "The committee for class c is then the highest scoring candidate committee containing only words from c . For example , below are the committee members discovered for the semantic classes A , B , and C from Section 1 equency count of all features of all words .", "Mutual information is commonly used to measure the association strength between two words Church and Hanks 1989 .", "A well known problem is that mutual information is biased towards infrequent elements features .", "We therefore multiply mief with the following discounting factor n m where n is the number of words and N cef By averaging the feature vectors of the committee members of a particular semantic class , we obtain a grammatical template , or signature , for that class .", "For example , Figure 1 shows an excerpt of the grammatical signature for concept B in Section 1 .", "The vector is obtained by averaging the feature vectors for the words Curtis Joseph , John Vanbiesbrouck , Mike Richter , and Tommy Salo the committee of this concept .", "The quot ; V subj N sprawl feature indicates a subject verb relationship between the concept and the verb sprawl while quot ; N appo N goaltender quot ; indicates an apposition relationship between the concept and the noun goaltender .", "The in a relationship means that the right hand side of the relationship is the head e . g . sprawl is the head of the subject verb relationship .", "The two columns of numbers indicate the frequency and mutual information score for each feature respectively .", "In order to discover the characteristics of human naming conventions , we manually named 50 concepts discovered by CBC .", "For each concept , we extracted the relationships between the concept committee and the assigned label .", "We then added the mutual information scores for each extracted relationship among the 50 concepts .", "The top 4 highest scoring relationships are To name a class , we simply search for these syntactic relationships in the signature of a concept .", "We sum up the mutual information scores for each term that occurs in these relationships with a committee of a class .", "The highest scoring term is the name of the class .", "For example , the top 5 scoring terms that occurred in these relationships with the signature of the concept represented by the committee Curtis Joseph , John Vanbiesbrouck , Mike Richter , Tommy Salo are goalie 40 . 37 goaltender 33 . 64 goalkeeper 19 . 22 player 14 . 55 backup 9 . 40 The numbers are the total mutual information scores of each name in the four syntactic relationships .", "In this section , we present an evaluation of the class labeling algorithm and of the hyponym relationships discovered by our system .", "We used Minipar Lin 1994 , a broad coverage parser , to parse 3GB of newspaper text from the Aquaint TREC 9 collection .", "We collected the frequency counts of the grammatical relationships contexts output by Minipar and used them to compute the pointwise mutual information vectors described in Section 3 . 1 .", "We used the 1432 noun clusters extracted by CBC1 as the list of concepts to name .", "For each concept , we then used our algorithm described in Section 3 to extract the top 20 names for each concept .", "Out of the 1432 noun concepts , we were unable to name 21 1 . 5 of them .", "This occurs when a concept's committee members do not occur in any of the four syntactic relationships described in Section 0 .", "We performed a manual evaluation of the remaining 1411 concepts .", "We randomly selected 125 concepts and their top 5 highest ranking names according to our algorithm .", "Table 1 shows the first 10 randomly selected concepts each concept is represented by three of its committee members .", "For each concept , we added to the list of names a human generated name obtained from an annotator looking at only the concept instances .", "We also appended concept names extracted from WordNet .", "For each concept that contains at least five instances in the WordNet hierarchy , we named the concept with the most frequent common ancestor of each pair of instances .", "Up to five names were generated by WordNet for each concept .", "Because of the low coverage of proper nouns in WordNet , only 33 of the 125 concepts we evaluated had WordNet generated labels .", "We presented to three human judges the 125 randomly selected concepts together with the system , human , and WordNet generated names randomly ordered .", "That way , there was no way for a judge to know the source of a label nor the system's ranking of the labels .", "For each name , we asked the judges to assign a score of correct , partially correct , or incorrect .", "We then computed the mean reciprocal rank MRR of the system , human , and WordNet labels .", "For each concept , a naming scheme receives a score of 1 M where M is the rank of the first name judged correct .", "Table 2 shows the results .", "Table 3 shows similar results for a more lenient evaluation where M is the rank of the first name judged correct or partially correct .", "Our system achieved an overall MRR score of 77 . 1 .", "We performed much better than the baseline WordNet 19 . 9 because of the lack of coverage mostly proper nouns in the hierarchy .", "For the 33 concepts that WordNet named , it achieved a score of 75 . 3 and a lenient score of 82 . 7 , which is high considering the simple algorithm we used to extract labels using WordNet .", "The Kappa statistic Siegel and Castellan Jr . 1988 measures the agreements between a set of judges' assessments correcting for chance agreements where P A is the probability of agreement between the judges and P E is the probability that the judges agree by chance on an assessment .", "An experiment with K 0 . 8 is generally viewed as reliable and 0 . 67 K 0 . 8 allows tentative conclusions .", "The Kappa statistic for our experiment is K 0 . 72 .", "The human labeling is at a disadvantage since only one label was generated per concept .", "Therefore , the human scores either 1 or 0 for each concept .", "Our system's highest ranking name was correct 72 of the time .", "Table 4 shows the percentage of semantic classes with a correct label in the top 1 5 ranks returned by our system .", "Overall , 41 . 8 of the top 5 names extracted by our system were judged correct .", "The overall accuracy for the top 4 , top 3 , top 2 , and top 1 names are 44 . 4 , 48 . 8 , 58 . 5 , and 72 respectively .", "Hence , the name ranking of our algorithm is effective .", "The 1432 CBC concepts contain 18 , 000 unique words .", "For each concept to which a word belongs , we extracted up to 3 hyponyms , one for each of the top 3 labels for the concept .", "The result was 159 , 000 hyponym relationships .", "24 are shown in the Appendix .", "Two judges annotated two random samples of 100 relationships one from all 159 , 000 hyponyms and one from the subset of 65 , 000 proper nouns .", "For each instance , the judges were asked to decide whether the hyponym relationship was correct , partially correct or incorrect .", "Table 5 shows the results .", "The strict measure counts a score of 1 for each correctly judged instance and 0 otherwise .", "The lenient measure also gives a score of 0 . 5 for each instance judged partially correct .", "Many of the CBC concepts contain noise .", "For example , the wine cluster Zinfandel , merlot , Pinot noir , Chardonnay , Cabernet Sauvignon , cabernet , riesling , Sauvignon blanc , Chenin blanc , sangiovese , syrah , Grape , Chianti . . . contains some incorrect instances such as grape , appelation , and milk chocolate .", "Each of these instances will generate incorrect hyponyms such as grape is wine and milk chocolate is wine .", "This hyponym extraction task would likely serve well for evaluating the accuracy of lists of semantic classes .", "Table 5 shows that the hyponyms involving proper nouns are much more reliable than common nouns .", "Since WordNet contains poor coverage of proper nouns , these relationships could be useful to enrich it .", "Semantic extraction tasks are notoriously difficult to evaluate for recall .", "To approximate recall , we conducted two question answering QA tasks answering definition questions and performing QA information retrieval .", "We chose the 50 definition questions that appeared in the QA track of TREC2003 Voorhees , 2003 .", "For example quot ; Who is Aaron Copland ? quot ; and quot ; What is the Kama Sutra ? quot ; For each question we looked for at most five corresponding concepts in our hyponym list .", "For example , for Aaron Copland , we found the following hypernyms composer , music , and gift .", "We compared our system with the concepts in WordNet and Fleischman et al . 's instance concept relations Fleischman et al .", "Table 6 shows the percentage of correct answers in the top 1 and top 5 returned answers from each system .", "All systems seem to have similar performance on the top 1 answers , but our system has many more answers in the top 5 .", "This shows that our system has comparatively higher recall for this task .", "Passage retrieval is used in QA to supply relevant information to an answer pinpointing module .", "The higher the performance of the passage retrieval module , the higher will be the performance of the answer pinpointing module .", "The passage retrieval module can make use of the hyponym relationships that are discovered by our system .", "Given a question such as quot ; What color . . . quot ; , the likelihood of a correct answer being present in a retrieved passage is greatly increased if we know the set of all possible colors and index them in the document collection appropriately .", "We used the hyponym relations learned by our system to perform semantic indexing on a QA passage retrieval task .", "We selected the 179 questions from the QA track of TREC 2003 that had an explicit semantic answer type e . g .", "quot ; What band was Jerry Garcia with ? quot ; and quot ; What color is the top stripe on the U . S . flag ? quot ; .", "For each expected semantic answer type corresponding to a given question e . g . band and color , we indexed the entire TREC 2002 IR collection with our system's hyponyms .", "We compared the passages returned by the passage retrieval module with and without the semantic indexing .", "We counted how many of the 179 questions had a correct answer returned in the top 1 and top 100 passages .", "Table 7 shows the results .", "Our system shows small gains in the performance of the IR output .", "In the top 1 category , the performance improved by 20 .", "This may lead to better answer selections .", "Current state of the art concept discovery algorithms generate lists of instances of semantic classes but stop short of labeling the classes with concept names .", "Class labels would serve useful in applications such as question answering to map a question concept into a semantic class and then search for answers within that class .", "We propose here an algorithm for automatically labeling concepts that searches for syntactic patterns within a grammatical template for a class .", "Of the 1432 noun concepts discovered by CBC , our system labelled 98 . 5 of them with an NRR score of 77 . 1 in a human evaluation .", "Hyponym relationships were then easily extracted , one for each instance concept label pair .", "We extracted 159 , 000 hyponyms and achieved a precision of 68 .", "On a subset of 65 , 000 proper names , our performance was 81 . 5 .", "This work forms an important attempt to building large scale semantic knowledge bases .", "Without being able to automatically name a cluster and extract hyponym hypernym relationships , the utility of automatically generated clusters or manually compiled lists of terms is limited .", "Of course , it is a serious open question how many names each cluster concept should have , and how good each name is .", "Our method begins to address this thorny issue by quantifying the name assigned to a class and by simultaneously assigning a number that can be interpreted to reflect the strength of membership of each element to the class .", "This is potentially a significant step away from traditional all or nothing semantic ontology representations to a concept representation scheme that is more nuanced and admits multiple names and graded set memberships .", "The authors wish to thank the reviewers for their helpful comments .", "This research was partly supported by NSF grant EIA 0205111 ."], "summary_lines": ["Automatically Labeling Semantic Classes\n", "Systems that automatically discover semantic classes have emerged in part to address the limitations of broad-coverage lexical resources such as WordNet and Cyc.\n", "The current state of the art discovers many semantic classes but fails to label their concepts.\n", "We propose an algorithm labeling semantic classes and for leveraging them to extract is-a relationships using a top-down approach.\n", "The relationships automatically learned in our system include appositions, nominal subjects, such as relationships, and like relationships.\n", "Our syntactical co-occurrence approach has worst-case time complexity O (n2k), where n is the number of words in the corpus and k is the feature space.\n", "Given a collection of news articles that is both cleaner and smaller than Web document collections, a syntactic parser is applied to document sentences in order to identify and exploit syntactic dependencies for the purpose of selecting candidate class labels.\n"]}
{"article_lines": ["Advances In Domain Independent Linear Text Segmentation", "This paper describes a method for linear text segmentation which is twice as accurate and over seven times as fast as the state of the art Reynar , 1998 .", "Inter sentence similarity is replaced by rank in the local context .", "Boundary locations are discovered by divisive clustering .", "Even moderately long documents typically address several topics or different aspects of the same topic .", "The aim of linear text segmentation is to discover the topic boundaries .", "The uses of this procedure include information retrieval Hearst and Plaunt , 1993 ; Hearst , 1994 ; Yaari , 1997 ; Reynar , 1999 , summarization Reynar , 1998 , text understanding , anaphora resolution Kozima , 1993 , language modelling Morris and Hirst , 1991 ; Beeferman et al . , 1997b and improving document navigation for the visually disabled Choi , 2000 .", "This paper focuses on domain independent methods for segmenting written text .", "We present a new algorithm that builds on previous work by Reynar Reynar , 1998 ; Reynar , 1994 .", "The primary distinction of our method is the use of a ranking scheme and the cosine similarity measure van Rijsbergen , 1979 in formulating the similarity matrix .", "We propose that the similarity values of short text segments is statistically insignificant .", "Thus , one can only rely on their order , or rank , for clustering .", "Existing work falls into one of two categories , lexical cohesion methods and multi source methods Yaari , 1997 .", "The former stem from the work of Halliday and Hasan Halliday and Hasan , 1976 .", "They proposed that text segments with similar vocabulary are likely to be part of a coherent topic segment .", "Implementations of this idea use word stem repetition Youmans , 1991 ; Reynar , 1994 ; Ponte and Croft , 1997 , context vectors Hearst , 1994 ; Yaari , 1997 ; Kaufmann , 1999 ; Eichmann et al . , 1999 , entity repetition Kan et al . , 1998 , semantic similarity Morris and Hirst , 1991 ; Kozima , 1993 , word distance model Beeferman et al . , 1997a and word frequency model Reynar , 1999 to detect cohesion .", "Methods for finding the topic boundaries include sliding window Hearst , 1994 , lexical chains Morris , 1988 ; Kan et al . , 1998 , dynamic programming Ponte and Croft , 1997 ; Heinonen , 1998 , agglomerative clustering Yaari , 1997 and divisive clustering Reynar , 1994 .", "Lexical cohesion methods are typically used for segmenting written text in a collection to improve information retrieval Hearst , 1994 ; Reynar , 1998 .", "Multi source methods combine lexical cohesion with other indicators of topic shift such as cue phrases , prosodic features , reference , syntax and lexical attraction Beeferman et al . , 1997a using decision trees Miike et al . , 1994 ; Kurohashi and Nagao , 1994 ; Litman and Passonneau , 1995 and probabilistic models Beeferman et al . , 1997b ; Hajime et al . , 1998 ; Reynar , 1998 .", "Work in this area is largely motivated by the topic detection and tracking TDT initiative Allan et al . , 1998 .", "The focus is on the segmentation of transcribed spoken text and broadcast news stories where the presentation format and regular cues can be exploited to improve accuracy .", "Our segmentation algorithm takes a list of tokenized sentences as input .", "A tokenizer Grefenstette and Tapanainen , 1994 and a sentence boundary disambiguation algorithm Palmer and Hearst , 1994 ; Reynar and Ratnaparkhi , 1997 or EAGLE Reynar et al . , 1997 may be used to convert a plain text document into the acceptable input format .", "Punctuation and uninformative words are removed from each sentence using a simple regular expression pattern matcher and a stopword list .", "A stemming algorithm Porter , 1980 is then applied to the remaining tokens to obtain the word stems .", "A dictionary of word stem frequencies is constructed for each sentence .", "This is represented as a vector of frequency counts .", "Let fi , i denote the frequency of word j in sentence i .", "The similarity between a pair of sentences 1 , y For short text segments , the absolute value of sim x , y is unreliable .", "An additional occurrence of a common word reflected in the numerator causes a disproportionate increase in sim x , y unless the denominator related to segment length is large .", "Thus , in the context of text segmentation where a segment has typically 100 informative tokens , one can only use the metric to estimate the order of similarity between sentences , e . g . a is more similar to b than c . Furthermore , language usage varies throughout a document .", "For instance , the introduction section of a document is less cohesive than a section which is about a particular topic .", "Consequently , it is inappropriate to directly compare the similarity values from different regions of the similarity matrix .", "In non parametric statistical analysis , one compares the rank of data sets when the qualitative behaviour is similar but the absolute quantities are unreliable .", "We present a ranking scheme which is an adaptation of that described in O'Neil and Denos , 1992 .", "'The contrast of the image has been adjusted to highlight the image features .", "Each value in the similarity matrix is replaced by its rank in the local region .", "The rank is the number of neighbouring elements with a lower similarity value .", "Figure 2 shows an example of image ranking using a 3 x 3 rank mask with output range 0 , 8 .", "For segmentation , we used a 11 x 11 rank mask .", "The output is expressed as a ratio r equation 2 to circumvent normalisation problems consider the cases when the rank mask is not contained in the image .", "of elements with a lower value To demonstrate the effect of image ranking , the process was applied to the matrix shown in figure 1 to produce figure 32 .", "Notice the contrast has been improved significantly .", "Figure 4 illustrates the more subtle effects of our ranking scheme . r x is the rank 1 x 11 mask of x which is a sine wave with decaying mean , amplitude and frequency equation 3 .", "The final process determines the location of the topic boundaries .", "The method is based on Reynar's maximisation algorithm Reynar , 1998 ; Helfman , 1996 ; Church , 1993 ; Church and Helfman , 1993 .", "A text segment is defined by two sentences i , j inclusive .", "This is represented as a square region along the diagonal of the rank matrix .", "Let si , j denote the sum of the rank values in a segment and ai , j j i 1 2 be the inside area .", "B b1 , . . . , 197 4 is a list of in coherent text segments . sk and ak refers to the sum of rank and area of segment k in B .", "D is the inside density of B see equation 4 . ak To initialise the process , the entire document is placed in B as one coherent text segment .", "Each step of the process splits one of the segments in B .", "The split point is a potential boundary which maximises D . Figure 5 shows a working example .", "The number of segments to generate , in , is determined automatically .", "Den is the inside density of n segments and SD n , Den Den 1 is the gradient .", "For a document with b potential boundaries , b steps of divisive clustering generates D 1 , . . . , D b 1 and bD 2 , oD b 1 see figure 6 and 7 .", "An unusually large reduction in 6D suggests the optiinal clustering has been obtained3 see n 10 in the threshold , p c x to dD c 1 . 2 works well in practice The running time of each step is dominated by the computation of sk .", "Given si , j is constant , our algorithm pre computes all the values to improve speed performance .", "The procedure computes the values along diagonals , starting from the main diagonal and works towards the corner .", "The method has a complexity of order 171 5 . n2 .", "Let ri , j refer to the rank value in the rank matrix R and S to the sum of rank matrix .", "Given R of size n X 77 , , S is computed in three steps see equation 5 .", "Figure 8 shows the result of applying this procedure to the rank matrix in figure 5 .", "The definition of a topic segment ranges from complete stories Allan et al . , 1998 to summaries Ponte and Croft , 1997 .", "Given the quality of an algorithm is task dependent , the following experiments focus on the relative performance .", "Our evaluation strategy is a variant of that described in Reynar , 1998 , 71 73 and the TDT segmentation task Allan et al . , 1998 .", "We assume a good algorithm is one that finds the most prominent topic boundaries .", "An artificial test corpus of 700 samples is used to assess the accuracy and speed performance of segmentation algorithms .", "A sample is a concatenation of ten text segments .", "A segment is the first n sentences of a randomly selected document from the Brown corpus' .", "A sample is characterised by the range of n . The corpus was generated by an automatic procedure5 .", "Table 1 presents the corpus statistics . p erroriref , hyp , k p misslref , hyp , diff , k p diffl ref , k 6 p fairef , hyp , same , k p samelref , k Speed performance is measured by the average number of CPU seconds required to process a test sample6 .", "Segmentation accuracy is measured by the error metric equation 6 , fa false alarms proposed in Beeferman et al . , 1999 .", "Low error probability indicates high accuracy .", "Other performance measures include the popular precision and recall metric PR Hearst , 1994 , fuzzy PR Reynar , 1998 and edit distance Ponte and Croft , 1997 .", "The problems associated with these metrics are discussed in Beeferman et al . , 1999 .", "Five degenerate algorithms define the baseline for the experiments .", "B does not propose any boundaries .", "B reports all potential boundaries as real boundaries .", "B partitions the sample into regular segments .", "B i . , ? randomly selects any number of boundaries as real boundaries .", "B r , b randomly selects b boundaries as real boundaries .", "The accuracy of the last two algorithms are computed analytically .", "We consider the status of in potential boundaries as a bit string 1 4 topic boundary .", "The terms p iniss awl p fa in equation 6 corresponds to p samelk and p difflk 1 p samelk .", "Equation 7 , 8 and 9 gives the general form of p samelk , B r , ? and Berm , respectively' .", "Table 2 presents the experimental results .", "The values in row two and three , four and five are not actually the same .", "However , their differences are insignificant according to the Kolmogorov Smirnov , or KS test Press et al . , 1992 .", "We compare three versions of the TextTiling algorithm Hearst , 1994 .", "H94 c , d is Hearst's C implementation with default parameters .", "H94 e . 7 . uses the recommended parameters k 6 , w 20 .", "H94 3 , , , is my implementation of the algorithm .", "Experimental result table 3 shows H94 , , d and H94 , , are more accurate than H94 j , , , .", "We suspect this is due to the use of a different stopword list and stemming algorithm .", "Five versions of Reynar's optimisation algorithm Reynar , 1998 were evaluated .", "R98 and R98 7 rn are exact implementations of his maximisation and minimisation algorithm .", "R98 8 , , 08 is my version of the maximisation algorithm which uses the cosine coefficient instead of dot density for measuring similarity .", "It incorporates the optimisations described in section 3 . 4 .", "R98 , , d0t is the modularised version of R98 for experimenting with different similarity measures .", "R98 , , , , uses a variant of Kozima's semantic similarity measure Kozima , 1993 to compute block similarity .", "Word similarity is a function of word cooccurrence statistics in the given document .", "Words that belong to the same sentence are considered to be related .", "Given the co occurrence frequencies f wi , wi , the transition probability matrix t is computed by equation 10 .", "Equation 11 defines our spread activation scheme . s denotes the word similarity matrix , x is the number of activation steps and norm y converts a matrix y into a transition matrix . x 5 was used in the experiment .", "Experimental result table 4 shows the cosine coefficient and our spread activation method improved segmentation accuracy .", "The speed optimisations significantly reduced the execution time .", "We compare three versions of Segmenter Kan et at , 1998 .", "K98 p is the original Perl implementation of the algorithm version 1 . 6 .", "K98 i is my implementation of the algorithm .", "K98 j , , i is a version of K98 i which uses a document specific chain breaking strategy .", "The distribution of link distances are used to identify unusually long links .", "The threshold is a function p c x VT , of the mean p and variance We found c 1 works well in practice .", "Table 5 summarises the experimental results .", "K98 performed performed significantly better than K98 J , .", "This is due to the use of a different part of speech tagger and shallow parser .", "The difference in speed is largely due to the programming languages and term clustering strategies .", "Our chain breaking strategy improved accuracy compare K98 i with K98 j , a Two versions of our algorithm were developed , C99 and C99 b .", "The former is an exact implementation of the algorithm described in this paper .", "The latter is given the expected number of topic segments for fair comparison with R98 .", "Both algorithms used a 11 x 11 ranking mask .", "The first experiment focuses on the impact of our automatic termination strategy on C99 b table 6 .", "C99 b is marginally more accurate than C99 .", "This indicates our automatic termination strategy is effective but not optimal .", "The minor reduction in speed performance is acceptable .", "The second experiment investigates the effect of different ranking mask size on the performance of C99 table 7 .", "Execution time increases with mask size .", "A 1 x 1 ranking mask reduces all the elements in the rank matrix to zero .", "Interestingly , the increase in ranking mask size beyond 3 x 3 has insignificant effect on segmentation accuracy .", "This suggests the use of extrema for clustering has a greater impact on accuracy than linearising the similarity scores figure 4 .", "Experimental result table 8 shows our algorithm C99 is more accurate than existing algorithms .", "A two fold increase in accuracy and seven fold increase in speed was achieved compare C99 b with R98 .", "If one disregards segmentation accuracy , H94 has the best algorithmic performance linear .", "C99 , K98 and R98 are all polynomial time algorithms .", "The significance of our results has been confirmed by both t test and KS test .", "A segmentation algorithm has two key elements , a , clustering strategy and a similarity measure .", "Our results show divisive clustering R98 is more precise than sliding window H94 and lexical chains K98 for locating topic boundaries .", "Four similarity measures were examined .", "The cosine coefficient R98 s , 0 and dot density measure R98 m , doo yield similar results .", "Our spread activation based semantic measure R98 , sa improved accuracy .", "This confirms that although Kozima's approach Kozima , 1993 is computationally expensive , it does produce more precise segmentation .", "The most significant improvement was due to our ranking scheme which linearises the cosine coefficient , .", "Our experiments demonstrate that given insufficient data , the qualitative behaviour of the cosine measure is indeed more reliable than the actual values .", "Although our evaluation scheme is sufficient for this comparative study , further research requires a large scale , task independent benchmark .", "It would be interesting to compare C99 with the multi source method described in Beeferman et al . , 1999 using the TDT corpus .", "We would also like to develop a linear time and multi source version of the algorithm .", "This paper has benefitted from the comments of Mary McGee Wood and the anonymous reviewers .", "Thanks are due to my parents and department for making this work possible ; Jeffrey Reynar for discussions and guidance on the segmentation problem ; Hideki Kozima for help on the spread activation measure ; Min Yen Kan and Marti Hearst for their segmentation algorithms ; Daniel Oram for references to image processing techniques ; Magnus Rattray and Stephen Marsland for help on statistics and mathematics ."], "summary_lines": ["Advances In Domain Independent Linear Text Segmentation\n", "This paper describes a method for linear text segmentation which is twice as accurate and over seven times as fast as the state-of-the-art (Reynar, 1998).\n", "Inter-sentence similarity is replaced by rank in the local context.\n", "Boundary locations are discovered by divisive clustering.\n", "We design an artificial dataset, built by concatenating short pieces of texts that have been extracted from the Brown corpus.\n"]}
{"article_lines": ["Reading Level Assessment Using Support Vector Machines And Statistical Language Models", "Reading proficiency is a fundamental component of language competency .", "However , finding topical texts at an appropriate reading level for foreign and second language learners is a challenge for teachers .", "This task can be addressed with natural language processing technology to assess reading level .", "Existing measures of reading level are not well suited to this task , but previous work and our own pilot experiments have shown the benefit of using statistical language models .", "In this paper , we also use support vector machines to combine features from traditional reading level measures , statistical language models , and other language processing tools to produce a better method of assessing reading level .", "The U . S . educational system is faced with the challenging task of educating growing numbers of students for whom English is a second language U . S . Dept . of Education , 2003 .", "In the 2001 2002 school year , Washington state had 72 , 215 students 7 . 2 of all students in state programs for Limited English Proficient LEP students Bylsma et al . , 2003 .", "In the same year , one quarter of all public school students in California and one in seven students in Texas were classified as LEP U . S . Dept . of Education , 2004 .", "Reading is a critical part of language and educational development , but finding appropriate reading material for LEP students is often difficult .", "To meet the needs of their students , bilingual education instructors seek out high interest level texts at low reading levels , e . g . texts at a first or second grade reading level that support the fifth grade science curriculum .", "Teachers need to find material at a variety of levels , since students need different texts to read independently and with help from the teacher .", "Finding reading materials that fulfill these requirements is difficult and time consuming , and teachers are often forced to rewrite texts themselves to suit the varied needs of their students .", "Natural language processing NLP technology is an ideal resource for automating the task of selecting appropriate reading material for bilingual students .", "Information retrieval systems successfully find topical materials and even answer complex queries in text databases and on the World Wide Web .", "However , an effective automated way to assess the reading level of the retrieved text is still needed .", "In this work , we develop a method of reading level assessment that uses support vector machines SVMs to combine features from statistical language models LMs , parse trees , and other traditional features used in reading level assessment .", "The results presented here on reading level assessment are part of a larger project to develop teacher support tools for bilingual education instructors .", "The larger project will include a text simplification system , adapting paraphrasing and summarization techniques .", "Coupled with an information retrieval system , these tools will be used to select and simplify reading material in multiple languages for use by language learners .", "In addition to students in bilingual education , these tools will also be useful for those with reading related learning disabilities and adult literacy students .", "In both of these situations , as in the bilingual education case , the student s reading level does not match his her intellectual level and interests .", "The remainder of the paper is organized as follows .", "Section 2 describes related work on reading level assessment .", "Section 3 describes the corpora used in our work .", "In Section 4 we present our approach to the task , and Section 5 contains experimental results .", "Section 6 provides a summary and description of future work .", "This section highlights examples and features of some commonly used measures of reading level and discusses current research on the topic of reading level assessment using NLP techniques .", "Many traditional methods of reading level assessment focus on simple approximations of syntactic complexity such as sentence length .", "The widelyused Flesch Kincaid Grade Level index is based on the average number of syllables per word and the average sentence length in a passage of text Kincaid et al . , 1975 as cited in Collins Thompson and Callan , 2004 .", "Similarly , the Gunning Fog index is based on the average number of words per sentence and the percentage of words with three or more syllables Gunning , 1952 .", "These methods are quick and easy to calculate but have drawbacks sentence length is not an accurate measure of syntactic complexity , and syllable count does not necessarily indicate the difficulty of a word .", "Additionally , a student may be familiar with a few complex words e . g . dinosaur names but unable to understand complex syntactic constructions .", "Other measures of readability focus on semantics , which is usually approximated by word frequency with respect to a reference list or corpus .", "The Dale Chall formula uses a combination of average sentence length and percentage of words not on a list of 3000 easy words Chall and Dale , 1995 .", "The Lexile framework combines measures of semantics , represented by word frequency counts , and syntax , represented by sentence length Stenner , 1996 .", "These measures are inadequate for our task ; in many cases , teachers want materials with more difficult , topic specific words but simple structure .", "Measures of reading level based on word lists do not capture this information .", "In addition to the traditional reading level metrics , researchers at Carnegie Mellon University have applied probabilistic language modeling techniques to this task .", "Si and Callan 2001 conducted preliminary work to classify science web pages using unigram models .", "More recently , Collins Thompson and Callan manually collected a corpus of web pages ranked by grade level and observed that vocabulary words are not distributed evenly across grade levels .", "They developed a smoothed unigram classifier to better capture the variance in word usage across grade levels Collins Thompson and Callan , 2004 .", "On web text , their classifier outperformed several other measures of semantic difficulty the fraction of unknown words in the text , the number of distinct types per 100 token passage , the mean log frequency of the text relative to a large corpus , and the Flesch Kincaid measure .", "The traditional measures performed better on some commercial corpora , but these corpora were calibrated using similar measures , so this is not a fair comparison .", "More importantly , the smoothed unigram measure worked better on the web corpus , especially on short passages .", "The smoothed unigram classifier is also more generalizable , since it can be trained on any collection of data .", "Traditional measures such as Dale Chall and Lexile are based on static word lists .", "Although the smoothed unigram classifier outperforms other vocabulary based semantic measures , it does not capture syntactic information .", "We believe that higher order n gram models or class n gram models can achieve better performance by capturing both semantic and syntactic information .", "This is particularly important for the tasks we are interested in , when the vocabulary i . e . topic and grade level are not necessarily well matched .", "Our work is currently focused on a corpus obtained from Weekly Reader , an educational newspaper with versions targeted at different grade levels Weekly Reader , 2004 .", "These data include a variety of labeled non fiction topics , including science , history , and current events .", "Our corpus consists of articles from the second , third , fourth , and fifth grade editions of the newspaper .", "We design classifiers to distinguish each of these four categories .", "This corpus contains just under 2400 articles , distributed as shown in Table 1 .", "Additionally , we have two corpora consisting of articles for adults and corresponding simplified versions for children or other language learners .", "Barzilay and Elhadad 2003 have allowed us to use their corpus from Encyclopedia Britannica , which contains articles from the full version of the encyclopedia and corresponding articles from Britannica Elementary , a new version targeted at children .", "The Western Pacific Literacy Network s 2004 web site has an archive of CNN news stories and abridged versions which we have also received permission to use .", "Although these corpora do not provide an explicit grade level ranking for each article , broad categories are distinguished .", "We use these data as a supplement to the Weekly Reader corpus for learning models to distinguish broad reading level classes than can serve to provide features for more detailed classification .", "Table 2 shows the size of the supplemental corpora .", "Existing reading level measures are inadequate due to their reliance on vocabulary lists and or a superficial representation of syntax .", "Our approach uses ngram language models as a low cost automatic approximation of both syntactic and semantic analysis .", "Statistical language models LMs are used successfully in this way in other areas of NLP such as speech recognition and machine translation .", "We also use a standard statistical parser Charniak , 2000 to provide syntactic analysis .", "In practice , a teacher is likely to be looking for texts at a particular level rather than classifying a group of texts into a variety of categories .", "Thus we construct one classifier per category which decides whether a document belongs in that category or not , rather than constructing a classifier which ranks documents into different categories relative to each other .", "Statistical LMs predict the probability that a particular word sequence will occur .", "The most commonly used statistical language model is the n gram model , which assumes that the word sequence is an n 1 th order Markov process .", "For example , for the common trigram model where n 3 , the probability of sequence w is The parameters of the model are estimated using a maximum likelihood estimate based on the observed frequency in a training corpus and smoothed using modified Kneser Ney smoothing Chen and Goodman , 1999 .", "We used the SRI Language Modeling Toolkit Stolcke , 2002 for language model training .", "Our first set of classifiers consists of one n gram language model per class c in the set of possible classes C . For each text document t , we can calculate the likelihood ratio between the probability given by the model for class c and the probabilities given by the other models for the other classes where we assume uniform prior probabilities P c .", "The resulting value can be compared to an empirically chosen threshold to determine if the document is in class c or not .", "For each class c , a language model is estimated from a corpus of training texts .", "In addition to using the likelihood ratio for classification , we can use scores from language models as features in another classifier e . g . an SVM .", "For example , perplexity PP is an information theoretic measure often used to assess language models where H t c is the entropy relative to class c of a length m word sequence t w1 , . . . , wm , defined as Low perplexity indicates a better match between the test data and the model , corresponding to a higher probability P t c .", "Perplexity scores are used as features in the SVM model described in Section 4 . 3 .", "The likelihood ratio described above could also be used as a feature , but we achieved better results using perplexity .", "Feature selection is a common part of classifier design for many classification problems ; however , there are mixed results in the literature on feature selection for text classification tasks .", "In CollinsThompson and Callan s work 2004 on readability assessment , LM smoothing techniques are more effective than other forms of explicit feature selection .", "However , feature selection proves to be important in other text classification work , e . g .", "Lee and Myaeng s 2002 genre and subject detection work and Boulis and Ostendorf s 2005 work on feature selection for topic classification .", "For our LM classifiers , we followed Boulis and Ostendorf s 2005 approach for feature selection and ranked words by their ability to discriminate between classes .", "Given P c w , the probability of class c given word w , estimated empirically from the training set , we sorted words based on their information gain IG .", "Information gain measures the difference in entropy when w is and is not included as a feature .", "The most discriminative words are selected as features by plotting the sorted IG values and keeping only those words below the knee in the curve , as determined by manual inspection of the graph .", "In an early experiment , we replaced all remaining words with a single unknown tag .", "This did not result in an effective classifier , so in later experiments the remaining words were replaced with a small set of general tags .", "Motivated by our goal of representing syntax , we used part of speech POS tags as labeled by a maximum entropy tagger Ratnaparkhi , 1996 .", "These tags allow the model to represent patterns in the text at a higher level than that of individual words , using sequences of POS tags to capture rough syntactic information .", "The resulting vocabulary consisted of 276 words and 56 POS tags .", "Support vector machines SVMs are a machine learning technique used in a variety of text classification problems .", "SVMs are based on the principle of structural risk minimization .", "Viewing the data as points in a high dimensional feature space , the goal is to fit a hyperplane between the positive and negative examples so as to maximize the distance between the data points and the plane .", "SVMs were introduced by Vapnik 1995 and were popularized in the area of text classification by Joachims 1998a .", "The unit of classification in this work is a single article .", "Our SVM classifiers for reading level use the following features The OOV scores are relative to the most common 100 , 200 and 500 words in the lowest grade level grade 2 2 .", "For each article , we calculated the percentage of a all word instances tokens and b all unique words types not on these lists , resulting in three token OOV rate features and three type OOV rate features per article .", "The parse features are generated using the Charniak parser Charniak , 2000 trained on the standard Wall Street Journal Treebank corpus .", "We chose to use this standard data set as we do not have any domain specific treebank data for training a parser .", "Although clearly there is a difference between news text for adults and news articles intended for children , inspection of some of the resulting parses showed good accuracy .", "Ideally , the language model scores would be for LMs from domain specific training data i . e . more Weekly Reader data .", "However , our corpus is limited and preliminary experiments in which the training data was split for LM and SVM training were unsuccessful due to the small size of the resulting data sets .", "Thus we made use of the Britannica and CNN articles to train models of three n gram orders on child text and adult text .", "This resulted in 12 LM perplexity features per article based on trigram , bigram and unigram LMs trained on Britannica adult , Britannica Elementary , CNN adult and CNN abridged text .", "For training SVMs , we used the SVMUght toolkit developed by Joachims 1998b .", "Using development data , we selected the radial basis function kernel and tuned parameters using cross validation and grid search as described in Hsu et al . , 2003 .", "We divide the Weekly Reader corpus described in Section 3 into separate training , development , and test sets .", "The number of articles in each set is shown in Table 3 .", "The development data is used as a test set for comparing classifiers , tuning parameters , etc , and the results presented in this section are based on the test set .", "We present results in three different formats .", "For analyzing our binary classifiers , we use Detection Error Tradeoff DET curves and precision recall corpus as divided into training , development and test sets .", "The dev and test sets are the same size and each consist of approximately 5 of the data for each grade level . measures .", "For comparison to other methods , e . g .", "Flesch Kincaid and Lexile , which are not binary classifiers , we consider the percentage of articles which are misclassified by more than one grade level .", "Detection Error Tradeoff curves show the tradeoff between misses and false alarms for different threshold values for the classifiers .", "Misses are positive examples of a class that are misclassified as negative examples ; false alarms are negative examples misclassified as positive .", "DET curves have been used in other detection tasks in language processing , e . g .", "Martin et al . 1997 .", "We use these curves to visualize the tradeoff between the two types of errors , and select the minimum cost operating point in order to get a threshold for precision and recall calculations .", "The minimum cost operating point depends on the relative costs of misses and false alarms ; it is conceivable that one type of error might be more serious than the other .", "After consultation with teachers future users of our system , we concluded that there are pros and cons to each side , so for the purpose of this analysis we weighted the two types of errors equally .", "In this work , the minimum cost operating point is selected by averaging the percentages of misses and false alarms at each point and choosing the point with the lowest average .", "Unless otherwise noted , errors reported are associated with these actual operating points , which may not lie on the convex hull of the DET curve .", "Precision and recall are often used to assess information retrieval systems , and our task is similar .", "Precision indicates the percentage of the retrieved documents that are relevant , in this case the percentage of detected documents that match the target grade level .", "Recall indicates the percentage of the total number of relevant documents in the data set that are retrieved , in this case the percentage of the total number of documents from the target level that are detected . these classifiers , indicated by large dots in the plot , are in the range of 33 43 , with only one over 40 .", "The curves for bigram and unigram models have similar shapes , but the trigram models outperform the lower order models .", "Error rates for the bigram models range from 37 45 and the unigram models have error rates in the 39 49 range , with all but one over 40 .", "Although our training corpus is small the feature selection described in Section 4 . 2 allows us to use these higher order trigram models .", "By combining language model scores with other features in an SVM framework , we achieve our best results .", "Figures 2 and 3 show DET curves for this set of classifiers on the development set and test set , respectively .", "The grade 2 and 5 classifiers have the best performance , probably because grade 3 and 4 must be distinguished from other classes at both higher and lower levels .", "Using threshold values selected based on minimum cost on the development set , indicated by large dots on the plot , we calculated precision and recall on the test set .", "Results are presented in Table 4 .", "The grade 3 classifier has high recall but relatively low precision ; the grade 4 classifier does better on precision and reasonably well on recall .", "Since the minimum cost operating points do not correspond to the equal error rate i . e . equal percentage of misses and false alarms there is variation in the precision recall tradeoff for the different grade level classifiers .", "For example , for class 3 , the operating point corresponds to a high probability of false alarms and a lower probability of misses , which results in low precision and high recall .", "For operating points chosen on the convex hull of the DET curves , the equal error rate ranges from 12 25 for the different grade levels .", "We investigated the contribution of individual features to the overall performance of the SVM classifier and found that no features stood out as most important , and performance was degraded when any particular features were removed .", "We also compared error rates for the best performing SVM classifier with two traditional reading level measures , Flesch Kincaid and Lexile .", "The Flesch Kincaid Grade Level index is a commonly used measure of reading level based on the average number of syllables per word and average sentence length .", "The Flesch Kincaid score for a document is intended to directly correspond with its grade level .", "We chose the Lexile measure as an example of a reading level classifier based on word lists . 3 Lexile scores do not correlate directly to numeric grade levels , however a mapping of ranges of Lexile scores to their corresponding grade levels is available on the Lexile web site Lexile , 2005 .", "For each of these three classifiers , Table 5 shows the percentage of articles which are misclassified by more than one grade level .", "Flesch Kincaid performs poorly , as expected since its only features are sen3Other classifiers such as Dale Chall do not have automatic software available . tence length and average syllable count .", "Although this index is commonly used , perhaps due to its simplicity , it is not accurate enough for the intended application .", "Our SVM classifier also outperforms the Lexile metric .", "Lexile is a more general measure while our classifier is trained on this particular domain , so the better performance of our model is not entirely surprising .", "Importantly , however , our classifier is easily tuned to any corpus of interest .", "To test our classifier on data outside the Weekly Reader corpus , we downloaded 10 randomly selected newspaper articles from the Kidspost edition of The Washington Post 2005 .", "Kidspost is intended for grades 3 8 .", "We found that our SVM classifier , trained on the Weekly Reader corpus , classified four of these articles as grade 4 and seven articles as grade 5 with one overlap with grade 4 .", "These results indicate that our classifier can generalize to other data sets .", "Since there was no training data corresponding to higher reading levels , the best performance we can expect for adult level newspaper articles is for our classifiers to mark them as the highest grade level , which is indeed what happened for 10 randomly chosen articles from standard edition of The Washington Post .", "Statistical LMs were used to classify texts based on reading level , with trigram models being noticeably more accurate than bigrams and unigrams .", "Combining information from statistical LMs with other features using support vector machines provided the best results .", "Future work includes testing additional classifier features , e . g . parser likelihood scores and features obtained using a syntax based language model such as Chelba and Jelinek 2000 or Roark 2001 .", "Further experiments are planned on the generalizability of our classifier to text from other sources e . g . newspaper articles , web pages ; to accomplish this we will add higher level text as negative training data .", "We also plan to test these techniques on languages other than English , and incorporate them with an information retrieval system to create a tool that may be used by teachers to help select reading material for their students ."], "summary_lines": ["Reading Level Assessment Using Support Vector Machines And Statistical Language Models\n", "Reading proficiency is a fundamental component of language competency.\n", "However, finding topical texts at an appropriate reading level for foreign and second language learners is a challenge for teachers.\n", "This task can be addressed with natural language processing technology to assess reading level.\n", "Existing measures of reading level are not well suited to this task, but previous work and our own pilot experiments have shown the benefit of using statistical language models.\n", "In this paper, we also use support vector machines to combine features from traditional reading level measures, statistical language models, and other language processing tools to produce a better method of assessing reading level.\n", "We develop a SVM categoriser combining a classifier based on trigram language models (one for each level of difficulty), some parsing features such as average tree height, and variables traditionally used in readability.\n", "We use syntactic features, such as parse tree height or the number of passive sentences, to predict reading grade levels.\n"]}
{"article_lines": ["Intricacies Of Collins Parsing Model", "M . University of Pennsylvania This article documents a large set of heretofore unpublished details Collins used in his parser , such that , along with Collins 1999 thesis , this article contains all information necessary to duplicate Collins benchmark results .", "Indeed , these as yet unpublished details account for an 11 relative increase in error from an implementation including all details to a clean room implementation of Collins model .", "We also show a cleaner and equally well performing method for the handling of punctuation and conjunction and reveal certain other probabilistic oddities about Collins parser .", "We not only analyze the effect of the unpublished details , but also reanalyze the effect of certain well known details , revealing that bilexical dependencies are barely used by the model and that head choice is not nearly as important to overall parsing performance as once thought .", "Finally , we perform experiments that show that the true discriminative power of lexicalization appears to lie in the fact that unlexicalized syntactic structures are generated conditioning on the headword and its part of speech .", "This article documents a large set of heretofore unpublished details Collins used in his parser , such that , along with Collins 1999 thesis , this article contains all information necessary to duplicate Collins benchmark results .", "Indeed , these as yet unpublished details account for an 11 relative increase in error from an implementation including all details to a clean room implementation of Collins model .", "We also show a cleaner and equally well performing method for the handling of punctuation and conjunction and reveal certain other probabilistic oddities about Collins parser .", "We not only analyze the effect of the unpublished details , but also reanalyze the effect of certain well known details , revealing that bilexical dependencies are barely used by the model and that head choice is not nearly as important to overall parsing performance as once thought .", "Finally , we perform experiments that show that the true discriminative power of lexicalization appears to lie in the fact that unlexicalized syntactic structures are generated conditioning on the headword and its part of speech .", "Michael Collins 1996 , 1997 , 1999 parsing models have been quite influential in the field of natural language processing .", "Not only did they achieve new performance benchmarks on parsing the Penn Treebank Marcus , Santorini , and Marcinkiewicz 1993 , and not only did they serve as the basis of Collins own future work Collins 2000 ; Collins and Duffy 2002 , but they also served as the basis of important work on parser selection Henderson and Brill 1999 , an investigation of corpus variation and the effectiveness of bilexical dependencies Gildea 2001 , sample selection Hwa 2001 , bootstrapping non English parsers Hwa , Resnik , and Weinberg 2002 , and the automatic labeling of semantic roles and predicate argument extraction Gildea and Jurafsky 2000 ; Gildea and Palmer 2002 , as well as that of other research efforts .", "Recently , in order to continue our work combining word sense with parsing Bikel 2000 and the study of language dependent and independent parsing features Bikel and Chiang 2000 , we built a multilingual parsing engine that is capable of instantiating a wide variety of generative statistical parsing models Bikel 2002 . 1 As an appropriate baseline model , we chose to instantiate the parameters of Collins Model 2 .", "This task proved more difficult than it initially appeared .", "Starting with Collins 1999 thesis , we reproduced all the parameters described but did not achieve nearly the same high performance on the well established development test set of Section 00 of the Penn Treebank .", "Together with Collins thesis , this article contains all the information necessary to replicate Collins parsing results . 2 Specifically , this article describes all the as yet unpublished details and features of Collins model and some analysis of the effect of these features with respect to parsing performance , as well as some comparative analysis of the effects of published features . 3 In particular , implementing Collins model using only the published details causes an 11 increase in relative error over Collins own published results .", "That is , taken together , all the unpublished details have a significant effect on overall parsing performance .", "In addition to the effects of the unpublished details , we also have new evidence to show that the discriminative power of Collins model does not lie where once thought Bilexical dependencies play an extremely small role in Collins models Gildea 2001 , and head choice is not nearly as critical as once thought .", "This article also discusses the rationale for various parameter choices .", "In general , we will limit our discussion to Collins Model 2 , but we make occasional reference to Model 3 , as well .", "There are three primary motivations for this work .", "First , Collins parsing model represents a widely used and cited parsing model .", "As such , if it is not desirable to use it as a black box it has only recently been made publicly available , then it should be possible to replicate the model in full , providing a necessary consistency among research efforts employing it .", "Careful examination of its intricacies will also allow researchers to deviate from the original model when they think it is warranted and accurately document those deviations , as well as understand the implications of doing so .", "The second motivation is related to the first science dictates that experiments be replicable , for this is the way we may test and validate them .", "The work described here comes in the wake of several previous efforts to replicate this particular model , but this is the first such effort to provide a faithful and equally well performing emulation of the original .", "The third motivation is that a deep understanding of an existing model its intricacies , the interplay of its many features provides the necessary platform for advancement to newer , better models .", "This is especially true in an area like statistical parsing that has seen rapid maturation followed by a soft plateau in performance .", "Rather than simply throwing features into a new model and measuring their effect in a crude way using standard evaluation metrics , this work aims to provide a more thorough understanding of the nature of a model s features .", "This understanding not only is useful in its own right but should help point the way toward newer features to model or better modeling techniques , for we are in the best position for advancement when we understand existing strengths and limitations .", "2 In the course of replicating Collins results , it was brought to our attention that several other researchers had also tried to do this and had also gotten performance that fell short of Collins published results .", "For example , Gildea 2001 reimplemented Collins Model 1 but obtained results with roughly 16 . 7 more relative error than Collins reported results using that model .", "The Collins parsing model decomposes the generation of a parse tree into many small steps , using reasonable independence assumptions to make the parameter estimation problem tractable .", "Even though decoding proceeds bottom up , the model is defined in a top down manner .", "Every nonterminal label in every tree is lexicalized the label is augmented to include a unique headword and that headword s part of speech that the node dominates .", "The lexicalized PCFG that sits behind Model 2 has rules of the form where P , L ; , R ; , and H are all lexicalized nonterminals , and P inherits its lexical head from its distinguished head child , H . In this generative model , first P is generated , then its head child H , then each of the left and right modifying nonterminals are generated from the head outward .", "The modifying nonterminals L ; and R ; are generated conditioning on P and H , as well as a distance metric based on what material intervenes between the currently generated modifying nonterminal and H and an incremental subcategorization frame feature a multiset containing the arguments of H that have yet to be generated on the side of H in which the currently generated nonterminal falls .", "Note that if the modifying nonterminals were generated completely independently , the model would be very impoverished , but in actuality , because it includes the distance and subcategorization frame features , the model captures a crucial bit of linguistic reality , namely , that words often have well defined sets of complements and adjuncts , occurring with some well defined distribution in the right hand sides of a context free rewriting system .", "The process proceeds recursively , treating each newly generated modifier as a parent and then generating its head and modifier children ; the process terminates when lexicalized preterminals are generated .", "As a way to guarantee the consistency of the model , the model also generates two hidden STOP nonterminals as the leftmost and rightmost children of every parent see Figure 7 .", "To the casual reader of Collins thesis , it may not be immediately apparent that there are quite a few preprocessing steps for each annotated training tree and that these steps are crucial to the performance of the parser .", "We identified 11 preprocessing steps necessary to prepare training trees when using Collins parsing model The order of presentation in the foregoing list is not arbitrary , as some of the steps depend on results produced in previous steps .", "Also , we have separated the steps into their functional units ; an implementation could combine steps that are independent of one another for clarity , our implementation does not , however .", "Finally , we note that the final step , head finding , is actually required by some of the previous steps in certain cases ; in our implementation , we selectively employ a head finding module during the first 10 steps where necessary .", "A few of the preprocessing steps rely on the notion of a coordinated phrase .", "In this article , the conditions under which a phrase is considered coordinated are slightly more detailed than is described in Collins thesis .", "A node represents a coordinated phrase if In the Penn Treebank , a coordinating conjunction is any preterminal node with the label CC .", "This definition essentially picks out all phrases in which the head child is truly conjoined to some other phrase , as opposed to a phrase in which , say , there is an initial CC , such as an S that begins with the conjunction but .", "As a preprocessing step , pruning of unnecessary nodes simply removes preterminals that should have little or no bearing on parser performance .", "In the case of the English Treebank , the pruned subtrees are all preterminal subtrees whose root label is one of , , . .", "There are two reasons to remove these types of subtrees when parsing the English Treebank First , in the treebanking guidelines Bies 1995 , quotation marks were given the lowest possible priority and thus cannot be expected to appear within constituent boundaries in any kind of consistent way , and second , neither of these types of preterminals nor any punctuation marks , for that matter counts towards the parsing score .", "An NP is basal when it does not itself dominate an NP ; such NP nodes are relabeled NPB .", "More accurately , an NP is basal when it dominates no other NPs except possessive NPs , where a possessive NP is an NP that dominates POS , the preterminal possessive A nonhead NPB child of NP requires insertion of extra NP . marker for the Penn Treebank .", "These possessive NPs are almost always themselves base NPs and are therefore almost always relabeled NPB .", "For consistency s sake , when an NP has been relabeled as NPB , a normal NP node is often inserted as a parent nonterminal .", "This insertion ensures that NPB nodes are always dominated by NP nodes .", "The conditions for inserting this extra NP level are slightly more detailed than is described in Collins thesis , however .", "The extra NP level is added if one of the following conditions holds In postprocessing , when an NPB is an only child of an NP node , the extra NP level is removed by merging the two nodes into a single NP node , and all remaining NPB nodes are relabeled NP .", "The insertion of extra NP levels above certain NPB nodes achieves a degree of consistency for NPs , effectively causing the portion of the model that generates children of NP nodes to have less perplexity .", "Collins appears to have made a similar effort to improve the consistency of the NPB model .", "NPB nodes that have sentential nodes as their final rightmost child are repaired The sentential child is raised so that it becomes a new right sibling of the NPB node see Figure 3 . 6 While such a transformation is reasonable , it is interesting to note that Collins parser performs no equivalent detransformation when parsing is complete , meaning that when the parser produces the repaired structure during testing , there is a spurious NP bracket . 7 The gap feature is discussed extensively in chapter 7 of Collins thesis and is applicable only to his Model 3 .", "The preprocessing step in which gap information is added locates every null element preterminal , finds its co indexed WHNP antecedent higher up in the tree , replaces the null element preterminal with a special trace tag , and threads the gap feature in every nonterminal in the chain between the common ancestor of the antecedent and the trace .", "The threaded gap feature is represented by appending g to every node label in the chain .", "The only detail we would like to highlight here is that an implementation of this preprocessing step should check for cases in which threading is impossible , such as when two filler gap dependencies cross .", "An implementation should be able to handle nested filler gap dependencies , however .", "The node labels of sentences with no subjects are transformed from S to SG .", "This step enables the parsing model to be sensitive to the different contexts in which such subjectless sentences occur as compared to normal S nodes , since the subjectless sentences are functionally acting as noun phrases .", "Collins example of illustrates the utility of this transformation .", "However , the conditions under which an S may be relabeled are not spelled out ; one might assume that every S whose subject identified in the Penn Treebank with the SBJ function tag dominates a null element should be relabeled SG .", "In actuality , the conditions are much stricter .", "An S is relabeled SG when the following conditions hold The latter two conditions appear to be an effort to capture only those subjectless sentences that are based around gerunds , as in the flying planes example . 8 Removing null elements simply involves pruning the tree to eliminate any subtree that dominates only null elements .", "The special trace tag that is inserted in the step that adds gap information Section 4 . 5 is excluded , as it is specifically chosen to be something other than the null element preterminal marker which is NONE in the Penn Treebank .", "The step in which punctuation is raised is discussed in detail in chapter 7 of Collins thesis .", "The main idea is to raise punctuation which is any preterminal subtree in which the part of speech is either a comma or a colon to the highest possible point in the tree , so that it always sits between two other nonterminals .", "Punctuation that occurs at the very beginning or end of a sentence is raised away , that is , pruned .", "In addition , any implementation of this step should handle the case in which multiple punctuation elements appear as the initial or final children of some node , as well as the more pathological case in which multiple punctuation elements appear along the left or right frontier of a subtree see Figure 4 .", "Finally , it is not clear what to do with nodes that dominate only punctuation preterminals .", "Our implementation simply issues a warning in such cases and leaves the punctuation symbols untouched .", "Head children are not exempt from being relabeled as arguments .", "Collins employs a small set of heuristics to mark certain nonterminals as arguments , by appending A to the nonterminal label .", "This section reveals three unpublished details about Collins argument finding This step simply involves stripping away all nonterminal augmentations , except those that have been added from other preprocessing steps such as the A augmentation for argument labels .", "This includes the stripping away of all function tags and indices marked by the Treebank annotators .", "Head moves from right to left conjunct in a coordinated phrase , except when the parent nonterminal is NPB .", "With arguments identified as described in Section 4 . 9 , if a subjectless sentence is found to have an argument prior to its head , this step detransforms the SG so that it reverts to being an S . Head finding is discussed at length in Collins thesis , and the head finding rules used are included in his Appendix A .", "There are a few unpublished details worth mentioning , however .", "There is no head finding rule for NX nonterminals , so the default rule of picking the leftmost child is used . 10 NX nodes roughly represent the N level of syntax and in practice often denote base NPs .", "As such , the default rule often picks out a less thanideal head child , such as an adjective that is the leftmost child in a base NP .", "Collins thesis discusses a case in which the initial head is modified when it is found to denote the right conjunct in a coordinated phrase .", "That is , if the head rules pick out a head that is preceded by a CC that is non initial , the head should be modified to be the nonterminal immediately to the left of the CC see Figure 6 .", "An important detail is that such head movement does not occur inside base NPs .", "That is , a phrase headed by NPB may indeed look as though it constitutes a coordinated phrase it has a CC that is noninitial but to the left of the currently chosen head but the currently chosen head should remain chosen . 11 As we shall see , there is exceptional behavior for base NPs in almost every part of the Collins parser .", "10 In our first attempt at replicating Collins results , we simply employed the same head finding rule for NX nodes as for NP nodes .", "This choice yields different but not necessarily inferior results .", "11 In Section 4 . 1 , we defined coordinated phrases in terms of heads , but here we are discussing how the head finder itself needs to determine whether a phrase is coordinated .", "It does this by considering the potential new choice of head If the head finding rules pick out a head that is preceded by a noninitial CC Jane , will moving the head to be a child to the left of the CC John yield a coordinated phrase ?", "If so , then the head should be moved except when the parent is NPB . vi feature is true when generating right hand STOP nonterminal , because the NP the will to continue contains a verb .", "The trainer s job is to decompose annotated training trees into a series of head and modifier generation steps , recording the counts of each of these steps .", "Referring to 1 , each H , Li , and Ri are generated conditioning on previously generated items , and each of these events consisting of a generated item and some maximal history context is counted .", "Even with all this decomposition , sparse data are still a problem , and so each probability estimate for some generated item given a maximal context is smoothed with coarser distributions using less context , whose counts are derived from these top level head and modifier generation counts .", "As mentioned in Section 3 , instead of generating each modifier independently , the model conditions the generation of modifiers on certain aspects of the history .", "One such function of the history is the distance metric .", "One of the two components of this distance metric is what we will call the verb intervening feature , which is a predicate vi that is true if a verb has been generated somewhere in the surface string of the previously generated modifiers on the current side of the head .", "For example , in Figure 7 , when generating the right hand STOP nonterminal child of the VP , the vi predicate is true , because one of the previously generated modifiers on the right side of the head dominates a verb , continue . 12 More formally , this feature is most easily defined in terms of a recursively defined cv contains verb predicate , which is true if and only if a node dominates a verb Bikel Intricacies of Collins Parsing Model Referring to 2 , we define the verb intervening predicate recursively on the first order Markov process generating modifying nonterminals and similarly for right modifiers .", "What is considered to be a verb ?", "While this is not spelled out , as it happens , a verb is any word whose part of speech tag is one of VB , VBD , VBG , VBN , VBP , VBZ .", "That is , the cv predicate returns true only for these preterminals and false for all other preterminals .", "Crucially , this set omits MD , which is the marker for modal verbs .", "Another crucial point about the vi predicate is that it does not include verbs that appear within base NPs .", "Put another way , in order to emulate Collins model , we need to amend the definition of cv by stipulating that cv NPB false .", "One oddity of Collins trainer that we mention here for the sake of completeness is that it skips certain training trees .", "For odd historical reasons , 13 the trainer skips all trees with more than 500 tokens , where a token is considered in this context to be a word , a nonterminal label , or a parenthesis .", "This oddity entails that even some relatively short sentences get skipped because they have lots of tree structure .", "In the standard Wall Street Journal training corpus , Sections 02 21 of the Penn Treebank , there are 120 such sentences that are skipped .", "Unless there is something inherently wrong with these trees , one would predict that adding them to the training set would improve a parser s performance .", "As it happens , there is actually a minuscule and probably statistically insignificant drop in performance see Table 5 when these trees are included .", "5 . 3 . 1 The Threshold Problem .", "Collins mentions in chapter 7 of his thesis that a ll words occurring less than 5 times in training data , and words in test data which have never been seen in training , are replaced with the UNKNOWN token page 186 . The frequency below which words are considered unknown is often called the unknownword threshold .", "Unfortunately , this term can also refer to the frequency above which words are considered known .", "As it happens , the unknown word threshold Collins uses in his parser for English is six , not five . 14 To be absolutely unambiguous , words that occur fewer than six times , which is to say , words that occur five times or fewer , in the data are considered unknown . words into the parsing model , then , is simply to map all low frequency words in the training data to some special UNKNOWN token before counting top level events for parameter estimation where low frequency means below the unknown word threshold .", "Collins trainer actually does not do this .", "Instead , it does not directly modify any of the words in the original training trees and proceeds to break up these unmodified trees into the top level events .", "After these events have been collected 13 This phrase was taken from a comment in one of Collins preprocessing Perl scripts .", "14 As with many of the discovered discrepancies between the thesis and the implementation , we determined the different unknown word threshold through reverse engineering , in this case , through an analysis of the events output by Collins trainer . and counted , the trainer selectively maps low frequency words when deriving counts for the various context back off levels of the parameters that make use of bilexical statistics .", "If this mapping were performed uniformly , then it would be identical to mapping low frequency words prior to top level event counting ; this is not the case , however .", "We describe the details of this unknown word mapping in Section 6 . 9 . 2 .", "While there is a negligible yet detrimental effect on overall parsing performance when one uses an unknown word threshold of five instead of six , when this change is combined with the obvious method for handling unknown words , there is actually a minuscule improvement in overall parsing performance see Table 5 .", "All parameters that generate trees in Collins model are estimates of conditional probabilities .", "Even though the following overview of parameter classes presents only the maximal contexts of the conditional probability estimates , it is important to bear in mind that the model always makes use of smoothed probability estimates that are the linear interpolation of several raw maximum likelihood estimates , using various amounts of context we explore smoothing in detail in Section 6 . 8 .", "In Sections 4 . 5 and 4 . 9 , we saw how the raw Treebank nonterminal set is expanded to include nonterminals augmented with A and g . Although it is not made explicit in Collins thesis , Collins model uses two mapping functions to remove these augmentations when including nonterminals in the history contexts of conditional probabilities .", "Presumably this was done to help alleviate sparse data problems .", "We denote the argument removal mapping function as alpha and the gap removal mapping function as gamma .", "For example Since gap augmentations are present only in Model 3 , the gamma function effectively is the identity function in the context of Models 1 and 2 .", "The head nonterminal is generated conditioning on its parent nonterminal label , as well as the headword and head tag which they share , since parents inherit their lexical head information from their head children .", "More specifically , an unlexicalized head nonterminal label is generated conditioning on the fully lexicalized parent nonterminal .", "We denote the parameter class as follows When the model generates a head child nonterminal for some lexicalized parent nonterminal , it also generates a kind of subcategorization frame subcat on either side of the head child , with the following maximal context A fully lexicalized tree .", "The VP node is the head child of S . Probabilistically , it is as though these subcats are generated with the head child , via application of the chain rule , but they are conditionally independent . 15 These subcats may be thought of as lists of requirements on a particular side of a head .", "For example , in Figure 8 , after the root node of the tree has been generated see Section 6 . 10 , the head child VP is generated , conditioning on both the parent label S and the headword of that parent , sat VBD .", "Before any modifiers of the head child are generated , both a left and right subcat frame are generated .", "In this case , the left subcat is NP A and the right subcat is , meaning that there are no required elements to be generated on the right side of the head .", "Subcats do not specify the order of the required arguments .", "They are dynamically updated multisets When a requirement has been generated , it is removed from the multiset , and subsequent modifiers are generated conditioning on the updated multiset . 16 The implementation of subcats in Collins parser is even more specific Subcats are multisets containing various numbers of precisely six types of items NP A , S A , SBAR A , VP A , g , and miscellaneous .", "The g indicates that a gap must be generated and is applicable only to Model 3 .", "Miscellaneous items include all nonterminals that were marked as arguments in the training data that were not any of the other named types .", "There are rules for determining whether NPs , Ss , SBARs , and VPs are arguments , and the miscellaneous arguments occur as the result of the argument finding rule for PPs , which states that the first non PRN , non part of speech tag that occurs after the head of a PP should be marked as an argument , and therefore nodes that are not one of the four named types can be marked .", "As mentioned above , after a head child and its left and right subcats are generated , modifiers are generated from the head outward , as indicated by the modifier nonterminal indices in Figure 1 .", "A fully lexicalized nonterminal has three components the nonterminal label , the headword , and the headword s part of speech .", "Fully lexicalized modifying nonterminals are generated in two steps to allow for the parameters to be independently smoothed , which , in turn , is done to avoid sparse data problems .", "These two steps estimate the joint event of all three components using the chain rule .", "In the A tree containing both punctuation and conjunction . first step , a partially lexicalized version of the nonterminal is generated , consisting of the unlexicalized label plus the part of speech of its headword .", "These partially lexicalized modifying nonterminals are generated conditioning on the parent label , the head label , the headword , the head tag , the current state of the dynamic subcat , and a distance metric .", "Symbolically , the parameter classes are where denotes the distance metric . 17 As discussed above , one of the two components of this distance metric is the vi predicate .", "The other is a predicate that simply reports whether the current modifier is the first modifier being generated , that is , whether i 1 .", "The second step is to generate the headword itself , where , because of the chain rule , the conditioning context consists of everything in the histories of expressions 7 and 8 plus the partially lexicalized modifier .", "As there are some interesting idiosyncrasies with these headword generation parameters , we describe them in more detail in Section 6 . 9 .", "6 . 5 . 1 Inconsistent Model .", "As discussed in Section 4 . 8 , punctuation is raised to the highest position in the tree .", "This means that in some sense , punctuation acts very much like a coordinating conjunction , in that it conjoins the two siblings between which it sits .", "Observing that it might be helpful for conjunctions to be generated conditioning on both of their conjuncts , Collins introduced two new parameter classes in his thesis parser , Ppunc and PCC . 18 As per the definition of a coordinated phrase in Section 4 . 1 , conjunction via a CC node or a punctuation node always occurs posthead i . e . , as a right sibling of the head .", "Put another way , if a conjunction or punctuation mark occurs prehead , it is 17 Throughout this article we use the notation L w , t i to refer to the three items that constitute a fully lexicalized left modifying nonterminal , which are the unlexicalized label Li , its headword wLi , and its part of speech tLi , and similarly for right modifiers .", "We use L t i to refer to the two items Li and tLi of a partially lexicalized nonterminal .", "Finally , when we do not wish to distinguish between a left and right modifier , we use M w , t i , M t i , and Mi . not generated via this mechanism . 19 Furthermore , even if there is arbitrary material between the right conjunct and the head , the parameters effectively assume that the left conjunct is always the head child .", "For example , in Figure 9 , the rightmost NP bushy bushes is considered to be conjoined to the leftmost NP short grass , which is the head child , even though there is an intervening NP tall trees .", "The new parameters are incorporated into the model by requiring that all modifying nonterminals be generated with two boolean flags coord , indicating that the nonterminal is conjoined to the head via a CC , and punc , indicating that the nonterminal is conjoined to the head via a punctuation mark .", "When either or both of these flags is true , the intervening punctuation or conjunction is generated via appropriate instances of the Ppun , PCC parameter classes .", "For example , the model generates the five children in Figure 9 in the following order first , the head child is generated , which is the leftmost NP short grass , conditioning on the parent label and the headword and tag .", "Then , since modifiers are always generated from the head outward , the right sibling of the head , which is the tall trees NP , is generated with both the punc and CC flags false .", "Then , the rightmost NP bushy bushes is generated with both the punc and CC booleans true , since it is considered to be conjoined to the head child and requires the generation of an intervening punctuation mark and conjunction .", "Finally , the intervening punctuation is generated conditioning on the parent , the head , and the right conjunct , including the headwords of the two conjoined phrases , and the intervening CC is similarly generated .", "A simplified version of the probability of generating all these children is summarized as follows The idea is that using the chain rule , the generation of two conjuncts and that which conjoins them is estimated as one large joint event . 20 This scheme of using flags to trigger the Ppun , and PCC parameters is problematic , at least from a theoretical standpoint , as it causes the model to be inconsistent .", "Figure 10 shows three different trees that would all receive the same probability from Collins model .", "The problem is that coordinating conjunctions and punctuation are not generated as first class words , but only as triggered from these punc and coord flags , meaning that the number of such intervening conjunctive items and the order in which they are to be generated is not specified .", "So for a given sentence tree pair containing a conjunction and or a punctuation mark , there is an infinite number of similar sentence tree pairs with arbitrary amounts of conjunctive material between the same two nodes .", "Because all of these trees have the same , nonzero probability , the sum ETP T , where T is a possible tree generated by the model , diverges , meaning the model is inconsistent Booth and Thompson 1973 .", "Another consequence of not generating posthead conjunctions and punctuation as first class words is that they The Collins model assigns equal probability to these three trees . do not count when calculating the head adjacency component of Collins distance metric .", "When emulating Collins model , instead of reproducing the Ppun , and PCC parameter classes directly in our parsing engine , we chose to use a different mechanism that does not yield an inconsistent model but still estimates the large joint event that was the motivation behind these parameters in the first place .", "6 . 5 . 2 History Mechanism .", "In our emulation of Collins model , we use the history , rather than the dedicated parameter classes PCC and Ppun , , to estimate the joint event of generating a conjunction or punctuation mark and its two conjuncts .", "The first big change that results is that we treat punctuation preterminals and CCs as first class objects , meaning that they are generated in the same way as any other modifying nonterminal .", "The second change is a little more involved .", "First , we redefine the distance metric to consist solely of the vi predicate .", "Then , we add to the conditioning context a mapped version of the previously generated modifier according to the following where Mi is some modifier Li or Ri . 21 So , the maximal context for our modifying nonterminal parameter class is now defined as follows where side is a boolean valued event that indicates whether the modifier is on the left or right side of the head .", "By treating CC and punctuation nodes as first class nonterminals and by adding the mapped version of the previously generated modifier , we have , in one fell swoop , incorporated the no intervening component of Collins distance metric the i 0 case of the delta function and achieved an estimate of the joint event of a conjunction and its conjuncts , albeit with different dependencies , that is , a different application of the chain rule .", "To put this parameterization change in sharp relief , consider the abstract tree structure To a first approximation , under the old parameterization , the conjunction of some node R1 with a head H and a parent P looked like this \u02c6pH H P \u02c6pR R1 , coord 1 P , H \u02c6pCC CC P , H , R1 whereas under the new parameterization , it looks like this Either way , the probability of the joint conditional event H , CC , R1 P is being estimated , but with the new method , there is no need to add two new specialized parameter classes , and the new method does not introduce inconsistency into the model .", "Using less simplification , the probability of generating the five children of Figure 9 is now 21 Originally , we had an additional mechanism that attempted to generate punctuation and conjunctions with conditional independence .", "One of our reviewers astutely pointed out that the mechanism led to a deficient model the very thing we have been trying to avoid , and so we have subsequently removed it from our model .", "The removal leads to a 0 . 05 absolute reduction in F measure which in this case is also a 0 . 05 relative increase in error on sentences of length 40 words in Section 00 of the Penn Treebank .", "As this difference is not at all statistically significant according to a randomized stratified shuffling test Cohen 1995 , all evaluations reported in this article are with the original model .", "As shown in Section 8 . 1 , this new parameterization yields virtually identical performance to that of the Collins model . 22 As we have already seen , there are several ways in which base NPs are exceptional in Collins parsing model .", "This is partly because the flat structure of base NPs in the Penn Treebank suggested the use of a completely different model by which to generate them .", "Essentially , the model for generating children of NPB nodes is a bigrams of nonterminals model .", "That is , it looks a great deal like a bigram language model , except that the items being generated are not words , but lexicalized nonterminals .", "Heads of NPB nodes are generated using the normal head generation parameter , but modifiers are always generated conditioning not on the head , but on the previously generated modifier .", "That is , we modify expressions 7 and 8 to be Though it is not entirely spelled out in his thesis , Collins considers the previously generated modifier to be the head child , for all intents and purposes .", "Thus , the subcat and distance metrics are always irrelevant , since it is as though the current modifier is right next to the head . 23 Another consequence of this is that NPBs are never considered to be coordinated phrases as mentioned in Section 4 . 12 , and thus CCs dominated by NPB are never generated using a PCC parameter ; instead , they are generated using a normal modifying nonterminal parameter .", "Punctuation dominated by NPB , on the other hand , is still , as always , generated via Ppunc parameters , but crucially , the modifier is always conjoined via the punctuation mark to the pseudohead that is the previously generated modifier .", "Consequently , when some right modifier Ri is generated , the previously generated modifier on the right side of the head , Ri 1 , is never a punctuation preterminal , but always the previous real i . e . , nonpunctuation preterminal . 24 Base NPs are also exceptional with respect to determining chart item equality , the comma pruning rule , and general beam pruning see Section 7 . 2 for details .", "Two parameter classes that make their appearance only in Appendix E of Collins thesis are those that compute priors on lexicalized nonterminals .", "These priors are used as a crude proxy for the outside probability of a chart item see Baker 1979 and Lari and Young 1990 for full descriptions of the Inside Outside algorithm .", "Previous work Goodman 1997 has shown that the inside probability alone is an insufficient scoring metric when comparing chart items covering the same span during decoding and that some estimate of the outside probability of a chart item should be factored into the score .", "A prior on the root lexicalized nonterminal label of the derivation forest represented by a particular chart item is used for this purpose in Collins parser .", "22 As described in Bikel 2002 , our parsing engine allows easy experimentation with a wide variety of different generative models , including the ability to construct history contexts from arbitrary numbers of previously generated modifiers .", "The mapping function delta and the transition function tau presented in this section are just two examples of this capability .", "The prior of a lexicalized nonterminal M w , t is broken down into two separate estimates using parameters from two new classes , Ppriorw and PpriorNT where \u02c6p M w , t is smoothed with \u02c6p M t and estimates using the parameters of the Ppriorw class are unsmoothed .", "Many of the parameter classes in Collins model and indeed , in most statistical parsing models define conditional probabilities with very large conditioning contexts .", "In this case , the conditioning contexts represent some subset of the history of the generative process .", "Even if there were orders of magnitude more training data available , the large size of these contexts would cause horrendous sparse data problems .", "The solution is to smooth these distributions that are made rough primarily by the abundance of zeros .", "Collins uses the technique of deleted interpolation , which smoothes the distributions based on full contexts with those from coarser models that use less of the context , by successively deleting elements from the context at each back off level .", "As a simple example , the head parameter class smoothes PH0 H P , wh , th with PH1 H P , th and PH2 H P .", "For some conditional probability p A B , let us call the reduced context at the ith back off level Oi B , where typically O0 B B .", "Each estimate in the back off chain is computed via maximum likelihood ML estimation , and the overall smoothed estimate with n back off levels is computed using n 1 smoothing weights , denoted A0 , . . . , An 2 .", "These weights are used in a recursive fashion The smoothed version ei pi A Oi B of an unsmoothed ML estimate ei \u02c6pi A Oi B at back off level i is computed via the formula So , for example , with three levels of back off , the overall smoothed estimate would be defined as Each smoothing weight can be conceptualized as the confidence in the estimate with which it is being multiplied .", "These confidence values can be derived in a number of sensible ways ; the technique used by Collins was adapted from that used in Bikel et al . 1997 , which makes use of a quantity called the diversity of the history context Witten and Bell 1991 , which is equal to the number of unique futures observed in training for that history context .", "6 . 8 . 1 Deficient Model .", "As previously mentioned , n back off levels require n 1 smoothing weights .", "Collins parser effectively uses n weights , because the estimator always adds an extra , constant valued estimate to the back off chain .", "Collins parser hardcodes this extra value to be a vanishingly small but nonzero probability of 10 19 , resulting in smoothed estimates of the form when there are three levels of back off .", "The addition of this constant valued en 10 19 causes all estimates in the parser to be deficient , as it ends up throwing away probability mass .", "More formally , the proof leading to equation 17 no longer holds The distribution sums to less than one there is no history context in the model for which there are 1019 possible outcomes . 25 for computing smoothing weights is where ci is the count of the history context Oi B and ui is the diversity of that context . 26 The multiplicative constant five is used to give less weight to the back off levels with more context and was optimized by looking at overall parsing performance on the development test set , Section 00 of the Penn Treebank .", "We call this constant the smoothing factor and denote it as ff .", "As it happens , the actual formula for computing smoothing weights in Collins implementation is where ft is an unmentioned smoothing term .", "For every parameter class except the subcat parameter class and Ppriorw , ft 0 and ff 5 . 0 .", "For the subcat parameter class , ft 5 . 0 and ff 0 .", "For Ppriorw , ft 1 . 0 and ff 0 . 0 .", "This curiously means that diversity is not used at all when smoothing subcat generation probabilities . 27 The second case in 19 handles the situation in which the history context was never observed in training , that is , where ci ui 0 , which would yield an undefined value 25 Collins used this technique to ensure that even futures that were never seen with an observed history context would still have some probability mass , albeit a vanishingly small one Collins , personal communication , January 2003 .", "Another commonly used technique would be to back off to the uniform distribution , which has the desirable property of not producing deficient estimates .", "As with all of the treebank or model specific aspects of the Collins parser , our engine uses equation 16 or 18 depending on the value of a particular run time setting .", "26 The smoothing weights can be viewed as confidence values for the probability estimates with which they are multiplied .", "The Witten Bell technique crucially makes use of the quantity ni ui , the average number of transitions from the history context Oi B to a possible future .", "With a little algebraic manipulation , we have a quantity that is at its maximum when ni ci and at its minimum when ni 1 , that is , when every future observed in training was unique .", "This latter case represents when the model is most uncertain , in that the transition distribution from Oi B is uniform and poorly trained one observation per possible transition .", "Because these smoothing weights measure , in some sense , the closeness of the observed distribution to uniform , they can be viewed as proxies for the entropy of the distribution p Oi B .", "Back off levels for PLw PRw , the modifier headword generation parameter classes . wLiand tLi are , respectively , the headword and its part of speech of the nonterminal Li .", "This table is basically a reproduction of the last column of Table 7 . 1 in Collins thesis .", "Our new parameter class for the generation of headwords of modifying nonterminals . when ft 0 .", "In such situations , making \u03bbi 0 throws all remaining probability mass to the smoothed back off estimate , ei 1 .", "This is a crucial part of the way smoothing is done If a particular history context \u03c6i B has never been observed in training , the smoothed estimate using less context , \u03c6i 1 B , is simply substituted as the best guess for the estimate using more context ; that is , ei ei 1 . 28 As mentioned in Section 6 . 4 , fully lexicalized modifying nonterminals are generated in two steps .", "First , the label and part of speech tag are generated with an instance of PL or PR .", "Next , the headword is generated via an instance of one of two parameter classes , PLw or PRw .", "The back off contexts for the smoothed estimates of these parameters are specified in Table 1 .", "Notice how the last level of back off is markedly different from the previous two levels in that it removes nearly all the elements of the history In the face of sparse data , the probability of generating the headword of a modifying nonterminal is conditioned only on its part of speech . order to capture the most data for the crucial last level of back off , Collins uses words that occur on either side of the headword , resulting in a general estimate \u02c6p w I t , as opposed to \u02c6pLw wLi I tLi .", "Accordingly , in our emulation of Collins model , we replace the left and right word parameter classes with a single modifier headword generation parameter class that , as with 11 , includes a boolean side component that is deleted from the last level of back off see Table 2 .", "Even with this change , there is still a problem .", "Every headword in a lexicalized parse tree is the modifier of some other headword except the word that is the head of the entire sentence i . e . , the headword of the root nonterminal .", "In order to properly duplicate Collins model , an implementation must take care that the P w I t model includes counts for these important headwords . 29 The low frequency word Fido is mapped to UNKNOWN , but only when it is generated , not when it is conditioned upon .", "All the nonterminals have been lexicalized except for preterminals to show where the heads are .", "6 . 9 . 2 Unknown Word Mapping .", "As mentioned above , instead of mapping every lowfrequency word in the training data to some special UNKNOWN token , Collins trainer instead leaves the training data untouched and selectively maps words that appear in the back off levels of the parameters from the PL . and PR . parameter classes .", "Rather curiously , the trainer maps only words that appear in the futures of these parameters , but never in the histories .", "Put another way , low frequency words are generated as UNKNOWN but are left unchanged when they are conditioned upon .", "For example , in Figure 11 , where we assume Fido is a low frequency word , the trainer would derive counts for the smoothed parameter the word would not be mapped .", "This strange mapping scheme has some interesting consequences .", "First , imagine what happens to words that are truly unknown , that never occurred in the training data .", "Such words are mapped to the UNKNOWN token outright before parsing .", "Whenever the parser estimates a probability with such a truly unknown word in the history , it will necessarily throw all probability mass to the backed off estimate e1 in our earlier notation , since UNKNOWN effectively never occurred in a history context during training .", "The second consequence is that the mapping scheme yields a superficient 30 model , if all other parts of the model are probabilistically sound which is actually Back off structure for PTOPNT and PTOPw , which estimate the probability of generating H w , t as the root nonterminal of a parse tree .", "PTOPNT is unsmoothed . n a not applicable . not the case here .", "With a parsing model such as Collins that uses bilexical dependencies , generating words in the course of parsing is done very much as it is in a bigram language model Every word is generated conditioning on some previously generated word , as well as some hidden material .", "The only difference is that the word being conditioned upon is often not the immediately preceding word in the sentence .", "However , one could plausibly construct a consistent bigram language model that generates words with the same dependencies as those in a statistical parser that uses bilexical dependencies derived from head lexicalization .", "Collins personal communication , January 2003 notes that his parser s unknownword mapping scheme could be made consistent if one were to add a parameter class that estimated \u02c6p w UNKNOWN , where w E VL U UNKNOWN .", "The values of these estimates for a given sentence would be constant across all parses , meaning that the superficiency of the model would be irrelevant when determining arg max P T S .", "It is assumed that all trees that can be generated by the model have an implicit nonterminal TOP that is the parent of the observed root .", "The observed lexicalized root nonterminal is generated conditioning on TOP which has a prior probability of 1 . 0 using a parameter from the class PTOP .", "This special parameter class is mentioned in a footnote in chapter 7 of Collins thesis .", "There are actually two parameter classes used to generated observed roots , one for generating the partially lexicalized root nonterminal , which we call PTOPNT , and the other for generating the headword of the entire sentence , which we call PTOPw .", "Table 3 gives the unpublished back off structure of these two additional parameter classes .", "Note that PTOPw backs off to simply estimating \u02c6p w t .", "Technically , it should be estimating \u02c6pNT w t , which is to say the probability of a word s occurring with a tag in the space of lexicalized nonterminals .", "This is different from the last level of back off in the modifier headword parameter classes , which is effectively estimating \u02c6p w t in the space of lexicalized preterminals .", "The difference is that in the same sentence , the same headword can occur with the same tag in multiple nodes , such as sat in Figure 8 , which occurs with the tag VBD three times instead of just once in the tree shown there .", "Despite this difference , Collins parser uses counts from the shared last level of back off of the PLw and PRw parameters when delivering e1 estimates for the PTOPw parameters .", "Our parsing engine emulates this count sharing for PTOPw by default , by sharing counts from our PMw parameter class .", "Parsing , or decoding , is performed via a probabilistic version of the CKY chartparsing algorithm .", "As with normal CKY , even though the model is defined in a topdown , generative manner , decoding proceeds bottom up .", "Collins thesis gives a pseuSince the goal of the decoding process is to determine the maximally likely theory , if during decoding a proposed chart item is equal or , technically , equivalent to an item that is already in the chart , the one with the greater score survives .", "Chart item equality is closely tied to the generative parameters used to construct theories We want to treat two chart items as unequal if they represent derivation forests that would be considered unequal according to the output elements and conditioning contexts of the parameters used to generate them , subject to the independence assumptions of the model .", "For example , for two chart items to be considered equal , they must have the same label the label of the root of their respective derivation forests subtrees , the same headword and tag , and the same left and right subcat .", "They must also have the same head label that is , label of the head child .", "If a chart item s root label is an NP node , its head label is most often an NPB node , given the extra NP levels that are added during preprocessing to ensure that NPB nodes are always dominated by NP nodes .", "In such cases , the chart item will contain a back pointer to the chart item that represents the base NP .", "Curiously , however , Collins implementation considers the head label of the NP chart item not to be NPB , but rather the head label of the NPB chart item .", "In other words , to get the head label of an NP chart item , one must peek through the NPB and get at the NPB s head label .", "Presumably , this was done as a consideration for the NPB nodes being extra nodes , in some sense .", "It appears to have little effect on overall parsing accuracy , however .", "Ideally , every parse theory could be kept in the chart , and when the root symbol has been generated for all theories , the top ranked one would win . In order to speed things up , Collins employs three different types of pruning .", "The first form of pruning is to use a beam The chart memoizes the highest scoring theory in each span , and if a proposed chart item for that span is not within a certain factor of the top scoring item , it is not added to the chart .", "Collins reports in his thesis that he uses a beam width of 105 .", "As it happens , the beam width for his thesis experiments was 104 .", "Interestingly , there is a negligible difference in overall parsing accuracy when this wider beam is used see Table 5 .", "An interesting modification to the standard beam in Collins parser is that for chart items representing NP or NP A derivations with more than one child , the beam is expanded to be 104 e3 .", "We suspect that Collins made this modification after he added the base NP model , to handle the greater perplexity associated with NPs .", "The second form of pruning employed is a comma constraint .", "Collins observed that in the Penn Treebank data , 96 of the time , when a constituent contained a comma , the word immediately following the end of the constituent s span was either a comma or the end of the sentence .", "So for speed reasons , the decoder rejects all theories that would generate constituents that violate this comma constraint . 31 There is a subtlety to Collins implementation of this form of pruning , however .", "Commas are quite common within parenthetical phrases .", "Accordingly , if a comma in an input Overall parsing results using only details found in Collins 1997 , 1999 .", "The first two lines show the results of Collins parser and those of our parser in its complete emulation mode i . e . , including unpublished details .", "All reported scores are for sentences of length 40 words .", "LR labeled recall and LP labeled precision are the primary scoring metrics .", "CBs is the number of crossing brackets .", "0 CBs and 2 CBs are the percentages of sentences with 0 and 2 crossing brackets , respectively .", "F the F measure is the evenly weighted harmonic mean of precision and recall , or 1 LP LR sentence occurs after an opening parenthesis and before a closing parenthesis or the end of the sentence , it is not considered a comma for the purposes of the comma constraint .", "Another subtlety is that the comma constraint should effectively not be employed when pursuing theories of an NPB subtree .", "As it turns out , using the comma constraint also affects accuracy , as shown in Section 8 . 1 .", "The final form of pruning employed is rather subtle Within each cell of the chart that contains items covering some span of the sentence , Collins parser uses buckets of items that share the same root nonterminal label for their respective derivations .", "Only 100 of the top scoring items covering the same span with the same nonterminal label are kept in a particular bucket , meaning that if a new item is proposed and there are already 100 items covering the same span with the same label in the chart , then it will be compared to the lowest scoring item in the bucket .", "If it has a higher score , it will be added to the bucket and the lowest scoring item will be removed ; otherwise , it will not be added .", "Apparently , this type of pruning has little effect , and so we have not duplicated it in our engine . 32 When the parser encounters an unknown word , the first best tag delivered by Ratnaparkhi s 1996 tagger is used .", "As it happens , the tag dictionary built up when training contains entries for every word observed , even low frequency words .", "This means that during decoding , the output of the tagger is used only for those words that are truly unknown , that is , that were never observed in training .", "For all other words , the chart is seeded with a separate item for each tag observed with that word in training .", "In this section we present the results of effectively doing a clean room implementation of Collins parsing model , that is , using only information available in Collins 1997 , 1999 , as shown in Table 4 .", "The clean room model has a 10 . 6 increase in F measure error compared to Collins parser and an 11 . 0 increase in F measure error compared to our engine in its complete emulation of Collins Model 2 .", "This is comparable to the increase in error seen when removing such published features as the verb intervening component of the distance metric , which results in an F measure error increase of 9 . 86 , or the subcat feature , which results in a 7 . 62 increase in F measure error . 33 Therefore , while the collection of unpublished details presented in Sections 4 7 is disparate , in toto those details are every bit as important to overall parsing performance as certain of the published features .", "This does not mean that all the details are equally important .", "Table 5 shows the effect on overall parsing performance of independently removing or changing certain of the more than 30 unpublished details . 34 Often , the detrimental effect of a particular change is quite insignificant , even by the standards of the performance obsessed world of statistical parsing , and occasionally , the effect of a change is not even detrimental at all .", "That is why we do not claim the importance of any single unpublished detail , but rather that of their totality , given that several of the unpublished details are , most likely , interacting .", "However , we note that certain individual details , such as the universal p w It model , do appear to have a much more marked effect on overall parsing accuracy than others .", "The previous section accounts for the noticeable effects of all the unpublished details of Collins model .", "But what of the details that were published ?", "In chapter 8 of his thesis , Collins gives an account on the motivation of various features of his model , including the distance metric , the model s use of subcats and their interaction with the distance metric , and structural versus semantic preferences .", "In the discussion of this last issue , Collins points to the fact that structural preferences which , in his model , are 33 These F measures and the differences between them were calculated from experiments presented in Collins 1999 , page 201 ; these experiments , unlike those on which our reported numbers are based , were on all sentences , not just those of length 40 words .", "As Collins notes , removing both the distance metric and subcat features results in a gigantic drop in performance , since without both of these features , the model has no way to encode the fact that flatter structures should be avoided in several crucial cases , such as for PPs , which tend to prefer one argument to the right of their head children .", "34 As a reviewer pointed out , the use of the comma constraint is a published detail .", "However , the specifics of how certain commas do not apply to the constraint is an unpublished detail , as mentioned in Section 7 . 2 .", "Number of times our parsing engine was able to deliver a probability for the various levels of back off of the modifier word generation model , PMw , when testing on Section 00 , having trained on Sections 02 21 .", "In other words , this table reports how often a context in the back off chain of PMw that was needed during decoding was observed in training . modeled primarily by the PL and PR parameters often provide the right information for disambiguating competing analyses , but that these structural preferences may be overridden by semantic preferences .", "Bilexical statistics Eisner 1996 , as represented by the maximal context of the PLw and PRw parameters , serve as a proxy for such semantic preferences , where the actual modifier word as opposed to , say , merely its part of speech indicates the particular semantics of its head .", "Indeed , such bilexical statistics were widely assumed for some time to be a source of great discriminative power for several different parsing models , including that of Collins .", "However , Gildea 2001 reimplemented Collins Model 1 essentially Model 2 but without subcats and altered the PLw and PRw parameters so that they no longer had the top level of context that included the headword he removed back off level 0 , as depicted in Table 1 .", "In other words , Gildea removed all bilexical statistics from the overall model .", "Surprisingly , this resulted in only a 0 . 45 absolute reduction in F measure 3 . 3 relative increase in error .", "Unfortunately , this result was not entirely conclusive , in that Gildea was able to reimplement Collins baseline model only partially , and the performance of his partial reimplementation was not quite as good as that of Collins parser . 35 Training on Sections 02 21 , we have duplicated Gildea s bigram removal experiment , except that our chosen test set is Section 00 instead of Section 23 and our chosen model is the more widely used Model 2 .", "Using the mode that most closely emulates Collins Model 2 , with bigrams , our engine obtains a recall of 89 . 89 and a precision of 90 . 14 on sentences of length 40 words see Table 8 , Model Mtw , tw .", "Without bigrams , performance drops only to 89 . 49 on recall , 89 . 95 on precision an exceedingly small drop in performance see Table 8 , Model Mtw , t .", "In an additional experiment , we have examined the number of times that the parser is able , while decoding Section 00 , to deliver a requested probability for the modifier word generation model using the increasingly less specific contexts of the three back off levels .", "The results are presented in Table 6 .", "Back off level 0 indicates the use of the full history context , which contains the head child s headword .", "Note that probabilities making use of this full context , that is , making use of bilexical dependencies , are available only 1 . 49 of the time .", "Combined with the results from the previous experiment , this suggests rather convincingly that such statistics are far less significant than once thought to the overall discriminative power of Collins models , confirming Gildea s result for Model 2 . 36 If not bilexical statistics , then surely , one might think , head choice is critical to the performance of a head driven lexicalized statistical parsing model .", "Partly to this end , in Chiang and Bikel 2002 , we explored methods for recovering latent information in treebanks .", "The second half of that paper focused on a use of the Inside Outside algorithm to reestimate the parameters of a model defined over an augmented tree space , where the observed data were considered to be the gold standard labeled bracketings found in the treebank , and the hidden data were considered to be the headlexicalizations , one of the most notable tree augmentations performed by modern statistical parsers .", "These expectation maximization EM experiments were motivated by the desire to overcome the limitations imposed by the heuristics that have been heretofore used to perform head lexicalization in treebanks .", "In particular , it appeared that the head rules used in Collins parser had been tweaked specifically for the English Penn Treebank .", "Using EM would mean that very little effort would need to be spent on developing head rules , since EM could take an initial model that used simple heuristics and optimize it appropriately to maximize the likelihood of the unlexicalized observed training trees .", "To test this , we performed experiments with an initial model trained using an extremely simplified head rule set in which all rules were of the form if the parent is X , then choose the left rightmost child . A surprising side result was that even with this simplified set of head rules , overall parsing performance still remained quite high .", "Using our simplified head rule set for English , our engine in its Model 2 emulation mode achieved a recall of 88 . 55 and a precision of 88 . 80 for sentences of length 40 words in Section 00 see Table 7 .", "So contrary to our expectations , the lack of careful head choice is not crippling in allowing the parser to disambiguate competing theories and is a further indication that semantic preferences , as represented by conditioning on a headword , rarely override structural ones .", "Given that bilexical dependencies are almost never used and have a surprisingly small effect on overall parsing performance , and given that the choice of head is not terribly critical either , one might wonder what power , if any , head lexicalization is providing .", "The answer is that even when one removes bilexical dependencies from the model , there are still plenty of lexico structural dependencies , that is , structures being generated conditioning on headwords and headwords being generated conditioning on structures .", "To test the effect of such lexicostructural dependencies in our lexicalized PCFGstyle formalism , we experimented with the removal of the head tag th and or the head word wh from the conditioning contexts of the PMw and PM parameters .", "The recertainly points to the utility of caching probabilities the 219 million are tokens , not types .", "Parsing performance with various models on Section 00 of the Penn Treebank .", "PM is the parameter class for generating partially lexicalized modifying nonterminals a nonterminal label and part of speech .", "PMw is the parameter class that generates the headword of a modifying nonterminal .", "Together , PM and PMw generate a fully lexicalized modifying nonterminal .", "The check marks indicate the inclusion of the headword wh and its part of speech th of the lexicalized head nonterminal H th , wh in the conditioning contexts of PM and PMw .", "See Table 4 for definitions of the remaining column headings . sults are shown in Table 8 .", "Model Mtw , tw shows our baseline , and Model M\u03c6 , \u03c6 shows the effect of removing all dependence on the headword and its part of speech , with the other models illustrating varying degrees of removing elements from the two parameter classes conditioning contexts .", "Notably , including the headword wh in or removing it from the PM contexts appears to have a significant effect on overall performance , as shown by moving from Model Mtw , t to Model Mt , t and from Model Mtw , \u03c6 to Model Mt , \u03c6 .", "This reinforces the notion that particular headwords have structural preferences , so that making the PM parameters dependent on headwords would capture such preferences .", "As for effects involving dependence on the head tag th , observe that moving from Model Mtw , t to Model Mtw , \u03c6 results in a small drop in both recall and precision , whereas making an analogous move from Model Mt , t to Model Mt , \u03c6 results in a drop in recall , but a slight gain in precision the two moves are analogous in that in both cases , th is dropped from the context of PMw .", "It is not evident why these two moves do not produce similar performance losses , but in both cases , the performance drops are small relative to those observed when eliminating wh from the conditioning contexts , indicating that headwords matter far more than parts of speech for determining structural preferences , as one would expect .", "We have documented what we believe is the complete set of heretofore unpublished details Collins used in his parser , such that , along with Collins 1999 thesis , thi s article contains all information necessary to duplicate Collins benchmark results .", "Indeed , these as yet unpublished details account for an 11 relative increase in error from an implementation including all details to a clean room implementation of Collins model .", "We have also shown a cleaner and equally well performing method for the handling of punctuation and conjunction , and we have revealed certain other probabilistic oddities about Collins parser .", "We have not only analyzed the effect of the unpublished details but also reanalyzed the effect of certain well known details , revealing that bilexical dependencies are barely used by the model and that head choice is not nearly as important to overall parsing performance as once thought .", "Finally , we have performed experiments that show that the true discriminative power of lexicalization appears to lie in the fact that unlexicalized syntactic structures are generated conditioning on the headword and head tag .", "These results regarding the lack of reliance on bilexical statistics suggest that generative models still have room for improvement through the employment of bilexical class statistics , that is , dependencies among head modifier word classes , where such classes may be defined by , say , WordNet synsets .", "Such dependencies might finally be able to capture the semantic preferences that were thought to be captured by standard bilexical statistics , as well as to alleviate the sparse data problems associated with standard bilexical statistics .", "This is the subject of our current research .", "This section contains tables for all parameter classes in Collins Model 3 , with appropriate modifications and additions from the tables presented in Collins thesis .", "The notation is that used throughout this article .", "In particular , for notational brevity we use M w , t i to refer to the three items Mi , tMi , and wMi that constitute some fully lexicalized modifying nonterminal and similarly M t i to refer to the two items Mi and tMi that constitute some partially lexicalized modifying nonterminal .", "The unlexicalized nonterminal mapping functions alpha and gamma are defined in Section 6 . 1 .", "As a shorthand , y M t i y Mi , tMi .", "The head generation parameter class , PH , gap generation parameter class , PG , and subcat generation parameter classes , PsubcatL and PsubcatR , have back off structures as follows The two parameter classes for generating modifying nonterminals that are not dominated by a base NP , PM and PMw , have the following back off structures .", "Recall that back off level 2 of the PMw parameters includes words that are the heads of the observed roots of sentences that is , the headword of the entire sentence .", "The two parameter classes for generating modifying nonterminals that are children of base NPs NPB nodes , PM , NPB and PMw , NPB , have the following back off structures .", "Back off level 2 of the PMw , NPB parameters includes words that are the heads of the observed roots of sentences that is , the headword of the entire sentence .", "Also , note that there is no coord flag , as coordinating conjunctions are generated in the same way as regular modifying nonterminals when they are dominated by NPB .", "Finally , we define M0 H , that is , the head nonterminal label of the base NP that was generated using a PH parameter .", "The two parameter classes for generating punctuation and coordinating conjunctions , Ppunc and Pcoord , have the following back off structures Collins , personal communication , October 2001 , where 2 type ttype The parameter classes for generating fully lexicalized root nonterminals given the hidden root TOP , PTOP and PTOPw , have the following back off structures identical to Table 3 ; n a not applicable .", "The parameter classes for generating prior probabilities on lexicalized nonterminals M w , t , Ppriorw and PpriorNT , have the following back off structures , where prior is a dummy variable to indicate that Ppriorwis not smoothed although the Ppriorw parameters still have an associated smoothing weight ; see note 27 .", "I would especially like to thank Mike Collins for his invaluable assistance and great generosity while I was replicating his thesis results and for his comments on a prerelease draft of this article .", "Many thanks to David Chiang and Dan Gildea for the many valuable discussions during the course of this work .", "Also , thanks to the anonymous reviewers for their helpful and astute observations .", "Finally , thanks to my Ph . D . advisor Mitch Marcus , who during the course of this work was , as ever , a source of keen insight and unbridled optimism .", "This work was supported in part by NSF grant no .", "SBR 89 20239 and DARPA grant no .", "N66001 00 1 8915 ."], "summary_lines": ["Intricacies Of Collins Parsing Model\n", "This article documents a large set of heretofore unpublished details Collins used in his parser, such that, along with Collins\u2019 (1999) thesis, this article contains all information necessary to duplicate Collins\u2019 benchmark results.\n", "Indeed, these as-yet-unpublished details account for an 11% relative increase in error from an implementation including all details to a clean-room implementation of Collins\u2019 model.\n", "We also show a cleaner and equally well-performing method for the handling of punctuation and conjunction and reveal certain other probabilistic oddities about Collins\u2019 parser.\n", "We not only analyze the effect of the unpublished details, but also reanalyze the effect of certain well-known details, revealing that bilexical dependencies are barely used by the model and that head choice is not nearly as important to overall parsing performance as once thought.\n", "Finally, we perform experiments that show that the true discriminative power of lexicalization appears to lie in the fact that unlexicalized syntactic structures are generated conditioning on the headword and its part of speech.\n", "The results suggest that the power of Collins-style parsing models did not lie primarily with the use of bilexical dependencies as was once thought, but in lexico-structural dependencies, that is, predicting syntactic structures conditioning on head words.\n", "We show that bilexical-information is used in only 1.49% of the decisions in Collins' Model-2 parser, and that removing this information results in an exceedingly small drop in performance.\n"]}
{"article_lines": ["Phrase Dependency Parsing for Opinion Mining", "In this paper , we present a novel approach for mining opinions from product reviews , where it converts opinion mining task to identify product features , expressions of opinions and relations between them .", "By taking advantage of the observation that a lot of product features are phrases , a concept of phrase dependency parsing is introduced , which extends traditional dependency parsing to phrase level .", "This concept is then implemented for extracting relations between product features and expressions of opinions .", "Experimental evaluations show that the mining task can benefit from phrase dependency parsing .", "As millions of users contribute rich information to the Internet everyday , an enormous number of product reviews are freely written in blog pages , Web forums and other consumer generated mediums CGMs .", "This vast richness of content becomes increasingly important information source for collecting and tracking customer opinions .", "Retrieving this information and analyzing this content are impossible tasks if they were to be manually done .", "However , advances in machine learning and natural language processing present us with a unique opportunity to automate the decoding of consumers opinions from online reviews .", "Previous works on mining opinions can be divided into two directions sentiment classification and sentiment related information extraction .", "The former is a task of identifying positive and negative sentiments from a text which can be a passage , a sentence , a phrase and even a word Somasundaran et al . , 2008 ; Pang et al . , 2002 ; Dave et al . , 2003 ; Kim and Hovy , 2004 ; Takamura et al . , 2005 .", "The latter focuses on extracting the elements composing a sentiment text .", "The elements include source of opinions who expresses an opinion Choi et al . , 2005 ; target of opinions which is a receptor of an opinion Popescu and Etzioni , 2005 ; opinion expression which delivers an opinion Wilson et al . , 2005b .", "Some researchers refer this information extraction task as opinion extraction or opinion mining .", "Comparing with the former one , opinion mining usually produces richer information .", "In this paper , we define an opinion unit as a triple consisting of a product feature , an expression of opinion , and an emotional attitude positive or negative .", "We use this definition as the basis for our opinion mining task .", "Since a product review may refer more than one product feature and express different opinions on each of them , the relation extraction is an important subtask of opinion mining .", "Consider the following sentences and its size 4 cannot be beat 4 .", "The phrases underlined are the product features , marked with square brackets are opinion expressions .", "Product features and opinion expressions with identical superscript compose a relation .", "For the first sentence , an opinion relation exists between the Canon SD500 and recommend , but not between picture and recommend .", "The example shows that more than one relation may appear in a sentence , and the correct relations are not simple Cartesian product of opinion expressions and product features .", "Simple inspection of the data reveals that product features usually contain more than one word , such as LCD screen , image color , Canon PowerShot SD500 , and so on .", "An incomplete product feature will confuse the successive analysis .", "For example , in passage Image color is disappointed , the negative sentiment becomes obscure if only image or color is picked out .", "Since a product feature could not be represented by a single word , dependency parsing might not be the best approach here unfortunately , which provides dependency relations only between words .", "Previous works on relation extraction usually use the head word to represent the whole phrase and extract features from the word level dependency tree .", "This solution is problematic because the information provided by the phrase itself can not be used by this kind of methods .", "And , experimental results show that relation extraction task can benefit from dependencies within a phrase .", "To solve this issue , we introduce the concept of phrase dependency parsing and propose an approach to construct it .", "Phrase dependency parsing segments an input sentence into phrases and links segments with directed arcs .", "The parsing focuses on the phrases and the relations between them , rather than on the single words inside each phrase .", "Because phrase dependency parsing naturally divides the dependencies into local and global , a novel tree kernel method has also been proposed .", "The remaining parts of this paper are organized as follows In Section 2 we discuss our phrase dependency parsing and our approach .", "In Section 3 , experiments are given to show the improvements .", "In Section 4 , we present related work and Section 5 concludes the paper .", "Fig .", "1 gives the architecture overview for our approach , which performs the opinion mining task in three main steps 1 constructing phrase dependency tree from results of chunking and dependency parsing ; 2 extracting candidate product features and candidate opinion expressions ; 3 extracting relations between product features and opinion expressions .", "Dependency grammar is a kind of syntactic theories presented by Lucien Tesni ere 1959 .", "In dependency grammar , structure is determined by the relation between a head and its dependents .", "In general , the dependent is a modifier or complement ; the head plays a more important role in determining the behaviors of the pair .", "Therefore , criteria of how to establish dependency relations and how to distinguish the head and dependent in such relations is central problem for dependency grammar .", "Fig .", "2 a shows the dependency representation of an example sentence .", "The root of the sentence is enjoyed .", "There are seven pairs of dependency relationships , depicted by seven arcs from heads to dependents .", "Currently , the mainstream of dependency parsing is conducted on lexical elements relations are built between single words .", "A major information loss of this word level dependency tree compared with constituent tree is that it doesn t explicitly provide local structures and syntactic categories i . e .", "NP , VP labels of phrases Xia and Palmer , 2001 .", "On the other hand , dependency tree provides connections between distant words , which are useful in extracting long distance relations .", "Therefore , compromising between the two , we extend the dependency tree node with phrases .", "That implies a noun phrase Cannon SD500 PowerShot can be a dependent that modifies a verb phrase head really enjoy using with relation type dobj .", "The feasibility behind is that a phrase is a syntactic unit regardless of the length or syntactic category Santorini and Kroch , 2007 , and it is acceptable to substitute a single word by a phrase with same syntactic category in a sentence .", "Formally , we define the dependency parsing with phrase nodes as phrase dependency parsing .", "A dependency relationship which is an asymmetric binary relationship holds between two phrases .", "One is called head , which is the central phrase in the relation .", "The other phrase is called dependent , which modifies the head .", "A label representing the relation type is assigned to each dependency relationship , such as subj subject , obj object , and so on .", "Fig . 2 c shows an example of phrase dependency parsing result .", "By comparing the phrase dependency tree and the word level dependency tree in Fig . 2 , the former delivers a more succinct tree structure .", "Local words in same phrase are compacted into a single node .", "These words provide local syntactic and semantic effects which enrich the phrase they belong to .", "But they should have limited influences on the global tree topology , especially in applications which emphasis the whole tree structures , such as tree kernels .", "Pruning away local dependency relations by additional phrase structure information , phrase dependency parsing accelerates following processing of opinion relation extraction .", "To construct phrase dependency tree , we propose a method which combines results from an existing shallow parser and a lexical dependency parser .", "A phrase dependency tree is defined as T V , E , where V is the set of phrases , E is the dependency relations among the phrases in V representing by direct edges .", "To reserve the word level dependencies inside a phrase , we define a nested structure for a phrase Ti in V Ti U , Ei .", "Vi v1 , v2 , , vm is the internal words , Ei is the internal dependency relations .", "We conduct the phrase dependency parsing in this way traverses word level dependency tree in preorder visits root node first , then traverses the children recursively .", "When visits a node R , searches in its children and finds the node set D which are in the same phrase with R according Algorithm 1 Pseudo Code for constructing the phrase dependency tree INPUT OUTPUT phrase dependency tree T V , E where to the shallow parsing result .", "Compacts D and R into a single node .", "Then traverses all the remaining children in the same way .", "The algorithm is shown in Alg .", "The output of the algorithm is still a tree , for we only cut edges which are compacted into a phrase , the connectivity is keeped .", "Note that there will be inevitable disagrees between shallow parser and lexical dependency parser , the algorithm implies that we simply follow the result of the latter one the phrases from shallow parser will not appear in the final result if they cannot be found in the procedure .", "Consider the following example Fig . 2 shows the procedure of phrase dependency parsing .", "Fig . 2 a is the result of the lexical dependency parser .", "Shallow parsers result is shown in Fig . 2 b .", "Chunk phrases NP We , VP really enjoyed using and NP the Canon PowerShot SD500 are nodes in the output phrase dependency tree .", "When visiting node enjoyed in Fig . 2 a , the shallow parser tells that really and using which are children of enjoy are in the same phrase with their parent , then the three nodes are packed .", "The final phrase dependency parsing tree is shown in the Fig .", "2 c .", "In this work , we define that product features are products , product parts , properties of products , properties of parts , company names and related objects .", "For example , in consumer electronic domain , Canon PowerShot , image quality , camera , laptop are all product features .", "From analyzing the labeled corpus , we observe that more than 98 of product features are in a single phrase , which is either noun phrase NP or verb phrase VP .", "Based on it , all NPs and VPs are selected as candidate product features .", "While prepositional phrases PPs and adjectival phrases ADJPs are excluded .", "Although it can cover nearly all the true product features , the precision is relatively low .", "The large amount of noise candidates may confuse the relation extraction classifier .", "To shrink the size of candidate set , we introduce language model by an intuition that the more likely a phrase to be a product feature , the more closely it related to the product review .", "In practice , for a certain domain of product reviews , a language model is build on easily acquired unlabeled data .", "Each candidate NP or VP chunk in the output of shallow parser is scored by the model , and cut off if its score is less than a threshold .", "Opinion expressions are spans of text that express a comment or attitude of the opinion holder , which are usually evaluative or subjective phrases .", "We also analyze the labeled corpus for opinion expressions and observe that many opinion expressions are used in multiple domains , which is identical with the conclusion presented by Kobayashi et al . 2007 .", "They collected 5 , 550 opinion expressions from various sources .", "The coverage of the dictionary is high in multiple domains .", "Motivated by those observations , we use a dictionary which contains 8221 opinion expressions to select candidates Wilson et al . , 2005b .", "An assumption we use to filter candidate opinion expressions is that opinion expressions tend to appear closely with product features , which is also used to extract product features by Hu and Liu 2004 .", "In our experiments , the tree distance between product feature and opinion expression in a relation should be less than 5 in the phrase dependency parsing tree .", "This section describes our method on extracting relations between opinion expressions and product features using phrase dependency tree .", "Manually built patterns were used in previous works which have an obvious drawback that those patterns can hardly cover all possible situations .", "By taking advantage of the kernel methods which can search a feature space much larger than that could be represented by a feature extraction based approach , we define a new tree kernel over phrase dependency trees and incorporate this kernel within an SVM to extract relations between opinion expressions and product features .", "The potential relation set consists of the all combinations between candidate product features and candidate opinion expressions in a sentence .", "Given a phrase dependency parsing tree , we choose the subtree rooted at the lowest common parent LCP of opinion expression and product feature to represent the relation .", "Dependency tree kernels has been proposed by Culotta and Sorensen , 2004 .", "Their kernel is defined on lexical dependency tree by the convolution of similarities between all possible subtrees .", "However , if the convolution containing too many irrelevant subtrees , over fitting may occur and decreases the performance of the classifier .", "In phrase dependency tree , local words in a same phrase are compacted , therefore it provides a way to treat local dependencies and global dependencies differently Fig .", "As a consequence , these two kinds of dependencies will not disturb each other in measuring similarity .", "Later experiments prove the validity of this statement .", "We generalize the definition by Culotta and Sorensen , 2004 to fit the phrase dependency tree .", "Use the symbols in Section 2 . 1 . 2 , 9 i and 9j are two trees with root Ri and Rj , K 9 i , Jj is the kernel function for them .", "Firstly , each tree node Tk E 9i is augmented with a set of features F , and an instance of F for Tk is Fk fk .", "A match function m Ti , Tj is defined on comparing a subset of nodes features M C_ F . And in the same way , a similarity function s Ti , Tj are defined on 5 C F where i 1 if fis fs C fs , fl 0 otherwise 3 For the given phrase dependency parsing trees , the kernel function K 9i , 9j is defined as folKc is the kernel function over Ri and Rj s children .", "Denote a is a continuous subsequence of indices a , a 1 , a l a for Ri s children where l a is its length , as is the s th element in a .", "And likewise b for Rj . where the constant 0 A 1 normalizes the effects of children subsequences length .", "Compared with the definitions in Culotta and Sorensen , 2004 , we add term Kin to handle the internal nodes of a pharse , and make this extension still satisfy the kernel function requirements composition of kernels is still a kernel Joachims et al . , 2001 .", "The consideration is that the local words should have limited effects on whole tree structures .", "So the kernel is defined on external children Kc and internal nodes Kin separately , annotator extracted 3595 relations , while the other annotator A2 extracted 3745 relations , an A1 d 3217 cases of them matched .", "In order to measure the annotation quality , we use the following metric to measure the inter annotator agreement , which is also used by Wiebe et al . 2005 . as the result , the local words are not involved in In this section , we describe the annotated corpus and experiment configurations including baseline We conducted experiments with labeled corpus which are selected from Hu and Liu 2004 , Jindal and Liu 2008 have built .", "Their documents are collected from Amazon . com and CNet . com , where products have a large number of reviews .", "They also manually labeled product features and polarity orientations .", "Our corpus is selected from them , which contains customer reviews of 11 products belong to 5 categories Diaper , Cell Phone , Digital Camera , DVD Player , and MP3 Player .", "Table 1 gives the detail statistics .", "Since we need to evaluate not only the product features but also the opinion expressions and relations between them , we asked two annotators to annotate them independently .", "The annotators started from identifying product features .", "Then for each product feature , they annotated the opinion expression which has relation with it .", "Finally , one Features for match function where agr a b represents the inter annotator agreement between annotator a and b , A and B are the sets of anchors annotated by annotators a and b . agr A1 A2 was 85 . 9 and agr A2 A1 was 89 . 5 .", "It indicates that the reliability of our annotated corpus is satisfactory .", "Results of extracting product features and opinion expressions are shown in Table 2 .", "We use precision , recall and F measure to evaluate performances .", "The candidate product features are extracted by the method described in Section 2 . 2 , whose result is in the first row .", "6760 of 24414 candidate product features remained after the filtering , which means we cut 72 of irrelevant candidates with a cost of 14 . 5 1 85 . 5 loss in true answers .", "Similar to the product feature extraction , the precision of extracting opinion expression is relatively low , while the recall is 75 . 2 .", "Since both product features and opinion expressions extractions are preprocessing steps , recall is more important .", "In order to compare with state of the art results , we also evaluated the following methods .", "Table 5 shows the performances of different relation extraction methods with in domain data .", "For each domain , we conducted 5 fold cross validation .", "Table 6 shows the performances of the extraction methods on cross domain data .", "We use the digital camera and cell phone domain as training set .", "The other domains are used as testing set .", "Table 5 presents different methods results in five domains .", "We observe that the three learning based methods SVM 1 , SVM WTree , SVM PTree perform better than the Adjacent baseline in the first three domains .", "However , in other domains , directly adjacent method is better than the learning based methods .", "The main difference between the first three domains and the last two domains is the size of data Table 1 .", "It implies that the simple Adjacent method is also competent when the training set is small .", "A further inspection into the result of first 3 domains , we can also conclude that 1 Tree kernels SVM WTree and SVM PTree are better than Adjacent , SVM 1 and SVM 2 in all domains .", "It proofs that the dependency tree is important in the opinion relation extraction .", "The reason for that is a connection between an opinion and its target can be discovered with various syntactic structures .", "2 The kernel defined on phrase dependency tree SVM PTree outperforms kernel defined on word level dependency tree SVMWTree by 4 . 8 in average .", "We believe the main reason is that phrase dependency tree provides a more succinct tree structure , and the separative treatment of local dependencies and global dependencies in kernel computation can indeed improve the performance of relation extraction .", "To analysis the results of preprocessing steps influences on the following relation extraction , we provide 2 additional experiments which the product features and opinion expressions are all correctly extracted respectively OERight and PFRight .", "These two results show that given an exactly extraction of opinion expression and product feature , the results of opinion relation extraction will be much better .", "Further , opinion expressions are more influential which naturally means the opinion expressions are crucial in opinion relation extraction .", "For evaluations on cross domain , the Adjacent method doesn t need training data , its results are the same as the in domain experiments .", "Note in Table 3 and Table 4 , we don t use domain related features in SVM 1 , SVM WTree , SVMPTree , but SVM 2 s features are domain dependent .", "Since the cross domain training set is larger than the original one in Diaper and DVD domain , the models are trained more sufficiently .", "The final results on cross domain are even better than in domain experiments on SVM 1 , SVM WTree , and SVM PTree with percentage of 4 . 6 , 8 . 6 , 10 . 3 in average .", "And the cross domain training set is smaller than in domain in MP3 , but it also achieve competitive performance with the in domain .", "On the other hand , SVM 2 s result decreased compared with the in domain experiments because the test domain changed .", "At the same time , SVM PTree outperforms other methods which is similar in in domain experiments .", "Opinion mining has recently received considerable attention .", "Amount of works have been done on sentimental classification in different levels Zhang et al . , 2009 ; Somasundaran et al . , 2008 ; Pang et al . , 2002 ; Dave et al . , 2003 ; Kim and Hovy , 2004 ; Takamura et al . , 2005 .", "While we focus on extracting product features , opinion expressions and mining relations in this paper .", "Kobayashi et al . 2007 presented their work on extracting opinion units including opinion holder , subject , aspect and evaluation .", "Subject and aspect belong to product features , while evaluation is the opinion expression in our work .", "They converted the task to two kinds of relation extraction tasks and proposed a machine learning based method which combines contextual clues and statistical clues .", "Their experimental results showed that the model using contextual clues improved the performance .", "However since the contextual information in a domain is specific , the model got by their approach can not easily converted to other domains .", "Choi et al . 2006 used an integer linear programming approach to jointly extract entities and relations in the context of opinion oriented information extraction .", "They identified expressions of opinions , sources of opinions and the linking relation that exists between them .", "The sources of opinions denote to the person or entity that holds the opinion .", "Another area related to our work is opinion expressions identification Wilson et al . , 2005a ; Breck et al . , 2007 .", "They worked on identifying the words and phrases that express opinions in text .", "According to Wiebe et al . 2005 , there are two types of opinion expressions , direct subjective expressions and expressive subjective elements .", "In this paper , we described our work on mining opinions from unstructured documents .", "We focused on extracting relations between product features and opinion expressions .", "The novelties of our work included 1 we defined the phrase dependency parsing and proposed an approach to construct the phrase dependency trees ; 2 we proposed a new tree kernel function to model the phrase dependency trees .", "Experimental results show that our approach improved the performances of the mining task .", "This work was partially funded by Chinese NSF 60673038 , Doctoral Fund of Ministry of Education of China 200802460066 , and Shanghai Science and Technology Development Funds 08511500302 .", "The authors would like to thank the reviewers for their useful comments ."], "summary_lines": ["Phrase Dependency Parsing for Opinion Mining\n", "In this paper, we present a novel approach for mining opinions from product reviews, where it converts opinion mining task to identify product features, expressions of opinions and relations between them.\n", "By taking advantage of the observation that a lot of product features are phrases, a concept of phrase dependency parsing is introduced, which extends traditional dependency parsing to phrase level.\n", "This concept is then implemented for extracting relations between product features and expressions of opinions.\n", "Experimental evaluations show that the mining task can benefit from phrase dependency parsing.\n", "We utilize the dependency parser to extract the noun phrases and verb phrases from the reviews as the aspect candidates.\n", "For a monolingual task, we use a shallow parser to convert lexical dependencies from a dependency parser into phrase dependencies.\n"]}
{"article_lines": ["? ? ? ? ? ? D ? ? ? ? ? ? ? ? a ? D ? ? ? f ? ? ? ? ? ? ?", "? ? ? ? ? ? ? j ? flfiL ? ? ? ? ? ? ? S ? ? ? ? ? a ? ? ? ? ? ? 2 ? ? ?", "? A ? ? ? j ? ? ? 0 ? ? ? ?", "? ? ? v ? 3 ? ? ? Ofi ? ? 5 ? j ? H ? ? ? ? ?", "? TR ? ? ? v ? ? ? ? ? ? ? m ?", "? Tfi ? fi ? ? ? ? ? 4 ? ? ? v ? ? ? ? ? ?", "? j ? v ? j ? flfi ? ? ?", "? ? ? ? ? ? ? S ? ? ? ? ? a ? ? ? ? ? ? ?", "? fl ? j ? flfi ? ? 4 ? ? ? ? ?", "? j ? v ? j ? flfi ? ? ? ?", "ff ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? j ? ? ? ? ? j ? ? ? ? ? ? ? ? ? ? ?", "ff ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? a ? ? ?", "ff ? 3 ? ? ? ? ? ? ? S ? ? ? ? ? a ? ? ?", "? ? ? ? ? ? ? ? ? ? ? ? ? a ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? v ? a ? fi ?", "? ? ? ? ? 0 ? ? ? ? ? ? ? ? ? ffi ? ? ? ?", "a sentence to be translated He is the first international student b part of the clump dictionary c one possible segmentation He is the first international student d possible translations is est , the le , first premier is the first est le premier , he il international international .", "p ? ? ? H ? v ? ? ? U ? ? j ? ? v ? ? ?", "? ? ? ? ? ? ? S ? ? ? ? ? a ? ? ? ? ? ? ? ?", "? ? ? ? ? v ? ? ? ? ? ? ? ? ? L ? ? ?", "? ? ? v ? L ? ? ? ? ? ? ? ? ? ? ? ?", "? H ? ? ? v ? ? ? ? ? ? v ? j ? ? ? ?", "? j ? a ? fi ? j ? flfi ? ? ? ?", "? fl ? ? ? j ? ? ? S ? ? ? z ?", "? ? ? ? ? j ? fi ? ? ? v ? 4 ? ? ?", "ff ? j ? ? ? ? ? ? ? j ? ? ?", "ff ? ? ? ? ? a ? D ? ? ? ? ? O ? ? ?", "? N ? flfi ? ? j ? ? ? ? ?", "ff ? ? ff ? ? d ? ? ? ?", "? . d e g ? ? fi ? ? ?", "? j ? , ? v ? j ? ? S ?", "? 2 ? v ? v ? ? ? ? ? ? ? L ? fi ? v ? ? ? ?", "? v ? v ? ? ? ? ? ? ? S ; ? ? ? ?", "ff ? ? ? ? ? v ? a ? fi ?", "? Tfi ? v ? v ? ? ? ? ? ? ? ? ? ?", "X 7 ? 98 4 ; ? 8 ? 9ACBED X ACBED ?", "8 4 ACBED X 8FACBED ?", "ff ? ? ? ? ? v ? v ? j ? ? ?", "i 9 dfehg i ? v ?", "? H ? j ? flfi ff ? ff ? D ? ?", "? ? ? ? ? ffi ? ? ? ? ? ? 3 ? v ?", "p ? ? ? ? ? ? ? H ? v ? a ?", "? ? ? Cfi ? ? ? ? ? ? ? ? ? ? ?", "? ? ? ? ? ? ? ? ? j ? flfiL ? ? ? ? ? ?", "? ? ? ? ? v ? ? ? ? ? ? ? v ?", "1 ndet la t det sg 2 subj France noun propn sg 3 top is verb vpres sg 4 ndet the det sg 5 nadj first adj 6 pred country noun cn sg 7 nadj western adj ? la t France is the first country western ?", "? ? ? ? ? ? ? ? v ? ? ? ? j ? d ? j ? ? ? ? ? ? ? ? ? D ?", "? H ? j ? ? j ? ? ? ?", "? ? ? ? ? H ? U ? ? ? ? ? S ?", "? D ? ? ? ffi ? ? ? ? D ? 3 ? ? ? ff ? ? ?", "0 ? ? v ? ff ? ? f ? ? ? ? ? j ? ?", "? aAh t p g h ? b g r4djoWh g ?", "? ? ? ? ? ? ? ? ? v ? j ? flfis ? ?", "? ? ? ? ? ? ? ? ? ? ? ? ? v ?", "? v ? j2 Gffi ? c ? ff ? ?", "ff ? ? ? ? ? s ? ? ? ? ? ? ? ? ? v ? j ? flfi ? ? ? ?", "? v ? KP ? ? ? S ? ? ? ? ?", "? H ? ? v ? ? ? ? ? ? ? v ? ? ?", "? ? ? 0 ? ? ? j ? j ? a ? ? ? ? v ?", "? j ? ? ? ? v ? ? ? ? ? H ? ? ? v ? ? ? ? ? ? v ? j ? ? ? ? ?", "? ? ? ? ? ? ? ? ? ? ? ? f ?", "? ? ? ? ? ? ? ? ? ? ? f ?", "? j ? ff ? ? ? ? ? j ? flfi ? ? ? ? v ?", "? j ? ? ? ? ? 0 ? ? ? v ? ? ? ? H ? v ? ? ?", "? j ? flfi ff ? ? ? ? ? v ? v ? j ? ? ?"], "summary_lines": ["Improving A Statistical MT System With Automatically Learned Rewrite Patterns\n", "Current clump-based statistical MT systems have two limitations with respect to word ordering: First, they lack a mechanism for expressing and using generalization that accounts for reorderings of linguistic phrases.\n", "Second, the ordering of target words in such systems does not respect linguistic phrase boundaries.\n", "To address these limitations, we propose to use automatically learned rewrite patterns to preprocess the source sentences so that they have a word order similar to that of the target langauge.\n", "Our system is a hybrid one.\n", "The basic model is statistical, but we use broad-coverage rule-based parsers in two ways - during training for learning rewrite patterns, and at runtime for reordering the source sentences.\n", "Our experiments show 10% relative improvement in Bleu measure.\n", "We describe an approach for translation from French to English, where reordering rules are acquired automatically.\n", "Our re-ordering rules are automatically learned from aligning parse trees for both the source and target sentences.\n"]}
{"article_lines": ["Tense As Discourse Anaphor", "In this paper , I consider a range of English expressions and show that their context dependency can be characterized in terms of two properties 1 .", "They specify entities in an evolving model of the discourse that the listener is constructing ; 2 .", "The particular entity specified depends on another entity in that part of the evolving quot ; discourse model quot ; that the listener is currently attending to . expressions have been called show how tensed clauses share these characteristics , usually just attributed to anaphoric noun phrases .", "This not only allows us to capture in a simple way the but difficult to prove intuition that is anaphoric , also contributes to our knowledge of what is needed for understanding narrative text .", "Philadelphia , PA 19104 6389 In this paper , I consider a range of English expressions and show that their context dependency can be characterized in terms of two properties 1 .", "They specify entities in an evolving model of the discourse that the listener is constructing ; 2 .", "The particular entity specified depends on another entity in that part of the evolving quot ; discourse model quot ; that the listener is currently attending to .", "Such expressions have been called anaphors .", "I show how tensed clauses share these characteristics , usually just attributed to anaphoric noun phrases .", "This not only allows us to capture in a simple way the oft stated but difficult to prove intuition that tense is anaphoric , but also contributes to our knowledge of what is needed for understanding narrative text .", "In this paper , I consider a range of English expressions and show that their context dependency can be characterized in terms of two properties entity in that part of the evolving quot ; discourse model quot ; that the listener is currently attending to .", "Two types of expressions have previously been described in these terms definite pronouns and certain definite noun phrases NPs .", "Researchers in computational linguistics and in artificial intelligence have called these expressions anaphors cf . , Woods 1978 , Sidner 1983 , Bobrow 1977 , Hirst 1981 , Webber 1983 .", "Linguists , however , have used this term somewhat differently .", "Many have restricted its use to expressions usually pronouns that can be treated analogously to variables in a logical language Chomsky 1980 .", "A view in linguistics that comes somewhat closer to the Al model can be found in a paper by Sag and Hankamer 1984 , who distinguish what they call deep or modelinterpretive anaphora from what they call surface anaphora or ellipsis .", "Under the former , they include personal pronouns , sentential quot ; it , quot ; and null complement anaphora , and under the latter , verb phrase VP ellipsis , sluicing , gapping , and stripping .", "The two types are distinguished by whether they make reference to the interpretation of an antecedent i . e . , some object in a model of the world constructed by the interpreter of the sentence of discourse deep anaphora or whether they are interpreted with respect to a previous logical form surface anaphora .", "While their deep anaphors include pronouns , Hankamer and Sag do not consider other expressions like NPs in discourse that might also be described in similar model interpretive terms , nor do they describe in any detail how model interpretation works for the expressions they consider .", "To avoid confusion then , I will use the term discourse anaphors for expressions that have these two properties . '", "My main point will be that tensed clauses share these properties as well , and hence should also be considered discourse anaphors .", "This will capture in a simple way the oft stated , but difficult to prove intuition that tense is anaphoric .", "To begin with , in Section 2 , I characterize the dependency of an anaphoric expression Xb on a discourse entity Ea in terms of an anaphoric function a Xb , Ea , that itself depends on 1 . the ontology of the specified entity Ea and 2 . discourse structure and its focusing effect on which Ea entities the listener is attending to .", "With respect to definite pronouns and NPs , this will essentially be a review of previous research .", "However , I will argue that some indefinite NPs should also be considered discourse anaphors in just this same way .", "In Section 3 , I will move on to tensed clauses and the notion of tense as anaphor , a notion that goes back to at least Leech in his monograph Meaning and the English Verb 1987 .", "I will review previous attempts to make the notion precise , attempts that require special purpose machinery to get them to work .", "Then I will show , in contrast , that the notion can more simply be made precise in terms of a set of similar anaphoric functions that again depend on ontology and discourse structure .", "Making clear these dependencies contributes to our knowledge of what is needed for understanding narrative text .", "The notion specify that I am using in my definition of discourse anaphora is based on the notion of a Discourse Model , earlier described in Webber 1983 .", "My basic premise is that in processing a narrative text , a listener is developing a model of at least two things 1 . the entities under discussion , along with their properties and relationships to one another , and 2 . the events and situations under discussion , along with their relationships to one another e . g . , consequential relations , simple ordering relations , elaboration relations , etc . .", "The representation as a whole I call the listener's Discourse Model .", "2 In this section , I will focus on NPs .", "In Section 3 , I will turn attention to tensed clauses .", "NPs may evoke entities into the listener's Discourse Model corresponding to individuals Example 1 , sets Example 2 , abstract individuals Example 3 , classes Example 4 , etc .", "3 An NP which evokes a discourse entity also specifies it .", "One way an NP would be considered anaphoric by the above definition would be if it specified an entity Ea in the model that had already been evoked by some other NP .", "In that case , one would say that the two NPs co specified the same entity .", "This basic arrangement is illustrated in Examples 1 3 above and is shown in Figure la . 5 Formally , one could say that there is an anaphoric function a , whose value , given the anaphoric noun phrase NPb and the discourse entity Ea , is Ea thatis , a NPb , Ea Ea .", "This can also be read as NPb specifies Ea by virtue of Ea .", "Definite pronouns are most often anaphoric in just this way .", "The other way an NP would be considered a discourse anaphor would be if it used some existing discourse entity Ea to evoke and specify a new discourse entity Eb , as in where NPb the driver makes use of the entity associated with the bus mentioned in 5a to specify a new entity the driver of that bus .", "Here the anaphoric function is of the form a NPb , Ea Eb .", "In cooperative discourse , there have to be constraints on the value of a NPb , Ea , since only NPb is given explicitly .", "In short , a cooperative speaker must be able to assume that the listener is able to both infer a possible a and single out Ea in his her evolving Discourse Mode1 . 6 This is illustrated in Figure lb .", "I will consider each of these two types of constraints in turn .", "Speakers assume listeners will have no problem with a when a NPb , Ea Ea .", "Inferring a in other cases follows in large part from the ontology of the entities specified by NPs i . e . , the ontology of our concepts of individuals , sets , mass terms , generics , etc .", "We view these as having parts e . g . , car the engine , the wheels , having functional relations e . g . , car the driver , having roles e . g . , wedding the bride , etc .", "These needn't be necessary parts , relations , roles , etc .", "Our ontology includes possible parts , relations , etc . , and these too make it possible for the listener to infer an a such that a NPb , Ea Eb e . g . , room the chandelier ; car the chauffeur ; wedding the flower girl .", "Such inferences are discussed at length in the literature , including Clark and Marshall 1981 , and Hobbs 1987 . 7 Before closing this section , there are two more things to say about NPs .", "First , the above definition of discourse anaphor does not apply to all definite NPs a definite NP can be used to refer to something unique in the speaker and listener's shared spatio temporal context e . g . , the telephone i . e . , the one that they both hear ringing or their shared culture e . g . , the government , to the unique representative of a class e . g . , the duck billed platypus , to an entire class or set e . g . , the stars , or to a functionally defined entity e . g . , the largest tomato in Scotland .", "None of these would be considered discourse anaphoric by the above definition .", "Secondly , though the definition implies that one must consider some indefinite NPs to be discourse anaphors , since they are essentially parasitic on a corresponding anaphoric definite NP , as in the following example b .", "The driver stopped the bus when the passengers began to sing quot ; Aida quot ; .", "The indefinite NP a passenger in 6a can be paraphrased as some one of the passengers , and thus is parasitic on the anaphoric definite NP the passengers mentioned explicitly in 6b .", "This does not imply that all indefinite NPs are discourse anaphors .", "In Mary met a boy with green hair or Fred built an oak desk , the indefinite NPs do not need to be interpreted with respect to another discourse entity and some inferrable relationship with that entity , in order to characterize the discourse entity they specify .", "In the next section , I will discuss the second kind of constraint on the function a NPb , Ea necessary for cooperative use of an anaphor constraints on identifiable Eas .", "These involve notions of discourse structure and discourse focus .", "Before I close , though , I want to point to where I'm going vis a vis the anaphoric character of tense and tensed clauses .", "In contrast with previous accounts of tense as pronoun or tense as loosely context dependent , I am going to claim that , like an anaphoric definite NP , The ideas presented in this section have been formulated and developed by Barbara Grosz and Candy Sidner , originally independently and later in joint research .", "It is not a summary of their work 8 it is limited to those of their ideas that are necessary to the concept of anaphor that I am advancing here and the concept of tense as anaphor , in particular .", "Sidner's thesis 1979 , 1983 presents an account of understanding definite pronouns and anaphoric definite NPs that reflects the ease with which people identify the intended specificand of definite pronouns except in highly ambiguous cases , as well as the intended specificand of anaphoric definite NPs .", "With respect to noun phrases but not clauses , Sidner makes the same assumption about evoking , specifying , and co specifying in a Discourse Model that I have made here .", "To understand anaphoric expressions , Sidner postulates three mechanisms The DF corresponds to that entity the listener is most attending to .", "Pronouns can most easily specify the current DF , slightly less easily a member of the PFL , and with slightly more difficulty , a stacked focus .", "Specifying an entity pronominally can shift the listener's attention to it , thereby promoting it to be the next DF .", "Anything else specified in the clause ends up on the PFL , ordered by its original syntactic position .", "Sidner introduced a separate quot ; agent focus quot ; to allow two entities to be specified pronominally in the same clause , but it was not a critical feature of her approach .", "As for anaphoric definite NPs , they can specify anything previously introduced whether on the PFL , a stacked focus , or anything else or anything related in a mutually inferrable way with the current DF or a member of the PFL .", "In terms of the constraints I mentioned above , it is only those discourse entities that are either the DF or on the PFL that can serve as Ea for an anaphoric definite NP . 9 In Sidner 1983 DFs always are stacked for possible resumption later .", "In Grosz and Sidner 1986 it is an entire focus space FS Grosz 1977 that gets stacked i . e . , the collection of entities L is attending to by virtue of the current discourse segment DS but only when the 9purpose of the current DS is taken to dominate that of the one upcoming .", "Dominance relations are also specified further according to the type of discourse .", "In Grosz and Sidner , they are defined for task related dialogues and arguments .", "For example , in arguments , one DS purpose DSP dominates another if the second provides evidence for a point made in the first .", "When the dominated DSP is satisfied , its corresponding FS is popped .", "This stack mechanism models the listener's attentional state .", "The relations between DSPs constitute the intentional structure of the text .", "Getting a listener to resume a DS via the stack mechanism is taken to require less effort on a speaker's part than returning to elaborate an argument or subtask description later on .", "The significance of Sidner 1983 and Grosz and Sidner 1986 for the current enterprise is that Computational Linguistics , Volume 14 , Number 2 , June 1988 63 Bonnie Lynn Webber Tense as Discourse Anaphor I reinterpret this in the current framework in terms of the anaphoric function a NPb , Ea .", "Within a discourse segment , the entity that is the DF is the most likely Ea .", "Over the discourse segment , other discourse entities in the segment's focus space may in turn become DF .", "With a change in discourse segment , however , the DF can change radically to an entity in the focus space associated with the new segment .", "To hint again at what is to come in Section 3 . 2 , I will propose a temporal analogue of DF , which I have called temporal focus TF .", "In Section 3 . 3 , I will show how gradual movements of the TF are tied in with the ontology of what a tensed clause specifies i . e . , an ontology of events and situations while more radical movements reflect the effect of discourse structure on TF .", "Tense may not seem prima facie anaphoric an isolated sentence like John went to bed or I met a man who looked like a basset hound appears to make sense in a way that a stand alone He went to bed or The man went to bed does not .", "On the other hand , if some time or event is established by the context i . e . , either by an event or situation described in the previous discourse or by a temporal adverbial in the current sentence cf .", "Passonneau , and Moens and Steedman , this volume , tense will invariably be interpreted with respect to it , as in In each case , the interpretation of John's going to bed is linked to an explicitly mentioned time or event .", "This is what underlies all discussion of the anaphoric quality of tense .", "The assumption that tense is anaphoric i . e . , that its interpretation is linked to some time or event derived from context goes back many years , although it is not a universally held belief cf .", "Comrie 1985 .", "Leech seems to express this view in his Meaning and the English Verb 63 INDEFINITE TIME Whereas the Present Perfect , in its indefinite past sense , does not name a specific point of time , a definite POINT OF ORIENTATION in the past is normally required for the appropriate use of the Simple Past Tense .", "The point of orientation may be specified in one of three ways a by an adverbial express of timewhen ; b by a preceding use of a Past or Perfect Tense ; and c by implicit definition ; i . e . , by assumption of a particular time from context .", "73 The Past Perfect Tense has the meaning of pastin the past , or more accurately , 'a time further in the past , seen from the viewpoint of a definite point of time already in the past' .", "That is , like the Simple Past Tense , the Past Perfect demands an already established past point of reference .", "Leech 47 Leech did not elaborate further on how reference points are used in the interpretation of simple past tense and past perfect tense , or on what has become the main problem in the semantics and pragmatics of tense reconciling the usual forward movement of events in narratives with a belief in the anaphoric or contextdependent character of tense .", "The first explicit reference I have to tense being anaphoric like a definite pronoun is in an article by McCawley 1971 110 , who said However the tense morpheme does not just express the time relationship between the clause it is in and the next higher clause it also refers to the time of the clause that it is in , and indeed , refers to it in a way that is rather like the way in which personal pronouns refer to what they stand for .", "McCawley also tried to fit in his view of tense as pronoun with the interpretation of tense in simple narratives .", "Here he proposed that the event described in one clause serves as the antecedent of the event described in the next , but that it may be related to that event by being either at the same time or quot ; shortly after quot ; it .", "He did not elaborate on when one relation would be assumed and when the other .", "Partee 1973 also noted the similarities between tense and definite pronouns .", "However , she subsequently recognized that taking simple past tense as directly analogous with pronouns was incompatible with the usual forward movement of time in the interpretation in a sequence of sentences denoting events Partee 1984 .", "Her response was a modification of the claim that tense is anaphoric , saying I still believe it is reasonable to characterize tense as anaphoric , or more broadly as context dependent , but I would no longer suggest that this requires them to be viewed as 'referring' to times as pronouns 'refer' to entities , or to treat times as arguments of predicates 256 .", "The particular context dependent process she proposes for interpreting tensed clauses follows that of Hinrichs 1986 , briefly described below .", "The examples presented above to illustrate the anaphoric quality of tense were all simple past .", "However , as Leech notes see above , the past perfect also makes demands on having some reference point already estabBonnie Lynn Webber Tense as Discourse Anaphor lished in the context .", "Thus it cannot be in terms of the event described in a tensed clause that tense is anaphoric .", "Instead , several people Steedman 1982 , Hinrichs 1986 , Bauerle 1979 have argued that it is that part of tense called by Reichenbach 1947 the point of reference here abbreviated RT that is anaphoric .", "This can be seen by considering the following example 8 . a . John went to the hospital . b .", "He had twisted his ankle on a patch of ice .", "It is not the point of the event here abbreviated ET of John's twisting his ankle that is interpreted anaphorically with respect to his going to the hospital .", "Rather , it is the RT of the second clause its ET is interpreted as prior to that because the clause is in the past perfect see above .", "I will now review briefly Hinrichs's proposal as to how tensed clauses are interpreted in context , in order to contrast it with the current proposal .", "In Hinrichs 1986 , Hinrichs makes the simplifying assumption that in a sequence of simple past sentences , the temporal order of events described cannot contradict the order of the sentences .", "This allows him to focus on the problem of characterizing those circumstances in which the event described by one sentence follows that described by the previous one Example 9 Hinrichs's Example 15 and when it overlaps it Example 10 Hinrichs' s Example 21 9 .", "The elderly gentleman wrote out the check , tore it from the book , and handed it to Costain .", "Mr . Darby slapped his forehead , then collected himself and opened the door again .", "The brush man was smiling at him hesitantly .", "Hinrichs bases his account on the Aktionsart of a tensed clause i . e . , its Vendlerian classification as an accomplishment , achievement , activity , or state including progressives .", "Assuming an initial reference point in a discourse , the event described by a tensed clause interpreted as an accomplishment or achievement will be included in that reference point and will also introduce a new reference point ordered after the old one .", "Events associated with the other Aktionsarten include the current reference point in the event time .", "This means that given a sequence of two clauses interpreted as accomplishments or achievements , their corresponding events will follow one another cf .", "Example 9 .", "On the other hand , given a sequence with at least one tensed clause interpreted as an activity or state including progressive , their corresponding events will be interpreted as overlapping each other cf .", "Example 10 .", "Hinrichs relates his reference point to that of Reichenbach .", "Thus , the anaphoric character of tense is based on RT and not on the events directly .", "However , Hinrichs's notion and Reichenbach's differ with respect to the time of the event described in the tensed clause .", "While Reichenbach talks about ET and RT being the same for nonprogressive past tense clauses , in Hinrichs's account the reference point can fall after the event if a nonprogressive past is interpreted as an accomplishment or an achievement .", "This is necessary to achieve the forward movement of narrative that Hinrichs assumes is always the case his simplifying assumption but it is not the same as Reichenbach's RT .", "It also leads to problems in cases where this simplifying assumption is just wrong where in a sequence of simple past tenses , there is what appears to be a quot ; backward quot ; movement of time , as in 11 . a .", "For an encore , John played the quot ; Moonlight Sonata quot ; . b .", "The opening movement he took rather tentatively , but then .", ". where the second clause should be understood as describing the beginning of the playing event in more detail , not as describing a subsequent event .", "In the account given below , both forward and backward movement of time fall out of the anaphoric character of tensed clauses , and the dependency of discourse anaphora on discourse structure . quot ; With that background , I will now show how tensed clauses share the two properties I set out in Section 1 repeated here and hence are further examples of discourse anaphora To do this , I need to explain the sense in which tensed clauses specify and the way in which that specification can depend on another element in the current context .", "Recall that I presume that a listener's developing discourse model represents both the entities being discussed , along with their properties and relations , and the events and situations being discussed , along with their relationships with another .", "For the rest of this paper , I want to ignore the former and focus on the latter .", "This I will call event situation structure , or E S structure .", "It represents the listener's best effort at interpreting the speaker's ordering of those events and situations in time and space .", "One problem in text understanding , then , is that of establishing where in the evolving E S structure to integrate the event or situation description in the next clause .", "In this framework , a tensed clause Cb provides two pieces of semantic information a a description of an event or situation , and b a particular configuration of ET , RT , and point of speech abbreviated ST .", "Here I may be departing from Reichenbach in treating ET , RT , and ST explicitly as elements of linguistic semantics , quite distinct from entities of type quot ; event quot ; in the events in the model follows in part from Cb's particular configuration of ET , RT , and ST .", "Both the characteristics of Eb i . e . , its ontology and the configuration of ET , RT , and ST are critical to my account of tense as discourse anaphor .", "The event ontology I assume follows that of Moens and Steedman this volume and of Passonneau this volume .", "Both propose that people interpret events as having a tripartite structure a quot ; nucleus quot ; in Moens and Steedman's terminology consisting of a preparatory phase prep , a culmination cul , and a consequent phase conseq as in Figure 2 .", "This tripartite structure permits a uniform account to be given of aspectual types in English and of how the interpretation of temporal adverbials interacts with the interpretation of tense and aspect .", "For example , the coercion of clauses from one interpretation to another is defined in terms of which parts of a nucleus they select and how those parts are described . I2 The ET RT ST configuration is significant in that , like Steedman 1982 , Dowty 1986 , Hinrichs 1986 , and Partee 1984 , I take RT as the basis for anaphora .", "To indicate this , I single it out as an independent argument to anaphoric functions , here labelled 0 .", "In particular , the following schema holds of a clause Cb linked anaphorically to an event Ea through its RT The relationship between Eb and Ea then falls out as a consequence of 1 . the particular ET RT ST configuration of Cb ; and 2 . the particular function 0 involved .", "In this case , the relationship between Eb and E then depends on the configuration of RTb and ETb .", "If ETb RTb , then minimally Eb is taken to coincide in some way with Ea .", "This is shown in Figure 3a .", "If ETb RTb as in the perfect tenses , Eb is taken to precede Ea .", "This is shown in Figure 3d .", "Alternatively , 0 may embody part of the tripartite ontology of events mentioned earlier 13prep links RTb to the preparatory phase of Ea as shown in Figure 3b i . e .", "Pprep Cb , Ea , RTb Eb while B conseq links RTb to the consequent phase of Ea as shown in Figure 3c i . e .", "There is a third possibility that RTb links to the culmination of Ea but it is not clear to me that it could be distinguished from the simpler 00 function given above , which links RTb to Ea itself .", "Also , while 3prep and la conseq relations for RTb might theoretically be possible for a perfect , it is not clear to me that these cases could be distinguished from the simpler 0o .", "In the case of perfects therefore , the relation between Eb and Ea is correspondingly indirect . 13 The following example illustrates the case where 0 and ETb RTb .", "12 . a . John played the piano . b . Mary played the kazoo .", "Sentence 12a . evokes a new event entity Ea describable as the event of John playing the piano .", "Since the tense of 12b is simple past , ETb RTb .", "Given 130 Cb , Ea , RTb Eb , then Eb is interpreted as coextensive with Ea .", "Whether this is further interpreted as two simultaneous events or a single event of their playing a duet depends on context and , perhaps , world knowledge as well .", "This is illustrated in Figure 4 .", "Example 8 repeated here illustrates the case 30 where ETb RTb . be described as having bought some flowers .", "This is shown in Figure 7 .", "8 . a . John went to the hospital . b .", "He had twisted his ankle on a patch of ice .", "Clause 8a . evokes an entity Ea describable as John's going to the hospital .", "Since 8b is past perfect , ETb RTb .", "Thus if po Cb , Ea , RTb Eb , the event Eb described by 8b is taken to be prior to Ea .", "As Moens Steedman this volume point out , the consequences of an event described with a perfect tense are still assumed to hold .", "Hence the overlap shown in Figure 5 The next example illustrates , conseq 13 . a . John went into the florist shop . b .", "He picked out three red roses , two white ones and one pale pink .", "Clause 13a evokes an entity Ea describable as John's going into a flower shop .", "Since Clause 13b is simple past , ETb RTb .", "Thus given pconseq Cb , Ea , RTb Eb , event Eb is taken as being part of the consequent phase of Ea .", "That is , John's picking out the roses is taken as happening after his going into the florist shop .", "This is shown in Figure 6 .", "The next example illustrates the case of pprep To summarize , I have claimed that 1 . the notion of specification makes sense with respect to tensed clauses ; 2 . one can describe the anaphoric relation in terms of the RT of a tensed clause Cb , its ET RT configuration , and an existing event or situation entity Ea that is , p Cb , Ea , RTb Eb ; and 3 . there are at least three 13 functions one , po , linking RTb to Ea itself , the other two Pprep and . Bconseq embodying parts of a tripartite ontology of events .", "In the next section , I will discuss constraints on the second argument to p Cb , Ea , RTb that is , constraints on which entities in the evolving E S structure the specification of a tensed clause can depend on .", "Recall from Section 2 . 2 that Sidner introduced the notion of a dynamically changing discourse focus DF to capture the intuition that at any point in the discourse , there is one discourse entity that is the prime focus of attention and that is the most likely although not the only possible specificand of a definite pronoun .", "In parallel , I propose a dynamically changing temporal focus TF , to capture a similar intuition that at any point in the discourse , there is one entity in E S structure that is most attended to and hence most likely to stand in an anaphoric relation with the RT of the next clause .", "That is , 13 Cb , TF , RTb Eb .", "If Cb is interpreted as part of the current discourse segment , after its interpretation there are three possibilities These relationships , which I will call maintenance and local movement of the TF , correspond to Sidner's DF moving gradually among the discourse entities in a discourse segment .", "They cover the same phenomena as the micromoves that Nakhimovsky describes in his paper this volume .", "More radical movement of TF correspond to changes in discourse structure .", "These Computational Linguistics , Volume 14 , Number 2 , June 1988 67 Bonnie Lynn Webber Tense as Discourse Anaphor cover similar phenomena to the macromoves described in Nakhimovsky , also this volume .", "In cases involving movements into and out of an embedded discourse segment , either 1 . the TF will shift to a different entity in E S structure either an existing entity or one created in recognition of an embedded narrative ; or 2 . it will return to the entity previously labeled TF , after completing an embedded narrative .", "Such movements are described in Section 3 . 3 . 2 .", "Other movements , signaled by temporal adverbials and when clauses , are not discussed in this paper . 14", "The following pair of examples illustrate maintenance and local movement of TF within a discourse segment and its link with E S structure construction .", "The first I discussed in the previous section to illustrate el , conseq The second is a variation on that example First consider Example 13 .", "The first clause 13a evokes an event entity Ea describable as John's going into the florist shop .", "Since its tense is simple past , Ea is interpreted as prior to ST .", "Since it begins the discourse , its status is special vis a vis both definite NPs and tensed clauses .", "That is , since no previous TF will have been established yet , the listener takes that entity Ea to serve as TF . '", "5 This is shown in Figure 8 Partee , and Dowty were out to achieve .", "Here it falls out simply from the discourse notion of a TF and from the particular anaphoric function Pconseq 16 Now consider Example 15 repeated here whose first clause is the same as Example 13a and hence would be processed in the same way .", "The tense of the next clause 15b is past perfect .", "As I noted above , the only anaphoric function on RTisb and an event entity that makes sense for perfect tenses is 00 that is , Given that perfect tenses imply ET RT , the event Eb specified by 15b will be interpreted as being prior to Ea .", "Moreover , since 15b is past perfect , the consequent phase of Eb is assumed to still hold with respect to RT , 5b .", "Hence the consequent phase of Eb overlaps Ea .", "Finally since TF is associated with the event entity at RTb , it remains at Ea .", "E S structure at this point resembles Figure 10 Now If Clause 13b is interpreted as being part of the same discourse segment as 13a it must be the case that 13 Ci3b , TF , RTi3b .", "Assume the listener takes p to be Pconseq on the basis of world knowledge that is , 13conseq C13b , TF , RTi3b .", "Since the tense of 13b is simple past , its RT and ET coincide .", "Thus 13b specifies a new entity Eb , located within the consequent phase of the TF that is , Ea and hence after it .", "I assume that , following the computation of the anaphoric function , TF becomes associated with the event entity located at RTb .", "In this case , it is Eb , and TF thereby moves forward cf .", "Figure 9 .", "As noted , this is the gradual forward movement of simple narratives that Hinrichs , Now Clause 15c is the same as 13b , and TF is the same as it was at the point of interpreting 13b .", "Thus not surprisingly , 15c produces the same change in E S Now structure and in the TF as 13b , resulting in the diagram shown in Figure 11 .", "To illustrate the effect of discourse structure on TF , consider the following variation on Example 15 , which had the same structure vis a vis sequence of tenses .", "The first two clauses a and b are the same as in Example 15 and lead to the same configuration of event entities in E S structure as shown in Figure 10 .", "But the most plausible interpretation of 16c is where the quot ; saying quot ; event is interpreted anaphorically with respect to the quot ; promising quot ; event that is , where 16b c are taken together as the start of an embedded discourse , describing an event prior to John's going to the florist's .", "To handle this , I assume , following Grosz and Sidner 1986 , that when the listener recognizes an embedded discourse segment , s he stores the current TF for possible resumption later . 17 However , I also assume the listener recognizes the embedding not when s he first encounters a perfect tensed clause Cb , since it needn't signal an embedded discourse , but later , when an immediately following simple past tense clause Cc is most sensibly interpreted with respect to the event entity Ei , that Cb evoked . 18 At this point , the listener moves TF from its current position to Eb , caching the previous value for possible resumption later .", "Following this gross movement , 13 Cc , TF , RTc will be computed .", "If is then interpreted as B there will be conseq or Pprepl a second movement of TF .", "'9 Coming back to Example 16 , if Clause 16c is taken as being part of a single discourse segment with 16a b , she saying something would have to be interpreted with respect to the current TF Ea John's going to the florist .", "This is implausible under all possible interpretations of 0 . 20 However , under the assumption that Et , is part of an embedded narrative , the listener can a posteriori shift TF to El , and consider the anaphoric relation with Et , as TF .", "At this point , the listener can plausibly take p to be B , conseq based on world knowledge .", "Since 16c is simple past , ETc RT , the quot ; saying quot ; event Ec is viewed as part of the consequent phase and hence following the quot ; promising quot ; event Eb .", "As in the first case , TF moves to the event located at RT i . e . , to E . This is shown roughly in Figure 12 .", "Notice that this involved two movements of TF once in response to a perceived embedded segment and a second time , in response to interpreting 3 as B r conseq Now consider the following extension to 16 d . So he picked out three red roses , two white ones , and one pale pink .", "As before , Clauses 17b c form an embedded narrative , but here the main narrative of John's visit to the florist shop , started at 17a , is continued at 17d .", "To handle this , I again assume that TF behaves much like Sidner's DF in response to the listener's recognition of the end of an embedded narrative that is , the cached TF is resumed and processing continues . 21 Under this assumption , Clauses 17a c are interpreted as in the previous example cf .", "Figure 12 .", "Recognizing Clause 17d as resuming the embedding segment , 22 the previously cached TF Ea the going into the florist shop event is resumed .", "Again assume that the listener takes the anaphoric function to be Pconseq Cd , TF , RTd Ed on the basis of world knowledge .", "Since Clause 17d is simple past ET RT , the picking out roses event Ed is viewed as part of the consequent phase and hence following the going into the florist shop event .", "This is shown roughly in Figure 13 Now getting the listener to interpret a text as an embedded narrative requires providing him her with another event or situation that TF can move to .", "One way in English is via a perfect tensed clause , which Computational Linguistics , Volume 14 , Number 2 , June 1988 69 Bonnie Lynn Webber Tense as Discourse Anaphor explicitly evokes another event , temporally earlier than the one currently in focus .", "Another way is by lexical indications of an embedded narrative , such as verbs of telling and NPs that themselves denote events or situations e . g . , ones headed by de verbal nouns .", "This is illustrated in Example 18 .", "Even though all its clauses are simple past ET RT , Clauses 18c d are most plausibly interpreted as indirect speech describing an event that has occurred prior to the quot ; telling quot ; event .", "I assume that in response to recognizing this kind of embedded narrative , the listener creates a new node of E S structure and shifts TF there , caching the previous value of TF for possible resumption later .", "The temporal location of this new node vis a vis the previous TF will depend on information in the tensed clause and on the listener's world knowledge .", "Notice that , as with embedded narratives cued by the use of a perfect tense , caching the previous TF for resumption later enables the correct interpretation of Clause 18e , which is most plausibly interpreted as following the telling about her sister event .", "An NP denoting an event or situation such as one headed by a noun like trip or by a de verbal noun like installation can also signal the upcoming possibility of an embedded narrative that will elaborate that event or situation past , upcoming , or hypothetical in more detail , as in Example 19 .", "In this case , the original NP and the subsequent clause s will be taken as cospecifying the same thing .", "The question here is how and when TF moves . c . She spent five weeks above the Arctic Circle with two friends . d . The three of them climbed Mt .", "McKinley .", "After interpreting Clause 19b , the TF is at the quot ; telling quot ; event .", "I claim that the NP her trip to Alaska , while evoking a discourse entity , does not affect the TF .", "If Clause 19c is interpreted as the start of an embedded narrative as it is here , TF moves to the event entity Ec it evokes caching the previous value Eb .", "At this point , using additional reasoning , the listener may recognize an anaphoric relation between Clause 19c and the discourse entity evoked by her trip to Alaska .", "Support for this , rather than assuming that an event denoting NP sets up a potential focus , just as I claim a perfect tensed clause does , comes from the reasoning required to understand the following parallel example , where I would claim TF does not move .", "I was talking with Mary yesterday .", "She told me about her trip to Alaska .", "She had spent five weeks above the Arctic Circle with two friends .", "The three of them had climbed Mt .", "McKinley .", "She said that next year they would go for Aconcagua .", "The event described in Clause 20c is the same as that described in Clause 19c , and should be interpreted anaphorically with respect to the entity her trip to Alaska in the same way .", "If this is the case , however , then the anaphoric link does not follow from the movement of TF .", "Example 20 above illustrates one case of an anaphoric function on an NP and a tensed clause , specifically B Cb , E , RTb where the entity Ea has been evoked by an NP rather than a clause .", "Another possibility is that a NPb , Ea Eb , where NPb is definite by virtue of an entity evoked by a clause rather than an NP that is , Eb , is associated with either the preparatory culmination consequent structure of Ea , as in 21 . a . Mary climbed Mt .", "McKinley . b .", "The preparations took her longer than the ascent . or its associated role structure , as in 22 . a . John bought a television . b .", "Although he had intended to buy a 13 quot ; b w set , the salesman convinced him to buy a 25 quot ; color , back projection job . where the salesman fills a particular role in the buying event .", "Next , notice that ambiguities arise when there is more than one way to plausibly segment the discourse , as in the following example 23 . a . I told Frank about my meeting with Ira . b .", "We talked about ordering a Butterfly .", "Here it is plausible to take Clause 23b as the beginning of an embedded narrative , whereby the quot ; talking about quot ; event is interpreted against a new node of E S structure , situated prior to the quot ; telling Frank quot ; event .", "In this case , we is Ira and me .", "It is also plausible to take 23b as continuing the current narrative , whereby the quot ; talking about quot ; event is interpreted with respect to the quot ; telling Frank quot ; event .", "In contrast here , we is Frank and me .", "Finally , consider things from the point of view of generation .", "If some event Eb is part of the preparatory phase of some event Ea , and a description of Ea has just been generated using the simple past tense , then Eb could be described using either the simple past , as in Example 24 or past perfect , as in Example 25 .", "In the case of Example 24 , the listener reader recognizes that Eb is part of the preparatory phase of Ea and that Eb therefore precedes Ea .", "In the case of Example 25 , the listener would first recognize that Eb precedes Ea because of the past perfect , but then recognize Eb as part of the preparatory phase of Ea .", "On the other hand , if Eb simply precedes Ea , but a description of Ea has been generated first , then Eb must be described with a past perfect Example 26 simple past would not be sufficient Example 27 .", "26 . a . John went to the hospital . b .", "He had broken his ankle , walking on a patch of ice .", "27 . a . John went to the hospital . b .", "He broke his ankle , walking on a patch of ice .", "In this paper , I have presented a uniform characterization of discourse anaphora in a way that includes definite pronouns , definite NPs , and tensed clauses .", "In doing so , I have argued that the successful use of discourse anaphors depends on two different things 1 . speakers' and listeners' mutual beliefs about the ontology of the things and events being discussed , and 2 . speakers' and listeners' mutual focus of attention .", "The former implicates semantics in the explanation of discourse anaphora , the latter , discourse itself .", "It is important that we as researchers recognize these as two separate systems , as the properties of discourse as an explanatory device are very different from those of semantics .", "This work was partially supported by ARO grant DAA29 884 9 0027 , NSF grant MCS 8219116 CER , and DARPA grant N00014 85 K 0018 to the University of Pennsylvania , by DARPA grant N00014 85 C 0012 to UNISYS Paoli Research Center , and an Alvey grant to the Centre for Speech Technology Research , University of Edinburgh .", "My thanks to Becky Passonneau , Debby Dahl , Mark Steedman , Ethel Schuster , Candy Sidner , Barbara Grosz , Ellen Bard , Anne Anderson , Tony Sanford , Simon Garrod , and Rich Thomason for their helpful comments on the many earlier versions of this paper ."], "summary_lines": ["Tense As Discourse Anaphor\n", "In this paper, I consider a range of English expressions and show that their context-dependency can be characterized in terms of two properties: 1. They specify entities in an evolving model of the discourse that the listener is constructing; 2. The particular entity specified depends on another entity in that part of the evolving \"discourse model\" that the listener is currently attending to.\n", "Such expressions have been called anaphors.\n", "I show how tensed clauses share these characteristics, usually just attributed to anaphoric noun phrases.\n", "This not only allows us to capture in a simple way the oft-stated but difficult-to-prove intuition that tense is anaphoric, but also contributes to our knowledge of what is needed for understanding narrative text.\n", "We improve upon the above work by specifying rules for how events are related to one another in a discourse and Sing and Sing defined semantic constraints through which events can be related (Sing, 1997).\n"]}
{"article_lines": ["Exploiting Syntactic Structure for Language Modeling", "The paper presents a language model that develops syntactic structure and uses it to extract meaningful information from the word history , thus enabling the use of long distance dependencies .", "The model assigns probability to every joint sequence of words binary parse structure with headword annotation and operates in a left to right manner therefore usable for automatic speech recognition .", "The model , its probabilistic parameterization , and a set of experiments meant to evaluate its predictive power are presented ; an improvement over standard trigram modeling is achieved .", "The main goal of the present work is to develop a language model that uses syntactic structure to model long distance dependencies .", "During the summer96 DoD Workshop a similar attempt was made by the dependency modeling group .", "The model we present is closely related to the one investigated in Chelba et al . , 1997 , however different in a few important aspects our model operates in a left to right manner , allowing the decoding of word lattices , as opposed to the one referred to previously , where only whole sentences could be processed , thus reducing its applicability to n best list re scoring ; the syntactic structure is developed as a model component ; our model is a factored version of the one in Chelba et al . , 1997 , thus enabling the calculation of the joint probability of words and parse structure ; this was not possible in the previous case due to the huge computational complexity of the model .", "Our model develops syntactic structure incrementally while traversing the sentence from left to right .", "This is the main difference between our approach and other approaches to statistical natural language parsing .", "Our parsing strategy is similar to the incremental syntax ones proposed relatively recently in the linguistic community Philips , 1996 .", "The probabilistic model , its parameterization and a few experiments that are meant to evaluate its potential for speech recognition are presented .", "Consider predicting the word after in the sentence the contract ended with a loss of 7 cents after trading as low as 89 cents .", "A 3 gram approach would predict after from 7 , cents whereas it is intuitively clear that the strongest predictor would be ended which is outside the reach of even 7 grams .", "Our assumption is that what enables humans to make a good prediction of after is the syntactic structure in the past .", "The linguistically correct partial parse of the word history when predicting after is shown in Figure 1 .", "The word ended is called the headword of the constituent ended with . . . and ended is an exposed headword when predicting after topmost headword in the largest constituent that contains it .", "The syntactic structure in the past filters out irrelevant words and points to the important ones , thus enabling the use of long distance information when predicting the next word .", "Our model will attempt to build the syntactic structure incrementally while traversing the sentence left to right .", "The model will assign a probability P W , T to every sentence W with every possible POStag assignment , binary branching parse , nonterminal label and headword annotation for every constituent of T . Let W be a sentence of length n words to which we have prepended s and appended s so that wo s and w , , 1 s .", "Let Wk be the word k prefix wo wk of the sentence and Wk Tk the word parse k prefix .", "To stress this point , a word parse k prefix contains for a given parse only those binary subtrees whose span is completely included in the word k prefix , excluding wo s .", "Single words along with their POStag can be regarded as root only trees .", "Figure 2 shows a word parse k prefix ; h_O h_ m are the exposed heads , each head being a pair headword , nonterminal label , or word , POStag in the case of a root only tree .", "A complete parse Figure 3 is any binary parse of the wi ti Wn , to Is , SE sequence with the restriction that s , TOP' is the only allowed head .", "Note that wi , ti .", "w , t needn't be a constituent , but for the parses where it is , there is no restriction on which of its words is the headword or what is the non terminal label that accompanies the headword .", "The model will operate by means of three modules unary , NTlabel , adjoin left , NTlabel or adj oin right , NTlabel until it passes control to the PREDICTOR by taking a null transition .", "NTlabel is the non terminal label assigned to the newly built constituent and left , right specifies where the new headword is inherited from .", "The operations performed by the PARSER are illustrated in Figures 4 6 and they ensure that all possible binary branching parses with all possible headword and non terminal label assignments for the w1 wk word sequence can be generated .", "The following algorithm formalizes the above description of the sequential generation of a sentence with a complete parse .", "The unary transition is allowed only when the most recent exposed head is a leaf of the tree a regular word along with its POStag hence it can be taken at most once at a given position in the input word string .", "The second subtree in Figure 2 provides an example of a unary transition followed by a null transition .", "It is easy to see that any given word sequence with a possible parse and headword annotation is generated by a unique sequence of model actions .", "This will prove very useful in initializing our model parameters from a treebank see section 3 . 5 .", "The probability P W , T of a word sequence W and a complete parse T can be broken into where As can be seen , wk , tk , Wk 171 1 , Pt 14 1 is one of the Nk word parse k prefixes WkTk at position k in the sentence , i I , Nk .", "To ensure a proper probabilistic model 1 we have to make sure that 2 , 3 and 4 are well defined conditional probabilities and that the model halts with probability one .", "Consequently , certain PARSER and WORD PREDICTOR probabilities must be given specific values P adjoin right , TOP' WkTk 1 , if h_O s , TOP' and h_ 1 . word 0 s ensure that the parse generated by our model is consistent with the definition of a complete parse ; The word predictor model 2 predicts the next word based on the preceding 2 exposed heads , thus making the following equivalence classification After experimenting with several equivalence classifications of the word parse prefix for the tagger model , the conditioning part of model 3 was reduced to using the word to be tagged and the tags of the two most recent exposed heads Model 4 assigns probability to different parses of the word k prefix by chaining the elementary operations described above .", "The workings of the parser module are similar to those of Spatter Jelinek et al . , 1994 .", "The equivalence classification of the WkTk word parse we used for the parser model 4 was the same as the one used in Collins , 1996 It is worth noting that if the binary branching structure developed by the parser were always rightbranching and we mapped the POStag and nonterminal label vocabularies to a single type then our model would be equivalent to a trigram language model .", "All model components WORD PREDICTOR , TAGGER , PARSER are conditional probabilistic models of the type P y xi , x2 , , x , 2 where y , x1 , x2 , . . . , xn , belong to a mixed bag of words , POStags , non terminal labels and parser operations y only .", "For simplicity , the modeling method we chose was deleted interpolation among relative frequency estimates of different orders f . using a recursive mixing scheme As can be seen , the context mixing scheme discards items in the context in right to left order .", "The A coefficients are tied based on the range of the count C xi , , xn .", "The approach is a standard one which doesn't require an extensive description given the literature available on it Jelinek and Mercer , 1980 .", "Since the number of parses for a given word prefix Wk grows exponentially with k , I Tk 1 0 2k , the state space of our model is huge even for relatively short sentences so we had to use a search strategy that prunes it .", "Our choice was a synchronous multistack search algorithm which is very similar to a beam search .", "Each stack contains hypotheses partial parses that have been constructed by the same number of predictor and the same number of parser operations .", "The hypotheses in each stack are ranked according to the ln P W , T score , highest on top .", "The width of the search is controlled by two parameters above pruning strategy proved to be insufficient so we chose to also discard all hypotheses whose score is more than the log probability threshold below the score of the topmost hypothesis .", "This additional pruning step is performed after all hypotheses in stage k' have been extended with the null parser transition and thus prepared for scanning a new word .", "The conditional perplexity calculated by assigning to a whole sentence the probability where T argmaxTP W , T , is not valid because it is not causal when predicting wk i we use T which was determined by looking at the entire sentence .", "To be able to compare the perplexity of our model with that resulting from the standard trigram approach , we need to factor in the entropy of guessing the correct parse I' before predicting wk i , based solely on the word prefix Wk .", "The probability assignment for the word at position k 1 in the input sentence is made using which ensures a proper probability over strings W , where Sk is the set of all parses present in our stacks at the current stage k . Another possibility for evaluating the word level perplexity of our model is to approximate the probability of a whole sentence where T k is one of the quot ; N best quot ; in the sense defined by our search parses for W . This is a deficient probability assignment , however useful for justifying the model parameter re estimation .", "The two estimates 8 and 10 are both consistent in the sense that if the sums are carried over all possible parses we get the correct value for the word level perplexity of our model .", "The major problem we face when trying to reestimate the model parameters is the huge state space of the model and the fact that dynamic programming techniques similar to those used in HMM parameter re estimation cannot be used with our model .", "Our solution is inspired by an HMM re estimation technique that works on pruned N best trellises Byrne et al . , 1998 .", "Let W , T k , k 1 .", "N be the set of hypotheses that survived our pruning strategy until the end of the parsing process for sentence W . Each of them was produced by a sequence of model actions , chained together as described in section 2 ; let us call the sequence of model actions that produced a given W , T the derivation W , T .", "Let an elementary event in the derivation W , T word to tag , ho . tag , h_i . tag .", "The probability associated with each model action is determined as described in section 3 . 1 , based on counts C m y m , x m , one set for each model component .", "Assuming that the deleted interpolation coefficients and the count ranges used for tying them stay fixed , these counts are the only parameters to be re estimated in an eventual re estimation procedure ; indeed , once a set of counts C m y m , x m is specified for a given model m , we can easily calculate in ky Xn for all context orders n 0 . .", ". maximum order mode m ; This is all we need for calculating the probability of an elementary event and then the probability of an entire derivation .", "One training iteration of the re estimation procedure we propose is described by the following algorithm N best parse development data ; counts . Ei prepare counts . E i 1 for each model component c gather_counts development model_c ; In the parsing stage we retain for each quot ; N best quot ; hypothesis W , T k , k 1 .", "N , only the quantity 0 w77 0 p w , T 0 N , N1 ' p k w7 T k and its derivation W , T k .", "We then scan all the derivations in the quot ; development set quot ; and , for each occurrence of the elementary event y m , x m in derivation W , T k we accumulate the value 0 W , T k in the C m y m , x m counter to be used in the next iteration .", "The intuition behind this procedure is that 0 W , T k is an approximation to the P T k W probability which places all its mass on the parses that survived the parsing process ; the above procedure simply accumulates the expected values of the counts Om y m , x m under the 0 W , T k conditional distribution .", "As explained previously , the Om y m , x m counts are the parameters defining our model , making our procedure similar to a rigorous EM approach Dempster et al . , 1977 .", "A particular and very interesting case is that of events which had count zero but get a non zero count in the next iteration , caused by the quot ; N best quot ; nature of the re estimation process .", "Consider a given sentence in our quot ; development quot ; set .", "The quot ; N best quot ; derivations for this sentence are trajectories through the state space of our model .", "They will change from one iteration to the other due to the smoothing involved in the probability estimation and the change of the parameters event counts defining our model , thus allowing new events to appear and discarding others through purging low probability events from the stacks .", "The higher the number of trajectories per sentence , the more dynamic this change is expected to be .", "The results we obtained are presented in the experiments section .", "All the perplexity evaluations were done using the left to right formula 8 L2RPPL for which the perplexity on the quot ; development set quot ; is not guaranteed to decrease from one iteration to another .", "However , we believe that our reestimation method should not increase the approximation to perplexity based on 10 SUM PPL again , on the quot ; development set quot ; ; we rely on the consistency property outlined at the end of section 3 . 3 to correlate the desired decrease in L2R PPL with that in SUM PPL .", "No claim can be made about the change in either L2R PPL or SUM PPL on test data .", "Each model component WORD PREDICTOR , TAGGER , PARSER is trained initially from a set of parsed sentences , after each parse tree W , T undergoes These are the initial parameters used with the reestimation procedure described in the previous section .", "In order to get initial statistics for our model components we needed to binarize the UPenn Treebank Marcus et al . , 1995 parse trees and percolate headwords .", "The procedure we used was to first percolate headwords using a context free CF rulebased approach and then binarize the parses by using a rule based approach again .", "The headword of a phrase is the word that best represents the phrase , all the other words in the phrase being modifiers of the headword .", "Statistically speaking , we were satisfied with the output of an enhanced version of the procedure described in Collins , 1996 also known under the name quot ; Magerman Sz Black Headword Percolation Rules quot ; .", "Once the position of the headword within a constituent equivalent with a CF production of the type Z 4 Y1 . . . Y , 2 , where Z , Yr , are nonterminal labels or POStags only for Yi is identified to be k , we binarize the constituent as follows depending on the Z identity , a fixed rule is used to decide which of the two binarization schemes in Figure 8 to apply .", "The intermediate nodes created by the above binarization schemes receive the nonterminal label Z' .", "Due to the low speed of the parser 200 wds min for stack depth 10 and log probability threshold 6 . 91 nats 1 1000 we could carry out the reestimation technique described in section 3 . 4 on only 1 Mwds of training data .", "For convenience we chose to work on the UPenn Treebank corpus .", "The vocabulary sizes were The training data was split into quot ; development quot ; set 929 , 564wds sections 00 20 and quot ; check set quot ; 73 , 760wds sections 21 22 ; the test set size was 82 , 430wds sections 23 24 .", "The quot ; check quot ; set has been used for estimating the interpolation weights and tuning the search parameters ; the quot ; development quot ; set has been used for gathering estimating counts ; the test set has been used strictly for evaluating model performance .", "Table 1 shows the results of the re estimation technique presented in section 3 . 4 .", "We achieved a reduction in test data perplexity bringing an improvement over a deleted interpolation trigram model whose perplexity was 167 . 14 on the same training test data ; the reduction is statistically significant according to a sign test .", "Simple linear interpolation between our model and the trigram model yielded a further improvement in PPL , as shown in Table 2 .", "The interpolation weight was estimated on check data to be A 0 . 36 .", "An overall relative reduction of 11 over the trigram model has been achieved .", "The large difference between the perplexity of our model calculated on the quot ; development quot ; set used for model parameter estimation and quot ; test quot ; set unseen data shows that the initial point we choose for the parameter values has already captured a lot of information from the training data .", "The same problem is encountered in standard n gram language modeling ; however , our approach has more flexibility in dealing with it due to the possibility of reestimating the model parameters .", "We believe that the above experiments show the potential of our approach for improved language models .", "Our future plans include", "This research has been funded by the NSF IRI 19618874 grant STIMULATE .", "The authors would like to thank Sanjeev Khudanpur for his insightful suggestions .", "Also to Harry Printz , Eric Ristad , Andreas Stolcke , Dekai Wu and all the other members of the dependency modeling group at the summer96 DoD Workshop for useful comments on the model , programming support and an extremely creative environment .", "Also thanks to Eric Brill , Sanjeev Khudanpur , David Yarowsky , Radu Florian , Lidia Mangu and Jun Wu for useful input during the meetings of the people working on our STIMULATE grant ."], "summary_lines": ["Exploiting Syntactic Structure for Language Modeling\n", "The paper presents a language model that develops syntactic structure and uses it to extract meaningful information from the word history, thus enabling the use of long distance dependencies.\n", "The model assigns probability to every joint sequence of words-binary-parse-structure with headword annotation and operates in a left-to-right manner -- therefore usable for automatic speech recognition.\n", "The model, its probabilistic parameterization, and a set of experiments meant to evaluate its predictive power are presented; an improvement over standard trigram modeling is achieved.\n", "We choose the lexical heads of the two previous constituents as determined by a shift-reduce parser and find that this works better than a trigram model.\n", "We condition on linguistically relevant words by assigning partial phrase structure to the history and percolating headwords.\n"]}
{"article_lines": ["Entity Based Cross Document Core f erencing Using the Vector Space Model", "Cross document coreference occurs when the same person , place , event , or concept is discussed in more than one text source .", "Computer recognition of this phenomenon is important because it helps break quot ; the document boundary quot ; by allowing a user to examine information about a particular entity from multiple text sources at the same time .", "In this paper we describe a cross document coreference resolution algorithm which uses the Vector Space Model to resolve ambiguities between people having the same name .", "In addition , we also describe a scoring algorithm for evaluating the cross document coreference chains produced by our system and we compare our algorithm to the scoring algorithm used in the MUC 6 within document coreference task .", "Cross document coreference occurs when the same person , place , event , or concept is discussed in more than one text source .", "Computer recognition of this phenomenon is important because it helps break quot ; the document boundary quot ; by allowing a user to examine information about a particular entity from multiple text sources at the same time .", "In particular , resolving cross document coreferences allows a user to identify trends and dependencies across documents .", "Cross document coreference can also be used as the central tool for producing summaries from multiple documents , and for information fusion , both of which have been identified as advanced areas of research by the TIPSTER Phase III program .", "Cross document coreference was also identified as one of the potential tasks for the Sixth Message Understanding Conference MUC 6 but was not included as a formal task because it was considered too ambitious Grishman 94 .", "In this paper we describe a highly successful crossdocument coreference resolution algorithm which uses the Vector Space Model to resolve ambiguities between people having the same name .", "In addition , we also describe a scoring algorithm for evaluating the cross document coreference chains produced by our system and we compare our algorithm to the scoring algorithm used in the MUC 6 within document coreference task .", "Cross document coreference is a distinct technology from Named Entity recognizers like IsoQuest's NetOwl and IBM's Textract because it attempts to determine whether name matches are actually the same individual not all John Smiths are the same .", "Neither NetOwl or Textract have mechanisms which try to keep same named individuals distinct if they are different people .", "Cross document coreference also differs in substantial ways from within document coreference .", "Within a document there is a certain amount of consistency which cannot be expected across documents .", "In addition , the problems encountered during within document coreference are compounded when looking for coreferences across documents because the underlying principles of linguistics and discourse context no longer apply across documents .", "Because the underlying assumptions in crossdocument coreference are so distinct , they require novel approaches .", "Figure 1 shows the architecture of the crossdocument system developed .", "The system is built upon the University of Pennsylvania's within document coreference system , CAMP , which participated in the Seventh Message Understanding Conference MUC 7 within document coreference task MUC7 1998 .", "Our system takes as input the coreference processed documents output by CAMP .", "It then passes these documents through the SentenceExtractor module which extracts , for each document , all the sentences relevant to a particular entity of interest .", "The VSM Disambiguate module then uses a vector space model algorithm to compute similarities between the sentences extracted for each pair of documents .", "Oliver quot ; Biff quot ; Kelly of Weymouth succeeds John Perry as president of the Massachusetts Golf Association .", "quot ; We will have continued growth in the future , quot ; said Kelly , who will serve for two years .", "quot ; There's been a lot of changes and there will be continued changes as we head into the year 2000 . quot ; Details about each of the main steps of the crossdocument coreference algorithm are given below . consider the two extracts in Figures 2 and 4 .", "The coreference chains output by CAMP for the two extracts are shown in Figures 3 and 5 . tractor module produces a quot ; summary quot ; of the article with respect to the entity of interest .", "These summaries are a special case of the query sensitive techniques being developed at Penn using CAMP .", "Therefore , for doc . 36 Figure 2 , since at least one of the three noun phrases quot ; John Perry , quot ; quot ; he , quot ; and quot ; Perry quot ; in the coreference chain of interest appears in each of the three sentences in the extract , the summary produced by SentenceExtractor is the extract itself .", "On the other hand , the summary produced by SentenceExtractor for the coreference chain of interest in doc . 38 is only the first sentence of the extract because the only element of the coreference chain appears in this sentence .", "The University of Pennsylvania's CAMP system resolves within document coreferences for several different classes including pronouns , and proper names Baldwin 95 .", "It ranked among the top systems in the coreference task during the MUC 6 and the MUC 7 evaluations .", "The coreference chains output by CAMP enable us to gather all the information about the entity of interest in an article .", "This information about the entity is gathered by the SentenceExtractor module and is used by the VSM Disambiguate module for disambiguation purposes .", "Consider the extract for doc . 36 shown in Figure 2 .", "We are able to include the fact that the John Perry mentioned in this article was the president of the Massachusetts Golf Association only because CAMP recognized that the quot ; he quot ; in the second sentence is coreferent with quot ; John Perry quot ; in the first .", "And it is this fact which actually helps VSM Disambiguate decide that the two John Perrys in doc . 36 and doc . 38 are the same person .", "The vector space model used for disambiguating entities across documents is the standard vector space model used widely in information retrieval Salton 89 .", "In this model , each summary extracted by the SentenceExtractor module is stored as a vector of terms .", "The terms in the vector are in their morphological root form and are filtered for stop words words that have no information content like a , the , of , an , . . . .", "If Si and S2 are the vectors for the two summaries extracted from documents D1 and D21 then their similarity is computed as where tj is a term present in both Si and S2 , W1j is the weight of the term t3 in S1 and w23 is the weight of ti in S2 .", "The weight of a term tj in the vector St for a summary is given by where t f is the frequency of the term t3 in the summary , N is the total number of documents in the collection being examined , and df is the number of documents in the collection that the term tj occurs in .", "MI 42 4n is the cosine normalization factor and is equal to the Euclidean length of the vector Si .", "The VSM Disambiguate module , for each summary Si , computes the similarity of that summary with each of the other summaries .", "If the similarity computed is above a pre defined threshold , then the entity of interest in the two summaries are considered to be coreferent .", "The cross document coreference system was tested on a highly ambiguous test set which consisted of 197 articles from 1996 and 1997 editions of the New York Times .", "The sole criteria for including an article in the test set was the presence or the absence of a string in the article which matched the quot ; John .", "? Smithr regular expression .", "In other words , all of the articles either contained the name John Smith or contained some variation with a middle initial name .", "The system did not use any New York Times data for training purposes .", "The answer keys regarding the cross document chains were manually created , but the scoring was completely automated .", "There were 35 different John Smiths mentioned in the articles .", "Of these , 24 of them only had one article which mentioned them .", "The other 173 articles were regarding the 11 remaining John Smiths .", "The background of these John Smiths , and the number of articles pertaining to each , varied greatly .", "Descriptions of a few of the John Smiths are Chairman and CEO of General Motors , assistant track coach at UCLA , the legendary explorer , and the main character in Disney's Pocahontas , former president of the Labor Party of Britain .", "In order to score the cross document coreference chains output by the system , we had to map the cross document coreference scoring problem to a within document coreference scoring problem .", "This was done by creating a meta document consisting Of the file names of each of the documents that the system was run on .", "Assuming that each of the documents in the data set was about a single John Smith , the cross document coreference chains produced by the system could now be evaluated by scoring the corresponding within document coreference chains in the meta document .", "We used two different scoring algorithms for scoring the output .", "The first was the standard algorithm for within document coreference chains which was used for the evaluation of the systems participating in the MUC 6 and the MUC 7 coreference tasks .", "The shortcomings of the MUC scoring algorithm when used for the cross document coreference task forced us to develop a second algorithm .", "Details about both these algorithms follow .", "The MUC algorithm computes precision and recall statistics by looking at the number of links identified by a system compared to the links in an answer key .", "In the model theoretic description of the algorithm that follows , the term quot ; key quot ; refers to the manually annotated coreference chains the truth while the term quot ; response quot ; refers to the coreference chains output by a system .", "An equivalence set is the transitive closure of a coreference chain .", "The algorithm , developed by Vilain 95 , computes recall in the following way .", "First , let S be an equivalence set generated by the key , and let R1 . . . R , . be equivalence classes generated by the response .", "Then we define the following functions over S fully reunite any components of the p S partition .", "We note that this is simply one fewer than the number of elements in the partition , that is , m S Ip S I 1 .", "Looking in isolation at a single equivalence class in the key , the recall error for that class is just the number of missing links divided by the number of correct links , i . e . , Precision is computed by switching the roles of the key and response in the above formulation .", "While the Vilain 95 provides intuitive results for coreference scoring , it however does not work as well in the context of evaluating cross document coreference .", "There are two main reasons .", "The algorithm does not give any credit for separating out singletons entities that occur in chains consisting only of one element , the entity itself from other chains which have been identified .", "This follows from the convention in coreference annotation of not identifying those entities that are markable as possibly coreferent with other entities in the text .", "Rather , entities are only marked as being coreferent if they actually are coreferent with other entities in the text .", "This shortcoming could be easily enough overcome with different annotation conventions and with minor changes to the algorithm , but it is worth noting .", "All errors are considered to be equal .", "The MUC scoring algorithm penalizes the precision numbers equally for all types of errors .", "It is our position that , for certain tasks , some coreference errors do more damage than others .", "Consider the following examples suppose the truth contains two large coreference chains and one small one Figure 6 , and suppose Figures 7 and 8 show two different responses .", "We will explore two different precision errors .", "The first error will connect one of the large coreference chains with the small one Figure 7 .", "The second error occurs when the two large coreference chains are related by the errant coreferent link Figure 8 .", "It is our position that the second error is more damaging because , compared to the first error , the second error makes more entities coreferent that should not be .", "This distinction is not reflected in the Vilain 95 scorer which scores both responses as having a precision score of 90 Figure 9 .", "Imagine a scenario where a user recalls a collection of articles about John Smith , finds a single article about the particular John Smith of interest and wants to see all the other articles about that individual .", "In commercial systems with News data , precision is typically the desired goal in such settings .", "As a result we wanted to model the accuracy of the system on a per document basis and then build a more global score based on the sum of the user's experiences .", "Consider the case where the user selects document 6 in Figure 8 .", "This a good outcome with all the relevant documents being found by the system and no extraneous documents .", "If the user selected document 1 , then there are 5 irrelevant documents in the systems output precision is quite low then .", "The goal of our scoring algorithm then is to model the precision and recall on average when looking for more documents about the same person based on selecting a single document .", "Instead of looking at the links produced by a system , our algorithm looks at the presence absence of entities from the chains produced .", "Therefore , we compute the precision and recall numbers for each entity in the document .", "The numbers computed with respect to each entity in the document are then combined to produce final precision and recall numbers for the entire output .", "For an entity , i , we define the precision and recall with respect to that entity in Figure 10 .", "The final precision and recall numbers are computed by the following two formulae Final Precision E wi Precisioni Final Recall E wi Recalls where N is the number of entities in the document , and wi is the weight assigned to entity i in the document .", "For all the examples and the experiments in this paper we assign equal weights to each entity i . e . wi 1 N .", "We have also looked at the possibilities of using other weighting schemes .", "Further details about the B CUBED algorithm including a model theoretic version of the algorithm can be found in Bagga 98a .", "Consider the response shown in Figure 7 .", "Using the B CUBED algorithm , the precision for entity 6 in the document equals 2 7 because the chain output for the entity contains 7 elements , 2 of which are correct , namely 6 , 7 .", "The recall for entity 6 , however , is 2 2 because the chain output for the entity has 2 correct elements in it and the quot ; truth quot ; chain for the entity only contains those 2 elements .", "Figure 9 shows the final precision and recall numbers computed by the B CUBED algorithm for the examples shown in Figures 7 and 8 .", "The figure also shows the precision and recall numbers for each entity ordered by entity numbers .", "The B CUBED algorithm does overcome the the two main shortcomings of the MUC scoring algorithm discussed earlier .", "It implicitly overcomes the first number of correct elements in the output chain containing entityi number of elements in the output chain containing entity , number of correct elements in the output chain containing entity , number of elements in the truth chain containing entity , Figure 10 Definitions for Precision and Recall for an Entity i shortcoming of the MUC 6 algorithm by calculating the precision and recall numbers for each entity in the document irrespective of whether an entity is part of a coreference chain .", "Consider the responses shown in Figures 7 and 8 .", "We had mentioned earlier that the error of linking the the two large chains in the second response is more damaging than the error of linking one of the large chains with the smaller chain in the first response .", "Our scoring algorithm takes this into account and computes a final precision of 58 and 76 for the two responses respectively .", "In comparison , the MUC algorithm computes a precision of 90 for both the responses Figure 9 .", "Figure 11 shows the precision , recall , and F Measure with equal weights for both precision and recall using the B CUBED scoring algorithm .", "The Vector Space Model in this case constructed the space of terms only from the summaries extracted by SentenceExtractor .", "In comparison , Figure 12 shows the results using the B CUBED scoring algorithm when the vector space model constructed the space of terms from the articles input to the system it still used the summaries when computing the similarity .", "The importance of using CAMP to extract summaries is verified by comparing the highest FMeasures achieved by the system for the two cases .", "The highest F Measure for the former case is 84 . 6 while the highest F Measure for the latter case is 78 . 0 .", "In comparison , for this task , named entity tools like NetOwl and Textract would mark all the John Smiths the same .", "Their performance using our Threshold Figure 11 Precision , Recall , and F Measure Using the B CUBED Algorithm With Training On the Summaries scoring algorithm is 23 precision , and 100 recall .", "Figures 13 and 14 show the precision , recall , and F Measure calculated using the MUC scoring algorithm .", "Also , the baseline case when all the John Smiths are considered to be the same person achieves 83 precision and 100 recall .", "The high initial precision is mainly due to the fact that the MUC algorithm assumes that all errors are equal .", "We have also tested our system on other classes of cross document coreference like names of companies , and events .", "Details about these experiments can be found in Bagga 98b .", "As a novel research problem , cross document coreference provides an different perspective from related phenomenon like named entity recognition and within document coreference .", "Our system takes summaries about an entity of interest and uses various information retrieval metrics to rank the similarity of the summaries .", "We found it quite challenging to arrive at a scoring metric that satisfied our intuitions about what was good system output v . s . bad , but we have developed a scoring algorithm that is an improvement for this class of data over other within document coreference scoring algorithms .", "Our results are quite encouraging with potential performance being as good as 84 . 6 F Measure .", "The first author was supported in part by a Fellowship from IBM Corporation , and in part by the Institute for Research in Cognitive Science at the University of Pennsylvania ."], "summary_lines": ["Entity-Based Cross-Document Core f erencing Using the Vector Space Model\n", "Cross-document coreference occurs when the same person, place, event, or concept is discussed in more than one text source.\n", "Computer recognition of this phenomenon is important because it helps break \"the document boundary\" by allowing a user to examine information about a particular entity from multiple text sources at the same time.\n", "In this paper we describe a cross-document coreference resolution algorithm which uses the Vector Space Model to resolve ambiguities between people having the same name.\n", "In addition, we also describe a scoring algorithm for evaluating the cross-document coreference chains produced by our system and we compare our algorithm to the scoring algorithm used in the MUC-6 (within document) coreference task.\n", "\n", "\n", "We proposed entity-based cross-document co-referencing which uses co-reference chains of each document to generate its summary and then use the summary rather than the whole article to select informative words to be the features of the document.\n"]}
{"article_lines": ["Why Doesn't EM Find Good HMM POS Taggers ?", "This paper investigates why the HMMs es timated by Expectation Maximization EM produce such poor results as Part of Speech POS taggers .", "We find that the HMMs es timated by EM generally assign a roughlyequal number of word tokens to each hid den state , while the empirical distribution of tokens to POS tags is highly skewed .", "This motivates a Bayesian approach using a sparse prior to bias the estimator toward such a skewed distribution .", "We investigate Gibbs Sampling GS and Variational Bayes VB estimators and show that VB con verges faster than GS for this task and thatVB significantly improves 1 to 1 tagging ac curacy over EM .", "We also show that EM does nearly as well as VB when the number of hidden HMM states is dramatically reduced .", "We also point out the high variance in all of these estimators , and that they requiremany more iterations to approach conver gence than usually thought .", "It is well known that Expectation Maximization EM performs poorly in unsupervised induction of linguistic structure Carroll and Charniak , 1992 ; Merialdo , 1994 ; Klein , 2005 ; Smith , 2006 .", "In ret rospect one can certainly find reasons to explain this failure after all , likelihood does not appear in thewide variety of linguistic tests proposed for identi fying linguistic structure Fromkin , 2001 . This paper focuses on unsupervised part ofspeech POS tagging , because it is perhaps the sim plest linguistic induction task .", "We suggest that onereason for the apparent failure of EM for POS tagging is that it tends to assign relatively equal numbers of tokens to each hidden state , while the em pirical distribution of POS tags is highly skewed , like many linguistic and non linguistic phenomena Mitzenmacher , 2003 .", "We focus on first order Hid den Markov Models HMMs in which the hidden state is interpreted as a POS tag , also known as bitag models .", "In this setting we show that EM performs poorlywhen evaluated using a ? 1 to 1 accuracy ?", "evalua tion , where each POS tag corresponds to at most one hidden state , but is more competitive when evaluatedusing a ? many to 1 accuracy ?", "evaluation , where sev eral hidden states may correspond to the same POStag .", "We explain this by observing that the distribution of hidden states to words proposed by the EM estimated HMMs is relatively uniform , while the empirical distribution of POS tags is heavily skewed towards a few high frequency tags .", "Based on this , we propose a Bayesian prior that biases the sys tem toward more skewed distributions and show that this raises the 1 to 1 accuracy significantly .", "Finally , we show that a similar increase in accuracy can be achieved by reducing the number of hidden states in the models estimated by EM .", "There is certainly much useful information that bitag HMMs models cannot capture .", "Toutanova etal .", "2003 describe a wide variety of morphological and distributional features useful for POS tagging , and Clark 2003 proposes ways of incorporat ing some of these in an unsupervised tagging model .", "However , bitag models are rich enough to capture at least some distributional information i . e . , the tag 296for a word depends on the tags assigned to its neighbours .", "Moreover , more complex models add addi tional complicating factors that interact in ways stillpoorly understood ; for example , smoothing is gen erally regarded as essential for higher order HMMs , yet it is not clear how to integrate smoothing into un supervised estimation procedures Goodman , 2001 ; Wang and Schuurmans , 2005 .", "Most previous work exploiting unsupervised training data for inferring POS tagging models has focused on semi supervised methods in the in which the learner is provided with a lexicon specifying the possible tags for each word Merialdo , 1994 ; Smith and Eisner , 2005 ; Goldwater and Griffiths , 2007 or a small number of ? prototypes ?", "for each POS Haghighi and Klein , 2006 .", "In the context of semi supervised learning using a tag lexicon , Wang and Schuurmans 2005 observe discrepencies between the empirical and estimated tag frequencies similar to those observed here , and show that constraining the estimation procedure to preserve the empiricalfrequencies improves tagging accuracy .", "This ap proach cannot be used in an unsupervised setting since the empirical tag distribution is not available .", "However , as Banko and Moore 2004 point out , the accuracy achieved by these unsupervised methodsdepends strongly on the precise nature of the su pervised training data in their case , the ambiguity of the tag lexicon available to the system , which makes it more difficult to understand the behaviour of such systems .", "All of the experiments described below have the same basic structure an estimator is used to infera bitag HMM from the unsupervised training cor pus the words of Penn Treebank PTB Wall Street Journal corpus Marcus et al , 1993 , and then the resulting model is used to label each word of that corpus with one of the HMM ? s hidden states .", "This section describes how we evaluate how well thesesequences of hidden states correspond to the gold standard POS tags for the training corpus here , the PTB POS tags .", "The chief difficulty is determining the correspondence between the hidden states and the gold standard POS tags . Perhaps the most straightforward method of es tablishing this correspondence is to deterministically map each hidden state to the POS tag it co occurs most frequently with , and return the proportion of the resulting POS tags that are the same as the POS tags of the gold standard corpus .", "We call this themany to 1 accuracy of the hidden state sequence be cause several hidden states may map to the same POS tag and some POS tags may not be mapped to by any hidden states at all .", "As Clark 2003 points out , many to 1 accuracy has several defects .", "If a system is permitted to posit an unbounded number of hidden states which is notthe case here then it can achieve a perfect many to 1 accuracy by placing every word token into its own unique state .", "Cross validation , i . e . , identifying themany to 1 mapping and evaluating on different subsets of the data , would answer many of these objections .", "Haghighi and Klein 2006 propose constrain ing the mapping from hidden states to POS tags so that at most one hidden state maps to any POS tag .", "This mapping is found by greedily assigning hidden states to POS tags until either the hidden states or POS tags are exhausted note that if the number ofhidden states and POS tags differ , some will be unas signed .", "We call the accuracy of the POS sequence obtained using this map its 1 to 1 accuracy .", "Finally , several authors have proposed using information theoretic measures of the divergence between the hidden state and POS tag sequences .", "Goldwater and Griffiths 2007 propose using the Variation of Information VI metric described byMeila ?", "We regard the assignments of hidden states and POS tags to the words of the cor pus as two different ways of clustering those words , and evaluate the conditional entropy of each clus tering conditioned on the other .", "The VI is the sum of these conditional entropies .", "Specifically , given a corpus labeled with hidden states and POS tags , if p ? y , p ? t and p ? y , t are the empirical probabilities of a hidden state y , a POS tag t , and the cooccurance of y and t respectively , then the mutual information I , entropies H and variation of information VI are defined as follows H Y ? ?", "y p ? y log p ? y H T ? ?", "t p ? t log p ? t I Y , T ? y , t p ? y , t log p ? y , t p ? y p ? t H Y T H Y ?", "I Y , T 297 H T Y H T ?", "I Y , T VI Y , T H Y T H T Y As Meila ?", "2003 shows , VI is a metric on the space of probability distributions whose value reflects the divergence between the two distributions , and only takes the value zero when the two distributions are identical .", "Expectation Maximization There are several excellent textbook presentations of Hidden Markov Models and the Forward Backward algorithm for Expectation Maximization Jelinek , 1997 ; Manning and Schu ? tze , 1999 ; Bishop , 2006 , so we do not cover them in detail here .", "Conceptu ally , a Hidden Markov Model generates a sequence of observations x x0 , . . .", ", xn here , the wordsof the corpus by first using a Markov model to gen erate a sequence of hidden states y y0 , . . .", ", yn which will be mapped to POS tags during evalua tion as described above and then generating each word xi conditioned on its corresponding state yi .", "We insert endmarkers at the beginning and ending of the corpus and between sentence boundaries , and constrain the estimator to associate endmarkers with a state that never appears with any other observationtype this means each sentence can be processed in dependently by first order HMMs ; these endmarkers are ignored during evaluation . In more detail , the HMM is specified by multi nomials ? y and ? y for each hidden state y , where ? y specifies the distribution over states following y and ? y specifies the distribution over observations x given state y . yi yi ? 1 y ? Multi ? y xi yi y ? Multi ? y 1 We used the Forward Backward algorithm to perform Expectation Maximization , which is a procedure that iteratively re estimates the model param eters ? , ? , converging on a local maximum of the likelihood .", "Specifically , if the parameter estimate attime is ? , ? , then the re estimated parame ters at time 1 are ? 1 y ? y E ny ? , y E ny 2 ? 1 x y E nx , y E ny 6 . 95E 06 7 . 00E 06 7 . 05E 06 7 . 10E 06 7 . 15E 06 0 250 500 750 1000 ? lo g lik el ih oo d Iteration Figure 1 Variation in negative log likelihood with increasing iterations for 10 EM runs from different random starting points . where nx , y is the number of times observation x oc curs with state y , ny ? , y is the number of times state y ?", "follows y and ny is the number of occurences of state y ; all expectations are taken with respect to the model ? , ? . We took care to implement this and the other al gorithms used in this paper efficiently , since optimal performance was often only achieved after several hundred iterations .", "It is well known that EM oftentakes a large number of iterations to converge in likelihood , and we found this here too , as shown in Fig ure 1 .", "As that figure makes clear , likelihood is still increasing after several hundred iterations . Perhaps more surprisingly , we often found dramatic changes in accuracy in the order of 5 occur ing after several hundred iterations , so we ran 1 , 000 iterations of EM in all of the experiments describedhere ; each run took approximately 2 . 5 days compu tation on a 3 . 6GHz Pentium 4 .", "It ? s well known thataccuracy often decreases after the first few EM it erations which we also observed ; however in our experiments we found that performance improves again after 100 iterations and continues improving roughly monotonically .", "Figure 2 shows how 1 to 1 accuracy varies with iteration during 10 runs from different random starting points .", "Note that 1 to 1 accuracy at termination ranges from 0 . 38 to 0 . 45 ; a spread of 0 . 07 . We obtained a dramatic speedup by working directly with probabilities and rescaling after each ob servation to avoid underflow , rather than workingwith log probabilities thanks to Yoshimasa Tsu 298 0 . 35 0 . 37 0 . 39 0 . 41 0 . 43 0 . 45 0 . 47 0 250 500 750 10001 to 1 a ccura cy IterationFigure 2 Variation in 1 to 1 accuracy with increas ing iterations for 10 EM runs from different random starting points .", "ruoka for pointing this out .", "Since we evaluatedthe accuracy of the estimated tags after each iteration , it was important that decoding be done effi ciently as well .", "While most researchers use Viterbidecoding to find the most likely state sequence , maximum marginal decoding which labels the observa tion xi with the state yi that maximizes the marginal probability P yi x , ? , ? is faster because it re uses the forward and backward tables already constructed by the Forward Backward algorithm .", "Moreover , in separate experiments we found that the maximum marginal state sequence almost always scored higherthan the Viterbi state sequence in all of our evalua tions , and at modest numbers of iterations up to 50 often scored more than 5 better . We also noticed a wide variance in the perfor mance of models due to random initialization both ? and ? are initially jittered to break symmetry ; thiswide variance was observed with all of the estima tors investigated in this paper .", "This means we cannot compare estimators on the basis of single runs , so we ran each estimator 10 times from different random starting points and report both mean and standard deviation for all scores .", "Finally , we also experimented with annealing , in which the parameters ? and ? are raised to the power 1 T , where T is a ? temperature ?", "parameter that isslowly lowered toward 1 at each iteration accord ing to some ? annealing schedule ? .", "We experimented with a variety of starting temperatures and annealing schedules e . g . , linear , exponential , etc , but wereunable to find any that produced models whose like 0E 0 1E 5 2E 5 Fre quen cy Tag hidden state sorted by frequency PT B V B EM EM 25 Figure 3 The average number of words labeled with each hidden state or tag for the EM , VB with ? x ? y 0 . 1 and EM 25 estimators EM 25 is the EM estimator with 25 hidden states .", "lihoods were significantly higher i . e . , the models fit better than those found without annealing .", "The evaluation of the models produced by the EM and other estimators is presented in Table 1 .", "It is difficult to compare these with previous work , but Haghighi and Klein 2006 report that in a completely unsupervised setting , their MRF model , which uses a large set of additional features and amore complex estimation procedure , achieves an average 1 to 1 accuracy of 41 . 3 .", "Because they provide no information about the variance in this accuracy it is difficult to tell whether there is a signifi cant difference between their estimator and the EM estimator , but it is clear that when EM is run longenough , the performance of even very simple models like the bitag HMM is better than generally rec ognized . As Table 1 makes clear , the EM estimator pro duces models that are extremely competitive in many to 1 accuracy and Variation of Information , but are significantly worse in 1 to 1 accuracy .", "Wecan understand these results by comparing the dis tribution of words to hidden states to the distribution of words to POS tags in the gold standard evaluation corpus .", "As Figure 3 shows , the distribution of words to POS tags is highly skewed , with just 6 POS tags , NN , IN , NNP , DT , JJ and NNS , accounting for over 55 of the tokens in the corpus .", "By contrast , the EM distribution is much flatter .", "This also explains why the many to 1 accuracy is so much better than the one to one accuracy ; presumably several hidden 299 Estimator 1 to 1 Many to 1 VI H T Y H Y T EM 50 0 . 40 0 . 02 0 . 62 0 . 01 4 . 46 0 . 08 1 . 75 0 . 04 2 . 71 0 . 06 VB 0 . 1 , 0 . 1 50 0 . 47 0 . 02 0 . 50 0 . 02 4 . 28 0 . 09 2 . 39 0 . 07 1 . 89 0 . 06 VB 0 . 1 , 10 ? 4 50 0 . 46 0 . 03 0 . 50 0 . 02 4 . 28 0 . 11 2 . 39 0 . 08 1 . 90 0 . 07 VB 10 ? 4 , 0 . 1 50 0 . 42 0 . 02 0 . 60 0 . 01 4 . 63 0 . 07 1 . 86 0 . 03 2 . 77 0 . 05 VB 10 ? 4 , 10 ? 4 50 0 . 42 0 . 02 0 . 60 0 . 01 4 . 62 0 . 07 1 . 85 0 . 03 2 . 76 0 . 06 GS 0 . 1 , 0 . 1 50 0 . 37 0 . 02 0 . 51 0 . 01 5 . 45 0 . 07 2 . 35 0 . 09 3 . 20 0 . 03 GS 0 . 1 , 10 ? 4 50 0 . 38 0 . 01 0 . 51 0 . 01 5 . 47 0 . 04 2 . 26 0 . 03 3 . 22 0 . 01 GS 10 ? 4 , 0 . 1 50 0 . 36 0 . 02 0 . 49 0 . 01 5 . 73 0 . 05 2 . 41 0 . 04 3 . 31 0 . 03 GS 10 ? 4 , 10 ? 4 50 0 . 37 0 . 02 0 . 49 0 . 01 5 . 74 0 . 03 2 . 42 0 . 02 3 . 32 0 . 02 EM 40 0 . 42 0 . 03 0 . 60 0 . 02 4 . 37 0 . 14 1 . 84 0 . 07 2 . 55 0 . 08 EM 25 0 . 46 0 . 03 0 . 56 0 . 02 4 . 23 0 . 17 2 . 05 0 . 09 2 . 19 0 . 08 EM 10 0 . 41 0 . 01 0 . 43 0 . 01 4 . 32 0 . 04 2 . 74 0 . 03 1 . 58 0 . 05 Table 1 Evaluation of models produced by the various estimators .", "The values of the Dirichlet prior param eters for ? x and ? y appear in the estimator name for the VB and GS estimators , and the number of hidden states is given in parentheses .", "Reported values are means over all runs , followed by standard deviations .", "10 runs were performed for each of the EM and VB estimators , while 5 runs were performed for the GSestimators .", "Each EM and VB run consisted of 1 , 000 iterations , while each GS run consisted of 50 , 000 it erations .", "For the estimators with 10 runs , a 3 standard error 95 confidence interval is approximately the same as the standard deviation .", "states are being mapped onto a single POS tag .", "This is also consistent with the fact that the cross entropy H T Y of tags given hidden states is relatively low i . e . , given a hidden state , the tag is relatively predictable , while the cross entropy H Y T is rela tively high .", "and Variational Bayes A Bayesian estimator combines a likelihood termP x ? , ? and a prior P ? , ? to estimate the poste rior probability of a model or hidden state sequence .", "We can use a Bayesian prior to bias our estimatortowards models that generate more skewed distributions .", "Because HMMs and PCFGs are prod ucts of multinomials , Dirichlet distributions are a particularly natural choice for the priors since they are conjugate to multinomials , which simplifies both the mathematical and computational aspects of theproblem .", "The precise form of the model we investi gated is ? y ? y ? Dir ? y ? y ? x ? Dir ? x yi yi ? 1 y ? Multi ? y xi yi y ? Multi ? y Informally , ? y controls the sparsity of the state to state transition probabilities while ? x controls thesparsity of the state to observation emission proba bilities .", "As ? x approaches zero the prior strongly prefers models in which each hidden state emitsas few words as possible .", "This captures the intu ition that most word types only belong to one POS , since the minimum number of non zero state toobservation transitions occurs when each observa tion type is emitted from only one state .", "Similarly , as ? y approaches zero the state to state transitions become sparser . There are two main techniques for Bayesian esti mation of such models Markov Chain Monte Carlo MCMC and Variational Bayes VB .", "MCMC en compasses a broad range of sampling techniques , including component wise Gibbs sampling , which is the MCMC technique we used here Robert and Casella , 2004 ; Bishop , 2006 .", "In general , MCMCtechniques do not produce a single model that char acterizes the posterior , but instead produce a stream of samples from the posterior .", "The application of MCMC techniques , including Gibbs sampling , to HMM inference problems is relatively well known see Besag 2004 for a tutorial introduction and Goldwater and Griffiths 2007 for an applicationof Gibbs sampling to HMM inference for semi 300 supervised and unsupervised POS tagging .", "The Gibbs sampler produces state sequences y sampled from the posterior distribution P y x , ? ?", "P x , y ? , ? P ? ? y P ? ? x d ?", "d ? Because Dirichlet priors are conjugate to multinomials , it is possible to integrate out the model parameters ? and ? to yield the conditional distribu tion for yi shown in Figure 4 .", "For each observation xi in turn , we resample its state yi conditioned on the states y ? i of the other observations ; eventually the distribution of state sequences converges to the desired posterior .", "Each iteration of the Gibbs sampler is much faster than the Forward Backward algorithm both take time linear in the length of the string , but for an HMM with s hidden states , each iteration of the Gibbs sampler takes O s time while each iteration of the Forward Backward algorithm takes O s2 time , so we ran 50 , 000 iterations of all samplers which takes roughly the same elapsed time as 1 , 000 Forward Backward iterations .", "As can be seen from Table 1 , the posterior state sequences we obtained are not particularly good . Further , when we examined how the posterior like lihoods varied with increasing iterations of Gibbs sampling , it became apparent that the likelihood was still increasing after 50 , 000 iterations .", "Moreover , when comparing posterior likelihoods from different runs with the same prior parameters but differ ent random number seeds , none of the likelihoods crossed , which one would expect if the samplers had converged and were mixing well Robert and Casella , 2004 .", "Just as with EM , we experimented with a variety of annealing regimes , but were unable to find any which significantly improved accuracy or posterior likelihood . We also experimented with evaluating state se quences found using maximum posterior decoding i . e . , model parameters are estimated from the posterior sample , and used to perform maximum posterior decoding rather than the samples from the pos terior produced by the Gibbs sampler .", "We found that the maximum posterior decoding sequences usually scored higher than the posterior samples , but the scores converged after the first thousand iterations . Since the posterior samples are produced as a byproduct of Gibbs sampling while maximum poste rior decoding requires an additional time consuming step that does not have much impact on scores , we used the posterior samples to produce the results in Table 1 . In contrast to MCMC , Variational Bayesian in ference attempts to find the function Q y , ? , ? thatminimizes an upper bound of the negative log likeli hood Jordan et al , 1999 ? log P x ? log ? Q y , ? , ? P x , y , ? , ? Q y , ? , ? dy d ?", "d ? ? ?", "Q y , ? , ? log P x , y , ? , ? Q y , ? , ? dy d ?", "d ? 3 The upper bound in 3 is called the Variational Free Energy .", "We make a ? mean field ?", "assumption thatthe posterior can be well approximated by a factor ized modelQ in which the state sequence y does not covary with the model parameters ? , ?", "this will be true if , for example , there is sufficient data that the posterior distribution has a peaked mode P x , y , ? , ? ?", "Q y , ? , ? Q1 y Q2 ? , ? The calculus of variations is used to minimize theKL divergence between the desired posterior distri bution and the factorized approximation .", "It turnsout that if the likelihood and conjugate prior be long to exponential families then the optimalQ1 andQ2 do too , and there is an EM like iterative pro cedure that finds locally optimal model parameters Bishop , 2006 .", "This procedure is especially attractive for HMMinference , since it involves only a minor modification to the M step of the Forward Backward algo rithm .", "MacKay 1997 and Beal 2003 describe Variational Bayesian VB inference for HMMs in detail , and Kurihara and Sato 2006 describe VBfor PCFGs which only involves a minor modifica tion to the M step of the Inside Outside algorithm .", "Specifically , the E step for VB inference for HMMs is the same as in EM , while theM step is as follows ? ? 1 y ? y f E ny ? , y ? y f E ny s ? y 4 ? ? 1 x y f E nx , y ? x f E ny m ? x f v exp ? v ? v v 7 ? g v ? 12 ? v 1 ?", "1 v g x ? log x 0 . 04167x ? 2 0 . 00729x ? 4 0 . 00384x ? 6 ? 0 . 00413x ? 8 . . .", "5 301 P yi x , y ? i , ? ?", "nxi , yi ? x nyi m ? x nyi , yi ? 1 ? y nyi ? 1 s ? y nyi 1 , yi I yi ? 1 yi yi 1 ? y nyi I yi ? 1 yi Figure 4 The conditional distribution for state yi used in the Gibbs sampler , which conditions on the states y ? i for all observations except xi .", "Here m is the number of possible observations i . e . , the size of the vocabulary , s is the number of hidden states and I ?", "is the indicator function i . e . , equal to one if its argument is true and zero otherwise , nx , y is the number of times observation x occurs with state y , ny ? , y is the number of times state y ?", "follows y , and ny is the number of times state y occurs ; these counts are from x ? i , y ? i , i . e . , excluding xi and yi .", "0 1 2 0 1 2 Figure 5 The scaling function y f x exp ? x curved line , which is bounded above by the line y x and below by the line y x ?", "where ? is the digamma function the derivative ofthe log gamma function ; 5 gives an asymptotic ap proximation , and the remaining quantities are just as in the EM updates 2 , i . e . , nx , y is the number of times observation x occurs with state y , ny ? , y is the number of times state y ?", "follows y , ny is the number of occurences of state y , s is the number of hiddenstates and m is the number of observations ; all ex pectations are taken with respect to the variational parameters ? ? , ? ? . A comparison between 4 and 2 reveals two dif ferences between the EM and VB updates .", "First , the Dirichlet prior parameters ? are added to the expected counts .", "Second , these posterior counts which are in fact parameters of the Dirichlet pos terior Q2 are passed through the function f v exp ? v , which is plotted in Figure 5 .", "When v 0 , f v ? v ? 0 . 5 , so roughly speaking , VB for multinomials involves adding ? ? 0 . 5 to the expected counts when they are much larger than zero , where ? is the Dirichlet prior parameter .", "Thus VB canbe viewed as a more principled version of the well known ad hoc technique for approximating Bayesian estimation with EM that involves adding ? ? 1 to the expected counts .", "However , in the ad hoc approach the expected count plus ? ? 1 may be less than zero , resulting in a value of zero for the corresponding parameter Johnson et al , 2007 ; Goldwater and Grif fiths , 2007 .", "VB avoids this problem because f v is always positive when v 0 , even when v is small .", "Note that because the counts are passed through f , the updated values for ? ?", "and ? ?", "in 4 are in general not normalized ; this is because the variational free energy is only an upper bound on the negative log likelihood Beal , 2003 . We found that in general VB performed much bet ter than GS .", "Computationally it is very similar to EM , and each iteration takes essentially the same time as an EM iteration .", "Again , we experimented with annealing in the hope of speeding convergence , but could not find an annealing schedule that significantly lowered the variational free energy the quan tity that VB optimizes .", "While we had hoped that theBayesian prior would bias VB toward a common solution , we found the same sensitivity to initial condi tions as we found with EM , so just as for EM , we ran the estimator for 1 , 000 iterations with 10 different random initializations for each combination of prior parameters .", "Table 1 presents the results of VB runs with several different values for the Dirichlet priorparameters .", "Interestingly , we obtained our best per formance on 1 to 1 accuracy when the Dirchlet prior ? x 0 . 1 , a relatively large number , but best per formance on many to 1 accuracy was achieved with a much lower value for the Dirichlet prior , namely ? x 10 ? 4 .", "The Dirichlet prior ? y that controls 302sparsity of the state to state transitions had little ef fect on the results .", "We did not have computational resources to fully explore other values for the prior a set of 10 runs for one set of parameter values takes 25 computer days .", "As Figure 3 shows , VB can produce distributions of hidden states that are peaked in the same way that POS tags are .", "In fact , with the priors used here , VB produces state sequences in which only a subset ofthe possible HMM states are in fact assigned to ob servations .", "This shows that rather than fixing the number of hidden states in advance , the Bayesian prior can determine the number of states ; this idea is more fully developed in the infinite HMM of Beal et al .", "2002 and Teh et al 2006 .", "EM already performs well in terms of the many to 1 accuracy , but we wondered if there might be some way to improve its 1 to 1 accuracy and VI score .", "In section 3 we suggested that one reason for its poorperformance in these evaluations is that the distri butions of hidden states it finds tend to be fairly flat , compared to the empirical distribution of POS tags .", "As section 4 showed , a suitable Bayesian priorcan bias the estimator towards more peaked distribu tions , but we wondered if there might be a simpler way of achieving the same result .", "We experimented with dramatic reductions in the number of hidden states in the HMMs estimated by EM .", "This should force the hidden states to bemore densely populated and improve 1 to 1 accu racy , even though this means that there will be nohidden states that can possibly map onto the less fre quent POS tags i . e . , we will get these words wrong .", "In effect , we abandon the low frequency POS tags in the hope of improving the 1 to 1 accuracy of the high frequency tags .", "As Table 1 shows , this markedly improves both the 1 to 1 accuracy and the VI score .", "A 25 state HMM estimated by EM performs effectively as wellas the best VB model in terms of both 1 to 1 accu racy and VI score , and runs 4 times faster because it has only half the number of hidden states .", "6 Conclusion and future work .", "This paper studied why EM seems to do so badly in HMM estimation for unsupervised POS tagging .", "In fact , we found that it doesn ? t do so badly at all thebitag HMM estimated by EM achieves a mean 1 to 1 tagging accuracy of 40 , which is approximately the same as the 41 . 3 reported by Haghighi and Klein , 2006 for their sophisticated MRF model .", "Then we noted the distribution of words to hidden states found by EM is relatively uniform , comparedto the distribution of words to POS tags in the eval uation corpus .", "This provides an explanation of why the many to 1 accuracy of EM is so high while the 1 to 1 accuracy and VI of EM is comparatively low .", "We showed that either by using a suitable Bayesian prior or by simply reducing the number of hidden states it is possible to significantly improve both the 1 to 1 accuracy and the VI score , achieving a 1 to 1 tagging accuracy of 46 .", "We also showed that EM and other estimators take much longer to converge than usually thought , and often require several hundred iterations to achieve optimal performance .", "We also found that there is considerable variance in the performance of all of these estimators , so in general multiple runs fromdifferent random starting points are necessary in or der to evaluate an estimator ? s performance .", "Finally , there may be more sophisticated ways of improving the 1 to 1 accuracy and VI score thanthe relatively crude methods used here that primarily reduce the number of available states .", "For example , we might obtain better performance by us ing EM to infer an HMM with a large number of states , and then using some kind of distributionalclustering to group similar HMM states ; these clusters , rather than the underlying states , would be in terpreted as the POS tag labels .", "Also , the Bayesian framework permits a wide variety of different priors besides Dirichlet priors explored here .", "For example , it should be possible to encode linguistic knowledge such markedness preferences in a prior , and there are other linguistically uninformative priors , such the ? entropic priors ?", "of Brand 1999 , that may be worth exploring .", "AcknowledgementsI would like to thank Microsoft Research for providing an excellent environment in which to con duct this work , and my friends and colleagues at Microsoft Research , especially Bob Moore , ChrisQuirk and Kristina Toutanova , for their helpful com ments on this paper ."], "summary_lines": ["Why Doesn't EM Find Good HMM POS-Taggers?\n", "This paper investigates why the HMMs estimated by Expectation-Maximization (EM) produce such poor results as Part-of-Speech (POS) taggers.\n", "We find that the HMMs estimated by EM generally assign a roughly equal number of word tokens to each hidden state, while the empirical distribution of tokens to POS tags is highly skewed.\n", "This motivates a Bayesian approach using a sparse prior to bias the estimator toward such a skewed distribution.\n", "We investigate Gibbs Sampling (GS) and Variational Bayes (VB) estimators and show that VB converges faster than GS for this task and that VB significantly improves 1-to-1 tagging accuracy over EM.\n", "We also show that EM does nearly as well as VB when the number of hidden HMM states is dramatically reduced.\n", "We also point out the high variance in all of these estimators, and that they require many more iterations to approach convergence than usually thought.\n", "we demonstrate good performance on unsupervised part-of-speech tagging (using a dictionary) with a Bayesian model.\n"]}
{"article_lines": ["An Unsupervised Approach To Recognizing Discourse Relations", "We present an unsupervised approach to discourse relations of hold between arbitrary spans of texts .", "We show that discourse relation classifiers trained on examples that are automatically extracted from massive amounts of text can be used to distinguish between some of these relations with accuracies as high as 93 , even when the relations are not explicitly marked by cue phrases .", "In the field of discourse research , it is now widely agreed that sentences clauses are usually not understood in isolation , but in relation to other sentences clauses .", "Given the high level of interest in explaining the nature of these relations and in providing definitions for them Mann and Thompson , 1988 ; Hobbs , 1990 ; Martin , 1992 ; Lascarides and Asher , 1993 ; Hovy and Maier , 1993 ; Knott and Sanders , 1998 , it is surprising that there are no robust programs capable of identifying discourse relations that hold between arbitrary spans of text .", "Consider , for example , the sentence clause pairs below . from the sale of expensive , high technology systems like laser designated missiles , aircraft electronic warfare systems , tactical radios , anti radiation bombs and battlefield mobility systems .", "In these examples , the discourse markers But and because help us figure out that a CONTRAST relation holds between the text spans in 1 and an EXPLANATION EVIDENCE relation holds between the spans in 2 .", "Unfortunately , cue phrases do not signal all relations in a text .", "In the corpus of Rhetorical Structure trees www . isi . edu marcu discourse built by Carlson et al . 2001 , for example , we have observed that only 61 of 238 CONTRAST relations and 79 out of 307 EXPLANATION EVIDENCE relations that hold between two adjacent clauses were marked by a cue phrase .", "So what shall we do when no discourse markers are used ?", "If we had access to robust semantic interpreters , we could , for example , infer from sentence 1 . a that cannot buy arms legally libya , infer from sentence 1 . b that can buy arms legally rwanda , use our background knowledge in order to infer that similar libya , rwanda , and apply Hobbs s 1990 definitions of discourse relations to arrive at the conclusion that a CONTRAST relation holds between the sentences in 1 .", "Unfortunately , the state of the art in NLP does not provide us access to semantic interpreters and general purpose knowledge bases that would support these kinds of inferences .", "The discourse relation definitions proposed by others Mann and Thompson , 1988 ; Lascarides and Asher , 1993 ; Knott and Sanders , 1998 are not easier to apply either because they assume the ability to automatically derive , in addition to the semantics of the text spans , the intentions and illocutions associated with them as well .", "In spite of the difficulty of determining the discourse relations that hold between arbitrary text spans , it is clear that such an ability is important in many applications .", "First , a discourse relation recognizer would enable the development of improved discourse parsers and , consequently , of high performance single document summarizers Marcu , 2000 .", "In multidocument summarization DUC , 2002 , it would enable the development of summarization programs capable of identifying contradictory statements both within and across documents and of producing summaries that reflect not only the similarities between various documents , but also their differences .", "In question answering , it would enable the development of systems capable of answering sophisticated , non factoid queries , such as what were the causes ofX ? or what contradicts Y ? , which are beyond the state of the art of current systems TREC , 2001 .", "In this paper , we describe experiments aimed at building robust discourse relation classification systems .", "To build such systems , we train a family of Naive Bayes classifiers on a large set of examples that are generated automatically from two corpora a corpus of 41 , 147 , 805 English sentences that have no annotations , and BLIPP , a corpus of 1 , 796 , 386 automatically parsed English sentences Charniak , 2000 , which is available from the Linguistic Data Consortium www . ldc . upenn . edu .", "We study empirically the adequacy of various features for the task of discourse relation classification and we show that some discourse relations can be correctly recognized with accuracies as high as 93 .", "In order to build a discourse relation classifier , one first needs to decide what relation definitions one is going to use .", "In Section 1 , we simply relied on the reader s intuition when we claimed that a CONTRAST relation holds between the sentences in 1 .", "In reality though , associating a discourse relation with a text span pair is a choice that is clearly influenced by the theoretical framework one is willing to adopt .", "If we adopt , for example , Knott and Sanders s 1998 account , we would say that the relation between sentences 1 . a and 1 . b is ADDITIVE , because no causal connection exists between the two sentences , PRAGMATIC , because the relation pertains to illocutionary force and not to the propositional content of the sentences , and NEGATIVE , because the relation involves a CONTRAST between the two sentences .", "In the same framework , the relation between clauses 2 . a and 2 . b will be labeled as CAUSAL SEMANTICPOSITIVE NONBASIC .", "In Lascarides and Asher s theory 1993 , we would label the relation between 2 . a and 2 . b as EXPLANATION because the event in 2 . b explains why the event in 2 . a happened perhaps by CAUSING it .", "In Hobbs s theory 1990 , we would also label the relation between 2 . a and 2 . b as EXPLANATION because the event asserted by 2 . b CAUSED or could CAUSE the event asserted in 2 . a .", "And in Mann and Thompson theory 1988 , we would label sentence pairs 1 . a , 1 . b as CONTRAST because the situations presented in them are the same in many respects the purchase of arms , because the situations are different in some respects Libya cannot buy arms legally while Rwanda can , and because these situations are compared with respect to these differences .", "By a similar line of reasoning , we would label the relation between 2 . a and 2 . b as EVIDENCE .", "The discussion above illustrates two points .", "First , it is clear that although current discourse theories are built on fundamentally different principles , they all share some common intuitions .", "Sure , some theories talk about negative polarity while others about contrast .", "Some theories refer to causes , some to potential causes , and some to explanations .", "But ultimately , all these theories acknowledge that there are such things as CONTRAST , CAUSE , and EXPLANATION relations .", "Second , given the complexity of the definitions these theories propose , it is clear why it is difficult to build programs that recognize such relations in unrestricted texts .", "Current NLP techniques do not enable us to reliably infer from sentence 1 . a that cannot buy arms legally libya and do not give us access to general purpose knowledge bases that assert that similar libya , rwanda .", "The approach we advocate in this paper is in some respects less ambitious than current approaches to discourse relations because it relies upon a much smaller set of relations than those used by Mann and Thompson 1988 or Martin 1992 .", "In our work , we decide to focus only on four types of relations , which we call CONTRAST , CAUSE EXPLANATIONEVIDENCE CEV , CONDITION , and ELABORATION .", "We define these relations in Section 2 . 2 .", "In other respects though , our approach is more ambitious because it focuses on the problem of recognizing such discourse relations in unrestricted texts .", "In other words , given as input sentence pairs such as those shown in 1 2 , we develop techniques and programs that label the relations that hold between these sentence pairs as CONTRAST , CAUSEEXPLANATION EVIDENCE , CONDITION , ELABORATION or NONE OF THE ABOVE , even when the discourse relations are not explicitly signalled by discourse markers .", "The discourse relations we focus on are defined at a much coarser level of granularity than in most discourse theories .", "For example , we consider that a CONTRAST relation holds between two text spans if one of the following relations holds CONTRAST , ANTITHESIS , CONCESSION , or OTHERWISE , as defined by Mann and Thompson 1988 , CONTRAST or VIOLATED EXPECTATION , as defined by Hobbs 1990 , or any of the relations characterized by this regular expression of cognitive primitives , as defined by Knott and Sanders 1998 CAUSAL ADDITIVE SEMANTIC PRAGMATIC NEGATIVE .", "In other words , in our approach , we do not distinguish between contrasts of semantic and pragmatic nature , contrasts specific to violated expectations , etc .", "Table 1 shows the definitions of the relations we considered .", "The advantage of operating with coarsely defined discourse relations is that it enables us to automatically construct relatively low noise datasets that can be used for learning .", "For example , by extracting sentence pairs that have the keyword But at the beginning of the second sentence , as the sentence pair shown in 1 , we can automatically collect many examples of CONTRAST relations .", "And by extracting sentences that contain the keyword because , we can automatically collect many examples of CAUSE EXPLANATION EVIDENCE relations .", "As previous research in linguistics Halliday and Hasan , 1976 ; Schiffrin , 1987 and computational linguistics Marcu , 2000 show , some occurrences of but and because do not have a discourse function ; and others signal other relations than CONTRAST and CAUSE EXPLANATION .", "So we can expect the examples we extract to be noisy .", "However , empirical work of Marcu 2000 and Carlson et al . 2001 suggests that the majority of occurrences of but , for example , do signal CONTRAST relations .", "In the RST corpus built by Carlson et al . 2001 , 89 out of the 106 occurrences of but that occur at the beginning of a sentence signal a CONTRAST relation that holds between the sentence that contains the word but and the sentence that precedes it .", "Our hope is that simple extraction methods are sufficient for collecting low noise training corpora .", "In order to collect training cases , we mined in an unsupervised manner two corpora .", "The first corpus , which we call Raw , is a corpus of 1 billion words of unannotated English 41 , 147 , 805 sentences that we created by catenating various corpora made available over the years by the Linguistic Data Consortium .", "The second , called BLIPP , is a corpus of only 1 , 796 , 386 sentences that were parsed automatically by Charniak 2000 .", "We extracted from both corpora all adjacent sentence pairs that contained the cue phrase But at the beginning of the second sentence and we automatically labeled the relation between the two sentence pairs as CONTRAST .", "We also extracted all the sentences that contained the word but in the middle of a sentence ; we split each extracted sentence into two spans , one containing the words from the beginning of the sentence to the occurrence of the keyword but and one containing the words from the occurrence of but to the end of the sentence ; and we labeled the relation between the two resulting text spans as CONTRAST as well .", "Table 2 lists some of the cue phrases we used in order to extract CONTRAST , CAUSEEXPLANATION EVIDENCE , ELABORATION , and CONDITION relations and the number of examples extracted from the Raw corpus for each type of discourse relation .", "In the patterns in Table 2 , the symbols BOS and EOS denote BeginningOfSentence and EndOfSentence boundaries , the stand for occurrences of any words and punctuation marks , the square brackets stand for text span boundaries , and the other words and punctuation marks stand for the cue phrases that we used in order to extract discourse relation examples .", "For example , the pattern BOS Although , EOS is used in order to extract examples of CONTRAST relations that hold between a span of text delimited to the left by the cue phrase Although occurring in the beginning of a sentence and to the right by the first occurrence of a comma , and a span of text that contains the rest of the sentence to which Although belongs .", "We also extracted automatically 1 , 000 , 000 examples of what we hypothesize to be non relations , by randomly selecting non adjacent sentence pairs that are at least 3 sentences apart in a given text .", "We label such examples NO RELATION SAME TEXT .", "And we extracted automatically 1 , 000 , 000 examples of what we hypothesize to be cross document nonrelations , by randomly selecting two sentences from distinct documents .", "As in the case of CONTRAST and CONDITION , the NO RELATION examples are also noisy because long distance relations are common in well written texts .", "We hypothesize that we can determine that a CONTRAST relation holds between the sentences in 3 even if we cannot semantically interpret the two sentences , simply because our background knowledge tells us that good and fails are good indicators of contrastive statements .", "John is good in math and sciences .", "3 Paul fails almost every class he takes .", "Similarly , we hypothesize that we can determine that a CONTRAST relation holds between the sentences in 1 , because our background knowledge tells us that embargo and legally are likely to occur in contexts of opposite polarity .", "In general , we hypothesize that lexical item pairs can provide clues about the discourse relations that hold between the text spans in which the lexical items occur .", "To test this hypothesis , we need to solve two problems .", "First , we need a means to acquire vast amounts of background knowledge from which we can derive , for example , that the word pairs good fails and embargo legally are good indicators of CONTRAST relations .", "The extraction patterns described in Table 2 enable us to solve this problem . 1 Second , given vast amounts of training material , we need a means to learn which pairs of lexical items are likely to co occur in conjunction with each discourse relation and a means to apply the learned parameters to any pair of text spans in order to determine the discourse relation that holds between them .", "We solve the second problem in a Bayesian probabilistic framework .", "We assume that a discourse relation that holds between two text spans , , is determined by the word pairs in the cartesian product defined over the words in the two text spans .", "In general , a word pair can signal any relation .", "We determine the most likely discourse relation that holds between two text spans and by taking the maximum over , which according to Bayes rule , amounts to taking the maximum over .", "If we assume that the word pairs in the cartesian product are independent , is equivalent to .", "The values are computed using maximum likelihood estimators , which are smoothed using the Laplace method Manning and Sch utze , 1999 .", "For each discourse relation pair , we train a word pair based classifier using the automatically derived training examples in the Raw corpus , from which we first removed the cue phrases used for extracting the examples .", "This ensures that our classifiers do not learn , for example , that the word pair if then is a good indicator of a CONDITION relation , which would simply amount to learning to distinguish between the extraction patterns used to construct the corpus .", "We test each classifier on a test corpus of 5000 examples labeled with and 5000 examples labeled with , which ensures that the baseline is the same for all combinations and , namely 50 .", "Table 3 shows the performance of all discourse relation classifiers .", "As one can see , each classifier outperforms the 50 baseline , with some classifiers being as accurate as that that distinguishes between CAUSE EXPLANATION EVIDENCE and ELABORATION relations , which has an accuracy of 93 .", "We have also built a six way classifier to distinguish between all six relation types .", "This classifier has a performance of 49 . 7 , with a baseline of 16 . 67 , which is achieved by labeling all relations as CONTRASTS .", "We also examined the learning curves of various classifiers and noticed that , for some of them , the addition of training examples does not appear to have a significant impact on their performance .", "For example , the classifier that distinguishes between CONTRAST and CAUSE EXPLANATION EVIDENCE relations has an accuracy of 87 . 1 when trained on 2 , 000 , 000 examples and an accuracy of 87 . 3 when trained on 4 , 771 , 534 examples .", "We hypothesized that the flattening of the learning curve is explained by the noise in our training data and the vast amount of word pairs that are not likely to be good predictors of discourse relations .", "To test this hypothesis , we decided to carry out a second experiment that used as predictors only a subset of the word pairs in the cartesian product defined over the words in two given text spans .", "To achieve this , we used the patterns in Table 2 to extract examples of discourse relations from the BLIPP corpus .", "As expected , the BLIPP corpus yielded much fewer learning cases 185 , 846 CONTRAST ; 44 , 776 CAUSE EXPLANATION EVIDENCE ; 55 , 699 CONDITION ; and 33 , 369 ELABORATION relations .", "To these examples , we added 58 , 000 NO RELATION SAME TEXT and 58 , 000 NO RELATION DIFFERENT TEXTS relations .", "To each text span in the BLIPP corpus corresponds a parse tree Charniak , 2000 .", "We wrote a simple program that extracted the nouns , verbs , and cue phrases in each sentence clause .", "We call these the most representative words of a sentence discourse unit .", "For example , the most representative words of the sentence in example 4 , are those shown in italics .", "Italy s unadjusted industrial production fell in Jan 4 uary 3 . 4 from a year earlier but rose 0 . 4 from December , the government said We repeated the experiment we carried out in conjunction with the Raw corpus on the data derived from the BLIPP corpus as well .", "Table 4 summarizes the results .", "Overall , the performance of the systems trained on the most representative word pairs in the BLIPP corpus is clearly lower than the performance of the systems trained on all the word pairs in the Raw corpus .", "But a direct comparison between two classifiers trained on different corpora is not fair because with just 100 , 000 examples per relation , the systems trained on the Raw corpus are much worse than those trained on the BLIPP data .", "The learning curves in Figure 1 are illuminating as they show that if one uses as features only the most representative word pairs , one needs only about 100 , 000 training examples to achieve the same level of performance one achieves using 1 , 000 , 000 training examples and features defined over all word pairs .", "Also , since the learning curve for the BLIPP corpus is steeper than the learning curve for the Raw corpus , this suggests that discourse relation classifiers trained on most representative word pairs and millions of training examples can achieve higher levels of performance than classifiers trained on all word pairs unannotated data .", "The results in Section 3 indicate clearly that massive amounts of automatically generated data can be used to distinguish between discourse relations defined as discussed in Section 2 . 2 .", "What the experiments manually labeled RST relations that hold between elementary discourse units .", "Performance results are shown in bold ; baselines are shown in normal fonts . in Section 3 do not show is whether the classifiers built in this manner can be of any use in conjunction with some established discourse theory .", "To test this , we used the corpus of discourse trees built in the style of RST by Carlson et al . 2001 .", "We automatically extracted from this manually annotated corpus all CONTRAST , CAUSE EXPLANATION EVIDENCE , CONDITION and ELABORATION relations that hold between two adjacent elementary discourse units .", "Since RST Mann and Thompson , 1988 employs a finer grained taxonomy of relations than we used , we applied the definitions shown in Table 1 .", "That is , we considered that a CONTRAST relation held between two text spans if a human annotator labeled the relation between those spans as ANTITHESIS , CONCESSION , OTHERWISE or CONTRAST .", "We retrained then all classifiers on the Raw corpus , but this time without removing from the corpus the cue phrases that were used to generate the training examples .", "We did this because when trying to determine whether a CONTRAST relation holds between two spans of texts separated by the cue phrase but , for example , we want to take advantage of the cue phrase occurrence as well .", "We employed our classifiers on the manually labeled examples extracted from Carlson et al . s corpus 2001 .", "Table 5 displays the performance of our two way classifiers for relations defined over elementary discourse units .", "The table displays in the second row , for each discourse relation , the number of examples extracted from the RST corpus .", "For each binary classifier , the table lists in bold the accuracy of our classifier and in non bold font the majority baseline associated with it .", "The results in Table 5 show that the classifiers learned from automatically generated training data can be used to distinguish between certain types of RST relations .", "For example , the results show that the classifiers can be used to distinguish between CONTRAST and CAUSE EXPLANATION EVIDENCE relations , as defined in RST , but not so well between ELABORATION and any other relation .", "This result is consistent with the discourse model proposed by Knott et al . 2001 , who suggest that ELABORATION relations are too ill defined to be part of any discourse theory .", "The analysis above is informative only from a machine learning perspective .", "From a linguistic perspective though , this analysis is not very useful .", "If no cue phrases are used to signal the relation between two elementary discourse units , an automatic discourse labeler can at best guess that an ELABORATION relation holds between the units , because ELABORATION relations are the most frequently used relations Carlson et al . , 2001 .", "Fortunately , with the classifiers described here , one can label some of the unmarked discourse relations correctly .", "For example , the RST annotated corpus of Carlson et al . 2001 contains 238 CONTRAST relations that hold between two adjacent elementary discourse units .", "Of these , only 61 are marked by a cue phrase , which means that a program trained only on Carlson et al . s corpus could identify at most 61 238 of the CONTRAST relations correctly .", "Because Carlson et al . s corpus is small , all unmarked relations will be likely labeled as ELABORATIONs .", "However , when we run our CONTRAST vs . ELABORATION classifier on these examples , we can label correctly 60 of the 61 cue phrase marked relations and , in addition , we can also label 123 of the 177 relations that are not marked explicitly with cue phrases .", "This means that our classifier contributes to an increase in accuracy from to ! ! !", "Similarly , out of the 307 CAUSE EXPLANATION EVIDENCE relations that hold between two discourse units in Carlson et al . s corpus , only 79 are explicitly marked .", "A program trained only on Carlson et al . s corpus , would , therefore , identify at most 79 of the 307 relations correctly .", "When we run our CAUSEEXPLANATION EVIDENCE vs . ELABORATION classifier on these examples , we labeled correctly 73 of the 79 cue phrase marked relations and 102 of the 228 unmarked relations .", "This corresponds to an increase in accuracy from to .", "In a seminal paper , Banko and Brill 2001 have recently shown that massive amounts of data can be used to significantly increase the performance of confusion set disambiguators .", "In our paper , we show that massive amounts of data can have a major impact on discourse processing research as well .", "Our experiments show that discourse relation classifiers that use very simple features achieve unexpectedly high levels of performance when trained on extremely large data sets .", "Developing lower noise methods for automatically collecting training data and discovering features of higher predictive power for discourse relation classification than the features presented in this paper appear to be research avenues that are worthwhile to pursue .", "Over the last thirty years , the nature , number , and taxonomy of discourse relations have been among the most controversial issues in text discourse linguistics .", "This paper does not settle the controversy .", "Rather , it raises some new , interesting questions because the lexical patterns learned by our algorithms can be interpreted as empirical proof of existence for discourse relations .", "If text production was not governed by any rules above the sentence level , we should have not been able to improve on any of the baselines in our experiments .", "Our results suggest that it may be possible to develop fully automatic techniques for defining empirically justified discourse relations .", "Acknowledgments .", "This work was supported by the National Science Foundation under grant number IIS 0097846 and by the Advanced Research and Development Activity ARDA s Advanced Question Answering for Intelligence AQUAINT Program under contract number MDA908 02 C 0007 ."], "summary_lines": ["An Unsupervised Approach To Recognizing Discourse Relations\n", "We present an unsupervised approach to recognizing discourse relations of CONTRAST, EXPLANATION-EVIDENCE, CONDITION and ELABORATION that hold between arbitrary spans of texts.\n", "We show that discourse relation classifiers trained on examples that are automatically extracted from massive amounts of text can be used to distinguish between some of these relations with accuracies as high as 93%, even when the relations are not explicitly marked by cue phrases.\n", "We use a pattern based approach to extract instances of discourse relations such as Contrast and Elaboration from unlabeled corpora.\n", "We propose a method to identify discourse relations between text segments using Naive Bayes classifiers trained on a huge corpus.\n"]}
{"article_lines": ["Edge Based Best First Chart Parsing", "Best first probabilistic chart parsing attempts to parse efficiently by working on edges that are judged quot ; best quot ; by some probabilistic figure of merit FOM .", "Recent work has used probabilistic context free grammars PCFGs to assign probabilities to constituents , and to use these probabilities as the starting point for the FOM .", "This paper extends this approach to using a probabilistic FOM to judge edges incomplete constituents , thereby giving a much finergrained control over parsing effort .", "We show how this can be accomplished in a particularly simple way using the common idea of binarizing the PCFG .", "The results obtained are about a facof twenty improvement over the best results that is , our parser achieves equivalent results using one twentieth the number of edges .", "Furthermore we show that this improvement is obtained with parsing precision and recall levels superior to those achieved by exhaustive parsing .", "Finding one or all parses for a sentence according to a context free grammar requires search .", "Fortunately , there are well known 0 n3 algorithms for parsing , where n is the length of the sentence .", "Unfortunately , for large grammars such as the PCFG induced from the Penn II WSJ corpus , which contains around 1 . 6 104 rules and longish sentences say , 40 words and punctuation , even 0 713 looks pretty bleak .", "One well known 0 n3 parsing method Kay , 1980 is chart parsing .", "In this approach one maintains an agenda of items remaining to be processed , one of which is processed during each iteration .", "As each item is pulled off the agenda , it is added to the chart unless it is already there , in which case it can be discarded and used to extend and create additional items .", "In quot ; exhaustive quot ; chart parsing one removes items from the agenda in some relatively simple way last in , first out is common , and continues to do so until nothing remains .", "A commonly discussed alternative is to remove the constituents from the agenda according to a figure of merit FOM .", "The idea is that the FOM selects quot ; good quot ; items to be processed , leaving the quot ; bad quot ; ones the ones that are not , in fact , part of the correct parse sitting on the agenda .", "When one has a completed parse , or perhaps several possible parses , one simply stops parsing , leaving items remaining on the agenda .", "The time that would have been spent processing these remaining items is time saved , and thus time earned .", "In our work we have found that exhaustively parsing maximum 40 word sentences from the Penn II treebank requires an average of about 1 . 2 million edges per sentence .", "Numbers like this suggest that any approach that offers the possibility of reducing the work load is well worth pursuing , a fact that has been noted by several researchers .", "Early on , Kay 1980 suggested the use of the chart agenda for this purpose .", "More recently , the statistical approach to language processing and the use of probabilistic context free grammars PCFGs has suggested using the PCFG probabilities to create a FOM .", "Bobrow 1990 and Chitrao and Grishman 1990 introduced best first PCFG parsing , the approach taken here .", "Subsequent work has suggested different FOMs built from PCFG probabilities Miller and Fox .", "1994 Kochman and Kupin .", "1991 N1agerman and Marcus , 1991 .", "Probably the most extensive comparison of possible metrics for best first PCFG parsing is that of Caraballo and Charniak henceforth C C Forthcoming .", "They consider a large number of FOMs , and view them as approximations of some quot ; ideal quot ; but only computable after the fact FOM .", "Of these they recommend one as the best of the lot .", "In this paper we basically adopt both their framework and their recommended FOM .", "The next section describes their work in more detail , Besides C C the work that is most directly comparable to ours is that of Goodman 1997 and Ratnaparki 1997 .", "Goodman uses an FOM that is similar to that of C C but one that should , in general , be somewhat more accurate .", "However , both Goodman's and Ratnaparki's work assumes that one is doing a beam search of some sort , rather than a best first search , and their FOM are unfortunately tied to their frameworks and thus cannot be adopted here .", "We briefly compare our results to theirs in Section 5 .", "As noted , our paper takes off from that of C C and uses the same FOM .", "The major difference is simply that our parser uses the FOM to rank edges including incomplete edges , rather than simply completed constituents , as was done by C C .", "What is interesting about our approach is that such a seemingly simple change can produce rather dramatic results .", "Rather than the thousands of edges required by C C , the parser presented here requires hundreds , or even , if one is willing to pay a small price in accuracy , tens .", "In the approach taken in C C , only completed edges , i . e . , constituents , are entered into the agenda ; incomplete edges are always processed as soon as they are constructed .", "At each iteration the constituent with the highest figure of merit is removed from the agenda , added to the chart , and used to extend current partially completed constituents .", "Thus we characterize their work as constituent based best first chart parsing .", "C C take as an quot ; ideal quot ; FOM the quantity to , , .", "Here NJ , is a constituent of type i e . g . , NP , VP , etc . that spans the constituents from j up to but not including k , and tom are the n parts of speech tags of the sentence .", "Note that C C simplify parsing by assuming that the input is a sequence of tags , not words .", "We make the same assumption in this paper .", "Thus taking p N ; rk I to , , as an FOM says that one should work on the constituent that is most likely to be correct . given the tags of the sentence .", "As p N ; , k to , n can only be computed precisely after a full parse of the sentence , C C derive several approximations , in each case starting from the well known equation for p Nlk I tom in terms of the inside and outside probabilities , 3 1V . 1 , k and where fi Mjkj and a N'k are defined as follows 4 Informally , this can be obtained by approximating the outside probability a N . ; , k in Equation 1 with a bitag estimate .", "Of the five terms in Equation 4 , two can be directly estimated from training data the quot ; boundary statistics quot ; p N . , k I tj the probability of a constituent of type NIAstarting just after the tag tj and p tk I NIA the probability of tk appearing just after the end of a constituent of type N . 4 .", "The tag sequence probabilitiy in the denominator is approximated using a bi tag approximation The basic algorithm then is quite simple .", "One uses the standard chart parsing algorithm , except at each iteration one takes from the agenda the constituent that maximizes the FOM described in Equation 4 .", "There are , however , two minor complexities that need to be noted .", "The first relates to the inside probability 0 N 4 .", "C C approximate it with the sum of the probabilities of all the parses for Nlk found at that point in the parse .", "This in turn requires a somewhat complicated scheme to avoid repeatedly re evaluating Equation 4 whenever a new parse is found .", "In this paper we adopt a slightly simpler method .", "We approximate 3 Nk by the most probable parse for Ni , k , rather than the sum of all the parses . j We justify this on the grounds that our parser eventually returns the most probable parse , so it seems reasonable to base our metric on its value .", "This also simplifies updating 13 N3 , k when new parses are found for Nk .", "Our algorithm compares the probability of the new parse to the best already found for Ni 4 .", "If the old one is higher , nothing need be done .", "If the new one is higher , it is simply added to the agenda .", "The second complexity has to do with the fact that in Equation 4 the probability of the tags t3 , k are approximated using two different distributions , once in the numerator where we use the PCFG probabilities , and once in the denominator , where we use the bi tag probabilities .", "One fact noted by C C , but not discussed in their paper , is that typically the bitag model gives higher probabilities for a tag sequence than does the PCFG distribution .", "For any single tag t3 , the difference is not much , but as we use Equation 4 to compute our FOM for larger constituents , the numerator becomes smaller and smaller with respect to the denominator , effectively favoring smaller constituents .", "To avoid this one needs to normalize the two distributions to produce more similar results .", "We have empirically measured the normalization factor and found that the bi tag distribution produces probabilities that are approximately 1 . 3 times those produced by the PCFG distribution , on a per word basis .", "We correct for this by making the PCFG probability of a known tag ri 1 .", "This has the effect of multiplying the inside probability 3 N . 4 by rik J .", "In Section 4 we show how the behavior of our algorithm changes for is between 1 . 0 and 2 . 4 .", "Informally , our algorithm differs from the one presented in C C primarily in that we rank all edges , incomplete as well as complete , with respect to the FOM .", "A straight forward way to extend C C in this fashion is to transform the grammar so that all productions are either unary or binary .", "Once this has been done there is no need for incomplete edges at all in bottomup parsing , and parsing can be performed using the CKY algorithm , suitably extended to handle unary productions .", "One way to convert a PCFG into this form is left factoring Hoperoft and Ullman , 1979 .", "Left factoring replaces each production A 4 p , where p is the production probability and 101 n 2 , with the following set of binary productions In these productions A is the ith element of 3 and 13i , j' is the subsequence A of 0 , but treated as a 'new' single non terminal in the left factored grammar the quote marks indicate that this subsequence is to be considered a single symbol .", "For example , the production VP 4 V NP NP PP 0 . 7 left factors to the following productions VP 4 NP NP' PP 0 . 7 NP NP' NP' PP 1 . 0 NP' 4 V NP 1 . 0 It is not difficult to show that the left factored grammar defines the same probability distribution over strings as the original grammar , and to devise a tree transformation that maps each parse tree of the original grammar into a unique parse tree of the left factored grammar of the same probability .", "In fact , the assumption that all productions are at most binary is not extraordinary , since tabular parsers that construct complete parse forests in worst case 0 n3 time explicitly or implicitly convert their grammars into binary branching form Lang , 1974 ; Lang , 1991 .", "Sikkel and Nijholt 1997 describe in detail the close relationship between the CKY algorithm , the Earley algorithm and a bottom up variant of the Earley algorithm .", "The key observation is that the 'new' non terminals 01 , i' in a CKY parse using a left factored grammar correspond to the set of non empty incomplete edges A 4 01 , z .", "13z 1 , n in the bottom up variant of the Earley algorithm , where A 131 , , is a production of the original grammar .", "Specifically , the fundamental rule of chart parsing Kay , 1980 , which combines an incomplete edge A a BO with a complete edge B 7 to yield the edge A a B 0 , corresponds to the left factored productions aB' a B if 3 is non empty or A 'a' B if i3 is empty .", "Thus in general a single 'new' non terminal in a CKY parse using the left factored grammar abbreviates several incomplete edges in the Earley algorithm .", "For our experiment , we used a tree bank grammar induced from sections 2 21 of the Penn Wall Street Journal text Marcus et al . , 1993 , with section 22 reserved for testing .", "All sentences of length greater than 40 were ignored for testing purposes as done in both C C and Goodman 1997 .", "We applied the binarization technique described above to the grammar .", "We chose to measure the amount of work done by the parser in terms of the average number of edges popped off the agenda before finding a parse .", "This method has the advantage of being platform independent , as well as providing a measure of quot ; perfection quot ; .", "Here , perfection is the minimum number of edges we would need to pop off the agenda in order to create the correct parse .", "For the binarized grammar , where each popped edge is a completed constituent , this number is simply the number of terminals plus nonterminals in the sentence on average , 47 . 5 .", "Our algorithm includes some measures to reduce the number of items on the agenda , and thus presumably the number of popped edges .", "Each time we add a constituent to the chart , we combine it with the constituents on either side of it , potentially creating several new edges .", "For each of these new edges , we check to see if a matching constituent i . e . a constituent with the same head , start , and end points already exists in either the agenda or the chart .", "If there is no match , we simply add the new edge to the agenda .", "If there is a match but the old parse of Nild , is better than the new one , we discard the new parse .", "Finally , if we have found a better parse of N . 4 , we add the new edge to the agenda , removing the old one if it has not already been popped .", "We tested the parser on section section 22 of the WSJ text with various normalization constants . 77 , working on each sentence only until we reached the first full parse .", "For each sentence we recorded the number of popped edges needed to reach the first parse , and the precision and recall of that parse .", "The average number of popped edges to first parse as a function of q is shown in Figure 1 , and the average precision and recall are shown in Figure 2 .", "The number of popped edges decreases as ij increases from 1 . 0 to 1 . 7 , then begins to increase again .", "See Section 5 for discussion of these results .", "The precision and recall also decrease as 77 increases .", "Note that , because we used a binarized grammer for parsing , the trees produced by the parser contain binarized labels rather than the labels in the treebank .", "In order to calculate precision and recall . we quot ; debinarized quot ; the parser's output and then calculated the figures as usual .", "These results suggest two further questions Is the higher accuracy with lower 77 due in part to the higher number of edges popped ?", "If so , can we gain accuracy with higher i by letting the parser continue past the first parse i . e . pop more edges ?", "To answer these questions , we ran the parser again , this time allowing it to continue parsing until it had popped 20 times as many edges as needed to reach the first parse .", "The results of this experiment are shown in Figure 3 , where we plot precision recall 2 henceforth quot ; accuracy quot ; as a function of edges .", "Note that regardless of 7 the accuracy of the parse increases given extra time , but that all of the increase is achieved with only 1 . 5 to 2 times as many edges as needed for the first parse .", "For ij between 1 . 0 and 1 . 2 , the highest accuracy is almost the same , about 75 . 2 , but this value is reached with an average of slightly under 400 edges when 77 1 . 2 , compared to about 650 when 7 1 . 0 .", "To better understand the experimental results it first behooves us to compare them to those achieved previously .", "Goodman's results 1997 are hard to compare against ours because his parser returns more than a singe best parse and because he measures processing time , not edges .", "However he does give edges second for one of his 2000 4000 6000 8000 10000 parsers and this plus his parsing times suggests that for him edges sentence will measure in the tens of thousands a far cry from our hundreds .", "Ratnaparki's 1997 beam search parsing procedure produces higher accuracy results than our PCFG model , and achieves this with a beam width of 20 .", "Unfortunately his paper does not give statistics which can be directly compared with ours .", "The work by C C is easier to compare .", "In Figure 4 we reproduce C C's results on the percentage of sentences length 18 26 parsed as a function of number of edges used .", "We performed the same experiment , and our results are included there as well .", "This figure makes dramatic the order of magnitude improvement provided by our new scheme , but it is not too easy to read numbers off of it .", "Such numbers are provided in Table 1 .", "Our figures were obtained using ri 1 . 2 .", "As can be seen , our parser requires about one twentieth the number of edges required by C C .", "Indeed , the low average number of edges to first parse is probably the most striking thing about our results .", "Even allowing for the fact that considerably more edges must be pushed than are popped , the total number of edges required to first parse is quite small .", "Since the average number of edges required to construct just the left factored test corpus trees is 47 . 5 , our parsing system considers as few as 3 times as many edges as are required to actually produce the output tree .", "Almost as interesting , if i is below 1 . 4 , the precision and recall scores of the first parse are better than those obtained by running the parser to exhaustion , even though the probability of the first parses our algorithm returns cannot be higher than that found by the exhaustive version .", "Furthermore , as seen in Figure 3 , running our parser past the first parse by a small amount 150 of the edges required for the first parse produces still more accurate parses .", "At 150 of the minimum number of edges and I 1 . 2 the precision recall figures are about 2 above those for the maximum likelihood parse .", "We have two possibly related theories of these phenomona .", "It may be that the FOM metric used to select constituents forces our parser to concentrate on edges which are plausible given their surrounding preterminals ; information which is ignored by the exhaustive maximum likelihood parser .", "Alternatively , it may be that because our FOM causes our parser to prefer edges with a high inside times estimated outside probability , it is in fact partially mimicking Goodman's Goodman , 1996 'Labelled Recall' parsing algorithm , which does not return the highest probability parse but attempts to maximize labelled bracket recall with the test set .", "Finally , it is interesting to note that the minimum number of edges per parse is reached when 77 1 . 65 , which is considerably larger than the theoretical estimate of 1 . 3 given earlier .", "Notice that one effect of increasing r is to raise the FOM for longer constituents .", "It may be that on average a partial parse is completed fastest if larger constituents receive more attention since they are more likely to lead quickly to a complete analysis , which would be one consequence of the larger than expected n . This last hypothesis is also consistent with the observation that average precision and recall sharply falls off when 77 is increased beyond its theoretically optimal value , since then the parser is presumably focusing on relatively larger constituents and ignoring other , strictly more plausible , smaller ones .", "It is worth noting that while we have presented the use of edge based best first chart parsing in the service of a rather pure form of PCFG parsing , there is no particular reason to assume that the technique is so limited in its domain of applicability .", "One can imagine the same techniques coupled with more informative probability distributions , such as lexicalized PCFGs Charniak , 1997 , or even grammars not based upon literal rules , but probability distributions that describe how rules are built up from smaller components Magerman , 1995 ; Collins , 1997 .", "Clearly further research is warranted .", "Be this as it may , the take home lesson from this paper is simple combining an edge based agenda with the figure of merit from C C To the best of our knowledge this is currently the most effecient parsing technique for PCFG grammars induced from large tree banks .", "As such we strongly recommend this technique to others interested in PCFG parsing ."], "summary_lines": ["Edge-Based Best-First Chart Parsing\n", "Best-first probabilistic chart parsing attempts to parse efficiently by working on edges that are judged \"best\" by some probabilistic figure of merit (FOM).\n", "Recent work has used probabilistic context-free grammars (PCFGs) to assign probabilities to constituents, and to use these probabilities as the starting point for the FOM.\n", "This paper extends this approach to using a probabilistic FOM to judge edges (incomplete constituents), thereby giving a much finer-grained control over parsing effort.\n", "We show how this can be accomplished in a particularly simple way using the common idea of binarizing the PCFG.\n", "The results obtained are about a factor of twenty improvement over the best prior results - that is, our parser achieves equivalent results using one twentieth the number of edges.\n", "Furthermore we show that this improvement is obtained with parsing precision and recall levels superior to those achieved by exhaustive parsing.\n", "We introduce overparsing as a technique to improve parse accuracy by continuing parsing after the first complete parse tree is found.\n"]}
{"article_lines": ["Annealing Structural Bias In Multilingual Weighted Grammar Induction", "first show how a structural bias improve the accuracy of state of the art dependency grammar induction models trained by EM from unannotated examples Klein and Manning , 2004 .", "Next , by annealing the free parameter that controls this bias , we achieve further improvements .", "We then describe an alternative kind of structural bias , toward broken hypotheses consisting of partial structures over segmented sentences , and show a similar pattern of improvement .", "We relate this approach to contrastive estimation Smith and Eisner , 2005a , apply the latter to grammar induction in six languages , and show that our new approach improves accuracy by 1 17 absolute over CE and 8 30 over EM , achieving to our knowledge the best results on this to date .", "Our method , is a general technique with broad applicability to hidden structure discovery problems .", "Inducing a weighted context free grammar from flat text is a hard problem .", "A common starting point for weighted grammar induction is the Expectation Maximization EM algorithm Dempster et al . , 1977 ; Baker , 1979 .", "EM s mediocre performance Table 1 reflects two problems .", "First , it seeks to maximize likelihood , but a grammar that makes the training data likely does not necessarily assign a linguistically defensible syntactic structure .", "Second , the likelihood surface is not globally concave , and learners such as the EM algorithm can get trapped on local maxima Charniak , 1993 .", "We seek here to capitalize on the intuition that , at least early in learning , the learner should search primarily for string local structure , because most structure is local . 1 By penalizing dependencies between two words that are farther apart in the string , we obtain consistent improvements in accuracy of the learned model 3 .", "We then explore how gradually changing S over time affects learning 4 we start out with a strong preference for short dependencies , then relax the preference .", "The new approach , structural annealing , often gives superior performance .", "An alternative structural bias is explored in 5 .", "This approach views a sentence as a sequence of one or more yields of separate , independent trees .", "The points of segmentation are a hidden variable , and during learning all possible segmentations are entertained probabilistically .", "This allows the learner to accept hypotheses that explain the sentences as independent pieces .", "In 6 we briefly review contrastive estimation Smith and Eisner , 2005a , relating it to the new method , and show its performance alone and when augmented with structural bias .", "In this paper we use a simple unlexicalized dependency model due to Klein and Manning 2004 .", "The model is a probabilistic head automaton grammar Alshawi , 1996 with a split form that renders it parseable in cubic time Eisner , 1997 .", "Let x x1 , x2 , . . . , xn be the sentence . x0 is a special wall symbol , , on the left of every sentence .", "A tree y is defined by a pair of functions yleft and yright both 0 , 1 , 2 , . . . , n , 211 , 2 , . . . , n1 that map each word to its sets of left and right dependents , respectively .", "The graph is constrained to be a projective tree rooted at each word except has a single parent , and there are no cycles or crossing dependencies . 2 yleft 0 is taken to be empty , and yright 0 contains the sentence s single head .", "Let yi denote the subtree rooted at position i .", "The probability P yi xi of generating this subtree , given its head word xi , is defined recursively where firsty j is a predicate defined to be true iff xj is the closest child on either side to its parent xi .", "The probability of the entire tree is given by pe x , y P y0 .", "The parameters O are the conditional distributions pstop and pchild .", "Experimental baseline EM .", "Following common practice , we always replace words by part ofspeech POS tags before training or testing .", "We used the EM algorithm to train this model on POS sequences in six languages .", "Complete experimental details are given in the appendix .", "Performance with unsupervised and supervised model selection across different \u03bb values in add \u03bb smoothing and three initializers O 0 is reported in Table 1 .", "The supervised selected model is in the 40 55 F1 accuracy range on directed dependency attachments .", "Here F1 Pz precision Pz recall ; see appendix .", "Supervised model selection , which uses a small annotated development set , performs almost as well as the oracle , but unsupervised model selection , which selects the model that maximizes likelihood on an unannotated development set , is often much worse .", "Hidden variable estimation algorithms including EM typically work by iteratively manipulating the model parameters O to improve an objective function F O .", "EM explicitly alternates between the computation of a posterior distribution over hypotheses , pp y x where y is any tree with yield x , and computing a new parameter estimate O . 3 with a locality bias at varying S . Each curve corresponds to a different language and shows performance of supervised model selection within a given S , across A and O1 values .", "See Table 3 for performance of models selected across Ss .", "We decode with S 0 , though we found that keeping the training time value of S would have had almost no effect .", "The EM baseline corresponds to S 0 .", "One way to bias a learner toward local explanations is to penalize longer attachments .", "This was done for supervised parsing in different ways by Collins 1997 , Klein and Manning 2003 , and McDonald et al . 2005 , all of whom considered intervening material or coarse distance classes when predicting children in a tree .", "Eisner and Smith 2005 achieved speed and accuracy improvements by modeling distance directly in a ML estimated deficient generative model .", "Here we use string distance to measure the length of a dependency link and consider the inclusion of a sum of lengths feature in the probabilistic model , for learning only .", "Keeping our original model , we will simply multiply into the probability of each tree another factor that penalizes long dependencies , giving where y i yleft i U yright i .", "Note that if \u03b4 0 , we have the original model .", "As \u03b4 oc , the new model p' will favor parses with shorter dependencies .", "The dynamic programming algorithms remain the same as before , with the appropriate ea i j factor multiplied in at each attachment between xi and xj .", "Note that when \u03b4 0 , pe pe .", "Experiment .", "We applied a locality bias to the same dependency model by setting \u03b4 to different we show performance with add 10 smoothing , the all zero initializer , for three languages with three different initial values 6o .", "Time progresses from left to right .", "Note that it is generally best to start at 6o 0 ; note also the importance of picking the right point on the curve to stop .", "See Table 3 for performance of models selected across smoothing , initialization , starting , and stopping choices , in all six languages . values in 1 , 0 . 2 see Eq .", "The same initializers Off0 and smoothing conditions were tested .", "Performance of supervised model selection among models trained at different \u03b4 values is plotted in Fig .", "When a model is selected across all conditions 3 initializers x 6 smoothing values x 7 \u03b4s using annotated development data , performance is notably better than the EM baseline using the same selection procedure see Table 3 , second column .", "The central idea of this paper is to gradually change anneal the bias \u03b4 .", "Early in learning , local dependencies are emphasized by setting \u03b4 0 .", "Then \u03b4 is iteratively increased and training repeated , using the last learned model to initialize .", "This idea bears a strong similarity to deterministic annealing DA , a technique used in clustering and classification to smooth out objective functions that are piecewise constant hence discontinuous or bumpy non concave Rose , 1998 ; Ueda and Nakano , 1998 .", "In unsupervised learning , DA iteratively re estimates parameters like EM , but begins by requiring that the entropy of the posterior pp y x be maximal , then gradually relaxes this entropy constraint .", "Since entropy is concave in O , the initial task is easy maximize a concave , continuous function .", "At each step the optimization task becomes more difficult , but the initializer is given by the previous step and , in practice , tends to be close to a good local maximum of the more difficult objective .", "By the last iteration the objective is the same as in EM , but the annealed search process has acted like a good initializer .", "This method was applied with some success to grammar induction models by Smith and Eisner 2004 .", "In this work , instead of imposing constraints on the entropy of the model , we manipulate bias toward local hypotheses .", "As \u03b4 increases , we penalize long dependencies less .", "We call this structural annealing , since we are varying the strength of a soft constraint bias on structural hypotheses .", "In structural annealing , the final objective would be the same as EM if our final \u03b4 , \u03b4f 0 , but we found that annealing farther \u03b4f 0 works much better . 4 Experiment Annealing \u03b4 .", "We experimented with annealing schedules for \u03b4 .", "We initialized at \u03b40 E 1 , 0 . 4 , 0 . 21 , and increased \u03b4 by 0 . 1 in the first case or 0 . 05 in the others up to \u03b4f 3 .", "Models were trained to convergence at each \u03b4epoch .", "Model selection was applied over the same initialization and regularization conditions as before , \u03b40 , and also over the choice of \u03b4f , with stopping allowed at any stage along the \u03b4 trajectory .", "Trajectories for three languages with three different \u03b40 values are plotted in Fig .", "Generally speaking , \u03b40 0 performs better .", "There is consistently an early increase in performance as \u03b4 increases , but the stopping \u03b4f matters tremendously .", "Selected annealed \u03b4 models surpass EM in all six languages ; see the third column of Table 3 .", "Note that structural annealing does not always outperform fixed \u03b4 training English and Portuguese .", "This is because we only tested a few values of \u03b40 , since annealing requires longer runtime .", "A related way to focus on local structure early in learning is to broaden the set of hypotheses to include partial parse structures .", "If x x1 , x2 , . . . , xn , the standard approach assumes that x corresponds to the vertices of a single dependency tree .", "Instead , we entertain every hypothesis in which x is a sequence of yields from separate , independently generated trees .", "For example , x1 , x2 , x3 is the yield of one tree , x4 , x5 is the with structural annealing on the breakage weight 3 .", "Here we show performance with add 10 smoothing , the all zero initializer , for three languages with three different initial values , Qo .", "Time progresses from left large , Q to right .", "See Table 3 for performance of models selected across smoothing , initialization , and stopping choices , in all six languages . yield of a second , and x6 , . . . , xn is the yield of a third .", "One extreme hypothesis is that x is n singlenode trees .", "At the other end of the spectrum is the original set of hypotheses full trees on x .", "Each has a nonzero probability .", "Segmented analyses are intermediate representations that may be helpful for a learner to use to formulate notions of probable local structure , without committing to full trees . 5 We only allow unobserved breaks , never positing a hard segmentation of the training sentences .", "Over time , we increase the bias against broken structures , forcing the learner to commit most of its probability mass to full trees .", "At first glance broadening the hypothesis space to entertain all 2n 1 possible segmentations may seem expensive .", "In fact the dynamic programming computation is almost the same as summing or maximizing over connected dependency trees .", "For the latter , we use an inside outside algorithm that computes a score for every parse tree by computing the scores of items , or partial structures , through a bottom up process .", "Smaller items are built first , then assembled using a set of rules defining how larger items can be built . 6 Now note that any sequence of partial trees over x can be constructed by combining the same items into trees .", "The only difference is that we are willing to consider unassembled sequences of these partial trees as hypotheses , in addition to the fully connected trees .", "One way to accomplish this in terms of yright 0 is to say that the root , , is allowed to have multiple children , instead of just one .", "Here , these children are independent of each other e . g . , generated by a unigram Markov model .", "In supervised dependency parsing , Eisner and Smith 2005 showed that imposing a hard constraint on the whole structure specifically that each non dependency arc cross fewer than k words can give guaranteed O nk2 runtime with little to no loss in accuracy for simple models .", "This constraint could lead to highly contrived parse trees , or none at all , for some sentences both are avoided by the allowance of segmentation into a sequence of trees each attached to .", "The construction of the vine sequence of s children takes only O n time once the chart has been assembled .", "Our broadened hypothesis model is a probabilistic vine grammar with a unigram model over s children .", "We allow but do not require segmentation of sentences , where each independent child of is the root of one of the segments .", "We do not impose any constraints on dependency length .", "Now the total probability of an n length sentence x , marginalizing over its hidden structures , sums up not only over trees , but over segmentations of x .", "For completeness , we must include a probability model over the number of trees generated , which could be anywhere from 1 to n . The model over the number T of trees given a sentence of length n will take the following log linear form where Q E R is the sole parameter .", "When Q 0 , every value of T is equally likely .", "For Q 0 , the model prefers larger structures with few breaks .", "At the limit Q oc , we achieve the standard learning setting , where the model must explain x using a single tree .", "We start however at Q 0 , where the model prefers smaller trees with more breaks , in the limit preferring each word in x to be its own tree .", "We could describe brokenness as a feature in the model whose weight , Q , is chosen extrinsically and time dependently , rather than empirically just as was done with S . Annealing \u03b2 resembles the popular bootstrapping technique Yarowsky , 1995 , which starts out aiming for high precision , and gradually improves coverage over time .", "With strong bias \u03b2 0 , we seek a model that maintains high dependency precision on non attachments by attaching most tags to .", "Over time , as this is iteratively weakened \u03b2 oo , we hope to improve coverage dependency recall .", "Bootstrapping was applied to syntax learning by Steedman et al . 2003 .", "Our approach differs in being able to remain partly agnostic about each tag s true parent e . g . , by giving 50 probability to attaching to , whereas Steedman et al . make a hard decision to retrain on a whole sentence fully or leave it out fully .", "In earlier work , Brill and Marcus 1992 adopted a local first iterative merge strategy for discovering phrase structure .", "Experiment Annealing \u03b2 .", "We experimented with different annealing schedules for \u03b2 .", "The initial value of \u03b2 , \u03b20 , was one of 1 , 0 , 2 .", "After EM training , \u03b2 was diminished by 10 ; this was repeated down to a value of \u03b2f 3 .", "Performance after training at each \u03b2 value is shown in Fig .", "3 . 7 We see that , typically , there is a sharp increase in performance somewhere during training , which typically lessens as \u03b2 oo .", "Starting \u03b2 too high can also damage performance .", "This method , then , is not robust to the choice of \u03bb , \u03b20 , or \u03b2f , nor does it always do as well as annealing \u03b4 , although considerable gains are possible ; see the fifth column of Table 3 .", "By testing models trained with afixed value of \u03b2 for values in 1 , 1 , we ascertained that the performance improvement is due largely to annealing , not just the injection of segmentation bias fourth vs . fifth column of Table 3 . 8", "Contrastive estimation CE was recently introduced Smith and Eisner , 2005a as a class of alternatives to the likelihood objective function locally maximized by EM .", "CE was found to outperform EM on the task of focus in this paper , when applied to English data Smith and Eisner , 2005b .", "Here we review the method briefly , show how it performs across languages , and demonstrate that it can be combined effectively with structural bias .", "Contrastive training defines for each example xi a class of presumably poor , but similar , instances called the neighborhood , N xi , and seeks to maximize At this point we switch to a log linear rather than stochastic parameterization of the same weighted grammar , for ease of numerical optimization .", "All this means is that O specifically , pstop and pchild in Eq .", "1 is now a set of nonnegative weights rather than probabilities .", "Neighborhoods that can be expressed as finitestate lattices built from xi were shown to give significant improvements in dependency parser quality over EM .", "Performance of CE using two of those neighborhoods on the current model and datasets is shown in Table 2 . 9 0 mean diagonal Gaussian smoothing was applied , with different variances , and model selection was applied over smoothing conditions and the same initializers as before .", "Four of the languages have at least one effective CE condition , supporting our previous English results Smith and Eisner , 2005b , but CE was harmful for Bulgarian and Mandarin .", "Perhaps better neighborhoods exist for these languages , or there is some ideal neighborhood that would perform well for all languages .", "Our approach of allowing broken trees 5 is a natural extension of the CE framework .", "Contrastive estimation views learning as a process of moving posterior probability mass from implicit negative examples to explicit positive examples .", "The positive evidence , as in MLE , is taken to be the observed data .", "As originally proposed , CE allowed a redefinition of the implicit negative evidence from all other sentences as in MLE to sentences like xi , but perturbed . Allowing segmentation of the training sentences redefines the positive and negative evidence .", "Rather than moving probability mass only to full analyses of the training example xi , we also allow probability mass to go to partial analyses of xi .", "By injecting a bias S 7 0 or Q oc among tree hypotheses , however , we have gone beyond the CE framework .", "We have added features to the tree model dependency length sum , number of breaks , whose weights we extrinsically manipulate over time to impose locality bias CN and improve search on CN .", "Another idea , not explored here , is to change the contents of the neighborhood N over time .", "Experiment Locality Bias within CE .", "We combined CE with a fixed S locality bias for neighborhoods that were successful in the earlier CE experiment , namely DELETEORTRANSPOSE1 for German , English , Turkish , and Portuguese .", "Our results , shown in the seventh column of Table 3 , show that , in all cases except Turkish , the combination improves over either technique on its own .", "We leave exploration of structural annealing with CE to future work .", "Experiment Segmentation Bias within CE .", "For language , N pairs where CE was effective , we trained models using CE with a fixedQ segmentation model .", "Across conditions Q E 1 , 1 , these models performed very badly , hypothesizing extremely local parse trees typically over 90 of dependencies were length 1 and pointed in the same direction , compared with the 60 70 length 1 rate seen in gold standards .", "To understand why , consider that the CE goal is to maximize the score of a sentence and all its segmentations while minimizing the scores of neighborhood sentences and their segmentations .", "An ngram model can accomplish this , since the same n grams are present in all segmentations of x , and some different n grams appear in N x for LENGTH and DELETEORTRANSPOSE1 .", "A bigram like model that favors monotone branching , then , is not a bad choice for a CE learner that must account for segmentations of x and N x .", "Why doesn t CE without segmentation resort to n gram like models ?", "Inspection of models trained using the standard CE method no segmentation with transposition based neighborhoods TRANSPOSE1 and DELETEORTRANSPOSE1 did have high rates of length 1 dependencies , while the poorly performing DELETE1 models found low length 1 rates .", "This suggests that a bias toward locality n gram ness is built into the former neighborhoods , and may partly explain why CE works when it does .", "We achieved a similar locality bias in the likelihood framework when we broadened the hypothesis space , but doing so under CE over focuses the model on local structures .", "We compared errors made by the selected EM condition with the best overall condition , for each language .", "We found that the number of corrected attachments always outnumbered the number of new errors by a factor of two or more .", "Further , the new models are not getting better by merely reversing the direction of links made by EM ; undirected accuracy also improved significantly under a sign test p 10 6 , across all six languages .", "While the most common corrections were to nouns , these account for only 25 41 of corrections , indicating that corrections are not all of the same kind . Finally , since more than half of corrections in every language involved reattachment to a noun or a verb content word , we believe the improved models to be getting closer than EM to the deeper semantic relations between words that , ideally , syntactic models should uncover .", "One weakness of all recent weighted grammar induction work including Klein and Manning 2004 , Smith and Eisner 2005b , and the present paper is a sensitivity to hyperparameters , including smoothing values , choice of N for CE , and annealing schedules not to mention initialization .", "This is quite observable in the results we have presented .", "An obstacle for unsupervised learning in general is the need for automatic , efficient methods for model selection .", "For annealing , inspiration may be drawn from continuation methods ; see , e . g . , Elidan and Friedman 2005 .", "Ideally one would like to select values simultaneously for many hyperparameters , perhaps using a small annotated corpus as done here , extrinsic figures of merit on successful learning trajectories , or plausibility criteria Eisner and Karakos , 2005 .", "Grammar induction serves as a tidy example for structural annealing .", "In future work , we envision that other kinds of structural bias and annealing will be useful in other difficult learning problems where hidden structure is required , including machine translation , where the structure can consist of word correspondences or phrasal or recursive syntax with correspondences .", "The technique bears some similarity to the estimation methods described by Brown et al . 1993 , which started by estimating simple models , using each model to seed the next .", "We have presented a new unsupervised parameter estimation method , structural annealing , for learning hidden structure that biases toward simplicity and gradually weakens anneals the bias over time .", "We applied the technique to weighted dependency grammar induction and achieved a significant gain in accuracy over EM and CE , raising the state of the art across six languages from 42 54 to 58 73 accuracy ."], "summary_lines": ["Annealing Structural Bias In Multilingual Weighted Grammar Induction\n", "We first show how a structural locality bias can improve the accuracy of state-of-the-art dependency grammar induction models trained by EM from unannotated examples (Klein and Manning, 2004).\n", "Next, by annealing the free parameter that controls this bias, we achieve further improvements.\n", "We then describe an alternative kind of structural bias, toward \"broken\" hypotheses consisting of partial structures over segmented sentences, and show a similar pattern of improvement.\n", "We relate this approach to contrastive estimation (Smith and Eisner, 2005a), apply the latter to grammar induction in six languages, and show that our new approach improves accuracy by 1-17% (absolute) over CE (and 8-30% over EM), achieving to our knowledge the best results on this task to date.\n", "Our method, structural annealing, is a general technique with broad applicability to hidden-structure discovery problems.\n", "We penalize the approximate posterior over dependency structures in a natural language grammar induction task to avoid long range dependencies between words.\n", "We propose structural annealing (SA), in which a strong bias for local dependency attachments is enforced early in learning, and then gradually relaxed.\n", "Our annealing approach tries to alter the space of hidden outputs in the E-step over time to facilitate learning in the M-step.\n"]}
{"article_lines": ["Assessing Agreement On Classification Tasks The Kappa Statistic", "Currently , computational linguists and cognitive scientists working in the area of discourse and dialogue argue that their subjective judgments are reliable using several different statistics , none of which are easily interpretable or comparable to each other .", "Meanwhile , researchers in content analysis have already experienced the same difficulties and come up with a solution in the kappa statistic .", "We discuss what is wrong with reliability measures as they are currently used for discourse and dialogue work in computational linguistics and cognitive science , and argue that we would be better off as afield adopting techniques from content analysis .", "Currently , computational linguists and cognitive scientists working in the area of discourse and dialogue argue that their subjective judgments are reliable using several different statistics , none of which are easily interpretable or comparable to each other .", "Meanwhile , researchers in content analysis have already experienced the same difficulties and come up with a solution in the kappa statistic .", "We discuss what is wrong with reliability measures as they are currently used for discourse and dialogue work in computational linguistics and cognitive science , and argue that we would be better off as afield adopting techniques from content analysis .", "Computational linguistic and cognitive science work on discourse and dialogue relies on subjective judgments .", "For instance , much current research on discourse phenomena distinguishes between behaviors which tend to occur at or around discourse segment boundaries and those which do not Passonneau and Litman 1993 ; Kowtko , Isard , and Doherty 1992 ; Litman and Hirschberg 1990 ; Cahn 1992 .", "Although in some cases discourse segments are defined automatically e . g . , Rodrigues and Lopes' 1992 definition based on temporal relationships , more usually discourse segments are defined subjectively , based on the intentional structure of the discourse , and then other phenomena are related to them .", "At one time , it was considered sufficient when working with such judgments to show examples based on the authors' interpretation paradigmatically , Grosz and Sidner 1986 , but also countless others .", "Research was judged according to whether or not the reader found the explanation plausible .", "Now , researchers are beginning to require evidence that people besides the authors themselves can understand , and reliably make , the judgments underlying the research .", "This is a reasonable requirement , because if researchers cannot even show that people can agree about the judgments on which their research is based , then there is no chance of replicating the research results .", "Unfortunately , as a field we have not yet come to agreement about how to show reliability of judgments .", "For instance , consider the following arguments for reliability .", "We have chosen these examples both for the clarity of their arguments and because , taken as a set , they introduce the full range of issues we wish to discuss . possible to mark conversational move boundaries , cite separately for each of three naive coders the ratio of the number of times they agreed with an quot ; expert quot ; coder about the existence of a boundary over the number of times either the naive coder or the expert marked a boundary .", "They do not describe any restrictions on possible boundary sites .", "Although 1 and KID's use of 2 differ slightly from Litman and Hirschberg's use of 2 , 3 and 4 in clearly designating one coder as an quot ; expert , quot ; all of these studies have n coders place some kind of units into m exclusive categories .", "Note that the cases of testing for the existence of a boundary can be treated as coding quot ; yes quot ; and quot ; no quot ; categories for each of the possible boundary sites ; this treatment is used by measures 3 and 4 but not by measure 1 .", "All four approaches seem reasonable when taken at face value .", "However , the four measures of reliability bear no relationship to each other .", "Worse yet , since none of them take into account the level of agreement one would expect coders to reach by chance , none of them are interpretable even on their own .", "We first explain what effect chance expected agreement has on each of these measures , and then argue that we should adopt the kappa statistic Siegel and Castellan 1988 as a uniform measure of reliability .", "Measure 2 seems a natural choice when there are two coders , and there are several possible extensions when there are more coders , including citing separate agreement figures for each important pairing as KID do by designating an expert , counting a unit as agreed only if all coders agree on it , or measuring one agreement over all possible pairs of coders thrown in together .", "Taking just the two coder case , the amount of agreement we would expect coders to reach by chance depends on the number and relative proportions of the categories used by the coders .", "For instance , consider what happens when the coders randomly place units into categories instead of using an established coding scheme .", "If there are two categories occurring in equal proportions , on average the coders would agree with each other half of the time each time the second coder makes a choice , there is a fifty fifty chance of coming up with the same category as the first coder .", "If , instead , the two coders were to use four categories in equal proportions , we would expect them to agree 25 of the time since no matter what the first coder chooses , there is a 25 chance that the second coder will agree .", "And if both coders were to use one of two categories , but use one of the categories 95 of the time , we would expect them to agree 90 . 5 of the time . 952 . 052 , or , in words , 95 of the time the first coder chooses the first category , with a . 95 chance of the second coder also choosing that category , and 5 of the time the first coder chooses the second category , with a . 05 chance of the second coder also doing so .", "This makes it impossible to interpret raw agreement figures using measure 2 .", "This same problem affects all of the possible ways of extending measure 2 to more than two coders .", "Now consider measure 3 , which has an advantage over measure 2 when there is a pool of coders , none of whom should be distinguished , in that it produces one figure that sums reliability over all coder pairs .", "Measure 3 still falls foul of the same problem with expected chance agreement as measure 2 because it does not take into account the number of categories occurring in the coding scheme .", "Measure 4 is a different approach to measuring over multiple undifferentiated coders .", "Note that although Passonneau and Litman are looking at the presence or absence of discourse segment boundaries , measure 4 takes into account agreement that a prosodic phrase boundary is not a discourse segment boundary , and therefore treats the problem as a two category distinction .", "Measure 4 falls foul of the same basic problem with chance agreement as measures 2 and 3 , but in addition , the statistic itself guarantees at least 50 agreement by only pairing off coders against the majority opinion .", "It also introduces an quot ; expert quot ; coder by the back door in assuming that the majority is always right , although this stance is somewhat at odds with Passonneau and Litman's subsequent assessment of a boundary's strength , from one to seven , based on the number of coders who noticed it .", "Measure 1 looks at almost exactly the same type of problem as measure 4 , the presence or absence of some kind of boundary .", "However , since one coder is explicitly designated as an quot ; expert , quot ; it does not treat the problem as a two category distinction , but looks only at cases where either coder marked a boundary as present .", "Without knowing the density of conversational move boundaries in the corpus , this makes it difficult to assess how well the coders agreed on the absence of boundaries , or to compare measures 1 and 4 .", "In addition , note that since false positives and missed negatives are rolled together in the denominator of the figure , measure 1 does not really distinguish expert and naive coder roles as much as it might .", "Nonetheless , this style of measure does have some advantages over measures 2 , 3 , and 4 , since these measures produce artificially high agreement figures when one category of a set predominates , as is the case with boundary judgments .", "One would expect measure 1 's results to be high under any circumstances , and it is not affected by the density of boundaries .", "So far , we have shown that all four of these measures produce figures that are at best , uninterpretable and at worst , misleading .", "KID make no comment about the meaning of their figures other than to say that the amount of agreement they show is reasonable ; Silverman et al . simply point out that where figures are calculated over different numbers of categories , they are not comparable .", "On the other hand , Passonneau and Litman note that their figures are not properly interpretable and attempt to overcome this failing to some extent by showing that the agreement which they have obtained at least significantly differs from random agreement .", "Their method for showing this is complex and of no concern to us here , since all it tells us is that it is safe to assume that the coders were not coding randomly reassuring , but no guarantee of reliability .", "It is more important to ask how different the results are from random and whether or not the data produced by coding is too noisy to use for the purpose for which it was collected .", "The concerns of these researchers are largely the same as those in the field of content analysis see especially Krippendorff 1980 and Weber 1985 , which has been through the same problems as we are currently facing and in which strong arguments have been made for using the kappa coefficient of agreement Siegel and Castellan 1988 as a measure of reliability !", "The kappa coefficient K measures pairwise agreement among a set of coders making category judgments , correcting for expected chance agreement where P A is the proportion of times that the coders agree and P E is the proportion of times that we would expect them to agree by chance , calculated along the lines of the intuitive argument presented above .", "For complete instructions on how to calculate K , see Siegel and Castellan 1988 .", "When there is no agreement other than that which would be expected by chance , K is zero .", "When there is total agreement , K is one .", "It is possible , and sometimes useful , to test whether or not K is significantly different from chance , but more importantly , interpretation of the scale of agreement is possible .", "Krippendorff 1980 discusses what constitutes an acceptable level of agreement , while giving the caveat that it depends entirely on what one intends to do with the , coding .", "For instance , he claims that finding associations between two variables that both rely on coding schemes with K . 7 is often impossible , and says that content analysis researchers generally think of K . 8 as good reliability , with . 67 K . 8 allowing tentative conclusions to be drawn .", "We would add two further caveats .", "First , although kappa addresses many of the problems we have been struggling with as a field , in order to compare K across studies , the underlying assumptions governing the calculation of chance expected agreement still require the units over which coding is performed to be chosen sensibly and comparably .", "To see this , compare , for instance , what would happen to the statistic if the same discourse boundary agreement data were calculated variously over a base of clause boundaries , transcribed word boundaries , and transcribed phoneme boundaries .", "Where no sensible choice of unit is available pretheoretically , measure 1 may still be preferred .", "Secondly , coding discourse and dialogue phenomena , and especially coding segment boundaries , may be inherently more difficult than many previous types of content analysis for instance , 1 There are several variants of the kappa coefficient in the literature , including one , Scott's pi , which actually has been used at least once in our field , to assess agreement on move boundaries in monologues using action assembly theory Grosz and Sidner 1986 .", "Krippendorff's a is more general than Siegel and Castellan's K in that Krippendorff extends the argument from category data to interval and ratio scales ; this extension might be useful for , for instance , judging the reliability of TOBI break index coding , since some researchers treat these codes as inherently scalar Silverman et al . 1992 .", "Krippendorff's a and Siegel and Castellan's K differ slightly when used on category judgments in the assumptions under which expected agreement is calculated .", "Here we use Siegel and Castellan's K because they explain their statistic more clearly , but the value of a is so closely related , especially under the usual expectations for reliability studies , that Krippendorff's statements about a hold , and we conflate the two under the more general name quot ; kappa . quot ; The advantages and disadvantages of different forms and extensions of kappa have been discussed in many fields but especially in medicine ; see , for example , Berry 1992 ; Goldman 1992 ; Kraemer 1980 ; Soeken and Prescott 1986 . dividing newspaper articles based on subject matter .", "Whether we have reached or will be able to reach a reasonable level of agreement in our work as a field remains to be seen ; our point here is merely that if , as a community , we adopt clearer statistics , we will be able to compare results in a standard way across different coding schemes and experiments and to evaluate current developments and that will illuminate both our individual results and the way forward .", "In assessing the amount of agreement among coders of category distinctions , the kappa statistic normalizes for the amount of expected chance agreement and allows a single measure to be calculated over multiple coders .", "This makes it applicable to the studies we have described , and more besides .", "However , we have yet to discuss the role of expert coders in such studies .", "KID designate one particular coder as the expert .", "Passonneau and Litman have only naive coders , but in essence have an expert opinion available on each unit classified in terms of the majority opinion .", "Silverman et al . treat all coders indistinguishably , although they do build an interesting argument about how agreement levels shift when a number of less experienced transcribers are added to a pool of highly experienced ones .", "We would argue that in subjective codings such as these , there are no real experts .", "We concur with Krippendorff that what counts is how totally naive coders manage based on written instructions .", "Comparing naive and expert coding as KID do can be a useful exercise , but rather than assessing the naive coders' accuracy , it in fact measures how well the instructions convey what these researchers think they do .", "Krippendorff gives well established techniques that generalize on this sort of quot ; odd man out quot ; result , which involve isolating particular coders , categories , and kinds of units to establish the source of any disagreement .", "In Passonneau and Litman , the reason for comparing to the majority opinion is less clear .", "Despite our argument , there are occasions when one opinion should be treated as the expert one .", "For instance , one can imagine determining whether coders using a simplified coding scheme match what can be obtained by some better but more expensive method , which might itself be either objective or subjective .", "In these cases , we would argue that it is still appropriate to use the kappa statistic , in a variation which looks only at pairings of agreement with the expert opinion rather than at all possible pairs of coders .", "This variation could be achieved by interpreting P A as the proportion of times that the naive coders agree with the expert and P E as the proportion of times we would expect the naive coders to agree with the expert by chance .", "We have shown that existing measures of reliability in discourse and dialogue work are difficult to interpret , and we have suggested a replacement measure , the kappa statistic , which has a number of advantages over these measures .", "Kappa is widely accepted in the field of content analysis .", "It is interpretable , allows different results to be compared , and suggests a set of diagnostics in cases where the reliability results are not good enough for the required purpose .", "We suggest that this measure be adopted more widely within our own research community .", "This work was supported by grant number Human Computer Interaction and an G9111013 of the U . K . Joint Councils Interdisciplinary Research Centre Grant"], "summary_lines": ["Assessing Agreement On Classification Tasks: The Kappa Statistic\n", "Currently, computational linguists and cognitive scientists working in the area of discourse and dialogue argue that their subjective judgments are reliable using several different statistics, none of which are easily interpretable or comparable to each other.\n", "Meanwhile, researchers in content analysis have already experienced the same difficulties and come up with a solution in the kappa statistic.\n", "We discuss what is wrong with reliability measures as they are currently used for discourse and dialogue work in computational linguistics and cognitive science, and argue that we would be better off as afield adopting techniques from content analysis.\n", "Our method, kappa statistic, is used extensively in empirical studies of discourse (Carletta, 1996).\n"]}
{"article_lines": ["Domain Specific Sense Distributions And Predominant Sense Acquisition", "Distributions of the senses of words are often highly skewed .", "This fact is exploitedby word sense disambiguation WSD sys tems which back off to the predominant sense of a word when contextual clues arenot strong enough .", "The domain of a doc ument has a strong influence on the sensedistribution of words , but it is not feasi ble to produce large manually annotated corpora for every domain of interest .", "In this paper we describe the construction of three sense annotated corpora in different domains for a sample of English words . We apply an existing method for acquiring predominant sense information automatically from raw text , and for our sam ple demonstrate that 1 acquiring suchinformation automatically from a mixeddomain corpus is more accurate than de riving it from SemCor , and 2 acquiringit automatically from text in the same do main as the target domain performs best by a large margin .", "We also show that for an all words WSD task this automatic method is best focussed on words that are salient to the domain , and on words with a different acquired predominant sense in that domain compared to that acquired from a balanced corpus .", "From analysis of manually sense tagged corpora , Kilgarriff 2004 has demonstrated that distributions of the senses of words are often highly skewed .", "Most researchers working on word sense disambiguation WSD use manually sense tagged data such as SemCor Miller et al , 1993 to train statistical classi fiers , but also use the information in SemCor on theoverall sense distribution for each word as a back off model .", "In WSD , the heuristic of just choosing themost frequent sense of a word is very powerful , especially for words with highly skewed sense distri butions Yarowsky and Florian , 2002 .", "Indeed , only 5 out of the 26 systems in the recent SENSEVAL 3 English all words task Snyder and Palmer , 2004 outperformed the heuristic of choosing the most fre quent sense as derived from SemCor which wouldgive 61 . 5 precision and recall1 .", "Furthermore , sys tems that did outperform the first sense heuristic did so only by a small margin the top score being 65 precision and recall .", "Over a decade ago , Gale et al 1992 observed the tendency for one sense of a word to prevail in a given discourse .", "To take advantage of this , a method for automatically determining the ? one sense ?", "given a discourse or document is required .", "Magnini et al 2002 have shown that information about the do main of a document is very useful for WSD .", "This isbecause many concepts are specific to particular domains , and for many words their most likely mean ing in context is strongly correlated to the domain of the document they appear in .", "Thus , since word sense distributions are skewed and depend on the domain at hand we would like to know for each domain of application the most likely sense of a word .", "However , there are no extant domain specificsense tagged corpora to derive such sense distribution information from .", "Producing them would be ex tremely costly , since a substantial corpus would have to be annotated by hand for every domain of interest .", "In response to this problem , McCarthy et al 2004 proposed a method for automatically inducing the1This figure is the mean of two different estimates Sny der and Palmer , 2004 , the difference being due to multiword handling .", "419 predominant sense of a word from raw text .", "They carried out a limited test of their method on text in two domains using subject field codes Magnini andCavaglia , 2000 to assess whether the acquired pre dominant sense information was broadly consistent with the domain of the text it was acquired from . But they did not evaluate their method on hand tagged domain specific corpora since there was no such data publicly available .", "In this paper , we evaluate the method on domainspecific text by creating a sense annotated gold standard2 for a sample of words .", "We used a lexical sam ple because the cost of hand tagging several corpora for an all words task would be prohibitive .", "We show that the sense distributions of words in this lexical sample differ depending on domain .", "We also showthat sense distributions are more skewed in domain specific text .", "Using McCarthy et al ? s method , weautomatically acquire predominant sense informa tion for the lexical sample from the raw corpora , and evaluate the accuracy of this and predominant sense information derived from SemCor .", "We show that in our domains and for these words , first sense information automatically acquired from a general corpus is more accurate than first senses derived from SemCor .", "We also show that deriving first senseinformation from text in the same domain as the tar get data performs best , particularly when focusing on words which are salient to that domain .", "The paper is structured as follows .", "In section 2 we summarise McCarthy et al ? s predominant sense method .", "We then section 3 describe the new gold standard corpora , and evaluate predominant sense accuracy section 4 .", "We discuss the results with a proposal for applying the method to an all words task , and an analysis of our results in terms of this proposal before concluding with future directions .", "We use the method described in McCarthy et al 2004 for finding predominant senses from raw text .", "The method uses a thesaurus obtained from the text by parsing , extracting grammatical relations and then listing each word with its top nearest neighbours , where is a constant .", "Like McCarthy 2This resource will be made publicly available for research purposes in the near future .", "et al 2004 we use and obtain our thesaurus using the distributional similarity metric described by Lin 1998 .", "We use WordNet WN as our sense inventory .", "The senses of a word are each assigned a ranking score which sums over the distributional similarity scores of the neighbours and weights eachneighbour ? s score by a WN Similarity score Pat wardhan and Pedersen , 2003 between the sense of and the sense of the neighbour that maximises the WN Similarity score .", "This weight is normalised by the sum of such WN similarity scores between all senses of and and the senses of the neighbour that maximises this score .", "We use the WN Similarity jcnscore Jiang and Conrath , 1997 since this gave rea sonable results for McCarthy et al and it is efficientat run time given precompilation of frequency information .", "The jcn measure needs word frequency information , which we obtained from the British National Corpus BNC Leech , 1992 .", "The distributional thesaurus was constructed using subject , direct object adjective modifier and noun modifier re lations .", "In our experiments , we compare for a sampleof nouns the sense rankings created from a bal anced corpus the BNC with rankings created from domain specific corpora FINANCE and SPORTS extracted from the Reuters corpus Rose et al , 2002 .", "In more detail , the three corpora are BNC The ? written ?", "documents , amounting to 3209 documents around 89 . 7M words , and covering a wide range of topic domains .", "FINANCE 117734 FINANCE documents around 32 . 5M words topic codes ECAT and MCAT SPORTS 35317 SPORTS documents around 9 . 1M words topic code GSPO We computed thesauruses for each of these corpora using the procedure outlined in section 2 .", "3 . 1 Word Selection .", "In our experiments we used FINANCE and SPORTS domains .", "To ensure that a significant number of the chosen words are relevant for these domains , we did not choose the words for our experiments completely randomly .", "The first selection criterionwe applied used the Subject Field Code SFC re 420 source Magnini and Cavaglia , 2000 , which assignsdomain labels to synsets in WN version 1 . 6 .", "We se lected all the polysemous nouns in WN 1 . 6 that have at least one synset labelled SPORT and one synset labelled FINANCE .", "This reduced the set of words to 38 .", "However , some of these words were fairly obscure , did not occur frequently enough in one of the domain corpora or were simply too polysemous . We narrowed down the set of words using the crite ria 1 frequency in the BNC 1000 , 2 at most12 senses , and 3 at least 75 examples in each corpus .", "Finally a couple of words were removed because the domain specific sense was particularly ob scure3 .", "The resulting set consists of 17 words4 club , manager , record , right , bill , check , competition , conversion , crew , delivery , division , fishing , reserve , re turn , score , receiver , running We refer to this set of words as FS cds .", "The first four words occur in the BNC with high frequency 10000 occurrences , the last two with low frequency 2000 and the rest are mid frequency .", "Three further sets of words were selected on the basis of domain salience .", "We chose eight words that are particularly salient in the Sport corpus referred to as S sal , eight in the Finance corpus F sal , and seven that had equal not necessarily high salience in both , eq sal .", "We computed salience as a ratio of normalised document frequencies , using the formula fifffl fffl where ffi !", "is the number of documents in domain containing the noun lemma , ffi !", "is the number of documents in domain , ffi is the total number of documents containing the noun and ffi is the total number of documents . To obtain the sets S sal , F sal and eq sal we gen erated the 50 most salient words for both domainsand 50 words that were equally salient for both do mains .", "These lists of 50 words were subjected to the same constraints as set FS cds , that is occurring in the BNC 1000 , having at most 12 senses , and having at least 75 examples in each corpus .", "From the remaining words we randomly sampled 8 words 3For example the Finance sense of ? eagle ?", "a former gold coin in US worth 10 dollars is very unlikely to be found . 4One more word , ? pitch ? , was in the original selection .", "However , we did not obtain enough usable annotated sentences sec tion 3 . 2 for this particular word and therefore it was discarded .", "from the Sport salience list and Finance list and 7 from the salience list for words with equal salience in both domains .", "The resulting sets of words are S sal fan , star , transfer , striker , goal , title , tie , coach F sal package , chip , bond , market , strike , bank , share , target eq sal will , phase , half , top , performance , level , country The average degree of polysemy for this set of 40 nouns in WN version 1 . 7 . 1 is 6 . 6 .", "3 . 2 The Annotation Task .", "For the annotation task we recruited linguistics stu dents from two universities .", "All ten annotators are native speakers of English . We set up annotation as an Open Mind Word Ex pert task5 .", "Open Mind is a web based system for annotating sentences .", "The user can choose a word from a pull down menu .", "When a word is selected , the user is presented with a list of sense definitions .", "The sense definitions were taken from WN1 . 7 . 1 andpresented in random order .", "Below the sense defini tions , sentences with the target word highlighted are given .", "Left of the sentence on the screen , there are as many tick boxes as there are senses for the word plus boxes for ? unclear ?", "and ? unlisted sense ? . The annotator is expected to first read the sense defi nitions carefully and then , after reading the sentence , decide which sense is best for the instance of the word in a particular sentence .", "Only the sentence inwhich the word appears is presented not more sur rounding sentences .", "In case the sentence does notgive enough evidence to decide , the annotator is ex pected to check the ? unclear ?", "box .", "When the correct sense is not listed , the annotator should check the ? unlisted sense ?", "box .", "The sentences to be annotated were randomly sampled from the corpora .", "The corpora were first part of speech tagged and lemmatised using RASP Briscoe and Carroll , 2002 .", "Up to 125 sentences were randomly selected for each word from eachcorpus .", "Sentences with clear problems e . g . contain ing a begin or end of document marker , or mostly not text were removed .", "The first 100 remaining sentences were selected for the task .", "For a few 5http www . teach computers . org word expert english 421words there were not exactly 100 sentences per cor pus available .", "The Reuters corpus contains quite a few duplicate documents .", "No attempts were made to remove duplicates .", "3 . 3 Characterisation of the Annotated Data .", "Most of the sentences were annotated by at least three people .", "Some sentences were only done by two annotators .", "The complete set of data comprises 33225 tagging acts .", "The inter annotator agreement on the complete set of data was 65 6 .", "For the BNC data it was 60 , for the Sports data 65 and for the Finance data 69 . This is lower than reported for other sets of anno tated data for example it was 75 for the nouns in the SENSEVAL 2 English all words task , but quite close to the reported 62 . 8 agreement between the first two taggings for single noun tagging for the SENSEVAL 3 English lexical sample task Mihalceaet al , 2004 .", "The fairest comparison is probably be tween the latter and the inter annotator agreement for the BNC data .", "Reasons why our agreement is relatively low include the fact that almost all of the sentences are annotated by three people , and also the high degree of polysemy of this set of words .", "Problematic cases The unlisted category was used as a miscellaneous category .", "In some cases a sense was truly missing from the inventory e . g . the word ? tie ?", "has a ? game ?", "sense in British English which is not included in WN 1 . 7 . 1 .", "In other cases we had not recognised thatthe word was really part of a multiword e . g . a num ber of sentences for the word ? chip ?", "contained themultiword ? blue chip ? .", "Finally there were a num ber of cases where the word had been assigned the wrong part of speech tag e . g . the verb ? will ?", "had often been mistagged as a noun .", "We identified and removed all these systematic problem cases from theunlisted senses .", "After removing the problematic un listed cases , we had between 0 . 9 FINANCE and 4 . 5 SPORTS unlisted instances left .", "We also had between 1 . 8 SPORTS and 4 . 8 BNC unclearinstances .", "The percentage of unlisted instances re flects the fit of WN to the data whilst that of unclear cases reflects the generality of the corpus . 6To compute inter annotator agreement we used Amruta Pu randare and Ted Pedersen ? s OMtoSVAL2 Package version 0 . 01 .", "The sense distributions WSD accuracy is strongly related to the entropy of the sense distribution of the target word Yarowskyand Florian , 2002 .", "The more skewed the sense dis tribution is towards a small percentage of the senses , the lower the entropy .", "Accuracy is related to this because there is more data both training and test shared between fewer of the senses .", "When the first sense is very predominant exceeding 80 it is hard for any WSD system to beat the heuristic of always selecting that sense Yarowsky and Florian , 2002 .", "The sense distribution for a given word may varydepending on the domain of the text being processed .", "In some cases , this may result in a differ ent predominant sense ; other characteristics of the sense distribution may also differ such as entropy ofthe sense distribution and the dominance of the pre dominant sense .", "In Table 1 we show the entropy per word in our sample and relative frequency relfr ofits first sense fs , for each of our three gold stan dard annotated corpora .", "We compute the entropy ofa word ? s sense distribution as a fraction of the pos sible entropy Yarowsky and Florian , 2002 , . 0 21fi3 5476982 ; 69869 where ? BADC 6982 ; 6986FE G H2IKJFL E MG . This measure reduces the impact of the number of senses of a word and focuses on the uncertainty within thedistribution .", "For each corpus , we also show the av erage entropy and average relative frequency of the first sense over all words . From Table 1 we can see that for the vast ma jority of words the entropy is highest in the BNC . However there are exceptions return , fan and ti tle for FINANCE and return , half , level , running strike and share for SPORTS .", "Surprisingly , eq sal words , which are not particularly salient in eitherdomain , also typically have lower entropy in the domain specific corpora compared to the BNC .", "Pre sumably this is simply because of this small set ofwords , which seem particularly skewed to the fi nancial domain .", "Note that whilst the distributionsin the domain specific corpora are more skewed to wards a predominant sense , only 7 of the 40 words in the FINANCE corpus and 5 of the 40 words in the SPORTS corpus have only one sense attested .", "Thus , even in domain specific corpora ambiguity is 422 Training Testing BNC FINANCE SPORTS BNC 40 . 7 43 . 3 33 . 2 FINANCE 39 . 1 49 . 9 24 . 0 SPORTS 25 . 7 19 . 7 43 . 7 Random BL 19 . 8 19 . 6 19 . 4 SemCor FS 32 . 0 32 . 9 33 . 9 35 . 0 16 . 3 16 . 8 Table 2 WSD using predominant senses , training and testing on all domain combinations .", "still present , even though it is less than for general text .", "We show the sense number of the first sense fs alongside the relative frequency of that sense .", "We use ? ucl ?", "for unclear and ? unl ?", "for unlisted senses where these are predominant in our annotated data . Although the predominant sense of a word is not al ways the domain specific sense in a domain specific corpus , the domain specific senses typically occurmore than they do in non relevant corpora .", "For ex ample , sense 11 of return a tennis stroke was notthe first sense in SPORTS , however it did have a rel ative frequency of 19 in that corpus and was absent from BNC and FINANCE .", "We have run the predominant sense finding algo rithm on the raw text of each of the three corporain turn the first step being to compute a distribu tional similarity thesaurus for each , as outlined in section 2 .", "We evaluate the accuracy of performingWSD purely with the predominant sense heuristic us ing all 9 combinations of training and test corpora .", "The results are presented in Table 2 .", "The random baseline is ? ADCON MP 82 ; 6 Q 476982 ; 6986M A . We also give the .", "accuracy using a first sense heuristic from SemCor ? SemCor FS ? ; the precision is given alongside inbrackets because a predominant sense is not sup plied by SemCor for every word .", "7 The automatic method proposes a predominant sense in every case .", "The best results are obtained when training on a domain relevant corpus .", "In all cases , when training on appropriate training data the automatic methodfor finding predominant senses beats both the ran dom baseline and the baseline provided by SemCor . Table 3 compares WSD accuracy using the auto matically acquired first sense on the 4 categories of 7There is one such word in our sample , striker .", "Test Train FS cds F sal S sal eq sal BNC APPR 33 . 3 51 . 5 39 . 7 48 . 0 BNC SC 28 . 3 44 . 0 24 . 6 36 . 2 FINANCE APPR 37 . 0 70 . 2 38 . 5 70 . 1 FINANCE SC 30 . 3 51 . 1 22 . 9 33 . 5 SPORTS APPR 42 . 6 18 . 1 65 . 7 46 . 9 SPORTS SC 9 . 4 38 . 1 13 . 2 12 . 2Table 3 WSD using predominant senses , with train ing data from the same domain or from SemCor .", "words FS cds , F sal , S sal and eq sal separately .", "Results using the training data from the appropriate domain e . g . SPORTS training data for SPORTS test data are indicated with ? APPR ?", "and contrasted with the results using SemCor data , indicated with ? SC ? .", "8We see that for words which are pertinent to the do main of the test text , it pays to use domain specific training data .", "In some other cases , e . g . F sal tested on SPORTS , it is better to use SemCor data .", "For the eq sal words , accuracy is highest when FINANCEdata is used for training , reflecting their bias to fi nancial senses as noted in section 3 . 3 .", "We are not aware of any other domain specific man ually sense tagged corpora .", "We have created sensetagged corpora from two specific domains for a sam ple of words , and a similar resource from a balanced corpus which covers a wide range of domains .", "Wehave used these resources to do a quantitative evaluation which demonstrates that automatic acquisi tion of predominant senses outperforms the SemCor baseline for this sample of words . The domain specific manually sense tagged resource is an interesting source of information in it self .", "It shows for example that at least for this particular lexical sample , the predominant sense is much more dominant in a specific domain than it is in the general case , even for words which are notparticularly salient in that domain .", "Similar obser vations can be made about the average number ofencountered senses and the skew of the sense distributions .", "It also shows that although the predom inant sense is more dominant and domain specific 8For SemCor , precision figures for the S sal words are up to 4 higher than the accuracy figures given , however they are still lower than accuracy using the domain specific corpora ; we leave them out due to lack of space .", "423 senses are used more within a specific domain , there is still a need for taking local context into account when disambiguating words .", "The predomi nant sense heuristic is hard to beat for some wordswithin a domain , but others remain highly ambiguous even within a specific domain .", "The return ex ample in section 3 . 3 illustrates this .", "Our results are for a lexical sample because we did not have the resources to produce manually tagged domain specific corpora for an all words task .", "Although sense distribution data derived fromSemCor can be more accurate than such informa tion derived automatically McCarthy et al , 2004 , in a given domain there will be words for whichthe SemCor frequency distributions are inappropriate or unavailable .", "The work presented here demonstrates that the automatic method for finding pre dominant senses outperforms SemCor on a sampleof words , particularly on ones that are salient to a do main .", "As well as domain salient words , there will be words which are not particularly salient but still havedifferent distributions than in SemCor .", "We therefore propose that automatic methods for determin ing the first sense should be used when either there is no manually tagged data , or the manually taggeddata seems to be inappropriate for the word and do main under consideration .", "While it is trivial to findthe words which are absent or infrequent in train ing data , such as SemCor , it is less obvious how to find words where the training data is not appropriate .", "One way of finding these words would be to look for differences in the automatic sense rankings of words in domain specific corpora compared to those of the same words in balanced corpora , such as the BNC .", "We assume that the sense rankings from a balancedtext will more or less correlate with a balanced resource such as SemCor .", "Of course there will be dif ferences in the corpus data , but these will be less radical than those between SemCor and a domain specific corpus .", "Then the automatic ranking methodshould be applied in cases where there is a clear deviation in the ranking induced from the domain specific corpus compared to that from the balanced cor pus .", "Otherwise , SemCor is probably more reliable if data for the given word is available .", "There are several possibilities for the definition of ? clear deviation ?", "above .", "One could look at differences in the ranking over all words , using a mea Training Testing FINANCE SPORTSFinance 35 . 5 Sports 40 . 9 SemCor 14 . 2 15 . 3 10 . 0 Table 4 WSD accuracy for words with a different first sense to the BNC .", "sure such as pairwise agreement of rankings or a ranking correlation coefficient , such as Spearman ? s . One could also use the rankings to estimate prob ability distributions and compare the distributions with measures such as alpha skew divergence Lee , 1999 .", "A simple definition would be where the rankings assign different predominant senses to a word .", "Taking this simple definition of deviation , we demonstrate how this might be done for our corpora .", "We compared the automatic rankings from the BNC with those from each domain specific corpus SPORTS and FINANCE for all polysemous nouns in SemCor .", "Although the majority are assigned thesame first sense in the BNC as in the domain spe cific corpora , a significant proportion 31 SPORTS and 34 FINANCE are not .", "For all words WSD in either of these domains , it would be these wordsfor which automatic ranking should be used .", "Ta ble 4 shows the WSD accuracy using this approach for the words in our lexical sample with a differentautomatically computed first sense in the BNC com pared to the target domain SPORTS or FINANCE .", "We trained on the appropriate domain for each test corpus , and compared this with using SemCor first sense data .", "The results show clearly that using this approach to decide whether to use automatic sense rankings performs much better than always using SemCor rankings .", "The method for automatically finding the predominant sense beat SemCor consistently in our experiments .", "So for some words , it pays to obtain auto matic information on frequency distributions from appropriate corpora .", "Our sense annotated corpora exhibit higher entropy for word sense distributions for domain specific text , even for words which are not specific to that domain .", "They also show that different senses predominate in different domains 424 and that dominance of the first sense varies to a great extent , depending on the word .", "Previous workin all words WSD has indicated that techniques us ing hand tagged resources outperform unsupervisedmethods .", "However , we demonstrate that it is possi ble to apply a fully automatic method to a subset ofpertinent words to improve WSD accuracy .", "The au tomatic method seems to lead to better performance for words that are salient to a domain .", "There are alsoother words which though not particularly domainsalient , have a different sense distribution to that an ticipated for a balanced corpus .", "We propose that inorder to tackle an all words task , automatic methods should be applied to words which have a sub stantial difference in sense ranking compared to that obtained from a balanced corpus .", "We demonstrate that for a set of words which meet this condition , the performance of the automatic method is far bet ter than when using data from SemCor .", "We will dofurther work to ascertain the best method for quanti fying ? substantial change ? .", "We also intend to exploit the automatic rankingto obtain information on sense frequency distribu tions rather than just predominant senses given the genre as well as the domain of the text .", "We plan to combine this with local context , using collocates of neighbours in the thesaurus , for contextual WSD .", "Acknowledgements We would like to thank Siddharth Patwardhan and Ted Pedersen for making the WN Similarity package available , Rada Mihalceaand Tim Chklovski for making the Open Mind software avail able to us and Julie Weeds for the thesaurus software .", "The work was funded by EU 2001 34460 project MEANING , UK EPSRC project ? Ranking Word Sense for Word Sense Disambiguation ?", "and the UK Royal Society ."], "summary_lines": ["Domain-Specific Sense Distributions And Predominant Sense Acquisition\n", "Distributions of the senses of words are often highly skewed.\n", "This fact is exploited by word sense disambiguation (WSD) systems which back off to the predominant sense of a word when contextual clues are not strong enough.\n", "The domain of a document has a strong influence on the sense distribution of words, but it is not feasible to produce large manually annotated corpora for every domain of interest.\n", "In this paper we describe the construction of three sense annotated corpora in different domains for a sample of English words.\n", "We apply an existing method for acquiring predominant sense information automatically from raw text, and for our sample demonstrate that (1) acquiring such information automatically from a mixed-domain corpus is more accurate than deriving it from SemCor, and (2) acquiring it automatically from text in the same domain as the target domain performs best by a large margin.\n", "We also show that for an all words WSD task this automatic method is best focussed on words that are salient to the domain, and on words with a different acquired predominant sense in that domain compared to that acquired from a balanced corpus.\n", "Our dataset is made up of 3 collections of documents: a domain-neutral corpus (BNC), and two domain-specific corpora (SPORTS and FINANCE).\n"]}
{"article_lines": ["Named Entity Recognition Using An HMM Based Chunk Tagger", "This paper proposes a Hidden Markov Model HMM and an HMM based chunk tagger , from which a named entity NE recognition NER system is built to recognize and classify names , times and numerical quantities .", "Through the HMM , our system is able to apply and integrate four types of internal and external evidences 1 simple deterministic internal feature of the words , such as capitalization and digitalization ; 2 internal semantic feature of important triggers ; 3 internal gazetteer feature ; 4 external macro context feature .", "In this way , the NER problem can be resolved effectively .", "Evaluation of our system on MUC 6 and MUC 7 English NE tasks achieves F measures of 96 . 6 and 94 . 1 respectively .", "It shows that the performance is significantly better than reported by any other machine learning system .", "Moreover , the performance is even consistently better than those based on handcrafted rules .", "Named Entity NE Recognition NER is to classify every word in a document into some predefined categories and quot ; none of the above quot ; .", "In the taxonomy of computational linguistics tasks , it falls under the domain of quot ; information extraction quot ; , which extracts specific kinds of information from documents as opposed to the more general task of quot ; document management quot ; which seeks to extract all of the information found in a document .", "Since entity names form the main content of a document , NER is a very important step toward more intelligent information extraction and management .", "The atomic elements of information extraction indeed , of language as a whole could be considered as the quot ; who quot ; , quot ; where quot ; and quot ; how much quot ; in a sentence .", "NER performs what is known as surface parsing , delimiting sequences of tokens that answer these important questions .", "NER can also be used as the first step in a chain of processors a next level of processing could relate two or more NEs , or perhaps even give semantics to that relationship using a verb .", "In this way , further processing could discover the quot ; what quot ; and quot ; how quot ; of a sentence or body of text .", "While NER is relatively simple and it is fairly easy to build a system with reasonable performance , there are still a large number of ambiguous cases that make it difficult to attain human performance .", "There has been a considerable amount of work on NER problem , which aims to address many of these ambiguity , robustness and portability issues .", "During last decade , NER has drawn more and more attention from the NE tasks Chinchor95a Chinchor98a in MUCs MUC6 MUC7 , where person names , location names , organization names , dates , times , percentages and money amounts are to be delimited in text using SGML mark ups .", "Previous approaches have typically used manually constructed finite state patterns , which attempt to match against a sequence of words in much the same way as a general regular expression matcher .", "Typical systems are Univ . of Sheffield's LaSIE II Humphreys 98 , ISOQuest's NetOwl Aone 98 Krupha 98 and Univ . of Edinburgh's LTG Mikheev 98 Mikheev 99 for English NER .", "These systems are mainly rule based .", "However , rule based approaches lack the ability of coping with the problems of robustness and portability .", "Each new source of text requires significant tweaking of rules to maintain optimal performance and the maintenance costs could be quite steep .", "The current trend in NER is to use the machine learning approach , which is more attractive in that it is trainable and adaptable and the maintenance of a machine learning system is much cheaper than that of a rule based one .", "The representative machine learning approaches used in NER are HMM BBN's IdentiFinder in Miller 98 Bikel 99 and KRDL's system Yu 98 for Chinese NER .", ", Maximum Entropy New York Univ .", "'s MEME in Borthwick 98 Borthwich99 and Decision Tree New York Univ .", "'s system in Sekine98 and SRA's system in Bennett 97 .", "Besides , a variant of Eric Brill's transformation based rules Brill95 has been applied to the problem Aberdeen 95 .", "Among these approaches , the evaluation performance of HMM is higher than those of others .", "The main reason may be due to its better ability of capturing the locality of phenomena , which indicates names in text .", "Moreover , HMM seems more and more used in NE recognition because of the efficiency of the Viterbi algorithm Viterbi67 used in decoding the NE class state sequence .", "However , the performance of a machine learning system is always poorer than that of a rule based one by about 2 Chinchor95b Chinchor98b .", "This may be because current machine learning approaches capture important evidence behind NER problem much less effectively than human experts who handcraft the rules , although machine learning approaches always provide important statistical information that is not available to human experts .", "As defined in McDonald96 , there are two kinds of evidences that can be used in NER to solve the ambiguity , robustness and portability problems described above .", "The first is the internal evidence found within the word and or word string itself while the second is the external evidence gathered from its context .", "In order to effectively apply and integrate internal and external evidences , we present a NER system using a HMM .", "The approach behind our NER system is based on the HMM based chunk tagger in text chunking , which was ranked the best individual system Zhou 00a Zhou 00b in CoNLL'2000 Tjong 00 .", "Here , a NE is regarded as a chunk , named quot ; NE Chunk quot ; .", "To date , our system has been successfully trained and applied in English NER .", "To our knowledge , our system outperforms any published machine learning systems .", "Moreover , our system even outperforms any published rule based systems .", "The layout of this paper is as follows .", "Section 2 gives a description of the HMM and its application in NER HMM based chunk tagger .", "Section 3 explains the word feature used to capture both the internal and external evidences .", "Section 4 describes the back off schemes used to tackle the sparseness problem .", "Section 5 gives the experimental results of our system .", "Section 6 contains our remarks and possible extensions of the proposed work .", "Given a token sequence G1n g1g2 g , the goal The second item in 2 1 is the mutual information between T1n and n simplify the computation of this item , we assume mutual information independence The basic premise of this model is to consider the raw text , encountered when decoding , as though it had passed through a noisy channel , where it had been originally marked with NE tags .", "The job of our generative model is to directly generate the original NE tags from the output words of the noisy channel .", "It is obvious that our generative model is reverse to the generative model of traditional HMM1 , as used in BBN's IdentiFinder , which models the original process that generates the NE class annotated words from the original NE tags .", "Another difference is that our model assumes mutual information independence 2 2 while traditional HMM assumes conditional probability independence I 1 .", "Assumption 2 2 is much looser than assumption I 1 because assumption I 1 has the same effect with the sum of assumptions 2 2 and I 3 2 .", "In this way , our model can apply more context information to determine the tag of current token .", "From equation 2 4 , we can see that We will not discuss both the first and second items further in this paper .", "This paper will focus on difference between our tagger and other traditional HMM based taggers , as used in BBN's IdentiFinder .", "Ideally , it can be estimated by using the forward backward algorithm Rabiner89 recursively for the 1st order Rabiner89 or 2nd order HMMs Watson 92 .", "However , an alternative back off modeling approach is applied instead in this paper more details in section 4 . arg max log Then we assume conditional probability word sequence and F1n f 1 f2 . . . fn is the word feature sequence .", "In the meantime , NE chunk tag ti is structural and consists of three parts Obviously , there exist some constraints between ti 1 and ti on the boundary and entity categories , as shown in Table 1 , where quot ; valid quot ; quot ; invalid quot ; means the tag sequence ti 1ti is valid invalid while quot ; valid on quot ; means ti 1ti is valid with an additional condition ECi 1 ECi .", "Such constraints have been used in Viterbi decoding algorithm to ensure valid NE chunking .", "As stated above , token is denoted as ordered pairs of word feature and word itself gi fi , wi .", "Here , the word feature is a simple deterministic computation performed on the word and or word string with appropriate consideration of context as looked up in the lexicon or added to the context .", "In our model , each word feature consists of several sub features , which can be classified into internal sub features and external sub features .", "The internal sub features are found within the word and or word string itself to capture internal evidence while external sub features are derived within the context to capture external evidence .", "Our model captures three types of internal sub features 1 1 f simple deterministic internal feature of the words , such as capitalization and digitalization ; 2 f 2 internal semantic feature of important triggers ; 3 f 3 internal gazetteer feature . f is the basic sub feature exploited in this model , as shown in Table 2 with the descending order of priority .", "For example , in the case of non disjoint feature classes such as ContainsDigitAndAlpha and ContainsDigitAndDash , the former will take precedence .", "The first eleven features arise from the need to distinguish and annotate monetary amounts , percentages , times and dates .", "The rest of the features distinguish types of capitalization and all other words such as punctuation marks .", "In particular , the FirstWord feature arises from the fact that if a word is capitalized and is the first word of the sentence , we have no good information as to why it is capitalized but note that AllCaps and CapPeriod are computed before FirstWord , and take precedence .", "This sub feature is language dependent .", "Fortunately , the feature computation is an extremely small part of the implementation .", "This kind of internal sub feature has been widely used in machine learning systems , such as BBN's IdendiFinder and New York Univ .", "'s MENE .", "The rationale behind this sub feature is clear a capitalization gives good evidence of NEs in Roman languages ; b Numeric symbols can automatically be grouped into categories .", "2 2 f is the semantic classification of important triggers , as seen in Table 3 , and is unique to our system .", "It is based on the intuitions that important triggers are useful for NER and can be classified according to their semantics .", "This sub feature applies to both single word and multiple words .", "This set of triggers is collected semi automatically from the NEs and their local context of the training data . determined by finding a match in the gazetteer of the corresponding NE type where n in Table 4 represents the word number in the matched word string .", "In stead of collecting gazetteer lists from training data , we collect a list of 20 public holidays in several countries , a list of 5 , 000 locations from websites such as GeoHive3 , a list of 10 , 000 organization names from websites such as Yahoo4 and a list of 10 , 000 famous people from websites such as Scope Systems5 .", "Gazetters have been widely used in NER systems to improve performance .", "For external evidence , only one external macro context feature 4 f , as shown in Table 5 , is captured in our model .", "4 f is about whether and how the encountered NE candidate is occurred in the list of NEs already recognized from the document , as shown in Table 5 n is the word number in the matched NE from the recognized NE list and m is the matched word number between the word string and the matched NE with the corresponding NE type . .", "This sub feature is unique to our system .", "The intuition behind this is the phenomena of name alias .", "During decoding , the NEs already recognized from the document are stored in a list .", "When the system encounters a NE candidate , a name alias algorithm is invoked to dynamically determine its relationship with the NEs in the recognized list .", "Initially , we also consider part of speech POS sub feature .", "However , the experimental result is disappointing that incorporation of POS even decreases the performance by 2 .", "This may be because capitalization information of a word is submerged in the muddy of several POS tags and the performance of POS tagging is not satisfactory , especially for unknown capitalized words since many of NEs include unknown capitalized words . .", "Therefore , POS is discarded .", "Given the model in section 2 and word feature in section 3 , the main problem is how to sufficient training data for every event whose conditional probability we wish to calculate .", "Unfortunately , there is rarely enough training data to compute accurate probabilities when decoding on new data , especially considering the complex word feature described above .", "In order to resolve the sparseness problem , two levels of back off modeling are applied to approximate 1 1 First level back off scheme is based on different contexts of word features and words themselves , and n descending order of fi 2 fi 1 fi wi , fi w ifi 1fi 2 , fi 1fiwi , fiwifi 1 , f i 1 wi 1 f i , fifi 1wi 1 , fi 2fi 1 f i , f i f i 1 f i 2 , fi wi , fi 2fi 1fi , fifi 1 and fi .", "2 The second level back off scheme is based on different combinations of the four sub features described in section 3 , and fk is approximated in the descending order of 12 3 4 f k f k f k fk , 1 3 fk fk , fk fk , 1 2 1 4fkfk and 1 fk .", "In this section , we will report the experimental results of our system for English NER on MUC 6 and MUC 7 NE shared tasks , as shown in Table 6 , and then for the impact of training data size on performance using MUC 7 training data .", "For each experiment , we have the MUC dry run data as the held out development data and the MUC formal test data as the held out test data .", "For both MUC 6 and MUC 7 NE tasks , Table 7 shows the performance of our system using MUC evaluation while Figure 1 gives the comparisons of our system with others .", "Here , the precision P measures the number of correct NEs in the answer file over the total number of NEs in the answer file and the recall R measures the number of correct NEs in the answer file over the total number of NEs in the key file while F measure is the weighted harmonic mean of precision and recall with \u03b22 1 .", "It shows that the performance is significantly better than reported by any other machine learning system .", "Moreover , the performance is consistently better than those based on handcrafted rules .", "With any learning technique , one important question is how much training data is required to achieve acceptable performance .", "More generally how does the performance vary as the training data size changes ?", "The result is shown in Figure 2 for MUC 7 NE task .", "It shows that 200KB of training data would have given the performance of 90 while reducing to 100KB would have had a significant decrease in the performance .", "It also shows that our system still has some room for performance improvement .", "This may be because of the complex word feature and the corresponding sparseness problem existing in our system .", "Another important question is about the effect of different sub features .", "Table 8 answers the question on MUC 7 NE task 1 Applying only 1 f gives our system the performance of 77 . 6 .", "2 2 f is very useful for NER and increases the performance further by 10 to 87 . 4 .", "3 4 f is impressive too with another 5 . 5 performance improvement .", "4 However , 3 f contributes only further 1 . 2 to the performance .", "This may be because information included in 3 f has already been captured by 2 f and f4 .", "Actually , the experiments show that the contribution of 3 f comes from where there is no explicit indicator information in around the NE and there is no reference to other NEs in the macro context of the document .", "The NEs contributed by 3 f are always well known ones , e . g .", "Microsoft , IBM and Bach a composer , which are introduced in texts without much helpful context .", "This paper proposes a HMM in that a new generative model , based on the mutual information independence assumption 2 3 instead of the conditional probability independence assumption I 1 after Bayes' rule , is applied .", "Moreover , it shows that the HMM based chunk tagger can effectively apply and integrate four different kinds of sub features , ranging from internal word information to semantic information to NE gazetteers to macro context of the document , to capture internal and external evidences for NER problem .", "It also shows that our NER system can reach quot ; near human performance quot ; .", "To our knowledge , our NER system outperforms any published machine learning system and any published rule based system .", "While the experimental results have been impressive , there is still much that can be done potentially to improve the performance .", "In the near feature , we would like to incorporate the following into our system"], "summary_lines": ["Named Entity Recognition Using An HMM-Based Chunk Tagger\n", "This paper proposes a Hidden Markov Model (HMM) and an HMM-based chunk tagger, from which a named entity (NE) recognition (NER) system is built to recognize and classify names, times and numerical quantities.\n", "Through the HMM, our system is able to apply and integrate four types of internal and external evidences: 1) simple deterministic internal feature of the words, such as capitalization and digitalization; 2) internal semantic feature of important triggers; 3) internal gazetteer feature; 4) external macro context feature.\n", "In this way, the NER problem can be resolved effectively.\n", "Evaluation of our system on MUC-6 and MUC-7 English NE tasks achieves F-measures of 96.6% and 94.1% respectively.\n", "It shows that the performance is significantly better than reported by any other machine-learning system.\n", "Moreover, the performance is even consistently better than those based on handcrafted rules.\n", "Our named entity recognition system recognizes various types of MUC-style named entities such as organization, location, person, date, time, money and percentage.\n"]}
{"article_lines": ["Learning Subjective Nouns Using Extraction Pattern Bootstrapping", "We explore the idea of creating a subjectivity classifier that uses lists of subjective nouns learned by bootstrapping algorithms .", "The goal of our research is to develop a system that can distinguish subjective sentences from objective sentences .", "First , we use two bootstrapping algorithms that exploit extraction patterns to learn sets of subjective nouns .", "Then we train a Naive Bayes classifier using the subjective nouns , discourse features , and subjectivity clues identified in prior research .", "The bootstrapping algorithms learned over 1000 subjective nouns , and the subjectivity classifier performed well , achieving 77 recall with 81 precision .", "Many natural language processing applications could benefit from being able to distinguish between factual and subjective information .", "Subjective remarks come in a variety of forms , including opinions , rants , allegations , accusations , suspicions , and speculation .", "Ideally , information extraction systems should be able to distinguish between factual information which should be extracted and non factual information which should be discarded or labeled as uncertain .", "Question answering systems should distinguish between factual and speculative answers .", "Multi perspective question answering aims to present multiple answers to the user based upon speculation or opinions derived from different sources .", "Multidocument summarization systems need to summarize different opinions and perspectives .", "Spam filtering systems must recognize rants and emotional tirades , among other things .", "In general , nearly any system that seeks to identify information could benefit from being able to separate factual and subjective information .", "Subjective language has been previously studied in fields such as linguistics , literary theory , psychology , and content analysis .", "Some manually developed knowledge resources exist , but there is no comprehensive dictionary of subjective language .", "Meta Bootstrapping Riloff and Jones , 1999 and Basilisk Thelen and Riloff , 2002 are bootstrapping algorithms that use automatically generated extraction patterns to identify words belonging to a semantic category .", "We hypothesized that extraction patterns could also identify subjective words .", "For example , the pattern expressed direct object often extracts subjective nouns , such as concern , hope , and support .", "Furthermore , these bootstrapping algorithms require only a handful of seed words and unannotated texts for training ; no annotated data is needed at all .", "In this paper , we use the Meta Bootstrapping and Basilisk algorithms to learn lists of subjective nouns from a large collection of unannotated texts .", "Then we train a subjectivity classifier on a small set of annotated data , using the subjective nouns as features along with some other previously identified subjectivity features .", "Our experimental results show that the subjectivity classifier performs well 77 recall with 81 precision and that the learned nouns improve upon previous state of the art subjectivity results Wiebe et al . , 1999 .", "In 2002 , an annotation scheme was developed for a U . S . government sponsored project with a team of 10 researchers the annotation instructions and project reports are available on the Web at http www . cs . pitt . edu wiebe pubs ardasummer02 .", "The scheme was inspired by work in linguistics and literary theory on subjectivity , which focuses on how opinions , emotions , etc . are expressed linguistically in context Banfield , 1982 .", "The scheme is more detailed and comprehensive than previous ones .", "We mention only those aspects of the annotation scheme relevant to this paper .", "The goal of the annotation scheme is to identify and characterize expressions of private states in a sentence .", "Private state is a general covering term for opinions , evaluations , emotions , and speculations Quirk et al . , 1985 .", "For example , in sentence 1 the writer is expressing a negative evaluation .", "Sentence 2 reflects the private state of Western countries .", "Mugabe s use of overwhelmingly also reflects a private state , his positive reaction to and characterization of his victory .", "Our data consists of English language versions of foreign news documents from FBIS , the U . S . Foreign Broadcast Information Service .", "The data is from a variety of publications and countries .", "The annotated corpus used to train and test our subjectivity classifiers the experiment corpus consists of 109 documents with a total of 2197 sentences .", "We used a separate , annotated tuning corpus of 33 documents with a total of 698 sentences to establish some experimental parameters . '", "Each document was annotated by one or both of two annotators , A and T . To allow us to measure interannotator agreement , the annotators independently annotated the same 12 documents with a total of 178 sentences .", "We began with a strict measure of agreement at the sentence level by first considering whether the annotator marked any private state expression , of any strength , anywhere in the sentence .", "If so , the sentence should be subjective .", "Otherwise , it is objective .", "Table 1 shows the contingency table .", "The percentage agreement is 88 , and the rc value is 0 . 71 .", "One would expect that there are clear cases of objective sentences , clear cases of subjective sentences , and borderline sentences in between .", "The agreement study supports this .", "In terms of our annotations , we define a sentence as borderline if it has at least one privatestate expression identified by at least one annotator , and all strength ratings of private state expressions are low .", "Table 2 shows the agreement results when such borderline sentences are removed 19 sentences , or 11 of the agreement test corpus .", "The percentage agreement increases to 94 and the rc value increases to 0 . 87 .", "As expected , the majority of disagreement cases involve low strength subjectivity .", "The annotators consistently agree about which are the clear cases of subjective sentences .", "This leads us to define the gold standard that we use in our experiments .", "A sentence is subjective if it contains at least one private state expression of medium or higher strength .", "The second class , which we call objective , consists of everything else .", "Thus , sentences with only mild traces of subjectivity are tossed into the objective category , making the system s goal to find the clearly subjective sentences .", "In the last few years , two bootstrapping algorithms have been developed to create semantic dictionaries by exploiting extraction patterns Meta Bootstrapping Riloff and Jones , 1999 and Basilisk Thelen and Riloff , 2002 .", "Extraction patterns were originally developed for information extraction tasks Cardie , 1997 .", "They represent lexico syntactic expressions that typically rely on shallow parsing and syntactic role assignment .", "For example , the pattern subject was hired would apply to sentences that contain the verb hired in the passive voice .", "The subject would be extracted as the hiree .", "Meta Bootstrapping and Basilisk were designed to learn words that belong to a semantic category e . g . , Tagger A Subj Obj truck is a VEHICLE and seashore is a LOCATION .", "Both algorithms begin with unannotated texts and seed words that represent a semantic category .", "A bootstrapping process looks for words that appear in the same extraction patterns as the seeds and hypothesizes that those words belong to the same semantic class .", "The principle behind this approach is that words of the same semantic class appear in similar pattern contexts .", "For example , the phrases lived in and traveled to will co occur with many noun phrases that represent LOCATIONS .", "In our research , we want to automatically identify words that are subjective .", "Subjective terms have many different semantic meanings , but we believe that the same contextual principle applies to subjectivity .", "In this section , we briefly overview these bootstrapping algorithms and explain how we used them to generate lists of subjective nouns .", "The Meta Bootstrapping MetaBoot process Riloff and Jones , 1999 begins with a small set of seed words that represent a targeted semantic category e . g . , 10 words that represent LOCATIONS and an unannotated corpus .", "First , MetaBoot automatically creates a set of extraction patterns for the corpus by applying and instantiating syntactic templates .", "This process literally produces thousands of extraction patterns that , collectively , will extract every noun phrase in the corpus .", "Next , MetaBoot computes a score for each pattern based upon the number of seed words among its extractions .", "The best pattern is saved and all of its extracted noun phrases are automatically labeled as the targeted semantic category . 2 MetaBoot then re scores the extraction patterns , using the original seed words as well as the newly labeled words , and the process repeats .", "This procedure is called mutual bootstrapping .", "A second level of bootstrapping the meta bootstrapping part makes the algorithm more robust .", "When the mutual bootstrapping process is finished , all nouns that were put into the semantic dictionary are reevaluated .", "Each noun is assigned a score based on how many different patterns extracted it .", "Only the five best nouns are allowed to remain in the dictionary .", "The other entries are discarded , and the mutual bootstrapping process starts over again using the revised semantic dictionary .", "Basilisk Thelen and Riloff , 2002 is a more recent bootstrapping algorithm that also utilizes extraction patterns to create a semantic dictionary .", "Similarly , Basilisk begins with an unannotated text corpus and a small set of seed words for a semantic category .", "The bootstrapping process involves three steps .", "1 Basilisk automatically generates a set of extraction patterns for the corpus and scores each pattern based upon the number of seed words among its extractions .", "This step is identical to the first step of Meta Bootstrapping .", "Basilisk then puts the best patterns into a Pattern Pool .", "2 All nouns3 extracted by a pattern in the Pattern Pool are put into a Candidate Word Pool .", "Basilisk scores each noun based upon the set of patterns that extracted it and their collective association with the seed words .", "3 The top 10 nouns are labeled as the targeted semantic class and are added to the dictionary .", "The bootstrapping process then repeats , using the original seeds and the newly labeled words .", "The main difference between Basilisk and MetaBootstrapping is that Basilisk scores each noun based on collective information gathered from all patterns that extracted it .", "In contrast , Meta Bootstrapping identifies a single best pattern and assumes that everything it extracted belongs to the same semantic class .", "The second level of bootstrapping smoothes over some of the problems caused by this assumption .", "In comparative experiments Thelen and Riloff , 2002 , Basilisk outperformed Meta Bootstrapping .", "But since our goal of learning subjective nouns is different from the original intent of the algorithms , we tried them both .", "We also suspected they might learn different words , in which case using both algorithms could be worthwhile .", "The Meta Bootstrapping and Basilisk algorithms need seed words and an unannotated text corpus as input .", "Since we did not need annotated texts , we created a much larger training corpus , the bootstrapping corpus , by gathering 950 new texts from the FBIS source mentioned in Section 2 . 2 .", "To find candidate seed words , we automatically identified 850 nouns that were positively correlated with subjective sentences in another data set .", "However , it is crucial that the seed words occur frequently in our FBIS texts or the bootstrapping process will not get off the ground .", "So we searched for each of the 850 nouns in the bootstrapping corpus , sorted them by frequency , and manually selected 20 high frequency words that we judged to be strongly subjective .", "Table 3 shows the 20 seed words used for both Meta Bootstrapping and Basilisk .", "We ran each bootstrapping algorithm for 400 iterations , generating 5 words per iteration .", "Basilisk generated 2000 nouns and Meta Bootstrapping generated 1996 nouns . 4 Table 4 shows some examples of extraction patterns that were discovered to be associated with subjective nouns .", "Meta Bootstrapping and Basilisk are semi automatic lexicon generation tools because , although the bootstrapping process is 100 automatic , the resulting lexicons need to be reviewed by a human . 5 So we manually reviewed the 3996 words proposed by the algorithms .", "This process is very fast ; it takes only a few seconds to classify each word .", "The entire review process took approximately 3 4 hours .", "One author did this labeling ; this person did not look at or run tests on the experiment corpus .", "We classified the words as StrongSubjective , WeakSubjective , or Objective .", "Objective terms are not subjective at all e . g . , chair or city .", "StrongSubjective terms have strong , unambiguously subjective connotations , such as bully or barbarian .", "WeakSubjective was used for three situations 1 words that have weak subjective connotations , such as aberration which implies something out of the ordinary but does not evoke a strong sense of judgement , 2 words that have multiple senses or uses , where one is subjective but the other is not .", "For example , the word plague can refer to a disease objective or an onslaught of something negative subjective , 3 words that are objective by themselves but appear in idiomatic expressions that are subjective .", "For example , the word eyebrows was labeled WeakSubjective because the expression raised eyebrows probably occurs more often in our corpus than literal references to eyebrows .", "Table 5 shows examples of learned words that were classified as StrongSubjective or WeakSubjective .", "Once the words had been manually classified , we could go back and measure the effectiveness of the algorithms .", "The graph in Figure 1 tracks their accuracy as the bootstrapping progressed .", "The X axis shows the number of words generated so far .", "The Y axis shows the percentage of those words that were manually classified as subjective .", "As is typical of bootstrapping algorithms , accuracy was high during the initial iterations but tapered off as the bootstrapping continued .", "After 20 words , both algorithms were 95 accurate .", "After 100 words Basilisk was 75 accurate and MetaBoot was 81 accurate .", "After 1000 words , accuracy dropped to about 28 for MetaBoot , but Basilisk was still performing reasonably well at 53 .", "Although 53 accuracy is not high for a fully automatic process , Basilisk depends on a human to review the words so 53 accuracy means that the human is accepting every other word , on average .", "Thus , the reviewer s time was still being spent productively even after 1000 words had been hypothesized .", "Table 6 shows the size of the final lexicons created by the bootstrapping algorithms .", "The first two columns show the number of subjective terms learned by Basilisk and Meta Bootstrapping .", "Basilisk was more prolific , generating 825 subjective terms compared to 522 for MetaBootstrapping .", "The third column shows the intersection between their word lists .", "There was substantial overlap , but both algorithms produced many words that the other did not .", "The last column shows the results of merging their lists .", "In total , the bootstrapping algorithms produced 1052 subjective nouns .", "To evaluate the subjective nouns , we trained a Naive Bayes classifier using the nouns as features .", "We also incorporated previously established subjectivity clues , and added some new discourse features .", "In this section , we describe all the feature sets and present performance results for subjectivity classifiers trained on different combinations of these features .", "The threshold values and feature representations used in this section are the ones that produced the best results on our separate tuning corpus .", "We defined four features to represent the sets of subjective nouns produced by the bootstrapping algorithms .", "BA Strong the set of StrongSubjective nouns generated by Basilisk BA Weak the set of WeakSubjective nouns generated by Basilisk MB Strong the set of StrongSubjective nouns generated by Meta Bootstrapping MB Weak the set of WeakSubjective nouns generated by Meta Bootstrapping For each set , we created a three valued feature based on the presence of 0 , 1 , or 2 words from that set .", "We used the nouns as feature sets , rather than define a separate feature for each word , so the classifier could generalize over the set to minimize sparse data problems .", "We will refer to these as the SubjNoun features .", "Wiebe , Bruce , O Hara 1999 developed a machine learning system to classify subjective sentences .", "We experimented with the features that they used , both to compare their results to ours and to see if we could benefit from their features .", "We will refer to these as the WBO features .", "WBO includes a set of stems positively correlated with the subjective training examples subjStems and a set of stems positively correlated with the objective training examples objStems .", "We defined a three valued feature for the presence of 0 , 1 , or 2 members of subjStems in a sentence , and likewise for objStems .", "For our experiments , subjStems includes stems that appear 7 times in the training set , and for which the precision is 1 . 25 times the baseline word precision for that training set . objStems contains the stems that appear 7 times and for which at least 50 of their occurrences in the training set are in objective sentences .", "WBO also includes a binary feature for each of the following the presence in the sentence of a pronoun , an adjective , a cardinal number , a modal other than will , and an adverb other than not .", "We also added manually developed features found by other researchers .", "We created 14 feature sets representing some classes from Levin , 1993 ; Ballmer and Brennenstuhl , 1981 , some Framenet lemmas with frame element experiencer Baker et al . , 1998 , adjectives manually annotated for polarity Hatzivassiloglou and McKeown , 1997 , and some subjectivity clues listed in Wiebe , 1990 .", "We represented each set as a three valued feature based on the presence of 0 , 1 , or 2 members of the set .", "We will refer to these as the manual features .", "We created discourse features to capture the density of clues in the text surrounding a sentence .", "First , we computed the average number of subjective clues and objective clues per sentence , normalized by sentence length .", "The subjective clues , subjClues , are all sets for which 3 valued features were defined above except objStems .", "The objective clues consist only of objStems .", "For senAvgClueRatesubj to be the average of ClueRate S over all sentences S and similarly for AvgClueRateobj .", "Next , we characterize the number of subjective and objective clues in the previous and next sentences as higher than expected high , lower than expected low , or expected medium .", "The value for ClueRatesubj S is high if ClueRatesubj S AvgClueRatesubj 1 . 3 ; low if ClueRatesubj S _ AvgClueRatesubj 1 . 3 ; otherwise it is medium .", "The values for ClueRateobj S are defined similarly .", "Using these definitions we created four features ClueRatesubj for the previous and following sentences , and ClueRateobj for the previous and following sentences .", "We also defined a feature for sentence length .", "Let AvgSentLen be the average sentence length .", "SentLen S is high if length S AvgSentLen 1 . 3 ; low if length S AvgSentLen 1 . 3 ; and medium otherwise .", "We conducted experiments to evaluate the performance of the feature sets , both individually and in various combinations .", "Unless otherwise noted , all experiments involved training a Naive Bayes classifier using a particular set of features .", "We evaluated each classifier using 25fold cross validation on the experiment corpus and used paired t tests to measure significance at the 95 confidence level .", "As our evaluation metrics , we computed accuracy Acc as the percentage of the system s classifications that match the gold standard , and precision Prec and recall Rec with respect to subjective sentences . represents the common baseline of assigning every sentence to the most frequent class .", "The Most Frequent baseline achieves 59 accuracy because 59 of the sentences in the gold standard are subjective .", "Row 2 is a Naive Bayes classifier that uses the WBO features , which performed well in prior research on sentence level subjectivity classification Wiebe et al . , 1999 .", "Row 1 shows a Naive Bayes classifier that uses unigram bag ofwords features , with one binary feature for the absence or presence in the sentence of each word that appeared during training .", "Pang et al . 2002 reported that a similar experiment produced their best results on a related classification task .", "The difference in accuracy between Rows 1 and 2 is not statistically significant Bag of Word s higher precision is balanced by WBO s higher recall .", "Next , we trained a Naive Bayes classifier using only the SubjNoun features .", "This classifier achieved good precision 77 but only moderate recall 64 .", "Upon further inspection , we discovered that the subjective nouns are good subjectivity indicators when they appear , but not every subjective sentence contains one of them .", "And , relatively few sentences contain more than one , making it difficult to recognize contextual effects i . e . , multiple clues in a region .", "We concluded that the appropriate way to benefit from the subjective nouns is to use them in tandem with other subjectivity clues .", "Table 8 shows the results of Naive Bayes classifiers trained with different combinations of features .", "The accuracy differences between all pairs of experiments in Table 8 are statistically significant .", "Row 3 uses only the WBO features also shown in Table 7 as a baseline .", "Row 2 uses the WBO features as well as the SubjNoun features .", "There is a synergy between these feature sets using both types of features achieves better performance than either one alone .", "The difference is mainly precision , presumably because the classifier found more and better combinations of features .", "In Row 1 , we also added the manual and discourse features .", "The discourse features explicitly identify contexts in which multiple clues are found .", "This classifier produced even better performance , achieving 81 . 3 precision with 77 . 4 recall .", "The 76 . 1 accuracy result is significantly higher than the accuracy results for all of the other classifiers in both Table 8 and Table 7 .", "Finally , higher precision classification can be obtained by simply classifying a sentence as subjective if it contains any of the StrongSubjective nouns .", "On our data , this method produces 87 precision with 26 recall .", "This approach could support applications for which precision is paramount .", "Several types of research have involved document level subjectivity classification .", "Some work identifies inflammatory texts e . g . , Spertus , 1997 or classifies reviews as positive or negative Turney , 2002 ; Pang et al . , 2002 .", "Tong s system Tong , 2001 generates sentiment timelines , tracking online discussions and creating graphs of positive and negative opinion messages over time .", "Research in genre classification may include recognition of subjective genres such as editorials e . g . , Karlgren and Cutting , 1994 ; Kessler et al . , 1997 ; Wiebe et al . , 2001 .", "In contrast , our work classifies individual sentences , as does the research in Wiebe et al . , 1999 .", "Sentence level subjectivity classification is useful because most documents contain a mix of subjective and objective sentences .", "For example , newspaper articles are typically thought to be relatively objective , but Wiebe et al . , 2001 reported that , in their corpus , 44 of sentences in articles that are not editorials or reviews were subjective .", "Some previous work has focused explicitly on learning subjective words and phrases .", "Hatzivassiloglou and McKeown , 1997 describes a method for identifying the semantic orientation of words , for example that beautiful expresses positive sentiments .", "Researchers have focused on learning adjectives or adjectival phrases Turney , 2002 ; Hatzivassiloglou and McKeown , 1997 ; Wiebe , 2000 and verbs Wiebe et al . , 2001 , but no previous work has focused on learning nouns .", "A unique aspect of our work is the use of bootstrapping methods that exploit extraction patterns .", "Turney , 2002 used patterns representing part of speech sequences , Hatzivassiloglou and McKeown , 1997 recognized adjectival phrases , and Wiebe et al . , 2001 learned N grams .", "The extraction patterns used in our research are linguistically richer patterns , requiring shallow parsing and syntactic role assignment .", "In recent years several techniques have been developed for semantic lexicon creation e . g . , Hearst , 1992 ; Riloff and Shepherd , 1997 ; Roark and Charniak , 1998 ; Caraballo , 1999 .", "Semantic word learning is different from subjective word learning , but we have shown that MetaBootstrapping and Basilisk could be successfully applied to subjectivity learning .", "Perhaps some of these other methods could also be used to learn subjective words .", "This research produced interesting insights as well as performance results .", "First , we demonstrated that weakly supervised bootstrapping techniques can learn subjective terms from unannotated texts .", "Subjective features learned from unannotated documents can augment or enhance features learned from annotated training data using more traditional supervised learning techniques .", "Second , Basilisk and Meta Bootstrapping proved to be useful for a different task than they were originally intended .", "By seeding the algorithms with subjective words , the extraction patterns identified expressions that are associated with subjective nouns .", "This suggests that the bootstrapping algorithms should be able to learn not only general semantic categories , but any category for which words appear in similar linguistic phrases .", "Third , our best subjectivity classifier used a wide variety of features .", "Subjectivity is a complex linguistic phenomenon and our evidence suggests that reliable subjectivity classification requires a broad array of features ."], "summary_lines": ["Learning Subjective Nouns Using Extraction Pattern Bootstrapping\n", "We explore the idea of creating a subjectivity classifier that uses lists of subjective nouns learned by bootstrapping algorithms.\n", "The goal of our research is to develop a system that can distinguish subjective sentences from objective sentences.\n", "First, we use two bootstrapping algorithms that exploit extraction patterns to learn sets of subjective nouns.\n", "Then we train a Naive Bayes classifier using the subjective nouns, discourse features, and subjectivity clues identified in prior research.\n", "The bootstrapping algorithms learned over 1000 subjective nouns, and the subjectivity classifier performed well, achieving 77% recall with 81% precision.\n", "We use manually derived pattern templates to extract subjective nouns by bootstrapping.\n", "We mine subjective nouns from unannotated texts with two bootstrapping algorithms that exploit lexico-syntactic extraction patterns and manually-selected subjective seeds.\n"]}
{"article_lines": ["Unsupervised Coreference Resolution in a Nonparametric Bayesian Model", "We present an unsupervised , nonparametric Bayesian approach to coreference resolution which models both global entity identity across a corpus as well as the sequential anaphoric structure within each document .", "While most existing coreference work is driven by pairwise decisions , our model is fully generative , producing each mention from a combination of global entity properties and local attentional state .", "Despite being unsupervised , our system achieves a 70 . 3 on the MUC 6 test set , broadly in the range of some recent supervised results .", "Referring to an entity in natural language can broadly be decomposed into two processes .", "First , speakers directly introduce new entities into discourse , entities which may be shared across discourses .", "This initial reference is typically accomplished with proper or nominal expressions .", "Second , speakers refer back to entities already introduced .", "This anaphoric reference is canonically , though of course not always , accomplished with pronouns , and is governed by linguistic and cognitive constraints .", "In this paper , we present a nonparametric generative model of a document corpus which naturally connects these two processes .", "Most recent coreference resolution work has focused on the task of deciding which mentions noun phrases in a document are coreferent .", "The dominant approach is to decompose the task into a collection of pairwise coreference decisions .", "One then applies discriminative learning methods to pairs of mentions , using features which encode properties such as distance , syntactic environment , and so on Soon et al . , 2001 ; Ng and Cardie , 2002 .", "Although such approaches have been successful , they have several liabilities .", "First , rich features require plentiful labeled data , which we do not have for coreference tasks in most domains and languages .", "Second , coreference is inherently a clustering or partitioning task .", "Naive pairwise methods can and do fail to produce coherent partitions .", "One classic solution is to make greedy left to right linkage decisions .", "Recent work has addressed this issue in more global ways .", "McCallum and Wellner 2004 use graph partioning in order to reconcile pairwise scores into a final coherent clustering .", "Nonetheless , all these systems crucially rely on pairwise models because clusterlevel models are much harder to work with , combinatorially , in discriminative approaches .", "Another thread of coreference work has focused on the problem of identifying matches between documents Milch et al . , 2005 ; Bhattacharya and Getoor , 2006 ; Daume and Marcu , 2005 .", "These methods ignore the sequential anaphoric structure inside documents , but construct models of how and when entities are shared between them . 1 These models , as ours , are generative ones , since the focus is on cluster discovery and the data is generally unlabeled .", "In this paper , we present a novel , fully generative , nonparametric Bayesian model of mentions in a document corpus .", "Our model captures both withinand cross document coreference .", "At the top , a hierarchical Dirichlet process Teh et al . , 2006 captures cross document entity and parameter sharing , while , at the bottom , a sequential model of salience captures within document sequential structure .", "As a joint model of several kinds of discourse variables , it can be used to make predictions about either kind of coreference , though we focus experimentally on within document measures .", "To the best of our ability to compare , our model achieves the best unsupervised coreference performance .", "We adopt the terminology of the Automatic Context Extraction ACE task NIST , 2004 .", "For this paper , we assume that each document in a corpus consists of a set of mentions , typically noun phrases .", "Each mention is a reference to some entity in the domain of discourse .", "The coreference resolution task is to partition the mentions according to referent .", "Mentions can be divided into three categories , proper mentions names , nominal mentions descriptions , and pronominal mentions pronouns .", "In section 3 , we present a sequence of increasingly enriched models , motivating each from shortcomings of the previous .", "As we go , we will indicate the performance of each model on data from ACE 2004 NIST , 2004 .", "In particular , we used as our development corpus the English translations of the Arabic and Chinese treebanks , comprising 95 documents and about 3 , 905 mentions .", "This data was used heavily for model design and hyperparameter selection .", "In section 5 , we present final results for new test data from MUC 6 on which no tuning or development was performed .", "This test data will form our basis for comparison to previous work .", "In all experiments , as is common , we will assume that we have been given as part of our input the true mention boundaries , the head word of each mention and the mention type proper , nominal , or pronominal .", "For the ACE data sets , the head and mention type are given as part of the mention annotation .", "For the MUC data , the head was crudely chosen to be the rightmost mention token , and the mention type was automatically detected .", "We will not assume any other information to be present in the data beyond the text itself .", "In particular , unlike much related work , we do not assume gold named entity recognition NER labels ; indeed we do not assume observed NER labels or POS tags at all .", "Our primary performance metric will be the MUC F1 measure Vilain et al . , 1995 , commonly used to evaluate coreference systems on a within document basis .", "Since our system relies on sampling , all results are averaged over five random runs .", "In this section , we present a sequence of generative coreference resolution models for document corpora .", "All are essentially mixture models , where the mixture components correspond to entities .", "As far as notation , we assume a collection of I documents , each with Ji mentions .", "We use random variables Z to refer to indices of entities .", "We will use 0z to denote the parameters for an entity z , and 0 to refer to the concatenation of all such 0z .", "X will refer somewhat loosely to the collection of variables associated with a mention in our model such as the head or gender .", "We will be explicit about X and 0z shortly .", "Our goal will be to find the setting of the entity indices which maximize the posterior probability where Z , X , and 0 denote all the entity indices , observed values , and parameters of the model .", "Note that we take a Bayesian approach in which all parameters are integrated out or sampled .", "The inference task is thus primarily a search problem over the index labels Z .", "The Weir Group1 , whose2 headquarters3 is in the US4 , is a large , specialized corporation5 investing in the area of electricity generation .", "This power plant6 , which7 will be situated in Rudong8 , Jiangsu9 , has an annual generation capacity of 2 . 4 million kilowatts .", "The Weir Group1 , whose1 headquarters2 is in the US3 , is a large , specialized corporation4 investing in the area of electricity generation .", "This power plant5 , which1 will be situated in Rudong6 , Jiangsu7 , has an annual generation capacity of 2 . 4 million kilowatts .", "The Weir Group1 , whose1 headquarters2 is in the US3 , is a large , specialized corporation4 investing in the area of electricity generation .", "This power plant5 , which5 will be situated in Rudong6 , Jiangsu7 , has an annual generation capacity of 2 . 4 million kilowatts .", "Our first , overly simplistic , corpus model is the standard finite mixture of multinomials shown in figure 1 a .", "In this model , each document is independent save for some global hyperparameters .", "Inside each document , there is a finite mixture model with a fixed number K of components .", "The distribution Q over components entities is a draw from a symmetric Dirichlet distribution with concentration \u03b1 .", "For each mention in the document , we choose a component an entity index z from Q .", "Entity z is then associated with a multinomial emission distribution over head words with parameters Oh , which are drawn from a symmetric Dirichlet over possible mention heads with concentration AH . 2 Note that here the X for a mention consists only of the mention head H . As we enrich our models , we simultaneously develop an accompanying Gibbs sampling procedure to obtain samples from P Z X . 3 For now , all heads H are observed and all parameters Q and 0 can be integrated out analytically for details see Teh et al . 2006 .", "The only sampling is for the values of Zi , j , the entity index of mention j in document i .", "The relevant conditional distribution is 4 where Hi , j is the head of mention j in document i .", "Expanding each term , we have the contribution of the prior 2In general , we will use a subscripted A to indicate concentration for finite Dirichlet distributions .", "Unless otherwise specified , A concentration parameters will be set to a 4 and omitted from diagrams .", "3One could use the EM algorithm with this model , but EM will not extend effectively to the subsequent models .", "4Here , Z i , j denotes Z Zi , j where nz is the number of elements of Z i , j with entity index z .", "Similarly we have for the contribution of the emissions where nh , z is the number of times we have seen head h associated with entity index z in Z , H i , j .", "A clear drawback of the finite mixture model is the requirement that we specify a priori a number of entities K for a document .", "We would like our model to select K in an effective , principled way .", "A mechanism for doing so is to replace the finite Dirichlet prior on Q with the non parametric Dirichlet process DP prior Ferguson , 1973 . 5 Doing so gives the model in figure 1 b .", "Note that we now list an infinite number of mixture components in this model since there can be an unbounded number of entities .", "Rather than a finite Q with a symmetric Dirichlet distribution , in which draws tend to have balanced clusters , we now have an infinite Q .", "However , most draws will have weights which decay exponentially quickly in the prior though not necessarily in the posterior .", "Therefore , there is a natural penalty for each cluster which is actually used .", "With Z observed during sampling , we can integrate out Q and calculate P Zi , j Z i , j analytically , using the Chinese restaurant process representation where znew is a new entity index not used in Z i , j and nz is the number of mentions that have entity index z .", "Aside from this change , sampling is identical to the finite mixture case , though with the number of clusters actually occupied in each sample drifting upwards or downwards .", "This model yielded a 54 . 5 Fi on our development data . 6 This model is , however , hopelessly crude , capturing nothing of the structure of coreference .", "Its largest empirical problem is that , unsurprisingly , pronoun mentions such as he are given their own clusters , not labeled as coreferent with any non pronominal mention see figure 2 a .", "While an entity specific multinomial distribution over heads makes sense for proper , and some nominal , mention heads , it does not make sense to generate pronominal mentions this same way .", "I . e . , all entities can be referred to by generic pronouns , the choice of which depends on entity properties such as gender , not the specific entity .", "We therefore enrich an entity s parameters 0 to contain not only a distribution over lexical heads 0h , but also distributions 0t , 09 , 0n over properties , where 0t parametrizes a distribution over entity types PER , LOC , ORG , MISC , and 09 for gender MALE , FEMALE , NEUTER , and 0n for number SG , PL . 7 We assume each of these property distributions is drawn from a symmetric Dirichlet distribution with small concentration parameter in order to encourage a peaked posterior distribution .", "Previously , when an entity z generated a mention , it drew a head word from 0hz .", "It now undergoes a more complex and structured process .", "It first draws an entity type T , a gender G , a number N from the distributions 0t , 09 , and 0n , respectively .", "Once the properties are fetched , a mention type M is chosen proper , nominal , pronoun , according to a global multinomial again with a symmetric Dirichlet prior and parameter AM .", "This corresponds to the temporary assumption that the speaker makes a random i . i . d . choice for the type of each mention .", "Our head model will then generate a head , conditioning on the entity , its properties , and the mention type , as shown in figure 3 b .", "If M is not a pronoun , the head is drawn directly from the entity head multinomial with parameters 0hz .", "Otherwise , it is drawn based on a global pronoun head distribution , conditioning on the entity properties and parametrized by 9 .", "Formally , it is given by Although we can observe the number and gender draws for some mentions , like personal pronouns , there are some for which properties aren t observed e . g . , it .", "Because the entity property draws are not all observed , we must now sample the unobserved ones as well as the entity indices Z .", "For instance , we could sample Ti , j , the entity type of pronominal mention j in document i , using , P Ti , j Z , N , G , H , T i , j P Ti , j Z P Hi , j T , N , G , H , where the posterior distributions on the right hand side are straightforward because the parameter priors are all finite Dirichlet .", "Sampling G and N are identical .", "Of course we have prior knowledge about the relationship between entity type and pronoun head choice .", "For example , we expect that he is used for mentions with T PERSON .", "In general , we assume that for each pronominal head we have a list of compatible entity types , which we encode via the prior on 0 .", "We assume 0 is drawn from a Dirichlet distribution where each pronoun head is given a synthetic count of 1 AP for each t , g , n where t is compatible with the pronoun and given AP otherwise .", "So , while it will be possible in the posterior to use he to refer to a non person , it will be biased towards being used with persons .", "This model gives substantially improved predictions 64 . 1 Fi on our development data .", "As can be seen in figure 2 b , this model does correct the systematic problem of pronouns being considered their own entities .", "However , it still does not have a preference for associating pronominal references to entities which are in any way local .", "We would like our model to capture how mention types are generated for a given entity in a robust and somewhat language independent way .", "The choice of entities may reasonably be considered to be independent given the mixing weights Q , but how we realize an entity is strongly dependent on context Ge et al . , 1998 .", "In order to capture this in our model , we enrich it as shown in figure 4 .", "As we proceed through a document , generating entities and their mentions , we maintain a list of the active entities and their saliences , or activity scores .", "Every time an entity is mentioned , we increment its activity score by 1 , and every time we move to generate the next mention , all activity scores decay by a constant factor of 0 . 5 .", "This gives rise to an ordered list of entity activations , L , where the rank of an entity decays exponentially as new mentions are generated .", "We call this list a salience list .", "Given a salience list , L , each possible entity z has some rank on this list .", "We discretize these ranks into five buckets S TOP 1 , HIGH 23 , MID 4 6 , LOW 7 , and NONE .", "Given the entity choices Z , both the list L and buckets S are deterministic see figure 4 .", "We assume that the mention type M is conditioned on S as shown in figure 4 .", "We note that correctly sampling an entity now requires that we incorporate terms for how a change will affect all future salience values .", "This changes our sampling equation for existing entities where the product ranges over future mentions in the document and Si , j , is the value of future salience feature given the setting of all entities , including setting the current entity Zi , j to z .", "A similar equation holds for sampling a new entity .", "Note that , as discussed below , this full product can be truncated as an approximation .", "This model gives a 71 . 5 Fi on our development data .", "Table 1 shows the posterior distribution of the mention type given the salience feature .", "This model fixes many anaphora errors and in particular fixes the second anaphora error in figure 2 c .", "One advantage of a fully generative approach is that we can allow entities to be shared between documents in a principled way , giving us the capacity to do cross document coreference .", "Moreover , sharing across documents pools information about the properties of an entity across documents .", "We can easily link entities across a corpus by assuming that the pool of entities is global , with global mixing weights Qo drawn from a DP prior with concentration parameter 'y .", "Each document uses the same global entities , but each has a documentspecific distribution Qi drawn from a DP centered on Q0 with concentration parameter \u03b1 .", "Up to the point where entities are chosen , this formulation follows the basic hierarchical Dirichlet process prior of Teh et al . 2006 .", "Once the entities are chosen , our model for the realization of the mentions is as before .", "This model is depicted graphically in figure 5 .", "Although it is possible to integrate out Q0 as we did the individual Qi , we instead choose for efficiency and simplicity to sample the global mixture distribution Q0 from the posterior distribution P Q0 Z . 8 The mention generation terms in the model and sampler are unchanged .", "In the full hierarchical model , our equation 1 for sampling entities , ignoring the salience component of section 3 . 4 , becomes where Qz0 is the probability of the entity z under the sampled global entity distribution and Qu0 is the unknown component mass of this distribution .", "The HDP layer of sharing improves the model s predictions to 72 . 5 F1 on our development data .", "We should emphasize that our evaluation is of course per document and does not reflect cross document coreference decisions , only the gains through crossdocument sharing see section 6 . 2 .", "Up until now , we ve discussed Gibbs sampling , but we are not interested in sampling from the posterior P Z X , but in finding its mode .", "Instead of sampling directly from the posterior distribution , we instead sample entities proportionally to exponentiated entity posteriors .", "The exponent is given by exp ci k 1 , where i is the current round number starting at i 0 , c 1 . 5 and k 20 is the total number of sampling epochs .", "This slowly raises the posterior exponent from 1 . 0 to ec .", "In our experiments , we found this procedure to outperform simulated annealing .", "We also found sampling the T , G , and N variables to be particularly inefficient , so instead we maintain soft counts over each of these variables and use these in place of a hard sampling scheme .", "We also found that correctly accounting for the future impact of salience changes to be particularly inefficient .", "However , ignoring those terms entirely made negligible difference in final accuracy . 9", "We present our final experiments using the full model developed in section 3 .", "As in section 3 , we use true mention boundaries and evaluate using the MUC F1 measure Vilain et al . , 1995 .", "All hyperparameters were tuned on the development set only .", "The document concentration parameter \u03b1 was set by taking a constant proportion of the average number of mentions in a document across the corpus .", "This number was chosen to minimize the squared error between the number of proposed entities and true entities in a document .", "It was not tuned to maximize the F1 measure .", "A coefficient of 0 . 4 was chosen .", "The global concentration coefficient y was chosen to be a constant proportion of \u03b1M , where M is the number of documents in the corpus .", "We found 0 . 15 to be a good value using the same least square procedure .", "The values for these coefficients were not changed for the experiments on the test sets .", "Our main evaluation is on the standard MUC 6 formal test set . 10 The standard experimental setup for this data is a 30 30 document train test split .", "Training our system on all 60 documents of the training and test set as this is in an unsupervised system , the unlabeled test documents are present at training time , but evaluating only on the test documents , gave 63 . 9 F1 and is labeled MUC 6 in table 2 a .", "One advantage of an unsupervised approach is that we can easily utilize more data when learning a model .", "We demonstrate the effectiveness of this fact by evaluating on the MUC 6 test documents with increasing amounts of unannotated training data .", "We first added the 191 documents from the MUC 6 dryrun training set which were not part of the training data for official MUC 6 evaluation .", "This model gave 68 . 0 F1 and is labeled DRYRUN TRAIN in table 2 a .", "We then added the ACE ENGLISH NWIRE training data , which is from a different corpora than the MUC 6 test set and from a different time period .", "This model gave 70 . 3 F1 and is labeled ENGLISHNWIRE in table 2 a .", "Our results on this test set are surprisingly comparable to , though slightly lower than , some recent supervised systems .", "McCallum and Wellner 2004 report 73 . 4 F1 on the formal MUC 6 test set , which is reasonably close to our best MUC 6 number of 70 . 3 F1 .", "McCallum and Wellner 2004 also report a much lower 91 . 6 F1 on only proper nouns mentions .", "Our system achieves a 89 . 8 F1 when evaluation is restricted to only proper mentions . 11 The et al . 2004 .", "A mention is proper if it is annotated with NER information .", "It is a pronoun if the head is on the list of English pronouns .", "Otherwise , it is a nominal mention .", "Note we do not use the NER information for any purpose but determining whether the mention is proper .", "11The best results we know on the MUC 6 test set using the standard setting are due to Luo et al . 2004 who report a 81 . 3 Fl much higher than others .", "However , it is not clear this is a comparable number , due to the apparent use of gold NER features , which provide a strong clue to coreference .", "Regardless , it is unsurprising that their system , which has many rich features , would outperform ours . closest comparable unsupervised system is Cardie and Wagstaff 1999 who use pairwise NP distances to cluster document mentions .", "They report a 53 . 6 F1 on MUC6 when tuning distance metric weights to maximize F1 on the development set .", "We also performed experiments on ACE 2004 data .", "Due to licensing restrictions , we did not have access to the ACE 2004 formal development and test sets , and so the results presented are on the training sets .", "We report results on the newswire section NWIRE in table 2b and the broadcast news section BNEWS in table 2b .", "These datasets include the prenominal mention type , which is not present in the MUC6 data .", "We treated prenominals analogously to the treatment of proper and nominal mentions .", "We also tested our system on the Chinese newswire and broadcast news sections of the ACE 2004 training sets .", "Our relatively higher performance on Chinese compared to English is perhaps due to the lack of prenominal mentions in the Chinese data , as well as the presence of fewer pronouns compared to English .", "Our ACE results are difficult to compare exactly to previous work because we did not have access to the restricted formal test set .", "However , we can perform a rough comparison between our results on the training data without coreference annotation to supervised work which has used the same training data with coreference annotation and evaluated on the formal test set .", "Denis and Baldridge 2007 report 67 . 1 F1 and 69 . 2 F1 on the English NWIRE and BNEWS respectively using true mention boundaries .", "While our system underperforms the supervised systems , its accuracy is nonetheless promising .", "The largest source of error in our system is between coreferent proper and nominal mentions .", "The most common examples of this kind of error are appositive usages e . g .", "George W . Bush , president of the US , visited Idaho .", "Another error of this sort can be seen in figure 2 , where the corporation mention is not labeled coreferent with the The Weir Group mention .", "Examples such as these illustrate the regular at least in newswire phenomenon that nominal mentions are used with informative intent , even when the entity is salient and a pronoun could have been used unambiguously .", "This aspect of nominal mentions is entirely unmodeled in our system .", "Since we do not have labeled cross document coreference data , we cannot evaluate our system s crossdocument performance quantitatively .", "However , in addition to observing the within document gains from sharing shown in section 3 , we can manually inspect the most frequently occurring entities in our corpora .", "Table 3 shows some of the most frequently occurring entities across the English ACE NWIRE corpus .", "Note that Bush is the most frequent entity , though his and others nominal cluster president is mistakenly its own entity .", "Merging of proper and nominal clusters does occur as can be seen in table 3 .", "We can use our model to for unsupervised NER tagging for each proper mention , assign the mode of the generating entity s distribution over entity types .", "Note that in our model the only way an entity becomes associated with an entity type is by the pronouns used to refer to it . 12 If we evaluate our system as an unsupervised NER tagger for the proper mentions in the MUC 6 test set , it yields a 12Ge et al . 1998 exploit a similar idea to assign gender to proper mentions . per label accuracy of 61 . 2 on MUC labels .", "Although nowhere near the performance of state ofthe art systems , this result beats a simple baseline of always guessing PERSON the most common entity type , which yields 46 . 4 .", "This result is interesting given that the model was not developed for the purpose of inferring entity types whatsoever .", "We have presented a novel , unsupervised approach to coreference resolution global entities are shared across documents , the number of entities is determined by the model , and mentions are generated by a sequential salience model and a model of pronounentity association .", "Although our system does not perform quite as well as state of the art supervised systems , its performance is in the same general range , despite the system being unsupervised ."], "summary_lines": ["Unsupervised Coreference Resolution in a Nonparametric Bayesian Model\n", "We present an unsupervised, nonparametric Bayesian approach to coreference resolution which models both global entity identity across a corpus as well as the sequential anaphoric structure within each document.\n", "While most existing coreference work is driven by pairwise decisions, our model is fully generative, producing each mention from a combination of global entity properties and local attentional state.\n", "Despite being unsupervised, our system achieves a 70.3 MUC F1 measure on the MUC-6 test set, broadly in the range of some recent supervised results.\n", "In our model, we use the distinction bewtween pronouns, nominals, and proper nouns.\n", "We evaluate the clustering properties of DPMMs by performing anaphora resolution with good results.\n"]}
{"article_lines": ["A Trainable Rule Based Algorithm For Word Segmentation", "This paper presents a trainable rule based algorithm for performing word segmentation .", "The algorithm provides a simple , language independent alternative to large scale lexical based segmenters requiring large amounts of knowledge engineering .", "As a stand alone segmenter , we show our algorithm to produce high performance Chinese segmentation .", "In addition , we show the transformation based algorithm to be effective in improving the output of several existing word segmentation algorithms in three different languages .", "This paper presents a trainable rule based algorithm for performing word segmentation .", "Our algorithm is effective both as a high accuracy stand alone segmenter and as a postprocessor that improves the output of existing word segmentation algorithms .", "In the writing systems of many languages , including Chinese , Japanese , and Thai , words are not delimited by spaces .", "Determining the word boundaries , thus tokenizing the text , is usually one of the first necessary processing steps , making tasks such as part of speech tagging and parsing possible .", "A variety of methods have recently been developed to perform word segmentation and the results have been published widely . '", "A major difficulty in evaluating segmentation algorithms is that there are no widely accepted guidelines as to what constitutes a word , and there is therefore no agreement on how to quot ; correctly quot ; segment a text in an unsegmented language .", "It is 1Most published segmentation work has been done for Chinese .", "For a discussion of recent Chinese segmentation work , see Sproat et al . 1996 . frequently mentioned in segmentation papers that native speakers of a language do not always agree about the quot ; correct quot ; segmentation and that the same text could be segmented into several very different and equally correct sets of words by different native speakers .", "Sproat et al . 1996 and Wu and Fung 1994 give empirical results showing that an agreement rate between native speakers as low as 75 is common .", "Consequently , an algorithm which scores extremely well compared to one native segmentation may score dismally compared to other , equally quot ; correct quot ; segmentations .", "We will discuss some other issues in evaluating word segmentation in Section 3 . 1 .", "One solution to the problem of multiple correct segmentations might be to establish specific guidelines for what is and is not a word in unsegmented languages .", "Given these guidelines , all corpora could theoretically be uniformly segmented according to the same conventions , and we could directly compare existing methods on the same corpora .", "While this approach has been successful in driving progress in NLP tasks such as part of speech tagging and parsing , there are valid arguments against adopting it for word segmentation .", "For example , since word segmentation is merely a preprocessing task for a wide variety of further tasks such as parsing , information extraction , and information retrieval , different segmentations can be useful or even essential for the different tasks .", "In this sense , word segmentation is similar to speech recognition , in which a system must be robust enough to adapt to and recognize the multiple speaker dependent quot ; correct quot ; pronunciations of words .", "In some cases , it may also be necessary to allow multiple quot ; correct quot ; segmentations of the same text , depending on the requirements of further processing steps .", "However , many algorithms use extensive domain specific word lists and intricate name recognition routines as well as hard coded morphological analysis modules to produce a predetermined segmentation output .", "Modifying or retargeting an existing segmentation algorithm to produce a different segmentation can be difficult , especially if it is not clear what and where the systematic differences in segmentation are .", "It is widely reported in word segmentation papers , 2 that the greatest barrier to accurate word segmentation is in recognizing words that are not in the lexicon of the segmenter .", "Such a problem is dependent both on the source of the lexicon as well as the correspondence in vocabulary between the text in question and the lexicon .", "Wu and Fung 1994 demonstrate that segmentation accuracy is significantly higher when the lexicon is constructed using the same type of corpus as the corpus on which it is tested .", "We argue that rather than attempting to construct a single exhaustive lexicon or even a series of domain specific lexica , it is more practical to develop a robust trainable means of compensating for lexicon inadequacies .", "Furthermore , developing such an algorithm will allow us to perform segmentation in many different languages without requiring extensive morphological resources and domain specific lexica in any single language .", "For these reasons , we address the problem of word segmentation from a different direction .", "We introduce a rule based algorithm which can produce an accurate segmentation of a text , given a rudimentary initial approximation to the segmentation .", "Recognizing the utility of multiple correct segmentations of the same text , our algorithm also allows the output of a wide variety of existing segmentation algorithms to be adapted to different segmentation schemes .", "In addition , our rule based algorithm can also be used to supplement the segmentation of an existing algorithm in order to compensate for an incomplete lexicon .", "Our algorithm is trainable and language independent , so it can be used with any unsegmented h . nguage .", "The key component of our trainable segmentation algorithm is Transformation based Error driven Learning , the corpus based language processing method introduced by Brill 1993a .", "This technique provides a simple algorithm for learning a sequence of rules that can be applied to various NLP tasks .", "It differs from other common corpus based methods in several ways .", "For one , it is weakly statistical , but not probabilistic ; transformation based approaches conseo , iitly require far less training data than most . . stical approaches .", "It is rule based , but relies on 'See , for example , Sproat et al . 1996 . machine learning to acquire the rules , rather than expensive manual knowledge engineering .", "The rules produced can be inspected , which is useful for gaining insight into the nature of the rule sequence and for manual improvement and debugging of the sequence .", "The learning algorithm also considers the entire training set at all learning steps , rather than decreasing the size of the training data as learning progresses , such as is the case in decision tree induction Quinlan , 1986 .", "For a thorough discussion of transformation based learning , see Ramshaw and Marcus 1996 .", "Brill's work provides a proof of viability of transformation based techniques in the form of a number of processors , including a widelydistributed part of speech tagger Brill , 1994 , a procedure for prepositional phrase attachment Brill and Resnik , 1994 , and a bracketing parser Brill , 1993b .", "All of these provided performance comparable to or better than previous attempts .", "Transformation based learning has also been successfully applied to text chunking Ramshaw and Marcus , 1995 , morphological disambiguation Oflazer and Tur , 1996 , and phrase parsing Vilain and Day , 1996 .", "Word segmentation can easily be cast as a transformation based problem , which requires an initial model , a goal state into which we wish to transform the initial model the quot ; gold standard quot ; , and a series of transformations to effect this improvement .", "The transformation based algorithm involves applying and scoring all the possible rules to training data and determining which rule improves the model the most .", "This rule is then applied to all applicable sentences , and the process is repeated until no rule improves the score of the training data .", "In this manner a sequence of rules is built for iteratively improving the initial model .", "Evaluation of the rule sequence is carried out on a test set of data which is independent of the training data .", "If we treat the output of an existing segmentation algorithm' as the initial state and the desired segmentation as the goal state , we can perform a series of transformations on the initial state removing extraneous boundaries and inserting new boundaries to obtain a more accurate approximation of the goal state .", "We therefore need only define an appropriate rule syntax for transforming this initial approxima'The quot ; existing quot ; algorithm does not need to be a large or even accurate system ; the algorithm can be arbitrarily simple as long as it assigns some form of initial segmentation . tion and prepare appropriate training data .", "For our experiments , we obtained corpora which had been manually segmented by native or nearnative speakers of Chinese and Thai .", "We divided the hand segmented data randomly into training and test sets .", "Roughly 80 of the data was used to train the segmentation algorithm , and 20 was used as a blind test set to score the rules learned from the training data .", "In addition to Chinese and Thai , we also performed segmentation experiments using a large corpus of English in which all the spaces had been removed from the texts .", "Most of our English experiments were performed using training and test sets with roughly the same 80 20 ratio , but in Section 3 . 4 . 3 we discuss results of English experiments with different amounts of training data .", "Unfortunately , we could not repeat these experiments with Chinese and Thai due to the small amount of handsegmented data available .", "There are three main types of transformations which can act on the current state of an imperfect segmentation In our syntax , Insert and Delete transformations can be triggered by any two adjacent characters a bigram and one character to the left or right of the bigram .", "Slide transformations can be triggered by a sequence of one , two , or three characters over which the boundary is to be moved .", "Figure 1 enumerates the 22 segmentation transformations we define .", "With the above algorithm in place , we can use the training data to produce a rule sequence to augment an initial segmentation approximation in order to obtain a better approximation of the desired segmentation .", "Furthermore , since all the rules are purely character based , a sequence can be learned for any character set and thus any language .", "We used our rule based algorithm to improve the word segmentation rate for several segmentation algorithms in three languages .", "Despite the number of papers on the topic , the evaluation and comparison of existing segmentation algorithms is virtually impossible .", "In addition to the problem of multiple correct segmentations of the same texts , the comparison of algorithms is difficult because of the lack of a single metric for reporting scores .", "Two common measures of performance are recall and precision , where recall is defined as the percent of words in the hand segmented text identified by the segmentation algorithm , and precision is defined as the percentage of words returned by the algorithm that also occurred in the hand segmented text in the same position .", "The component recall and precision scores are then used to calculate an F measure Rijsbergen , 1979 , where F 1 0 PRI 13P R .", "In this paper we will report all scores as a balanced F measure precision and recall weighted equally with 3 1 , such that For our Chinese experiments , the training set consisted of 2000 sentences 60187 words from a Xinhua news agency corpus ; the test set was a separate set of 560 sentences 18783 words from the same corpus . 5 We ran four experiments using this corpus , with four different algorithms providing the starting point for the learning of the segmentation transformations .", "In each case , the rule sequence learned from the training set resulted in a significant improvement in the segmentation of the test set .", "A very simple initial segmentation for Chinese is to consider each character a distinct word .", "Since the average word length is quite short in Chinese , with most words containing only 1 or 2 characters , 6 this character as word segmentation correctly identified many one character words and produced an initial segmentation score of F 40 . 3 .", "While this is a low segmentation score , this segmentation algorithm identifies enough words to provide a reasonable initial segmentation approximation .", "In fact , the CAW algorithm alone has been shown Buckley et al . , 1996 ; Broglio et al . , 1996 to be adequate to be used successfully in Chinese information retrieval .", "Our algorithm learned 5903 transformations from the 2000 sentence training set .", "The 5903 transformations applied to the test set improved the score from F 40 . 3 to 78 . 1 , a 63 . 3 reduction in the error rate .", "This is a very surprising and encouraging result , in that , from a very naive initial approximation using no lexicon except that implicit from the training data , our rule based algorithm is able to produce a series of transformations with a high segmentation accuracy .", "A common approach to word segmentation is to use a variation of the maximum matching algorithm , frequently referred to as the quot ; greedy algorithm . quot ; The greedy algorithm starts at the first character in a text and , using a word list for the language being segmented , attempts to find the longest word in the list starting with that character .", "If a word is found , the maximum matching algorithm marks a boundary at the end of the longest word , then begins the same longest match search starting at the character following the match .", "If no match is found in the word list , the greedy algorithm simply skips that character and begins the search starting at the next character .", "In this manner , an initial segmentation can be obtained that is more informed than a simple character as word approach .", "We applied the maximum matching algorithm to the test set using a list of 57472 Chinese words from the NMSU CHSEG segmenter described in the next section .", "This greedy algorithm produced an initial score of F 64 . 4 .", "A sequence of 2897 transformations was learned from the training set ; applied to the test set , they improved the score from F 64 . 4 to 84 . 9 , a 57 . 8 error reduction .", "From a simple Chinese word list , the rule based algorithm was thus able to produce asegmentation score comparable to segmentation algorithms developed with a large amount of domain knowledge as we will see in the next section .", "This score was improved further when combining the character as word CAW and the maximum matching algorithms .", "In the maximum matching algorithm described above , when a sequence of characters occurred in the text , and no subset of the sequence was present in the word list , the entire sequence was treated as a single word .", "This often resulted in words containing 10 or more characters , which is very unlikely in Chinese .", "In this experiment , when such a sequence of characters was encountered , each of the characters was treated as a separate word , as in the CAW algorithm above .", "This variation of the greedy algorithm , using the same list of 57472 words , produced an initial score of F 82 . 9 .", "A sequence of 2450 transformations was learned from the training set ; applied to the test set , they improved the score from F 82 . 9 to 87 . 7 , a 28 . 1 error reduction .", "The score produced using this variation of the maximum matching algorithm combined with a rule sequence 87 . 7 is nearly equal to the score produced by the NMSU segmenter segmenter 87 . 9 discussed in the next section .", "The previous three experiments showed that our rule sequence algorithm can produce excellent segmentation results given very simple initial segmentation algorithms .", "However , assisting in the adaptation of an existing algorithm to different segmentation schemes , as discussed in Section 1 , would most likely be performed with an already accurate , fullydeveloped algorithm .", "In this experiment we demonstrate that our algorithm can also improve the output of such a system .", "The Chinese segmenter CHSEG developed at the Computing Research Laboratory at New Mexico State University is a complete system for highaccuracy Chinese segmentation Jin , 1994 .", "In addition to an initial segmentation module that finds words in a text based on a list of Chinese words , CHSEG additionally contains specific modules for recognizing idiomatic expressions , derived words , Chinese person names , and foreign proper names .", "The accuracy of CHSEG on an 8 . 6MB corpus has been independently reported as F 84 . 0 Ponte and Croft , 1996 .", "For reference , Ponte and Croft report scores of F 86 . 1 and 83 . 6 for their probabilistic Chinese segmentation algorithms trained on over 100MB of data .", "On our test set , CHSEG produced a segmentation score of F 87 . 9 .", "Our rule based algorithm learned a sequence of 1755 transformations from the training set ; applied to the test set , they improved the score from 87 . 9 to 89 . 6 , a 14 . 0 reduction in the error rate .", "Our rule based algorithm is thus able to produce an improvement to an existing high performance system .", "Table 1 shows a summary of the four Chinese experiments .", "While Thai is also an unsegmented language , the Thai writing system is alphabetic and the average word length is greater than Chinese . 7 We would therefore expect that our character based transformations would not work as well with Thai , since a context of more than one character is necessary in many cases to make many segmentation decisions in alphabetic languages .", "The Thai corpus consisted of texts' from the Thai News Agency via NECTEC in Thailand .", "For our experiment , the training set consisted of 3367 sentences 40937 words ; the test set was a separate set of 1245 sentences 13724 words from the same corpus .", "The initial segmentation was performed using the maximum matching algorithm , with a lexicon of 9933 Thai words from the word separation filter in cite ; a Thai language Latex package .", "This greedy algorithm gave an initial segmentation score of F 48 . 2 on the test set .", "Our rule based algorithm learned a sequence of 731 transformations which improved the score from 48 . 2 to 63 . 6 , a 29 . 7 error reduction .", "While the alphabetic system is obviously harder to segment , we still see a significant reduction in the segmenter error rate using the transformation based algorithm .", "Nevertheless , it is doubtful that a segmentation with a score of 63 . 6 would be useful in too many applications , and this result will need to be significantly improved .", "Although English is not an unsegmented language , the writing system is alphabetic like Thai and the average word length is similar . '", "Since English language resources e . g . word lists and morphological analyzers are more readily available , it is instructive to experiment with a de segmented English corpus , that is , English texts in which the spaces have been removed and word boundaries are not explicitly indicated .", "The following shows an example of an English sentence and its de segmented version About 20 , 000 years ago the last ice age ended .", "About20 , 000yearsagothelasticeageended The results of such experiments can help us determine which resources need to be compiled in order to develop a high accuracy segmentation algorithm in unsegmented alphabetic languages such as Thai .", "In addition , we are also able to provide a more detailed error analysis of the English segmentation since the author can read English but not Thai .", "Our English experiments were performed using a corpus of texts from the Wall Street Journal WSJ .", "The training set consisted of 2675 sentences 64632 words in which all the spaces had been removed ; the test set was a separate set of 700 sentences 16318 words from the same corpus also with all spaces removed .", "For an initial experiment , segmentation was performed using the maximum matching algorithm , with a large lexicon of 34272 English words compiled from the WSJ . quot ; In contrast to the low initial Thai score , the greedy algorithm gave an initial English segmentation score of F 73 . 2 .", "Our rule based algorithm learned a sequence of 800 transformations , 'The average length of a word in our English data was 4 . 46 characters , compared to 5 . 01 for Thai and 1 . 60 for Chinese .", "10Note that the portion of the WSJ corpus used to compile the word list was independent of both the training and test sets used in the segmentation experiments . which improved the score from 73 . 2 to 79 . 0 , a 21 . 6 error reduction .", "The difference in the greedy scores for English and Thai demonstrates the dependence on the word list in the greedy algorithm .", "For example , an experiment in which we randomly removed half of the words from the English list reduced the performance of the greedy algorithm from 73 . 2 to 32 . 3 ; although this reduced English word list was nearly twice the size of the Thai word list 17136 vs . 9939 , the longest match segmentation utilizing the list was much lower 32 . 3 vs . 48 . 2 .", "Successive experiments in which we removed different random sets of half the words from the original list resulted in greedy algorithm performance of 39 . 2 , 35 . 1 , and 35 . 5 .", "Yet , despite the disparity in initial segmentation scores , the transformation sequences effect a significant error reduction in all cases , which indicates that the transformation sequences are effectively able to compensate to some extent for weaknesses in the lexicon .", "Table 2 provides a summary of the results using the greedy algorithm for each of the three languages .", "As mentioned above , lexical resources are more readily available for English than for Thai .", "We can use these resources to provide an informed initial segmentation approximation separate from the greedy algorithm .", "Using our native knowledge of English as well as a short list of common English prefixes and suffixes , we developed a simple algorithm for initial segmentation of English which placed boundaries after any of the suffixes and before any of the prefixes , as well as segmenting punctuation characters .", "In most cases , this simple approach was able to locate only one of the two necessary boundaries for recognizing full words , and the initial score was understandably low , F 29 . 8 .", "Nevertheless , even from this flawed initial approximation , our rule based algorithm learned a sequence of 632 transformations which nearly doubled the word recall , improving the score from 29 . 8 to 53 . 3 , a 33 . 5 error reduction .", "Since we had a large amount of English data , we also performed a classic experiment to determine the effect the amount of training data had on the ability of the rule sequences to improve segmentation .", "We started with a training set only slightly larger than the test set , 872 sentences , and repeated the maximum matching experiment described in Section 3 . 4 . 1 .", "We then incrementally increased the amount of training data and repeated the experiment .", "The results , summarized in Table 3 , clearly indicate not surprisingly that more training sentences produce both a longer rule sequence and a larger error reduction in the test data .", "Upon inspection of the English segmentation errors produced by both the maximum matching algorithm and the learned transformation sequences , one major category of errors became clear .", "Most apparent was the fact that the limited context transformations were unable to recover from many errors introduced by the naive maximum matching algorithm .", "For example , because the greedy algorithm always looks for the longest string of characters which can be a word , given the character sequence quot ; economicsituation quot ; , the greedy algorithm first recognized quot ; economics quot ; and several shorter words , segmenting the sequence as quot ; economics it u at io n quot ; .", "Since our transformations consider only a single character of context , the learning algorithm was unable to patch the smaller segments back together to produce the desired output quot ; economic situation quot ; .", "In some cases , the transformations were able to recover some of the word , but were rarely able to produce the full desired output .", "For example , in one case the greedy algorithm segmented quot ; humanactivity quot ; as quot ; humana c ti vi ty quot ; .", "The rule sequence was able to transform this into quot ; humana ctivity quot ; , but was not able to produce the desired quot ; human activity quot ; .", "This suggests that both the greedy algorithm and the transformation learning algorithm need to have a more global word model , with the ability to recognize the impact of placing a boundary on the longer sequences of characters surrounding that point .", "The results of these experiments demonstrate that a transformation based rule sequence , supplementing a rudimentary initial approximation , can produce accurate segmentation .", "In addition , they are able to improve the performance of a wide range of segmentation algorithms , without requiring expensive knowledge engineering .", "Learning the rule sequences can be achieved in a few hours and requires no language specific knowledge .", "As discussed in Section 1 , this simple algorithm could be used to adapt the output of an existing segmentation algorithm to different segmentation schemes as well as compensating for incomplete segmenter lexica , without requiring modifications to segmenters themselves .", "The rule based algorithm we developed to improve word segmentation is very effective for segmenting Chinese ; in fact , the rule sequences combined with a very simple initial segmentation , such as that from a maximum matching algorithm , produce performance comparable to manually developed segmenters .", "As demonstrated by the experiment with the NMSU segmenter , the rule sequence algorithm can also be used to improve the output of an already highly accurate segmenter , thus producing one of the best segmentation results reported in the literature .", "In addition to the excellent overall results in Chinese segmentation , we also showed the rule sequence algorithm to be very effective in improving segmentation in Thai , an alphabetic language .", "While the scores themselves were not as high as the Chinese performance , the error reduction was nevertheless very high , which is encouraging considering the simple rule syntax used .", "The current state of our algorithm , in which only three characters are considered at a time , will understandably perform better with a language like Chinese than with an alphabetic language like Thai , where average word length is much greater .", "The simple syntax described in Section 2 . 2 can , however , be easily extended to consider larger contexts to the left and the right of boundaries ; this extension would necessarily come at a corresponding cost in learning speed since the size of the rule space searched during training would grow accordingly .", "In the future , we plan to further investigate the application of our rule based algorithm to alphabetic languages .", "Acknowledgements This work would not have been possible without the assistance and encouragement of all the members of the MITRE Natural Language Group .", "This paper benefited greatly from discussions with and comments from Marc Vilain , Lynette Hirschman , Sam Bayer , and the anonymous reviewers ."], "summary_lines": ["A Trainable Rule-Based Algorithm For Word Segmentation\n", "This paper presents a trainable rule-based algorithm for performing word segmentation.\n", "The algorithm provides a simple, language-independent alternative to large-scale lexicai-based segmenters requiring large amounts of knowledge engineering.\n", "As a stand-alone segmenter, we show our algorithm to produce high performance Chinese segmentation.\n", "In addition, we show the transformation-based algorithm to be effective in improving the output of several existing word segmentation algorithms in three different languages.\n", "Our Chinese segmenter makes use of only a manually segmented corpus without referring to any lexicon.\n"]}
{"article_lines": ["Bidirectional Inference With The Easiest First Strategy For Tagging Sequence Data", "This paper presents a bidirectional inference algorithm for sequence labeling problems such as part of speech tag ging , named entity recognition and text chunking .", "The algorithm can enumerate all possible decomposition structures andfind the highest probability sequence together with the corresponding decomposi tion structure in polynomial time .", "We also present an efficient decoding algorithm based on the easiest first strategy , which gives comparably good performance tofull bidirectional inference with significantly lower computational cost .", "Exper imental results of part of speech tagging and text chunking show that the proposedbidirectional inference methods consis tently outperform unidirectional inference methods and bidirectional MEMMs give comparable performance to that achievedby state of the art learning algorithms in cluding kernel support vector machines .", "The task of labeling sequence data such as part of speech POS tagging , chunking shallow parsing and named entity recognition is one of the most im portant tasks in natural language processing .", "Conditional random fields CRFs Lafferty et al , 2001 have recently attracted much attention because they are free from so called label bias prob lems which reportedly degrade the performance of sequential classification approaches like maximum entropy markov models MEMMs .", "Although sequential classification approachescould suffer from label bias problems , they have sev eral advantages over CRFs .", "One is the efficiencyof training .", "CRFs need to perform dynamic programming over the whole sentence in order to compute feature expectations in each iteration of numerical optimization .", "Training , for instance , second order CRFs using a rich set of features can require prohibitive computational resources .", "Max marginmethods for structured data share problems of com putational cost Altun et al , 2003 . Another advantage is that one can employ a variety of machine learning algorithms as the local classifier .", "There is huge amount of work about developing classification algorithms that have high generalization performance in the machine learning community .", "Being able to incorporate such state of theart machine learning algorithms is important .", "Indeed , sequential classification approaches with kernel support vector machines offer competitive per formance in POS tagging and chunking Gimenez and Marquez , 2003 ; Kudo and Matsumoto , 2001 .", "One obvious way to improve the performance of sequential classification approaches is to enrich theinformation that the local classifiers can use .", "In stan dard decomposition techniques , the local classifiers cannot use the information about future tags e . g . the right side tags in left to right decoding , which would be helpful in predicting the tag of the targetword .", "To make use of the information about future tags , Toutanova et al proposed a tagging algo rithm based on bidirectional dependency networks 467 Toutanova et al , 2003 and achieved the best ac curacy on POS tagging on the Wall Street Journal corpus .", "As they pointed out in their paper , however , their method potentially suffers from ? collusion ?", "ef fects which make the model lock onto conditionally consistent but jointly unlikely sequences .", "In theirmodeling , the local classifiers can always use the in formation about future tags , but that could cause a double counting effect of tag information .", "In this paper we propose an alternative way of making use of future tags .", "Our inference method considers all possible ways of decomposition andchooses the ? best ?", "decomposition , so the informa tion about future tags is used only in appropriate situations .", "We also present a deterministic versionof the inference method and show their effective ness with experiments of English POS tagging and chunking , using standard evaluation sets .", "The task of labeling sequence data is to find the se quence of tags t1 . . . tn that maximizes the following probability given the observation o o1 . . . on P t1 . . . tn o .", "1 Observations are typically words and their lexicalfeatures in the task of POS tagging .", "Sequential clas sification approaches decompose the probability as follows , P t1 . . . tn o n ?", "i 1 p ti t1 . . . ti ? 1o .", "2 This is the left to right decomposition .", "If we make a first order markov assumption , the equation becomes P t1 . . . tn o n ?", "i 1 p ti ti ? 1o .", "3 Then we can employ a probabilistic classifier trained with the preceding tag and observations in order to obtain p ti ti ? 1o for local classification .", "A common choice for the local probabilistic classifier is maximum entropy classifiers Berger et al , 1996 .", "The best tag sequence can be efficiently computed by using a Viterbi decoding algorithm in polynomial time .", "t1 a t2 t3 o t1 b t2 t3 t1 c t2 t3 t1 d t2 t3 o o o Figure 1 Different structures for decomposition .", "The right to left decomposition is P t1 . . . tn o n ?", "i 1 p ti ti 1o .", "4 These two ways of decomposition are widely usedin various tagging problems in natural language pro cessing .", "The issue with such decompositions is that you have only the information about the preceding or following tags when performing local classifi cation .", "From the viewpoint of local classification , we want to give the classifier as much information as possible because the information about neighboring tags is useful in general .", "As an example , consider the situation where we are going to annotate a three word sentence withpart of speech tags .", "Figure 1 shows the four possi ble ways of decomposition .", "They correspond to the following equations a P t1 . . . t3 o P t1 o P t2 t1o P t3 t2o 5 b P t1 . . . t3 o P t3 o P t2 t3o P t1 t2o 6 c P t1 . . . t3 o P t1 o P t3 o P t2 t3t1o 7 d P t1 . . . t3 o P t2 o P t1 t2o P t3 t2o 8 a and b are the standard left to right and rightto left decompositions .", "Notice that in decomposi tion c , the local classifier can use the information about the tags on both sides when deciding t2 .", "If , for example , the second word is difficult to tag e . g . an unknown word , we might as well take the de composition structure c because the local classifier 468 can use rich information when deciding the tag of the most difficult word .", "In general if we have ann word sentence and adopt a first order markov assumption , we have 2n ? 1 possible ways of decomposition because each of the n ? 1 edges in the cor responding graph has two directions left to right or right to left .", "Our bidirectional inference method is to consider all possible decomposition structures and choose the ? best ?", "structure and tag sequence .", "We will show inthe next section that this is actually possible in poly nomial time by dynamic programming . As for the training , let us look at the equa tions of four different decompositions above .", "You can notice that there are only four types of local conditional probabilities P ti ti ? 1o , P ti ti 1o , P ti ti ? 1ti 1o , and P ti o . This means that if we have these four types of lo cal classifiers , we can consider any decompositionstructures in the decoding stage .", "These local classi fiers can be obtained by training with corresponding neighboring tag information .", "Training the first twotypes of classifiers is exactly the same as the training of popular left to right and right to left sequen tial classification models respectively .", "If we take a second order markov assumption , we need to train 16 types of local classifiers because each of the four neighboring tags of a classificationtarget has two possibilities of availability .", "In gen eral , if we take a k th order markov assumption , we need to train 22k types of local classifies .", "2 . 1 Polynomial Time Inference .", "This section describes an algorithm to find the de composition structure and tag sequence that give the highest probability .", "The algorithm for the first order case is an adaptation of the algorithm for decodingthe best sequence on a bidirectional dependency net work introduced by Toutanova et al , 2003 , which originates from the Viterbi decoding algorithm for second order markov models . Figure 2 shows a polynomial time decoding algorithm for our bidirectional inference .", "It enumer ates all possible decomposition structures and tag sequences by recursive function calls , and finds the highest probability sequence .", "Polynomial time isachieved by caching .", "Note that for each local clas sification , the function chooses the appropriate local function bestScore return bestScoreSub n 2 , ? end , end , end ? , ? L , L ? ; function bestScoreSub i 1 , ? ti ? 1 , ti , ti 1 ? , ? di ? 1 , di ?", "memorization if cached i 1 , ? ti ? 1 , ti , ti 1 ? , ? di ? 1 , di ? return cache i 1 , ? ti ? 1 , ti , ti 1 ? , ? di ? 1 , di ? ; left boundary case if i 1 if ? ti ? 1 , ti , ti 1 ?", "? start , start , start ?", "return 1 ; else return 0 ; recursive case P localClassification i , ? ti ? 1 , ti , ti 1 ? , ? di ? 1 , di ? ; return maxdi ? 2 maxti ? 2 P ?", "bestScoreSub i , ? ti ? 2 , ti ? 1 , ti ? , ? di ? 2 , di ? 1 ? ; function localClassification i , ? ti ? 1 , ti , ti 1 ? , ? di ? 1 , di ?", "if di ? 1 L di L return P ti ti 1 , o ; if di ? 1 L di R return P ti o ; if di ? 1 R di L return P ti ti ? 1ti 1 , o ; if di ? 1 R di R return P ti ti ? 1 , o ; Figure 2 Pseudo code for bidirectional inference for the first order conditional markov models .", "di is the direction of the edge between ti and ti 1 .", "classifier by taking into account the directions of the adjacent edges of the classification target .", "The second order case is similar but slightly morecomplex .", "Figure 3 shows the algorithm .", "The recur sive function needs to consider the directions of the four adjacent edges of the classification target , and maintain the directions of the two neighboring edgesto enumerate all possible edge directions .", "In addi tion , the algorithm rules out cycles in the structure .", "2 . 2 Decoding with the Easiest First Strategy .", "We presented a polynomial time decoding algorithm in the previous section .", "However , polynomial time is not low enough in practice .", "Indeed , even the Viterbi decoding of second order markov models for POS tagging is not practical unless some pruning methodis involved .", "The computational cost of the bidirec tional decoding algorithm presented in the previoussection is , of course , larger than that because it enu merates all possible directions of the edges on top of the enumeration of possible tag sequences .", "In this section we present a greedy version of the decoding method for bidirectional inference , which 469 function bestScore return bestScoreSub n 3 , ? end , end , end , end , end ? , ? L , L , L , L ? , ? L , L ? ; function bestScoreSub i 2 , ? ti ? 2 , ti ? 1 , ti , ti 1ti 2 ? , ? d ? i ? 1 , di ? 1 , di , d ? i 1 ? , ? di ? 2 , d ? i ? to avoid cycles if di ? 1 di di ! d ? i return 0 ; memorization if cached i 2 , ? ti ? 2 , ti ? 1 , ti , ti 1ti 2 ? , ? d ? i ? 1 , di ? 1 , di , d ? i 1 ? , ? di ? 2 , d ? i ? return cache i 2 , ? ti ? 2 , ti ? 1 , ti , ti 1ti 2 ? , ? d ? i ? 1 , di ? 1 , di , d ? i 1 ? , ? di ? 2 , d ? i ? ; left boundary case if i 2 if ? ti ? 2 , ti ? 1 , ti , ti 1 , ti 2 ?", "? start , start , start , start , start ?", "return 1 ; else return 0 ; recursive case P localClassification i , ? ti ? 2 , ti ? 1 , ti , ti 1 , ti 2 ? , ? d ? i ? 1 , di ? 1 , di , d ? i 1 ? ; return maxd ?", "i ? 2 maxdi ? 3 maxti ? 3 P ?", "bestScoreSub i 1 , ? ti ? 3 , ti ? 2 , ti ? 1 , titi 1 ? , ? d ? i ? 2 , di ? 2 , di ? 1 , d ? i ? , ? di ? 3 , d ? i ? 1 ? ; Figure 3 Pseudo code for bidirectional inference for the second order conditional markov models .", "di is the direction of the edge between ti and ti 1 .", "d ? i is the direction of the edge between ti ? 1 and ti 1 .", "We omit the localClassification function because it is the obvious extension of that for the first order case .", "is extremely simple and significantly more efficient than full bidirectional decoding . Instead of enumerating all possible decomposition structures , the algorithm determines the struc ture by adopting the easiest first strategy .", "The whole decoding algorithm is given below .", "Find the ? easiest ?", "word to tag . .", "Tag the word . .", "We assume in this paper that the ? easiest ?", "word to tag is the word for which the classifier outputs the highest probability .", "In finding the easiest word , we use the appropriate local classifier according to the availability of the neighboring tags .", "Therefore , in the first iteration , we always use the local classi fiers trained with no contextual tag information i . e .", "P ti o .", "Then , for example , if t3 has been tagged in the first iteration in a three word sentence , we use P t2 t3o to compute the probability for tagging t2 in the second iteration as in Figure 1 b .", "A naive implementation of this algorithm requires O n2 invocations of local classifiers , where n is the number of the words in the sentence , because we need to update the probabilities over the words ateach iteration .", "However , a k th order Markov as sumption obviously allows us to skip most of the probability updates , resulting in O kn invocations of local classifiers .", "This enables us to build a very efficient tagger .", "For local classifiers , we used a maximum entropy model which is a common choice for incorporating various types of features for classification problems in natural language processing Berger et al , 1996 .", "Regularization is important in maximum entropy modeling to avoid overfitting to the training data .", "For this purpose , we use the maximum entropy modeling with inequality constraints Kazama andTsujii , 2003 .", "The model gives equally good per formance as the maximum entropy modeling with Gaussian priors Chen and Rosenfeld , 1999 , and the size of the resulting model is much smaller thanthat of Gaussian priors because most of the param eters become zero .", "This characteristic enables us to easily handle the model data and carry out quick decoding , which is convenient when we repetitivelyperform experiments .", "This modeling has one param eter to tune , which is called the width factor .", "We tuned this parameter using the development data in each type of experiments .", "470 Current word wi ti Previous word wi ? 1 ti Next word wi 1 ti Bigram features wi ? 1 , wi ti wi , wi 1 ti Previous tag ti ? 1 ti Tag two back ti ? 2 ti Next tag ti 1 ti Tag two ahead ti 2 ti Tag Bigrams ti ? 2 , ti ? 1 ti ti ? 1 , ti 1 ti ti 1 , ti 2 ti Tag Trigrams ti ? 2 , ti ? 1 , ti 1 ti ti ? 1 , ti 1 , ti 2 ti Tag 4 grams ti ? 2 , ti ? 1 , ti 1 , ti 2 ti Tag Word ti ? 1 , wi ti combination ti 1 , wi ti ti ? 1 , ti 1 , wi ti Prefix features prefixes of wi ti up to length 10 Suffix features suffixes of wi ti up to length 10 Lexical features whether wi has a hyphen ti whether wi has a number ti whether wi has a capital letter ti whether wi is all capital tiTable 1 Feature templates used in POS tagging ex periments .", "Tags are parts of speech .", "Tag featuresare not necessarily used in all the models .", "For example , ? next tag ?", "features cannot be used in left to right models .", "To evaluate the bidirectional inference methods pre sented in the previous sections , we ran experimentson POS tagging and text chunking with standard En glish data sets .", "Although achieving the best accuracy is not the primary purpose of this paper , we explored usefulfeature sets and parameter setting by using develop ment data in order to make the experiments realistic .", "4 . 1 Part of speech tagging experiments .", "We split the Penn Treebank corpus Marcus et al , 1994 into training , development and test sets as in Collins , 2002 .", "Sections 0 18 are used as the train ing set .", "Sections 19 21 are the development set , andsections 22 24 are used as the test set .", "All the ex periments were carried out on the development set , except for the final accuracy report using the best setting .", "For features , we basically adopted the feature set Method Accuracy Speed tokens sec Left to right Viterbi 96 . 92 844 Right to left Viterbi 96 . 89 902 Dependency Networks 97 . 06 1 , 446 Easiest last 96 . 58 2 , 360 Easiest first 97 . 13 2 , 461 Full bidirectional 97 . 12 34Table 2 POS tagging accuracy and speed on the de velopment set .", "Method Accuracy Dep .", "Networks Toutanova et al , 2003 97 . 24 Perceptron Collins , 2002 97 . 11 SVM Gimenez and Marquez , 2003 97 . 05 HMM Brants , 2000 96 . 48 Easiest first 97 . 10 Full Bidirectional 97 . 15Table 3 POS tagging accuracy on the test set Sec tions 22 24 of the WSJ , 5462 sentences . provided by Toutanova et al , 2003 except for com plex features such as crude company name detectionfeatures because they are specific to the Penn Tree bank and we could not find the exact implementation details .", "Table 1 lists the feature templates used in our experiments .", "We tested the proposed bidirectional methods , conventional unidirectional methods and the bidirec tional dependency network proposed by Toutanova Toutanova et al , 2003 for comparison .", "All the models are second order .", "Table 2 shows the accuracy and tagging speed on the developmentdata 2 .", "Bidirectional inference methods clearly out performed unidirectional methods .", "Note that the easiest first decoding method achieves equally good performance as full bidirectional inference .", "Table 2 also shows that the easiest last strategy , where weselect and tag the most difficult word at each itera tion , is clearly a bad strategy . An example of easiest first decoding is given be low 1For dependency network and full bidirectional decoding , we conducted pruning because the computational cost was too large to perform exhaustive search .", "We pruned a tag candidate if the zero th order probability of the candidate P ti o was lower than one hundredth of the zero th order probability of the most likely tag at the token .", "2Tagging speed was measured on a server with an AMD Opteron 2 . 4GHz CPU .", "471 The DT 4 company NN 7 had VBD 11sought VBN 14 increases NNS 13 total ing VBG 12 2 80 . 3 CD 5 million CD 8 , , 1 or CC 6 22 CD 9 NN 10 . . 3 Each token represents Word PoS DecodingOrder .", "Typically , punctuations and articles are tagged first .", "Verbs are usually tagged in later stages because their tags are likely to be ambiguous .", "We applied our bidirectional inference methods to the test data .", "The results are shown in Table 3 .", "The table also summarizes the accuracies achieved by several other research efforts .", "The best accuracyis 97 . 24 achieved by bidirectional dependency net works Toutanova et al , 2003 with a richer set of features that are carefully designed for the corpus .", "A perceptron algorithm gives 97 . 11 Collins , 2002 .", "Gimenez and Marquez achieve 97 . 05 with support vector machines SVMs .", "This result indicates thatbidirectional inference with maximum entropy mod eling can achieve comparable performance to other state of the art POS tagging methods .", "4 . 2 Chunking Experiments .", "The task of chunking is to find non recursive phrases in a sentence .", "For example , a chunker segments the sentence ? He reckons the current account deficit willnarrow to only 1 . 8 billion in September ?", "into the fol lowing , NP He VP reckons NP the current accountdeficit VP will narrow PP to NP only 1 . 8 bil lion PP in NP September . We can regard chunking as a tagging task by con verting chunks into tags on tokens .", "There are severalways of representing text chunks Sang and Veen stra , 1999 .", "We tested the Start End representation in addition to the popular IOB2 representation since local classifiers can have fine grained informationon the neighboring tags in the Start End represen tation . For training and testing , we used the data set pro vided for the CoNLL 2000 shared task .", "The training set consists of section 15 18 of the WSJ corpus , and the test set is section 20 .", "In addition , we made the development set from section 21 3 .", "We basically adopted the feature set provided in 3We used the Perl script provided on http ilk . kub . nl ?", "sabine chunklink Current word wi ti Previous word wi ? 1 ti Word two back wi ? 2 ti Next word wi 1 ti Word two ahead wi 2 ti Bigram features wi ? 2 , wi ? 1 ti wi ? 1 , wi ti wi , wi 1 ti wi 1 , wi 2 ti Current POS pi ti Previous POS pi ? 1 ti POS two back pi ? 2 ti Next POS pi 1 ti POS two ahead pi 2 ti Bigram POS features pi ? 2 , pi ? 1 ti pi ? 1 , pi ti pi , pi 1 ti pi 1 , pi 2 ti Trigram POS features pi ? 2 , pi ? 1 , pi ti pi ? 1 , pi , pi 1 ti pi , pi 1 , pi 2 ti Previous tag ti ? 1 ti Tag two back ti ? 2 ti Next tag ti 1 ti Tag two ahead ti 2 ti Bigram tag features ti ? 2 , ti ? 1 ti ti ? 1 , ti 1 ti ti 1 , ti 2 tiTable 4 Feature templates used in chunking experi ments . Collins , 2002 and used POS trigrams as well .", "Ta ble 4 lists the features used in chunking experiments .", "Table 5 shows the results on the development set . Again , bidirectional methods exhibit better perfor mance than unidirectional methods .", "The differenceis bigger with the Start End representation .", "Depen dency networks did not work well for this chunking task , especially with the Start End representation .", "We applied the best model on the development set in each chunk representation type to the test data .", "Table 6 summarizes the performance on thetest set .", "Our bidirectional methods achieved F scores of 93 . 63 and 93 . 70 , which are better than the best F score 93 . 48 of the CoNLL 2000 shared task Sang and Buchholz , 2000 and comparable to those achieved by other state of the art methods .", "There are some reports that one can improve the performance of unidirectional models by combiningoutputs of multiple taggers .", "Shen et al 2003 re ported a 4 . 9 error reduction of supertagging by 472 Representation Method Order Recall Precision F score Speed tokens sec IOB2 Left to right 1 93 . 17 93 . 05 93 . 11 1 , 775 2 93 . 13 92 . 90 93 . 01 989 Right to left 1 92 . 92 92 . 82 92 . 87 1 , 635 2 92 . 92 92 . 74 92 . 87 927 Dependency Networks 1 92 . 71 92 . 91 92 . 81 2 , 534 2 92 . 61 92 . 95 92 . 78 1 , 893 Easiest first 1 93 . 17 93 . 04 93 . 11 2 , 441 2 93 . 35 93 . 32 93 . 33 1 , 248 Full Bidirectional 1 93 . 29 93 . 14 93 . 21 712 2 93 . 26 93 . 12 93 . 19 48 Start End Left to right 1 92 . 98 92 . 69 92 . 83 861 2 92 . 96 92 . 67 92 . 81 439 Right to left 1 92 . 92 92 . 83 92 . 87 887 2 92 . 89 92 . 74 92 . 82 451 Dependency Networks 1 87 . 10 89 . 56 88 . 32 1 , 894 2 87 . 16 89 . 44 88 . 28 331 Easiest first 1 93 . 33 92 . 95 93 . 14 1 , 950 2 93 . 31 92 . 95 93 . 13 1 , 016 Full Bidirectional 1 93 . 52 93 . 26 93 . 39 392 2 93 . 44 93 . 20 93 . 32 4 Table 5 Chunking F scores on the development set .", "Method Recall Precision F score SVM Kudoh and Matsumoto , 2000 93 . 51 93 . 45 93 . 48 SVM voting Kudo and Matsumoto , 2001 93 . 92 93 . 89 93 . 91 Regularized Winnow with basic features Zhang et al , 2002 93 . 60 93 . 54 93 . 57 Perceptron Carreras and Marquez , 2003 93 . 29 94 . 19 93 . 74 Easiest first IOB2 , second order 93 . 59 93 . 68 93 . 63 Full Bidirectional Start End , first order 93 . 70 93 . 65 93 . 70 Table 6 Chunking F scores on the test set Section 20 of the WSJ , 2012 sentences . pairwise voting between left to right and right to left taggers .", "Kudo et al 2001 attained performance improvement in chunking by conducting weighted voting of multiple SVMs trained with distinct chunk representations .", "The biggest difference between ourapproach and such voting methods is that the lo cal classifier in our bidirectional inference methodscan have rich information for decision .", "Also , vot ing methods generally need many tagging processes to be run on a sentence , which makes it difficult to build a fast tagger . Our algorithm can be seen as an ensemble classi fier by which we choose the highest probability oneamong the different taggers with all possible decom position structures .", "Although choosing the highest probability one is seemingly natural and one of the simplest ways for combining the outputs of differenttaggers , one could use a different method e . g . sum ming the probabilities over the outputs which share the same label sequence .", "Investigating the methods for combination should be an interesting direction of future work .", "As for the computational cost for training , our methods require us to train 22n types of classifiers when we adopt an nth order markov assumption .", "Inmany cases a second order model is sufficient because further increase of n has little impact on per formance .", "Thus the training typically takes four or 16 times as much time as it would take for training a single unidirectional tagger , which looks somewhatexpensive .", "However , because each type of classi fier can be trained independently , the training can be performed completely in parallel and run with the same amount of memory as that for training a single classifier .", "This advantage contrasts with the case for CRFs which requires substantial amount ofmemory and computational cost if one tries to incor porate higher order features about tag sequences .", "Tagging speed is another important factor inbuilding a practical tagger for large scale text min 473 ing .", "Our inference algorithm with the easiest first strategy needs no Viterbi decoding unlike MEMMs and CRFs , and makes it possible to perform very fast tagging with high precision .", "We have presented a bidirectional inference algo rithm for sequence labeling problems such as POStagging , named entity recognition and text chunking .", "The algorithm can enumerate all possible decomposition structures and find the highest prob ability sequence together with the corresponding decomposition structure in polynomial time .", "Wehave also presented an efficient bidirectional infer ence algorithm based on the easiest first strategy , which gives comparable performance to full bidirectional inference with significantly lower compu tational cost .", "Experimental results of POS tagging and textchunking show that the proposed bidirectional inference methods consistently outperform unidi rectional inference methods and our bidirectional MEMMs give comparable performance to thatachieved by state of the art learning algorithms in cluding kernel support vector machines .", "A natural extension of this work is to replace the maximum entropy modeling , which was used asthe local classifiers , with other machine learning algorithms .", "Support vector machines with appropri ate kernels is a good candidate because they havegood generalization performance as a single classi fier .", "Although SVMs do not output probabilities , theeasiest first method would be easily applied by considering the margins output by SVMs as the confi dence of local classification ."], "summary_lines": ["Bidirectional Inference With The Easiest-First Strategy For Tagging Sequence Data\n", "This paper presents a bidirectional inference algorithm for sequence labeling problems such as part-of-speech tagging, named entity recognition and text chunking.\n", "The algorithm can enumerate all possible decomposition structures and find the highest probability sequence together with the corresponding decomposition structure in polynomial time.\n", "We also present an efficient decoding algorithm based on the easiest-first strategy, which gives comparably good performance to full bidirectional inference with significantly lower computational cost.\n", "Experimental results of part-of-speech tagging and text chunking show that the proposed bidirectional inference methods consistently outperform unidirectional inference methods and bidirectional MEMMs give comparable performance to that achieved by state-of-the-art learning algorithms including kernel support vector machines.\n", "We propose easiest-first deterministic decoding.\n"]}
{"article_lines": ["Translating Named Entities Using Monolingual And Bilingual Resources", "Named entity phrases are some of the most difficult phrases to translate because new phrases can appear from nowhere , and because many are domain specific , not to be found in bilingual dictionaries .", "We present a novel algorithm for translating named entity phrases using easily obtainable monolingual and bilingual resources .", "We report on the application and evaluation of this algorithm in translating Arabic named entities to English .", "We also compare our results with the results obtained from human translations and a commercial system for the same task .", "Named entity phrases are being introduced in news stories on a daily basis in the form of personal names , organizations , locations , temporal phrases , and monetary expressions .", "While the identification of named entities in text has received significant attention e . g . , Mikheev et al . 1999 and Bikel et al .", "1999 , translation of named entities has not .", "This translation problem is especially challenging because new phrases can appear from nowhere , and because many named entities are domain specific , not to be found in bilingual dictionaries .", "A system that specializes in translating named entities such as the one we describe here would be an important tool for many NLP applications .", "Statistical machine translation systems can use such a system as a component to handle phrase translation in order to improve overall translation quality .", "CrossLingual Information Retrieval CLIR systems could identify relevant documents based on translations of named entity phrases provided by such a system .", "Question Answering QA systems could benefit substantially from such a tool since the answer to many factoid questions involve named entities e . g . , answers to who questions usually involve Persons Organizations , where questions involve Locations , and when questions involve Temporal Expressions .", "In this paper , we describe a system for ArabicEnglish named entity translation , though the technique is applicable to any language pair and does not require especially difficult to obtain resources .", "The rest of this paper is organized as follows .", "In Section 2 , we give an overview of our approach .", "In Section 3 , we describe how translation candidates are generated .", "In Section 4 , we show how monolingual clues are used to help re rank the translation candidates list .", "In Section 5 , we describe how the candidates list can be extended using contextual information .", "We conclude this paper with the evaluation results of our translation algorithm on a test set .", "We also compare our system with human translators and a commercial system .", "The frequency of named entity phrases in news text reflects the significance of the events they are associated with .", "When translating named entities in news stories of international importance , the same event will most likely be reported in many languages including the target language .", "Instead of having to come up with translations for the named entities often with many unknown words in one document , sometimes it is easier for a human to find a document in the target language that is similar to , but not necessarily a translation of , the original document and then extract the translations .", "Let s illustrate this idea with the following example We would like to translate the named entities that appear in the following Arabic excerpt The Arabic newspaper article from which we extracted this excerpt is about negotiations between the US and North Korean authorities regarding the search for the remains of US soldiers who died during the Korean war .", "We presented the Arabic document to a bilingual speaker and asked them to translate the locations t\u02c7swzyn h z an , awns an , and kw\u02c7g an\u02c7g . The translations they provided were Chozin Reserve , Onsan , and Kojanj .", "It is obvious that the human attempted to sound out names and despite coming close , they failed to get them correctly as we will see later .", "When translating unknown or unfamiliar names , one effective approach is to search for an English document that discusses the same subject and then extract the translations .", "For this example , we start by creating the following Web query that we use with the search engine Search Query 1 soldiers remains , search , North Korea , and US .", "This query returned many hits .", "The top document returned by the search engines we used contained the following paragraph The targeted area is near Unsan , which saw several battles between the U . S . Army s 8th Cavalry regiment and Chinese troops who launched a surprise offensive in late 1950 .", "This allowed us to create a more precise query by adding Unsan to the search terms Search Query 2 soldiers remains , search , North Korea , US , and Unsan .", "This search query returned only 3 documents .", "The first one is the above document .", "The third is the top level page for the second document .", "The second document contained the following excerpt Operations in 2001 will include areas of investigation near Kaechon , approximately 18 miles south of Unsan and Kujang .", "Kaechon includes an area nicknamed the Gauntlet , where the U . S . Army s 2nd Infantry Division conducted its famous fighting withdrawal along a narrow road through six miles of Chinese ambush positions during November and December 1950 .", "More than 950 missing in action soldiers are believed to be located in these three areas .", "The Chosin Reservoir campaign left approximately 750 Marines and soldiers missing in action from both the east and west sides of the reservoir in northeastern North Korea .", "This human translation method gives us the correct translation for the names we are interested in .", "Inspired by this , our goal is to tackle the named entity translation problem using the same approach described above , but fully automatically and using the least amount of hard to obtain bilingual resources .", "As shown in Figure 1 , the translation process in our system is carried out in two main steps .", "Given a named entity in the source language , our translation algorithm first generates a ranked list of translation candidates using bilingual and monolingual resources , which we describe in the Section 3 .", "Then , the list of candidates is re scored using different monolingual clues Section 4 .", "Named entity phrases can be identified fairly accurately e . g . , Bikel et al . 1999 report an FMEASURE of 94 . 9 .", "In addition to identifying phrase boundaries , named entity identifiers also provide the category and sub category of a phrase e . g . , ENTITY NAME , and PERSON .", "Different types of named entities are translated differently and hence our candidate generator has a specialized module for each type .", "Numerical and temporal expressions typically use a limited set of vocabulary words e . g . , names of months , days of the week , etc . and can be translated fairly easily using simple translation patterns .", "Therefore , we will not address them in this paper .", "Instead we will focus on person names , locations , and organizations .", "But before we present further details , we will discuss how words can be transliterated i . e . , sounded out , which is a crucial component of our named entity translation algorithm .", "Transliteration is the process of replacing words in the source language with their approximate phonetic or spelling equivalents in the target language .", "Transliteration between languages that use similar alphabets and sound systems is very simple .", "However , transliterating names from Arabic into English is a non trivial task , mainly due to the differences in their sound and writing systems .", "Vowels in Arabic come in two varieties long vowels and short vowels .", "Short vowels are rarely written in Arabic in newspaper text , which makes pronunciation and meaning highly ambiguous .", "Also , there is no oneto one correspondence between Arabic sounds and English sounds .", "For example , English P and B are both mapped into Arabic b ; Arabic h . and h into English H ; and so on .", "Stalls and Knight 1998 present an Arabic toEnglish back transliteration system based on the source channel framework .", "The transliteration process is based on a generative model of how an English name is transliterated into Arabic .", "It consists of several steps , each is defined as a probabilistic model represented as a finite state machine .", "First , an English word is generated according to its unigram probabilities .", "Then , the English word is pronounced with probability , which is collected directly from an English pronunciation dictionary .", "Finally , the English phoneme sequence is converted into Arabic writing with probability The transliterations proposed by this model are generally accurate .", "However , one serious limitation of this method is that only English words with known pronunciations can be produced .", "Also , human translators often transliterate words based on how they are spelled in the source language .", "For example , Graham is transliterated into Arabic as gr ah am and not as gr am .", "To address these limitations , we extend this approach by using a new spelling based model in addition to the phonetic based model .", "The spelling based model we propose described in detail in Al Onaizan and Knight , 2002 directly According to this model , the transliteration probability is given by the following equation maps English letter sequences into Arabic letter sequences with probability , which are trained on a small English Arabic name list without the need for English pronunciations .", "Since no pronunciations are needed , this list is easily obtainable for many language pairs .", "We also extend the model to include a letter trigram model in addition to the word unigram model .", "This makes it possible to generate words that are not already defined in the word unigram model .", "The transliteration score according to this model is given by 2 The phonetic based and spelling based models are combined into a single transliteration model .", "The transliteration score for an English word given an Arabic word is a linear combination of the phonetic based and the spelling based transliteration scores as follows Person names are almost always transliterated .", "The translation candidates for typical person names are generated using the transliteration module described above .", "Finite state devices produce a lattice containing all possible transliterations for a given name .", "The candidate list is created by extracting the n best transliterations for a given name .", "The score of each candidate in the list is the transliteration probability as given by Equation 3 .", "For example , the name klyntwn byl is transliterated into Bell Clinton , Bill Clinton , Bill Klington , etc .", "Words in organization and location names , on the other hand , are either translated e . g . , h z an as Reservoir or transliterated e . g . , t\u02c7swzyn as Chosin , and it is not clear when a word must be translated and when it must be transliterated .", "So to generate translation candidates for a given phrase , words in the phrase are first translated using a bilingual dictionary and they are also transliterated .", "Our candidate generator combines the dictionary entries and n best transliterations for each word in the given phrase into a regular expression that accepts all possible permutations of word translation transliteration combinations .", "In addition to the word transliterations and translations , English zero fertility words i . e . , words that might not have Arabic equivalents in the named entity phrase such as of and the are considered .", "This regular expression is then matched against a large English news corpus .", "All matches are then scored according to their individual word translation transliteration scores .", "The score for a given candidate is given by a modified IBM Model 1 probability Brown et al . , 1993 as follows where is the length of , is the length of , is a scaling factor based on the number of matches of found , and is the index of the English word aligned with according to alignment .", "The probability is a linear combination of the transliteration and translation score , where the translation score is a uniform probability over all dictionary entries for .", "The scored matches form the list of translation candidates .", "For example , the candidate list for h ly\u02c7g includes Bay ofPigs and Gulf ofPigs .", "Once a ranked list of translation candidates is generated for a given phrase , several monolingual English resources are used to help re rank the list .", "The candidates are re ranked according to the following equation 6 where is the re scoring factor used .", "Straight Web Counts Grefenstette , 1999 used phrase Web frequency to disambiguate possible English translations for German and Spanish compound nouns .", "We use normalized Web counts of named entity phrases as the first re scoring factor used to rescore translation candidates .", "For the klyntwn byl example , the top two translation candidates are Bell Clinton with transliteration score and Bill Clinton with score .", "The Web frequency counts of these two names are and respectively .", "This gives al h n azyr kyl \u02c7gwn . The transliteration module proposes Jon and John as possible transliterations for the first name , and Keele and Kyl among others for the last name .", "The normalized counts for the individual words are John , 0 . 9269 , Jon , 0 . 0688 , Keele , 0 . 0032 , and Kyl , 0 . 0011 .", "To use these normalized counts to score and rank the first name last name combinations in a way similar to a unigram language model , we would get the following name score pairs John Keele , 0 . 003 , John Kyl , 0 . 001 , Jon Keele , 0 . 0002 , and Jon Kyl , .", "However , the normalized phrase counts for the possible full names are Jon Kyl , 0 . 8976 , John Kyl , 0 . 0936 , John Keele , 0 . 0087 , and Jon Keele , 0 . 0001 , which is more desirable as Jon Kyl is an often mentioned US Senator .", "Co reference When a named entity is first mentioned in a news article , typically the full form of the phrase e . g . , the full name of a person is used .", "Later references to the name often use a shortened version of the name e . g , the last name of the person .", "Shortened versions are more ambiguous by nature than the full version of a phrase and hence more difficult to translate .", "Also , longer phrases tend to have more accurate Web counts than shorter ones as we have shown above .", "For example , the phrase alnw ab m\u02c7gls is translated as the House ofRepresentatives .", "The word al m\u02c7gls 2 might be used for later references to this phrase .", "In that case , we are confronted with the task of translating al m\u02c7gls which is ambiguous and could refer to a number of things including the Council when referring to al mn and as the Assembly when referring to al mt m\u02c7gls National Assembly .", "2 al m\u02c7gls is the same word as m\u02c7gls but with the definite article a attached .", "If we are able to determine that in fact it was referring to the House ofRepresentatives , then , we can translate it accurately as the House .", "This can be done by comparing the shortened phrase with the rest of the named entity phrases of the same type .", "If the shortened phrase is found to be a sub phrase of only one other phrase , then , we conclude that the shortened phrase is another reference to the same named entity .", "In that case we use the counts of the longer phrase to re rank the candidates of the shorter one .", "Contextual Web Counts In some cases straight Web counting does not help the re scoring .", "For example , the top two translation candidates for m arwn dwn ald are Donald Martin and Donald Marron .", "Their straight Web counts are 2992 and 2509 , respectively .", "These counts do not change the ranking of the candidates list .", "We next seek a more accurate counting method by counting phrases only if they appear within a certain context .", "Using search engines , this can be done using the boolean operator AND .", "For the previous example , we use Wall Street as the contextual information In this case we get the counts 15 and 113 for Donald Martin and Donald Marron , respectively .", "This is enough to get the correct translation as the top candidate .", "The challenge is to find the contextual information that provide the most accurate counts .", "We have experimented with several techniques to identify the contextual information automatically .", "Some of these techniques use document wide contextual information such as the title of the document or select key terms mentioned in the document .", "One way to identify those key terms is to use the tf . idfmeasure .", "Others use contextual information that is local to the named entity in question such as the words that precede and or succeed the named entity or other named entities mentioned closely to the one in question .", "The re scoring methods described above assume that the correct translation is in the candidates list .", "When it is not in the list , the re scoring will fail .", "To address this situation , we need to extrapolate from the candidate list .", "We do this by searching for the correct translation rather than generating it .", "We do that by using sub phrases from the candidates list us revised scores of and , respectively , which leads to the correct translation being ranked highest .", "It is important to consider counts for the full name rather than the individual words in the name to get accurate counts .", "To illustrate this point consider the person name m\u02c7gls the Security Council ; the House when referring to al nw ab m\u02c7gls the House ofRepresentatives ; or by searching for documents in the target language similar to the one being translated .", "For example , for a person name , instead of searching for the full name , we search for the first name and the last name separately .", "Then , we use the IdentiFinder named entity identifier Bikel et al . , 1999 to identify all named entities in the top retrieved documents for each sub phrase .", "All named entities of the type of the named entity in question e . g . , PERSON found in the retrieved documents and that contain the sub phrase used in the search are scored using our transliteration module and added to the list of translation candidates , and the re scoring is repeated .", "To illustrate this method , consider the name Coffee Annan , Coffee Engen , Coffee Anton , Coffee Anyone , and Covey Annan but not the correct translation Kofi Annan .", "We would like to find the most common person names that have either one of Coffee or Covey as a first name ; or Annan , Engen , Anton , or Anyone as a last name .", "One way to do this is to search using wild cards .", "Since we are not aware of any search engine that allows wild card Web search , we can perform a wild card search instead over our news corpus .", "The problem is that our news corpus is dated material , and it might not contain the information we are interested in .", "In this case , our news corpus , for example , might predate the appointment of Kofi Annan as the Secretary General of the UN .", "Alternatively , using a search engine , we retrieve the top matching documents for each of the names Coffee , Covey , Annan , Engen , Anton , and Anyone .", "All person names found in the retrieved documents that contain any of the first or last names we used in the search are added to the list of translation candidates .", "We hope that the correct translation is among the names found in the retrieved documents .", "The rescoring procedure is applied once more on the expanded candidates list .", "In this example , we add Kofi Annan to the candidate list , and it is subsequently ranked at the top .", "To address cases where neither the correct translation nor any of its sub phrases can be found in the list of translation candidates , we attempt to search for , instead of generating , translation candidates .", "This can be done by searching for a document in the target language that is similar to the one being translated from the source language .", "This is especially useful when translating named entities in news stories of international importance where the same event will most likely be reported in many languages including the target language .", "We currently do this by repeating the extrapolation procedure described above but this time using contextual information such as the title of the original document to find similar documents in the target language .", "Ideally , one would use a Cross Lingual IR system to find relevant documents more successfully .", "This section presents our evaluation results on the named entity translation task .", "We compare the translation results obtained from human translations , a commercial MT system , and our named entity translation system .", "The evaluation corpus consists of two different test sets , a development test set and a blind test set .", "The first set consists of 21 Arabic newspaper articles taken from the political affairs section of the daily newspaper Al Riyadh .", "Named entity phrases in these articles were hand tagged according to the MUC Chinchor , 1997 guidelines .", "They were then translated to English by a bilingual speaker a native speaker of Arabic given the text they appear in .", "The Arabic phrases were then paired with their English translations .", "The blind test set consists of 20 Arabic newspaper articles that were selected from the political section of the Arabic daily Al Hayat .", "The articles have already been translated into English by professional translators . 3 Named entity phrases in these articles were hand tagged , extracted , and paired with their English translations to create the blind test set .", "Table 1 shows the distribution of the named entity phrases into the three categories PERSON , ORGANIZATION , and LOCATION in the two data sets .", "The English translations in the two data sets were reviewed thoroughly to correct any wrong translations made by the original translators .", "For example , to find the correct translation of a politician s name , official government web pages were used to find the test sets into the categories PERSON , ORGANIZATION , and LOCATION .", "The numbers shown are the ratio of each category to the total . correct spelling .", "In cases where the translation could not be verified , the original translation provided by the human translator was considered the correct translation .", "The Arabic phrases and their correct translations constitute the gold standard translation for the two test sets .", "According to our evaluation criteria , only translations that match the gold standard are considered as correct .", "In some cases , this criterion is too rigid , as it will consider perfectly acceptable translations as incorrect .", "However , since we use it mainly to compare our results with those obtained from the human translations and the commercial system , this criterion is sufficient .", "The actual accuracy figures might be slightly higher than what we report here .", "In order to evaluate human performance at this task , we compared the translations by the original human translators with the correct translations on the goldstandard .", "The errors made by the original human translators turned out to be numerous , ranging from simple spelling errors e . g . , Custa Rica vs . Costa Rica to more serious errors such as transliteration errors e . g . , John Keele vs . Jon Kyl and other translation errors e . g . , Union Reserve Council vs . Federal Reserve Board .", "The Arabic documents were also translated using a commercial Arabic to English translation system . 4 The translation of the named entity phrases are then manually extracted from the translated text .", "When compared with the gold standard , nearly half of the phrases in the development test set and more than a third of the blind test were translated incorrectly by the commercial system .", "The errors can be classified into several categories including poor transliterations e . g . , Koln Baol vs . Colin Powell , translating a name instead of sounding it out e . g . , O Neill s urine vs . Paul O Neill , wrong translation e . g . , Joint Corners Organization vs . Joint Chiefs of Staff or wrong word order e . g . , the Church of the Orthodox Roman .", "Table 2 shows a detailed comparison of the translation accuracy between our system , the commercial system , and the human translators .", "The translations obtained by our system show significant improvement over the commercial system .", "In fact , in some cases it outperforms the human translator .", "When we consider the top 20 translations , our system s overall accuracy 84 is higher than the human s 75 . 3 on the blind test set .", "This means that there is a lot of room for improvement once we consider more effective re scoring methods .", "Also , the top 20 list in itself is often useful in providing phrasal translation candidates for general purpose statistical machine translation systems or other NLP systems .", "The strength of our translation system is in translating person names , which indicates the strength of our transliteration module .", "This might also be attributed to the low named entity coverage of our bilingual dictionary .", "In some cases , some words that need to be translated as opposed to transliterated are not found in our bilingual dictionary which may lead to incorrect location or organization translations but does not affect person names .", "The reason word translations are sometimes not found in the dictionary is not necessarily because of the spotty coverage of the dictionary but because of the way we access definitions in the dictionary .", "Only shallow morphological analysis e . g . , removing prefixes and suffixes is done before accessing the dictionary , whereas a full morphological analysis is necessary , especially for morphologically rich languages such as Arabic .", "Another reason for doing poorly on organizations is that acronyms and abbreviations in the Arabic text e . g . , w as , the Saudi Press Agency are currently not handled by our system .", "The blind test set was selected from the FBIS 2001 Multilingual Corpus .", "The FBIS data is collected by the Foreign Broadcast Information Service for the benefit of the US government .", "We suspect that the human translators who translated the documents into English are somewhat familiar with the genre of the articles and hence the named entities on the development and blind test sets .", "Only a match with the translation in the gold standard is considered a correct translation .", "The human translator results are obtained by comparing the translations provided by the original human translator with the translations in the gold standard .", "The Sakhr results are for the Web version of Sakhr s commercial system .", "The Top 1 results of our system considers whether the correct answer is the top candidate or not , while the Top 20 results considers whether the correct answer is among the top 20 candidates .", "Overall is a weighted average of the three named entity categories . tally .", "Straight Web Counts re score candidates based on their Web counts .", "Contextual Web Counts uses Web counts within a given context we used here title of the document as the contextual information .", "In Co reference , if the phrase to be translated is part of a longer phrase then we use the the ranking of the candidates for the longer phrase to re rank the candidates of the short one , otherwise we leave the list as is . that appear in the text .", "On the other hand , the development test set was randomly selected by us from our pool of Arabic articles and then submitted to the human translator .", "Therefore , the human translations in the blind set are generally more accurate than the human translations in the development test .", "Another reason might be the fact that the human translator who translated the development test is not a professional translator .", "The only exception to this trend is organizations .", "After reviewing the translations , we discovered that many of the organization translations provided by the human translator in the blind test set that were judged incorrect were acronyms or abbreviations for the full name of the organization e . g . , the INC instead of the Iraqi National Congress .", "As we described earlier in this paper , our translation system first generates a list of translation candidates , then re scores them using several re scoring methods .", "The list of translation candidates we used for these experiments are of size 20 .", "The re scoring methods are applied incrementally where the reranked list of one module is the input to the next module .", "Table 3 shows the translation accuracy after each of the methods we evaluated .", "The most effective re scoring method was the simplest , the straight Web counts .", "This is because re scoring methods are applied incrementally and straight Web counts was the first to be applied , and so it helps to resolve the easy cases , whereas the other methods are left with the more difficult cases .", "It would be interesting to see how rearranging the order in which the modules are applied might affect the overall accuracy of the system .", "The re scoring methods we used so far are in general most effective when applied to person name translation because corpus phrase counts are already being used by the candidate generator for producing candidates for locations and organizations , but not for persons .", "Also , the re scoring methods we used were initially developed and applied to person names .", "More effective re scoring methods are clearly needed especially for organization names .", "One method is to count phrases only if they are tagged by a named entity identifier with the same tag we are interested in .", "This way we can eliminate counting wrong translations such as enthusiasm when translating h . m as Hamas .", "We have presented a named entity translation algorithm that performs at near human translation accuracy when translating Arabic named entities to English .", "The algorithm uses very limited amount of hard to obtain bilingual resources and should be easily adaptable to other languages .", "We would like to apply to other languages such as Chinese and Japanese and to investigate whether the current algorithm would perform as well or whether new algorithms might be needed .", "Currently , our translation algorithm does not use any dictionary of named entities and they are translated on the fly .", "Translating a common name incorrectly has a significant effect on the translation accuracy .", "We would like to experiment with adding a small named entity translation dictionary for common names and see if this might improve the overall translation accuracy .", "This work was supported by DARPA ITO grant N66001 00 1 9814 ."], "summary_lines": ["Translating Named Entities Using Monolingual And Bilingual Resources\n", "Named entity phrases are some of the most difficult phrases to translate because new phrases can appear from nowhere, and because many are domain specific, not to be found in bilingual dictionaries.\n", "We present a novel algorithm for translating named entity phrases using easily obtainable monolingual and bilingual resources.\n", "We report on the application and evaluation of this algorithm in translating Arabic named entities to English.\n", "We also compare our results with the results obtained from human translations and a commercial system for the same task.\n", "We show that use of outside linguistic resources such as WWW counts of transliteration candidates can greatly boost transliteration accuracy.\n", "A spelling-based model is described that directly maps English letter sequences into Arabic letter sequences with associated probability that are trained on a small English/Arabic name list without the need for English pronunciations.\n", "The phonetics-based and spelling-based models have been linearly combined into a single transliteration model.\n", "We use Web statistics information to validate the translation candidates generated by language model, and obtained the accuracy of 72.6% in Arabic-English OOV word translation.\n"]}
{"article_lines": ["Articles Recognizing Contextual Polarity An Exploration of Features for Phrase Level Sentiment Analysis", "Many approaches to automatic sentiment analysis begin with a large lexicon of words marked with their prior polarity also called semantic orientation .", "However , the contextual polarity of the phrase in which a particular instance of a word appears may be quite different from the word s prior polarity .", "Positive words are used in phrases expressing negative sentiments , or vice versa .", "Also , quite often words that are positive or negative out of context are neutral in context , meaning they are not even being used to express a sentiment .", "The goal of this work is to automatically distinguish between prior and contextual polarity , with a focus on understanding which features are important for this task .", "Because an important aspect of the problem is identifying when polar terms are being used in neutral contexts , features for distinguishing between neutral and polar instances are evaluated , as well as features for distinguishing between positive and negative contextual polarity .", "The evaluation includes assessing the performance of features across multiple machine learning algorithms .", "For all learning algorithms except one , the combination of all features together gives the best performance .", "Another facet of the evaluation considers how the presence of neutral instances affects the performance offeatures for distinguishing between positive and negative polarity .", "These experiments show that the presence of neutral instances greatly degrades the performance of these features , and that perhaps the best way to improve performance across all polarity classes is to improve the system s ability to identify when an instance is neutral .", "Many approaches to automatic sentiment analysis begin with a large lexicon of words marked with their prior polarity also called semantic orientation .", "However , the contextual polarity of the phrase in which a particular instance of a word appears may be quite different from the word s prior polarity .", "Positive words are used in phrases expressing negative sentiments , or vice versa .", "Also , quite often words that are positive or negative out of context are neutral in context , meaning they are not even being used to express a sentiment .", "The goal of this work is to automatically distinguish between prior and contextual polarity , with a focus on understanding which features are important for this task .", "Because an important aspect of the problem is identifying when polar terms are being used in neutral contexts , features for distinguishing between neutral and polar instances are evaluated , as well as features for distinguishing between positive and negative contextual polarity .", "The evaluation includes assessing the performance of features across multiple machine learning algorithms .", "For all learning algorithms except one , the combination of all features together gives the best performance .", "Another facet of the evaluation considers how the presence of neutral instances affects the performance offeatures for distinguishing between positive and negative polarity .", "These experiments show that the presence of neutral instances greatly degrades the performance of these features , and that perhaps the best way to improve performance across all polarity classes is to improve the system s ability to identify when an instance is neutral .", "Sentiment analysis is a type of subjectivity analysis Wiebe 1994 that focuses on identifying positive and negative opinions , emotions , and evaluations expressed in natural language .", "It has been a central component in applications ranging from recognizing inflammatory messages Spertus 1997 , to tracking sentiments over time in online discussions Tong 2001 , to classifying positive and negative reviews Pang , Lee , and Vaithyanathan 2002 ; Turney 2002 .", "Although a great deal of work in sentiment analysis has targeted documents , applications such as opinion question answering Yu and Hatzivassiloglou 2003 ; Maybury 2004 ; Stoyanov , Cardie , and Wiebe 2005 and review mining to extract opinions about companies and products Morinaga et al . 2002 ; Nasukawa and Yi 2003 require sentence level or even phrase level analysis .", "For example , if a question answering system is to successfully answer questions about people s opinions , it must be able not only to pinpoint expressions of positive and negative sentiments , such as we find in sentence 1 , but also to determine when an opinion is not being expressed by a word or phrase that typically does evoke one , such as condemned in sentence 2 .", "A common approach to sentiment analysis is to use a lexicon with information about which words and phrases are positive and which are negative .", "This lexicon may be manually compiled , as is the case with the General Inquirer Stone et al . 1966 , a resource often used in sentiment analysis .", "Alternatively , the information in the lexicon may be acquired automatically .", "Acquiring the polarity of words and phrases is itself an active line of research in the sentiment analysis community , pioneered by the work of Hatzivassiloglou and McKeown 1997 on predicting the polarity or semantic orientation of adjectives .", "Various techniques have been proposed for learning the polarity of words .", "They include corpus based techniques , such as using constraints on the co occurrence in conjunctions of words with similar or opposite polarity Hatzivassiloglou and McKeown 1997 and statistical measures of word association Turney and Littman 2003 , as well as techniques that exploit information about lexical relationships Kamps and Marx 2002 ; Kim and Hovy 2004 and glosses Esuli and Sebastiani 2005 ; Andreevskaia and Bergler 2006 in resources such as WordNet .", "Acquiring the polarity of words and phrases is undeniably important , and there are still open research challenges , such as addressing the sentiments of different senses of words Esuli and Sebastiani 2006b ; Wiebe and Mihalcea 2006 , and so on .", "However , what the polarity of a given word or phrase is when it is used in a particular context is another problem entirely .", "Consider , for example , the underlined positive and negative words in the following sentence .", "The first underlined word is Trust .", "Although many senses of the word trust express a positive sentiment , in this case , the word is not being used to express a sentiment at all .", "It is simply part of an expression referring to an organization that has taken on the charge of caring for the environment .", "The adjective well is considered positive , and indeed it is positive in this context .", "However , the same is not true for the words reason and reasonable .", "Out of context , we would consider both of these words to be positive . 1 In context , the word reason is being negated , changing its polarity from positive to negative .", "The phrase no reason at all to believe changes the polarity of the proposition that follows ; because reasonable falls within this proposition , its polarity becomes negative .", "The word polluters has a negative connotation , but here in the context of the discussion of the article and its position in the sentence , polluters is being used less to express a sentiment and more to objectively refer to companies that pollute .", "To clarify how the polarity of polluters is affected by its subject role , consider the purely negative sentiment that emerges when it is used as an object They are polluters .", "We call the polarity that would be listed for a word in a lexicon the word s prior polarity , and we call the polarity of the expression in which a word appears , considering the context of the sentence and document , the word s contextual polarity .", "Although words often do have the same prior and contextual polarity , many times a word s prior and contextual polarities differ .", "Words with a positive prior polarity may have a negative contextual polarity , or vice versa .", "Quite often words that are positive or negative out of context are neutral in context , meaning that they are not even being used to express a sentiment .", "Similarly , words that are neutral out of context , neither positive or negative , may combine to create a positive or negative expression in context .", "The focus of this work is on the recognition of contextual polarity in particular , disambiguating the contextual polarity of words with positive or negative prior polarity .", "We begin by presenting an annotation scheme for marking sentiment expressions and their contextual polarity in the Multi perspective Question Answering MPQA opinion corpus .", "We show that , given a set of subjective expressions identified from the existing annotations in the MPQA corpus , contextual polarity can be annotated reliably .", "Using the contextual polarity annotations , we conduct experiments in automatically distinguishing between prior and contextual polarity .", "Beginning with a large lexicon of clues tagged with prior polarity , we identify the contextual polarity of the instances of those clues in the corpus .", "The process that we use has two steps , first classifying each clue as being in a neutral or polar phrase , and then disambiguating the contextual polarity of the clues marked as polar .", "For each step in the process , we experiment with a variety of features and evaluate the performance of the features using several different machine learning algorithms .", "Our experiments reveal a number of interesting findings .", "First , being able to accurately identify neutral contextual polarity when a positive or negative clue is not being used to express a sentiment is an important aspect of the problem .", "The importance of neutral examples has previously been noted for classifying the sentiment of documents Koppel and Schler 2006 , but ours is the first work to explore how neutral instances affect classifying the contextual polarity of words and phrases .", "In particular , we found that the performance of features for distinguishing between positive and negative polarity greatly degrades when neutral instances are included in the experiments .", "We also found that achieving the best performance for recognizing contextual polarity requires a wide variety of features .", "This is particularly true for distinguishing between neutral and polar instances .", "Although some features help to increase polar or neutral recall or precision , it is only the combination of features together that achieves significant improvements in accuracy over the baselines .", "Our experiments show that for distinguishing between positive and negative instances , features capturing negation are clearly the most important .", "However , there is more to the story than simple negation .", "Features that capture relationships between instances of clues also perform well , indicating that identifying features that represent more complex interdependencies between sentiment clues may be an important avenue for future research .", "The remainder of this article is organized as follows .", "Section 2 gives an overview of some of the things that can influence contextual polarity .", "In Section 3 , we describe our corpus and present our annotation scheme and inter annotator agreement study for marking contextual polarity .", "Sections 4 and 5 describe the lexicon used in our experiments and how the contextual polarity annotations are used to determine the gold standard tags for instances from the lexicon .", "In Section 6 , we consider what kind of performance can be expected from a simple , prior polarity classifier .", "Section 7 describes the features that we use for recognizing contextual polarity , and our experiments and results are presented in Section 8 .", "In Section 9 we discuss related work , and we conclude in Section 10 .", "Phrase level sentiment analysis is not a simple problem .", "Many things besides negation can influence contextual polarity , and even negation is not always straightforward .", "Negation may be local e . g . , not good , or involve longer distance dependencies such as the negation of the proposition e . g . , does not look very good or the negation of the subject e . g . , no one thinks that it s good .", "In addition , certain phrases that contain negation words intensify rather than change polarity e . g . , not only good but amazing .", "Contextual polarity may also be influenced by modality whether the proposition is asserted to be real realis or not real irrealis no reason at all to believe is irrealis , for example ; word sense e . g . , Environmental Trust vs .", "He has won the people s trust ; the syntactic role of a word in the sentence whether the word is the subject or object of a copular verb consider polluters are versus they are polluters ; and diminishers such as little e . g . , little truth , little threat .", "Polanyi and Zaenen 2004 give a detailed discussion of many of these types of polarity influencers .", "Many of these contextual polarity influencers are represented as features in our experiments .", "Contextual polarity may also be influenced by the domain or topic .", "For example , the word cool is positive if used to describe a car , but it is negative if it is used to describe someone s demeanor .", "Similarly , a word such as fever is unlikely to be expressing a sentiment when used in a medical context .", "We use one feature in our experiments to represent the topic of the document .", "Another important aspect of contextual polarity is the perspective of the person who is expressing the sentiment .", "For example , consider the phrase failed to defeat in the sentence Israel failed to defeat Hezbollah .", "From the perspective of Israel , failed to defeat is negative .", "From the perspective of Hezbollah , failed to defeat is positive .", "Therefore , the contextual polarity of this phrase ultimately depends on the perspective of who is expressing the sentiment .", "Although automatically detecting this kind of pragmatic influence on polarity is beyond the scope of this work , this as well as the other types of polarity influencers all are considered when annotating contextual polarity .", "For the experiments in this work , we need a corpus that is annotated comprehensively for sentiment expressions and their contextual polarity .", "Rather than building a corpus from scratch , we chose to add contextual polarity annotations to the existing annotations in the Multi perspective Question Answering MPQA opinion corpus2 Wiebe , Wilson , and Cardie 2005 .", "The MPQA corpus is a collection of English language versions of news documents from the world press .", "The documents contain detailed , expression level annotations of attributions and private states Quirk et al . 1985 .", "Private states are mental and emotional states ; they include beliefs , speculations , intentions , and sentiments , among others .", "Although sentiments are not distinguished from other types of private states in the existing annotations , they are a subset of what already is annotated .", "This makes the annotations in the MPQA corpus a good starting point for annotating sentiment expressions and their contextual polarity .", "When developing our annotation scheme for sentiment expressions and contextual polarity , there were three main questions to address .", "First , which of the existing annotations in the MPQA corpus have the possibility of being sentiment expressions ?", "Second , which of the possible sentiment expressions actually are expressing sentiments ?", "Third , what coding scheme should be used for marking contextual polarity ?", "The MPQA annotation scheme has four types of annotations objective speech event frames , two types of private state frames , and agent frames that are used for marking speakers of speech events and experiencers of private states .", "A full description of the MPQA annotation scheme and an agreement study evaluating key aspects of the scheme are found in Wiebe , Wilson , and Cardie 2005 .", "The two types of private state frames , direct subjective frames and expressive subjective element frames , are where we will find sentiment expressions .", "Direct subjective frames are used to mark direct references to private states as well as speech events in which private states are being expressed .", "For example , in the following sentences , fears , praised , and said are all marked as direct subjective annotations .", "The word fears directly refers to a private state ; praised refers to a speech event in which a private state is being expressed ; and said is marked as direct subjective because a private state is being expressed within the speech event referred to by said .", "Expressive subjective elements indirectly express private states through the way something is described or through a particular wording .", "In example 6 , the phrase full of absurdities is an expressive subjective element .", "Subjectivity Banfield 1982 ; Wiebe 1994 refers to the linguistic expression of private states , hence the names for the two types of private state annotations .", "All expressive subjective elements are included in the set of annotations that have the possibility of being sentiment expressions , but the direct subjective frames to include in this set can be pared down further .", "Direct subjective frames have an attribute , expression intensity , that captures the contribution of the annotated word or phrase to the overall intensity of the private state being expressed .", "Expression intensity ranges from neutral to high .", "In the given sentences , fears and praised have an expression intensity of medium , and said has an expression intensity of neutral .", "A neutral expression intensity indicates that the direct subjective phrase itself is not contributing to the expression of the private state .", "If this is the case , then the direct subjective phrase cannot be a sentiment expression .", "Thus , only direct subjective annotations with a non neutral expression intensity are included in the set of annotations that have the possibility of being sentiment expressions .", "We call this set of annotations , the union of the expressive subjective elements and the direct subjective frames with a non neutral intensity , the subjective expressions in the corpus ; these are the annotations we will mark for contextual polarity .", "Table 1 gives a sample of subjective expressions marked in the MPQA corpus .", "Although many of the words and phrases express what we typically think of as sentiments , others do not , for example , believes , very definitely , and unconditionally and without delay .", "Now that we have identified which annotations have the possibility of being sentiment expressions , the next question is which of these annotated words and phrases are actually expressing sentiments .", "We define a sentiment as a positive or negative emotion , evaluation , or stance .", "On the left of Table 2 are examples of positive sentiments ; examples of negative sentiments are on the right .", "Sample of subjective expressions from the MPQA corpus . victory of justice and freedom such a disadvantageous situation grown tremendously must such animosity not true at all throttling the voice imperative for harmonious society disdain and wrath glorious so exciting disastrous consequences could not have wished for a better situation believes freak show the embodiment of two sided justice if you re not with us , you re against us appalling vehemently denied very definitely everything good and nice once and for all under no circumstances shameful mum most fraudulent , terrorist and extremist enthusiastically asked number one democracy hate seems to think gross misstatement indulging in blood shed and their lunaticism surprised , to put it mildly take justice to pre historic times unconditionally and without delay so conservative that it makes Pat Buchanan look vegetarian those digging graves for others , get engraved themselves lost the reputation of commitment to principles of human justice ultimately the demon they have reared will eat up their own vitals The final issue to address is the actual annotation scheme for marking contextual polarity .", "The scheme we developed has four tags positive , negative , both , and neutral .", "The positive tag is used to mark positive sentiments .", "The negative tag is used to mark negative sentiments .", "The both tag is applied to expressions in which both a positive and negative sentiment are being expressed .", "Subjective expressions with positive , negative , or both tags are our sentiment expressions .", "The neutral tag is used for all other subjective expressions , including emotions , evaluations , and stances that are neither positive or negative .", "Instructions for the contextual polarity annotation scheme are available at http www . cs . pitt . edu mpqa databaserelease polarityCodingInstructions . txt .", "Following are examples from the corpus of each of the different contextual polarity annotations .", "Each underlined word or phrase is a subjective expression that was marked in the original MPQA annotations . 3 In bold following each subjective expression is the contextual polarity with which it was annotated .", "To measure the reliability of the polarity annotation scheme , we conducted an agreement study with two annotators4 using 10 documents from the MPQA corpus .", "The 10 documents contain 447 subjective expressions .", "Table 3 shows the contingency table for the two annotators judgments .", "Overall agreement is 82 , with a kappa value of 0 . 72 .", "As part of the annotation scheme , annotators are asked to judge how certain they are in their polarity tags .", "For 18 of the subjective expressions , at least one annotator used the uncertain tag when marking polarity .", "If we consider these cases to be borderline and exclude them from the study , percent agreement increases to 90 and kappa rises to 0 . 84 .", "Table 4 shows the revised contingency table with the uncertain cases removed .", "This shows that annotator agreement is especially high when both annotators are certain , and that annotators are certain for over 80 of their tags .", "Note that all annotations are included in the experiments .", "In total , all 19 , 962 subjective expressions in the 535 documents 11 , 112 sentences of the MPQA corpus were annotated with their contextual polarity as just described . 5 Three annotators carried out the task the two who participated in the annotation study and a third who was trained later . 6 Table 5 gives the distribution of the contextual polarity tags .", "Looking at this table , we see that a small majority of subjective expressions 54 . 6 are expressing a positive , negative , or both positive and negative sentiment .", "We refer to these expressions as polar in context .", "Many of the subjective expressions are neutral and do not express a sentiment .", "This suggests that , although sentiment is a major type of subjectivity , distinguishing other prominent types of subjectivity will be important for future work in subjectivity analysis .", "As many NLP applications operate at the sentence level , one important issue to consider is the distribution of sentences with respect to the subjective expressions they contain .", "In the 11 , 112 sentences in the MPQA corpus , 28 contain no subjective expressions , 24 contain only one , and 48 contain two or more .", "Of the 5 , 304 sentences containing two or more subjective expressions , 17 contain mixtures of positive and negative expressions , and 61 contain mixtures of polar positive negative both and neutral subjective expressions .", "For the experiments in this article , we use a lexicon of over 8 , 000 subjectivity clues .", "Subjectivity clues are words and phrases that may be used to express private states .", "In other words , subjectivity clues have subjective usages , though they may have objective usages as well .", "For this work , only single word clues are used .", "To compile the lexicon , we began with the list of subjectivity clues from Riloff and Wiebe 2003 , which includes the positive and negative adjectives from Hatzivassiloglou and McKeown 1997 .", "The words in this list were grouped in previous work according to their reliability as subjectivity clues .", "Words that are subjective in most contexts are considered strong subjective clues , indicated by the strongsubj tag .", "Words that may only have certain subjective usages are considered weak subjective clues , indicated by the weaksubj tag .", "We expanded the list using a dictionary and a thesaurus , and added words from the General Inquirer positive and negative word lists Stone et al . 1966 that we judged to be potentially subjective . 7 We also gave the new words strongsubj and weaksubj reliability tags .", "The final lexicon has a coverage of 67 of subjective expressions in the MPQA corpus , where coverage is the percentage of subjective expressions containing one or more instances of clues from the lexicon .", "The coverage of just sentiment expressions is even higher 75 .", "The next step was to tag the clues in the lexicon with their prior polarity positive , negative , both , or neutral .", "A word in the lexicon is tagged as positive if out of context it seems to evoke something positive , and negative if it seems to evoke something negative .", "If a word has both positive and negative meanings , it is tagged with the polarity that seems the most common .", "A word is tagged as both if it is at the same time both positive and negative .", "For example , the word bittersweet evokes something both positive and negative .", "Words like brag are also tagged as both , because the one who is bragging is expressing something positive , yet at the same time describing someone as bragging is expressing a negative evaluation of that person .", "A word is tagged as neutral if it does not evoke anything positive or negative .", "For words that came from positive and negative word lists Stone et al . 1966 ; Hatzivassiloglou and McKeown 1997 , we largely retained their original polarity .", "However , we did change the polarity of a word if we strongly disagreed with its original class . 8 For example , the word apocalypse is listed as positive in the General Inquirer ; we changed its prior polarity to negative for our lexicon .", "By far , the majority of clues in the lexicon 92 . 8 are marked as having either positive 33 . 1 or negative 59 . 7 prior polarity .", "Only a small number of clues 0 . 3 are marked as having both positive and negative polarity .", "We refer to the set of clues marked as positive , negative , or both as sentiment clues .", "A total of 6 . 9 of the clues in the lexicon are marked as neutral .", "Examples of neutral clues are verbs such as feel , look , and think , and intensifiers such as deeply , entirely , and practically .", "Although the neutral clues make up a small proportion of the total words in the lexicon , we retain them for our later experiments in recognizing contextual polarity because many of them are good clues that a sentiment is being expressed e . g . , feels slighted , feels satisfied , look kindly on , look forward to .", "Including them increases the coverage of the system .", "At the end of the previous section , we considered the distribution of sentences in the MPQA corpus with respect to the subjective expressions they contain .", "It is interesting to compare that distribution with the distribution of sentences with respect to the instances they contain of clues from the lexicon .", "We find that there are more sentences with two or more clue instances 62 than sentences with two or more subjective expressions 48 .", "More importantly , many more sentences have mixtures of positive and negative clue instances than actually have mixtures of positive and negative subjective expressions .", "Only 880 sentences have a mixture of both positive and negative subjective expressions , whereas 3 , 234 sentences have a mixture of positive and negative clue instances .", "Thus , a large number of positive and negative instances are either neutral in context , or they are combining to form more complex polarity expressions .", "Either way , this provides strong evidence of the need to be able to disambiguate the contextual polarity of subjectivity and sentiment clues .", "In the experiments described in the following sections , the goal is to classify the contextual polarity of the expressions that contain instances of the subjectivity clues in our lexicon .", "However , determining which clue instances are part of the same expression and identifying expression boundaries are not the focus of this work .", "Thus , instead of trying to identify and label each expression , in the following experiments , each clue instance is labeled individually as to its contextual polarity .", "We define the gold standard contextual polarity of a clue instance in terms of the manual annotations Section 3 as follows .", "If a clue instance is not in a subjective expression and therefore not in a sentiment expression , its gold class is neutral .", "If a clue instance appears in just one subjective expression or in multiple subjective expressions with the same contextual polarity , its gold class is the contextual polarity of the subjective expression s .", "If a clue instance appears in a mixture of negative and neutral subjective expressions , its gold class is negative ; if it is in a mixture of positive and neutral subjective expressions , its gold class is positive .", "Finally , if a clue instance appears in at least one positive and one negative subjective expression or in a subjective expression marked as both , then its gold class is both .", "A clue instance can appear in more than one subjective expression because in the MPQA annotation scheme , it is possible for direct subjective frames and expressive subjective elements frames to overlap .", "Before delving into the task of recognizing contextual polarity , an important question to address is how useful prior polarity alone is for identifying contextual polarity .", "To answer this question , we create a classifier that simply assumes the contextual polarity of a clue instance is the same as the clue s prior polarity .", "We explore this classifier s performance on a small amount of development data , which is not part of the data used in the subsequent experiments .", "This simple classifier has an accuracy of 48 .", "From the confusion matrix given in Table 6 , we see that 76 of the errors result from words with non neutral prior polarity appearing in phrases with neutral contextual polarity .", "Only 12 of the errors result from words with neutral prior polarity appearing in expressions with non neutral contextual polarity , and only 11 of the errors come from words with a positive or negative prior polarity appearing in expressions with the opposite contextual polarity .", "Table 6 also shows that positive clues tend to be used in negative expressions far more often than negative clues tend to be used in positive expressions .", "Given that by far the largest number of errors come from clues with positive , negative , or both prior polarity appearing in neutral contexts , we were motivated to try a two step approach to the problem of sentiment classification .", "The first step , Neutral Polar Classification , tries to determine if an instance is neutral or polar in context .", "The second step , Polarity Classification , takes all instances that step one classified as polar , and tries to disambiguate their contextual polarity .", "This two step approach is illustrated in Figure 1 .", "The features used in our experiments were motivated both by the literature and by exploration of the contextual polarity annotations in our development data .", "A number Two step approach to recognizing contextual polarity . of features were inspired by the paper on contextual polarity influencers by Polanyi and Zaenan 2004 .", "Other features are those that have been found useful in the past for recognizing subjective sentences Wiebe , Bruce , and O Hara 1999 ; Wiebe and Riloff 2005 .", "For distinguishing between neutral and polar instances , we use the features listed in Table 7 .", "For ease of description , we group the features into six sets word features , general modification features , polarity modification features , structure features , sentence features , and one document feature .", "Word Features In addition to the word token the token of the clue instance being classified , the word features include the parts of speech of the previous word , the word itself , and the next word .", "The prior polarity and reliability class features represent those pieces of information about the clue which are taken from the lexicon .", "General Modification Features These are binary features that capture different types of relationships involving the clue instance .", "The first four features involve relationships with the word immediately before or after the clue instance .", "The preceded by adjective feature is true if the clue instance is a noun preceded by an adjective .", "The preceded by adverb feature is true if the preceding word is an adverb other than not .", "The preceded by intensifier feature is true if the preceding word is an intensifier , and the self intensifier feature is true if the clue instance itself is an intensifier .", "A word is considered to be an intensifier if it appears in a list of intensifiers and if it precedes a word of the appropriate part of speech e . g . , an intensifier adjective must come before a noun .", "The list of intensifiers is a compilation of those listed in Quirk et al . 1985 , intensifiers identified from existing entries in the subjectivity lexicon , and intensifiers identified during explorations of the development data .", "The modifies modifed by features involve the dependency parse tree of the sentence , obtained by first parsing the sentence Collins 1997 and then converting the tree into its dependency representation Xia and Palmer 2001 .", "In a dependency representation , every node in the tree structure is a surface word i . e . , there are no abstract nodes such as NP or VP .", "The parent word is called the head , and its children are its modifiers .", "The edge between a parent and a child specifies the grammatical relationship between the two words .", "Figure 2 shows an example of a dependency parse tree .", "Instances of clues in the tree are marked with the clue s prior polarity and reliability class from the lexicon .", "For each clue instance , the modifies modifed by features capture whether there are adj , mod , or vmod relationships between the clue instance and any other instances from the lexicon .", "Specifically , the modifies strongsubj feature is true if the clue instance and its parent share an adj , mod , or vmod relationship , and if its parent is an instance of a strongsubj clue from the lexicon .", "The modifies weaksubj feature is the same , except that it looks in the parent for an instance of a weaksubj clue .", "The modified by strongsubj The dependency tree for the sentence The human rights report poses a substantial challenge to the U . S . interpretation of good and evil .", "Prior polarity and reliability class are marked in parentheses for words that match clues from the lexicon . feature is true for a clue instance if one of its children is an instance of a strongsubj clue , and if the clue instance and its child share an adj , mod , or vmod relationship .", "The modified by weaksubj feature is the same , except that it looks for instances of weaksubj clues in the children .", "Although the adj and vmod relationships are typically local , the mod relationship involves longer distance as well as local dependencies .", "Figure 2 helps to illustrate these features .", "The modifies weaksubj feature is true for substantial , because substantial modifies challenge , which is an instance of a weaksubj clue .", "For rights , the modifies weaksubj feature is false , because rights modifies report , which is not an instance of a weaksubj clue .", "The modified by weaksubj feature is false for substantial , because it has no modifiers that are instances of weaksubj clues .", "For challenge , the modified by weaksubj feature is true because it is being modified by substantial , which is an instance of a weaksubj clue .", "Polarity Modification Features The modifies polarity , modified by polarity , and conj polarity features capture specific relationships between the clue instance and other sentiment clues it may be related to .", "If the clue instance and its parent in the dependency tree share an obj , adj , mod , or vmod relationship , the modifies polarity feature is set to the prior polarity of the parent .", "If the parent is not in the prior polarity lexicon , its prior polarity is considered neutral .", "If the clue instance is at the root of the tree and has no parent , the value of the feature is notmod .", "The modified by polarity feature is similar , looking for adj , mod , and vmod relationships and other sentiment clues in the children of the clue instance .", "The conj polarity feature determines if the clue instance is in a conjunction .", "If so , the value of this feature is its sibling s prior polarity .", "As before , if the sibling is not in the Wilson , Wiebe , and Hoffmann Recognizing Contextual Polarity lexicon , its prior polarity is neutral .", "If the clue instance is not in a conjunction , the value for this feature is notmod .", "Figure 2 also helps to illustrate these modification features .", "The word substantial with positive prior polarity modifies the word challenge with negative prior polarity .", "Therefore the modifies polarity feature is negative for substantial , and the modified by polarity feature is positive for challenge .", "The words good and evil are in a conjunction together ; thus the conj polarity feature is negative for good and positive for evil .", "Structure Features These are binary features that are determined by starting with the clue instance and climbing up the dependency parse tree toward the root , looking for particular relationships , words , or patterns .", "The in subject feature is true if we find a subj relationship on the path to the root .", "The in copular feature is true if in subject is false and if a node along the path is both a main verb and a copular verb .", "The in passive feature is true if a passive verb pattern is found on the climb .", "The in subject and in copular features were motivated by the intuition that the syntactic role of a word may influence whether a word is being used to express a sentiment .", "For example , consider the word polluters in each of the following two sentences .", "In the first sentence , polluters is simply being used as a referring expression .", "In the second sentence , polluters is clearly being used to express a negative evaluation of the farmers .", "The motivation for the in passive feature was previous work by Riloff and Wiebe 2003 , who found that different words are more or less likely to be subjective depending on whether they are in the active or passive .", "Sentence Features These are features that previously were found useful for sentence level subjectivity classification Wiebe , Bruce , and O Hara 1999 ; Wiebe and Riloff 2005 .", "They include counts of strongsubj and weaksubj clue instances in the current , previous and next sentences , counts of adjectives and adverbs other than not in the current sentence , and binary features to indicate whether the sentence contains a pronoun , a cardinal number , and a modal other than will .", "Document Feature There is one document feature representing the topic or domain of the document .", "The motivation for this feature is that whether or not a word is expressing a sentiment or even a private state in general may depend on the subject of the discourse .", "For example , the words fever and sufferer may express a negative sentiment in certain contexts , but probably not in a health or medical context , as is the case in the following sentence .", "14 The disease can be contracted if a person is bitten by a certain tick or if a person comes into contact with the blood of a congo fever sufferer .", "In the creation of the MPQA corpus , about two thirds of the documents were selected to be on one of the 10 topics listed in Table 8 .", "The documents for each topic were identified by human searches and by an information retrieval system .", "The remaining documents were semi randomly selected from a very large pool of documents from the world press .", "In the corpus , these documents are listed with the topic miscellaneous .", "Rather than leaving these documents unlabeled , we chose to label them using the following general domain categories economics , general politics , health , report events , and war and terrorism .", "Table 9 lists the features that we use for step two , polarity classification .", "Word token , word prior polarity , and the polarity modification features are the same as described for neutral polar classification .", "We use two features to capture two different types of negation .", "The negated feature is a binary feature that is used to capture more local negations Its value is true if a negation word or phrase is found within the four words preceding the clue instance , and if the negation word is not also in a phrase that acts as an intensifier rather than a negator .", "Examples of phrases that intensify rather than negate are not only and nothing if not .", "The negated subject feature captures a longer distance type of negation .", "This feature is true if the subject of the clause containing the clue instance is negated .", "For example , the negated subject feature is true for support in the following sentence .", "15 No politically prudent Israeli could support either of them .", "The last three polarity features look in a window of four words before the clue instance , searching for the presence of particular types of polarity influencers .", "General polarity shifters reverse polarity e . g . , little truth , little threat .", "Negative polarity shifters typically make the polarity of an expression negative e . g . , lack of understanding .", "Positive polarity shifters typically make the polarity of an expression positive e . g . , abate the damage .", "The polarity influencers that we used were identified through explorations of the development data .", "We have two primary goals with our experiments in recognizing contextual polarity .", "The first is to evaluate the features described in Section 7 as to their usefulness for this task .", "The second is to investigate the importance of recognizing neutral instances recognizing when a sentiment clue is not being used to express a sentiment for classifying contextual polarity .", "To evaluate features , we investigate their performance , both together and separately , across several different learning algorithms .", "Varying the learning algorithm allows us to verify that the features are robust and that their performance is not the artifact of a particular algorithm .", "We experiment with four different types of machine learning boosting , memory based learning , rule learning , and support vector learning .", "For boosting , we use BoosTexter Schapire and Singer 2000 AdaBoost . MH .", "For rule learning , we use Ripper Cohen 1996 .", "For memory based learning , we use TiMBL Daelemans et al . 2003b IB1 k nearest neighbor .", "For support vector learning , we use SVM light and SVM multiclass Joachims 1999 .", "SVM light is used for the experiments involving binary classification neutral polar classification , and SVM multiclass is used for experiments with more than two classes .", "These machine learning algorithms were chosen because they have been used successfully for a number of natural language processing tasks , and they represent several different types of learning .", "For all of the classification algorithms except for SVM , the features for a clue instance are represented as they are presented in Section 7 .", "For SVM , the representations for numeric and discrete valued features are changed .", "Numeric features , such as the count of strongsubj clue instances in a sentence , are scaled to range between 0 and 1 .", "Discrete valued features , such as the reliability class feature , are converted into multiple binary features .", "For example , the reliability class feature is represented by two binary features one for whether the clue instance is a strongsubj clue and one for whether the clue instance is a weaksubj clue .", "To investigate the importance of recognizing neutral instances , we perform two sets of polarity classification step two experiments .", "First , we experiment with classifying the polarity of all gold standard polar instances the clue instances identified as polar in context by the manual polarity annotations .", "Second , we experiment with using the polar instances identified automatically by the neutral polar classifiers .", "Because the second set of experiments includes the neutral instances misclassified in step one , we can compare results for the two sets of experiments to see how the noise of neutral instances affects the performance of the polarity features .", "All experiments are performed using 10 fold cross validation over a test set of 10 , 287 sentences from 494 MPQA corpus documents .", "We measure performance in terms of accuracy , recall , precision , and F measure .", "Accuracy is simply the total number of instances correctly classified .", "Recall , precision , and F measure for a given class C are defined as follows .", "Recall is the percentage of all instances of class C correctly identified .", "all instances of C Precision is the percentage of instances classified as class C that are class C in truth .", "Prec C instances of C correctly identified all instances identified as C F measure is the harmonic mean of recall and precision .", "In our two step process for recognizing contextual polarity , the first step is neutral polar classification , determining whether each instance of a clue from the lexicon is neutral or polar in context .", "In our test set , there are 26 , 729 instances of clues from the lexicon .", "The features we use for this step were listed above in Table 7 and described in Section 7 . 1 .", "In this section , we perform two sets of experiments .", "In the first , we compare the results of neutral polar classification using all the neutral polar features against two baselines .", "The first baseline uses just the word token feature .", "The second baseline word priorpol uses the word token and prior polarity features .", "In the second set of experiments , we explore the performance of different sets of features for neutral polar classification .", "Research has shown that the performance of learning algorithms for NLP tasks can vary widely depending on their parameter settings , and that the optimal parameter settings can also vary depending on the set of features being evaluated Daelemans et al . 2003a ; Hoste 2005 .", "Although the goal of this work is not to identify the optimal configuration for each algorithm and each set of features , we still want to make a reasonable attempt to find a good configuration for each algorithm .", "To do this , we perform 10 fold cross validation of the more challenging baseline classifier word priorpol on the development data , varying select parameter settings .", "The results from those experiments are then used to select the parameter settings for each algorithm .", "For BoosTexter , we vary the number of rounds of boosting .", "For TiMBL , we vary the value for k the number of neighbors and the distance metric overlap or modified value difference metric MVDM .", "For Ripper , we vary whether negative tests are disallowed for nominal ! n and set ! s valued attributes and how much to simplify the hypothesis S .", "For SVM , we experiment with linear , polynomial , and radial basis function kernels .", "Table 10 gives the settings selected for the neutral polar classification experiments for the different learning algorithms .", "Table 11 .", "For each algorithm , we give the results for the two baseline classifiers , followed by the results for the classifier trained using all the neutral polar features .", "The results shown in bold are significantly better than both baselines two sided t test , p 0 . 05 for the given algorithm .", "Working together , how well do the neutral polar features perform ?", "For BoosTexter , TiMBL , and Ripper , the classifiers trained using all the features improve significantly over the two baselines in terms of accuracy , polar recall , polar F measure , and neutral precision .", "Neutral F measure is also higher , but not significantly so .", "These consistent results across three of the four algorithms show that the neutral polar features are helpful for determining when a sentiment clue is actually being used to express a sentiment .", "Interestingly , Ripper is the only algorithm for which the word token baseline performed better than the word priorpol baseline .", "Nevertheless , the prior polarity feature is an important component in the performance of the Ripper classifier using all the features .", "Excluding prior polarity from this classifier results in a significant decrease in performance for every metric .", "Decreases range from 2 . 5 for neutral recall to 9 . 5 for polar recall .", "The best SVM classifier is the word priorpol baseline .", "In terms of accuracy , this classifier does not perform much worse than the BoosTexter and TiMBL classifiers that use all the neutral polar features The SVM word priorpol baseline classifier has an accuracy of 75 . 6 , and both the BoosTexter and TiMBL classifiers have an accuracy of 76 . 5 .", "However , the BoosTexter and TiMBL classifiers using all the features perform notably better in terms of polar recall and F measure .", "The BoosTexter and TiMBL classifiers have polar recalls that are 7 and 9 . 2 higher than the SVM baseline .", "Polar F measures for BoosTexter and TiMBL are 3 . 9 and 4 . 5 higher .", "These increases are significant for p 0 . 01 .", "8 . 1 . 2 Feature Set Evaluation .", "To evaluate the contribution of the various features for neutral polar classification , we perform a series of experiments in which different sets of neutral polar features are added to the word priorpol baseline and new classifiers are trained .", "We then compare the performance of these new classifiers to the word priorpol baseline , with the exception of the Ripper classifiers , which we compare to the higher word baseline .", "Table 12 lists the sets of features tested in these experiments .", "The feature sets generally correspond to how the neutral polar features are presented in Table 7 , although some of the groups are broken down into more fine grained sets that we believe capture meaningful distinctions .", "Table 13 gives the results for these experiments .", "Increases and decreases for a given metric as compared to the word priorpol baseline word baseline for Ripper are indicated by or , respectively .", "Where changes are significant at the p 0 . 1 level , or are used , and where changes are significant at the p 0 . 05 level , or are used .", "An nc indicates no change a change of less than 0 . 05 compared to the baseline .", "What does Table 13 reveal about the performance of various feature sets for neutral polar classification ?", "Most noticeable is that no individual feature sets stand out as strong performers .", "The only significant improvements in accuracy come from the PARTSOF SPEECH and RELIABILITY CLASS feature sets for Ripper .", "These improvements are perhaps not surprising given that the Ripper baseline was much lower to begin with .", "Very few feature sets show any improvement for SVM .", "Again , this is not unexpected given that all the features together performed worse than the word priorpol baseline Increases and decreases for a given metric as compared to the word priorpol baseline word baseline for Ripper are indicated by or , respectively ; or indicates the change is significant at the p 0 . 1 level ; or indicates significance at the p 0 . 05 level ; nc indicates no change . for SVM .", "The performance of the feature sets for BoosTexter and TiMBL are perhaps the most revealing .", "In the previous experiments using all the features together , these algorithms produced classifiers with the same high performance .", "In these experiments , six different feature sets for each algorithm show improvements in accuracy over the baseline , yet none of those improvements are significant .", "This suggests that achieving the highest performance for neutral polar classification requires a wide variety of features working together in combination .", "We further test this result by evaluating the effect of removing the features that produced either no change or a drop in accuracy from the respective all feature classifiers .", "For example , we train a TiMBL neutral polar classifier using all the features except for those in the PRECEDED POS , INTENSIFY , STRUCTURE , CURSENT COUNTS , and TOPIC feature sets , and then compare the performance of this new classifier to the TiMBL , allfeature classifier .", "Although removing the non performing features has little effect for BoosTexter , performance does drop for both TiMBL and Ripper .", "The primary source of this performance drop is a decrease in polar recall 2 for TiMBL and 3 . 2 for Ripper .", "Although no feature sets stand out in Table 13 as far as giving an overall high performance , there are some features that consistently improve performance across the different algorithms .", "The reliability class of the clue instance RELIABILITY CLASS improves accuracy over the baseline for all four algorithms .", "It is the only feature that does so .", "The RELCLASS MOD features give improvements for all metrics for BoosTexter , Ripper , and TiMBL , as well as improving polar F measure for SVM .", "The PARTS OFSPEECH features are also fairly consistent , improving performance for all the algorithms except for SVM .", "There are also a couple of feature sets that consistently do not improve performance for any of the algorithms the INTENSIFY and PRECEDED POS features .", "For the second step of recognizing contextual polarity , we classify the polarity of all clue instances identified as polar in step one .", "The features for polarity classification were listed in Table 9 and described in Section 7 . 2 .", "We investigate the performance of the polarity features under two conditions 1 perfect neutral polar recognition and 2 automatic neutral polar recognition .", "For condition 1 , we identify the polar instances according to the gold standard , manual contextual polarity annotations .", "In the test data , 9 , 835 instances of the clues from the lexicon are polar in context according to the manual annotations .", "Experiments under condition 1 classify these instances as having positive , negative , or both positive and negative polarity .", "For condition 2 , we take the best performing neutral polar classifier for each algorithm and use the output from those algorithms to identify the polar instances .", "Because polar instances now are being identified automatically , there will be noise in the form of misclassified neutral instances .", "Therefore , for experiments under condition 2 we include the neutral class and perform four way classification instead of three way .", "Condition 1 allows us to investigate the performance of the different polarity features without the noise of misclassified neutral instances .", "Also , because the set of polar instances being classified is the same for all the algorithms , condition 1 allows us to compare the performance of the polarity features across the different algorithms .", "However , condition 2 is the more natural one .", "It allows us to see how the noise of neutral instances affects the performance of the polarity features .", "The following sections describe three sets of experiments .", "First , we investigate the performance of the polarity features used together for polarity classification under condition 1 .", "As before , the word and word priorpol classifiers provide our baselines .", "In the second set of experiments , we explore the performance of different sets of features for polarity classification , again assuming perfect recognition of the polar instances .", "Finally , we experiment with polarity classification using all the polarity features under condition 2 , automatic recognition of the polar instances .", "As before , we use the development data to select the parameter settings for each algorithm .", "The settings for polarity classification are given in Table 14 .", "They were selected based on the performance of the word priorpol baseline classifier under condition 2 .", "8 . 2 . 1 Classification Results Condition 1 .", "The results for polarity classification using all the polarity features , assuming perfect neutral polar recognition for step one , are given in Table 15 .", "For each algorithm , we give the results for the two baseline classifiers , followed by the results for the classifier trained using all the polarity features .", "For the metrics where the polarity features perform statistically better than both baselines two sided t test , p 0 . 05 , the results are given in bold .", "How well do the polarity features perform working all together ?", "For all algorithms , the polarity classifier using all the features significantly outperforms both baselines in terms of accuracy , positive F measure , and negative F measure .", "These consistent improvements in performance across all four algorithms show that these features are quite useful for polarity classification .", "One interesting thing that Table 15 reveals is that negative polarity words are much more straightforward to recognize than positive polarity words , at least in this corpus .", "For the negative class , precisions and recalls for the word priorpol baseline range from 82 . 2 to 87 . 2 .", "For the positive class , precisions and recalls for the word priorpol baseline range from 63 . 7 to 76 . 7 .", "However , it is with the positive class that polarity features seem to help the most .", "With the addition of the polarity features , positive F measure improves by 5 points on average ; improvements in negative F measures average only 2 . 75 points .", "8 . 2 . 2 Feature Set Evaluation .", "To evaluate the performance of the various features for polarity classification , we again perform a series of ablation experiments .", "As before , we start with the word priorpol baseline classifier , add different sets of polarity features , train new classifiers , and compare the results of the new classifiers to the baseline .", "Increases and decreases for a given metric as compared to the word priorpol baseline are indicated by or , respectively ; or indicates the change is significant at the p 0 . 1 level ; or indicates significance at the p 0 . 05 level .", "Table 16 lists the sets of features tested in each experiment , and Table 17 shows the results of the experiments .", "Results are reported as they were previously in Section 8 . 1 . 2 , with increases and decreases compared to the baseline for a given metric indicated by or , respectively .", "Looking at Table 17 , we see that all three sets of polarity features help to increase performance as measured by accuracy and positive and negative F measures .", "This is true for all the classification algorithms .", "As we might expect , including the negation features has the most marked effect on the performance of polarity classification , with statistically significant improvements for most metrics across all the algorithms . 9 The polarity modification features also seem to be important for polarity classification , in particular for disambiguating the positive instances .", "For all the algorithms except TiMBL , including the polarity modification features results in significant improvements for at least one of the positive metrics .", "The polarity shifters also help classification , but they seem to be the weakest of the features Including them does not result in significant improvements for any algorithm .", "Another question that is interesting to consider is how much the word token feature contributes to polarity classification , given all the other polarity features .", "Is it enough to know the prior polarity of a word , whether it is being negated , and how it is related to other polarity influencers ?", "To answer this question , we train classifiers using all the polarity features except for word token .", "Table 18 gives the results for these classifiers ; for comparison , the results for the all feature polarity classifiers are also given .", "Interestingly , excluding the word token feature produces only small changes in the overall results .", "The results for BoosTexter and Ripper are slightly lower , and the results for SVM are practically unchanged .", "TiMBL actually shows a slight improvement , with the exception of the both class .", "This provides further evidence of the strength of the polarity features .", "Also , a classifier not tied to actual word tokens may potentially be a more domain independent classifier .", "8 . 2 . 3 Classification Results Condition 2 .", "The experiments in Section 8 . 2 . 1 show that the polarity features perform well under the ideal condition of perfect recognition of polar instances .", "The next question to consider is how well the polarity features perform under the more natural but less than perfect condition of automatic recognition of polar instances .", "To investigate this , the polarity classifiers including the baselines for each algorithm in these experiments start with the polar instances identified by the best performing neutral polar classifier for that algorithm from Section 8 . 1 . 1 .", "The results for these experiments are given in Table 19 .", "As before , statistically significant improvements over both baselines are given in bold .", "How well do the polarity features perform in the presence of noise from misclassified neutral instances ?", "Our first observation comes from comparing Table 15 with Table 19 Polarity classification results are much lower for all classifiers with the noise of neutral instances .", "Yet in spite of this , the polarity features still produce classifiers that outperform the baselines .", "For three of the four algorithms , the classifier using all the polarity features has the highest accuracy .", "For BoosTexter and TiMBL , the improvements in accuracy over both baselines are significant .", "Also for all algorithms , using the polarity features gives the highest positive and negative F measures .", "Because the set of polarity instances being classified by each algorithm is different , we cannot directly compare the results from one algorithm to the next .", "Although the two step approach to recognizing contextual polarity allows us to focus our investigation on the performance of features for both neutral polar classification and polarity classification , the question remains How does the two step approach compare to recognizing contextual polarity in a single classification step ?", "The results shown in Table 20 help to answer this question .", "The first row in Table 20 for each algorithm shows the combined result for the two stages of classification .", "For BoosTexter , TiMBL , and Ripper , this is the combination of results from using all the neutral polar features for step one , together with the results from using all of the polarity features for step two . 10 For SVM , this is the combination of results from the word priorpol baseline from step one , together with results for using all the polarity features for step two .", "Recall that the word priorpol classifier was the best neutral polar classifier for SVM see Table 11 .", "The second rows for BoosTexter , TiMBL , and Ripper show the results of a single classifier trained to recognize contextual polarity using all the neutral polar and polarity features together .", "For SVM , the second row shows the results of classifying the contextual polarity using just the word token feature .", "This classifier outperformed all others for SVM .", "In the table , the best result for each metric for each algorithm is highlighted in bold .", "When comparing the two step and one step approaches , contrary to our expectations , we see that the one step approach performs about as well or better than the two step approach for recognizing contextual polarity .", "For SVM , the improvement in accuracy achieved by the two step approach is significant , but this is not true for the other algorithms .", "One fairly consistent difference between the two approaches is that the two step approach scores slightly higher for neutral F measure , and the onestep approach achieves higher F measures for the polarity classes .", "The difference in negative F measure is significant for BoosTexter , TiMBL , and Ripper .", "The exception to this is SVM .", "For SVM , the two step approach achieves significantly higher positive and negative F measures .", "One last question we consider is how much the neutral polar features contribute to the performance of the one step classifiers .", "The third line in Table 20 for BoosTexter , TiMBL , and Ripper gives the results for a one step classifier trained without the neutral polar features .", "Although the differences are not always large , excluding the neutral polar features consistently degrades performance in terms of accuracy and positive , negative , and neutral F measures .", "The drop in negative F measure is significant for all three algorithms , the drop in neutral F measure is significant for BoosTexter and TiMBL , and the drop in accuracy is significant for TiMBL and Ripper and for BoosTexter at the p 0 . 1 level .", "The modest drop in performance that we see when excluding the neutral polar features in the one step approach seems to suggest that discriminating between neutral and polar instances is helpful but not necessarily crucial .", "However , consider Figure 3 .", "In this figure , we show the F measures for the positive , negative , and both classes for the BoosTexter polarity classifier that uses the gold standard neutral polar instances from Table 15 and for the BoosTexter one step polarity classifier that uses all features from Table 20 .", "Plotting the same sets of results for the other three algorithms produces very similar figures .", "The difference when the classifiers have to contend with the noise from neutral instances is dramatic .", "Although Table 20 shows that there is room for improvement across all the contextual polarity classes , Figure 3 shows us that perhaps the best way to achieve these improvements is to improve the ability to discriminate the neutral class from the others .", "Other researchers who have worked on classifying the contextual polarity of sentiment expressions are Yi et al . 2003 , Popescu and Etzioni 2005 , and Suzuki , Takamura , and Okumura 2006 .", "Yi et al . use a lexicon and manually developed patterns to classify contextual polarity .", "Their patterns are high quality , yielding quite high precision over the set of expressions that they evaluate .", "Popescu and Etzioni use an unsupervised classification technique called relaxation labeling Hummel and Zucker 1983 to recognize the contextual polarity of words that are at the heads of select opinion phrases .", "They take an iterative approach , using relaxation labeling first to determine the contextual polarities of the words , then again to label the polarities of the words with respect to their targets .", "A third stage of relaxation labeling then is used to assign final polarities to the words , taking into consideration the presence of other polarity terms and negation .", "As we do , Popescu and Etzioni use features that represent conjunctions and dependency relations between polarity words .", "Suzuki et al . use a bootstrapping approach to classify the polarity of tuples of adjectives and their target nouns in Japanese blogs .", "Included in the features that they use are the words that modify the adjectives and the word that the adjective modifies .", "They consider the effect of a single negation term , the Japanese equivalent of not .", "Our work in recognizing contextual polarity differs from this research on expression level sentiment analysis in several ways .", "First , the set of expressions they evaluate is limited either to those that target specific items of interest , such as products and product features , or to tuples of adjectives and nouns .", "In contrast , we seek to classify the contextual polarity of all instances of words from a large lexicon of subjectivity clues that appear in the corpus .", "Included in the lexicon are not only adjectives , but nouns , verbs , adverbs , and even modals .", "Our work also differs from other research in the variety of features that we use .", "As other researchers do , we consider negation and the words that directly modify or are modified by the expression being classified .", "However , with negation , we have features for both local and longer distance types of negation , and we take care to count negation terms only when they are actually being used to negate , excluding , for example , negation terms when they are used in phrases that intensify e . g . , not only .", "We also include contextual features to capture the presence of other clue instances in the surrounding sentences , and features that represent the reliability of clues from the lexicon .", "Finally , a unique aspect of the work presented in this article is the evaluation of different features for recognizing contextual polarity .", "We first presented the features explored in this research in Wilson , Wiebe , and Hoffman 2005 , but this work significantly extends that initial evaluation .", "We explore the performance of features across different learning algorithms , and we evaluate not only features for discriminating between positive and negative polarity , but features for determining when a word is or is not expressing a sentiment in the first place neutral in context .", "This is also the first work to evaluate the effect of neutral instances on the performance of features for discriminating between positive and negative contextual polarity .", "Recognizing contextual polarity is just one facet of the research in automatic sentiment analysis .", "Research ranges from work on learning the prior polarity semantic orientation of words and phrases e . g . , Hatzivassiloglou and McKeown 1997 ; Kamps and Marx 2002 ; Turney and Littman 2003 ; Hu and Liu 2004 ; Kim and Hovy 2004 ; Esuli and Sebastiani 2005 ; Takamura , Inui , and Okumura 2005 ; Popescu and Etzioni 2005 ; Andreevskaia and Bergler 2006 ; Esuli and Sebastiani 2006a ; Kanayama and Nasukawa 2006 to characterizing the sentiment of documents , such as recognizing inflammatory messages Spertus 1997 , tracking sentiment over time in online discussions Tong 2001 , and classifying the sentiment of online messages e . g . , Das and Chen 2001 ; Koppel and Schler 2006 , customer feedback data Gamon 2004 , or product and movie reviews e . g . , Turney 2002 ; Pang , Lee , and Vaithyanathan 2002 ; Dave , Lawrence , and Pennock 2003 ; Beineke , Hastie , and Vaithyanathan 2004 ; Mullen and Collier 2004 ; Bai , Padman , and Airoldi 2005 ; Whitelaw , Garg , and Argamon 2005 ; Kennedy and Inkpen 2006 ; Koppel and Schler 2006 .", "Identifying prior polarity is a different task than recognizing contextual polarity , although the two tasks are complementary .", "The goal of identifying prior polarity is to automatically acquire the polarity of words or phrases for listing in a lexicon .", "Our work on recognizing contextual polarity begins with a lexicon of words with established prior polarities and then disambiguates in the corpus the polarity being expressed by the phrases in which instances of those words appear .", "To make the relationship between that task and ours clearer , some word lists that are used to evaluate methods for recognizing prior polarity positive and negative word lists from the General Inquirer Stone et al . 1966 and lists of positive and negative adjectives created for evaluation by Hatzivassiloglou and McKeown 1997 are included in the prior polarity lexicon used in our experiments .", "For the most part , the features explored in this work differ from the ones used to identify prior polarity with just a few exceptions .", "Using a feature to capture conjunctions between clue instances was motivated in part by the work of Hatzivassiloglou and McKeown 1997 .", "They use constraints on the co occurrence in conjunctions of words with similar or opposite polarity to predict the prior polarity of adjectives .", "Esuli and Sebastiani 2005 consider negation in some of their experiments involving WordNet glosses .", "Takamura et al . 2005 use negation words and phrases , including phrases such as lack of that are members in our lists of polarity shifters , and conjunctive expressions that they collect from corpora .", "Esuli and Sebastiani 2006a is the only work in prior polarity identification to include a neutral objective category and to consider a three way classification between positive , negative , and neutral words .", "Although identifying prior polarity is a different task , they report a finding similar to ours , namely , that accuracy is lower when neutral words are included .", "Some research in sentiment analysis classifies the sentiments of sentences .", "Morinaga et al . 2002 , Yu and Hatzivassiloglou 2003 , Kim and Hovy 2004 , Hu and Liu 2004 , and Grefenstette et al .", "2004 11 all begin by first creating prior polarity lexicons .", "Yu and Hatzivassiloglou then assign a sentiment to a sentence by averaging the prior semantic orientations of instances of lexicon words in the sentence .", "Thus , they do not identify the contextual polarity of individual phrases containing clue instances , which is the focus of this work .", "Morinaga et al . only consider the positive or negative clue instance in each sentence that is closest to some target reference ; Kim and Hovy , Hu and Liu , and Grefenstette et al . multiply or count the prior polarities of clue instances in the sentence .", "These researchers also consider local negation to reverse polarity , with Morinaga et al . also taking into account the negating effect of words like insufficient .", "However , they do not use the other types of features that we consider in our experiments .", "Kaji and Kitsuregawa 2006 take a different approach to recognizing positive and negative sentences .", "They bootstrap from information easily obtained in Pro and Con HTML tables and lists , and from one high precision linguistic pattern , to automatically construct a large corpus of positive and negative sentences .", "They then use this corpus to train a naive Bayes sentence classifier .", "In contrast to our work , sentiment classification in all of this research is restricted to identifying only positive and negative sentences excluding our both and neutral categories .", "In addition , only one sentiment is assigned per sentence ; our system assigns contextual polarity to individual expressions , which would allow for a sentence to be assigned to multiple sentiment categories .", "As we saw when exploring the contextual polarity annotations , it is not uncommon for sentences to contain more than one sentiment expression .", "Classifying the sentiment of documents is a very different task than recognizing the contextual polarity of words and phrases .", "However , some researchers have reported findings about document level classification that are similar to our findings about phrase level classification .", "Bai et al . 2005 argue that dependencies among key sentiment terms are important for classifying document sentiment .", "Similarly , we show that features for capturing when clue instances modify each other are important for phrase level classification , in particular , for identifying positive expressions .", "Gamon 2004 achieves his best results for document classification using a wide variety of features , including rich linguistic features , such as features that capture constituent structure , features that combine part of speech and semantic relations e . g . , sentence subject or negated context , and features that capture tense information .", "We also achieve our best results for phrase level classification using a wide variety of features , many of which are linguistically rich .", "Kennedy and Inkpen 2006 report consistently higher results for document sentiment classification when select polarity influencers , including negators and intensifiers , are included . 12 Koppel and Schler 2006 demonstrate the importance of neutral examples for document level classification .", "In this work , we show that being able to correctly identify neutral instances is also very important for phraselevel sentiment analysis .", "Being able to determine automatically the contextual polarity of words and phrases is an important problem in sentiment analysis .", "In the research presented in this article , we tackle this problem and show that it is much more complex than simply determining whether a word or phrase is positive or negative .", "In our analysis of a corpus with annotations of subjective expressions and their contextual polarity , we find that positive and negative words from a lexicon are used in neutral contexts much more often than they are used in expressions of the opposite polarity .", "The importance of identifying when contextual polarity is neutral is further revealed in our classification experiments When neutral instances are excluded , the performance of features for distinguishing between positive and negative polarity greatly improves .", "A focus of this research is on understanding which features are important for recognizing contextual polarity .", "We experiment with a wide variety of linguistically motivated features , and we evaluate the performance of these features using several different machine learning algorithms .", "Features for distinguishing between neutral and polar instances are evaluated , as well as features for distinguishing between positive and negative contextual polarity .", "For classifying neutral and polar instances , we find that , although some features produce significant improvements over the baseline in terms of polar or neutral recall or precision , it is the combination of features together that is needed to achieve significant improvements in accuracy .", "For classifying positive and negative contextual polarity , features for capturing negation prove to be the most important .", "However , we find that features that also perform well are those that capture when a word is or is not modifying or being modified by other polarity terms .", "This suggests that identifying features that represent more complex interdependencies between polarity clues will be an important avenue for future research .", "Another direction for future work will be to expand our lexicon using existing techniques for acquiring the prior polarity of words and phrases .", "It follows that a larger lexicon will have a greater coverage of sentiment expressions .", "However , expanding the lexicon with automatically acquired prior polarity tags may result in an even greater proportion of neutral instances to contend with .", "Given the degradation in performance created by the neutral instances , whether expanding the lexicon automatically will result in improved performance for recognizing contextual polarity is an empirical question .", "Finally , the overall goal of our research is to use phrase level sentiment analysis in higher level NLP tasks , such as opinion question answering and summarization .", "We would like to thank the anonymous reviewers for their valuable comments and suggestions .", "This work was supported in part by an Andrew Mellow Predoctoral Fellowship , by the NSF under grant IIS 0208798 , by the Advanced Research and Development Activity ARDA , and by the European IST Programme through the AMIDA Integrated Project FP6 0033812 ."], "summary_lines": ["Articles: Recognizing Contextual Polarity: An Exploration of Features for Phrase-Level Sentiment Analysis\n", "Many approaches to automatic sentiment analysis begin with a large lexicon of words marked with their prior polarity (also called semantic orientation).\n", "However, the contextual polarity of the phrase in which a particular instance of a word appears may be quite different from the word\u2019s prior polarity.\n", "Positive words are used in phrases expressing negative sentiments, or vice versa.\n", "Also, quite often words that are positive or negative out of context are neutral in context, meaning they are not even being used to express a sentiment.\n", "The goal of this work is to automatically distinguish between prior and contextual polarity, with a focus on understanding which features are important for this task.\n", "Because an important aspect of the problem is identifying when polar terms are being used in neutral contexts, features for distinguishing between neutral and polar instances are evaluated, as well as features for distinguishing between positive and negative contextual polarity.\n", "The evaluation includes assessing the performance of features across multiple machine learning algorithms.\n", "For all learning algorithms except one, the combination of all features together gives the best performance.\n", "Another facet of the evaluation considers how the presence of neutral instances affects the performance of features for distinguishing between positive and negative polarity.\n", "These experiments show that the presence of neutral instances greatly degrades the performance of these features, and that perhaps the best way to improve performance across all polarity classes is to improve the system\u2019s ability to identify when an instance is neutral.\n", "We explore the difference between prior and contextual polarity: words that lose polarity in context, or whose polarity is reversed because of context.\n"]}
{"article_lines": ["A Bootstrapping Method For Learning Semantic Lexicons Using Extraction Pattern Contexts", "This paper describes a bootstrapping algorithm called Basilisk that learns highquality semantic lexicons for multiple categories .", "Basilisk begins with an unannotated corpus and seed words for each semantic category , which are then bootstrapped to learn new words for each category .", "Basilisk hypothesizes the semantic class of a word based on collective information over a large body of extraction pattern contexts .", "We evaluate Basilisk on six semantic categories .", "The semantic lexicons produced by Basilisk have higher precision than those produced by previous techniques , with several categories showing substantial improvement .", "In recent years , several algorithms have been developed to acquire semantic lexicons automatically or semi automatically using corpus based techniques .", "For our purposes , the term semantic lexicon will refer to a dictionary of words labeled with semantic classes e . g . , bird is an ANIMAL and truck is a VEHICLE .", "Semantic class information has proven to be useful for many natural language processing tasks , including information extraction Riloff and Schmelzenbach , 1998 ; Soderland et al . , 1995 , anaphora resolution Aone and Bennett , 1996 , question answering Moldovan et al . , 1999 ; Hirschman et al . , 1999 , and prepositional phrase attachment Brill and Resnik , 1994 .", "Although some semantic dictionaries do exist e . g . , WordNet Miller , 1990 , these resources often do not contain the specialized vocabulary and jargon that is needed for specific domains .", "Even for relatively general texts , such as the Wall Street Journal Marcus et al . , 1993 or terrorism articles MUC4 Proceedings , 1992 , Roark and Charniak Roark and Charniak , 1998 reported that 3 of every 5 terms generated by their semantic lexicon learner were not present in WordNet .", "These results suggest that automatic semantic lexicon acquisition could be used to enhance existing resources such as WordNet , or to produce semantic lexicons for specialized domains .", "We have developed a weakly supervised bootstrapping algorithm called Basilisk that automatically generates semantic lexicons .", "Basilisk hypothesizes the semantic class of a word by gathering collective evidence about semantic associations from extraction pattern contexts .", "Basilisk also learns multiple semantic classes simultaneously , which helps constrain the bootstrapping process .", "First , we present Basilisk s bootstrapping algorithm and explain how it differs from previous work on semantic lexicon induction .", "Second , we present empirical results showing that Basilisk outperforms a previous algorithm .", "Third , we explore the idea of learning multiple semantic categories simultaneously by adding this capability to Basilisk as well as another bootstrapping algorithm .", "Finally , we present results showing that learning multiple semantic categories simultaneously improves performance .", "Basilisk Bootstrapping Approach to SemantIc Lexicon Induction using Semantic Knowledge is a weakly supervised bootstrapping algorithm that automatically generates semantic lexicons .", "Figure 1 shows the high level view of Basilisk s bootstrapping process .", "The input to Basilisk is an unannotated text corpus and a few manually defined seed words for each semantic category .", "Before bootstrapping begins , we run an extraction pattern learner over the corpus which generates patterns to extract every noun phrase in the corpus .", "The bootstrapping process begins by selecting a subset of the extraction patterns that tend to extract the seed words .", "We call this the pattern pool .", "The nouns extracted by these patterns become candidates for the lexicon and are placed in a candidate word pool .", "Basilisk scores each candidate word by gathering all patterns that extract it and measuring how strongly those contexts are associated with words that belong to the semantic category .", "The five best candidate words are added to the lexicon , and the process starts over again .", "In this section , we describe Basilisk s bootstrapping algorithm in more detail and discuss related work .", "The input to Basilisk is a text corpus and a set of seed words .", "We generated seed words by sorting the words in the corpus by frequency and manually identifying the 10 most frequent nouns that belong to each category .", "These seed words form the initial semantic lexicon .", "In this section we describe the learning process for a single semantic category .", "In Section 3 we will explain how the process is adapted to handle multiple categories simultaneously .", "To identify new lexicon entries , Basilisk relies on extraction patterns to provide contextual evidence that a word belongs to a semantic class .", "As our representation for extraction patterns , we used the AutoSlog system Riloff , 1996 .", "AutoSlog s extraction patterns represent linguistic expressions that extract a noun phrase in one of three syntactic roles subject , direct object , or prepositional phrase object .", "For example , three patterns that would extract people are subject was arrested , murdered direct object , and collaborated with pp object .", "Extraction patterns represent linguistic contexts that often reveal the meaning of a word by virtue of syntax and lexical semantics .", "Extraction patterns are typically designed to capture role relationships .", "For example , consider the verb robbed when it occurs in the active voice .", "The subject of robbed identifies the perpetrator , while the direct object of robbed identifies the victim or target .", "Before bootstrapping begins , we run AutoSlog exhaustively over the corpus to generate an extraction Generate all extraction patterns in the corpus and record their extractions . pattern for every noun phrase that appears .", "The patterns are then applied to the corpus and all of their extracted noun phrases are recorded .", "Figure 2 shows the bootstrapping process that follows , which we explain in the following sections .", "The first step in the bootstrapping process is to score the extraction patterns based on their tendency to extract known category members .", "All words that are currently defined in the semantic lexicon are considered to be category members .", "Basilisk scores each pattern using the RlogF metric that has been used for extraction pattern learning Riloff , 1996 .", "The score for each pattern is computed as where Fi is the number of category members extracted by patterni and Ni is the total number of nouns extracted by patterni .", "Intuitively , the RlogF metric is a weighted conditional probability ; a pattern receives a high score if a high percentage of its extractions are category members , or if a moderate percentage of its extractions are category members and it extracts a lot of them .", "The top N extraction patterns are put into a pattern pool .", "Basilisk uses a value of N 20 for the first iteration , which allows a variety of patterns to be considered , yet is small enough that all of the patterns are strongly associated with the category . 1 The purpose of the pattern pool is to narrow down the field of candidates for the lexicon .", "Basilisk collects all noun phrases NPs extracted by patterns in the pattern pool and puts the head noun of each NP into the candidate word pool .", "Only these nouns are considered for addition to the lexicon .", "As the bootstrapping progresses , using the same value N 20 causes the candidate pool to become stagnant .", "For example , let s assume that Basilisk performs perfectly , adding only valid category words to the lexicon .", "After some number of iterations , all of the valid category members extracted by the top 20 patterns will have been added to the lexicon , leaving only non category words left to consider .", "For this reason , the pattern pool needs to be infused with new patterns so that more nouns extractions become available for consideration .", "To achieve this effect , we increment the value of N by one after each bootstrapping iteration .", "This ensures that there is always at least one new pattern contributing words to the candidate word pool on each successive iteration .", "The next step is to score the candidate words .", "For each word , Basilisk collects every pattern that extracted the word .", "All extraction patterns are used during this step , not just the patterns in the pattern pool .", "Initially , we used a scoring function that computes the average number of category members extracted by the patterns .", "The formula is where Pi is the number of patterns that extract wordi , and Fj is the number of distinct category members extracted by pattern j .", "A word receives a high score if it is extracted by patterns that also have a tendency to extract known category members .", "As an example , suppose the word Peru is in the candidate word pool as a possible location .", "Basilisk finds all patterns that extract Peru and computes the average number of known locations extracted by those patterns .", "Let s assume that the three patterns shown below extract Peru and that the underlined words are known locations .", "Peru would receive a score of 2 3 2 3 2 . 3 .", "Intuitively , this means that patterns that extract Peru also extract , on average , 2 . 3 known location words .", "Unfortunately , this scoring function has a problem .", "The average can be heavily skewed by one pattern that extracts a large number of category members .", "For example , suppose word w is extracted by 10 patterns , 9 which do not extract any category members but the tenth extracts 50 category members .", "The average number of category members extracted by these patterns will be 5 .", "This is misleading because the only evidence linking word w with the semantic category is a single , high frequency extraction pattern which may extract words that belong to other categories as well .", "To alleviate this problem , we modified the scoring function to compute the average logarithm of the number of category members extracted by each pattern .", "The logarithm reduces the influence of any single pattern .", "We will refer to this scoring metric as the AvgLog function , which is defined below .", "Since log2 1 0 , we add one to each frequency count so that patterns which extract a single category member contribute a positive value .", "Using this scoring metric , all words in the candidate word pool are scored and the top five words are added to the semantic lexicon .", "The pattern pool and the candidate word pool are then emptied , and the bootstrapping process starts over again .", "Several weakly supervised learning algorithms have previously been developed to generate semantic lexicons from text corpora .", "Riloff and Shepherd Riloff and Shepherd , 1997 developed a bootstrapping algorithm that exploits lexical co occurrence statistics , and Roark and Charniak Roark and Charniak , 1998 refined this algorithm to focus more explicitly on certain syntactic structures .", "Hale , Ge , and Charniak Ge et al . , 1998 devised a technique to learn the gender of words .", "Caraballo Caraballo , 1999 and Hearst Hearst , 1992 created techniques to learn hypernym hyponym relationships .", "None of these previous algorithms used extraction patterns or similar contexts to infer semantic class associations .", "Several learning algorithms have also been developed for named entity recognition e . g . , Collins and Singer , 1999 ; Cucerzan and Yarowsky , 1999 .", "Collins and Singer , 1999 used contextual information of a different sort than we do .", "Furthermore , our research aims to learn general nouns e . g . , artist rather than proper nouns , so many of the features commonly used to great advantage for named entity recognition e . g . , capitalization and title words are not applicable to our task .", "The algorithm most closely related to Basilisk is meta bootstrapping Riloff and Jones , 1999 , which also uses extraction pattern contexts for semantic lexicon induction .", "Meta bootstrapping identifies a single extraction pattern that is highly correlated with a semantic category and then assumes that all of its extracted noun phrases belong to the same category .", "However , this assumption is often violated , which allows incorrect terms to enter the lexicon .", "Riloff and Jones acknowledged this issue and used a second level of bootstrapping the Meta bootstrapping level to alleviate this problem .", "While meta bootstrapping trusts individual extraction patterns to make unilateral decisions , Basilisk gathers collective evidence from a large set of extraction patterns .", "As we will demonstrate in Section 2 . 2 , Basilisk s approach produces better results than meta bootstrapping and is also considerably more efficient because it uses only a single bootstrapping loop meta bootstrapping uses nested bootstrapping .", "However , meta bootstrapping produces category specific extraction patterns in addition to a semantic lexicon , while Basilisk focuses exclusively on semantic lexicon induction .", "To evaluate Basilisk s performance , we ran experiments with the MUC 4 corpus MUC 4 Proceedings , 1992 , which contains 1700 texts associated with terrorism .", "We used Basilisk to learn semantic lexicons for six semantic categories BUILDING , EVENT , HUMAN , LOCATION , TIME , and WEAPON .", "Before we ran these experiments , one of the authors manually labeled every head noun in the corpus that was found by an extraction pattern .", "These manual annotations were the gold standard .", "Table 1 shows the breakdown of semantic categories for the head nouns .", "These numbers represent a baseline an algorithm that randomly selects words would be expected to get accuracies consistent with these numbers .", "Three semantic lexicon learners have previously been evaluated on the MUC 4 corpus Riloff and Shepherd , 1997 ; Roark and Charniak , 1998 ; Riloff and Jones , 1999 , and of these meta bootstrapping achieved the best results .", "So we implemented the meta bootstrapping algorithm ourselves to directly compare its performance with that of Basilisk .", "A difference between the original implementation and ours is that our version learns individual nouns as does Basilisk instead of noun phrases .", "We believe that learning individual nouns is a more conservative approach because noun phrases often overlap e . g . , high power bombs and incendiary bombs would count as two different lexicon entries in the original meta bootstrapping algorithm .", "Consequently , our meta bootstrapping results differ from those reported in Riloff and Jones , 1999 .", "Figure 3 shows the results for Basilisk ba 1 and meta bootstrapping mb 1 .", "We ran both algorithms for 200 iterations , so that 1000 words were added to the lexicon 5 words per iteration .", "The X axis shows the number of words learned , and the Y axis shows how many were correct .", "The Y axes have different ranges because some categories are more prolific than others .", "Basilisk outperforms meta bootstrapping for every category , often substantially .", "For the human and location categories , Basilisk learned hundreds of words , with accuracies in the 80 89 range through much of the bootstrapping .", "It is worth noting that Basilisk s performance held up well on the human and location categories even at the end , achieving 79 . 5 795 1000 accuracy for humans and 53 . 2 532 1000 accuracy for locations .", "We also explored the idea of bootstrapping multiple semantic classes simultaneously .", "Our hypothesis was that errors of confusion2 between semantic categories can be lessened by using information about multiple categories .", "This hypothesis makes sense only if a word cannot belong to more than one semantic class .", "In general , this is not true because words are often polysemous .", "But within a limited domain , a word usually has a dominant word sense .", "Therefore we make a one sense per domain assumption similar Figure 4 illustrates what happens when a semantic lexicon is generated for a single category .", "The seed words for the category in this case , category C are represented by the solid black area in category C s territory .", "The hypothesized words in the growing lexicon are represented by a shaded area .", "The goal of the bootstrapping algorithm is to expand the area of hypothesized words so that it exactly matches the category s true territory .", "If the shaded area expands beyond the category s true territory , then incorrect words have been added to the lexicon .", "In Figure 4 , category C has claimed a significant number of words that belong to categories B and E . When generating a lexicon for one category at a time , these confusion errors are impossible to detect because the learner has no knowledge of the other categories .", "Figure 5 shows the same search space when lexicons are generated for six categories simultaneously .", "If the lexicons cannot overlap , then we constrain the ability of a category to overstep its bounds .", "Category C is stopped when it begins to encroach upon the territories of categories B and E because words in those areas have already been claimed .", "The easiest way to take advantage of multiple categories is to add simple conflict resolution that enforces the one sense per domain constraint .", "If more than one category tries to claim a word , then we use conflict resolution to decide which category should win .", "We incorporated a simple conflict resolution procedure into Basilisk , as well as the metabootstrapping algorithm .", "For both algorithms , the conflict resolution procedure works as follows .", "1 If a word is hypothesized for category A but has already been assigned to category B during a previous iteration , then the category A hypothesis is discarded .", "2 If a word is hypothesized for both category A and category B during the same iteration , then it to the one sense per discourse observation Gale et al . , 1992 that a word belongs to a single semantic category within a limited domain .", "All of our experiments involve the MUC 4 terrorism domain and corpus , for which this assumption seems appropriate .", "Figure 4 shows one way of viewing the task of semantic lexicon induction .", "The set of all words in the corpus is visualized as a search space .", "Each category owns a certain territory within the space demarcated with a dashed line , representing the words that are true members of that category .", "Not all territories are the same size , since some categories have more members than others .", "E . . . . . . . . . . . . . . r , is assigned to the category for which it receives the highest score .", "In Section 3 . 4 , we will present empirical results showing how this simple conflict resolution scheme affects performance .", "Simple conflict resolution helps the algorithm recognize when it has encroached on another category s territory , but it does not actively steer the bootstrapping in a more promising direction .", "A more intelligent way to handle multiple categories is to incorporate knowledge about other categories directly into the scoring function .", "We modified Basilisk s scoring function to prefer words that have strong evidence for one category but little or no evidence for competing categories .", "Each word wi in the candidate word pool receives a score for category ca based on the following formula where AvgLog is the candidate scoring function used previously by Basilisk see Equation 3 and the max function returns the maximum AvgLog value over all competing categories .", "For example , the score for each candidate LOCATION word will be its AvgLog score for the LOCATION category minus its maximum AvgLog score for all other categories .", "A word is ranked highly only if it has a high score for the targeted category and there is little evidence that it belongs to a different category .", "This has the effect of steering the bootstrapping process away from ambiguous parts of the search space .", "We will use the abbreviation 1CAT to indicate that only one semantic category was bootstrapped , and MCAT to indicate that multiple semantic categories were simultaneously bootstrapped .", "Figure 6 compares the performance of Basilisk MCAT with conflict resolution ba M against Basilisk 1CAT ba 1 .", "Most categories show small performance gains , with the BUILDING , LOCATION , and WEAPON categories benefitting the most .", "However , the improvement usually doesn t kick in until many bootstrapping iterations have passed .", "This phenomenon is consistent with the visualization of the search space in Figure 5 .", "Since the seed words for each category are not generally located near each other in the search space , the bootstrapping process is unaffected by conflict resolution until the categories begin to encroach on each other s territories .", "Learning multiple categories improves the performance of meta bootstrapping dramatically for most categories .", "We were surprised that the improvement for meta bootstrapping was much We also measured the recall of Basilisk s lexicons after 1000 words had been learned , based on the gold standard data shown in Table 1 .", "The recall results range from 40 60 , which indicates that a good percentage of the category words are being found , although there are clearly more category words lurking in the corpus .", "Basilisk s bootstrapping algorithm exploits two ideas 1 collective evidence from extraction patterns can be used to infer semantic category associations , and 2 learning multiple semantic categories simultaneously can help constrain the bootstrapping process .", "The accuracy achieved by Basilisk is substantially higher than that of previous techniques for semantic lexicon induction on the MUC 4 corpus , and empirical results show that both of Basilisk s ideas contribute to its performance .", "We also demonBuilding theatre store cathedral temple palace penitentiary academy houses school mansions Event ambush assassination uprisings sabotage takeover incursion kidnappings clash shoot out Human boys snipers detainees commandoes extremists deserter narcoterrorists demonstrators cronies missionaries Location suburb Soyapango capital Oslo regions cities neighborhoods Quito corregimiento Time afternoon evening decade hour March weeks Saturday eve anniversary Wednesday Weapon cannon grenade launchers firebomb car bomb rifle pistol machineguns firearms strated that learning multiple semantic categories simultaneously improves the meta bootstrapping algorithm , which suggests that this is a general observation which may improve other bootstrapping algorithms as well .", "This research was supported by the National Science Foundation under award IRI 9704240 ."], "summary_lines": ["A Bootstrapping Method For Learning Semantic Lexicons Using Extraction Pattern Contexts\n", "This paper describes a bootstrapping algorithm called Basilisk that learns high-quality semantic lexicons for multiple categories.\n", "Basilisk begins with an unannotated corpus and seed words for each semantic category, which are then bootstrapped to learn new words for each category.\n", "Basilisk hypothesizes the semantic class of a word based on collective information over a large body of extraction pattern contexts.\n", "We evaluate Basilisk on six semantic categories.\n", "The semantic lexicons produced by Basilisk have higher precision than those produced by previous techniques, with several categories showing substantial improvement.\n", "We learn multiple semantic categories simultaneously, relying on the assumption that a word cannot belong to more than one semantic category.\n"]}
{"article_lines": ["Chinese Segmentation with a Word Based Perceptron Algorithm", "Standard approaches to Chinese word segmentation treat the problem as a tagging task , assigning labels to the characters in the sequence indicating whether the character marks a word boundary .", "Discriminatively trained models based on local character features are used to make the tagging decisions , with Viterbi decoding finding the highest scoring segmentation .", "In this paper we propose an alternative , word based segmentor , which uses features based on complete words and word sequences .", "The generalized perceptron algorithm is used for discriminative training , and we use a beamsearch decoder .", "Closed tests on the first and show that our system is competitive with the best in the literature , achieving the highest reported F scores for a number of corpora .", "Words are the basic units to process for most NLP tasks .", "The problem of Chinese word segmentation CWS is to find these basic units for a given sentence , which is written as a continuous sequence of characters .", "It is the initial step for most Chinese processing applications .", "Chinese character sequences are ambiguous , often requiring knowledge from a variety of sources for disambiguation .", "Out of vocabulary OOV words are a major source of ambiguity .", "For example , a difficult case occurs when an OOV word consists , possible segmentations include A the discussion 'L .", "will TR very MSA be successful and A the discussion meeting TR very MSA be successful .", "The ambiguity can only be resolved with contextual information outside the sentence .", "Human readers often use semantics , contextual information about the document and world knowledge to resolve segmentation ambiguities .", "There is no fixed standard for Chinese word segmentation .", "Experiments have shown that there is only about 75 agreement among native speakers regarding the correct word segmentation Sproat et al . , 1996 .", "Also , specific NLP tasks may require different segmentation criteria .", "For example , J L ; 5' , W f j ! could be treated as a single word Bank of Beijing for machine translation , while it is more naturally segmented into J L Beijing Wf j !", "bank for tasks such as text to speech synthesis .", "Therefore , supervised learning with specifically defined training data has become the dominant approach .", "Following Xue 2003 , the standard approach to of characters which have themselves been seen as words ; here an automatic segmentor may split the OOV word into individual single character words .", "Typical examples of unseen words include Chinese names , translated foreign names and idioms .", "The segmentation of known words can also be ambiguous .", "For example , iK IITiiI should be iK here IITi iI flour in the sentence iK IITiiI H TR quot ; flour and rice are expensive here or iK here IITiiI inside in the sentence iK IITiiITR it s cold inside here .", "The ambiguity can be resolved with information about the neighboringn words .", "In comparison , for the sentences tih supervised learning for CWS is to treat it as a tagging beam and the importance of word based features . task .", "Tags are assigned to each character in the sen We compare the accuracy of our final system to the tence , indicating whether the character is a single state of the art CWS systems in the literature using character word or the start , middle or end of a multi the first and second SIGHAN bakeoff data .", "Our syscharacter word .", "The features are usually confined to tem is competitive with the best systems , obtaining a five character window with the current character the highest reported F scores on a number of the in the middle .", "In this way , dynamic programming bakeoff corpora .", "These results demonstrate the imalgorithms such as the Viterbi algorithm can be used portance of word based features for CWS .", "Furtherfor decoding . more , our approach provides an example of the poSeveral discriminatively trained models have re tential of search based discriminative training methcently been applied to the CWS problem .", "Exam ods for NLP tasks .", "ples include Xue 2003 , Peng et al . 2004 and Shi 2 The Perceptron Training Algorithm and Wang 2007 ; these use maximum entropy ME We formulate the CWS problem as finding a mapping and conditional random field CRF models Ratna from an input sentence x E X to an output sentence parkhi , 1998 ; Lafferty et al . , 2001 .", "An advantage y E Y , where X is the set of possible raw sentences of these models is their flexibility in allowing knowl and Y is the set of possible segmented sentences . edge from various sources to be encoded as features .", "Given an input sentence x , the correct output segContextual information plays an important role in mentation F x satisfies word segmentation decisions ; especially useful is in F x arg max Score y formation about surrounding words .", "Consider the yEGEN x sentence Q A , which can be from A where GEN x denotes the set of possible segmen among which Q foreign A companies , tations for an input sentence x , consistent with notaor Q in China foreign companies A tion from Collins 2002 .", "business .", "Note that the five character window The score for a segmented sentence is computed surrounding is the same in both cases , making by first mapping it into a set of features .", "A feature the tagging decision for that character difficult given is an indicator of the occurrence of a certain pattern the local window .", "However , the correct decision can in a segmented sentence .", "For example , it can be the be made by comparison of the two three word win occurrence of as a single word , or the occurdows containing this character . rence of separated from ITii in two adjacent In order to explore the potential of word based words .", "By defining features , a segmented sentence models , we adapt the perceptron discriminative is mapped into a global feature vector , in which each learning algorithm to the CWS problem .", "Collins dimension represents the count of a particular fea 2002 proposed the perceptron as an alternative to ture in the sentence .", "The term global feature vecthe CRF method for HMM style taggers .", "However , tor is used by Collins 2002 to distinguish between our model does not map the segmentation problem feature count vectors for whole sequences and the to a tag sequence learning problem , but defines fea local feature vectors in ME tagging models , which tures on segmented sentences directly .", "Hence we are Boolean valued vectors containing the indicator use a beam search decoder during training and test features for one element in the sequence . ing ; our idea is similar to that of Collins and Roark Denote the global feature vector for segmented 2004 who used a beam search decoder as part of sentence y with 4b y E Rd , where d is the total a perceptron parsing model .", "Our work can also be number of features in the model ; then Score y is seen as part of the recent move towards search based computed by the dot product of vector 4b y and a learning methods which do not rely on dynamic pro parameter vector \u03b1 E Rd , where \u03b1z is the weight for gramming and are thus able to exploit larger parts of the ith feature the context for making decisions Daume III , 2006 .", "Score y b y \u03b1 We study several factors that influence the performance of the perceptron word segmentor , including the averaged perceptron method , the size of the 841 Inputs training examples xi , yi The perceptron training algorithm is used to determine the weight values \u03b1 .", "The training algorithm initializes the parameter vector as all zeros , and updates the vector by decoding the training examples .", "Each training sentence is turned into the raw input form , and then decoded with the current parameter vector .", "The output segmented sentence is compared with the original training example .", "If the output is incorrect , the parameter vector is updated by adding the global feature vector of the training example and subtracting the global feature vector of the decoder output .", "The algorithm can perform multiple passes over the same training sentences .", "Figure 1 gives the algorithm , where N is the number of training sentences and T is the number of passes over the data .", "Note that the algorithm from Collins 2002 was designed for discriminatively training an HMM style tagger .", "Features are extracted from an input sequence x and its corresponding tag sequence y Our algorithm is not based on an HMM .", "For a given input sequence x , even the length of different candidates y the number of words is not fixed .", "Because the output sequence y the segmented sentence contains all the information from the input sequence x the raw sentence , the global feature vector 4 x , y is replaced with 4 y , which is extracted from the candidate segmented sentences directly .", "Despite the above differences , since the theorems of convergence and their proof Collins , 2002 are only dependent on the feature vectors , and not on the source of the feature definitions , the perceptron algorithm is applicable to the training of our CWS model .", "The averaged perceptron algorithm Collins , 2002 was proposed as a way of reducing overfitting on the training data .", "It was motivated by the votedperceptron algorithm Freund and Schapire , 1999 and has been shown to give improved accuracy over the non averaged perceptron on a number of tasks .", "Let N be the number of training sentences , T the number of training iterations , and \u03b1n , t the parameter vector immediately after the nth sentence in the tth iteration .", "The averaged parameter vector \u03b3 E Rd is defined as To compute the averaged parameters \u03b3 , the training algorithm in Figure 1 can be modified by keeping a total parameter vector \u03c3n , t E \u03b1n , t , which is updated using \u03b1 after each training example .", "After the final iteration , \u03b3 is computed as \u03c3n , t NT .", "In the averaged perceptron algorithm , \u03b3 is used instead of \u03b1 as the final parameter vector .", "With a large number of features , calculating the total parameter vector \u03c3n , t after each training example is expensive .", "Since the number of changed dimensions in the parameter vector \u03b1 after each training example is a small proportion of the total vector , we use a lazy update optimization for the training process . 1 Define an update vector \u03c4 to record the number of the training sentence n and iteration t when each dimension of the averaged parameter vector was last updated .", "Then after each training sentence is processed , only update the dimensions of the total parameter vector corresponding to the features in the sentence .", "Except for the last example in the last iteration , when each dimension of \u03c4 is updated , no matter whether the decoder output is correct or not .", "Denote the sth dimension in each vector before processing the nth example in the tth iteration as \u03b1n 1 , t s , \u03c3n 1 , t and \u03c4n 1 , t n\u03c4 , s , t\u03c4 , s .", "that the decoder output zn , t is different from the training example yn .", "Now \u03b1n , t We found that this lazy update method was significantly faster than the naive method .", "The decoder reads characters from the input sentence one at a time , and generates candidate segmentations incrementally .", "At each stage , the next incoming character is combined with an existing candidate in two different ways to generate new candidates it is either appended to the last word in the candidate , or taken as the start of a new word .", "This method guarantees exhaustive generation of possible segmentations for any input sentence .", "Two agendas are used the source agenda and the target agenda .", "Initially the source agenda contains an empty sentence and the target agenda is empty .", "At each processing stage , the decoder reads in a character from the input sentence , combines it with each candidate in the source agenda and puts the generated candidates onto the target agenda .", "After each character is processed , the items in the target agenda are copied to the source agenda , and then the target agenda is cleaned , so that the newly generated candidates can be combined with the next incoming character to generate new candidates .", "After the last character is processed , the decoder returns the candidate with the best score in the source agenda .", "Figure 2 gives the decoding algorithm .", "For a sentence with length l , there are 2l 1 different possible segmentations .", "To guarantee reasonable running speed , the size of the target agenda is limited , keeping only the B best candidates .", "The feature templates are shown in Table 1 .", "Features 1 and 2 contain only word information , 3 to 5 contain character and length information , 6 and 7 contain only character information , 8 to 12 contain word and character information , while 13 and 14 contain append the character to the last word word and length information .", "Any segmented sentence is mapped to a global feature vector according to these templates .", "There are 356 , 337 features with non zero values after 6 training iterations using the development data .", "For this particular feature set , the longest range features are word bigrams .", "Therefore , among partial candidates ending with the same bigram , the best one will also be in the best final candidate .", "The decoder can be optimized accordingly when an incoming character is combined with candidate items as a new word , only the best candidate is kept among those having the same last word .", "Among the character tagging CWS models , Li et al . 2005 uses an uneven margin alteration of the traditional perceptron classifier Li et al . , 2002 .", "Each character is classified independently , using information in the neighboring five character window .", "Liang 2005 uses the discriminative perceptron algorithm Collins , 2002 to score whole character tag sequences , finding the best candidate by the global score .", "It can be seen as an alternative to the ME and CRF models Xue , 2003 ; Peng et al . , 2004 , which do not involve word information .", "Wang et al . 2006 incorporates an N gram language model in ME tagging , making use of word information to improve the character tagging model .", "The key difference between our model and the above models is the wordbased nature of our system .", "One existing method that is based on sub word information , Zhang et al . 2006 , combines a CRF and a rule based model .", "Unlike the character tagging models , the CRF submodel assigns tags to subwords , which include single character words and the most frequent multiple character words from the training corpus .", "Thus it can be seen as a step towards a word based model .", "However , sub words do not necessarily contain full word information .", "Moreover , sub word extraction is performed separately from feature extraction .", "Another difference from our model is the rule based submodel , which uses a dictionary based forward maximum match method described by Sproat et al . 1996 .", "Two sets of experiments were conducted .", "The first , used for development , was based on the part of Chinese Treebank 4 that is not in Chinese Treebank 3 since CTB3 was used as part of the first bakeoff .", "This corpus contains 240K characters 150K words and 4798 sentences .", "80 of the sentences 3813 were randomly chosen for training and the rest 985 sentences were used as development testing data .", "The accuracies and learning curves for the non averaged and averaged perceptron were compared .", "The influence of particular features and the agenda size were also studied .", "The second set of experiments used training and testing sets from the first and second international Chinese word segmentation bakeoffs Sproat and Emerson , 2003 ; Emerson , 2005 .", "The accuracies are compared to other models in the literature .", "F measure is used as the accuracy measure .", "Define precision p as the percentage of words in the decoder output that are segmented correctly , and recall r as the percentage of gold standard output words that are correctly segmented by the decoder .", "The balanced F measure is 2pr p r .", "CWS systems are evaluated by two types of tests .", "The closed tests require that the system is trained only with a designated training corpus .", "Any extra knowledge is not allowed , including common surnames , Chinese and Arabic numbers , European letters , lexicons , part of speech , semantics and so on .", "The open tests do not impose such restrictions .", "Open tests measure a model s capability to utilize extra information and domain knowledge , which can lead to improved performance , but since this extra information is not standardized , direct comparison between open test results is less informative .", "In this paper , we focus only on the closed test .", "However , the perceptron model allows a wide range of features , and so future work will consider how to integrate open resources into our system .", "In this experiment , the agenda size was set to 16 , for both training and testing .", "Table 2 shows the precision , recall and F measure for the development set after 1 to 10 training iterations , as well as the number of mistakes made in each iteration .", "The corresponding learning curves for both the non averaged and averaged perceptron are given in Figure 3 .", "The table shows that the number of mistakes made in each iteration decreases , reflecting the convergence of the learning algorithm .", "The averaged perceptron algorithm improves the segmentation accuracy at each iteration , compared with the nonaveraged perceptron .", "The learning curve was used to fix the number of training iterations at 6 for the remaining experiments .", "Reducing the agenda size increases the decoding speed , but it could cause loss of accuracy by eliminating potentially good candidates .", "The agenda size also affects the training time , and resulting model , since the perceptron training algorithm uses the decoder output to adjust the model parameters .", "Table 3 shows the accuracies with ten different agenda sizes , each used for both training and testing .", "Accuracy does not increase beyond B 16 .", "Moreover , the accuracy is quite competitive even with B as low as 4 .", "This reflects the fact that the best segmentation is often within the current top few candidates in the agenda . 2 Since the training and testing time generally increases as N increases , the agenda size is fixed to 16 for the remaining experiments .", "Our CWS model is highly dependent upon word information .", "Most of the features in Table 1 are related to words .", "Table 4 shows the accuracy with various features from the model removed .", "Among the features , vocabulary words feature 1 and length prediction by characters features 3 to 5 showed strong influence on the accuracy , while word bigrams feature 2 and special characters in them features 11 and 12 showed comparatively weak influence .", "Four training and testing corpora were used in the first bakeoff Sproat and Emerson , 2003 , including the Academia Sinica Corpus AS , the Penn Chinese Treebank Corpus CTB , the Hong Kong City University Corpus CU and the Peking University Corpus PU .", "However , because the testing data from the Penn Chinese Treebank Corpus is currently unavailable , we excluded this corpus .", "The corpora are encoded in GB PU , CTB and BIG5 AS , CU .", "In order to test them consistently in our system , they are all converted to UTF8 without loss of information .", "The results are shown in Table 5 .", "We follow the format from Peng et al . 2004 .", "Each row represents a CWS model .", "The first eight rows represent models from Sproat and Emerson 2003 that participated in at least one closed test from the table , row Peng represents the CRF model from Peng et al . 2004 , and the last row represents our model .", "The first three columns represent tests with the AS , CU and PU corpora , respectively .", "The best score in each column is shown in bold .", "The last two columns represent the average accuracy of each model over the tests it participated in SAV , and our average over the same tests OAV , respectively .", "For each row the best average is shown in bold .", "We achieved the best accuracy in two of the three corpora , and better overall accuracy than the majority of the other models .", "The average score of S10 is 0 . 7 higher than our model , but S10 only participated in the HK test .", "Four training and testing corpora were used in the second bakeoff Emerson , 2005 , including the Academia Sinica corpus AS , the Hong Kong City University Corpus CU , the Peking University Corpus PK and the Microsoft Research Corpus MR .", "Different encodings were provided , and the UTF8 data for all four corpora were used in this experiment .", "Following the format of Table 5 , the results for this bakeoff are shown in Table 6 .", "We chose the three models that achieved at least one best score in the closed tests from Emerson 2005 , as well as the sub word based model of Zhang et al . 2006 for comparison .", "Row Zh a and Zh b represent the pure sub word CRF model and the confidence based combination of the CRF and rule based models , respectively .", "Again , our model achieved better overall accuracy than the majority of the other models .", "One system to achieve comparable accuracy with our system is Zh b , which improves upon the sub word CRF model Zh a by combining it with an independent dictionary based submodel and improving the accuracy of known words .", "In comparison , our system is based on a single perceptron model .", "In summary , closed tests for both the first and the second bakeoff showed competitive results for our system compared with the best results in the literature .", "Our word based system achieved the best Fmeasures over the AS 96 . 5 and CU 94 . 6 corpora in the first bakeoff , and the CU 95 . 1 and MR 97 . 2 corpora in the second bakeoff .", "We proposed a word based CWS model using the discriminative perceptron learning algorithm .", "This model is an alternative to the existing characterbased tagging models , and allows word information to be used as features .", "One attractive feature of the perceptron training algorithm is its simplicity , consisting of only a decoder and a trivial update process .", "We use a beam search decoder , which places our work in the context of recent proposals for searchbased discriminative learning algorithms .", "Closed tests using the first and second SIGHAN CWS bakeoff data demonstrated our system to be competitive with the best in the literature .", "Open features , such as knowledge of numbers and European letters , and relationships from semantic networks Shi and Wang , 2007 , have been reported to improve accuracy .", "Therefore , given the flexibility of the feature based perceptron model , an obvious next step is the study of open features in the segmentor .", "Also , we wish to explore the possibility of incorporating POS tagging and parsing features into the discriminative model , leading to joint decoding .", "The advantage is two fold higher level syntactic information can be used in word segmentation , while joint decoding helps to prevent bottomup error propagation among the different processing steps .", "This work is supported by the ORS and Clarendon Fund .", "We thank the anonymous reviewers for their insightful comments ."], "summary_lines": ["Chinese Segmentation with a Word-Based Perceptron Algorithm\n", "Standard approaches to Chinese word segmentation treat the problem as a tagging task, assigning labels to the characters in the sequence indicating whether the character marks a word boundary.\n", "Discriminatively trained models based on local character features are used to make the tagging decisions, with Viterbi decoding finding the highest scoring segmentation.\n", "In this paper we propose an alternative, word-based segmentor, which uses features based on complete words and word sequences.\n", "The generalized perceptron algorithm is used for discriminative training, and we use a beam-search decoder.\n", "Closed tests on the first and second SIGHAN bakeoffs show that our system is competitive with the best in the literature, achieving the highest reported F-scores for a number of corpora.\n", "We also provide a feature template for Chinese word segmentation.\n"]}
{"article_lines": ["A Stochastic Japanese Morphological Analyzer Using a Forward DP Backward A N Best Search Algor i thm Masa . aki NAGATA NTT Network Information Systems l , bor ttorics 1 2356 Take , Yokosuka Shi , Kanagaw t , 238 03 Japan tel 4 81 468 59 2796 fax 81 468 59 3428 e mail nagata nttnly . ntt .", "j l Abstract We present a novel method for segmenting the input sentence into words and assigning parts of speech to the words .", "It consists of a statistical language model and an efficient wo pa qs N best search algorithm .", "The algorithm does not require delimiters between words .", "Thus it is suitable for written Japanese .", "qhe proposed Japanese morphological nalyzer achieved 95 . l recall and 94 . 6 precision for open text when it was trained and tested on the ATI ?", "1 Introduct ion In recent years , we have seen a fair number of l al ers re porting accuracies ofmore than 95 for English part of speech tagging with statistical language modeling tech niques 2 4 , 10 , 11 .", "On the other hand , there are few works on stochastic Japanese morphological nalysis 9 , 12 , 14 , and they dont seem to have convinced the Japanese NLP community that the statistically based teclmiques are superior to conventional rule based tech niques uch as 16 , 17 .", "We show in this paper that we can buihl a stochastic Japanese morphological nalyzer that offers approxi mately 95 accuracy on a statistical language model ing technique and an efficient two pass N best search strategy .", "We used tile simple tri POS model as the tagging model for Japanese .", "Probability estimates were ob tained after training on the ATI l ialogue Database 5 , whose word segmentation a d part of speech tag assignment were laboriously performed by hand .", "We propose a novel search strategy for getting the N best morphological nalysis hypotheses for the in put sentence .", "It consists of the forward dynamic pro gramming search and the backward A search .", "The proposed algorithm amalgamates and extends three well known algorithms in different fields the Minimum Connective Cost Method 7 for Japanese morphologi cal analysis , Extended Viterbi Algorithm for charac ter recognition 6 , and l ee Trellis N Best Search for speech recognition 15 .", "We also propose a novel method for handling un known words uniformly within the statistical pproach .", "Using character trigrams ms tim word model , it gener ates the N best word hypotheses that match the left most substrings starting at a given position in the input senten ce .", "Moreover , we propose a novel method for evaluat ing the performance of morphological analyzers .", "Un like English , Japanese does not place spaces between words .", "It is difficult , even for native Japanese , to place word boundaries consistently because of the aggluti native nature of the language .", "Thus , there were no standard performance metrics .", "We applied bracketing accuracy measures 1 , which is originally used for En glish parsers , to Japanese morphological nalyzers .", "We also slightly extended the original definition to describe the accuracy of tile N best candidates .", "In the following sections , we first describe the tech niques used in the proposed morphological nalyzer , we then explain the cwduation metrics and show the systems performance by experimental results .", "2 Tagging Model 2 . 1 Tr i POS Mode l and Re la t ive F re quency Tra in ing We used the tri POS or triclass , tri tag , tri Ggram etc .", "model Ls tile tagging model for Japanese .", "Con sider a word segmentation f the input sentence W wl w2 .", "w , and a sequence of tags T t i ts .", ", t , , of the same length .", "The morphological analysis tmsk cau I e formally defined , finding a set of word segmen tat . ion and parts of speech ssignment that maximize the joint probability of word sequence arm tag sequence P W , 7 .", "In the tri POS model , the joint probability is approximated bythe product of parts of speech trigram probabilities P t i l t i_2 , t i_ l and word output probabil ities for given part of speech P wl ll r w , r r tdt , _o . , t , _x r w , lt4 1 i 1 201 In practice , we consider sentence boundaries s special symbols as follows .", "P W , T P ql P wtltt P t , .", "l , tl P w21t I P tilti_2 , ti_l P willi P t , , _l , ? , , 2 i 3 where indicates the sentence boundary marker .", "If we have some tagged text available , we can estimate the probabilities P tdti_2 , ti_l and P wiltl by comput ing the relative frequencies of the corresponding events on this data .", "N ti_2 , ti 1 , tl P tifti 2ti t f qltl 2ti x iV t i _ .", ", , t i _ , 3 P wilti f wilt , N w , t 1 N t where f indicates the relative frequency , N w , t is t ! , e number of times a given word w appears with tag l , aid N li_2 , ti l , tl is the number of times that sequer ce l i _2t i _ l l i appears in the text .", "It is inevitable to s irer from sparse data problem in the part of speech tag tri gram probability I .", "To handle open text , trigram Frol ability is smoothed by interpolated estimation , wi ich simply interpolates trlgram , bigram , unigram , and ze rogram relative frequencies 8 , P t i lq_ , , q_ l qaf tilt , _2 , q_ , A q2f tdti_l qtf ti qoV 5 where f indicates the relative . frequency and V is a uniform probability that each tag will occur .", "The non negative weights qi satisfy q3 q q1 q0 1 , and they are adjusted so as to make the observed ata most probable after the adjustment by using EM algorithm .", "2 . 2 Order Reduct ion and Recursive Tracing In order to understand the search algorithm described in the next section , we will introduce the second order HMM and extended Viterbi algorithm 6 .", "Considering the combined state sequence U ltltt2 . . , ttn , where ul tl and ui ti tli , we have P uilui_l P tilti_ , ti_l 6 Substituting Equation 6 into Equation l , we have lWe used 120 part of speedl tags .", "In the ATR Corpus , 26 parts of speech , 13 conjugation types , and 7 conjugation forms are defined .", "Out of 26 , 5 parts of speech ave conjugation .", "Since we used a list of part of speech , conjugation type , and conjuga tion form as a tag , there are 119 tags in the AIT ?", "We added the sentence boundary marker to them .", "aTo handle open text , word output probahility P lo i l t i must also be smoothed .", "Tiffs problem is discussed in a later section Ls the unknown word problem .", "Equation model .", "Wi we have P W , r 1 .", "P mlui a P wilti 7 i 1 7 have the same form as the first order Conshler the partial word sequence HI and the partial tag sequence Ti t l .", "t l , F w , ?", "w , _ , , , P d , .", "p wdtO 8 Equation 8 suggests that , to find the maxlmmn P I , Vi , 7 for each ul , we need only to remember the maximum P W ? _I , 7 _1 , extend each of these prob abilities to every ul by computing Eqnation 8 , and select the m ; uxinmm P Vi , Ti for each ui .", "thus , by increasing i by 1 to n , selecting the u . ttlat maximize P W . , 7 , and backtracing the sequence leading to the nmxinmm probability , we can get the optimal tag seqnence .", "3 Search S t ra tegy The search algorithm consists of a forward dynamic programming search and a backward A search .", "First , a linear time dynamic programming is used for record ing the scores of all partial paths in a table 3 .", "A back ward A algorithm based tree search is then used to extend the partial paths .", "Partial paths extended in the backward tree search are ranked by their correspond ing fill path scores , which are cmnputed by adding the scores of backward partial path scores to the cot responding best possihle scores of the remaining paths which are prerecorded in the forward search .", "Since the score of the incomplete portion of a path is exactly known , the backward search is admissible .", "That is , the top N candidates are exact .", "3 . 1 The Forward DP Search Table 1 shows the two data structures used in our al gorithm .", "The st , t , cture parse stores tile information of a word and the best partial path up to the word .", "Parse . s ta r t and parse . end are the indices of tile start and end positions of the word in the sentence .", "Parse . pos is tile part of speech tag , which is a list of part of speech , conjugation type , and conjugation form in our system for Japanese .", "Parse . n th order tate is a list of the last two parts of speech tags includ ing that of the current word .", "This slot corresponds to the combined state in the second order IIMM .", "Parse . prob so fa r is the score of the best partial path from the beginning of the sentence to the word .", "Parse . prev ? ous i the pointer to the best previous parse structure as in conventional Viterbi decoding , which is not necessary if we use the backward N best search .", "ln fact , we use two tables , pa se .", "ist and path ap .", "The reason is described later .", "202 The structure word represents the word information in the dictionary including its lexical form , part of speech tag , and word output probability given tt , e part of speech .", "Table h Data structures for the N best algorithm start end pea nth order state prob ao far previous parse strttctule tim beginning pasition of the word the end position of the word part of speech tag of the word a list of the la t two parts , f speech the b , t partial path score from the start a pointer to previous parse strllettlle word structure form lexical f , . n of the word l Oa part of speech tag of the word prob _ word outlmt probability Before explaining tim forward search , we will de fine some flmctions and tables used in the algo rithm .", "In the forward search , we use a table called parse list , whose key is the end position of the parse structure , and wlm , se value is a list of parse structures that have the best partial path scores for each combined state at the end position .", "Function reg is ter to parse l i s t registers a parse structure against the parse list and maintains the best par tim parses .", "Function get parse l i s t returns a list of parse structnres at the specified position .", "We also use the fimetion l e l tmost subst r ings which returns a list of word structures in the dictionary whose lexical form matches the substrings tarting at the .", "specified position in the input sentence .", "funct ion orward paae string begin i n i t ia l a tepO ; It Pods spec ia l symbols at both ends .", "for iffil to length s t r ing do foreach parse in get parse l i s t i do foreach word ill l e f tmost subat r ings a t r ing , i , I 7 poa ngrma append parse . nth order stato , l i s t word . poa if traneprob poe ngrtm O then new parse .", "make parseO ; new parse . mtart i ; new parse . end i length word . form ; hey pares , poe word . pea ; new parae . nth order mtate rest pos ngram ; naw paree . preb ee far parae . prob so far transprob pos ngram word . prob ; new parse . previous paras ; reg ie ter parse to parae l t s t new parse ; reg is ter paree to path ma p new parse ; endif elld end end f inn l e tQp ; i Randlan t r tmai t ion to tho e d symbol .", "end Figure h The forward DP search algorithm Figure 1 shows the central part of the forward dy namic programming search algorithm .", "It starts from the beg , string of tim inlmt sentence , and proceeds char attar by character .", "At each point in tim sentence , it looks up the combination of the best partial parses ending at the point and word hypotheses tarting at that point .", "If tim connection of a partial parse and a word llypothesis is allowed by the tagging model , a new continuation parse is made and registered in the parse l i s t .", "The partial path score for the new con titular , on parse is the product of the best partial path score up to the poi , g , the trigram probability of the last three parts of speech tags and the word output probability for LIfe part of speech 4 .", "3 . 2 The Backward A Search The backward search uses a table called path map , whose key is the end position of tile parse structure , and whose value is a list of parse structures that have the best partial path scores for each distinct combin ties of the start position and the combined state .", "The dilference 1 etween parse l i s t and path map is that path map is classi ied by tim start position of the last word in addition to tim combined state .", "This distinction is crucial for the proposed N best algorithm , l or tim tbrward search to tind a parse that maximizes Equation 1 , it is the parts of speech se quence that matters .", "For the backward N best search , how wet , we want N most likely word segmentation and part of speech sequence .", "Parse list may shadow less probable candidates that have the same part of speech sc qnence for the best scoring candidate , but differ in tim segmentaL , on of the last word .", "As shown in Figure 1 , path map is made during the forward search by the function reg is ter parse to path map , which regis ters a parse structure to path map and maintains the best partial parses in the tables criteria .", "Now we describe the central part of tim backward A search algorithm .", "But we assume that the readers know the A algorithm , and exphtin only the way we applied the algorithm to the problem .", "We consider a parse structure , q a state in A search .", "Two slates are e plat if their parse structures have the same start position , end position , and com bined state .", "The backward search starts at the end of the input , sentence , and backtracks to the beginning of the sentence using tim path map .", "Initial states are obtained by looking up the entries of tim sentence nd position of the path map .", "The suc cessor states are obtained by first , looking u 1 tim en tries of the path map at the start position of the cur rent parse , then cbecldng whether they satisfy the con straint of the combined state transition in the second order IIMM , aim whether the transition is allowed by the tagging model .", "The combined state transition con straint means that tim part of speech sequence in the parse . n th order s ta te of the current parse , ignor 4 In Figure 1 , function transprob returns the probability of given trlgraln .", "Functions i n i t ia l s tep and f ina l s tep treat be t l a l iS l t ons I L sltl i l l llce , Ol l l dl l l ieg , 203 ing the last element , equals that of tile previous parse , ignoring the first element .", "The state transition cost of the backward search is the product of the part of speech trigram probability and the word output probability .", "Tile score estimate of the remaining portion of a path is obtained from the parse . prob so ar slot in the parse structure .", "The backward search generates the N best hypothe ses sequentially and there is no need to preset N . The complexity of the backward search is significantly less than that of the forward search .", "4 Word Mode l To handle open text , we have to cope with unknown words .", "Since Japanese do not put spaces between words , we have to identify unknown words at first .", "To do this , we can look at the spelling character sequence that may constitute a word , or look at the context o identify words that are acceptable in this context .", "Once word hypotheses for unknown words are gener ated , the proposed N best algorithm will find tile most likely word segmentation a d part of speech assignment taking into account he entire sentence .", "Therefore , we can formalize the unknown word problem as letermin ing the span of an unknown word , assigning its part of speech , and estimating its probability given its part of speech .", "Let us call a computational model that determines the probability of any word hypothesis given its lexi cal form and its part of speech the word model .", "The word model must account for morphology and word for marion to estimate the part of speech and tile probabil ity of a word hypothesis .", "For tile first approxinmtion , we used the character trigram of each part of sl eech as the word model .", "Let C cic . . , c , denote the sequence of n charac ters that constitute word zv whose part of speech is t . We approximate the probability of the word given part of speech P wlt by tile trigram probabilities , p , , , Iz P , C f , , l , , 1 , u IX P , , lc , r , 1c . . _l , . . , i 3 9 where special symbol indicates ttle word boundary marker .", "Character trigram probabilities are estimated from the training corpus by computing relative fre quency of character bigram and trigram that appeared in words tagged as t . Pt cilci 2 , q i f , c l , ? , Nt ci_2 , Ci_l , ci N , c _ . , i lO where Nt ci_2 , ci_ , ci is tile total number of times character trigram ci_2ci_ el appears in words tagged as t in the training corpus .", "Note that the character trigram probabilities reflect the frequency of word to kens in tile training corpus .", "Since there are more than 3 , 000 characters in Japanese , trigram probabilities are smoothed by interpolated estimation to cope with the sparse data problem .", "It is ideal to make this character trigram model for all open clmss categories , llowever , the amount of train ing data is too small for low frequency categories if we divide it by part of speech tags .", "Therefore , we made trigram models only for tile 4 most frequent parts of speech that are open categories and have no conju gation .", "They are common noun , proper noun , sahen no ln5 and nun lera l .", "est imate paxt of spoech ; Hiyako Hotel C 2 . 7621915641723623E 7 f 6 . 3406095003694205E 9 l 5 , 840424519473811E 19 5 . 7364195413101E 29 est imate part of speech I 9 9 4 1 , 8053860295767367E 6 M o . s1224s6sls404 zE 17 Jdrl 2 . 288684007246524E 17 , 7 . sos s3 aso211e 20 ; p roper noun ; common noun ; sa_han noun ; numeral ; numeral ; proper noun ; common noun ; sahen noun Figure 2 N best Tags for Unknown Words Figure 2 show two examples of part of speech estima tion for unknown words .", "Each trigram model returns a probability if the input string is a word belonging to the category .", "In both examples , the correct category has the largest probability .", "get lef tmo st subst riags uit h word model i 4 M 2 . 519457597358691E 7 ; f j 2 . 3449215070189967E 8 tlfj l i 7 . 02439907471337451 9 i , , 1 2 . 375650975098567E 9 , l J 4 .", "; il 5 . 706S 4990251415E IO t . j cj , 4 . 735628004876359E 13 , 1 8 . 9289423481071831 14 i b , i l 7 . 266613344265452E 14 1 , i 6 . 866d9949613207E 16 , l b RlJlIiG , I 2 . 45302390s251351sE 17 Figure 3 N Best Word lIylmtheses Figure 3 shows the N best word hypotheses gener ated by using tile character trigram models .", "A word hypothesis is a list of word boundary , part of speech assignment , and word probability that matches tile left most substrings starting at a given position in tile input sentence .", "In the forward search , to handle unknown words , word hypotheses are generated at every posi tion in addition to the ones generated by the function leftmost subs ; r ings , which are the words found ill tile dictionary , llowever , ill our system , we limited the ntunl er of word hyl otheses generated at each position to 10 , for efficiency reasons .", "aA noun tlmt can be used a s a verb when it is followed by a forlna , verb s tr t , 204 5 Eva luat ion Measures We applied the performance measures for English parsers 1 to Japanese morphological analyzers .", "The basic idea is that morphological nalysis for a sentence can be thought of as a set of labeled brackets , where a bracket corresponds to word segmentation and its la .", "bel corresponds to part of speech .", "We then compare the brackets contained in the systems output to the brackets contained in the standard analysis .", "For the N best candidate , we will make the union of t , e brack ets contained in each candidate , and compare thenr to the brackets in the standard .", "For comparison , we court , the number of I rackcts in the standard data Std , the number of brackets in the system output Sys , and the nunlber of match ing brackets M .", "We then calculate the nleasurcs of recall M Std and precision M Sys .", "We also connt the number of crossings , which is tile mmtber of c , mes where a bracketed sequence from the standard data overlaps a bracketed sequence from tile system output , but neither sequence is completely coutained in the other .", "We defined two equaiity criteria of brackets for counting tim number of matching brackets .", "Two brack ets are unlabeled bracket equal if the boundaries of the two brackets are tile same .", "Two brackets are labeled bracket . equal if the labels of the brackets ark the same in addition to unlabeled I racket equal .", "In comparing the consistency of the word segmentations of two brack clings , wllich we call structure consistency , we count the measures recall , precision , crossings by unlabeled bracket equal .", "In comparing the consistency of part of speech assignment in addition to word segmenta tion , which we call label consistency , we couut them by labeled bracket equal .", "S9433 3fi658235fi b tRllDll iil ill ! lll , Til l t J lJ ltlDil , l .", "l k o I i fd 43 , I0367483fi46801 Figure 4 N Best Morphological Analysis hypotheses For example , Figure 4 shows a sample of N hest anal ysls hypotheses , where the first candidate is the correct analysis a .", "For the second candhlate , since there are !", "rackets in tim correct data Std 9 , 11 brackets in the second candidate Sys l l , and 8 nlatciiing brackets M 8 , tile recall and precision with respect to label consistency are 8 9 and 8 11 , respectively .", "For the top 6Probabilities me in liiltura log b se .", "two candidates , since tliere ; ire 12 distinct brackets in tile systems otll . litlt and 9 Inatehing brackets , tile re call and precision with respect o hal el consistency are 9 9 aud 9 12 , respeetiwqy .", "For the third candidate , since the correct data and the third candidate differ in just one part of Sl eech tag , the recall and precision wittl respect o structure consistency are 9 9 and 9 9 , respectiw ly .", "6 Exper iment Table 2 The aillount of training and test data _ trahling texts closed test open 10 o 0 Sentences 1 5 10 i0 13899 Words 149059 13176 Characters _ 267 , 122 9422 98997 We used the NII Dialogue Databaae 5 to train and test the proposed morphological nalysis method .", "It is a corpus of approxiumtely 800 , 000 words whose word segmentatio , l and part of speech tag assigmnent were laboriously performed by hand .", "In tiffs experilneut , we only used one fourth of the AFt .", "Corl us , a portion of the keyl oard dialogues in the conference registration domain .", "First , we selected 1 , 000 test sentences for all open test , arid used I . he others for training .", "Tile corpus was divided into 90 R r training and 10 for test ing .", "We then selected 1 , 000 sentences from tile traiu ing set and used them for a closed test .", "The number of sentences , words , and characters for each test set and training texts are shown iu Pable 2 .", "The training texts contained 6580 word types and 6945 tag trigram types .", "There were 247 unknown word types and 213 unknown tag trigram types in tim open test senteuces .", "Thus , both part of speech tri gralrl l robabilities alld word output probabilities must be snioothed to handle open texts .", "Table 3 Perccld .", "; ige of words correctly segmented and tagged raw part o speech bigram aud trigrmn I 2 I 98 l I 8 9 .", "7 90 . 7 0 . 007 I os . , 8 a .", "s 84 . a o . m2 I 9a . 2 I 7 s .", "o I 79 . 6 I o . o15 I k 5 I lii ?", "I i n I r . o I o . o s_l First , as a I reliminary experiment , we compared tile perforn ances of part of speech bigram and trigram .", "Table 3 shows the percentages of words correctly seg mented and tagged , tested on the closed test sentences .", "The trigram model achiew ; d 97 . 5 recall and 97 . 8 precision flu the top candidate , while tile bigram model achiew . d 96 . 2 recall and 96 . 6 precision .", "Although both tagging models sllow very high l erformanee , tile 20 , 5 trigram model outperformed tile bigram model in every metric .", "We then tested the proposed system , which uses smoothed part of speech trigram with word model , on the open test sentences .", "Table 4 shows tile percentages of words correctly segmented and tagged .", "In Table 4 , label consistency 2 represents the accuracy of segmen tation and tagging ignoring the difference in conjuga tion form .", "For open texts , tile morphological nalyzer achieved 95 . 1 recall and 94 . 6 precision for the top candidate , and 97 . 8 recall and 73 . 2 precision for the 5 best candidates .", "This performance is very encouraging , and is comparable to the state of the art stochastic tagger for English 2 4 , 10 , 11 .", "Since the segmentation accuracy of the proposed sys tem is relatively high 97 . 7 recall and 97 . 2 precision for the top candidate compared to the morphologi cal analysis accuracy , it is likely that we can improve the part of speech assignment accuracy by refining the statistically based tagging model .", "We find a fair num ber of tagging errors happened in conjugation forms .", "We assume that this is caused by the fact that the Japanese tag set used in tile ATR .", "Corpus is not de tailed enough to capture the complicated Japanese verb morphology .", "100 95 90 5 80q 75 90 65 60 Hogpho loq lca l Ana ly l t s Accuracy fo r N I les t Sentences r iw t r tq ram c losed ce t a . raw b i t tam , a t , ?", "a othed t r t ram wi th Iopen text o , .", "Imoothed t r lq ram wi word a lo t le l l opes te t . . . . raw m wi th word moOel l apen text .", "iraw t r r l ra w i thout word nloc el lopes text . 12 .", "I I , I 2 3 4 R ink Figure 5 Tile percentage of sentences correctly seg mented and tagged .", "Figure 5 shows tile percentage of sentences not words correctly segmented and tagged .", "For open texts , the sentence accuracy of the raw part of speech trigram without word model is 62 . 7 for the top candidate and 70 . 4 for the top 5 , while that of smoothed trigram with word model is 66 . 9 for the top and 80 . 3 for the top 5 .", "We can see that , by smoothing tile part ofsllecch trigram and by adding word model to handle unknown words , the accuracy and robustness of the morpholog ical analyzer is significantly improved .", "Ilowever , tile sentence accuracy for closed texts is still significantly better that that for ol en texts .", "It is clear that more research as to be done on the smoothing problem .", "7 Discuss ion Morphological analysis is an important practical prob lem with potential apl lication in many areas including kana to kanji conversion 7 , speech recognition , charac ter recognition , speech synthesis , text revision support , information retrieval , and machine translation .", "Most conventional Japanese morphological nalyzers use rule based heuristic searches .", "They usually use a connectivity rnatrix part of sl eech pair grammar , as the language model .", "To rank the morphological nal ysis hypotheses , they usually use heuristics uch as Longest Match Method or Least Bunsetsus Number Method 16 .", "There are some statistically based approaches to Japanese morphological nalysis .", "The tagging models previously used are either part of speech I igram 9 , 14 or Character based IIMM 12 .", "Both hemistic based and statistically based ap proaches use t . he Minimum Connective Cost Method 7 , which is a linear time dynamic programming algo rithm that finds the morphological hypothesis that has tile minimal connective cost i . e .", "bigram ba sed cost as derived by certain criteria .", "qo handle unknown words , most Japanese morpho logical analyzers u , e character type heuristics 17 , which is a string of the same character type is likely to constitute a word .", "There is one stochastic approach that uses bigram of word formation unit 13 .", "tlowever , it does not learn probabilities from training texts , but learns them fiom machine readable dictionaries , and the model is not incorporated in working morphologi cal analyzers , as fitr as the author knows .", "The unique features of the proposed Japanese mor phological analyzer is that it can find tile exact N most likely hyl otheses using part of speech trigram , and it can handle unlmown words using character trigram .", "The algoril . hm can naturally be extended to handle any higher order Markov models .", "Moreover , it can nat nrally be extended to handle lattice style input that is often used as t . he output of speech recognition and character ecognition systems , by extending the func tion le f tmost subat r inga so as to return a list of words in the dictionary that matches the substrings in tile input lattice stmting at the specified p xqition .", "For future wotk , we have to study the most effective way of generating word hypotheses that can handh .", "un known words .", "Currently , we are limiting the number of word hypotheses to reduce ambiguity at tile cost of ac curacy .", "We have also to study tile word model for open categories thai , have conjugation , because the training 7Kana to kan j i convers ion i s a pop alar J I Lpanese input method on computer using ASCII keyboard .", "Phonetic tranHerip tion by Roams ASCII characters are input and converted fir , st to the Japanese syllabary hiragana which is then converted to orthographic trm scrlption ncluding Chinese character kanjl .", "206 Table 4 The percentage of words correctly segmented and tagged smoothed trigram with word model smoothed trigram with word model open text lal el consistency recall precision crossings 95 . 1 94 . 6 0 . 013 96 . 5 88 . 0 0 . 023 97 . 3 82 . 1 0 . 031 97 . 6 77 . 4 0 . 016 97 . 8 73 . 2 0 . 061 label consistency 2 recall precision crossings 95 . 9 95 . 4 J . 013 97 . 0 90 . 3 0 . 023 97 . 6 85 . 1 0 . 031 97 . 9 80 . 7 0 . 046 98 . 1 77 . 1 0 . 060 structure consistency recall precision 97 . 7 97 . 2 98 . 2 94 . 4 98 . 5 91 . 7 98 . 7 89 . 6 98 . 8 87 . 9 crossings 0 . 013 0 . 022 0 . 029 0 . 044 0 . 056 data gets too small to make trigrams if we divide it by tags .", "We will probably have to tie some parameters to solve the insufficient data problem .", "Moreover , we have to study the method to adapt he system to a new domain .", "To develop an m supervised learning method , like the forward backward algorithm for IIMM , is an urgent goal , since we cant always ex pect the availability of manually segmented and tagged data .", "We can think of an EM algorithm by replacing maximization with summation in the extended Viterbi algorithm , but we dont know how to handle unknown words in this algorithm .", "8 Conc lus ion We have developed a stochastic Japanese morphologi cal analyzer .", "It uses a statistical tagging model and an efficient two pass earch algorithm to llnd the N best morphological nalysis hypotheses for the input sen tence .", "Its word segmentation a d tagging accuracy is approxlmatcly 95 , which is comparable to the star . e of the art stochastic tagger for English .", "7 8 9 10 11 13 Re ferences 14 1 Black , E . et al .", "A Procedure for Quantit . a tively Comparing the Syntactic Coverage of En glish Grammars , I AIHA Speech an I Nalma Language Workshop , pp . 306 311 , Morgan Kauf 15 mann , 1991 .", "2 Charniak , E . , Ilendrickson , C . , Jacol son , N . , and Perkowitz , M . Equations for Part of Speech Tagging , AAAI 93 , I 1 . 784 789 , 1993 .", "16 3 Church , K . A Stochastic Part of Speech Tagger and Noun Phrase Parser for English , ANLP 88 , pp . 136 143 , 1988 .", "4 Cutting , D . , Kupiec , J . , Pederseu , J . , and Sibnn , P . A Practical Part of Speech Tagger , ANLP 17 92 , pp . 133 140 , 1992 .", "5 Ehara , T . , Ogura , K . and Morimoto , T . hrl Dialogue Database , 1CSLP 90 , pp . 1093 1096 , 1990 .", "6 IIe , Y . Extended Viterbi Algorithm for Second Order Ilidden Markov Process , ICIR 88 , pp . 718 720 , 1988 .", "Ilisamitsu , T . and Nitta , Y . Morphological Analysis by Minimum Connetivc Cost Method , echnical Report S GNLC 90 8 , IEICE , pp . 17 24 , 1990 in Japanese .", "Jelinek , F . Self organized language modeling for speech recognition , IBM Report , 1985 Reprinted in Readings in Speech Recognition , 1 i . 450 506 .", "Matsunobu , E . , lIitaka , T . , and Yoshida , S . Syn t . actic Analysis by Stochastic P , UNSETSU Gram mar , Technical Rel ort SIGNL 56 3 , IPSJ , 1986 in Japanese .", "Tagging Text with a Probabilistic Moder , ICASSP 9I , pp . 809 812 , 1991 .", "Meteer , M . W . , Schwartz , R . and Weischedel , R . IOST Using l robal ilities in Language Process ing , lJCAI 9 t , pp . 960 965 , 1991 .", "Murakaini , J . and Sagayama , S . llidden Markov Model applied to Morphological Analysis , 45th National Meeting of the IPSJ , Vol . 3 , pp . 161 162 , 1992 in Japanese .", "and llital ? a , T . Japanese Word For marion Model and Its Evahmtion , Trans IPSJ , Vol . 34 , No . 9 , pp . 1944 1955 , 1993 in Japanese .", "Sakai , S . Morphological Category l ; igram A Single Language Model for botl , Spoken l , anguage and Text , ISS1 93 , I 1 . 87 90 , 1993 .", "Soong , F . K . aml lluang E . A Tree Trellis Based Fast Search for Finding the N Best Sen tence llypotheses in Continuous Speech Recogni tion , ICASS P 9 I , pp . 705 708 , 1991 .", "Yoshinmra , K , llitaka , T . , and Yoshida , S . Mor phological Analysis of Non marked off Japanese Sentences hy the Least llUNSETSUs Number Method , trans .", "I1 3 , Vol . 24 , No . l , pp . 40 46 , 19811 in Japanese .", "Yoshimura , K . , Takeuchi , M . , Tsuda , K . and Shudo , K . Morphological Analysis of Japanese Sent . ences Containing Unknown Words , Trans .", "IPSJ , Vol . 30 , No . 3 , pp . 294 301 , 1989 in Japanese ."], "summary_lines": ["A Stochastic Japanese Morphological Analyzer Using A Forward-DP Backward-A* N-Best Search Algorithm\n", "We present a novel method for segmenting the input sentence into words and assigning parts of speech to the words.\n", "It consists of a statistical language model and an efficient two-pass N-best search algorithm.\n", "The algorithm does not require delimiters between words.\n", "Thus it is suitable for written Japanese.\n", "The proposed Japanese morphological analyzer achieved 95.l% recall and 94.6% precision for open text when it was trained and tested on the ATR Corpus.\n", "We propose a method to search for the N best sets.\n"]}
{"article_lines": ["A Multi Pass Sieve for Coreference Resolution", "Most coreference resolution models determine if two mentions are coreferent using a single function over a set of constraints or features .", "This approach can lead to incorrect decisions as lower precision features often overwhelm the smaller number of high precision ones .", "To overcome this problem , we propose a simple coreference architecture based on a sieve that applies tiers of deterministic coreference models one at a time from highest to lowest precision .", "Each tier builds on the previous tier s entity cluster output .", "Further , our model propagates global information by sharing attributes e . g . , gender and number across mentions in the same cluster .", "This cautious sieve guarantees that stronger features are given precedence over weaker ones and that each decision is made using all of the information available at the time .", "The framework is highly modular new coreference modules can be plugged in without any change to the other modules .", "In spite of its simplicity , our approach outperforms many state of the art supervised and unsupervised models on several standard corpora .", "This suggests that sievebased approaches could be applied to other NLP tasks .", "Recent work on coreference resolution has shown that a rich feature space that models lexical , syntactic , semantic , and discourse phenomena is crucial to successfully address the task Bengston and Roth , 2008 ; Haghighi and Klein , 2009 ; Haghighi and Klein , 2010 .", "When such a rich representation is available , even a simple deterministic model can achieve state of the art performance Haghighi and Klein , 2009 .", "By and large most approaches decide if two mentions are coreferent using a single function over all these features and information local to the two mentions . 1 This is problematic for two reasons 1 lower precision features may overwhelm the smaller number of high precision ones , and 2 local information is often insufficient to make an informed decision .", "Consider this example The second attack occurred after some rocket firings aimed , apparently , toward the israelis , apparently in retaliation .", "we re checking our facts on that one .", ". . . the president , quoted by ari fleischer , his spokesman , is saying he s concerned the strike will undermine efforts by palestinian authorities to bring an end to terrorist attacks and does not contribute to the security of israel .", "Most state of the art models will incorrectly link we to the israelis because of their proximity and compatibility of attributes both we and the israelis are plural .", "In contrast , a more cautious approach is to first cluster the israelis with israel because the demonymy relation is highly precise .", "This initial clustering step will assign the correct animacy attribute inanimate to the corresponding geo political entity , which will prevent the incorrect merging with the mention we animate in later steps .", "We propose an unsupervised sieve like approach to coreference resolution that addresses these is1As we will discuss below , some approaches use an additional component to infer the overall best mention clusters for a document , but this is still based on confidence scores assigned using local information . sues .", "The approach applies tiers of coreference models one at a time from highest to lowest precision .", "Each tier builds on the entity clusters constructed by previous models in the sieve , guaranteeing that stronger features are given precedence over weaker ones .", "Furthermore , each model s decisions are richly informed by sharing attributes across the mentions clustered in earlier tiers .", "This ensures that each decision uses all of the information available at the time .", "We implemented all components in our approach using only deterministic models .", "All our components are unsupervised , in the sense that they do not require training on gold coreference links .", "The contributions of this work are the following We show that a simple scaffolding framework that deploys strong features through tiers of models performs significantly better than a single pass model .", "Additionally , we propose several simple , yet powerful , new features .", "This work builds upon the recent observation that strong features outweigh complex models for coreference resolution , in both supervised and unsupervised learning setups Bengston and Roth , 2008 ; Haghighi and Klein , 2009 .", "Our work reinforces this observation , and extends it by proposing a novel architecture that a allows easy deployment of such features , and b infuses global information that can be readily exploited by these features or constraints .", "Most coreference resolution approaches perform the task by aggregating local decisions about pairs of mentions Bengston and Roth , 2008 ; Finkel and Manning , 2008 ; Haghighi and Klein , 2009 ; Stoyanov , 2010 .", "Two recent works that diverge from this pattern are Culotta et al . 2007 and Poon and Domingos 2008 .", "They perform coreference resolution jointly for all mentions in a document , using first order probabilistic models in either supervised or unsupervised settings .", "Haghighi and Klein 2010 propose a generative approach that models entity clusters explicitly using a mostly unsupervised generative model .", "As previously mentioned , our work is not constrained by first order or Bayesian formalisms in how it uses cluster information .", "Additionally , the deterministic models in our tiered model are significantly simpler , yet perform generally better than the complex inference models proposed in these works .", "From a high level perspective , this work falls under the theory of shaping , defined as a method of successive approximations for learning Skinner , 1938 .", "This theory is known by different names in many NLP applications Brown et al . 1993 used simple models as stepping stones for more complex word alignment models ; Collins 1999 used cautious decision list learning for named entity classification ; Spitkovsky et al .", "2010 used baby steps for unsupervised dependency parsing , etc .", "To the best of our knowledge , we are the first to apply this theory to coreference resolution .", "Intra document coreference resolution clusters together textual mentions within a single document based on the underlying referent entity .", "Mentions are usually noun phrases NPs headed by nominal or pronominal terminals .", "To facilitate comparison with most of the recent previous work , we report results using gold mention boundaries .", "However , our approach does not make any assumptions about the underlying mentions , so it is trivial to adapt it to predicted mention boundaries e . g . , see Haghighi and Klein 2010 for a simple mention detection model .", "We used the following corpora for development and evaluation We used the first corpus ACE2004 ROTH DEV for development .", "The other corpora are reserved for testing .", "We parse all documents using the Stanford parser Klein and Manning , 2003 .", "The syntactic information is used to identify the mention head words and to define the ordering of mentions in a given sentence detailed in the next section .", "For a fair comparison with previous work , we do not use gold named entity labels or mention types but , instead , take the labels provided by the Stanford named entity recognizer NER Finkel et al . , 2005 .", "We use three evaluation metrics widely used in the literature a pairwise F1 Ghosh , 2003 computed over mention pairs in the same entity cluster ; b MUC Vilain et al . , 1995 which measures how many predicted clusters need to be merged to cover the gold clusters ; and c B3 Amit and Baldwin , 1998 which uses the intersection between predicted and gold clusters for a given mention to mark correct mentions and the sizes of the the predicted and gold clusters as denominators for precision and recall , respectively .", "We refer the interested reader to X . Luo , 2005 ; Finkel and Manning , 2008 for an analysis of these metrics .", "Our sieve framework is implemented as a succession of independent coreference models .", "We first describe how each model selects candidate mentions , and then describe the models themselves .", "Given a mention mi , each model may either decline to propose a solution in the hope that one of the subsequent models will solve it or deterministically select a single best antecedent from a list of previous mentions m1 , . . . , mi 1 .", "We sort candidate antecedents using syntactic information provided by the Stanford parser , as follows Same Sentence Candidates in the same sentence are sorted using left to right breadth first traversal of syntactic trees Hobbs , 1977 .", "Figure 1 shows an example of candidate ordering based on this traversal .", "The left to right ordering favors subjects , which tend to appear closer to the beginning of the sentence and are more probable antecedents .", "The breadthfirst traversal promotes syntactic salience by ranking higher noun phrases that are closer to the top of the parse tree Haghighi and Klein , 2009 .", "If the sentence containing the anaphoric mention contains multiple clauses , we repeat the above heuristic separately in each S constituent , starting with the one containing the mention .", "Previous Sentence For all nominal mentions we sort candidates in the previous sentences using rightto left breadth first traversal .", "This guarantees syntactic salience and also favors document proximity .", "For pronominal mentions , we sort candidates in previous sentences using left to right traversal in order to favor subjects .", "Subjects are more probable antecedents for pronouns Kertz et al . , 2006 .", "For example , this ordering favors the correct candidate pepsi for the mention they pepsi says it expects to double quaker s snack food growth rate . after a month long courtship , they agreed to buy quaker oats . . .", "In a significant departure from previous work , each model in our framework gets possibly incomplete clustering information for each mention from the earlier coreference models in the multi pass system .", "In other words , each mention mi may already be assigned to a cluster Cj containing a set of mentions Cj mj1 , . . . , mj ; mi E Cj .", "Unassigned mentions are unique members of their own cluster .", "We use this information in several ways Attribute sharing Pronominal coreference resolution discussed later in this section is severely affected by missing attributes which introduce precision errors because incorrect antecedents are selected due to missing information and incorrect attributes which introduce recall errors because correct links are not generated due to attribute mismatch between mention and antecedent .", "To address this issue , we perform a union of all mention attributes e . g . , number , gender , animacy in a given cluster and share the result with all cluster mentions .", "If attributes from different mentions contradict each other we maintain all variants .", "For example , our naive number detection assigns singular to the mention a group of students and plural to five students .", "When these mentions end up in the same cluster , the resulting number attributes becomes the set singular , plural .", "Thus this cluster can later be merged with both singular and plural pronouns .", "Mention selection Traditionally , a coreference model attempts to resolve every mention in the text , which increases the likelihood of errors .", "Instead , in each of our models , we exploit the cluster information received from the previous stages by resolving only mentions that are currently first in textual order in their cluster .", "For example , given the following ordered list of mentions , mi , m2 , m3 , m4 , m5 , m6 , where the superscript indicates cluster id , our model will attempt to resolve only m2 and m4 .", "These two are the only mentions that have potential antecedents and are currently marked as the first mentions in their clusters .", "The intuition behind this heuristic is two fold .", "First , early cluster mentions are usually better defined than subsequent ones , which are likely to have fewer modifiers or are pronouns Fox , 1993 .", "Several of our models use this modifier information .", "Second , by definition , first mentions appear closer to the beginning of the document , hence there are fewer antecedent candidates to select from , and fewer opportunities to make a mistake .", "Search Pruning Finally , we prune the search space using discourse salience .", "We disable coreference for first cluster mentions that a are or start with indefinite pronouns e . g . , some , other , or b start with indefinite articles e . g . , a , an .", "One exception to this rule is the model deployed in the first pass ; it only links mentions if their entire extents match exactly .", "This model is triggered for all nominal mentions regardless of discourse salience , because it is possible that indefinite mentions are repeated in a document when concepts are discussed but not instantiated , e . g . , a sports bar below We now describe the coreference models implemented in the sieve .", "For clarity , we summarize them in Table 1 and show the cumulative performance as they are added to the sieve in Table 2 .", "This model links two mentions only if they contain exactly the same extent text , including modifiers and determiners , e . g . , the Shahab 3 ground ground missile .", "As expected , this model is extremely precise , with a pairwise precision over 96 .", "This model links two mentions if any of the conditions below are satisfied Appositive the two nominal mentions are in an appositive construction , e . g . , Israel s Deputy Defense Minister , Ephraim Sneh , said . . . We use the same syntactic rules to detect appositions as Haghighi and Klein 2009 .", "Predicate nominative the two mentions nominal or pronominal are in a copulative subject object relation , e . g . , The New York based College Board is a nonprofit organization that administers the SATs and promotes higher education Poon and Domingos , 2008 .", "Role appositive the candidate antecedent is headed by a noun and appears as a modifier in an NP whose head is the current mention , e . g . , actress Rebecca Schaeffer .", "This feature is inspired by Haghighi and Klein 2009 , who triggered it only if the mention is labeled as a person by the NER .", "We constrain this heuristic more in our work we allow this feature to match only if a the mention is labeled as a person , b the antecedent is animate we detail animacy detection in Pass 7 , and c the antecedent s gender is not neutral .", "Relative pronoun the mention is a relative pronoun that modifies the head of the antecedent NP , e . g . , the finance street which has already formed in the Waitan district .", "Acronym both mentions are tagged as NNP and one of them is an acronym of the other , e . g . , Agence France Presse . . . AFP .", "We use a simple acronym detection algorithm , which marks a mention as an acronym of another if its text equals the sequence of upper case characters in the other mention .", "We will adopt better solutions for acronym detection in future work Schwartz , 2003 .", "Demonym one of the mentions is a demonym of the other , e . g . , Israel . . . Israeli .", "For demonym detection we use a static list of countries and their gentilic forms from Wikipedia . 3 All the above features are extremely precise .", "As shown in Table 2 the pairwise precision of the sieve after adding these features is over 95 and recall increases 5 points .", "Linking a mention to an antecedent based on the naive matching of their head words generates a lot of spurious links because it completely ignores possibly incompatible modifiers Elsner and Charniak , 2010 .", "For example , Yale University and Harvard University have similar head words , but they are obviously different entities .", "To address this issue , this pass implements several features that must all be matched in order to yield a link Cluster head match the mention head word matches any head word in the antecedent cluster .", "Note that this feature is actually more relaxed than naive head matching between mention and antecedent candidate because it is satisfied when the mention s head matches the head of any entity in the candidate s cluster .", "We constrain this feature by enforcing a conjunction with the features below .", "Word inclusion all the non stop4 words in the mention cluster are included in the set of non stop words in the cluster of the antecedent candidate .", "This heuristic exploits the property of discourse that it is uncommon to introduce novel information in later mentions Fox , 1993 .", "Typically , mentions of the same entity become shorter and less informative as the narrative progresses .", "For example , the two mentions in . . . intervene in the Florida Supreme Court s move . . . does look like very dramatic change made by the Florida court point to the same entity , but the two mentions in the text below belong to different clusters The pilot had confirmed . . . he had turned onto the correct runway but pilots behind him say he turned onto the wrong runway .", "Compatible modifiers only the mention s modifiers are all included in the modifiers of the antecedent candidate .", "This feature models the same discourse property as the previous feature , but it focuses on the two individual mentions to be linked , rather than their entire clusters .", "For this feature we only use modifiers that are nouns or adjectives .", "Not i within i the two mentions are not in an iwithin i construct , i . e . , one cannot be a child NP in the other s NP constituent Haghighi and Klein , 2009 .", "This pass continues to maintain high precision 91 pairwise while improving recall significantly over 6 points pairwise and almost 8 points MUC .", "Passes 4 and 5 are different relaxations of the feature conjunction introduced in Pass 3 , i . e . , Pass 4 removes the compatible modifiers only feature , while Pass 5 removes the word inclusion constraint .", "All in all , these two passes yield an improvement of 1 . 7 pairwise F1 points , due to recall improvements .", "Table 2 shows that the word inclusion feature is more precise than compatible modifiers only , but the latter has better recall .", "This pass relaxes the cluster head match heuristic by allowing the mention head to match any word in the cluster of the candidate antecedent .", "For example , this heuristic matches the mention Sanders to a cluster containing the mentions Sauls , the judge , Circuit Judge N . Sanders Sauls .", "To maintain high precision , this pass requires that both mention and antecedent be labeled as named entities and the types coincide .", "Furthermore , this pass implements a conjunction of the above features with word inclusion and not i within i .", "This pass yields less than 1 point improvement in most metrics .", "With one exception Pass 2 , all the previous coreference models focus on nominal coreference resolution .", "However , it would be incorrect to say that our framework ignores pronominal coreference in the first six passes .", "In fact , the previous models prepare the stage for pronominal coreference by constructing precise clusters with shared mention attributes .", "These are crucial factors for pronominal coreference .", "Like previous work , we implement pronominal coreference resolution by enforcing agreement constraints between the coreferent mentions .", "We use the following attributes for these constraints Number we assign number attributes based on a a static list for pronouns ; b NER labels mentions marked as a named entity are considered singular with the exception of organizations , which can be both singular or plural ; c part of speech tags NN S tags are plural and all other NN tags are singular ; and d a static dictionary from Bergsma and Lin , 2006 .", "Gender we assign gender attributes from static lexicons from Bergsma and Lin , 2006 ; Ji and Lin , 2009 .", "Person we assign person attributes only to pronouns .", "However , we do not enforce this constraint when linking two pronouns if one appears within quotes .", "This is a simple heuristic for speaker detection , e . g . , I and she point to the same person in I voted my conscience , she said .", "Animacy we set animacy attributes using a a static list for pronouns ; b NER labels , e . g . , PERSON is animate whereas LOCATION is not ; and c a dictionary boostrapped from the web Ji and Lin , 2009 .", "NER label from the Stanford NER .", "If we cannot detect a value , we set attributes to unknown and treat them as wildcards , i . e . , they can match any other value .", "This final model raises the pairwise recall of our system almost 22 percentage points , with only an 8 point drop in pairwise precision .", "Table 2 shows that similar behavior is measured for all other metrics .", "After all passes have run , we take the transitive closure of the generated clusters as the system output .", "We present the results of our approach and other relevant prior work in Table 3 .", "We include in the table all recent systems that report results under the same conditions as our experimental setup i . e . , using gold mentions and use the same corpora .", "We exclude from this analysis two notable works that report results only on a version of the task that includes finding mentions Haghighi and Klein , 2010 ; Stoyanov , 2010 .", "The Haghighi and Klein 2009 numbers have two variants with semantics S and without S .", "To measure the contribution of our multi pass system , we also present results from a single pass variant of our system that uses all applicable features from the multi pass system marked as single pass in the table .", "Our sieve model outperforms all systems on two out of the four evaluation corpora ACE2004ROTH DEV and ACE2004 NWIRE , on all metrics .", "On the corpora where our model is not best , it ranks a close second .", "For example , in ACE2004CULOTTA TEST our system has a B3 F1 score only . 4 points lower than Bengston and Roth 2008 and it outperforms all unsupervised approaches .", "In MUC6 TEST , our sieve s B3 F1 score is 1 . 8 points lower than Haghighi and Klein 2009 S , but it outperforms a supervised system that used gold named entity labels .", "Finally , the multi pass architecture always beats the equivalent single pass system with its contribution ranging between 1 and 4 F1 points depending on the corpus and evaluation metric .", "Our approach has the highest precision on all corpora , regardless of evaluation metric .", "We believe this is particularly useful for large scale NLP applications that use coreference resolution components , e . g . , question answering or information extraction .", "These applications can generally function without coreference information so it is beneficial to provide such information only when it is highly precise .", "The sieve model outperforms all other systems on at least two test sets , even though most of the other models are significantly richer .", "Amongst the comparisons , several are supervised Bengston and Roth , 2008 ; Finkel and Manning , 2008 ; Culotta et al . , 2007 .", "The system of Haghighi and Klein 2009 S uses a lexicon of semantically compatible noun pairs acquired transductively , i . e . , with knowledge of the mentions in the test set .", "Our system does not rely on labeled corpora for training like supervised approaches nor access to corpora during testing like Haghighi and Klein 2009 .", "The system that is closest to ours is Haghighi and Klein 2009 S .", "Like us , they use a rich set of features and deterministic decisions .", "However , theirs is a single pass model with a smaller feature set no cluster level , acronym , demonym , or animacy information .", "Table 3 shows that on the two corpora where results for this system are available , we outperform it considerably on all metrics .", "To understand if the difference is due to the multi pass architecture or the richer feature set we compared Haghighi and Klein , 2009 S against both our multi pass system and its single pass variant .", "The comparison indicates that both these contributions help our single pass system outperforms Haghighi and Klein 2009 consistently , and the multi pass architecture further improves the performance of our single pass system between 1 and 4 F1 points , depending on the corpus and evaluation metric .", "Recent unsupervised coreference work from Haghighi and Klein 2009 included a novel semantic component that matched related head words e . g . , AOL is a company learned from select wikipedia articles .", "They first identified articles relevant to the entity mentions in the test set , and then bootstrapped from known syntactic patterns for apposition and predicate nominatives in order to learn a database of related head pairs .", "They show impressive gains by using these learned pairs in coreference decisions .", "This type of learning using test set mentions is often described as transductive .", "Our work instead focuses on an approach that does not require access to the dataset beforehand .", "We thus did not include a similar semantic component in our system , given that running a bootstrapping learner whenever a new data set is encountered is not practical and , ultimately , reduces the usability of this NLP component .", "However , our results show that our sieve algorithm with minimal semantic information still performs as well as the Haghighi and Klein 2009 system with semantics .", "The sieve architecture offers benefits beyond improved accuracy .", "Its modular design provides a flexibility for features that is not available in most supervised or unsupervised systems .", "The sieve allows new features to be seamlessly inserted without affecting or even understanding the other components .", "For instance , once a new high precision feature or group of features is inserted as its own stage , it will benefit later stages with more precise clusters , but it will not interfere with their particular algorithmic decisions .", "This flexibility is in sharp contrast to supervised classifiers that require their models to be retrained on labeled data , and unsupervised systems that do not offer a clear insertion point for new features .", "It can be difficult to fully understand how a system makes a single decision , but the sieve allows for flexible usage with minimal effort .", "Table 4 shows the number of incorrect pair wise links generated by our system on the MUC6 TEST corpus .", "The table indicates that most of our errors are for nominal mentions .", "For example , the combined precision plus recall number of errors for proper or common noun mentions is three times larger than the number of errors made for pronominal mentions .", "The table also highlights that most of our errors are recall errors .", "There are eight times more recall errors than precision errors in our output .", "This is a consequence of our decision to prioritize highly precise features in the sieve .", "The above analysis illustrates that our next effort should focus on improving recall .", "In order to understand the limitations of our current system , we randomly selected 60 recall errors 20 for each mention type and investigated their causes .", "Not surprisingly , the causes are unique to each type .", "For proper nouns , 50 of recall errors are due to mention lengthening , mentions that are longer than their earlier mentions .", "For example , Washingtonbased USAir appears after USAir in the text , so our head matching components skip it because their high precision depends on disallowing new modifiers as the discourse proceeds .", "When the mentions were reversed as is the usual case , they match .", "The common noun recall errors are very different from proper nouns 17 of the 20 random examples can be classified as semantic knowledge .", "These errors are roughly evenly split between recognizing categories of names e . g . , Gitano is an organization name hence it should match the nominal antecedent the company , and understanding hypernym relations like settlements and agreements .", "Pronoun errors come in two forms .", "Roughly 40 of these errors are attribute mismatches involving sometimes ambiguous uses of gender and number e . g . , she with Pat Carney .", "Another 40 are not semantic or attribute based , but rather simply arise due to the order in which we check potential antecedents .", "In all these situations , the correct links are missed because the system chooses a closer incorrect antecedent .", "These four highlighted errors lengthening , semantics , attributes , ordering add up to 77 of all recall errors in the selected set .", "In general , each error type is particular to a specific mention type .", "This suggests that recall improvements can be made by focusing on one mention type without aversely affecting the others .", "Our sieve based approach to coreference uniquely allows for such new models to be seamlessly inserted .", "We presented a simple deterministic approach to coreference resolution that incorporates documentlevel information , which is typically exploited only by more complex , joint learning models .", "Our sieve architecture applies a battery of deterministic coreference models one at a time from highest to lowest precision , where each model builds on the previous model s cluster output .", "Despite its simplicity , our approach outperforms or performs comparably to the state of the art on several corpora .", "An additional benefit of the sieve framework is its modularity new features or models can be inserted in the system with limited understanding of the other features already deployed .", "Our code is publicly released5 and can be used both as a stand alone coreference system and as a platform for the development of future systems .", "The strong performance of our system suggests the use of sieves in other NLP tasks for which a variety of very high precision features can be designed and non local features can be shared ; likely candidates include relation and event extraction , template slot filling , and author name deduplication .", "We gratefully acknowledge the support of the Defense Advanced Research Projects Agency DARPA Machine Reading Program under Air Force Research Laboratory AFRL prime contract no .", "FA8750 09 C 0181 .", "Any opinions , findings , and conclusion or recommendations expressed in this material are those of the author s and do not necessarily reflect the view of DARPA , AFRL , or the US government .", "Many thanks to Jenny Finkel for writing a reimplementation of much of Haghighi and Klein 2009 , which served as the starting point for the work reported here .", "We also thank Nicholas Rizzolo and Dan Roth for helping us replicate their experimental setup , and Heng Ji and Dekang Lin for providing their gender lexicon ."], "summary_lines": ["A Multi-Pass Sieve for Coreference Resolution\n", "Most coreference resolution models determine if two mentions are coreferent using a single function over a set of constraints or features.\n", "This approach can lead to incorrect decisions as lower precision features often overwhelm the smaller number of high precision ones.\n", "To overcome this problem, we propose a simple coreference architecture based on a sieve that applies tiers of deterministic coreference models one at a time from highest to lowest precision.\n", "Each tier builds on the previous tier\u2019s entity cluster output.\n", "Further, our model propagates global information by sharing attributes (e.g., gender and number) across mentions in the same cluster.\n", "This cautious sieve guarantees that stronger features are given precedence over weaker ones and that each decision is made using all of the information available at the time.\n", "The framework is highly modular: new coreference modules can be plugged in without any change to the other modules.\n", "In spite of its simplicity, our approach outperforms many state-of-the-art supervised and unsupervised models on several standard corpora.\n", "This suggests that sieve-based approaches could be applied to other NLP tasks.\n", "Our rule based model obtains competitive result with less time.\n", "The candidate antecedents for the pronoun are ordered based on a notion of discourse salience that favors syntactic salience and document proximity.\n", "We develop accurate unsupervised systems that exploit simple but robust linguistic principles.\n"]}
{"article_lines": ["A SEMANTIC CONCORDANCE George A . Miller , Claudia Leacock , Randee Tengi , Ross T . Bunker Cogn i t ive Sc ience Laboratory Pr inceton Un ivers i ty Pr inceton , NJ 08542 ABSTRACT A semantic oncordance is a textual corpus and a lexicon So com bined that every substantive word in the text is linked to its appropriate nse in the lexicon .", "Thus it can be viewed either as a corpus in which words have been tagged syntactically and semanti cally , or as a lexicon in which example sentences can be found for many definitions .", "A semantic oncordance is being constructed to use in studies of sense resolution in context semantic disambigua tion .", "The Brown Corpus is the text and WordNet is the lexicon .", "Semantic tags pointers to WordNet synsets are inserted in the text manually using an interface , ConText , that was designed to facili tate the task .", "Another interface supports earches of the tagged text .", "Some practical uses for semantic oncordances are proposed .", "INTRODUCTION We wish to propose a new version of an old idea .", "Lexi cographers have traditionally based their work on a corpus of examples taken from approved usage , but considerations of cost usually limit published ictionaries to lexical entries having only a scattering of phrases to illustrate the usages from which definitions were derived .", "As a consequence of this economic pressure , most dictionaries are relatively weak in providing contextual information someone learning English as a second language will find in an English diction ary many alternative meanings for a common word , but little or no help in determining the linguistic ontexts in which the word can be used to express those different meanings .", "Today , however , large computer memories are affordable enough that this limitation can be removed ; it would now be feasible to publish a dictionary electronically along with all of the citation sentences on which it was based .", "The result ing combination would be more than a lexicon and more than a corpus ; we propose to call it a semantic concordance .", "If the corpus is some specific text , it is a specific semantic concordance ; ff the corpus includes many different exts , it is a universal semantic concordance .", "We have begun constructing a universal semantic oncor dance in conjunction with our work on a lexical database .", "The result can be viewed either as a collection of passages in which words have been tagged syntactically and semanti eally , or as a lexicon in which illustrative sentences can be found for many definitions .", "At the present time , the correla tion of a lexical meaning with examples in which a word is used to express that meaning must be done by hand .", "Manual semantic tagging is tedious ; it should be done automatically as soon as it is possible to resolve word senses in context automatically .", "It is hoped that the manual creation of a semantic oncordance will provide an appropriate environ ment for developing and testing those automatic procedures .", "WORDNET A LEX ICAL DATABASE The lexical component of the universal semantic oncor dance that we are constructing is WordNet , an on line lexi cal resource inspired by current psycholinguistic theories of haman lexical memory 1 , 2 .", "A standard , handheld iction ary is organized alphabetically ; it puts together words that are spelled alike and scatters words with related meanings .", "Although on line versions of such standard ictionaries can relieve a user of alphabetical searches , it is clearly inefficient to use a computer merely as a rapid page turner .", "WordNet is an example of a more efficient combination of traditional lexicography and modern computer science .", "The most ambitious feature of WordNet is the attempt o organize lexical information in terms of word meanings , rather than word forms .", "WordNet is organized by semantic relations rather than by semantic omponents within the open class categories of noun , verb , adjective , and adverb ; closed class categories of words pronouns , prepositions , conjunctions , etc .", "are not included in WordNet .", "The semantic relations among open class words include synonymy and antonymy which are semantic relations between words and which are found in all four syntactic categories ; hyponymy and hypernymy which are semantic relations between concepts and which organize nouns into a categorical hierarchy ; meronymy and holonymy which represent part whole relations among noun concepts ; and troponymy manner relations and entailment relations between verb concepts .", "These semantic relations were chosen to be intuitively obvious to nonlinguists and to have broad applicability throughout the lexicon .", "The basic elements of WordNet are sets of synonyms or synsets , which are taken to represent lexicalized concepts .", "A synset is a group of words that are synonymous , in the sense that there are contexts in which they can be inter changed without changing the meaning of the statement .", "For example , WordNet distinguishes between the synsets 303 board , plank , a stout length of sawn timber board , committee , agroup with supervisory powers In the context , He nailed a board across the entrance , the word plank can be substituted for board .", "In the con text , The board announced last quarters dividend , the word committee can be substituted for board .", "WordNet also provides sentence frames for each sense of every verb , indicating the kinds of simple constructions into which the verb can enter .", "WordNet contains only uninflected or base forms of words , so the interface to WordNet includes raorphy , a morpho logical analyzer that is applied to input strings to generate the base forms .", "For example , given went as the input string , rnorphy returns go ; given children , it returns child , etc .", "raorphy first checks an exception list ; if the input string is not found , it then uses standard rules of detachment .", "Words like fountain pen that are composed of two or more simpler words with spaces between them are called collocations .", "Since collocations are less polysemous than are individual words , their inclusion in WordNet promises to simplify the task of sense resolution .", "However , the mor phology of collocations poses certain problems .", "Special algorithms are required for inflected forms of some colloca tions for example , standing astride of will return the phrasal verb , stand astride of .", "As of the time this is written , WordNet contains more than 83 , 800 entries unique character strings , words and colloca tions and more than 63 , 300 lexicalized concepts synsets , plus defining glosses ; altogether there are more than 118 , 600 entry concept airs .", "The semantic relations are represented by more than 87 , 600 pointers between concepts .", "Approximately 43 of the entries are collocations .", "Approx imately 63 of the synsets include definitional glosses .", "And approximately 14 of the nouns and 25 of the verbs are polysemous .", "WordNet continues to grow at a rate of almost 1 , 000 con cepts a month .", "The task of semantic tagging has provided a useful stimulus to improve both coverage and precision .", "THE BROWN CORPUS The textual component of our universal semantic oncor dance is taken from the Brown Corpus 3 , 4 .", "The corpus was assembled at Brown University in 1963 64 under the direction of W . Nelson Francis with the intent of making it broadly representative of American English writing .", "It con tains 500 samples , each approximately 2 , 000 words long , for a total of approximately 1 , 014 , 000 running words of text , where a word is defined graphically as a string of con tiguous alphanumeric characters with a space at either end .", "The genres of writing range from newspaper reporting to technical writing , and from fiction to philosophical essays .", "The computer readable form of the Brown Corpus has been used in a wide variety of research studies , and many labora tories have obtained permission to use it .", "It was initially used for studies of word frequencies , and subsequently was made available with syntactic tags for each word .", "Since it is well known in a variety of contexts , and widely available , the Brown Corpus seemed agood place to begin .", "SEMANTIC TAGGING Two contrasting strategies for connecting a lexicon and a corpus emerge depending on where the process tarts .", "The targeted approach starts with the lexicon target a polysemous word , extract all sentences from the corpus in which that word occurs , categorize the instances and write definitions for each sense , and create a pointer between each instance of the word and its appropriate sense in the lexicon ; then target another word and repeat he process .", "The tar geted approach as the advantage that concentrating on a single word should produce better definitions it is , after all , the procedure that lexicographers egard as ideal .", "And it also makes immediately available a classification of sen tences that can be used to test alternative methods of automatic sense resolution .", "The alternative strategy starts with the corpus and proceeds through it word by word the sequential pproach .", "This pro cedure has the advantage of immediately revealing deficiencies in the lexicon not only missing words which could be found more directly , but also missing senses and indistinguishable definitions deficiencies that would not surface so quickly with the targeted approach .", "Since the promise of improvements in WordNet was a major motive for pursuing this research , we initially adopted the sequential approach for the bulk of our semantic tagging .", "A second advantage of the sequential pproach emerged as the work proceeded .", "One objective test of the adequacy of a lexicon is to use it to tag a sample of text , and to record the number of times it fails to have a word , or fails to have the appropriate sense for a word .", "We have found that such records for WordNet show considerable variability depend ing on the particular passage that is tagged , but over several months the averaged estimates of its coverage have been slowly improving coverage it is currently averaging a little better than 96 .", "CONTEXT A TAGGING INTERFACE The task of semantically tagging a text by hand is notori ously tedious , but the tedium can be reduced with an appropriate user interface .", "ConText is an X windows inter face designed specifically for annotating written texts with WordNet sense tags 5 .", "Since WordNet contains only open class words , ConText is used to tag only nouns , verbs , adjectives , and adverbs ; that is to say , only about 50 of the running words in the Brown Corpus are semantically tagged .", "304 Manual tagging with ConText requires a user to examine each word of the text in its context of use and to decide which WordNet sense was intended .", "In order to facilitate this task , ConText displays the word to be tagged in its con .", "text , along with the WordNet synsets for all of the senses of that word in the appropriate part of speech .", "For example , when the person doing the tagging reaches horse in the sentence The horse and men were saved , but the oxen drowned .", "ConText displays WordNet synsets for five meanings of noun horse 1 . sawhorse , horse , sawbuck , buck a framework used by carpenters 2 . knight , horse a chess piece 3 . horse a gymnastic apparatus 4 . heroin , diacetyl morphine , H , horse , junk , scag , smack a morphine derivative 5 . horse , Equus caballus herbivorous quadruped The tagger uses the cursor to indicate the appropriate sense 5 , in this example , at which point ConText attaches a label , or semantic tag , to that word in the text .", "ConText then moves on to men , the next content word , and the process repeats .", "If the word is missing , or ff the appropriate sense is missing , the tagger can insert comments calling for the necessary evisions of WordNet .", "Input to ConText In the current version of ConText , text to be tagged semanti cally must be preprocessed to indicate collocations and proper nouns by concatenating them with underscores and to provide syntactic tags .", "Since different corpora come in different formats and so requke slighdy different prepro cessing , we have not tried to incorporate the preprocessor into ConText itself .", "A tokenizer searches the input text for collocations that WordNet knows about and when one is found it is made into a unit by connecting its parts with underscores .", "For exam ple , if a text contains the collocation took place , the tok enizer will convert it to took_place .", "ConText can then display the synset for take place rather than successive synsets for take and place .", "Syntactic tags indicate the part of speech of each word in the input text .", "We have used an automatic syntactic tagger developed by Eric Brill 6 which he generously adapted to our needs .", "For example , store can be a noun or a verb ; when the syntactic tagger encounters an instance of store it tries to decide from the context whether it is being used as a noun or a verb .", "ConText then uses this syntactic tag to determine which part of speech to display to the user .", "Con Text also uses syntactic tags in order to skip over closed class words .", "Since the automatic syntactic tagger sometimes makes mistakes , ConText allows the user to change the part of speech that is being displayed , or to tag words that should not have been skipped .", "After the text has been syntactically tagged , all contiguous strings of proper nouns are joined with an underscore .", "For example , the string Mr . Charles C . Carpenter is output as Mr . _Charles_C . _Carpenter .", "Here , too , the user can manually correct any mistaken concatenations .", "An example may clarify what is involved in preprocessing .", "The 109th sentence inpassage k13 of the Brown Corpus is He went down the hall to Eugenes bathroom , to turn on the hot water heater , and on the side of the tub he saw a pair of blue wool swimming trunks .", "After preprocessing , this sentence is passed to ConText in the following form br kl3 109 He PP went_down VB the DT hall NN to TO Eugene NP POS s NN bathroom NN J , to TO turn_on VB the DT hot water NN heater NN , , and CC on IN the DT side NN of IN the DT tub NN he PP saw VBD a DT pair NN of IN blue JJ wool NN swimming_trunks NN . .", "The version displayed to the tagger , however , looks like the Brown Corpus , except that collocations are indicated by underscores .", "Note , incidentally , that the processor has made a mistake in this example went_down as in the ship went down is not the sense intendeed here .", "Output of ConText The output of ConText is a file containing the original text annotated with WordNet semantic tags ; semantic tags are given in square brackets , and denote the particular WordNet synset that is appropriate .", "For example , when hall is tagged with noun . artifact . l it means that the word is being used to express the concept defined by the synset containing hal l l in the noun . artifact file .", "Since WordNet is con stantly growing and changing , references to the lexicogra phers files have been retained ; if the lexical component were frozen , some more general identifier could be used instead .", "In cases where the appropriate sense of a word is not in WordNet , the user annotates that word with a com ment that is later sent to the appropriate lexicographer .", "After the lexicographer has edited WordNet , the text must be retagged .", "In the retag mode , ConText skips from one commented word to the next .", "In addition to the syntactic and semantic tags , ConText adds SGML markers and reformats the text one word to a line .", "The SGML markers delimit sentences s , sentence numbers stn , words in the text wd , base forms of text words mwd , comments cmt , proper nouns pn , part of speech tags tag and semantic tags sn or msn .", "The sentence preprocessed above might come out of ConText looking like this stn 109 stn 305 wd He wd tag PP tag wd went wd mwd go mwd msn verb . motion . 6 msnxtag VB tag wd down wd wd the w d tag DT tag wd hall wd sn noun . artifact .", "1 sn tag NN tag wd to wd tag TO tag wd Eugene wd pn person pn sn noun . Tops . 0 sn tag NP tag wd wd tag POS tag wd s wd tag NN tag wd bathroom wd sn noun . artifact . 0 sn tag NN tag wd , wd tag , tag wd to wd tag TO tag wd turn_on wd sn verb . contact . 0 sn tag VB tag wd the wd tag DT tag wd hot water heater wd cm t WORD_MIS ING cmt tag NN tag wd , wd tag , tag wd and wd tag CC tag wd on wd tag IN tag wd the w d tag DT tag wd side wd sn noun . location . 0 sn tag NN tag wd of wd tag IN tag wd the wd tag DT tag wd tub wd sn noun . artifact .", "1 sn tag NN tag wd he wd tag PP tag wd saw wd mwd sce mwd msn verb . perception . 0 msnxtag VBD tag wd a wd tag DT tag wd pair wd sn noun . quantity . 0 sn tag NN tag wd of wd tag IN tag wd blue wd sn adj . all . 0 . col . 3 sn tag JJ tag wd wool wd sn noun . artifact . 0 sn tag NN tag wd swimming_ . trunks wd sn noun . artifact . 0 sn tag NN tag wd . wd tag . tag s Note that the tokenizcrs mistaken linking of went_down has now been corrected by the tagger .", "Also note cmt WORD_MISSING cmt on line 16 of the output that comment indicates that the tagger has connected hot water and heater to form the collocation hot water heater , which was not in WordNet .", "This illustrates the kind of comments that are passed on to the lexicogra phers , who use them to edit or add to WordNet .", "The WordNet database is constantly growing and changing .", "Consequently , previously tagged texts must be updated periodically .", "In the update mode , ConText searches the tagged files for pointers to WordNet senses that have subse quently been revised .", "A new semantic tag must then be inserted by the tagger .", "5 . 3 Tracking As the number of semantically tagged files increased , the difficulty of keeping track of which files had beeen prepro cessed , which had been tagged , which were ready to be retagged , which had been retagged , and which were com plete and cleared for use made it necessary to create a mas ter traacking system that would handle the record keeping automatically .", "Scripts were written that allowed an adminis trator to preprocess files and add them to the tracking sys tem .", "Once files are in the tracking system , other scripts keep a log of all the tagging activities pertaining to each file , and insure that taggers will not try to perform operations that are invalid for files with a given status .", "The administrator can easily generate simple reports on the status of all files in the tracking system .", "QUERYING THE TAGGED TEXT A program to query the semantically tagged database has also been written p rsent print sentences allows a user to retrieve sentences by entering the base form of a word and its semantic tag .", "It was developed as a simple interface to the semantic oncordance , and puts the burden of know ing the words semantic tag on the user .", "This program is useful to the lexicographers , who are intimately familiar with WordNet semantic tags and who use it to find sample sentences .", "A more robust interface is needed , however .", "Presently under development is a comprehensive querying tool that will allow a user the flexibility of specifying vari ous retrieval criteria and display options .", "Envisioned is an X Windows application with two main windows one area for entering searching information and another for display ing the retrieved sentences .", "A primary search key is the only required component .", "Additional search keys can be specified to find words that co occur in sentences .", "This alone is a powerful improvement over p rsent .", "Other options will restrict or expand the retrieval , as listed here 1 .", "Search only given part s of speech .", "Search only for a specific sense .", "Expand search to include sentences for synonyms of search key .", "Expand search to include sentences for hyponyms of search key .", "Use primary key and all secondary keys , or primary key and any secondary key .", "Search for a secondary key that is within n words of the primary key .", "As important as specifying searching criteria is how the retrieved information is displayed .", "An option will be pro vided to display retrieved sentences in a concordance format all the target words vertically aligned and surrounded by context o the windows borders or left justified .", "Search keys will be highlighted in the retrieved sentences .", "306 Implementation f this program requires the creation of a master list of semantically tagged words .", "Each line in the alphabetized list contains the target word , its semantic tag , and for each sentence containing the word , a list of all the co occurring nouns , verbs , adjectives , and adverbs with numbers indicating their position in the sentence .", "For exam ple , the sentence already dissected provides a context for hall that might look like this hall 5 noun . artifact . l bathroom 10 noun . artifact . 0 ; hot water heater 15 noun . artifact . 0 ; side 19 noun . location . 0 ; tub 22 noun . artifact . l ; pair 25 noun . quantity . 0 ; wool 28 noun . artifact . 0 ; swimming_trunks 29 noun . artifact . 0 go 2 verb . motion . 6 ; turn_on 13 verb . contact . 0 ; see 23 verb . perception . 0 blue 27 adj . all . col . 3 Collecting entries for this sense of hall provides valuable information about he contexts in which it can occur .", "APPL ICAT IONS Our reasons for building this universal semantic oncor dance were to test and improve the coverage of WordNet and to develop resources for developing and testing pro cedures for the automatic sense resolution in context .", "It should be pointed out , however , that semantic oncordances can have other uses .", "Instruction Dictionaries are said to have evolved from the interlinear notations that medieval scholars added for difficult Latin words 7 .", "Such notations were found to be useful in teach ing students ; as the number of such notations grew , collec tions of them were extracted and arranged in lists .", "When the lists took on a life of their own their educational origins were largely forgotten .", "A semantic oncordance brings this story back to its origins lexical footnotes indicating the meaning that is appropriate to the context are immediately available lectronically .", "One obvious educational use of a semantic oncordance would be for people trying to learn English as a second language .", "By providing them with the appropriate sense of an unfamiliar word , they are spared the task of selecting a sense from the several alternatives listed in a standard ic tionary .", "Moreover , they can retrieve other sentences that illustrate the same usage of the word , and from such sen tences they can acquire both local and topical information about the use of a word 1 local information about the grammatical constructions in which that word can express the given concept , and 2 topical information about other words that are likely to be used when that concept is dis cussed .", "A use for specific semantic oncordances would be in sci ence education !", "much of the new learning demanded of beginning students in any field of science is terminological .", "Sense Frequencies Much attention has been paid to word frequencies , but rela tively little to the frequencies of occurrence of different meanings .", "Some lexicographers have atempted to order the senses of polysemous words from the most to the least fre quent , but the more general question has not been asked because the data for answering it have not been available .", "We have enough tagged text now , however , to get an idea what such data would look like .", "For example , here are prel irninary data for the 10 most frequent concepts expressed by nouns , based on some 80 selections from the Brown Corpus 172 year , timeperiod 144 person , individual , someone , man , mortal , human , soul , a human being 139 man , adult_male , a grown man 105 consequence , effect , outcome , result , upshot , a phenomenon that follows and is caused by some previous phenomenon 104 night , night_time , dark , time after sunset and before sunrise while it is dark outside 102 kind , sort , type , form , sculpture isa form of art or what kind of man is this ?", "94 eye , eyeball , oculus , optic , peeper , organ of sight 89 day , daytime , daylight , time after sunrise and before sunset while it is light outside 88 set , class , category , type , family , a collection of things haring acommon attribute 87 number , count , complement , adefinite quantity Our limited experience suggests , however , that such statis tics depend critically on the subject matter of the corpus that is used .", "Sense Co occurrences One shortcoming of WordNet that several users have pointed out to us is its lack of topical organization .", "Peter Mark Rogets original conception of his thesaurus relied heavily on his list of topics , which enabled him to pull together in one place all of the words used to talk about a given topic .", "This tradition of topical organization has sur vived in many modern thesauri , even though it requires a double look up by the reader .", "For example , under base ball a topically organized thesaurus would pull together words like batter , team , lineup , diamond , homer , hit , and so on .", "Topical organization obviously facilitates sense resolution if the topic is baseball , the mean ing of ball will differ from its meaning when the topic is , say , dancing .", "In WordNet , those same words are scattered about a baseball is an artifact , batters are people , a team is a group , a lineup is a list , a diamond is a location , a homer is 307 an act , to hit is a verb , and so on .", "By itself , WordNet does not provide topical groupings of words that can be used for sense resolution .", "One solution would be to draw up a list of topics and index all of the WordNet synsets to the topics in which they are likely to occur .", "Chapman 8 , for example , uses 1 , 073 such classes and categories .", "But such lists are necessarily arbi gary .", "A universal semantic oncordance should be able to accomplish the same result in a more natural way .", "That is to say , a passage discussing baseball would use words together in their baseball senses ; a passage discussing the drug trade would use words together with senses appropriate to that topic , and so on .", "Instead of a long list of topics , the corpus should include a large variety of passages .", "In order to take advantage of this aspect of universal seman tic concordances , it is necessary to be able to query the tex tual component for associated concepts .", "Data on sense co occurrences build up slowly , of course , but they will be a valuable by product of this line of work .", "Testing We are developing a version of the ConText interface that can be used for psychometric testing .", "The taggers task in using ConText resembles an extended multiple choice examination , and we believe that that feature can be adapted to test reading comprehension .", "Given a text that has already been tagged , readers comprehension can be tested by seeing whether they are able to choose correct senses on the basis of the contexts of use .", "No doubt here are other , even better uses for semantic on cordances .", "As the variety of potential applications grows , however , the need to automate the process of semantic tag ging will become ever more pressing .", "But we must begin with what we have .", "We are now finishing a first installment of semantically tagged text consisting of 100 passages from the Brown Corpus ; as soon as that much has been completed and satisfactorily cleaned up , we plan to make it , and the corresponding WordNet database , available to other labora tories that also have permission to use the Brown Corpus .", "We expect hat such distribution will stimulate further uses for semantic oncordances , uses that we have not yet ima gined .", "CONCLUSION The fact that we have control of the lexical component of our semantic oncordance enables us to shape the lexicon to fit the corpus .", "It would be possible , of course , to create a specific semantic oncordance with a lexicon limited strictly to the words occurring in the accompanying corpus .", "That constraint would have certain size advantages , but would miss the opportunity to build a single general exicon onto which a wide variety of corpora could be mapped .", "The universal semantic concordance described here has enabled us to improve WordNet and has given us a tool for our studies of sense resolution in context .", "In the course of this exercise , however , it has become apparent to us that cross referencing a lexicon and a textual corpus produces a hybrid resource that will be useful in a variety of practical and scientific applications .", "It has occurred to us that seman tic concordances might be even more useful if a richer syn tactic component could be incorporated , but how best to accomplish that is presently a question for the future .", "ACKNOWLEDGMENTS This work has been supported in part by Grant No .", "N00014 91 J 1634 from the Defense Advanced Research Projects Agency , Information and Technology Office , and the Office of Naval Research , and in part by grants from the James S . McDonnell Foundation and from the Pew Charit able Trusts .", "We are indebted to Henry Ku era nd W . Nel son Francis for permission to use the Brown Corpus in our research .", "And we are indebted for assistance and advice to Anthony Adler , Christiane Fellbaum , Kathy Garuba , Dawn Golding , Brian Gustafson , Benjamin Johnson Laird , Philip N . Johnson Laird , Shari Landes , Elyse Michaels , Katherine Miller , Jeff Tokazewski , and Pamela Wakefield .", "The desig nation , semantic oncordance , was suggested to us by Susan Chipman .", ", WordNet An on line lexical data base .", "International Journal of Lexicography special issue , 3 4 235 312 , 1990 .", "Miller , G . A . and Fellbaum , C . Semantic networks of English .", "Cognition special issue , 41 1 3 197 229 , 1991 .", "Ku era , H . and Francis , W . N . Computational nalysis of present day American English .", "Providence , RI Brown University Press , 1967 .", "Francis , W . N . and Ku era , H . Frequency analysis of English Usage Lexicon and Grammar .", "Boston , MA Houghton Mifflin , 1982 .", "Leacock , C . ConText A toot for semantic tagging of text Users guide .", "Cognitive Science Laboratory , Princeton University CSL Report No .", "54 , February 1993 .", "Brill , E . A simple rule based part of speech tagger .", "In Proceedings of Speech and Natural Language Workshop , 112 116 , February 1992 .", "San Mateo , CA Morgan Kaufman .", "Landauer , S . I . Dictionaries The art and craft of lexi cography .", "New York Scribners , 1984 .", "Rogets International Thesaurus , 5th edition .", "New York HarpcrCollins , 1992 ."], "summary_lines": ["A Semantic Concordance\n", "A semantic concordance is a textual corpus and a lexicon so combined that every substantive word in the text is linked to its appropriate sense in the lexicon.\n", "Thus it can be viewed either as a corpus in which words have been tagged syntactically and semantically, or as a lexicon in which example sentences can be found for many definitions.\n", "A semantic concordance is being constructed to use in studies of sense resolution in context (semantic disambiguation).\n", "The Brown Corpus is the text and WordNet is the lexicon.\n", "Semantic tags (pointers to WordNet synsets) are inserted in the text manually using an interface, ConText, that was designed to facilitate the task.\n", "Another interface supports searches of the tagged text.\n", "Some practical uses for semantic concordances are proposed.\n", "We present SemCor, a balanced, semantically annotated dataset, with all content words manually tagged by trained lexicographers.\n"]}
{"article_lines": ["A Smorgasbord Of Features For Statistical Machine Translation", "We describe a methodology for rapid experimentation in statistical machine translation which we use to add a large number of features to a baseline system exploiting features from a wide range of levels of syntactic representation .", "Feature values were combined in a log linear model to select the highest scoring candidate from an list .", "Feature weights were optimized directly against the BLEU evaluation metric on held out data .", "We present results for a small selection of features at each level of syntactic representation .", "Despite the enormous progress in machine translation MT due to the use of statistical techniques in recent years , state of the art statistical systems often produce translations with obvious errors .", "Grammatical errors include lack of a main verb , wrong word order , and wrong choice of function words .", "Frequent problems of a less grammatical nature include missing content words and incorrect punctuation .", "In this paper , we attempt to address these problems by exploring a variety of new features for scoring candidate translations .", "A high quality statistical translation system is our baseline , and we add new features to the existing set , which are then combined in a log linear model .", "To allow an easy integration of new features , the baseline system provides an n best list of candidate translations which is then reranked using the new features .", "This framework allows us to incorporate different types of features , including features based on syntactic analyses of the source and target sentences , which we hope will address the grammaticality of the translations , as well as lower level features .", "As we work on n best lists , we can easily use global sentence level features .", "We begin by describing our baseline system and the n best rescoring framework within which we conducted our experiments .", "We then present a selection of new features , progressing from word level features to those based to part of speech tags and syntactic chunks , and then to features based on Treebank based syntactic parses of the source and target sentences .", "The goal is the translation of a text given in some source language into a target language .", "We are given a source Chinese sentence f fJ1 f1 , . . . , fj , . . . , fJ , which is to be translated into a target English sentence e eI1 e1 , . . . , ei , . . . , eI Among all possible target sentences , we will choose the sentence with the highest probability As an alternative to the often used source channel approach Brown et al . , 1993 , we directly model the posterior probability Pr eI1 fJ1 Och and Ney , 2002 using a log linear combination of feature functions .", "In this framework , we have a set of M feature functions hm eI1 , fJ1 , m 1 , . . . , M . For each feature function , there exists a model parameter Am , m 1 , . . . , M . The direct translation probability is given by We obtain the following decision rule The standard criterion for training such a log linear model is to maximize the probability of the parallel training corpus consisting of S sentence pairs f fs , es s 1 , . . . , S .", "However , this does not guarantee optimal performance on the metric of translation quality by which our system will ultimately be evaluated .", "For this reason , we optimize the parameters directly against the BLEU metric on held out data .", "This is a more difficult optimization problem , as the search space is no longer convex .", "However , certain properties of the BLEU metric can be exploited to speed up search , as described in detail by Och 2003 .", "We use this method of optimizing feature weights throughout this paper .", "Our baseline MT system is the alignment template system described in detail by Och , Tillmann , and Ney 1999 and Och and Ney 2004 .", "In the following , we give a short description of this baseline model .", "The probability model of the alignment template system for translating a sentence can be thought of in distinct stages .", "First , the source sentence words fJ1 are grouped to phrases fK1 .", "For each phrase f an alignment template z is chosen and the sequence of chosen alignment templates is reordered according to \u03c0K1 .", "Then , every phrase f produces its translation e using the corresponding alignment template z .", "Finally , the sequence of phrases eK1 constitutes the sequence of words eI1 .", "Our baseline system incorporated the following feature functions Alignment Template Selection Each alignment template is chosen with probability p z f , estimated by relative frequency .", "The corresponding feature function in our log linear model is the log probability of the product of p z f for all used alignment templates used .", "Word Selection This feature is based on the lexical translation probabilities p e f , estimated using relative frequencies according to the highest probability wordlevel alignment for each training sentence .", "A translation probability conditioned on the source and target position within the alignment template p e f , i , j is interpolated with the position independent probability p e f .", "Phrase Alignment This feature favors monotonic alignment at the phrase level .", "It measures the amount of non monotonicity by summing over the distance in the source language of alignment templates which are consecutive in the target language .", "Language Model Features As a language model feature , we use a standard backing off word based trigram language model Ney , Generet , and Wessel , 1995 .", "The baseline system actually includes four different language model features trained on four different corpora the news part of the bilingual training data , a large Xinhua news corpus , a large AFP news corpus , and a set of Chinese news texts downloaded from the web .", "Word Phrase Penalty This word penalty feature counts the length in words of the target sentence .", "Without this feature , the sentences produced tend to be too short .", "The phrase penalty feature counts the number of phrases produced , and can allow the model to prefer either short or long phrases .", "Phrases from Conventional Lexicon The baseline alignment template system makes use of the ChineseEnglish lexicon provided by LDC .", "Each lexicon entry is a potential phrase translation pair in the alignment template system .", "To score the use of these lexicon entries which have no normal translation probability , this feature function counts the number of times such a lexicon entry is used .", "Additional Features A major advantage of the loglinear modeling approach is that it is easy to add new features .", "In this paper , we explore a variety of features based on successively deeper syntactic representations of the source and target sentences , and their alignment .", "For each of the new features discussed below , we added the feature value to the set of baseline features , re estimated feature weights on development data , and obtained results on test data .", "We worked with the Chinese English data from the recent evaluations , as both large amounts of sentence aligned training corpora and multiple gold standard reference translations are available .", "This is a standard data set , making it possible to compare results with other systems .", "In addition , working on Chinese allows us to use the existing Chinese syntactic treebank and parsers based on it .", "For the baseline MT system , we distinguish the following three different sentence or chunk aligned parallel training corpora most experiments described in this report this corpus consists of 993 sentences about 25K words in both languages .", "For development and test data , we have four English reference translations for each Chinese sentence .", "For each sentence in the development , test , and the blind test corpus a set of 16 , 384 different alternative translations has been produced using the baseline system .", "For extracting the n best candidate translations , an A search is used .", "These n best candidate translations are the basis for discriminative training of the model parameters and for re ranking .", "We used n best reranking rather than implementing new search algorithms .", "The development of efficient search algorithms for long range dependencies is very complicated and a research topic in itself .", "The reranking strategy enabled us to quickly try out a lot of new dependencies , which would not have been be possible if the search algorithm had to be changed for each new dependency .", "On the other hand , the use of n best list rescoring limits the possibility of improvements to what is available in the n best list .", "Hence , it is important to analyze the quality of the n best lists by determining how much of an improvement would be possible given a perfect reranking algorithm .", "We computed the oracle translations , that is , the set of translations from our n best list that yields the best BLEU score . 1 We use the following two methods to compute the BLEU score of an oracle translation 1Note that due to the corpus level holistic nature of the BLEU score it is not trivial to compute the optimal set of oracle translations .", "We use a greedy search algorithm for the oracle translations that might find only a local optimum .", "Empirically , we do not observe a dependence on the starting point , hence we believe that this does not pose a significant problem . n best list .", "The avBLEUr3 scores are computed with respect to three reference translations averaged over the four different choices of holding out one reference .", "The first method provides the theoretical upper bound of what BLEU score can be obtained by rescoring a given nbest list .", "Using this method with a 1000 best list , we obtain oracle translations that outperform the BLEU score of the human translations .", "The oracle translations achieve 113 against the human BLEU score on the test data Table 1 , while the first best translations obtain 79 . 2 against the human BLEU score .", "The second method uses a different references for selection and scoring .", "Here , using an 1000 best list , we obtain oracle translations with a relative human BLEU score of 88 . 5 .", "Based on the results of the oracle experiment , and in order to make rescoring computationally feasible for features requiring significant computation for each hypothesis , we used the top 1000 translation candidates for our experiments .", "The baseline system s BLEU score is 31 . 6 on the test set equivalent to the 1 best oracle in Table 1 .", "This is the benchmark against which the contributions of the additional features described in the remainder of this paper are to be judged .", "As a precursor to developing the various syntactic features described in this report , the syntactic representations on which they are based needed to be computed .", "This involved part of speech tagging , chunking , and parsing both the Chinese and English side of our training , development , and test sets .", "Applying the part of speech tagger to the often ungrammatical MT output from our n best lists sometimes led to unexpected results .", "Often the tagger tries to fix up ungrammatical sentences , for example by looking for a verb when none is present China NNP 14 CD open JJ border NN cities NNS achievements VBZ remarkable JJ Here , although achievements has never been seen as a verb in the tagger s training data , the prior for a verb in this position is high enough to cause a present tense verb tag to be produced .", "In addition to the inaccuracies of the MT system , the difference in genre from the tagger s training text can cause problems .", "For example , while our MT data include news article headlines with no verb , headlines are not included in the Wall Street Journal text on which the tagger is trained .", "Similarly , the tagger is trained on full sentences with normalized punctuation , leading it to expect punctuation at the end of every sentence , and produce a punctuation tag even when the evidence does not support it China NNP s POS economic JJ development NN and CC opening VBG up RP 14 CD border NN cities NNS remarkable JJ achievements .", "The same issues affect the parser .", "For example the parser can create verb phrases where none exist , as in the following example in which the tagger correctly did not identify a verb in the sentence These effects have serious implications for designing syntactic feature functions .", "Features such is there a verb phrase may not do what you expect .", "One solution would be features that involve the probability of a parse subtree or tag sequence , allowing us to ask how good a verb phrase is it ? Another solution is more detailed features examining more of the structure , such as is there a verb phrase with a verb ?", "These features , directly based on the source and target strings of words , are intended to address such problems as translation choice , missing content words , and incorrect punctuation .", "We used IBM Model 1 Brown et al . , 1993 as one of the feature functions .", "Since Model 1 is a bag of word translation model and it gives the sum of all possible alignment probabilities , a lexical co occurrence effect , or triggering effect , is expected .", "This captures a sort of topic or semantic coherence in translations .", "As defined by Brown et al . 1993 , Model 1 gives a probability of any given translation pair , which is We used GIZA to train the model .", "The training data is a subset 30 million words on the English side of the entire corpus that was used to train the baseline MT system .", "For a missing translation word pair or unknown words , where t fj ei 0 according to the model , a constant t fj ei 10 40 was used as a smoothing value .", "The average BLEU score average of the best four among different 20 search initial points is 32 . 5 .", "We also tried p e f ; M1 as feature function , but did not obtain improvements which might be due to an overlap with the word selection feature in the baseline system .", "The Model 1 score is one of the best performing features .", "It seems to fix the tendency of our baseline system to delete content words and it improves word selection coherence by the triggering effect .", "It is also possible that the triggering effect might work on selecting a proper verb noun combination , or a verb preposition combination .", "As shown in Figure 1 the alignment templates ATs used in the baseline system can appear in various configurations which we will call left right monotone and left right continuous .", "We built 2 out of these 4 models to distinguish two types of lexicalized re ordering of these ATs The left monotone model computes the total probability of all ATs being left monotone where the lower left corner of the AT touches the upper right corner of the previous AT .", "Note that the first word in the current AT may or may not immediately follow the last word in the previous AT .", "The total probability is the product over all alignment templates i , either P ATi is left monotone or 1 P ATi is left monotone .", "The right continuous model computes the total probability of all ATs being right continuous where the lower left corner of the AT touches the upper right corner of the previous AT and the first word in the current AT immediately follows the last word in the previous AT .", "The total probability is the product over all alignment templates i , either P ATi is right continuous or 1 P ATi is right continuous .", "In both models , the probabilities P have been estimated from the full training data train .", "By shallow syntax , we mean the output of the part ofspeech tagger and chunkers .", "We hope that such features can combine the strengths of tag and chunk based translation systems Schafer and Yarowsky , 2003 with our baseline system .", "This feature uses Chinese POS tag sequences as surrogates for Chinese words to model movement .", "Chinese words are too sparse to model movement , but an attempt to model movement using Chinese POS may be more successful .", "We hope that this feature will compensate for a weak model of word movement in the baseline system .", "Chinese POS sequences are projected to English using the word alignment .", "Relative positions are indicated for each Chinese tag .", "The feature function was also tried without the relative positions 14 measure open border cities The table shows an example tagging of an English hypothesis showing how it was generated from the Chinese sentence .", "The feature function is the log probability output by a trigram language model over this sequence .", "This is similar to the HMM Alignment model Vogel , Ney , and Tillmann , 1996 but in this case movement is calculated on the basis of parts of speech .", "The Projected POS feature function was one of the strongest performing shallow syntactic feature functions , with a BLEU score of 31 . 8 .", "This feature function can be thought of as a trade off between purely word based models , and full generative models based upon shallow syntax .", "Syntax based MT has shown promise in the work of , among others , Wu and Wong 1998 and Alshawi , Bangalore , and Douglas 2000 .", "We hope that adding features based on Treebank based syntactic analyses of the source and target sentences will address grammatical errors in the output of the baseline system .", "The most straightforward way to integrate a statistical parser in the system would be the use of the log of the parser probability as a feature function .", "Unfortunately , this feature function did not help to obtain better results it actually seems to significantly hurt performance .", "To analyze the reason for this , we performed an experiment to test if the used statistical parser assigns a higher probability to presumably grammatical sentences .", "The following table shows the average log probability assigned by the Collins parser to the 1 best produced , oracle and the reference translations We observe that the average parser log probability of the 1 best translation is higher than the average parse log probability of the oracle or the reference translations .", "Hence , it turns out that the parser is actually assigning higher probabilities to the ungrammatical MT output than to the presumably grammatical human translations .", "One reason for that is that the MT output uses fewer unseen words and typically more frequent words which lead to a higher language model probability .", "We also performed experiments to balance this effect by dividing the parser probability by the word unigram probability and using this normalized parser probability as a feature function , but also this did not yield improvements .", "A tree to string model is one of several syntaxbased translation models used .", "The model is a conditional probability p f T e .", "Here , we used a model defined by Yamada and Knight 2001 and Yamada and Knight 2002 .", "Internally , the model performs three types of operations on each node of a parse tree .", "First , it reorders the child nodes , such as changing VP VB NP PP into VP NP PP VB .", "Second , it inserts an optional word at each node .", "Third , it translates the leaf English words into Chinese words .", "These operations are stochastic and their probabilities are assumed to depend only on the node , and are independent of other operations on the node , or other nodes .", "The probability of each operation is automatically obtained by a training algorithm , using about 780 , 000 English parse tree Chinese sentence pairs .", "The probability of these operations \u03b8 ek , is assumed to depend on the edge of the tree being modified , eke , but independent of everything else , giving the following equation , where O varies over the possible alignments between the f and e and \u03b8 ekj is the particular operations in O for the edge eke .", "The model is further extended to incorporate phrasal translations performed at each node of the input parse tree Yamada and Knight , 2002 .", "An English phrase covered by a node can be directly translated into a Chinese phrase without regular reorderings , insertions , and leafword translations .", "The model was trained using about 780 , 000 English parse tree Chinese sentence pairs .", "There are about 3 million words on the English side , and they were parsed by Collins parser .", "Since the model is computationally expensive , we added some limitations on the model operations .", "As the base MT system does not produce a translation with a big word jump , we restrict the model not to reorder child nodes when the node covers more than seven words .", "For a node that has more than four children , the reordering probability is set to be uniform .", "We also introduced pruning , which discards partial subtree substring alignments if the probability is lower than a threshold .", "The model gives a sum of all possible alignment probabilities for a pair of a Chinese sentence and an English parse tree .", "We also calculate the probability of the best alignment according to the model .", "Thus , we have the folAs the model is computationally expensive , we sorted the n best list by the sentence length , and processed them from the shorter ones to the longer ones .", "We used 10 CPUs for about five days , and 273 997 development sentences and 237 878 test sentences were processed .", "The average BLEU score average of the best four among different 20 search initial points was 31 . 7 for both hTreeToStringSum and hTreeToStringViterbi .", "Among the processed development sentences , the model preferred the oracle sentences over the produced sentence in 61 of the cases .", "The biggest problem of this model is that it is computationally very expensive .", "It processed less than 30 of the n best lists in long CPU hours .", "In addition , we processed short sentences only .", "For long sentences , it is not practical to use this model as it is .", "A tree to tree translation model makes use of syntactic tree for both the source and target language .", "As in the tree to string model , a set of operations apply , each with some probability , to transform one tree into another .", "However , when training the model , trees for both the source and target languages are provided , in our case from the Chinese and English parsers .", "We began with the tree to tree alignment model presented by Gildea 2003 .", "The model was extended to handle dependency trees , and to make use of the word level alignments produced by the baseline MT system .", "The probability assigned by the tree to tree alignment model , given the word level alignment with which the candidate translation was generated , was used as a feature in our rescoring system .", "We trained the parameters of the tree transformation operations on 42 , 000 sentence pairs of parallel ChineseEnglish data from the Foreign Broadcast Information Service FBIS corpus .", "The lexical translation probabilities Pt were trained using IBM Model 1 on the 30 million word training corpus .", "This was done to overcome the sparseness of the lexical translation probabilities estimated while training the tree to tree model , which was not able to make use of as much training data .", "As a test of the tree to tree model s discrimination , we performed an oracle experiment , comparing the model scores on the first sentence in the n best list with candidate giving highest BLEU score .", "On the 1000 best list for the 993 sentence development set , restricting ourselves to sentences with no more than 60 words and a branching factor of no more than five in either the Chinese or English tree , we achieved results for 480 , or 48 of the 993 sentences .", "Of these 480 , the model preferred the produced over the oracle 52 of the time , indicating that it does not in fact seem likely to significantly improve BLEU scores when used for reranking .", "Using the probability of the source Chinese dependency parse aligning with the n best hypothesis dependency parse as a feature function , making use of the word level alignments , yields a 31 . 6 BLEU score identical to our baseline .", "The tree based feature functions described so far have the following limitations full parse tree models are expensive to compute for long sentences and for trees with flat constituents and there is limited reordering observed in the n best lists that form the basis of our experiments .", "In addition to this , higher levels of parse tree are rarely observed to be reordered between source and target parse trees .", "In this section we attack these problems using a simple Markov model for tree based alignments .", "It guarantees tractability compared to a coverage of approximately 30 of the n best list by the unconstrained tree based models , using the Markov model approach provides 98 coverage of the n best list .", "In addition , this approach is robust to inaccurate parse trees .", "The algorithm works as follows we start with word alignments and two parameters n for maximum number of words in tree fragment and k for maximum height of tree fragment .", "We proceed from left to right in the Chinese sentence and incrementally grow a pair of subtrees , one subtree in Chinese and the other in English , such that each word in the Chinese subtree is aligned to a word in the English subtree .", "We grow this pair of subtrees until we can no longer grow either subtree without violating the two parameter values n and k . Note that these aligned subtree pairs have properties similar to alignment templates .", "They can rearrange in complex ways between source and target .", "Figure 2 shows how subtree pairs for parameters n 3 and k 3 can be drawn for this sentence pair .", "In our experiments , we use substantially bigger tree fragments with parameters set to n 8 and k 9 .", "Once these subtree pairs have been obtained , we can easily assert a Markov assumption for the tree to tree and tree to string translation models that exploits these pairings .", "Let consider a sentence pair in which we have discovered n subtree pairs which we can call Frag0 , . . . , Fragn .", "We can then compute a feature function for the sentence pair using the tree to string translation model as follows the Tree to String model described in Section 6 . 2 we obtain a coverage improvement to 98 coverage from the original 30 .", "The accuracy of the tree to string model also improved with a BLEU score of 32 . 0 which is the best performing single syntactic feature .", "In this section , we consider another method for carving up the full parse tree .", "However , in this method , instead of subtree pairs we consider a decomposition of parse trees that provides each word with a fragment of the original parse tree as shown in Figure 3 .", "The formalism of TreeAdjoining Grammar TAG provides the definition what each tree fragment should be and in addition how to decompose the original parse trees to provide the fragments .", "Each fragment is a TAG elementary tree and the composition of these TAG elementary trees in a TAG derivation tree provides the decomposition of the parse trees .", "The decomposition into TAG elementary trees is done by augmenting the parse tree for source and target sentence with head word and argument or complement information using heuristics that are common to most contemporary statistical parsers and easily available for both English and Chinese .", "Note that we do not use the word alignment information for the decomposition into TAG elementary trees .", "Once we have a TAG elementary tree per word , we can create several models that score word alignments by exploiting the alignments between TAG elementary trees between source and target .", "Let tfi and tei be the TAG elementary trees associated with the aligned words fi and ei respectively .", "We experimented with two models over alignments unigram model over alignments ni P fi , tfi , ei , tei and conditional model Hi P ei , tei fi , tfi P fi 1 , tfi 1 fi , tfi We trained both of these models using the SRI Language Modeling Toolkit using 60K aligned parse trees .", "We extracted 1300 TAG elementary trees each for Chinese and for English .", "The unigram model gets a BLEU score of 31 . 7 and the conditional model gets a BLEU score of 31 . 9 . ture added to the baseline features on its own , and a combination of new features .", "The use of discriminative reranking of an n best list produced with a state of the art statistical MT system allowed us to rapidly evaluate the benefits of off the shelf parsers , chunkers , and POS taggers for improving syntactic well formedness of the MT output .", "Results are summarized in Table 2 ; the best single new feature improved the BLEU score from 31 . 6 to 32 . 5 .", "The 95 confidence intervals computed with the bootstrap resampling method are about 0 . 8 .", "In addition to experiments with single features we also integrated multiple features using a greedy approach where we integrated at each step the feature that most improves the BLEU score .", "This feature integration produced a statistically significant improvement of absolute 1 . 3 to 32 . 9 BLEU score .", "Our single best feature , and in fact the only single feature to produce a truly significant improvement , was the IBM Model 1 score .", "We attribute its success that it addresses the weakness of the baseline system to omit content words and that it improves word selection by employing a triggering effect .", "We hypothesize that this allows for better use of context in , for example , choosing among senses of the source language word .", "A major goal of this work was to find out if we can exploit annotated data such as treebanks for Chinese and English and make use of state of the art deep or shallow parsers to improve MT quality .", "Unfortunately , none of the implemented syntactic features achieved a statistically significant improvement in the BLEU score .", "Potential reasons for this might be tive to the grammaticality of MT output .", "This could not only make it difficult to see an improvement in the system s output , but also potentially mislead the BLEU based optimization of the feature weights .", "A significantly larger corpus for discriminative training and for evaluation would yield much smaller confidence intervals .", "Our discriminative training technique , which directly optimizes the BLEU score on a development corpus , seems to have overfitting problems with large number of features .", "One could use a larger development corpus for discriminative training or investigate alternative discriminative training criteria .", "The amount of annotated data that has been used to train the taggers and parsers is two orders of magnitude smaller than the parallel training data that has been used to train the baseline system or the word based features .", "Possibly , a comparable amount of annotated data e . g . a treebank with 100 million words is needed to obtain significant improvements .", "This is the first large scale integration of syntactic analysis operating on many different levels with a state of theart phrase based MT system .", "The methodology of using a log linear feature combination approach , discriminative reranking of n best lists computed with a state of the art baseline system allowed members of a large team to simultaneously experiment with hundreds of syntactic feature functions on a common platform .", "This material is based upon work supported by the National Science Foundation under Grant No ."], "summary_lines": ["A Smorgasbord Of Features For Statistical Machine Translation\n", "We describe a methodology for rapid experimentation in statistical machine translation which we use to add a large number of features to a baseline system exploiting features from a wide range of levels of syntactic representation.\n", "Feature values were combined in a log-linear model to select the highest scoring candidate translation from an n-best list.\n", "Feature weights were optimized directly against the BLEU evaluation metric on held-out data.\n", "We present results for a small selection of features at each level of syntactic representation.\n", "At the 2003 Johns Hopkins summer workshop on statistical machine translation, a large number of features were tested to discover which ones could improve a state-of-the-art translation system, and the only feature that produced a 'truly significant improvement' was the Model 1 score.\n", "The effects of integrating syntactic structure into a state-of-the-art statistical machine translation system are investigated.\n"]}
{"article_lines": ["A fully Bayesian approach to unsupervised part of speech tagging", "Unsupervised learning of linguistic structure is a difficult problem .", "A common approach is to define a generative model and maximize the probability of the hidden structure given the observed data .", "Typically , this is done using maximum likelihood estimation MLE of the model parameters .", "We show using part of speech tagging that a fully Bayesian approach can greatly improve performance .", "Rather than estimating a single set of parameters , the Bayesian approach integrates over all possible parameter values .", "This difference ensures that the learned structure will have high probability over a range of possible parameters , and permits the use of priors favoring the sparse distributions that are typical of natural language .", "Our model has the structure of a standard trigram HMM , yet its accuracy is closer to that of a state of the art discriminative model Smith and Eisner , 2005 , up to 14 percentage points better than MLE .", "We find improvements both when training from data alone , and using a tagging dictionary .", "Unsupervised learning of linguistic structure is a difficult problem .", "Recently , several new model based approaches have improved performance on a variety of tasks Klein and Manning , 2002 ; Smith and Eisner , 2005 .", "Nearly all of these approaches have one aspect in common the goal of learning is to identify the set of model parameters that maximizes some objective function .", "Values for the hidden variables in the model are then chosen based on the learned parameterization .", "Here , we propose a different approach based on Bayesian statistical principles rather than searching for an optimal set of parameter values , we seek to directly maximize the probability of the hidden variables given the observed data , integrating over all possible parameter values .", "Using part of speech POS tagging as an example application , we show that the Bayesian approach provides large performance improvements over maximum likelihood estimation MLE for the same model structure .", "Two factors can explain the improvement .", "First , integrating over parameter values leads to greater robustness in the choice of tag sequence , since it must have high probability over a range of parameters .", "Second , integration permits the use of priors favoring sparse distributions , which are typical of natural language .", "These kinds of priors can lead to degenerate solutions if the parameters are estimated directly .", "Before describing our approach in more detail , we briefly review previous work on unsupervised POS tagging .", "Perhaps the most well known is that of Merialdo 1994 , who used MLE to train a trigram hidden Markov model HMM .", "More recent work has shown that improvements can be made by modifying the basic HMM structure Banko and Moore , 2004 , using better smoothing techniques or added constraints Wang and Schuurmans , 2005 , or using a discriminative model rather than an HMM Smith and Eisner , 2005 .", "Non model based approaches have also been proposed Brill 1995 ; see also discussion in Banko and Moore 2004 .", "All of this work is really POS disambiguation learning is strongly constrained by a dictionary listing the allowable tags for each word in the text .", "Smith and Eisner 2005 also present results using a diluted dictionary , where infrequent words may have any tag .", "Haghighi and Klein 2006 use a small list of labeled prototypes and no dictionary .", "A different tradition treats the identification of syntactic classes as a knowledge free clustering problem .", "Distributional clustering and dimensionality reduction techniques are typically applied when linguistically meaningful classes are desired Sch utze , 1995 ; Clark , 2000 ; Finch et al . , 1995 ; probabilistic models have been used to find classes that can improve smoothing and reduce perplexity Brown et al . , 1992 ; Saul and Pereira , 1997 .", "Unfortunately , due to a lack of standard and informative evaluation techniques , it is difficult to compare the effectiveness of different clustering methods .", "In this paper , we hope to unify the problems of POS disambiguation and syntactic clustering by presenting results for conditions ranging from a full tag dictionary to no dictionary at all .", "We introduce the use of a new information theoretic criterion , variation of information Meil\u02c7a , 2002 , which can be used to compare a gold standard clustering to the clustering induced from a tagger s output , regardless of the cluster labels .", "We also evaluate using tag accuracy when possible .", "Our system outperforms an HMM trained with MLE on both metrics in all circumstances tested , often by a wide margin .", "Its accuracy in some cases is close to that of Smith and Eisner s 2005 discriminative model .", "Our results show that the Bayesian approach is particularly useful when learning is less constrained , either because less evidence is available corpus size is small or because the dictionary contains less information .", "In the following section , we discuss the motivation for a Bayesian approach and present our model and search procedure .", "Section 3 gives results illustrating how the parameters of the prior affect results , and Section 4 describes how to infer a good choice of parameters from unlabeled data .", "Section 5 presents results for a range of corpus sizes and dictionary information , and Section 6 concludes .", "In model based approaches to unsupervised language learning , the problem is formulated in terms of identifying latent structure from data .", "We define a model with parameters 0 , some observed variables w the linguistic input , and some latent variables t the hidden structure .", "The goal is to assign appropriate values to the latent variables .", "Standard approaches do so by selecting values for the model parameters , and then choosing the most probable variable assignment based on those parameters .", "For example , maximum likelihood estimation MLE seeks parameters 0 such that where P w 0 P w , t 0 .", "Sometimes , a non uniform prior distribution over 0 is introduced , in which case 0 is the maximum a posteriori MAP solution for 0 The values of the latent variables are then taken to be those that maximize P t w , 0 .", "In contrast , the Bayesian approach we advocate in this paper seeks to identify a distribution over latent variables directly , without ever fixing particular values for the model parameters .", "The distribution over latent variables given the observed data is obtained by integrating over all possible values of 0 This distribution can be used in various ways , including choosing the MAP assignment to the latent variables , or estimating expected values for them .", "To see why integrating over possible parameter values can be useful when inducing latent structure , consider the following example .", "We are given a coin , which may be biased t 1 or fair t 0 , each with probability . 5 .", "Let 0 be the probability of heads .", "If the coin is biased , we assume a uniform distribution over 0 , otherwise 0 . 5 .", "We observe w , the outcomes of 10 coin flips , and we wish to determine whether the coin is biased i . e . the value of t .", "Assume that we have a uniform prior on B , with p B 1 for all B 0 , 1 .", "First , we apply the standard methodology of finding the MAP estimate for B and then selecting the value of t that maximizes P t w , B .", "In this case , an elementary calculation shows that the MAP estimate is B nH 10 , where nH is the number of heads in w likewise , nT is the number of tails .", "Consequently , P t w , B favors t 1 for any sequence that does not contain exactly five heads , and assigns equal probability tot 1 and t 0 for any sequence that does contain exactly five heads a counterintuitive result .", "In contrast , using some standard results in Bayesian analysis we can show that applying Equation 3 yields approach is sensitive to the robustness of a choice of t to the value of B , as illustrated in Figure 1 .", "Even though a sequence Figure 1 a , P t B is only greater than 0 . 5 for a small range of B around B Figure 1 b , meaning that the choice oft 1 is not very robust to variation in B .", "In contrast , a sequence with nH 8 favors t 1 for a wide range of B around B .", "By integrating over B , Equation 3 takes into account the consequences of possible variation in B .", "Another advantage of integrating over B is that it permits the use of linguistically appropriate priors .", "In many linguistic models , including HMMs , the distributions over variables are multinomial .", "For a multinomial with parameters B . . . , BK , a natural choice of prior is the K dimensional Dirichlet distribution , which is conjugate to the For simplicity , we initially assume that all K parameters also known as hyperparameters of the Dirichlet distribution are equal to Q , i . e . the Diri chlet is symmetric .", "The value of Q determines which parameters B will have high probability when Q 1 , all parameter values are equally likely ; when Q 1 , multinomials that are closer to uniform are prior is conjugate to a distribution if the posterior has the same form as the pri d B as a function of B . mation .", "For a sequence of draws x . . . , xn from a multinomial distribution B with observed counts . . . , nK , a symmetric prior over B yields the MAP estimate Bk When Q 1 , standard MLE techniques such as EM can be used to find the MAP estimate simply by adding of size Q 1 to each of the expected counts nk at each iteration .", "However , when Q 1 , the values of B that set one or more of the Bk equal to 0 can have infinitely high posterior probability , meaning that MAP estimation can yield degenerate solutions .", "If , instead of estimating B , we integrate over all possible values , we no longer encounter such difficulties .", "Instead , the probability that outcome xi value of a latent variable , t , from observed data , w , chooses a value of t robust to uncertainty in B .", "a Posterior distribution on B given w . b Probability preferred ; and when Q 1 , high probability is assigned to sparse multinomials , where one or more parameters are at or near 0 .", "Typically , linguistic structures are characterized by sparse distributions e . g . , POS tags are followed with high probability by only a few other tags , and have highly skewed output distributions .", "Consequently , it makes sense to use a Dirichlet prior with Q 1 .", "However , as noted by Johnson et al . 2007 , this choice of Q leads to difficulties with MAP estiwhere nk is the number of times k occurred in x i .", "2 . 3 Inference See MacKay and Peto 1995 for a derivation .", "To perform inference in our model , we use Gibbs 2 . 2 Model Definition sampling Geman and Geman , 1984 , a stochastic Our model has the structure of a standard trigram procedure that produces samples from the posterior HMM , with the addition of symmetric Dirichlet pri distribution P t w , \u03b1 , \u03b2 a P w t , \u03b2 P t \u03b1 .", "We ors over the transition and output distributions initialize the tags at random , then iteratively resamti ti 1 t , ti 2 t , \u03c4 t , t Mult \u03c4 t , t ple each tag according to its conditional distribution wi ti t , \u03c9 t Mult \u03c9 t given the current values of all other tags .", "Exchange\u03c4 t , t \u03b1 Dirichlet \u03b1 ability allows us to treat the current counts of the \u03c9 t \u03b2 Dirichlet \u03b2 other tag trigrams and outputs as previous obserwhere ti and wi are the ith tag and word .", "We assume vations .", "The only complication is that resampling that sentence boundaries are marked with a distin a tag changes the identity of three trigrams at once , guished tag .", "For a model with T possible tags , each and we must account for this in computing its condiof the transition distributions \u03c4 t , t has T compo tional distribution .", "The sampling distribution for ti nents , and each of the output distributions \u03c9 t has is given in Figure 2 .", "Wt components , where Wt is the number of word In Bayesian statistical inference , multiple samples types that are permissible outputs for tag t . We will from the posterior are often used in order to obtain use \u03c4 and \u03c9 to refer to the entire transition and out statistics such as the expected values of model variput parameter sets .", "This model assumes that the ables .", "For POS tagging , estimates based on multiprior over state transitions is the same for all his ple samples might be useful if we were interested in , tories , and the prior over output distributions is the for example , the probability that two words have the same for all states .", "We relax the latter assumption in same tag .", "However , computing such probabilities Section 4 . across all pairs of words does not necessarily lead to Under this model , Equation 5 gives us a consistent clustering , and the result would be diffin ti 2 , ti 1 , ti \u03b1 cult to evaluate .", "Using a single sample makes stanP ti t i , \u03b1 6 dard evaluation methods possible , but yields subn ti 2 , ti 1 T\u03b1 optimal results because the value for each tag is samn ti , wi \u03b2 pled from a distribution , and some tags will be asP wi ti , t i , w i , \u03b2 7 signed low probability values .", "Our solution is to n ti Wti\u03b2 treat the Gibbs sampler as a stochastic search prowhere n ti 2 , ti 1 , ti and n ti , wi are the number of cedure with the goal of identifying the MAP tag seoccurrences of the trigram ti 2 , ti 1 , ti and the quence .", "This can be done using tempering annealtag word pair ti , wi in the i 1 previously gener ing , where a temperature of \u03c6 is equivalent to raisated tags and words .", "Note that , by integrating out ing the probabilities in the sampling distribution to the parameters \u03c4 and \u03c9 , we induce dependencies the power of 1 \u03c6 .", "As \u03c6 approaches 0 , even a single between the variables in the model .", "The probabil sample will provide a good MAP estimate . ity of generating a particular trigram tag sequence 3 Fixed Hyperparameter Experiments likewise , output depends on the number of times 3 . 1 Method that sequence output has been generated previ Our initial experiments follow in the tradition begun ously .", "Importantly , trigrams and outputs remain by Merialdo 1994 , using a tag dictionary to conexchangeable the probability of a set of trigrams strain the possible parts of speech allowed for each outputs is the same regardless of the order in which word .", "This also fixes Wt , the number of possible it was generated .", "The property of exchangeability is words for tag t . The dictionary was constructed by crucial to the inference algorithm we describe next . listing , for each word , all tags found for that word in 747 the entire WSJ treebank .", "For the experiments in this section , we used a 24 , 000 word subset of the treebank as our unlabeled training corpus .", "54 . 5 of the tokens in this corpus have at least two possible tags , with the average number of tags per token being 2 . 3 .", "We varied the values of the hyperparameters \u03b1 and Q and evaluated overall tagging accuracy .", "For comparison with our Bayesian HMM BHMM in this and following sections , we also present results from the Viterbi decoding of an HMM trained using MLE by running EM to convergence MLHMM .", "Where direct comparison is possible , we list the scores reported by Smith and Eisner 2005 for their conditional random field model trained using contrastive estimation CRF CE . 2 For all experiments , we ran our Gibbs sampling algorithm for 20 , 000 iterations over the entire data set .", "The algorithm was initialized with a random tag assignment and a temperature of 2 , and the temperature was gradually decreased to . 08 .", "Since our inference procedure is stochastic , our reported results are an average over 5 independent runs .", "Results from our model for a range of hyperparameters are presented in Table 1 .", "With the best choice of hyperparameters \u03b1 . 003 , Q 1 , we achieve average tagging accuracy of 86 . 8 .", "This far surpasses the MLHMM performance of 74 . 5 , and is closer to the 90 . 1 accuracy of CRF CE on the same data set using oracle parameter selection .", "The effects of \u03b1 , which determines the probabil2Results of CRF CE depend on the set of features used and the contrast neighborhood .", "In all cases , we list the best score reported for any contrast neighborhood using trigram but no spelling features .", "To ensure proper comparison , all corpora used in our experiments consist of the same randomized sets of sentences used by Smith and Eisner .", "Note that training on sets of contiguous sentences from the beginning of the treebank consistently improves our results , often by 1 2 percentage points or more .", "MLHMM scores show less difference between randomized and contiguous corpora .", "BHMM as a function of the hyperparameters \u03b1 and Q .", "Results are averaged over 5 runs on the 24k corpus with full tag dictionary .", "Standard deviations in most cases are less than . 5 . ity of the transition distributions , are stronger than the effects of Q , which determines the probability of the output distributions .", "The optimal value of . 003 for \u03b1 reflects the fact that the true transition probability matrix for this corpus is indeed sparse .", "As \u03b1 grows larger , the model prefers more uniform transition probabilities , which causes it to perform worse .", "Although the true output distributions tend to be sparse as well , the level of sparseness depends on the tag consider function words vs . content words in particular .", "Therefore , a value of Q that accurately reflects the most probable output distributions for some tags may be a poor choice for other tags .", "This leads to the smaller effect of Q , and suggests that performance might be improved by selecting a different Q for each tag , as we do in the next section .", "A final point worth noting is that even when \u03b1 Q 1 i . e . , the Dirichlet priors exert no influence the BHMM still performs much better than the MLHMM .", "This result underscores the importance of integrating over model parameters the BHMM identifies a sequence of tags that have high probability over a range of parameter values , rather than choosing tags based on the single best set of parameters .", "The improved results of the BHMM demonstrate that selecting a sequence that is robust to variations in the parameters leads to better performance .", "In our initial experiments , we experimented with different fixed values of the hyperparameters and reported results based on their optimal values .", "However , choosing hyperparameters in this way is timeconsuming at best and impossible at worst , if there is no gold standard available .", "Luckily , the Bayesian approach allows us to automatically select values for the hyperparameters by treating them as additional variables in the model .", "We augment the model with priors over the hyperparameters here , we assume an improper uniform prior , and use a single Metropolis Hastings update Gilks et al . , 1996 to resample the value of each hyperparameter after each iteration of the Gibbs sampler .", "Informally , to update the value of hyperparameter \u03b1 , we sample a proposed new value \u03b1 from a normal distribution with p \u03b1 and a . 1\u03b1 .", "The probability of accepting the new value depends on the ratio between P t w , \u03b1 and P t w , \u03b1 and a term correcting for the asymmetric proposal distribution .", "Performing inference on the hyperparameters allows us to relax the assumption that every tag has the same prior on its output distribution .", "In the experiments reported in the following section , we used two different versions of our model .", "The first version BHMM1 uses a single value of Q for all word classes as above ; the second version BHMM2 uses a separate Qj for each tag class j .", "In this set of experiments , we used the full tag dictionary as above , but performed inference on the hyperparameters .", "Following Smith and Eisner 2005 , we trained on four different corpora , consisting of the first 12k , 24k , 48k , and 96k words of the WSJ corpus .", "For all corpora , the percentage of ambiguous tokens is 54 55 and the average number of tags per token is 2 . 3 .", "Table 2 shows results for the various models and a random baseline averaged by the various models on different sized corpora .", "BHMM1 and BHMM2 use hyperparameter inference ; CRF CE uses parameter selection based on an unlabeled development set .", "Standard deviations a for the BHMM results fell below those shown for each corpus size . over 5 random tag assignments .", "Hyperparameter inference leads to slightly lower scores than are obtained by oracle hyperparameter selection , but both versions of BHMM are still far superior to MLHMM for all corpus sizes .", "Not surprisingly , the advantages of BHMM are most pronounced on the smallest corpus the effects of parameter integration and sensible priors are stronger when less evidence is available from the input .", "In the limit as corpus size goes to infinity , the BHMM and MLHMM will make identical predictions .", "In unsupervised learning , it is not always reasonable to assume that a large tag dictionary is available .", "To determine the effects of reduced or absent dictionary information , we ran a set of experiments inspired by those of Smith and Eisner 2005 .", "First , we collapsed the set of 45 treebank tags onto a smaller set of 17 the same set used by Smith and Eisner .", "We created a full tag dictionary for this set of tags from the entire treebank , and also created several reduced dictionaries .", "Each reduced dictionary contains the tag information only for words that appear at least d times in the training corpus the 24k corpus , for these experiments .", "All other words are fully ambiguous between all 17 classes .", "We ran tests with d 1 , 2 , 3 , 5 , 10 , and oc i . e . , knowledge free syntactic clustering .", "With standard accuracy measures , it is difficult to variation of information between clusterings induced by the assigned and gold standard tags as the amount of information in the dictionary is varied .", "Standard deviations Q for the BHMM results fell below those shown in each column .", "The percentage of ambiguous tokens and average number of tags per token for each value of d is also shown . evaluate the quality of a syntactic clustering when no dictionary is used , since cluster names are interchangeable .", "We therefore introduce another evaluation measure for these experiments , a distance metric on clusterings known as variation of information Meil\u02c7a , 2002 .", "The variation of information VI between two clusterings C the gold standard and C the found clustering of a set of data points is a sum of the amount of information lost in moving from C to C , and the amount that must be gained .", "It is defined in terms of entropy H and mutual information I V I C , C H C H C 2I C , C .", "Even when accuracy can be measured , VI may be more informative two different tag assignments may have the same accuracy but different VI with respect to the gold standard if the errors in one assignment are less consistent than those in the other .", "Table 3 gives the results for this set of experiments .", "One or both versions of BHMM outperform MLHMM in terms of tag accuracy for all values of d , although the differences are not as great as in earlier experiments .", "The differences in VI are more striking , particularly as the amount of dictionary information is reduced .", "When ambiguity is greater , both versions of BHMM show less confusion with respect to the true tags than does MLHMM , and BHMM2 performs the best in all circumstances .", "The confusion matrices in Figure 3 provide a more intuitive picture of the very different sorts of clusterings produced by MLHMM and BHMM2 when no tag dictionary is available .", "Similar differences hold to a lesser degree when a partial dictionary is provided .", "With MLHMM , different tokens of the same word type are usually assigned to the same cluster , but types are assigned to clusters more or less at random , and all clusters have approximately the same number of types 542 on average , with a standard deviation of 174 .", "The clusters found by BHMM2 tend to be more coherent and more variable in size in the 5 runs of BHMM2 , the average number of types per cluster ranged from 436 to 465 i . e . , tokens of the same word are spread over fewer clusters than in MLHMM , with a standard deviation between 460 and 674 .", "Determiners , prepositions , the possessive marker , and various kinds of punctuation are mostly clustered coherently .", "Nouns are spread over a few clusters , partly due to a distinction found between common and proper nouns .", "Likewise , modal verbs and the copula are mostly separated from other verbs .", "Errors are often sensible adjectives and nouns are frequently confused , as are verbs and adverbs .", "The kinds of results produced by BHMM1 and BHMM2 are more similar to each other than to the results of MLHMM , but the differences are still informative .", "Recall that BHMM1 learns a single value for Q that is used for all output distributions , while BHMM2 learns separate hyperparameters for each cluster .", "This leads to different treatments of difficult to classify low frequency items .", "In BHMM1 , these items tend to be spread evenly among all clusters , so that all clusters have similarly sparse output distributions .", "In BHMM2 , the system creates one or two clusters consisting entirely of very infrequent items , where the priors on these clusters strongly prefer uniform outputs , and all other clusters prefer extremely sparse outputs and are more coherent than in BHMM1 .", "This explains the difference in VI between the two systems , as well as the higher accuracy of BHMM1 for d 3 the single Q discourages placing lowfrequency items in their own cluster , so they are more likely to be clustered with items that have similar transition probabilities .", "The problem of junk clusters in BHMM2 might be alleviated by using a non uniform prior over the hyperparameters to encourage some degree of sparsity in all clusters .", "In this paper , we have demonstrated that , for a standard trigram HMM , taking a Bayesian approach to POS tagging dramatically improves performance over maximum likelihood estimation .", "Integrating over possible parameter values leads to more robust solutions and allows the use of priors favoring sparse distributions .", "The Bayesian approach is particularly helpful when learning is less constrained , either because less data is available or because dictionary information is limited or absent .", "For knowledgefree clustering , our approach can also be extended through the use of infinite models so that the number of clusters need not be specified in advance .", "We hope that our success with POS tagging will inspire further research into Bayesian methods for other natural language learning tasks ."], "summary_lines": ["A fully Bayesian approach to unsupervised part-of-speech tagging\n", "Unsupervised learning of linguistic structure is a difficult problem.\n", "A common approach is to define a generative model and maximize the probability of the hidden structure given the observed data.\n", "Typically, this is done using maximum-likelihood estimation (MLE) of the model parameters.\n", "We show using part-of-speech tagging that a fully Bayesian approach can greatly improve performance.\n", "Rather than estimating a single set of parameters, the Bayesian approach integrates over all possible parameter values.\n", "This difference ensures that the learned structure will have high probability over a range of possible parameters, and permits the use of priors favoring the sparse distributions that are typical of natural language.\n", "Our model has the structure of a standard trigram HMM, yet its accuracy is closer to that of a state-of-the-art discriminative model (Smith and Eisner, 2005), up to 14 percentage points better than MLE.\n", "We find improvements both when training from data alone, and using a tagging dictionary.\n", "In our model, the transition, emission and coupling parameters are governed by Dirichlet priors, and a token-level collapsed Gibbs sampler is used for inference.\n"]}
{"article_lines": ["Applying Many to Many Alignments and Hidden Markov Models to Letter to Phoneme Conversion", "Letter to phoneme conversion generally requires aligned training data of letters and phonemes .", "Typically , the alignments are limited to one to one alignments .", "We present a novel technique of training with many to many alignments .", "A letter chunking bigram prediction manages double letters and double phonemes automatically as opposed to preprocessing with fixed lists .", "We also apply an HMM method in conjunction with a local classification model to predict a global phoneme sequence given a word .", "The many to many alignments result in significant improvements over the traditional one to one approach .", "Our system achieves state of the art performance on several languages and data sets .", "Letter to phoneme L2P conversion requires a system to produce phonemes that correspond to a given written word .", "Phonemes are abstract representations of how words should be pronounced in natural speech , while letters or graphemes are representations of words in written language .", "For example , the phonemes for the word phoenix are f i n k s .", "The L2P task is a crucial part of speech synthesis systems , as converting input text graphemes into phonemes is the first step in representing sounds .", "L2P conversion can also help improve performance in spelling correction Toutanova and Moore , 2001 .", "Unfortunately , proper nouns and unseen words prevent a table look up approach .", "It is infeasible to construct a lexical database that includes every word in the written language .", "Likewise , orthographic complexity of many languages prevents us from using hand designed conversion rules .", "There are always exceptional rules that need to be added to cover a large vocabulary set .", "Thus , an automatic L2P system is desirable .", "Many data driven techniques have been proposed for letter to phoneme conversion systems , including pronunciation by analogy Marchand and Damper , 2000 , constraint satisfaction Van Den Bosch and Canisius , 2006 , Hidden Markov Model Taylor , 2005 , decision trees Black et al . , 1998 , and neural networks Sejnowski and Rosenberg , 1987 .", "The training data usually consists of written words and their corresponding phonemes , which are not aligned ; there is no explicit information indicating individual letter and phoneme relationships .", "These relationships must be postulated before a prediction model can be trained .", "Previous work has generally assumed one to one alignment for simplicity Daelemans and Bosch , 1997 ; Black et al . , 1998 ; Damper et al . , 2005 .", "An expectation maximization EM based algorithm Dempster et al . , 1977 is applied to train the aligners .", "However , there are several problems with this approach .", "Letter strings and phoneme strings are not typically the same length , so null phonemes and null letters must be introduced to make oneto one alignments possible , Furthermore , two letters frequently combine to produce a single phoneme double letters , and a single letter can sometimes produce two phonemes double phonemes .", "To help address these problems , we propose an automatic many to many aligner and incorporate it into a generic classification predictor for letter tophoneme conversion .", "Our many to many aligner automatically discovers double phonemes and double letters , as opposed to manually preprocessing data by merging phonemes using fixed lists .", "To our knowledge , applying many to many alignments to letter to phoneme conversion is novel .", "Once we have our many to many alignments , we use that data to train a prediction model .", "Many phoneme prediction systems are based on local prediction methods , which focus on predicting an individual phoneme given each letter in a word .", "Conversely , a method like pronunciation by analogy PbA Marchand and Damper , 2000 is considered a global prediction method predicted phoneme sequences are considered as a whole .", "Recently , Van Den Bosch and Canisius 2006 proposed trigram class prediction , which incorporates a constraint satisfaction method to produce a global prediction for letter to phoneme conversion .", "Both PbA and trigram class prediction show improvement over predicting individual phonemes , confirming that L2P systems can benefit from incorporating the relationship between phonemes in a sequence .", "In order to capitalize on the information found in phoneme sequences , we propose to apply an HMM method after a local phoneme prediction process .", "Given a candidate list of two or more possible phonemes , as produced by the local predictor , the HMM will find the best phoneme sequence .", "Using this approach , our system demonstrates an improvement on several language data sets .", "The rest of the paper is structured as follows .", "We describe the letter phoneme alignment methods including a standard one to one alignment method and our many to many approach in Section 2 .", "The alignment methods are used to align graphemes and phonemes before the phoneme prediction models can be trained from the training examples .", "In Section 3 , we present a letter chunk prediction method that automatically discovers double letters in grapheme sequences .", "It incorporates our manyto many alignments with prediction models .", "In Section 4 , we present our application of an HMM method to the local prediction results .", "The results of experiments on several language data sets are discussed in Section 5 .", "We conclude and propose future work in Section 6 .", "There are two main problems with one to one alignments First , consider the double letter problem .", "In most cases when the grapheme sequence is longer than the phoneme sequence , it is because some letters are silent .", "For example , in the word abode , pronounced a b o d , the letter a produces a null phoneme E .", "This is well captured by one to one aligners .", "However , the longer grapheme sequence can also be generated by double letters ; for example , in the word king , pronounced k i , the letters ng together produce the phoneme .", "In this case , one to one aligners using null phonemes will produce an incorrect alignment .", "This can cause problems for the phoneme prediction model by training it to produce a null phoneme from either of the letters n or g . In the double phoneme case , a new phoneme is introduced to represent a combination of two or more phonemes .", "For example , in the word fume with phoneme sequence f j u m , the letter u produces both the j and u phonemes .", "There are two possible solutions for constructing a oneto one alignment in this case .", "The first is to create a new phoneme by merging the phonemes j and u .", "This requires constructing a fixed list of new phonemes before beginning the alignment process .", "The second solution is to add a null letter in the grapheme sequence .", "However , the null letter not only confuses the phoneme prediction model , but also complicates the the phoneme generation phase .", "For comparison with our many to many approach , we implement a one to one aligner based on the epsilon scattering method Black et al . , 1998 .", "The method applies the EM algorithm to estimate the probability of mapping a letter l to a phoneme p , P l , p .", "The initial probability table starts by mapping all possible alignments between letters and phonemes for each word in the training data , introducing all possible null phoneme positions .", "For example , the word phoneme sequence pair abode b o d has five possible positions where a null phoneme can be added to make an alignment .", "The training process uses the initial probability table P l , p to find the best possible alignments for each word using the Dynamic Time Warping DTW algorithm Sankoff and Kruskal , 1999 .", "At each iteration , the probability table P l , p is re calculated based on the best alignments found in that iteration .", "Finding the best alignments and re calculating the probability table continues iteratively until there is no change in the probability table .", "The final probability table P l , p is used to find one to one alignments given graphemes and phonemes .", "We present a many to many alignment algorithm that overcomes the limitations of one to one aligners .", "The training of the many to many aligner is an extension of the forward backward training of a one to one stochastic transducer presented in Ristad and Yianilos , 1998 .", "Partial counts are counts of all possible mappings from letters to phonemes that are collected in the y table , while mapping probabilities initially uniform are maintained in the S table .", "For each grapheme phoneme sequence pair x , y , the EM many2many function Algorithm 1 calls the Expectation many2many function Algorithm 2 to collect partial counts .", "T and V are the lengths of x and y respectively .", "The maxX and maxY variables are the maximum lengths of subsequences used in a single mapping operation for x and y .", "For the task at hand , we set both maxX and maxY to 2 .", "The Maximization step function simply normalizes the partial counts to create a probability distribution .", "Normalization can be done over the whole table to create a joint distribution or per grapheme to create a conditional distribution .", "The Forward many2many function Algorithm 3 fills in the table \u03b1 , with each entry \u03b1 t , v being the sum of all paths through the transducer that generate the sequence pair xi , y .", "Analogously , the Backward many2many function fills in Q , with each entry Q t , v being the sum of all paths through the transducer that generate the sequence pair xt , yr .", "The constants DELX and DELY indicate whether or not deletions are allowed on either side .", "In our system , we allow letter deletions i . e . mapping of letters to null phoneme , but not phoneme deletions .", "Expectation many2many first calls the two functions to fill the \u03b1 and Q tables , and then uses the probabilities to calculate partial counts for every possible mapping in the sequence pair .", "The partial count collected at positions t and v in the sequence pair is the sum of all paths that generate the sequence pair and go through t , v , divided by the sum of all paths that generate the entire sequence pair \u03b1 T , V .", "Once the probabilities are learned , the Viterbi algorithm can be used to produce the most likely alignment as in the following equations .", "Back pointers to maximizing arguments are kept at each step so the alignment can be reconstructed .", "Given a set of words and their phonemes , alignments are made across graphemes and phonemes .", "For example , the word phoenix , with phonemes f i n i k s , is aligned as ph oe n i x f i n i ks The letters ph are an example of the double letter problem mapping to the single phoneme f , while the letter x is an example of the double phoneme problem mapping to both k and s in the phoneme sequence .", "These alignments provide more accurate grapheme to phoneme relationships for a phoneme prediction model .", "Our new alignment scheme provides more accurate alignments , but it is also more complex sometimes a prediction model should predict two phonemes for a single letter , while at other times the prediction model should make a prediction based on a pair of letters .", "In order to distinguish between these two cases , we propose a method called letter chunking .", "Once many to many alignments are built across graphemes and phonemes , each word contains a set of letter chunks , each consisting of one or two letters aligned with phonemes .", "Each letter chunk can be considered as a grapheme unit that contains either one or two letters .", "In the same way , each phoneme chunk can be considered as a phoneme unit consisting of one or two phonemes .", "Note that the double letters and double phonemes are implicitly discovered by the alignments of graphemes and phonemes .", "They are not necessarily consistent over the training data but based on the alignments found in each word .", "In the phoneme generation phase , the system has only graphemes available to predict phonemes , so there is no information about letter chunk boundaries .", "We cannot simply merge any two letters that have appeared as a letter chunk in the training data .", "For example , although the letter pair sh is usually pronounced as a single phoneme in English e . g . gash g ae f , this is not true universally e . g . gasholder g ae s h o l d r .", "Therefore , we implement a letter chunk prediction model to provide chunk boundaries given only graphemes .", "In our system , a bigram letter chunking prediction automatically discovers double letters based on instance based learning Aha et al . , 1991 .", "Since the many to many alignments are drawn from 1 0 , 1 1 , 1 2 , 2 0 , and 2 1 relationships , each letter in a word can form a chunk with its neighbor or stand alone as a chunk itself .", "We treat the chunk prediction as a binary classification problem .", "We generate all the bigrams in a word and determine whether each bigram should be a chunk based on its context .", "Table 1 shows an example of how chunking prediction proceeds for the word longs .", "Letters li 2 , li 1 , li 1 , and li 2 are the context of the bigram li ; chunk 1 if the letter bigram li is a chunk .", "Otherwise , the chunk simply consists of an individual letter .", "In the example , the word is decomposed as l o ng s , which can be aligned with its pronunciation l 6 N z .", "If the model happens to predict consecutive overlapping chunks , only the first of the two is accepted .", "Most of the previously proposed techniques for phoneme prediction require training data to be aligned in one to one alignments .", "Those models approach the phoneme prediction task as a classification problem a phoneme is predicted for each letter independently without using other predictions from the same word .", "These local predictions assume independence of predictions , even though there are clearly interdependencies between predictions .", "Predicting each phoneme in a word without considering other assignments may not satisfy the main goal of finding a set of phonemes that work together to form a word .", "A trigram phoneme prediction with constraint satisfaction inference Van Den Bosch and Canisius , 2006 was proposed to improve on local predictions .", "From each letter unit , it predicts a trigram class that has the target phoneme in the middle surrounded by its neighboring phonemes .", "The phoneme sequence is generated in such a way that it satisfies the trigram , bigram and unigram constraints .", "The overlapping predictions improve letter to phoneme performance mainly by repairing imperfect one to one alignments .", "However , the trigram class prediction tends to be more complex as it increases the number of target classes .", "For English , there are only 58 unigram phoneme classes but 13 , 005 tri gram phoneme classes .", "The phoneme combinations in the tri gram classes are potentially confusing to the prediction model because the model has more target classes in its search space while it has access to the same number of local features in the grapheme side .", "We propose to apply a supervised HMM method embedded with local classification to find the most likely sequence of phonemes given a word .", "An HMM is a statistical model that combines the observation likelihood probability ofphonemes given letters and transition likelihood probability of current phoneme given previous phonemes to predict each phoneme .", "Our approach differs from a basic Hidden Markov Model for letter to phoneme system Taylor , 2005 that formulates grapheme sequences as observation states and phonemes as hidden states .", "The basic HMM system for L2P does not provide good performance on the task because it lacks context information on the grapheme side .", "In fact , a pronunciation depends more on graphemes than on the neighboring phonemes ; therefore , the transition probability language model should affect the prediction decisions only when there is more than one possible phoneme that can be assigned to a letter .", "Our approach is to use an instance based learning technique as a local predictor to generate a set of phoneme candidates for each letter chunk , given its context in a word .", "The local predictor produces confidence values for Each candidate phoneme .", "We normalize the confidence values into values between 0 and 1 , and treat them as the emission probabilities , while the transition probabilities are derived directly from the phoneme sequences in the training data .", "The pronunciation is generated by considering both phoneme prediction values and transition probabilities .", "The optimal phoneme sequence is found with the Viterbi search algorithm .", "We limit the size of the context to n 3 in order to avoid overfitting and minimize the complexity of the model .", "Since the candidate set is from the classifier , the search space is limited to a small number of candidate phonemes 1 to 5 phonemes in most cases .", "The HMM postprocessing is independent of local predictions from the classifier .", "Instead , it selects the best phoneme sequence from a set of possible local predictions by taking advantage of the phoneme language model , which is trained on the phoneme sequences in the training data .", "We evaluated our approaches on CMUDict , Brulex , and German , Dutch and English Celex corpora Baayen et al . , 1996 .", "The corpora except English Celex are available as part of the Letterto Phoneme Conversion PRONALSYL Challenge1 .", "For the English Celex data , we removed duplicate words as well as words shorter than four letters .", "Table 2 shows the number of words and the language of each corpus .", "For all of our experiments , our local classifier for predicting phonemes is the instance based learning IB1 algorithm Aha et al . , 1991 implemented in the TiMBL package Daelemans et al . , 2004 .", "The HMM technique is applied as post processing to the instance based learning to provide a sequence prediction .", "In addition to comparing one toone and many to many alignments , we also compare our method to the constraint satisfaction inference method as described in Section 4 .", "The results are reported in word accuracy rate based on the 10 fold cross validation , with the mean and standard deviation values .", "Table 3 shows word accuracy performance across a variety of methods .", "We show results comparing the one to one aligner described in Section 2 . 1 and the one to one aligner provided by the PRONALSYL challenge .", "The PRONALSYS one to one alignments are taken directly from the PRONALSYL challenge , whose method is based on an EM algorithm .", "For both alignments , we use instancebased learning as the prediction model .", "Overall , our one to one alignments outperform the alignments provided by the data sets for all corpora .", "The main difference between the PRONALSYS one to one alignment and our one to one alignment is that our aligner does not allow a null letter on the grapheme side .", "Consider the word abomination a b n m i n e f a n the first six letters and phonemes are aligned the same way by both aligners abomin a b n m i n .", "However , the two aligners produce radically different alignments for the last five letters .", "The alignment provided by the PRONALSYS one to one alignments is e f a n Clearly , the latter alignment provides more information on how the graphemes map to the phonemes .", "Table 3 also shows that impressive improvements for all evaluated corpora are achieved by using many to many alignments rather than one to one alignments 1 1 align vs . M M align .", "The significant improvements , ranging from 2 . 7 to 7 . 6 in word accuracy , illustrate the importance of having more precise alignments .", "For example , we can now obtain the correct alignment for the second part of the word abomination Instead of adding a null phoneme in the phoneme sequence , the many to many aligner maps the letter chunk ti to a single phoneme .", "The HMM approach is based on the same hypothesis as the constraint satisfaction inference CSInf Van Den Bosch and Canisius , 2006 .", "The results in Table 3 1 1 CSInf vs . 1 1 HMM show that the HMM approach consistently improves performance over the baseline system 1 1 align , while the CSInf degrades performance on the Brulex data set .", "For the CSInf method , most errors are caused by trigram confusion in the prediction phase .", "The results of our best system , which combines the HMM method with the many to many alignments M M HMM , are better than the results reported in Black et al . , 1998 on both the CMUDict and German Celex data sets .", "This is true even though Black et al . 1998 use explicit lists of letterphoneme mappings during the alignment process , while our approach is a fully automatic system that does not require any handcrafted list .", "We presented a novel technique of applying manyto many alignments to the letter to phoneme conversion problem .", "The many to many alignments relax the constraint assumptions of the traditional one toone alignments .", "Letter chunking bigram prediction incorporates many to many alignments into the conventional phoneme prediction models .", "Finally , the HMM technique yields global phoneme predictions based on language models .", "Impressive word accuracy improvements are achieved when the many to many alignments are applied over the baseline system .", "On several languages and data sets , using the many to many alignments , word accuracy improvements ranged from 2 . 7 to 7 . 6 , as compared to one to one alignments .", "The HMM cooperating with the local predictions shows slight improvements when it is applied to the manyto many alignments .", "We illustrated that the HMM technique improves the word accuracy more consistently than the constraint based approach .", "Moreover , the HMM can be easily incorporated into the many to many alignment approach .", "We are investigating the possibility of integrating syllabification information into our system .", "It has been reported that syllabification can potentially improve pronunciation performance in English Marchand and Damper , 2005 .", "We plan to explore other sequence prediction approaches , such as discriminative training methods Collins , 2004 , and sequence tagging with Support Vector Machines SVM HMM Altun et al . , 2003 to incorporate more features context information into the phoneme generation model .", "We are also interested in applying our approach to other related areas such as morphology and transliteration .", "We would like to thank Susan Bartlett , Colin Cherry , and other members of the Natural Language Processing research group at University of Alberta for their helpful comments and suggestions .", "This research was supported by the Natural Sciences and Engineering Research Council of Canada ."], "summary_lines": ["Applying Many-to-Many Alignments and Hidden Markov Models to Letter-to-Phoneme Conversion\n", "Letter-to-phoneme conversion generally requires aligned training data of letters and phonemes.\n", "Typically, the alignments are limited to one-to-one alignments.\n", "We present a novel technique of training with many-to-many alignments.\n", "A letter chunking bigram prediction manages double letters and double phonemes automatically as opposed to preprocessing with fixed lists.\n", "We also apply an HMM method in conjunction with a local classification model to predict a global phoneme sequence given a word.\n", "The many-to-many alignments result in significant improvements over the traditional one-to-one approach.\n", "Our system achieves state-of-the-art performance on several languages and data sets.\n", "The M2M-aligner is based on the expectation maximization (EM) algorithm.\n", "M2M-aligner is a many-to-many (M-M) alignment algorithm based on EM that allows for mapping of multiple letters to multiple phonemes.\n"]}
{"article_lines": ["Espresso Leveraging Generic Patterns For Automatically Harvesting Semantic Relations", "this paper , we present a weakly supervised , general purpose , and accurate algorithm for harvesting semantic relations .", "The main contributions are i a method for exploiting generic patterns by filtering incorrect instances using the Web ; and ii a principled measure of pattern and instance reliability enabling the filtering algorithm .", "We present an empirical comof various state of the art systems , on different size and genre corpora , on extracting various general and specific relations .", "Experimental results show that our exploitation of generic patterns substantially increases system recall with small effect on overall precision .", "Recent attention to knowledge rich problems such as question answering Pasca and Harabagiu 2001 and textual entailment Geffet and Dagan 2005 has encouraged natural language processing researchers to develop algorithms for automatically harvesting shallow semantic resources .", "With seemingly endless amounts of textual data at our disposal , we have a tremendous opportunity to automatically grow semantic term banks and ontological resources .", "To date , researchers have harvested , with varying success , several resources , including concept lists Lin and Pantel 2002 , topic signatures Lin and Hovy 2000 , facts Etzioni et al . 2005 , and word similarity lists Hindle 1990 .", "Many recent efforts have also focused on extracting semantic relations between entities , such as entailments Szpektor et al . 2004 , is a Ravichandran and Hovy 2002 , part of Girju et al .", "2006 , and other relations .", "The following desiderata outline the properties of an ideal relation harvesting algorithm riety of relations i . e . , not just is a or part of .", "To our knowledge , no previous harvesting algorithm addresses all these properties concurrently .", "In this paper , we present Espresso , a generalpurpose , broad , and accurate corpus harvesting algorithm requiring minimal supervision .", "The main algorithmic contribution is a novel method for exploiting generic patterns , which are broad coverage noisy patterns i . e . , patterns with high recall and low precision .", "Insofar , difficulties in using these patterns have been a major impediment for minimally supervised algorithms resulting in either very low precision or recall .", "We propose a method to automatically detect generic patterns and to separate their correct and incorrect instances .", "The key intuition behind the algorithm is that given a set of reliable high precision patterns on a corpus , correct instances of a generic pattern will fire more with reliable patterns on a very large corpus , like the Web , than incorrect ones .", "Below is a summary of the main contributions of this paper Espresso addresses the desiderata as follows Previous work like Girju et al . 2006 that has made use of generic patterns through filtering has shown both high precision and high recall , at the expensive cost of much manual semantic annotation .", "Minimally supervised algorithms , like Hearst 1992 ; Pantel et al . 2004 , typically ignore generic patterns since system precision dramatically decreases from the introduced noise and bootstrapping quickly spins out of control .", "To date , most research on relation harvesting has focused on is a and part of .", "Approaches fall into two categories pattern and clustering based .", "Most common are pattern based approaches .", "Hearst 1992 pioneered using patterns to extract hyponym is a relations .", "Manually building three lexico syntactic patterns , Hearst sketched a bootstrapping algorithm to learn more patterns from instances , which has served as the model for most subsequent pattern based algorithms .", "Berland and Charniak 1999 proposed a system for part of relation extraction , based on the Hearst 1992 approach .", "Seed instances are used to infer linguistic patterns that are used to extract new instances .", "While this study introduces statistical measures to evaluate instance quality , it remains vulnerable to data sparseness and has the limitation of considering only one word terms .", "Improving upon Berland and Charniak 1999 , Girju et al . 2006 employ machine learning algorithms and WordNet Fellbaum 1998 to disambiguate part of generic patterns like X s Y and X of Y .", "This study is the first extensive attempt to make use of generic patterns .", "In order to discard incorrect instances , they learn WordNetbased selectional restrictions , like X scene 4 s Y movie 1 .", "While making huge grounds on improving precision recall , heavy supervision is required through manual semantic annotations .", "Ravichandran and Hovy 2002 focus on scaling relation extraction to the Web .", "A simple and effective algorithm is proposed to infer surface patterns from a small set of instance seeds by extracting substrings relating seeds in corpus sentences .", "The approach gives good results on specific relations such as birthdates , however it has low precision on generic ones like is a and partof .", "Pantel et al . 2004 proposed a similar , highly scalable approach , based on an edit distance technique , to learn lexico POS patterns , showing both good performance and efficiency .", "Espresso uses a similar approach to infer patterns , but we make use of generic patterns and apply refining techniques to deal with wide variety of relations .", "Other pattern based algorithms include Riloff and Shepherd 1997 , who used a semi automatic method for discovering similar words using a few seed examples , KnowItAll Etzioni et al . 2005 that performs large scale extraction of facts from the Web , Mann 2002 who used part of speech patterns to extract a subset of is a relations involving proper nouns , and Downey et al .", "2005 who formalized the problem of relation extraction in a coherent and effective combinatorial model that is shown to outperform previous probabilistic frameworks .", "Clustering approaches have so far been applied only to is a extraction .", "These methods use clustering algorithms to group words according to their meanings in text , label the clusters using its members lexical or syntactic dependencies , and then extract an is a relation between each cluster member and the cluster label .", "Caraballo 1999 proposed the first attempt , which used conjunction and apposition features to build noun clusters .", "Recently , Pantel and Ravichandran 2004 extended this approach by making use of all syntactic dependency features for each noun .", "The advantage of clustering approaches is that they permit algorithms to identify is a relations that do not explicitly appear in text , however they generally fail to produce coherent clusters from fewer than 100 million words ; hence they are unreliable for small corpora .", "Espresso is based on the framework adopted in Hearst 1992 .", "It is a minimally supervised bootstrapping algorithm that takes as input a few seed instances of a particular relation and iteratively learns surface patterns to extract more instances .", "The key to Espresso lies in its use of generic patters , i . e . , those broad coverage noisy patterns that extract both many correct and incorrect relation instances .", "For example , for part of relations , the pattern X of Y extracts many correct relation instances like wheel of the car but also many incorrect ones like house of representatives .", "The key assumption behind Espresso is that in very large corpora , like the Web , correct instances generated by a generic pattern will be instantiated by some reliable patterns , where reliable patterns are patterns that have high precision but often very low recall e . g . , X consists of Y for part of relations .", "In this section , we describe the overall architecture of Espresso , propose a principled measure of reliability , and give an algorithm for exploiting generic patterns .", "Espresso iterates between the following three phases pattern induction , pattern ranking selection , and instance extraction .", "The algorithm begins with seed instances of a particular binary relation e . g . , is a and then iterates through the phases until it extracts ti1 patterns or the average pattern score decreases by more than ti2 from the previous iteration .", "In our experiments , we set ti1 5 and ti2 50 .", "For our tokenization , in order to harvest multiword terms as relation instances , we adopt a slightly modified version of the term definition given in Justeson 1995 , as it is one of the most commonly used in the NLP literature Adj Noun Adj Noun NounPrep ?", "Adj Noun Noun In the pattern induction phase , Espresso infers a set of surface patterns P that connects as many of the seed instances as possible in a given corpus .", "Any pattern learning algorithm would do .", "We chose the state of the art algorithm described in Ravichandran and Hovy 2002 with the following slight modification .", "For each input instance x , y , we first retrieve all sentences containing the two terms x and y .", "The sentences are then generalized into a set of new sentences Sx , y by replacing all terminological expressions by a terminological label , TR .", "For example Term generalization is useful for small corpora to ease data sparseness .", "Generalized patterns are naturally less precise , but this is ameliorated by our filtering step described in Section 3 . 3 .", "As in the original algorithm , all substrings linking terms x and y are then extracted from Sx , y , and overall frequencies are computed to form P . In Ravichandran and Hovy 2002 , a frequency threshold on the patterns in P is set to select the final patterns .", "However , low frequency patterns may in fact be very good .", "In this paper , instead of frequency , we propose a novel measure of pattern reliability , r , , which is described in detail in Section 3 . 2 .", "Espresso ranks all patterns in P according to reliability rt and discards all but the top k , where k is set to the number of patterns from the previous iteration plus one .", "In general , we expect that the set of patterns is formed by those of the previous iteration plus a new one .", "Yet , new statistical evidence can lead the algorithm to discard a pattern that was previously discovered .", "In this phase , Espresso retrieves from the corpus the set of instances I that match any of the patterns in P . In Section 3 . 2 , we propose a principled measure of instance reliability , rt , for ranking instances .", "Next , Espresso filters incorrect instances using the algorithm proposed in Section 3 . 3 and then selects the highest scoring m instances , according to rt , as input for the subsequent iteration .", "We experimentally set m 200 .", "In small corpora , the number of extracted instances can be too low to guarantee sufficient statistical evidence for the pattern discovery phase of the next iteration .", "In such cases , the system enters an expansion phase , where instances are expanded as follows Web expansion New instances of the patterns in P are retrieved from the Web , using the Google search engine .", "Specifically , for each instance x , y E I , the system creates a set of queries , using each pattern in P instantiated with y .", "For example , given the instance Italy , country and the pattern Y such as X , the resulting Google query will be country such as .", "New instances are then created from the retrieved Web results e . g .", "Canada , country and added to I .", "The noise generated from this expansion is attenuated by the filtering algorithm described in Section 3 . 3 .", "Syntactic expansion New instances are created from each instance x , y E I by extracting sub terminological expressions from x corresponding to the syntactic head of terms .", "For example , the relation new record of a criminal conviction part of FBI report expands to new record part of FBI report , and record part of FBI report .", "Intuitively , a reliable pattern is one that is both highly precise and one that extracts many instances .", "The recall of a pattern p can be approximated by the fraction of input instances that are extracted by p . Since it is non trivial to estimate automatically the precision of a pattern , we are wary of keeping patterns that generate many instances i . e . , patterns that generate high recall but potentially disastrous precision .", "Hence , we desire patterns that are highly associated with the input instances .", "Pointwise mutual information Cover and Thomas 1991 is a commonly used metric for measuring this strength of association between two events x and y We define the reliability of a pattern p , r\u03c0 p , as its average strength of association across each input instance i in I , weighted by the reliability of each instance i where r\u03b9 i is the reliability of instance i defined below and maxpmi is the maximum pointwise mutual information between all patterns and all instances . r\u03c0 p ranges from 0 , 1 .", "The reliability of the manually supplied seed instances are r\u03b9 i 1 .", "The pointwise mutual information between instance i x , y and pattern p is estimated using the following formula where x , p , y is the frequency of pattern p instantiated with terms x and y and where the asterisk represents a wildcard .", "A well known problem is that pointwise mutual information is biased towards infrequent events .", "We thus multiply pmi i , p with the discounting factor suggested in Pantel and Ravichandran 2004 .", "Estimating the reliability of an instance is similar to estimating the reliability of a pattern .", "Intuitively , a reliable instance is one that is highly associated with as many reliable patterns as possible i . e . , we have more confidence in an instance when multiple reliable patterns instantiate it .", "Hence , analogous to our pattern reliability measure , we define the reliability of an instance i , r\u03b9 i , as where r\u03c0 p is the reliability of pattern p defined earlier and maxpmi is as before .", "Note that r\u03b9 i and r\u03c0 p are recursively defined , where r\u03b9 i 1 for the manually supplied seed instances .", "Generic patterns are high recall low precision patterns e . g , the pattern X of Y can ambiguously refer to a part of , is a and possession relations .", "Using them blindly increases system recall while dramatically reducing precision .", "Minimally supervised algorithms have typically ignored them for this reason .", "Only heavily supervised approaches , like Girju et al . 2006 have successfully exploited them .", "Espresso s recall can be significantly increased by automatically separating correct instances extracted by generic patterns from incorrect ones .", "The challenge is to harness the expressive power of the generic patterns while remaining minimally supervised .", "The intuition behind our method is that in a very large corpus , like the Web , correct instances of a generic pattern will be instantiated by many of Espresso s reliable patterns accepted in P . Recall that , by definition , Espresso s reliable patterns extract instances with high precision yet often low recall .", "In a very large corpus , like the Web , we assume that a correct instance will occur in at least one of Espresso s reliable pattern even though the patterns recall is low .", "Intuitively , our confidence in a correct instance increases when , i the instance is associated with many reliable patterns ; and ii its association with the reliable patterns is high .", "At a given Espresso iteration , where PR represents the set of previously selected reliable patterns , this intuition is captured by the following measure of confidence in an instance i x , y where T is the sum of the reliability scores r\u03c0 p for each pattern p PR , and where pointwise mutual information between instance i and pattern p is estimated with Google as follows An instance i is rejected if S i is smaller than some threshold i .", "Although this filtering may also be applied to reliable patterns , we found this to be detrimental in our experiments since most instances generated by reliable patterns are correct .", "In Espresso , we classify a pattern as generic when it generates more than 10 times the instances of previously accepted reliable patterns .", "In this section , we present an empirical comparison of Espresso with three state of the art systems on the task of extracting various semantic relations .", "We perform our experiments using the following two datasets Each corpus is pre processed using the Alembic Workbench POS tagger Day et al . 1997 .", "Below we describe the systems used in our empirical evaluation of Espresso .", "For ESP , we experimentally set i from Section 3 . 3 to i 0 . 4 for TREC and i 0 . 3 for CHEM by manually inspecting a small set of instances .", "Espresso is designed to extract various semantic relations exemplified by a given small set of seed instances .", "We consider the standard is a and part of relations as well as the following more specific relations ess or element object produces a result1 .", "For example , ammonia produces nitric oxide .", "We evaluate this relation on the CHEM corpus .", "For each semantic relation , we manually extracted a small set of seed examples .", "The seeds were used for both Espresso as well as RH02 .", "Table 1 lists a sample of the seeds as well as sample outputs from Espresso .", "We implemented the systems outlined in Section 4 . 1 , except for GI03 , and applied them to the TREC and CHEM datasets .", "For each output set , per relation , we evaluate the precision of the system by extracting a random sample of instances 50 for the TREC corpus and 20 for the CHEM corpus and evaluating their quality manually using two human judges a total of 680 instances were annotated per judge .", "For each instance , judges may assign a score of 1 for correct , 0 for incorrect , and 1 2 for partially correct .", "Example instances that were judged partially correct include analyst is a manager and pilot is a teacher .", "The kappa statistic Siegel and Castellan Jr . 1988 on this task was K 0 . 692 .", "The precision for a given set of instances is the sum of the judges scores divided by the total instances .", "Although knowing the total number of correct instances of a particular relation in any nontrivial corpus is impossible , it is possible to compute the recall of a system relative to another system s recall .", "Following Pantel et al . 2004 , we define the relative recall of system A given system B , RA B , as where RA is the recall of A , CA is the number of correct instances extracted by A , C is the unknown total number of correct instances in the corpus , PA is A s precision in our experiments , and A is the total number of instances discovered by A .", "Tables 2 8 report the total number of instances , precision , and relative recall of each system on the TREC 9 and CHEM corpora .", "The relative recall is always given in relation to the ESP system .", "For example , in Table 2 , RH02 has a relative recall of 5 . 31 with ESP , which means that the RH02 system outputs 5 . 31 times more correct relations than ESP at a cost of much lower precision .", "Similarly , PR04 has a relative recall of 0 . 23 with ESP , which means that PR04 outputs 4 . 35 fewer correct relations than ESP also with a smaller precision .", "We did not include the results from GI03 in the tables since the system is only applicable to part of relations and we did not reproduce it .", "However , the authors evaluated their system on a sample of the TREC9 dataset and reported 83 precision and 72 recall this algorithm is heavily supervised .", "In all tables , RH02 extracts many more relations than ESP , but with a much lower precision , because it uses generic patterns without filtering .", "The high precision of ESP is due to the effective reliability measures presented in Section 3 . 2 .", "Experimental results , for all relations and the two different corpus sizes , show that ESP greatly outperforms the other methods on precision .", "However , without the use of generic patterns , the ESP system shows lower recall in all but the production relation .", "As hypothesized , exploiting generic patterns using the algorithm from Section 3 . 3 substantially improves recall without much deterioration in precision .", "ESP shows one to two orders of magnitude improvement on recall while losing on average below 10 precision .", "The succession relation in Table 6 was the only relation where Espresso found no generic pattern .", "For other relations , Espresso found from one to five generic patterns .", "Table 4 shows the power of generic patterns where system recall increases by 577 times with only a 10 drop in precision .", "In Table 7 , we see a case where the combination of filtering with a large increase in retrieved instances resulted in both higher precision and recall .", "In order to better analyze our use of generic patterns , we performed the following experiment .", "For each relation , we randomly sampled 100 instances for each generic pattern and built a gold standard for them by manually tagging each instance as correct or incorrect .", "We then sorted the 100 instances according to the scoring formula S i derived in Section 3 . 3 and computed the average precision , recall , and F score of each top K ranked instances for each pattern5 .", "Due to lack of space , we only present the graphs for four of the 22 generic patterns X is a Y for the is a relation of Table 2 , X in the Y for the part of relation of Table 4 , X in Y for the part of relation of Table 5 , and X and Y for the reaction relation of Table 7 .", "Figure 1 illustrates the results .", "In each figure , notice that recall climbs at a much faster rate than precision decreases .", "This indicates that the scoring function of Section 3 . 3 effectively separates correct and incorrect instances .", "In Figure 1a , there is a big initial drop in precision that accounts for the poor precision reported in Table 1 .", "Recall that the cutoff points on S i were set to \u03c4 0 . 4 for TREC and \u03c4 0 . 3 for CHEM .", "The figures show that this cutoff is far from the maximum F score .", "An interesting avenue of future work would be to automatically determine the proper threshold for each individual generic pattern instead of setting a uniform threshold .", "5 We can directly compute recall here since we built a gold standard for each set of 100 samples .", "We proposed a weakly supervised , generalpurpose , and accurate algorithm , called Espresso , for harvesting binary semantic relations from raw text .", "The main contributions are i a method for exploiting generic patterns by filtering incorrect instances using the Web ; and ii a principled measure of pattern and instance reliability enabling the filtering algorithm .", "We have empirically compared Espresso s precision and recall with other systems on both a small domain specific textbook and on a larger corpus of general news , and have extracted several standard and specific semantic relations isa , part of , succession , reaction , and production .", "Espresso achieves higher and more balanced performance than other state of the art systems .", "By exploiting generic patterns , system recall substantially increases with little effect on precision .", "There are many avenues of future work both in improving system performance and making use of the relations in applications like question answering .", "For the former , we plan to investigate the use of WordNet to automatically learn selectional constraints on generic patterns , as proposed by Girju et al . 2006 .", "We expect here that negative instances will play a key role in determining the selectional restrictions .", "Espresso is the first system , to our knowledge , to emphasize concurrently performance , minimal supervision , breadth , and generality .", "It remains to be seen whether one could enrich existing ontologies with relations harvested by Espresso , and it is our hope that these relations will benefit NLP applications ."], "summary_lines": ["Espresso: Leveraging Generic Patterns For Automatically Harvesting Semantic Relations\n", "In this paper, we present Espresso, a weakly-supervised, general-purpose, and accurate algorithm for harvesting semantic relations.\n", "The main contributions are: i) a method for exploiting generic patterns by filtering incorrect instances using the Web; and ii) a principled measure of pattern and instance reliability enabling the filtering algorithm.\n", "We present an empirical comparison of Espresso with various state of the art systems, on different size and genre corpora, on extracting various general and specific relations.\n", "Experimental results show that our exploitation of generic patterns substantially increases system recall with small effect on overall precision.\n", "In the pattern induction step, our system computes a reliability score for each candidate pattern based on the weighted pointwise mutual information, PMI, of the pattern with all instances extracted so far.\n", "We induce specific reliable patterns in a bootstrapping manner for entity relation extraction.\n", "Our minimally-supervised Espresso algorithm is initialized with a single set that mixes seeds of heterogeneous types, such as leader-panel and oxygen-water, which respectively correspond to the member-of and sub-quantity-of relations in the taxonomy of Keet and Artale (2008).\n"]}
{"article_lines": ["Self Training for Biomedical Parsing", "Parser self training is the technique of taking an existing parser , parsing extra data and then creating a second parser by treating the extra data as further training data .", "Here we apply this technique to parser adaptation .", "In particular , we self train the standard Charniak Johnson Penn Treebank parser using unlabeled biomedical abstracts .", "This an of 84 . 3 on a standard test set of biomedical abstracts from the Genia corpus .", "This is a 20 error reduction over the best previous result on biomedical data 80 . 2 on the same test set .", "Parser self training is the technique of taking an existing parser , parsing extra data and then creating a second parser by treating the extra data as further training data .", "While for many years it was thought not to help state of the art parsers , more recent work has shown otherwise .", "In this paper we apply this technique to parser adaptation .", "In particular we self train the standard Charniak Johnson Penn Treebank C J parser using unannotated biomedical data .", "As is well known , biomedical data is hard on parsers because it is so far from more standard English .", "To our knowledge this is the first application of self training where the gap between the training and self training data is so large .", "In section two , we look at previous work .", "In particular we note that there is , in fact , very little data on self training when the corpora for self training is so different from the original labeled data .", "Section three describes our main experiment on standard test data Clegg and Shepherd , 2005 .", "Section four looks at some preliminary results we obtained on development data that show in slightly more detail how selftraining improved the parser .", "We conclude in section five .", "While self training has worked in several domains , the early results on self training for parsing were negative Steedman et al . , 2003 ; Charniak , 1997 .", "However more recent results have shown that it can indeed improve parser performance Bacchiani et al . , 2006 ; McClosky et al . , 2006a ; McClosky et al . , 2006b .", "One possible use for this technique is for parser adaptation initially training the parser on one type of data for which hand labeled trees are available e . g . , Wall Street Journal M . Marcus et al . , 1993 and then self training on a second type of data in order to adapt the parser to the second domain .", "Interestingly , there is little to no data showing that this actually works .", "Two previous papers would seem to address this issue the work by Bacchiani et al . 2006 and McClosky et al .", "2006b .", "However , in both cases the evidence is equivocal .", "Bacchiani and Roark train the Roark parser Roark , 2001 on trees from the Brown treebank and then self train and test on data from Wall Street Journal .", "While they show some improvement from 75 . 7 to 80 . 5 f score there are several aspects of this work which leave its results less than convincing as to the utility of selftraining for adaptation .", "The first is the parsing results are quite poor by modern standards . 1 Steedman et al . 2003 generally found that selftraining does not work , but found that it does help if the baseline results were sufficiently bad .", "Secondly , the difference between the Brown corpus treebank and the Wall Street Journal corpus is not that great .", "One way to see this is to look at out of vocabulary statistics .", "The Brown corpus has an out of vocabulary rate of approximately 6 when given WSJ training as the lexicon .", "In contrast , the out of vocabulary rate of biomedical abstracts given the same lexicon is significantly higher at about 25 Lease and Charniak , 2005 .", "Thus the bridge the selftrained parser is asked to build is quite short .", "This second point is emphasized by the second paper on self training for adaptation McClosky et al . , 2006b .", "This paper is based on the C J parser and thus its results are much more in line with modern expectations .", "In particular , it was able to achieve an f score of 87 on Brown treebank test data when trained and selftrained on WSJ like data .", "Note this last point .", "It was not the case that it used the self training to bridge the corpora difference .", "It self trained on NANC , not Brown .", "NANC is a news corpus , quite like WSJ data .", "Thus the point of that paper was that self training a WSJ parser on similar data makes the parser more flexible , not better adapted to the target domain in particular .", "It said nothing about the task we address here .", "Thus our claim is that previous results are quite ambiguous on the issue of bridging corpora for parser adaptation .", "Turning briefly to previous results on Medline data , the best comparative study of parsers is that of Clegg and Shepherd 2005 , which evaluates several statistical parsers .", "Their best result was an f score of 80 . 2 .", "This was on the Lease Charniak L C parser Lease and Charniak , 2005 . 2 A close second 1 behind was 'This is not a criticism of the work .", "The results are completely in line with what one would expect given the base parser and the relatively small size of the Brown treebank . the parser of Bikel 2004 .", "The other parsers were not close .", "However , several very good current parsers were not available when this paper was written e . g . , the Berkeley Parser Petrov et al . , 2006 .", "However , since the newer parsers do not perform quite as well as the C J parser on WSJ data , it is probably the case that they would not significantly alter the landscape .", "We used as the base parser the standardly available C J parser .", "We then self trained the parser on approximately 270 , 000 sentences a random selection of abstracts from Medline . 3 Medline is a large database of abstracts and citations from a wide variety of biomedical literature .", "As we note in the next section , the number 270 , 000 was selected by observing performance on a development set .", "We weighted the original WSJ hand annotated sentences equally with self trained Medline data .", "So , for example , McClosky et al . 2006a found that the data from the handannotated WSJ data should be considered at least five times more important than NANC data on an event by event level .", "We did no tuning to find out if there is some better weighting for our domain than one to one .", "The resulting parser was tested on a test corpus of hand parsed sentences from the Genia Treebank Tateisi et al . , 2005 .", "These are exactly the same sentences as used in the comparisons of the last section .", "Genia is a corpus of abstracts from the Medline database selected from a search with the keywords Human , Blood Cells , and Transcription Factors .", "Thus the Genia treebank data are all from a small domain within Biology .", "As already noted , the Medline abstracts used for self training were chosen randomly and thus span a large number of biomedical sub domains .", "The results , the central results of this paper , are shown in Figure 1 .", "Clegg and Shepherd 2005 do not provide separate precision and recall numbers .", "However we can see that the Medline self trained parser achieves an f score of 84 . 3 , which is an absolute reduction in error of 4 . 1 .", "This corresponds to an error rate reduction of 20 over the L C baseline .", "Prior to the above experiment on the test data , we did several preliminary experiments on development data from the Genia Treebank .", "These results are summarized in Figure 2 .", "Here we show the f score for four versions of the parser as a function of number of self training sentences .", "The dashed line on the bottom is the raw C J parser with no self training .", "At 80 . 4 , it is clearly the worst of the lot .", "On the other hand , it is already better than the 80 . 2 best previous result for biomedical data .", "This is solely due to the introduction of the 50 best reranker which distinguishes the C J parser from the preceding Charniak parser .", "The almost flat line above it is the C J parser with NANC self training data .", "As mentioned previously , NANC is a news corpus , quite like the original WSJ data .", "At 81 . 4 it gives us a one percent improvement over the original WSJ parser .", "The topmost line , is the C J parser trained on Medline data .", "As can be seen , even just a thousand lines of Medline is already enough to drive our results to a new level and it continues to improve until about 150 , 000 sentences at which point performance is nearly flat .", "However , as 270 , 000 sentences is fractionally better than 150 , 000 sentences that is the number of self training sentences we used for our results on the test set .", "Lastly , the middle jagged line is for an interesting idea that failed to work .", "We mention it in the hope that others might be able to succeed where we have failed .", "We reasoned that textbooks would be a particularly good bridging corpus .", "After all , they are written to introduce someone ignorant of a field to the ideas and terminology within it .", "Thus one might expect that the English of a Biology textbook would be intermediate between the more typical English of a news article and the specialized English native to the domain .", "To test this we created a corpus of seven texts BioBooks quot ; on various areas of biology that were available on the web .", "We observe in Figure 2 that for all quantities of self training data one does better with Medline than BioBooks .", "For example , at 37 , 000 sentences the BioBook corpus is only able to achieve and an f measure of 82 . 8 while the Medline corpus is at 83 . 4 .", "Furthermore , BioBooks levels off in performance while Medline has significant improvement left in it .", "Thus , while the hypothesis seems reasonable , we were unable to make it work .", "We self trained the standard C J parser on 270 , 000 sentences of Medline abstracts .", "By doing so we achieved a 20 error reduction over the best previous result for biomedical parsing .", "In terms of the gap between the supervised data and the self trained data , this is the largest that has been attempted .", "Furthermore , the resulting parser is of interest in its own right , being as it is the most accurate biomedical parser yet developed .", "This parser is available on the web . 4 Finally , there is no reason to believe that 84 . 3 is an upper bound on what can be achieved with current techniques .", "Lease and Charniak 2005 achieve their results using small amounts of hand annotated biomedical part ofspeech tagged data and also explore other possible sources or information .", "It is reasonable to assume that its use would result in further improvement .", "This work was supported by DARPA GALE contract HR0011 06 2 0001 .", "We would like to thank the BLLIP team for their comments ."], "summary_lines": ["Self-Training for Biomedical Parsing\n", "Parser self-training is the technique of taking an existing parser, parsing extra data and then creating a second parser by treating the extra data as further training data.\n", "Here we apply this technique to parser adaptation.\n", "In particular, we self-train the standard Charniak/Johnson Penn-Treebank parser using unlabeled biomedical abstracts.\n", "This achieves an f-score of 84.3% on a standard test set of biomedical abstracts from the Genia corpus.\n", "This is a 20% error reduction over the best previous result on biomedical data (80.2% on the same test set).\n"]}
{"article_lines": ["Combining Distributional And Morphological Information For Part Of Speech Induction", "In this paper we discuss algorithms for clustering words into classes from unlabelled text using unsupervised algorithms , based on distributional and morphological information .", "We show how the use of morphological information can improve the performance on rare words , and that this is robust across a wide range of languages .", "The task studied in this paper is the unsupervised learning of parts of speech , that is to say lexical categories corresponding to traditional notions of , for example , nouns and verbs .", "As is often the case in machine learning of natural language , there are two parallel motivations first a simple engineering one the induction of these categories can help in smoothing and generalising other models , particularly in language modelling for speech recognition as explored by Ney et al . , 1994 and secondly a cognitive science motivation exploring how evidence in the primary linguistic data can account for first language acquisition by infant children Finch and Chater , 1992a ; Finch and Chater , 1992b ; Redington et al . , 1998 .", "At this early phase of learning , only limited sources of information can be used primarily distributional evidence , about the contexts in which words occur , and morphological evidence , more strictly phonotactic or orthotactic evidence about the sequence of symbols letters or phonemes of which each word is formed .", "A number of different approaches have been presented for this task using exclusively distributional evidence to cluster the words together , starting with Lamb , 1961 and these have been shown to produce good results in English , Japanese and Chinese .", "These languages have however rather simple morphology and thus words will tend to have higher frequency than in more morphologically complex languages .", "In this paper we will address two issues first , whether the existing algorithms work adequately on a range of languages and secondly how we can incorporate morphological information .", "We are particularly interested in rare words as Rosenfeld , 2000 , pp . 1313 1314 points out , it is most important to cluster the infrequent words , as we will have reliable information about the frequent words ; and yet it is these words that are most difficult to cluster .", "We accordingly focus both in our algorithms and our evaluation on how to cluster words effectively that occur only a few times or not at all in the training data .", "In addition we are interested primarily in inducing small numbers of clusters at most 128 from comparatively small amounts of data using limited or no sources of external knowledge , and in approaches that will work across a wide range of languages , rather than inducing large numbers say 1000 from hundreds of millions of words .", "Note this is different from the common task of guessing the word category of an unknown word given a pre existing set of parts of speech , a task which has been studied extensively Mikheev , 1997 .", "Our approach will be to incorporate morphological information of a restricted form into a distributional clustering algorithm .", "In addition we will use a very limited sort of frequency information , since rare words tend to belong to open class categories .", "The input to the algorithm is a sequence of tokens , each of which is considered as a sequence of characters in a standard encoding .", "The rest of this paper is structured as follows we will first discuss the evaluation of the models in some detail and present some simple experiments we have performed here Section 2 .", "We will then discuss the basic algorithm that is the starting point for our research in Section 3 .", "Then we show how we can incorporate a limited form of morphological information into this algorithm in Section 4 .", "Section 5 presents the results of our evaluations on a number of data sets drawn from typologically distinct languages .", "We then briefly discuss the use of ambiguous models or soft clustering in Section 6 , and then finish with our conclusions and proposals for future work .", "A number of different approaches to evaluation have been proposed in the past .", "First , early work used an informal evaluation of manually comparing the clusters or dendrograms produced by the algorithms with the authors' intuitive judgment of the lexical categories .", "This is inadequate for a number of obvious reasons first it does not allow adequate comparison of different techniques , and secondly it restricts the languages that can easily be studied to those in which the researcher has competence thus limiting experimentation on a narrow range of languages .", "A second form of evaluation is to use some data that has been manually or semi automatically annotated with part of speech POS tags , and to use some information theoretic measure to look at the correlation between the 'correct' data and the induced POS tags .", "Specifically , one could look at the conditional entropy of the gold standard tags given the induced tags .", "We use the symbol W to refer to the random variable related to the word , G for the associated gold standard tag , and T for the tag produced by one of our algorithms .", "Recall that Thus low conditional entropy means that the mutual information between the gold and induced tags will be high .", "If we have a random set of tags the mutual information will be zero and the conditional entropy will be the same as the entropy of the tag set .", "Again , this approach has several weaknesses there is not a unique well defined set of part ofspeech tags , but rather many different possible sets that reflect rather arbitrary decisions by the annotators .", "To put the scores we present below in context , we note that using some data sets prepared for the AMALGAM project Atwell et al . , 2000 the conditional entropies between some data manually tagged with different tag sets varied from 0 . 22 between Brown and LOB tag sets to 1 . 3 between LLC and Unix Parts tag sets .", "Secondly , because of the Zipfian distribution of word frequencies , simple baselines that assign each frequent word to a different class , can score rather highly , as we shall see below .", "A third evaluation is to use the derived classification in a class based language model , and to measure the perplexity of the derived model .", "However it is not clear that this directly measures the linguistic plausibility of the classification .", "In particular many parts of speech relative pronouns for example represent long distance combinatorial properties , and a simple finite state model with local context such as a class n gram model Brown et al . , 1992 will not measure this .", "We can also compare various simple baselines , to see how they perform according to these simple measures .", "Frequent word baseline take the n 1 most frequent words and assign them each to a separate class , and put all remaining words in the remaining class .", "Word baseline each word is in its own class .", "We performed experiments on parts of the Wall Street Journal corpus , using the corpus tags .", "We chose sections 0 19 , a total of about 500 , 000 words .", "Table 1 shows that the residual conditional entropy with the word baseline is only 0 . 12 .", "This reflects lexical ambiguity .", "If all of the words were unambiguous , then the conditional entropy of the tag given the word would be zero .", "We are therefore justified in ignoring ambiguity for the moment , since it vastly improves the efficiency of the algorithms .", "Clearly as the number of clusters increases , the conditional entropy will decrease , as is demonstrated below .", "The basic methods here have been studied in detail by Ney et al . , 1994 , Martin et al . , 1998 and Brown et al . , 1992 .", "We assume a vocabulary of words V W1 , . . . .", "Our task is to learn a deterministic clustering , that is to say a class membership function g from V into the set of class labels , n .", "This clustering can be used to define a number of simple statistical models .", "The objective function we try to maximise will be the likelihood of some model i . e . the probability of the data with respect to the model .", "The simplest candidate for the model is the class bigram model , though the approach can also be extended to class trigram models .", "Suppose we have a corpus of length N , , wN .", "We can assume an additional sentence boundary token .", "Then the class bigram model defines the probability of the next word given the history as P wi IOC' P wilg wi P 9 wi 1 1g wi 2 It is not computationally feasible to search through all possible partitions of the vocabulary to find the one with the highest value of the likelihood ; we must therefore use some search algorithm that will give us a local optimum .", "We follow Ney et al . , 1994 ; Martin et al . , 1998 and use an exchange algorithm similar to the k means algorithm for clustering .", "This algorithm iteratively improves the likelihood of a given clustering by moving each word from its current cluster to the cluster that will give the maximum increase in likelihood , or leaving it in its original cluster if no improvement can be found .", "There are a number of different ways in which the initial clustering can be chosen ; it has been found , and our own experiments have tended to confirm this , that the initialisation method has little effect on the final quality of the clusters but can have a marked effect on the speed of convergence of the algorithm .", "A more important variation for our purposes is how the rare words are treated .", "Martin et al . , 1998 leave all words with a frequency of less than 5 in a particular class , from which they may not be moved .", "The second sort of information is information about the sequence of letters or phones that form each word .", "To take a trivial example , if we encounter an unknown word , say 212 , 000 then merely looking at the sequence of characters that compose it is enough to enable us to make a good guess as to its part of speech .", "Less trivially , if a word in English ends in ing , then it is quite likely to be a present participle .", "We can distinguish this sort of information , which perhaps could better be called orthotactic or phonotactic information from a richer sort which incorporates relational information between the words thus given a novel word that ends in quot ; ing quot ; such as quot ; derailing quot ; one could use the information that we had already seen the token quot ; derailed quot ; as additional evidence .", "One way to incorporate this simple source of information would be to use a mixture of string models alone , without distributional evidence .", "Some preliminary experiments not reported here established that this approach could only separate out the most basic differences , such as sequences of numbers .", "A more powerful approach is to combine the distributional information with the morphological information by composing the Ney Essen clustering model with a model for the morphology within a Bayesian framework .", "We use the same formula for the probability of the data given the model , but include an additional term for the probability of the model , that depends on the strings used in each cluster .", "We wish to bias the algorithm so that it will put words that are morphologically similar in the same cluster .", "We can consider thus a generative process that produces sets of clusters as used before .", "Consider the vocabulary V to be a subset of E where E is the set of characters or phonemes used , and let the model have for each cluster i a distribution over E say P . Then we define the probability of the partition the prior as ignoring irrelevant normalisation constants .", "This will give a higher probability to partitions where morphologically similar strings are in the same cluster .", "The models we will use here for the cluster dependent word string probabilities will be letter Hidden Markov Models HMMs .", "We decided to use HMMs rather than more powerful models , such as character trigram models , because we wanted models that were capable of modelling properties of the whole string ; though in English and in other European languages , local statistics such as those used by n gram models are adequate to capture most morphological regularities , in other languages this is not the case .", "Moreover , we wish to have comparatively weak models otherwise the algorithm will capture irrelevant orthotactic regularities such as a class of words starting with quot ; st quot ; in English .", "In addition we can modify this to incorporate information about frequency .", "We know that rare words are more likely to be nouns , proper nouns or members of some other open word class rather than say pronouns or articles .", "We can do this simply by adding prior class probabilities ai to the above equation giving We can use the maximum likelihood estimates for ozi which are just the number of distinct types in cluster i , divided by the total number of types in the corpus .", "This just has the effect of discriminating between classes that will have lots of types i . e . open class clusters and clusters that tend to have few types corresponding to closed class words .", "It is possible that in some languages there might be more subtle category related frequency effects , that could benefit from more complex models of frequency .", "We used texts prepared for the MULTEXT East project Erjavec and Ide , 1998 which consists of data George Orwell's novel 1984 in seven languages the original English together with Romanian , Czech , Slovene , Bulgarian , Estonian , and Hungarian .", "These are summarised in Table 2 .", "As can be seen they cover a wide range of language families ; furthermore Bulgarian is written in Cyrillic , which slightly stretches the range .", "Token type ratios range from 12 . 1 for English to 4 . 84 for Hungarian .", "The tags used are extremely fine grained , and incorporate a great deal of information about case , gender and so on in Hungarian for example 400 tags are used with 86 tags used only once .", "Table 3 shows the result of our cross linguistic evaluation on this data .", "Since the data sets are so small we decided to use the conditional entropy evaluation .", "Here DO refers to the distributional clustering algorithm where all words are clustered ; D5 leaves all words with frequency at most 5 in a seperate cluster , DM uses morphological information as well , DF uses frequency information and DMF uses morphological and frequency information .", "We evaluated it for all words , and also for words with frequency at most 5 .", "We can see that the use of morphological information consistently improves the results on the rare words by a substantial margin .", "In some cases , however , a simpler algorithm performs better when all the words are considered notably in Slovene and Estonian .", "We have also evaluated this method by comparing the perplexity of a class based language model derived from these classes .", "We constructed a class bigram model , using absolute interpolation with a singleton generalised distribution for the transition weights , and using absolute discounting with backing off for the membership output function .", "Ney et al . , 1994 ; Martin et al . , 1998 We trained the model on sections 00 09 of the Penn Treebank , 518769 tokens including sentence boundaries and punctuation and tested it on sections 10 l 9 537639 tokens .", "We used the full vocabulary of the training and test sets together which was 45679 , of which 14576 had frequency zero in the training data and thus had to be categorised based solely on their morphology and frequency .", "We did not reduce the vocabulary or change the capitalization in any way .", "We compared different models with varying numbers of clusters 32 64 and 128 .", "Table 4 shows the results of the perplexity evaluation on the WSJ data .", "As can be seen the models incorporating morphological information have slightly lower perplexity on the test data than the D5 model .", "Note that this is a global evaluation over all the words in the data , including words that do not occur in the training data at all .", "Figure 5 shows how the conditional entropy varies with respect to the frequency for these models .", "As can be seen the use of morphological information improves the preformance markedly for rare words , and that this effect reduces as the frequency increases .", "Note that the use of the frequency information worsens the performance for rare words according to this evaluation this is because the rare words are much more tightly grouped into just a few clusters , thus the entropy of the cluster tags is lower .", "Table 5 shows a qualitative evaluation of some of the clusters produced by the best performing model for 64 clusters on the WSJ data set .", "We selected the 10 clusters with the largest number of zero frequency word types in .", "We examined each cluster and chose a simple regular expression to describe it , and calculated the precision and recall for words of all frequency , and for words of zero frequency .", "Note that several of the clusters capture syntactically salient morphological regularities regular verb suffixes , noun suffixes and the presence of capitalisation are all detected , together with a class for numbers .", "In some cases these are split amongst more than one class , thus giving classes with high precision and low recall .", "We made no attempt to adjust the regular expressions to make these scores high we merely present them as an aid to an intuitive understanding of the composition of these clusters .", "Up until now we have considered only hard clusters , where each word is unambiguously assigned to a single class .", "Clearly , because of lexical ambiguity , we would like to be able to assign some words to more than one class .", "This is sometimes called soft clustering .", "Space does not permit an extensive analysis of the situation .", "We shall therefore report briefly on some experiments we have performed and our conclusions largely leaving this as an area for future research .", "Jardino and Adda , 1994 ; SchUtze , 1997 ; Clark , 2000 have presented models that account for ambiguity to some extent .", "The most principled way is to use Hidden Markov Models these provide the formal and technical apparatus required to train when the tags might be ambiguous .", "Murakami et al . , 1993 presents this idea together with a simple evaluation on English .", "We therefore extend our approach to allow ambiguous words , by changing our model from a deterministic to nondeterministic model .", "In this situation we want the states of the HMM to correspond to syntactic categories , and use the standard ExpectationMaximization EM algorithm to train it .", "To experiment with this we chose fullyconnected , randomly initialized Hidden Markov Models , with determined start and end states .", "We trained the model on the various sentences in the model , on WSJ data .", "With 5 substates , 20 iterations corpus , and then tagged the data with the most likely Viterbi tag sequence .", "We then evaluated the conditional entropy of the gold standard tags given the derived HMM tags .", "Table 6 shows the results of this evaluation on some English data for various numbers of states .", "As can be seen , increasing the number of states of the model does not reduce the conditional entropy of the gold standard tags ; rather it increases the lexical ambiguity of the model H TIW .", "This is because the states of the HMM will not necessarily correspond directly to syntactic categories rather they correspond to sets of words that occur in particular positions for example the model might have a state that corresponds to a noun that occurs before a main verb , and a separate state that corresponds to a noun after a main verb .", "One explanation for this is that the output function from each state of the HMM is a multinomial distribution over the vocabulary which is too powerful since it can memorise any set of words thus there is no penalty for the same word being produced by many different states .", "This suggests a solution that is to replace the multinomial distribution by a weaker distribution such as the Hidden Markov Models we have used before .", "This gives us a two level HMM a HMM where each state corresponds to a word , and where the output function is a HMM where each state corresponds to a letter .", "This relates to two other approaches that we are aware of Fine et al . , 1998 and Weber et al . , 2001 .", "Table 7 shows a simple evaluation of this approach ; we can see that this does not suffer from the same drawback as the previous approach though the results are still poor compared to the other approaches , and in fact are consistently worse than the baselines of Table 1 .", "The problem here is that we are restricted to using quite small HMMs which are insufficiently powerful to memorise large chunks of the vocabulary , and in addition the use of the Forward Backward algorithm is more computationally expensive by at least a factor of the number of states .", "We have applied several different algorithms to the task of identifying parts of speech .", "We have demonstrated that the use of morphological information can improve the performance of the algorithm with rare words quite substantially .", "We have also demonstrated that a very simple use of frequency can provide further improvements .", "Additionally we have tested this on a wide range of languages .", "Intuitively we have used all of the different types of information available when we encounter a new word , we know three things about it first , the context that it has appeared in , secondly the string of characters that it is made of , and thirdly that it is a new word and therefore rare .", "We have so far used only a limited form of morphological information that relies on properties of individual strings , and does not relate particular strings to each other .", "We plan to use this stronger form of information using Pair Hidden Markov Models as described in Clark , 2001 ."], "summary_lines": ["Combining Distributional And Morphological Information For Part Of Speech Induction\n", "In this paper we discuss algorithms for clustering words into classes from unlabelled text using unsupervised algorithms, based on distributional and morphological information.\n", "We show how the use of morphological information can improve the performance on rare words, and that this is robust across a wide range of languages.\n", "We propose a perplexity based test for the quality of the POS induction algorithm.\n", "We find that many-to-1 accuracy has several defects.\n"]}
{"article_lines": ["Automatic Identification Of Non Compositional Multi Word Expressions Using Latent Semantic Analysis", "Making use of latent semantic analysis , we explore the hypothesis that local linguistic context can serve to identify multi word expressions that have noncompositional meanings .", "We propose that vector similarity between distribution vectors associated with an MWE as a whole and those associated with its constitutent parts can serve as a good measure of the degree to which the MWE is compositional .", "We present experiments that show that low cosine similarity does , in fact , correlate with non compositionality .", "Identifying non compositional or idiomatic multi word expressions MWEs is an important subtask for any computational system Sag et al . , 2002 , and significant attention has been paid to practical methods for solving this problem in recent years Lin , 1999 ; Baldwin et al . , 2003 ; Villada Moir on and Tiedemann , 2006 .", "While corpus based techniques for identifying collocational multi word expressions by exploiting statistical properties of the co occurrence of the component words have become increasingly sophisticated Evert and Krenn , 2001 ; Evert , 2004 , it is well known that mere co occurrence does not well distinguish compositional from non compositional expressions Manning and Sch utze , 1999 , Ch .", "While expressions which may potentially have idiomatic meanings can be identified using various lexical association measures Evert and Krenn , 2001 ; Evert and Kermes , 2003 , other techniques must be used to determining whether or not a particular MWE does , in fact , have an idiomatic use .", "In this paper we explore the hypothesis that the local linguistic context can provide adequate cues for making this determination and propose one method for doing this .", "We characterize our task on analogy with wordsense disambiguation Sch utze , 1998 ; Ide and V eronis , 1998 .", "As noted by Sch utze , WSD involves two related tasks the general task of sense discrimination determining what senses a given word has and the more specific task of sense selection determining for a particular use of the word in context which sense was intended .", "For us the discrimination task involves determining for a given expression whether it has a non compositional interpretation in addition to its compositional interpretation , and the selection task involves determining in a given context , whether a given expression is being used compositionally or non compostionally .", "The German expression ins Wasser fallen , for example , has a noncompositional interpretation on which it means to fail to happen as in 1 and a compositional interpretation on which it means to fall into water as in 2 . 1 The discrimination task , then , is to identify ins Wasser fallen as an MWE that has an idiomatic meaning and the selection task is to determine that in 1 it is the compositional meaning that is intended , while in 2 it is the non compositional meaning .", "Following Sch utze 1998 and Landauer Dumais 1997 our general assumption is that the meaning of an expression can be modelled in terms of the words that it co occurs with its co occurrence signature .", "To determine whether a phrase has a non compositional meaning we compute whether the co occurrence signature of the phrase is systematically related to the cooccurrence signatures of its parts .", "Our hypothesis is that a systematic relationship is indicative of compositional interpretation and lack of a systematic relationship is symptomatic of noncompositionality .", "In other words , we expect compositional MWEs to appear in contexts more similar to those in which their component words appear than do non compositional MWEs .", "In this paper we describe two experiments that test this hypothesis .", "In the first experiment we seek to confirm that the local context of a known idiom can reliably distinguish idiomatic uses from non idiomatic uses .", "In the second experiment we attempt to determine whether the difference between the contexts in which an MWE appears and the contexts in which its component words appear can indeed serve to tell us whether the MWE has an idiomatic use .", "In our experiments we make use of lexical semantic analysis LSA as a model of contextsimilarity Deerwester et al . , 1990 .", "Since this technique is often used to model meaning , we will speak in terms of meaning similiarity .", "It should be clear , however , that we are only using the LSA vectors derived from context of occurrence in a corpus to model meaning and meaning composition in a very rough way .", "Our hope is simply that this rough model is sufficient to the task of identifying non compositional MWEs .", "Recent work which attempts to discriminate between compositional and non compositional MWEs include Lin 1999 , who used mutualinformation measures identify such phrases , Baldwin et al . 2003 , who compare the distribution of the head of the MWE with the distribution of the entire MWE , and Vallada Moir on Tiedemann 2006 , who use a word alignment strategy to identify non compositional MWEs making use of parallel texts .", "Schone Jurafsky 2001 applied LSA to MWE identification , althought they did not focus on distinguishing compositional from non compositional MWEs .", "Lin s goal , like ours , was to discriminate noncompositional MWEs from compositional MWEs .", "His method was to compare the mutual information measure of the constituents parts of an MWE with the mutual information of similar expressions obtained by substituting one of the constituents with a related word obtained by thesaurus lookup .", "The hope was that a significant difference between these measures , as in the case of red tape mutual information 5 . 87 compared to yellow tape 3 . 75 or orange tape 2 . 64 , would be characteristic of non compositional MWEs .", "Although intuitively appealing , Lin s algorithm only achieves precision and recall of 15 . 7 and 13 . 7 , respectively as compared to a gold standard generate from an idiom dictionary but see below for discussion .", "Schone Jurafsky 2001 evaluated a number of co occurrence based metrics for identifying MWEs , showing that , as suggested by Lin s results , there was need for improvement in this area .", "Since LSA has been used in a number of meaning related language tasks to good effect Landauer and Dumais , 1997 ; Landauer and Psotka , 2000 ; Cederberg and Widdows , 2003 , they had hoped to improve their results by identify non compositional expressions using a method similar to that which we are exploring here .", "Although they do not demonstrate that this method actually identifies non compositional expressions , they do show that the LSA similarity technique only improves MWE identification minimally .", "Baldwin et al . , 2003 focus more narrowly on distinguishing English noun noun compounds and verb particle constructions which are compositional from those which are not compositional .", "Their approach is methodologically similar to ours , in that they compute similarity on the basis of contexts of occurrance , making use of LSA .", "Their hypothesis is that high LSA based similarity between the MWE and each of its constituent parts is indicative of compositionality .", "They evaluate their technique by assessing the correlation between high semantic similarity of the constituents of an MWE to the MWE as a whole with the likelihood that the MWE appears in WordNet as a hyponym of one of the constituents .", "While the expected correlation was not attested , we suspect this to be more an indication of the inappropriateness of the evaluation used than of the faultiness of the general approach .", "Lin , Baldwin et al . , and Schone Jurafsky , all use as their gold standard either idiom dictionaries or WordNet Fellbaum , 1998 .", "While Schone Jurafsky show that WordNet is as good a standard as any of a number of machine readable dictionaries , none of these authors shows that the MWEs that appear in WordNet or in the MRDs are generally non compositional , in the relevant sense .", "As noted by Sag et al . 2002 many MWEs are simply institutionalized phrases whose meanings are perfectly compositional , but whose frequency of use or other non linguistic factors make them highly salient .", "It is certainly clear that many MWEs that appear in WordNet examples being law student , medical student , college man are perfectly compositional semantically .", "Zhai 1997 , in an early attempt to apply statistical methods to the extraction of noncompositional MWEs , made use of what we take to be a more appropriate evaluation metric .", "In his comparison among a number of different heuristics for identifying non compositional noun noun compounds , Zhai did his evaluation by applying each heuristic to a corpus of items hand classified as to their compositionality .", "Although Zhai s classification appears to be problematic , we take this to be the appropirate paradigm for evaluation in this domain , and we adopt it here .", "In our work we made use of the Word Space model of semantic similiarty Sch utze , 1998 and extended it slightly to MWEs .", "In this framework , meaning is modeled as an n dimensional vector , derived via singular value decomposition Deerwester et al . , 1990 from word co occurrence counts for the expression in question , a technique frequently referred to as Latent Semantic Analysis LSA .", "This kind of dimensionality reduction has been shown to improve performance in a number of text based domains Berry et al . , 1999 .", "For our experiments we used a local German newspaper corpus . 2 We built our LSA model with the Infomap Software package . 3 , using the 1000 most frequent words not on the 102 word hand generated stop list as the content bearing dimension words the columns of the matrix .", "The 20 , 000 most frequent content words were assigned row values by counting occurrences within a 30word window .", "SVD was used to reduce the dimensionality from 1000 to 100 , resulting in 100 dimensional meaning vectors for each word .", "In our experiments , MWEs were assigned meaningvectors as a whole , using the same proceedure .", "For meaning similarity we adopt the standard measure of cosine of the angle between two vectors the normalized correlation coefficient as a metric Sch utze , 1998 ; Baeza Yates and Ribeiro Neto , 1999 .", "On this metric , two expressions are taken to be unrelated if their meaning vectors are orthogonal the cosine is 0 and synonymous if their vectors are parallel the cosine is 1 .", "Figure 1 illustrates such a vector space in two dimensions .", "Note that the meaning vector for L offel spoon is quite similar to that for essen to eat but distant from sterben to die , while the meaning vector for the MWE den L offel abgeben is close to that for sterben .", "Indeed den L offel abgeben , like to kick the bucket , is a noncompositional idiom meaning to die .", "While den L offel abgeben is used almost exclusively in its idiomatic sense all four occurrences in our corpus , many MWEs are used regularly in both their idiomatic and in their literal senses .", "About two thirds of the uses of the MWE ins Wasser fallen in our corpus are idiomatic uses , and the remaing one third are literal uses .", "In our first experiment we tested the hypothesis that these uses could reliably be distinguished using distribution based models of their meaning .", "For this experiment we manually annotated the 67 occurrences of ins Wasser fallen in our corpus as to whether the expression was used compositionally literally or non compositionally idiomatically . 4 Marking this distinction we generate an LSA meaning vectors for the compositional uses and an LSA meaning vector for the non compositional uses of ins Wasser fallen .", "The vectors turned out , as expected , to be almost orthogonal , with a cosine of the angle between them of 0 . 02 .", "This result confirms that the linguistic contexts in which the literal and the idiomatic use of ins Wasser fallen appear are very different , indicating not surprisingly that the semantic difference between the literal meaning and the idiomatic meaning is reflected in the way these these phrases are used .", "Our next task was to investigate whether this difference could be used in particular cases to determine what the intended use of an MWE in a particular context was .", "To evaluate this , we did a 10 fold cross validation study , calculating the literal and idiomatic vectors for ins Wasser fallen on the basis of the training data and doing a simple nearest neighbor classification of each memember of the test set on the basis of the meaning vectors computed from its local context the 30 word window .", "Our result of an average accurace of 72 for our LSA based classifier far exceeds the simple maximum likelihood baseline of 58 .", "In the final part of this experiment we compared the meaning vector that was computed by summing over all uses of ins Wasser fallen with the literal and idiomatic vectors from above .", "Since idiomatic uses of ins Wasserfallen prevail in the corpus 2 3 vs . 1 3 , it is not surprisingly that the similarity to the literal vector 0 . 0946 is much than similarity to the idiomatic vector 0 . 3712 .", "To summarize Experiment I , which is a variant of a supervised phrase sense disambiguation task , demonstrates that we can use LSA to distinguish between literal and the idiomatic usage of an MWE by using local linguistic context .", "4This was a straightforward task ; two annotators annotated independently , with very high agreement kappa score of over 0 . 95 Carletta , 1996 .", "Occurrences on which the annotators disagreed were thrown out .", "Of the 64 occurrences we used , 37 were idiomatic and 27 were literal .", "In our second experiment we sought to make use of the fact that there are typically clear distributional difference between compositional and non compositional uses of MWEs to determine whether a given MWE indeed has noncompositional uses at all .", "In this experiment we made use of a test set of German Preposition Noun Verb collocation candidate database whose extraction is described by Krenn 2000 and which has been made available electronically . 5 From this database only word combinations with frequency of occurrence more than 30 in our test corpus were considered .", "Our task was to classify these 81 potential MWEs according whether or not thay have an idiomatic meaning .", "To accomplish this task we took the following approach .", "We computed on the basis of the distribution of the components of the MWE an estimate for the compositional meaning vector for the MWE .", "We then compared this to the actual vector for the MWE as a whole , with the expectation MWEs which indeed have non compositinoal uses will be distinguished by a relatively low vector similarity between the estimated compositional meaning vector and the actual meaning vector .", "In other words small similarity values should be diagnostic for the presense of non compositinoal uses of the MWE .", "We calculated the estimated compositional meaning vector by taking it to be the sum of the meaning vector of the parts , i . e . , the compositional meaning of an expression w1w2 consisting of two words is taken to be sum of the meaning vectors for the constituent words . 6 In order to maximize the independent contribution of the constituent words , the meaning vectors for these words were always computed from contexts in which they appear alone that is , not in the local context of the other constituent .", "We call the estimated compositional meaning vector the composed vector . 7 The comparisons we made are illustrated in Figure 2 , where vectors for the MWE auf die Strecke bleiben to fall by the wayside and the words Strecke route and bleiben to stay are mapped into two dimensions8 .", "the words Autobahn highway and eigenst andig independent are given for comparison .", "Here we see that the linear combination of the component words of the MWE is clearly distinct from that of the MWE as a whole .", "As a further illustration of the difference between the composed vector and the MWE vector , in Table 2 we list the words whose meaning vector is most similar to that of the MWE auf dis Strecke bleiben along with their similarity values , and in Table 3 we list those words whose meaning vector is most similar to the composed vector .", "The semantic differences among these two classes are readily apparent .", "0 . 769663 0 . 732372 0 . 731411 0 . 717294 0 . 704939 strecken to lengthen 0 . 743309 fahren to drive 0 . 741059 laufen to run 0 . 726631 fahrt drives 0 . 712352 schlie\u00dfen to close 0 . 704364 We recognize that the composed vector is clearly nowhere near a perfect model of compositional meaning in the general case .", "This can be illustrated by considering , for example , the MWE fire breathing .", "This expression is clearly compositional , as it denotes the process of producing combusting exhalation , exactly what the semantic combination rules of the English would predict .", "Nevertheless the distribution of fire breathing is quite unrelated to that of its constituents fire and breathing the former appears frequently with dragon and circus while the later appear frequently with blaze and lungs , respectively .", "Despite these principled objections , the composed vector provides a useful baseline for our investigation .", "We should note that a number of researchers in the LSA tradition have attempted to provide more compelling combinatory functions to capture the non linearity of linguistic compositional interpretation Kintsch , 2001 ; Widdows and Peters , 2003 .", "As a check we chose , at random , a number of simple clearly compositional word combinations not from the candidate MWE list .", "We expected that on the whole these would evidence a very high similarity measure when compared with their associated composed vector , and this is indeed the case , as shown in Table 1 .", "We also compared the literal and non literal vectors for ins Wasser fallen from the first experiment with the composed vector , computed out of the meaning vectors for Wasser and for fallen . 9 The difference isn t large , but nevertheless the composed vector is more similar to the literal vector cosine of 0 . 2937 than to the non literal vector cosine of 0 . 1733 .", "Extending to the general case , our task was to compare the composed vector to the actual vector for all the MWEs in our test set .", "The resulting cosine similarity values range from 0 . 01 to 0 . 80 .", "Our hope was that there would be a similarity threshold for distinguishing MWEs that have non compositional interpretations from those that do not .", "Indeed of the MWEs with a similarity values of under 0 . 1 , just over half are MWEs which were hand annotated to have non literal uses . 10 It used in their idiomatic sense apparently for humorous effect particularly frequently in contexts in which elements of the literal meaning were also present . 11 is clear then that the technique described is , prima facie , capable of detecting idiomatic MWEs .", "To evaluate the method , we used the careful manual annotation of the PNV database described by Krenn 2000 as our gold standard .", "By adopting different threshholds for the classification decision , we obtained a range of results trading off precision and recall .", "Table 4 illustrates this range .", "The F score measure is maximized in our experiments by adopting a similarity threshold of 0 . 2 .", "This means that MWEs which have a meaning vector whose cosine is under this value when compared with with the combined vector should be classified as having a non literal meaning .", "To compare our method with that proposed by Baldwin et al . 2003 , we applied their method to our materials , generating LSA vectors for the component content words in our candidate MWEs and comparing their semantic similarity to the MWEs LSA vector as a whole , with the expectation being that low similarity between the MWE as a whole and its component words is indication of the non compositionality of the MWE .", "The results are given in Table 5 .", "It is clear that while Baldwin et al . s expectation is borne out in the case of the constituent noun the non head , it is not in the case of the constituent verb the head .", "Even in the case of the nouns , however , the results are , for the most part , markedly inferior to the results we achieved using the composed vectors .", "There are a number of issues that complicate the workability of the unsupervised technique described here .", "We rely on there being enough non compositional uses of an idiomatic MWE in the corpus that the overall meaning vector for the MWE reflects this usage .", "If the literal meaning is overwhelmingly frequent , this will reduce the effectivity of the method significantly .", "A second problem concerns the relationship between the literal and the non literal meaning .", "Our technique relies on these meaning being highly distinct .", "If the meanings are similar , it is likely that local context will be inadequate to distinguish a compositional from a non compositional use of the expression .", "In our investigation it became apparent , in fact , that in the newspaper genre , highly idiomatic expressions such as ins Wasser fallen were often", "To summarize , in order to classify an MWE as non compositional , we compute an approximation of its compositional meaning and compare this with the meaning of the expression as it is used on the whole .", "One of the obvious improvements to the algorithm could come from better models for simulating compositional meaning .", "A further issue that can be explored is whether linguistic preprocessing would influence the results .", "We worked only on raw text data .", "There is some evidence Baldwin et al . , 2003 that part of speech tagging might improve results in this kind of task .", "We also only considered local word sequences .", "Certainly some recognition of the syntactic structure would improve results .", "These are , however , more general issues associated with MWE processing .", "Rather promising results were attained using only local context , however .", "Our study shows that the F score measure is maximized by taking as threshold for distinguishing non compositional phrases from compositional ones a cosine similarity value somewhere between 0 . 1 0 . 2 .", "An important point to be explored is that compositionality appears to come in degrees .", "As Bannard and Lascarides 2003 have noted , MWEs do not fall cleanly into the binary classes of compositional and non compositional expressions , but populate a continuum between the two extremes . While our experiment was designed to classify MWEs , the technique described here , of course , provides a means , if rather a blunt one , for quantifying the degreee of compositonality of an expression ."], "summary_lines": ["Automatic Identification Of Non-Compositional Multi-Word Expressions Using Latent Semantic Analysis\n", "Making use of latent semantic analysis, we explore the hypothesis that local linguistic context can serve to identify multi-word expressions that have non-compositional meanings.\n", "We propose that vector-similarity between distribution vectors associated with an MWE as a whole and those associated with its constitutent parts can serve as a good measure of the degree to which the MWE is compositional.\n", "We present experiments that show that low (cosine) similarity does, in fact, correlate with non-compositionality.\n", "we devise a supervised method in which they compute the meaning vectors for the literal and non literal usages of a given expression in the trainningdata.\n", "we used a supervised learning method to distinguish between compositional and non-compositional uses of an expression (in German text) by using contextual information in the form of Latent Semantic Analy sis (LSA) vectors.\n"]}
{"article_lines": ["The Berkeley FrameNet Project", "is a NSF supported project in corpus based computational lexicography , now in its second year NSF IRI 9618838 , quot ; Tools for Lexicon Building quot ; .", "The project's key features are a a commitment to corpus evidence for semantic and syntactic generalizations , and b the representation of the valences of its target words mostly nouns , adjectives , and verbs in which the semantic portion makes use of frame semantics .", "The resulting database will contain a descriptions of the semantic frames underlying the meanings of the words described , and b the valence representation semantic and syntactic of several thousand words and phrases , each accompanied by c a representative collection of annotated corpus attestations , which jointly exemplify the observed linkings between quot ; frame elements quot ; and their syntactic realizations e . g . grammatical function , phrase type , and other syntactic traits .", "This report will present the project's goals and workflow , and information about the computational tools that have been adapted or created in house for this work .", "The Berkeley FrameNet project' is producing frame semantic descriptions of several thousand English lexical items and backing up these descriptions with semantically annotated attestations from contemporary English corpora2 .", "These descriptions are based on hand tagged semantic annotations of example sentences extracted from large text corpora and systematic analysis of the semantic patterns they exemplify by lexicographers and linguists .", "The primary emphasis of the project therefore is the encoding , by humans , of semantic knowledge in machine readable form .", "The intuition of the lexicographers is guided by and constrained by the results of corpus based research using highperformance software tools .", "The semantic domains to be covered are HEALTH CARE , CHANCE , PERCEPTION , COMMUNICATION , TRANSACTION , TIME , SPACE , BODY parts and functions of the body , MOTION , LIFE STAGES , SOCIAL CONTEXT , EMOTION and COGNITION .", "The results of the project are a a lexical resource , called the FrameNet database3 , and b associated software tools .", "The database has three major components described in more detail below PLE SENTENCES which illustrate each of the potential realization patterns identified in the formula ; 4 and d links to the FRAME DATABASE and to other machine readable resources such as WordNet and COMLEX . marked up to exemplify the semantic and morphosyntactic properties of the lexical items .", "Several of these are schematized in Fig .", "These sentences provide empirical support for the lexicographic analysis provided in the frame database and lexicon entries .", "These three components form a highly relational and tightly integrated whole elements in each may point to elements in the other two .", "The database will also contain estimates of the relative frequency of senses and complementation patterns calculated by matching the senses and patterns in the hand tagged examples against the entire BNC corpus .", "The FrameNet work is in some ways similar to efforts to describe the argument structures of lexical items in terms of case roles or thetaroles , 5 but in FrameNet , the role names called frame elements or FEs are local to particular conceptual structures frames ; some of these are quite general , while others are specific to a small family of lexical items .", "For example , the TRANSPORTATION frame , within the domain of MOTION , provides MOVERS , MEANS of transportation , and PATHS ; 6 6The semantic frames for individual lexical units are typically quot ; blends quot ; of more than one basic frame ; from our point of view , the so called quot ; linking quot ; patterns proposed in LFG , HPSG , and Construction Grammar , operate on higher level frames of action giving agent , patient , instrument , motion and location giving theme , location , source , goal , path , and experience giving experiencer , stimulus , content , etc .", "In some but not all cases , the assignment of syntactic correlates to frame elements could be mediated by mapping them to the roles of one of the more abstract frames .", "6A detailed study of motion predicates would require a finer grained analysis of the Path element , separating out Source and Goal , and perhaps Direction and Area , but for a basic study of the transportation predicates such refined analysis is not necessary .", "In any case , our subframes associated with individual words inherit all of these while possibly adding some of their own .", "Fig .", "1 shows some of the subframes , as discussed below .", "The DRIVING frame , for example , specifies a DRIVER a principal MOVER , a VEHICLE a particularization of the MEANS element , and potentially CARGO or RIDER as secondary movers .", "In this frame , the DRIVER initiates and controls the movement of the VEHICLE .", "For most verbs in this frame , DRIVER or VEHICLE can be realized as subjects ; VEHICLE , RIDER , or CARGO can appear as direct objects ; and PATH and VEHICLE can appear as oblique complements .", "Some combinations of frame elements , or Frame Element Groups FEGs , for some real corpus sentences in the DRIVING frame are shown in Fig .", "A RIDING_1 frame has the primary mover role as RIDER , and allows as VEHICLE those driven by others . 7 In grammatical realizations of this frame , the RIDER can be the subject ; the VEHICLE can appear as a direct object or an oblique complement ; and the PATH is generally realized as an oblique .", "The FrameNet entry for each of these verbs will include a concise formula for all semanwork includes the separate analysis of the frame semantics of directional and locational expressions . tic and syntactic combinatorial possibilities , together with a collection of annotated corpus sentences in which each possibility is exemplified .", "The syntactic positions considered relevant for lexicographic description include those that are internal to the maximal projection of the target word the whole VP , AP , or NP for target V , A or N , and those that are external to the maximal projection under precise structural conditions ; the subject , in the case of VP , and the subject of support verbs in the case of AP and NP .", "8 Used in NLP , the FrameNet database should make it possible for a system which finds a valence bearing lexical item in a text to know for each of its senses where its individual arguments are likely to be found .", "For example , once a parser has found the verb drive and its direct object NP , the link to the DRIVING frame will suggest some semantics for that NP , e . g . that a person as direct object probably represents the RIDER , while a non human proper noun is probably the VEHICLE .", "For practical lexicography , the contribution of the FrameNet database will be its presentation of the full range of use possibilities for individual words , documented with corpus data , the model examples for each use , and the statistical information on relative frequency .", "The computational side of the FrameNet project is directed at efficiently capturing human insights into semantic structure .", "The majority of the work involved is marking text with semantic tags , specifying again by hand the structure of the frames to be treated , and writing dictionary style entries based the results of annotation and a priori descriptions .", "With the exception of the example sentence extraction component , all the software modules are highly interactive and have substantial user interface requirements .", "Most of this functionality is provided by WWW based programs written in PERL .", "Four processing steps are required produce the FrameNet database of frame semantic representations a generating initial descriptions of semantic and syntactic patterns for use in corpus queries and annotation quot ; Preparation quot ; , b extracting good example sentences quot ; Subcorpus Extraction quot ; , c marking by hand the constituents of interest quot ; Annotation quot ; , and d building a database of lexical semantic representations based on the annotations and other data quot ; Entry Writing quot ; .", "These are discussed briefly below and shown in Fig .", "As work on the project has progressed , we have defined several explicit roles which project participants play in the various steps . these roles are referred to as Vanguard 1 . 1 in Fig .", "3 , Annotators 3 . 1 and Rearguard 4 . 1 .", "These are purely functional designations the same person may play different roles at different times . 9 pares the initial descriptions of frames , including lists of frames and frame elements , and adds these to the Frame Database 5 . 1 using the Frame Description tool 1 . 2 .", "The Vanguard also selects the major vocabulary items for the frame the target words and the syntactic patterns that need to be checked for each word , which are entered in the Lexical Database 5 . 2 by means of the Lexical Database Tool 1 . 3 .", "Subcorpus Extraction .", "Based on the Vanguard's work , the subcorpus extraction tools 2 . 2 produce a representative collection of sentences containing these words .", "This selection of examples is achieved through a hybrid process partially controlled by the preliminary lexical description of each lemma .", "Sentences containing the lemma are extracted from from a corpus and classified into subcorpora by syntactic pattern 2 . 2 . 1 using a CASCADE FILTER 2 . 2 . 2 , 2 . 2 . 5 , 2 . 2 . 6 representing a partial regular expression grammar of English over part of speech tags cf .", "Gahl forthcoming , formatted for annotation 2 . 2 . 4 , and automatically sampled 2 . 2 . 3 down to an appropriate number .", "If these heuristics fail to find appropriate examples by means of syntactic patterns , sentences are selected using INTERACTIVE SELECreviews the skeletal lexical record created by the Vanguard , the annotated example sentences 5 . 3 , and the FEGs extracted from them , and builds both the entries for the lemmas in the Lexical Database 5 . 2 and the frame descriptions in the Frame Database 5 . 1 , using the Entry Writing Tools 4 . 2 .", "' We are building a quot ; constituent type identifier quot ; which will semi automatically assign Grammatical Function GF , and Phrase Type PT attributes to these FEmarked constituents , eliminating the need for Annotators to mark these .", "The data structures described above are implemented in SGML . 11 Each is described by a DTD , and these DTDs are structured to provide the necessary links between the components .", "The software suite currently supporting database development is an aggregate of existing software tools held together with PERL CGI based quot ; glue quot ; .", "In order to get the project started , we have depended on off theshelf software which in some cases is not ideal for our purposes .", "Nevertheless , using these programs allowed us to get the project up and running within just a few months .", "We describe below in approximate order of application the programs used and their state of completion .", "quot ; Eventually , we plan to migrate to an XML data model , which appears to provide more flexibility while reducing complexity .", "Also , the FrameNet software is being developed on Unix , but we plan to provide crossplatform capabilities by making our tool suite web based and XML compatible .", "SGML files into HTML for convenient viewing on the web , etc . are being written in PERL .", "RCS maintains version control over most files .", "At the time of writing , there is something in place for each of the major software components , though in some cases these are little more than stubs or quot ; toy quot ; implementations .", "Nearly 10 , 000 sentences exemplifying just under 200 lemmas have been annotated ; there are over 20 , 000 frame element tokens marked in these example sentences .", "About a dozen frames have been specified , which refer to 47 named frame elements .", "Most of these annotations have been accomplished in the last few months since the software for corpus extraction , frame description , and annotation became operational .", "We expect the inventory to increase rapidly .", "If the proportions cited hold constant as the Framenet database grows , the final database of 5 , 000 lexical units may contain 250 , 000 annotated sentences and over half a million tokens of frame elements ."], "summary_lines": ["The Berkeley FrameNet Project\n", "FrameNet is a three-year NSF-supported project in corpus-based computational lexicography, now in its second year (NSF IRI-9618838, \"Tools for Lexicon Building\").\n", "The project's key features are (a) a commitment to corpus evidence for semantic and syntactic generalizations, and (b) the representation of the valences of its target words (mostly nouns, adjectives, and verbs) in which the semantic portion makes use of frame semantics.\n", "The resulting database will contain (a) descriptions of the semantic frames underlying the meanings of the words described, and (b) the valence representation (semantic and syntactic) of several thousand words and phrases, each accompanied by (c) a representative collection of annotated corpus attestations, which jointly exemplify the observed linkings between \"frame elements\" and their syntactic realizations (e.g. grammatical function, phrase type, and other syntactic traits).\n", "This report will present the project's goals and workflow, and information about the computational tools that have been adapted or created in-house for this work.\n", "We present the FrameNet project in which we havee been developing a frame-semantic lexicon for the core vocabulary of English.\n"]}
{"article_lines": ["A Maximum Entropy Approach to Chinese Word Segmentation Jin Kiat Low 1 and Hwee Tou Ng 1 , 2 and Wenyuan Guo 2 1 .", "Department of Computer Science , National University of Singapore , 3 Science Drive 2 , Singapore 117543 2 .", "Singapore MIT Alliance , E4 04 10 , 4 Engineering Drive 3 , Singapore 117576 lowjinki , nght , guowy comp . nus . edu . sg Abstract We participated in the Second Inter national Chinese Word Segmentation Bakeoff .", "Specifically , we evaluated our Chinese word segmenter in the open track , on all four corpora , namely Academia Sinica AS , City University of Hong Kong CITYU , Microsoft Re search MSR , and Peking University PKU .", "Based on a maximum entropy approach , our word segmenter achieved the highest F measure for AS , CITYU , and PKU , and the second highest for MSR .", "We found that the use of an ex ternal dictionary and additional training corpora of different segmentation stan dards helped to further improve seg mentation accuracy .", "1 Chinese Word Segmenter The Chinese word segmenter we built is similar to the maximum entropy word segmenter we em ployed in our previous work Ng and Low , 2004 .", "Our word segmenter uses a maximum entropy framework Ratnaparkhi , 1998 ; Xue and Shen , 2003 and is trained on manually segmented sen tences .", "It classifies each Chinese character given the features derived from its surrounding context .", "Each Chinese character can be assigned one of four possible boundary tags s for a character that occurs as a single character word , b for a charac ter that begins a multi character i . e . , two or more characters word , e for a character that ends a multi character word , and m for a character that is neither the first nor last in a multi character word .", "Our implementation used the opennlp maximum entropy package v2 . 1 . 0 from sourceforge . 1 1 . 1 Basic Features The basic features of our word segmenter are similar to our previous work Ng and Low , 2004 a Cn n ? 2 , ? 1 , 0 , 1 , 2 b CnCn 1 n ? 2 , ? 1 , 0 , 1 c C ? 1C1 d Pu C0 e T C ? 2 T C ? 1 T C0 T C1 T C2 In the above feature templates , C refers to a Chinese character .", "Templates a ?", "c refer to a context of five characters the current character and two characters to its left and right .", "C0 denotes the current character , Cn C ? n denotes the character n positions to the right left of the current character .", "For example , given the charac ter sequence ? c ? ? ? ?", "? , when considering the character C0 ? ?", "? , C ? 2 denotes ? c ? , C1C2 denotes ? ? ?", "The punctuation feature , Pu C0 , checks whether C0 is a punctuation symbol such as ? ?", "For the type fea ture e , four type classes are defined numbers represent class 1 , dates ? ?", "? , ? ? , the Chinese characters for ? day ? , ? month ? , ? year ? , respectively represent class 2 , English letters represent class 3 , and other characters represent class 4 .", "For example , when considering the character ? ?", "in the character sequence ?", "T C2 11243 1http maxent . sourceforge . net 161 will be set to 1 ? ? ?", "is the Chinese character for ? 9 ?", "is the Chinese character for ? 0 ? .", "Besides these basic features , we also made use of character normalization .", "We note that char acters like punctuation symbols and Arabic dig its have different character codes in the ASCII , GB , and BIG5 encoding standard , although they mean the same thing .", "For example , comma ? , ?", "is represented as the hexadecimal value 0x2c in ASCII , but as the hexadecimal value 0xa3ac in GB .", "In our segmenter , these different character codes are normalized and replaced by the corre sponding character code in ASCII .", "Also , all Ara bic digits are replaced by the ASCII digit ? 0 ?", "to denote any digit .", "Incorporating character normal ization enables our segmenter to be more robust against the use of different encodings to represent the same character .", "For all the experiments that we conducted , training was done with a feature cutoff of 2 and 100 iterations , except for the AS corpus which had a feature cutoff of 3 .", "A major difficulty faced by a Chinese word segmenter is the presence of out of vocabulary OOV words .", "Segmenting a text with many OOV words tends to result in lower accuracy .", "We ad dress the problem of OOV words in two ways using an external dictionary containing a list of predefined words , and using additional training corpora which are not segmented according to the same segmentation standard .", "1 . 2 External Dictionary If a sequence of characters in a sentence matches a word in an existing dictionary , it may be a clue that the sequence of characters should be segmented as one word .", "We used an online dictionary from Peking University downloadable from the Internet2 , consisting of about 108 , 000 words of length one to four characters .", "If there is some sequence of neighboring characters around C0 in the sentence that matches a word in this dictionary , then we greedily choose the longest such matching word W in the dictionary .", "Let t0 be the boundary tag of C0 in W , L the number of characters in W , and C1 C ? 1 be the character 2http ccl . pku . edu . cn doubtfire Course Chinese 20Information 20Processing Source Code Chapter 8 Lexicon full 2000 . zip immediately following preceding C0 in the sentence .", "We then add the following features derived from the dictionary f Lt0 g Cnt0 n ? 1 , 0 , 1 For example , consider the sentence ? c ? ?", "When processing the current character C0 ? ?", "? , we will attempt to match the following candidate sequences ? ?", "against existing words in our dictionary .", "Suppose both ? ? ? ?", "are found in the dictionary .", "Then the longest matching word W chosen is ? c ? ?", "? , t0 is m , L is 3 , C ? 1 is ? c ? , and C1 is ? ? ? .", "1 . 3 Additional Training Corpora The presence of different standards in Chinese word segmentation limits the amount of training corpora available for the community , due to dif ferent organizations preparing training corpora in their own standards .", "Indeed , if one uniform seg mentation standard were adopted , more training data would have been available , and the OOV problem could be significantly reduced .", "We observed that although different segmenta tion standards exist , the differences are limited , and many words are still segmented in the same way across two different segmentation standards .", "As such , in our work , we attempt to incorporate corpora from other segmentation standards as ad ditional training data , to help reduce the OOV problem .", "Specifically , the steps taken are 1 .", "Perform training with maximum entropy modeling using the original training corpus D0 annotated in a given segmentation stan dard .", "Use the trained word segmenter to segment another corpus D annotated in a different segmentation standard .", "Suppose a Chinese character C in D is as signed a boundary tag t by the word seg menter with probability p . If t is identical to the boundary tag of C in the gold standard 162 annotated corpus D , and p is less than some threshold ? , then C with its surrounding context in D is used as additional training data .", "Add all such characters C as additional train ing data to the original training corpus D0 , and train a new word segmenter using the en larged training data .", "Evaluate the accuracy of the new word seg menter on the same test data annotated in the original segmentation standard of D0 .", "For the current bakeoff , when training a word segmenter on a particular training corpus , the ad ditional training corpora are all the three corpora in the other segmentation standards .", "For example , when training a word segmenter for the AS cor pus , the additional training corpora are CITYU , MSR , and PKU .", "The necessary character encod ing conversion between GB and BIG5 is per formed , and the probability threshold ?", "We found from our experiments that setting ?", "to a higher value did not further improve seg mentation accuracy , but would instead increase the training set size and incur longer training time .", "2 Testing During testing , the probability of a boundary tag sequence assignment t1 .", "tn given a character sequence C1 .", "Cn is determined by using the maximum entropy classifier to compute the prob ability that a boundary tag ti is assigned to each individual character Ci .", "If we were to just as sign each character the boundary tag with the highest probability , it is possible that the clas sifier produces a sequence of invalid tags e . g . , m followed by s .", "To eliminate such possibil ities , we implemented a dynamic programming algorithm which considers only valid boundary tag sequences given an input character sequence .", "At each character position i , the algorithm con siders each last word candidate ending at posi tion i and consisting of K characters in length K 1 , .", ", 20 in our experiments .", "To deter mine the boundary tag assignment to the last word W with K characters , the first character of W is assigned boundary tag b , the last character of W is assigned tag e , and the intervening characters Corpus R P F ROOV RIV AS 0 . 962 0 . 950 0 . 956 0 . 684 0 . 975 CITYU 0 . 967 0 . 956 0 . 962 0 . 806 0 . 980 MSR 0 . 969 0 . 968 0 . 968 0 . 736 0 . 975 PKU 0 . 968 0 . 969 0 . 969 0 . 838 0 . 976 Table 1 Our official SIGHAN bakeoff results are assigned tag m . If W is a single character word , then the single character is assigned tag s .", "In this way , the dynamic programming algorithm only considers valid tag sequences .", "After word segmentation is done by the maxi mum entropy classifier , a post processing step is applied to correct inconsistently segmented words made up of 3 or more characters .", "A word W is defined to be inconsistently segmented if the con catenation of 2 to 6 consecutive words elsewhere in the segmented output document matches W .", "In the post processing step , the segmentation of the characters of these consecutive words is changed so that they are segmented as a single word .", "To illustrate , if the concatenation of 2 consecutive words ? ? ?", "in the segmented out put document matches another word ? ?", "? n ? , then the 2 consecutive words ? ? ?", "will be re segmented as a single word ? ? ?", "3 Evaluation Results We evaluated our Chinese word segmenter in the open track , on all 4 corpora , namely Academia Sinica AS , City University of Hong Kong CITYU , Microsoft Research MSR , and Peking University PKU .", "Table 1 shows our of ficial SIGHAN bakeoff results .", "The columns R , P , and F show the recall , precision , and F mea sure , respectively .", "The columns ROOV and RIV show the recall on out of vocabulary words and in vocabulary words , respectively .", "Our Chinese word segmenter which participated in the bakeoff was trained with the basic features Section 1 . 1 , and made use of the external dictionary Sec tion 1 . 2 and additional training corpora Sec tion 1 . 3 .", "Our word segmenter achieved the high est F measure for AS , CITYU , and PKU , and the second highest for MSR .", "After the release of the official bakeoff results , 163 Corpus V1 V2 V3 V4 AS 0 . 953 0 . 955 0 . 956 0 . 956 CITYU 0 . 950 0 . 960 0 . 961 0 . 962 MSR 0 . 960 0 . 968 0 . 963 0 . 968 PKU 0 . 948 0 . 965 0 . 956 0 . 969 Table 2 Word segmentation accuracy F mea sure of different versions of our word segmenter we ran a series of experiments to determine the contribution of each component of our word seg menter , using the official scorer and test sets with gold standard segmentations .", "Version V1 used only the basic features Section 1 . 1 ; Version V2 used the basic features and additional features de rived from our external dictionary Section 1 . 2 ; Version V3 used the basic features but with ad ditional training corpora Section 1 . 3 ; and Ver sion V4 is our official submitted version combin ing basic features , external dictionary , and addi tional training corpora .", "Table 2 shows the word segmentation accuracy F measure of the differ ent versions of our word segmenter , when tested on the official test sets of the four corpora .", "The results indicate that the use of external dictionary increases segmentation accuracy .", "Similarly , the use of additional training corpora of different seg mentation standards also increases segmentation accuracy .", "4 Conclusion Using a maximum entropy approach , our Chi nese word segmenter achieves state of the art ac curacy , when evaluated on all four corpora in the open track of the Second International Chinese Word Segmentation Bakeoff .", "The use of an exter nal dictionary and additional training corpora of different segmentation standards helps to further improve segmentation accuracy .", "Acknowledgements This research is partially supported by a research grant R252 000 125 112 from National Univer sity of Singapore Academic Research Fund , as well as the Singapore MIT Alliance .", "References Hwee Tou Ng and Jin Kiat Low .", "Chinese part of speech tagging One at a time or all at once ?", "word based or character based ?", "In Proceedings of the 2004 Conference on Empirical Methods in Nat ural Language Processing EMNLP 2004 , pages 277 ? 284 .", "Adwait Ratnaparkhi .", "Maximum Entropy Mod els for Natural Language Ambiguity Resolution .", "Ph . D . thesis , University of Pennsylvania .", "Nianwen Xue and Libin Shen .", "Chinese word segmentation as LMR tagging .", "In Proceedings of the Second SIGHAN Workshop on Chinese Lan guage Processing , pages 176 ? 179 ."], "summary_lines": ["A Maximum Entropy Approach to Chinese Word Segmentation\n", "We participated in the Second International Chinese Word Segmentation Bakeoff.\n", "Specifically, we evaluated our Chinese word segmenter in the open track, on all four corpora, namely Academia Sinica (AS), City University of Hong Kong (CITYU), Microsoft Research (MSR), and Peking University (PKU).\n", "Based on a maximum entropy approach, our word segmenter achieved the highest F measure for AS, CITYU, and PKU, and the second highest for MSR.\n", "We found that the use of an external dictionary and additional training corpora of different segmentation standards helped to further improve segmentation accuracy.\n", "We present a post processing method to enhance the unknown word segmentation.\n", "We use templates representing numbers, dates, letters etc.\n"]}
{"article_lines": ["Building Deep Dependency Structures Using A Wide Coverage CCG Parser", "This paper describes a wide coverage statistical parser that uses Combinatory Categorial Grammar CCG to derive dependency structures .", "The parser differs from most existing wide coverage treebank parsers in capturing the long range dependencies inherent in constructions such as coordination , extraction , raising and control , as well as the standard local predicate argument dependencies .", "A set of dependency structures used for training and testing the parser is obtained from a treebank of CCG normal form derivations , which have been derived semi automatically from the Penn Treebank .", "The parser correctly recovers over 80 of labelled dependencies , and around 90 of unlabelled dependencies .", "Most recent wide coverage statistical parsers have used models based on lexical dependencies e . g .", "Collins 1999 , Charniak 2000 .", "However , the dependencies are typically derived from a context free phrase structure tree using simple head percolation heuristics .", "This approach does not work well for the long range dependencies involved in raising , control , extraction and coordination , all of which are common in text such as the Wall Street Journal .", "Chiang 2000 uses Tree Adjoining Grammar as an alternative to context free grammar , and here we use another mildly context sensitive formalism , Combinatory Categorial Grammar CCG , Steedman 2000 , which arguably provides the most linguistically satisfactory account of the dependencies inherent in coordinate constructions and extraction phenomena .", "The potential advantage from using such an expressive grammar is to facilitate recovery of such unbounded dependencies .", "As well as having a potential impact on the accuracy of the parser , recovering such dependencies may make the output more useful .", "CCG is unlike other formalisms in that the standard predicate argument relations relevant to interpretation can be derived via extremely non standard surface derivations .", "This impacts on how best to define a probability model for CCG , since the spurious ambiguity of CCG derivations may lead to an exponential number of derivations for a given constituent .", "In addition , some of the spurious derivations may not be present in the training data .", "One solution is to consider only the normal form Eisner , 1996a derivation , which is the route taken in Hockenmaier and Steedman 2002b . 1 Another problem with the non standard surface derivations is that the standard PARSEVAL performance measures over such derivations are uninformative Clark and Hockenmaier , 2002 .", "Such measures have been criticised by Lin 1995 and Carroll et al . 1998 , who propose recovery of headdependencies characterising predicate argument relations as a more meaningful measure .", "If the end result of parsing is interpretable predicate argument structure or the related dependency structure , then the question arises why build derivation structure at all ?", "A CCG parser can directly build derived structures , including longrange dependencies .", "These derived structures can be of any form we like for example , they could in principle be standard Penn Treebank structures .", "Since we are interested in dependency based parser evaluation , our parser currently builds dependency structures .", "Furthermore , since we want to model the dependencies in such structures , the probability model is defined over these structures rather than the derivation .", "The training and testing material for this CCG parser is a treebank of dependency structures , which have been derived from a set of CCG derivations developed for use with another normal form CCG parser Hockenmaier and Steedman , 2002b .", "The treebank of derivations , which we call CCGbank Hockenmaier and Steedman , 2002a , was in turn derived semi automatically from the handannotated Penn Treebank .", "In CCG , most language specific aspects of the grammar are specified in the lexicon , in the form of syntactic categories that identify a lexical item as either a functor or argument .", "For the functors , the category specifies the type and directionality of the arguments and the type of the result .", "For example , the following category for the transitive verb bought specifies its first argument as a noun phrase NP to its right and its second argument as an NP to its left , and its result as a sentence For parsing purposes , we extend CCG categories to express category features , and head word and dependency information directly , as follows The feature dcl specifies the category s S result as a declarative sentence , bought identifies its head , and the numbers denote dependency relations .", "Heads and dependencies are always marked up on atomic categories S , N , NP , PP , and conj in our implementation .", "The categories are combined using a small set of typed combinatory rules , such as functional application and composition see Steedman 2000 for details .", "Derivations are written as follows , with underlines indicating combinatory reduction and arrows indicating the direction of the application Formally , a dependency is defined as a 4 tuple hf f s ha , where hf is the head word of the functor , 2 f is the functor category extended with head and dependency information , s is the argument slot , and ha is the head word of the argument for example , the following is the object dependency yielded by the first step of derivation 3 The head of the infinitival complement s subject is identified with the head of the object , using the variable X . Unification then passes the head of the object to the subject of the infinitival , as in standard unification based accounts of control . 3 The kinds of lexical items that use the head passing mechanism are raising , auxiliary and control verbs , modifiers , and relative pronouns .", "Among the constructions that project unbounded dependencies are relativisation and right node raising .", "The following category for the relative pronoun category for words such as who , which , that shows how heads are co indexed for object extraction The derivation for the phrase The company that Marks wants to buy is given in Figure 1 with the features on S categories removed to save space , and the constant heads reduced to the first letter .", "Typeraising and functional composition , along with co indexing of heads , mediate transmission of the head of the NP the company onto the object of buy .", "The corresponding dependencies are given in the following figure , with the convention that arcs point away from arguments .", "The relevant argument slot in the functor category labels the arcs .", "Note that we encode the subject argument of the to category as a dependency relation Marks is a subject of to , since our philosophy at this stage is to encode every argument as a dependency , where possible .", "The number of dependency types may be reduced in future work .", "The DAG like nature of the dependency structures makes it difficult to apply generative modelling techniques Abney , 1997 ; Johnson et al . , 1999 , so we have defined a conditional model , similar to the model of Collins 1996 see also the conditional model in Eisner 1996b .", "While the model of Collins 1996 is technically unsound Collins , 1999 , our aim at this stage is to demonstrate that accurate , efficient wide coverage parsing is possible with CCG , even with an over simplified statistical model .", "Future work will look at alternative models . 4 4The reentrancies creating the DAG like structures are fairly limited , and moreover determined by the lexical categories .", "We conjecture that it is possible to define a generative model that includes the deep dependencies .", "The parse selection component must choose the most probable dependency structure , given the sentence S . A sentence S w1t1 w2t2 wntn is assumed to be a sequence of word , pos tag pairs .", "For our purposes , a dependency structure n is a C D pair , where C c1 c2 cn is the sequence of categories assigned to the words , and D hfi fi si hai i 1 m is the set of dependencies .", "The probability of a dependency structure can be written as follows The probability PCS can be approximated as follows where Xi is the local context for the ith word .", "We have explained elsewhere Clark , 2002 how suitable features can be defined in terms of the word , pos tag pairs in the context , and how maximum entropy techniques can be used to estimate the probabilities , following Ratnaparkhi 1996 .", "We assume that each argument slot in the category sequence is filled independently , and write PDC S as follows rj m where hai is the head word filling the argument slot of the ith dependency , and m is the number of dependencies entailed by the category sequence C . The estimation method is based on Collins 1996 .", "We assume that the probability of a dependency only depends on those words involved in the dependency , together with their categories .", "We follow Collins and base the estimate of a dependency probability on the following intuition given a pair of words , with a pair of categories , which are in the same sentence , what is the probability that the words are in a particular dependency relationship ?", "We again follow Collins in defining the following functions , where is the set of words in the data , and is the set of lexical categories .", "C ab cd for ac and bd is the number of times that word category pairs ab and cd are in the same word category sequence in the training data .", "CR ab cd is the number of times that ab and cd are in the same word category sequence , with a and c in dependency relation R . FRab cd is the probability that a and c are in dependency relation R , given thatab andcd are in the same word category sequence .", "The relative frequency estimate of the probability FRa b c d is as follows where cai is the lexical category of the argument head ai .", "The normalising factor ensures that the probabilities for each argument slot sum to one over all the word category pairs in the sequence . 5 This factor is constant for the given category sequence , but not for different category sequences .", "However , the dependency structures with high enough PCS to be among the highest probability structures are likely to have similar category sequences .", "Thus we ignore the normalisation factor , thereby simplifying the parsing process .", "A similar argument is used by Collins 1996 in the context of his parsing model .", "The estimate in equation 10 suffers from sparse data problems , and so a backing off strategy is employed .", "We omit details here , but there are four levels of back off the first uses both words and both categories ; the second uses only one of the words and both categories ; the third uses the categories only ; and a final level substitutes pos tags for the categories .", "One final point is that , in practice , the number of dependencies can vary for a given category sequence because multiple arguments for the same slot can 5One of the problems with the model is that it is deficient , assigning probability mass to dependency structures not licensed by the grammar .", "The parser analyses a sentence in two stages .", "First , in order to limit the number of categories assigned to each word in the sentence , a supertagger Bangalore and Joshi , 1999 assigns to each word a small number of possible lexical categories .", "The supertagger described in Clark 2002 assigns to each word all categories whose probabilities are within some constant factor , \u03b2 , of the highest probability category for that word , given the surrounding context .", "Note that the supertagger does not provide a single category sequence for each sentence , and the final sequence returned by the parser along with the dependencies is determined by the probability model described in the previous section .", "The supertagger is performing two roles cutting down the search space explored by the parser , and providing the categorysequence model in equation 8 .", "The supertagger consults a category dictionary which contains , for each word , the set of categories the word was seen with in the data .", "If a word appears at least K times in the data , the supertagger only considers categories that appear in the word s category set , rather than all lexical categories .", "The second parsing stage applies a CKY bottom up chart parsing algorithm , as described in Steedman 2000 .", "The combinatory rules currently used by the parser are as follows functional application forward and backward , generalised forward composition , backward composition , generalised backward crossed composition , and typeraising .", "There is also a coordination rule which conjoins categories of the same type . 6 Type raising is applied to the categories NP , PP , and Sadj NP adjectival phrase ; it is currently implemented by simply adding pre defined sets of type raised categories to the chart whenever an NP , PP or Sadj NP is present .", "The sets were chosen on the basis of the most frequent type raising rule instantiations in sections 02 21 of the CCGbank , which resulted in 8 type raised categories for NP , and 2 categories each for PP and Sadj NP .", "As well as combinatory rules , the parser also uses a number of lexical rules and rules involving punctuation .", "The set of rules consists of those occurring roughly more than 200 times in sections 02 21 of the CCGbank .", "For example , one rule used by the parser is the following This rule creates a nominal modifier from an ingform of a verb phrase .", "A set of rules allows the parser to deal with commas all other punctuation is removed after the supertagging phase .", "For example , one kind of rule treats a comma as a conjunct , which allows the NP object in John likes apples , bananas and pears to have three heads , which can all be direct objects of like . 7 The search space explored by the parser is reduced by exploiting the statistical model .", "First , a constituent is only placed in a chart cell if there is not already a constituent with the same head word , same category , and some dependency structure with a higher or equal score where score is the geometric mean of the probability of the dependency structure .", "This tactic also has the effect of eliminating spuriously ambiguous entries from the chart cf .", "Komagata 1997 .", "Second , a constituent is only placed in a cell if the score for its dependency structure is within some factor , a , of the highest scoring dependency structure for that cell .", "Sections 02 21 of the CCGbank were used for training 39 161 sentences ; section 00 for development 1 901 sentences ; and section 23 for testing 2 379 sentences . 8 Sections 02 21 were also used to obtain the category set , by including all categories that appear at least 10 times , which resulted in a set of 398 category types .", "The word category sequences needed for estimating the probabilities in equation 8 can be read directly from the CCGbank .", "To obtain dependencies for estimating PDC S , we ran the parser over the trees , tracing out the combinatory rules applied during the derivation , and outputting the dependencies .", "This method was also applied to the trees in section 23 to provide the gold standard test set .", "Not all trees produced dependency structures , since not all categories and type changing rules in the CCGbank are encoded in the parser .", "We obtained dependency structures for roughly 95 of the trees in the data .", "For evaluation purposes , we increased the coverage on section 23 to 990 2 352 sentences by identifying the cause of the parse failures and adding the additional rules and categories when creating the gold standard ; so the final test set consisted of gold standard dependency structures from 2 352 sentences .", "The coverage was increased to ensure the test set was representative of the full section .", "We emphasise that these additional rules and categories were not made available to the parser during testing , or used for training .", "Initially the parser was run with 0 001 for the supertagger an average of 38 categories per word , K 20 for the category dictionary , and a 0001 for the parser .", "A time out was applied so that the parser was stopped if any sentence took longer than 2 CPU minutes to parse .", "With these parameters , 2 098 of the 2 352 sentences received some analysis , with 206 timing out and 48 failing to parse .", "To deal with the 48 no analysis cases , the cut off for the category dictionary , K , was increased to 100 .", "Of the 48 cases , 23 sentences then received an analysis .", "To deal with the 206 time out cases , 0 was increased to 005 , which resulted in 181 of the 206 sentences then receiving an analysis , with 18 failing to parse , and 7 timing out .", "So overall , almost 98 of the 2 352 unseen sentences were given some analysis .", "To return a single dependency structure , we chose the most probable structure from the S dcl categories spanning the whole sentence .", "If there was no such category , all categories spanning the whole string were considered .", "To measure the performance of the parser , we compared the dependencies output by the parser with those in the gold standard , and computed precision and recall figures over the dependencies .", "Recall that a dependency is defined as a 4 tuple a head of a functor , a functor category , an argument slot , and a head of an argument .", "Figures were calculated for labelled dependencies LP , LR and unlabelled dependencies UP , UR .", "To obtain a point for a labelled dependency , each element of the 4 tuple must match exactly .", "Note that the category set we are using distinguishes around 400 distinct types ; for example , tensed transitive buy is treated as a distinct category from infinitival transitive buy .", "Thus this evaluation criterion is much more stringent than that for a standard pos tag label set there are around 50 pos tags used in the Penn Treebank .", "To obtain a point for an unlabelled dependency , the heads of the functor and argument must appear together in some relation either as functor or argument for the relevant sentence in the gold standard .", "The results are shown in Table 1 , with an additional column giving the category accuracy .", "As an additional experiment , we conditioned the dependency probabilities in 10 on a distance measure A .", "Distance has been shown to be a useful feature for context free treebank style parsers e . g .", "Collins 1996 , Collins 1999 , although our hypothesis was that it would be less useful here , because the CCG grammar provides many of the constraints given by A , and distance measures are biased against long range dependencies .", "We tried a number of distance measures , and the one used here encodes the relative position of the heads of the argument and functor left or right , counts the number of verbs between argument and functor up to 1 , and counts the number of punctuation marks up to 2 .", "The results are also given in Table 1 , and show that , as expected , adding distance gives no improvement overall .", "An advantage of the dependency based evaluation is that results can be given for individual dependency relations .", "Labelled precision and recall on Section 00 for the most frequent dependency types are shown in Table 2 for the model without distance measures . 9 The columns deps give the total number of dependencies , first the number put forward by the parser , and second the number in the gold standard .", "F score is calculated as 2 LP LR LP LR .", "We also give the scores for the dependencies created by the subject and object relative pronoun categories , including the headless object relative pronoun category .", "We would like to compare these results with those of other parsers that have presented dependencybased evaluations .", "However , the few that exist Lin , 1995 ; Carroll et al . , 1998 ; Collins , 1999 have used either different data or different sets of dependencies or both .", "In future work we plan to map our CCG dependencies onto the set used by Carroll and Briscoe and parse their evaluation corpus so a direct comparison can be made .", "As far as long range dependencies are concerned , it is similarly hard to give a precise evaluation .", "Note that the scores in Table 2 currently conflate extracted and in situ arguments , so that the scores for the direct objects , for example , include extracted objects .", "The scores for the relative pronoun categories give a good indication of the performance on extraction cases , although even here it is not possible at present to determine exactly how well the parser is performing at recovering extracted arguments .", "In an attempt to obtain a more thorough analysis , we analysed the performance of the parser on the 24 cases of extracted objects in the goldstandard Section 00 development set that were passed down the object relative pronoun category Sdcl NPX NPX NPX . 10 Of these , 10 41 . 7 were recovered correctly by the parser ; 10 were incorrect because the wrong category was assigned to the relative pronoun , 3 were incorrect because the relative pronoun was attached to the wrong noun , and 1 was incorrect because the wrong category was assigned to the predicate from which the object was 9Currently all the modifiers in nominal compounds are analysed in CCGbank as N N , as a default , since the structure of the compound is not present in the Penn Treebank .", "Thus the scores for N N are not particularly informative .", "Removing these relations reduces the overall scores by around 2 .", "Also , the scores in Table 2 are for around 95 of the sentences in Section 00 , because of the problem obtaining gold standard dependency structures for all sentences , noted earlier .", "10The number of extracted objects need not equal the occurrences of the category since coordination can introduce more than one object per category . extracted .", "The tendency for the parser to assign the wrong category to the relative pronoun in part reflects the fact that complementiser that is fifteen times as frequent as object relative pronoun that .", "However , the supertagger alone gets 74 of the object relative pronouns correct , if it is used to provide a single category per word , so it seems that our dependency model is further biased against object extractions , possibly because of the technical unsoundness noted earlier .", "It should be recalled in judging these figures that they are only a first attempt at recovering these long range dependencies , which most other widecoverage parsers make no attempt to recover at all .", "To get an idea of just how demanding this task is , it is worth looking at an example of object relativization that the parser gets correct .", "Figure 2 gives part of a dependency structure returned by the parser for a sentence from section 00 with the relations omitted . 11 Notice that both respect and confidence are objects of had .", "The relevant dependency quadruples found by the parser are the following", "This paper has shown that accurate , efficient widecoverage parsing is possible with CCG .", "Along with Hockenmaier and Steedman 2002b , this is the first CCG parsing work that we are aware of in which almost 98 of unseen sentences from the CCGbank can be parsed .", "The parser is able to capture a number of longrange dependencies that are not dealt with by existing treebank parsers .", "Capturing such dependencies is necessary for any parser that aims to support wide coverage semantic analysis say to support question answering in any domain in which the difference between questions like Which company did Marks sue ? and Which company sued Marks ? matters .", "An advantage of our approach is that the recovery of long range dependencies is fully integrated with the grammar and parser , rather than being relegated to a post processing phase .", "Because of the extreme naivety of the statistical model , these results represent no more than a first attempt at combining wide coverage CCG parsing with recovery of deep dependencies .", "However , we believe that the results are promising .", "In future work we will present an evaluation which teases out the differences in extracted and insitu arguments .", "For the purposes of the statistical modelling , we are also considering building alternative structures that include the long range dependencies , but which can be modelled using better motivated probability models , such as generative models .", "This will be important for applying the parser to tasks such as language modelling , for which the possibility of incremental processing of CCG appears particularly attractive .", "Thanks to Miles Osborne and the ACL 02 referees for comments .", "Various parts of the research were funded by EPSRC grants GR M96889 and GR R02450 and EU FET grant MAGICSTER ."], "summary_lines": ["Building Deep Dependency Structures Using A Wide-Coverage CCG Parser\n", "This paper describes a wide-coverage statistical parser that uses Combinatory Categorial Grammar (CCG) to derive dependency structures.\n", "The parser differs from most existing wide-coverage treebank parsers in capturing the long-range dependencies inherent in constructions such as coordination, extraction, raising and control, as well as the standard local predicate-argument dependencies.\n", "A set of dependency structures used for training and testing the parser is obtained from a treebank of CCG normal-form derivations, which have been derived (semi-) automatically from the Penn Treebank.\n", "The parser correctly recovers over 80% of labelled dependencies, and around 90% of unlabelled dependencies.\n", "We provide examples showing how heads can fill dependency slots during a derivation, and how long-range dependencies can be recovered through unification of co-indexed head variables.\n", "We define predicate argument structure for CCG in terms of the dependencies that hold between words with lexical functor categories and their arguments.\n"]}
{"article_lines": ["A Maximum Entropy Model for Prepositional Phrase Attachment Adwait Ratnaparkhi , Jeff Reynar , and Salim Roukos IBM Research D iv is ion Thomas J . Watson Research Center York town Heights , NY 10598 1 .", "Introduction A parser for natural language must often choose between two or more equally grammatical parses for the same sentence .", "Often the correct parse can be determined from the lexical properties of certain key words or from the context in which the sentence occurs .", "For example in the sentence , In July , the Environmental Protection Agency imposed agrad ual ban on virtually all uses of asbestos .", "the prepositional phrase on virtually all uses of asbestos can attach to either the noun phrase a gradual ban , yielding vP imposed JvP a gradual ban pp on virtually all uses of asbestos , or the verb phrase imposed , yielding vP imposed uP a gradual ban iop on virtually all uses off asbestos .", "For this example , a human annotators attachment decision , which for our purposes is the correct attachment , is to the noun phrase .", "We present in this paper methods for con structing statistical models for computing the probability of attachment decisions .", "These models could be then integrated into scoring the probability of an overall parse .", "We present our methods in the context of prepositional phrase PP at tachment .", "Earlier work 11 on PP attachment for verb phrases whether the PP attaches to the preceding noun phrase or to the verb phrase used statistics on co occurences of two bigrams the main verb V and preposition P bigram and the main noun in the object noun phrase N1 and preposition bigram .", "In this paper , we explore the use of more features to help in modeling the distribution of the binary PP attachment deci sion .", "We also describe a search procedure for selecting a good subset of features from a much larger pool of features for PP attachment .", "Obviously , the feature search cannot be Jeff Reynar , f rom University of Pennsylvania , worked on this project as a summer student at I . B . M .", "guaranteed tobe optimal but appears experimentally to yield a good subset of features as judged by the accuracy rate in making the PP attachment decisons .", "These search strategies can be applied to other attachment decisions .", "We use data from two treebanks the IBM Lancaster Treebank of Computer Manuals and the University of Pennsylvania WSJ treebank .", "We extract he verb phrases which include PP phrases either attached to the verb or to an object noun phrase .", "Then our model assigns a probability to either of the possible attachments .", "We consider models of the exponential family that are derived using the Maximum Entropy Principle 1 .", "We begin by an overview of ME models , then we describe our feature selection method and a method for constructing a larger pool of features from an exisiting set , and then give some of our results and conclusions .", "Maximum Entropy Modeling The Maximum Entropy model 1 produces aprobability dis tribution for the PP attachment decision using only informa tion from the verb phrase in which the attachment occurs .", "We denote the partially parsed verb phrase , i . e . , the verb phrase without the attachment decision , as a history h , and the conditional probability of an attachment asp dlh , where d 6 .", "0 , 1 and corresponds to a noun or verb attachment respectively .", "The probability model depends on certain features of the whole event h , d denoted by fi h , d .", "An example of a binary valued feature function is the indicator function that a particular V , P bigram occured along with the attachment decision being V , i . e .", "fprint , on h , d is one if and only if the main verb of h is print , the preposition is on , and d is V .", "As discussed in 6 , the ME principle leads to a model for p dlh which maximizes the training data log likelihood , a log p dlh , h , d where h , w is the empirical distribution of the training set , and where p dlh itself is an exponential model 250 p dlh k 11 eXY hd i 0 1 k YI e f h 0 d 0 i 0 4 .", "Head Noun of the Object of the Preposition N2 For example , questions on the history imposed a gradual ban on virtually all uses of asbestos , can only ask about he following four words At the maximum of the training data log likelihood , the model has the property that its k parameters , namely the Ats , satisfy k constraints on the expected values of feature functions , where the ith constraint is , EmA . f imposed ban on uses The notion of a head word here corresponds loosely to the notion of a lexical head .", "We use a small set of rules , called a Tree Head Table , to obtain the head word of a constituent 12 .", "We allow two types of binary valued questions The model expected value is , Emf h p d lh f i h , d h , d 1 .", "Questions about the presence of any n gram n _ 4 of the four head words , e . g . , a bigram maybe V i s , P o f .", "Features comprised solely of questions on words are denoted as word features .", "and the training data expected value , also called the desired value , is f , d f , h , d h , d The values of these k parameters can be obtained by one of many iterative algorithms .", "For example , one can use the Gen eralized Iterative Scaling algorithm of Darroch and Ratcliff 3 .", "As one increases the number of features , the achievable maximum of the training data likelihood increases .", "We de scribe in Section 3 a method for determining a reliable set of features .", "Features Feature functions allow us to use informative characteristics of the training set in estimating p dlh .", "A feature is defined as follows . h , d d _f 1 , i f fd OandVq6 Q , q h 1 O , otherwise .", "I . where Q is a set of binary valued questions about h . We restrict he questions in any Q ask only about he following four head words I .", "Head Verb V 2 .", "Head Noun N1 3 .", "Head Preposition P .", "Questions that involve the class membership of a head word .", "we use a binary hierarchy of classes derived by mutual information clustering which we describe below .", "Given a binary class hierarchy , we can associate a bit string with every word in the vocabulary .", "Then , by querying the value of certain bit positions we can con stmct binary questions .", "For example , we can ask whether about a bit position for any of the four head words , e . g . , Bi t 5 of P repos i t ion i .", "We discuss be low a richer set of these questions .", "Features comprised solely of questions about class bits are denoted as class features , and features containing questions about both class bits and words are denoted as mixed features 1 .", "Before discussing , feature selection and construction , we give a brief overview of the mutual information clustering of words .", "Mutual Information Bits Mutual information clustering , as described in 10 , creates a a class tree for a given vocab ulary .", "Initially , we take the C most frequent words usually 1000 and assign each one to its own class .", "We then take the C 1 st word , assign it to its own class , and merge the pair of classes that minimize the loss of average mutual informa tion .", "This repeats until all the words in the vocabulary have been exhausted .", "We then take our C classes , and use the same algorithm to merge classes that minimize the loss of mutual information , until one class remains .", "If we trace the order in which words and classes are merged , we can form a binary tree whose leaves consists of words and whose root is the class which spans the entire vocabulary .", "Consequently , we uniquely identify each word by its path from the root , which 1 See Table 7 for examples of features 251 can be represented by a string of binary digits .", "If a path lengt of a word is less than the maximum depth , we pad the bottor of the path with Os dummy left branches , so that all word are represented by an equally long bitstring .", "Class feature query the value of bits , and hence examine the path of th word in the mutual information tree .", "Special Features In addition to the types of features de scribed above , we employ two special features in the MI model , the Complement and the Null feature .", "The Comple ment , defined as fcomr , h , d dJ 1 , 0 , otherwise . ifffi hd 0Vfi 6 . , 4 will fire on a pair h , d when no other fi in the model applie , , The Initial feature is simply clef I1 , i f fd O fn zz h , d , 0 , otherwise and causes the ME model to match the a pr i o r i probability of seeing an N attachment .", "Feature Search The search problem here is to find an optimal set of features A4 for use in the ME model .", "We begin with a search space 79 of putative features , and use a feature ranking criterion which incrementally selects the features in . A4 , and also incremen tally expands the search space 79 .", "Initially 79 consists of all 1 , 2 , 3 and 4 gram word features of the four headwords that occur in the training histories 2 , and 4 all possible unigram class features 3 .", "We obtain E 15 k l word features from each training history , and , assuming each word is assigned m bits , a total of 2m 4 unigram class features , e . g . , there are 2m features per word B i t 1 o f Verb O , B i t 1 o f Verb 1 .", "B i t m o f Verb 0 , B i t m o f Verb I The feature search then proceeds as follows 1 .", "Initialize 79 as described above , initialize A , 4 to contain complement and null feature 2 .", "Select the best feature from 79 using Delta Likelihood rank 3 .", "Add it to . A4 2With a certain f requency cut off , usual ly 3 to 5 3 Also with a certain f requency cut off 0 . 85 0 . 8 0 . 75 0 . 7 0 . 65 0 . 6 0 . 55 0 . 5 PERFORMANCE Wall St . Journal io io .", "20 1 O0 120 140 160 180 200 Figure 1 Performance of Maximum Entropy Model on Wall St . Journal Data 4 .", "Train Maximum Entropy Model , using features in . A4 5 .", "Grow 79 based on last feature selected 6 . repeat from 2 If we measure the training entropy and test entropy after the addition of each feature , the training entropy will monotoni cally decrease while the test entropy will eventually reach a minimum due to overtraining .", "Test set performance usually peaks at the test entropy minimum see Fig .", "Delta Likelihood At step 2 in the search , we rank all fea tures in 7 9 by estimating their potential contribution to the log likelihood of the training set .", "Let q be the conditional probability distribution of the model with the features cur rently in A , 4 .", "Then for each f 6 79 , we compute , by estimat ing only , the probability distribution p that results when fi is added to the ME model p dlh q dlh e J , h , d 1 E q wlh e J h ?", "0 We then compute the increase in log likelihood with the new model 6L , IS h , w lnp wlh e h , w lnq wlh h , w h , w and choose the feature with the highest 6L .", "Features redun dmlt or correlated to those features already in . A . 4 will produce 252 1 0 . 9 0 . 0 . 7 0 . 6 0 . 5 ENTROPY Wall St . Journal Training 0 . 4 20 A dO dO .", "100 120 140 160 180 200 Figure 2 Entropy of Maximum Entropy Model on Wall St . Journal Data a zero or negligible 6L , and will therefore be outranked by genuinely informative features .", "The chosen feature is added to M and used in the ME Model .", "Growth of Putative Feature Set At step 5 in the search we expand the space 7 of putative features based on the feature last selected from 72 for addition to M . Given an n gram feature f i . e . , of type word , class or mixed that was last added to M , we create 2m . 4 new n 1 gram features which ask questions about class bits in addition to the questions asked in fi .", "E . g . , let fi h , d constrain d 0 and constrain h with the questions v imposed , P on Then , given fi h , d , the 2m new features generated for just the Head Noun are the following V imposed , P on , B i t 1 fo r Noun 0 V imposed , P on , B i t 1 fo r Noun 1 V imposed , P B i t m fo r Noun 0 on V imposed , P on , B i t m fo r Noun 1 We construct he remaining 6m features imilarly from the remaining 3head words .", "We skip the construction of features Computer Manuals Wall St . Journal Training Events 8264 20801 Test Events 943 3097 Table 1 Size of Data containing questions that are inconsistent or redundant with those word or class questions in fi .", "The newly created features are then added to P , and compete for selection in the next Delta Likelihood ranking process .", "This method allows the introduction of complex features on word classes while keeping the search space manageable ; P grows linearly with . M .", "Resu l ts We applied the Maximum Entropy model to sentences from two corpora , the I . B . M .", "Computer Manuals Data , annotated by Univ .", "of Lancaster , and the Wall St . Journal Data , annotated by Univ .", "The size of the training sets , test sets , and the results are shown in Tables 1 2 .", "The experiments in Table 2 differ in the following manner Words Only The search space P begins with all possible n gram word features with n being 1 , 2 , 3 , or 4 ; this feature set does not grow during the feature search .", "Classes Only The search space P begins with only un igram class features , and grows by dynamically contructing class n gram questions as described earlier .", "Word and Classes The search space P begins with all possible n gram word features and unigram class features , and grows by adding class questions as described earlier .", "The results in Table 2 are achieved in the neighborhood of about 200 features .", "As can be seen in Figure 1 , performance improves quickly as features are added and improves rather very slowly after the 60 th feature .", "The performance is fairly close for the various feature sets when a sufficient number of features are added .", "We also compared these results to a deci sion tree grown on the same 4 head word events .", "The same Experiment Computer Manuals Wall St . Journal Words Only 82 . 2 77 . 7 Classes Only 84 . 5 79 . 1 Words and Classes 84 . 1 81 . 6 Table 2 Performance of ME Model on Test Events 253 Domain Performance Computer Manuals 79 . 5 Wall St . Journal 77 . 7 Table 3 Decision Tree Performance mutual intbrmation bits were used for growing the decision trees .", "Table 3 gives the results on the same training and test data .", "The VIE models are slightly better than the decision tree models .", "For comparison , we obtained the PP attachment performances of 3 treebanking experts on a set of 300 randomly selected test events from the WSJ corpus .", "In the first trial , they were given only the four head words to make the attachment decision , and in the next , they were given the headwords along with the sentence in which they occurred .", "Figure 3 shows an example of the head words test a .", "The results of the treebankers and the performance of the ME model on that same set are shown in Table 5 .", "We also identified the set of 274 events on which treebankers , given the sentence , unanimously agreed .", "We defined this to be the truth set .", "We show in Table 6 the agreement on PP attachment of the original WSJ treebank parses with this consensus set , the average performance of the 3 human experts with head words only , and the ME model .", "The WSJ treebank indicates the accuracy rate of our training data , the human performance indicates how much information is in the headwords , and the ME model is still a good 12 4 the key is N , V , N , N , V , N , N , N , N , V , V , N , V , N , N , N , V , N , V percentage points behind .", "Selection Order Feature 1 Preposition of 2 Bit 2 of Head Noun 0 3 Preposition is to 4 Bit 12 of Head Noun 1 9 Head Noun million , Preposition in 30 Preposition to , Bit 8 of Object 1 47 Preposition in , Object months Table 4 Examples of Features Chosen for Wall St . Journal Data Average Human head words only 88 . 2 Average Human with whole sentence 93 . 2 ME Model 78 . 0 Table 5 Average Performance of Human ME Model on 300 Events of WSJ Data Events WSJ TB Human ME Model in Consensus Performance Performance Performance 274 95 . 7 92 . 5 80 . 7 Table 6 Human and ME model performance on consensus set for WSJ report mi l l l ion in charges report mi l l l ion for quarter ref lect ing sett lement of contracts carr ied all but one were in jur ies among workers had damage to bui ld ing be damage to some uses var ia t ion of design c i ted example of d istr ict leads Pepsi in share trai ls Pepsi in sales r isk conf l ict w i th U . S . r isk conf l ict over p lan oppose seat ing as delegate save some of plants introduced vers ions of cars lowered bids in ant ic ipat ion oversees trading on Nasdaq gained 1 to 19 Figure 3 Sample of 4 head words for PP attachment We also obtained the performances of 3 non experts on a set of 200 randomly selected test events from the Computer Manuals corpus .", "In this trial , the participants made attachment decisions given only the four head words .", "The results are shown in Table 7 .", "Conclusion The Maximum Entropy model predicts prepositional phrase attachment 10 percentage points less accurately than a tree banker , but it performs comparably toa non expert , assuming that only only the head words of the history are available in both cases .", "The biggest improvements to the ME model will come from better utilization of classes , and a larger history .", "Currently , the use of the mutual information class bits gives us a few percentage points in performance , but the ME model should gain more from other word classing schemes which are better tuned to the PP attachment problem .", "A scheme in which the word classes are built from the observed attach ment preferences of words ought to outperform the mutual information clustering method , which uses only word bigram distributions 10 .", "254 I Average Human I 77 3 ME Model 83 . 5 Table 7 Average Performance of Human ME Model on 200 Events of Computer Manuals Data Secondly , the ME model does not use information contained in the rest of the sentence , although it is apparently useful in predicting the attachment , as evidenced by a 5 average gain in the treebankers accuracy .", "Any implementation f this model using the rest of the sentence would require features on other words , and perhaps features on the sentences parse tree structure , coupled with an efficient incremental search .", "Such improvements should boost the performance of the model to that of treebankers .", "Already , the ME model out performs a decision tree confronted with the same task .", "We hope to use Maximum Entropy to predict other linguistic phe nomena that hinder the performance of most natural anguage parsers .", "Jaynes , E . T . , Information Theory and Statistical Mechanics .", "Kullback , S . , Information Theory in Statistics .", "Wiley , New York , 1959 .", "and Ratcliff , D . , Generalized Iterative Scaling for Log Linear Models , The Annals of Mathematical Statis tics , Vol .", "43 , pp 1470 1480 , 1972 .", "Delia Pietra , S . , Della Pietra , V . , Mercer , R . L . , Roukos , S . , Adaptive Language Modeling Using Minimum Discriminant Estimation , Proceedings oflCASSP 92 , pp .", "1 633 636 , San Francisco , March 1992 .", "Brown , P . , Delia Pietra , S . , Della Pietra , V . , Mercer , R . , Nadas , A . , and Roukos , S . , Maximum Entropy Methods and Their Applications to Maximum Likelihood Parameter Estimation of Conditional Exponential Models , A forthcoming IBM techni cal report .", "Berger , A . , Della Pietra , S . A . , and Della Pietra , V . J . . Maxi mum Entropy Methods in Machine Translation .", "manuscript in preparation .", "Black , E . , Garside , R . , and Leech , G . , 1993 .", "Statistically driven Computer Grammars of English The IBM Lancaster Approach .", "Atlanta , Georgia .", "Black , E . , Jelinek , F . , Lafferty , J . , Magerman , D . M . , Mercer , R . , and Roukos , S . , 1993 .", "Towards History based Grammars Using Richer Models for Probabilistic Parsing .", "In Proceed ings of the Association for Computational Linguistics , 1993 .", "Columbus , Ohio .", "Breiman , L . , Friedman , J . H . , Olshen , R . A . , and Stone , C . J . , 1984 .", "Classification and Regression Trees .", "Wadsworth and Brooks .", "Pacific Grove , California .", "Brown , P . F . , Della Pietra , V . J . , deSouza , P . V . , Lai , J . C . , and Mercer , R . L . Class based n gram Models of Natural Language .", "In Proceedings of the IBM Natural Language 1TL , March , 1990 .", "Hindle , D . and Rooth , M . 1990 .", "Structural Ambiguity and Lex ical Relations .", "In Proceedings ofthe June 1990 DARPA Speech and Natural Language Workshop .", "Hidden Valley , Pennsylva nia .", "Magerman , D . , 1994 .", "Natural Language Parsing as Statistical Pattern Recognition .", "Ph . D . dissertation , Stanford University , California ."], "summary_lines": ["A Maximum Entropy Model For Prepositional Phrase Attachment\n", "We construct a benchmark dataset of 27,937 pp-attachment quadruples extracted from the Wall Street Journal corpus.\n", "We train a maximum entropy model on (v, n1, p, n2) quadruples extracted from the Wall Street Journal corpus and achieve 81.6% accuracy.\n", "Our maximum entropy approach uses the mutual information clustering algorithm.\n"]}
{"article_lines": ["Anaphora For Everyone Pronominal Anaphora Resolution Without A Parser", "We present an algorithm for anaphora res olutkm which is a modified and extended version of that developed by Lappin and Leass , 994 .", "In contrast to that work , our al gorithm does not require in depth , full , syn . .", "tactic parsing of text .", "Instead , with minimal compromise in output quality , the modifica tions enable the resolution process to work from tile output of a part of speech tag ge ; enriched only with annotations of gram matica functkm of lexical items in the in put text stream .", "Evaluation of the results of our in tplementation demonstrates that ac curate anaphora resolution can be realized within natural anguage processing fl'ame works which do not , r cannot employ ro bust and rcqiable parsing components .", "l , appin and Leass , 1994 describe an algorithm for pronominal anaphora resolution with high rate of cor rect analyses .", "While one of the strong points of this algorithm is that it operates primarily on syntactic in formation ahme , this also turns out to be a limiting factor for its wide use current state of the art of prac tically applicable parsing technology still falls short of robust and reliable delivery of syntactic analysis of real texts to the level of detail and precision that the filters a nd constraints described by I , appin and l , eass assume .", "We are particularly interested in a class of text pro cessing applications , capable of delivery of content analysis to a depth inw lving non trivial amount of discourse processing , including anaphora resolution .", "The operational context prohibits us from making any assumptions concerning domain , style , and genre of input ; as a result , we have developed a text processing framework which builds its capabilities entirely on the basis of a considerably shallower linguistic analysis of the input stream , thus trading off depth of base level analysis for breadth of cown age .", "In this paper , we present work on modifying the lmp pin Leass algorithm in a way which enables it to work off a flat morpho syntactic analysis of the sentences of a text , while retaining a degree of quality and accuracy in pronorainal anaphora resolution comparable to that reported in Lappin and l , eass , 1994 .", "The modifica tions discussed below make the algorithm available to a wide range of text processing frameworks , which , due to the lack of full syntactic parsing capability , nor really would have been unable to use this high preci sion anap hora resolution tool .", "The work is additionally important , we feel , as it shows that informatkm about the content and logical structure of a text , in princi .", "pie a core requirement for higher level semantic and discourse processes , can be effectively approximated by the right mix of constituent analysis and inferences about functional relations .", "The base level linguistic analysis for actaphora resolu tion is the output of a part of speech tagger , augmented with syntactic function annotatkms for each input to .", "ken ; this kind of analysis is generated by the mor pbosyntactic tagging system described in Voutilainen et al , 1992 , Karlsson et al , 1995 hencehvth 1 , 1NC ; olq' .", "In addition to extremely high levels of accuracy in recall and precision of tag assignment VoutiJainen et al , 1992 report 99 . 77 ? , , overall recall and 95 . 54 overall preciskm , over a variety of text genres , and in comparison with other state of the art tagging sys tems , the primary motivation for adopting this system is the requirement todevelop a robust ext processor with anaphora resolution being just one of its discourse analysis functkms capable of reliably handling arbi trary kinds of input .", "The tagger provides a very simple analysis of the structure of the text for each lexical item in each sen tence , it provides a set of values which indicate the morphological , lexical , grammatical nd syntactic fea tures of the item in tile context in which it appears .", "In addition , the modified algorithm we present requh es annota tion of the input text stream by a simple position identification function which associates an integer with each token in a text sequentially we will refer to a to ken's integer value as its oJ et .", "As an example , given the text For 1995 the company set up its headquar ters in Hall l , the newest and most presti .", "gious of CeBIT's 23 hal Is . tile anaphora resolutkm algorithm would be presented with the h llowing analysis tream .", "Note , in particu .", "lar , the grammatical function information e . g . , SUl J , O q . FMAINV and the integer values e . g . , offt 39 asso cia ted with each token .", "For o f f139 for PREP ADVL 1995 o f f140 . . . .", "1995 NUM CARD P the o f f l41 the DET CENTRAL ART SG PL DN company o f f142 company N NOM SG PL SUBJ set off143 set V PAST VF IN FMAINV up of f144 up ADV ADVL ADVL i t s o f f145 . . . . it PRON GEN SG3 GN headquar ters o f f146 . . . . headquar ters N NOM SG PL OBJ in o f f147 . . . . in PREP NOM ADVL Ha l l o f f148 . . . . hal l N NOM SG NN l l o f f149 Ii NUM CARD P , o f f l50 . . . .", ", PUNCT the o f f l51 the DET CENTRAL ART SG PL DN newest o f f152 . . . . new A SUP PCOMPL O and of f153 . . . . and CC CC most o f f154 much ADV SUP AD A pres t ig ious o f f155 . . . . p res t ig ious A ABS P of o f f156 . . . . of PREP NOM OF CeBIT ' s o f f157 cebit N GEN SG GN 23 0f f158 . . . .", "23 NUM CARD QN ha l l s o f f159 . . . . hal l N NOM PL P . o f f160 . . . .", "PUNCT 2 . 1 Data collection .", "Although LINGSOFT does not provide specific infor mation about constituent structure , partial constituen cy specifically , identification of sequences of tokens as phrasal units can be inferred from the analysis by running the tagged text through a set of filters , which are stated as regular expressions over metatokens such as the ones illustrated above .", "For the purposes of anaphora resolution , the pri mary data set consists of a complete listing of all noun phrases , reduced to modifier head sequences .", "This data set is obtained by means of a phrasal grammar whose patterns characterize the composition of a noun phrase NP in terms of possible token sequences .", "The output of NP identification is a set of token feature matrix offset sequences , where offset value is deter mined by the offset of the first token in the sequence .", "The offset indicates the position of the NP in the text , and so provides crucial information about precedence relations .", "A secondary data set consists of observations about the syntactic ontexts in which the NPs identified by the phrasal grammar appear .", "These observations are derived using a set of patterns designed to detect nom inal sequences in two subordinate syntactic environ ments containment in an adverbial adjunct and con tainment in an NP i . e . , containment in a prepositional or clausal complement of a noun , or containment in a relative clause .", "This is accomplished by running a set of patterns which identify NPs that occur locally to ad verbs , relative pronouns , and noun preposition r noun complementizer sequences over the tagged text in con junction with the basic NP patterns described above .", "Because the syntactic , patterns are stated as regular ex pressions , misanalyses are inevitable .", "In practice , how ever , the extent o which incorrect analyses of syntactic context affect the overall accuracy of the algorithm is not large ; we will return to a discussion of this point in section 4 .", "A third set of patterns identifies and tags occurrences of expletive it .", "These patterns target occurrences of the pronoun it in certain contexts , e . g . , as the subject of members of a specific set of verbs seem , appear , etc . , or as the subject of adjectives with clausal complements .", "Once the extraction procedures are complete and the results unified , a set of discourse referents abstract ob jects which represent the participants inthe discourse is generated from the set of NP observations .", "A particu larly convenient implementation f discourse referents is to represent them as objects in the Common Lisp Object System , with slots which encode the following information parameters where ADJUNCT and EMBED indicate whether a discourse referent was observed in either of the two syntactic ontexts discussed above TEXT text form TYPE referential type e . g . , REF , PRO , RFLX AGR person , number , gender GFUN grammatical function ADJUNCT T o r NIL EMBED T o r NIL POS text position Note that each discourse referent contains information about itself and the context in which it appears , but the only information about its relation to other dis course referents is in the form of precedence r lations as determined by text position .", "The absence of explicit information about configurational relations marks the crucial difference between our algorithm and the Lap pin Leass algorithm .", "Lappin and Leass , 1994 use configurational information in two ways as a factor in the determination of the salience of a discourse refer ent discussed below , and as input to a set of disjoint reference filters .", "Our implementation seeks to perform exactly the same tasks by inferring hierarchical rela tions from a less rich base .", "The modifications and assumptions required to accomplish this goal will be highlighted in the following discussion .", "2 . 2 Anaphora resolution .", "Once the representation f the text has been recast as a set of discourse referents ordered by offset value , it is sent to the anaphora resolution algorithm proper .", "The basic logic of the algorithm parallels that of the Lap pin Leass algorithm .", "The interpretation procedure in volves moving through the text sentence by sentence and interpreting the discourse referents in each sen tence from left to right .", "There are two possible in terpretations of a discourse referent either it is taken to introduce a new participant in the discourse , or it is taken to refer to a previously interpreted iscourse referent .", "Coreference is determined by first eliminating from consideration those discourse referents to which an anaphoric expression cannot possibly refer , then se lecting the optimal antecedent from the candidates that remain , where optimality is determined by a salience measure .", "In order to present the details of anaphora resolution , we define below our notions and implementations of coreference and salience .", "2 . 2 . 1 Coreference As in the Lappin and Leass algorithm , the anaphor antecedent relation is established between two dis course referents cf .", "Helm , 1982 , Kamp , 1981 , hile the more general notion of coreference is represented in terms of equivalence classes of anaphorically re lated discourse referents , which we will refer to as COREF classes .", "Thus , the problem of interpreting an anaphoric expression boils down to the problem of es tablishing an anaphoric link between the anaphor and some previously interpreted iscourse referent pos sibly another anaphor ; a consequence of establishing 114 this link is that the anaphor becomes a member of the COREF class already associated with its antecedent .", "In our implementation , COREF classes are repre sented as objects in the Common Lisp Object System which contain information about the COREF class as a whole , including canonical form typically deter mined by the discourse referent which introduces the class , membership , and , most importantly , salience discussed below .", "1 The connection between a dis course referent and its COREF class is mediated through the COREF object as follows every discourse referent includes an information parameter which is a pointer to a COREF object ; discourse referents which have been determined to be coreferential share the same COREF value and so literally point to the same object .", "Imple menting coreference in this way provides a means of getting from any discourse referent in a COREF class to information about the class as a whole .", "2 . 2 . 2 Salience The information parameter of a COREF object most cru cial to anaphora resolution is its salience , which is de termined by the status of the members of the COREF class it re . presents with respect to 10 contextual , gram matical , and syntactic onstraints .", "Following Lappin and Leass , 1994 , we will refer to these constraints as salience factors .", "Individual salience factors are asso ciated with numerical values ; the overall salience , or salience weight of a COREF is the sum of the values of the salience factors that are satisfied by some member of the COREF class note that values may be satisfied at most once by each member of the class .", "The salience factors used by our algorithm are defined below with their values .", "Our salience factors mirror those used by Lappin and Leass , 1994 , with the exception of Poss s , discussed below , and CNTX S , which is sensitive to the context in which a discourse referent appears , where a context is a topically coherent segment of text , as deter mined by a text segmentation algorithm which follows Hearst , 1994 .", "SENT S 100 iff in the current sentence CNTX S 50 iff in the current context SUBJ S 80 iff GFUN subject EXST S 70 iff in an existential construction POSS S 65 iff GFUN possessive ACC S 50 iff GFUN direct object DAT S 40 iff GFUN indirect object OBLQ S 30 iff the complement of a preposition HEAD S 80 iff EMBED NIL ARG S 50 iff ADJUNCT NIL Note that the values of salience factors are arbitrary ; what is crucial , as pointed out by Lappin and Leass , 1994 , is the relational structure imposed on the factors by these values .", "The relative ranking of the factors is justified both linguistically , as a reflection of the role of the functional hierarchy in determining anaphoric relations cf .", "Keenan and Comrie , 1977 , as well as by experimental results both Lappin and Leass' and our own .", "For all factors except CNTX S and POSS S , we adopt the values derived from a series of experiments described in Lappin and Leass , 1994 which used dif ferent settings to determine the relative importance of 1The implementation of aCOREF object needs to be aware of po tenlial circularities , thus a COREF does not actually contain its member discourse r ferents , but rather alisting of their offsets , each factor as a function of the overall success of the algorithm .", "Our values for CNTX S and POSS S were de termined using similar tests .", "An important feature of our implementation of salience , following that of Lappin and Leass , is that it is variable the salience of a COREF class decreases and increases according to the frequency of reference to the class .", "When an anaphoric link is established between a pronoun and a previously introduced iscourse refer ent , the pronoun is added to the COREF class associated with the discourse referent , its COREF value is set to the COREF value of the antecedent i . e . , to the COREF ob ject which represents he class , and the salience of the COREF object is recalculated according to how the new member satisfies the set of salience factors .", "This final step raises the overall salience of the COREF , since the new member will minimally satisfy SENT S and CNTX S .", "Salience is not stable , however in order to realisti cally represent the local prominence of discourse ref erents in a text , a decay function is built into the algo rithm , so that salience weight decreases over time .", "If new members are not added , the salience weight of a COREF eventually reduces to zero .", "The consequence of this variability in salience is that a very general heuris tic for anaphora resolution is established resolve a pronoun to the most salient candidate antecedent .", "2 . 2 . 3 Interpretation As noted above , in terms of overall strategy , the resolu tion procedure follows that of Lappin and Leass .", "The first step in interpreting the discourse referents in a new sentence isto decrease the salience weights of the COREF classes that have already been established by a factor of two .", "Next , the algorithm locates all non anaphoric dis course referents in the sentence under consideration , generates a new COREF class for each one , and calcu lates its salience weight according to how the discourse referent satisfies the set of salience factors .", "The second step involves the interpretation f lexical anaphors reflexives and reciprocals .", "A list of candi date antecedent anaphor pairs is generated for every lexical anaphor , based on the hypothesis that a lexical anaphor must refer to a coargument .", "In the absence of configurational information , coarguments are iden tified using grammatical function information as de termined by LINGSOFT and precedence relations .", "A reflexive can have one of three possible grammatical function values direct object , indirect object , or oblique .", "In the first case , the closest preceding discourse referent with grammatical function value subject is identified as a possible antecedent .", "In the latter cases , both the clos est preceding subject and the closest preceding direct object hat is not separated from the anaphor by a sub ject are identified as possible antecedents .", "If more than one possible antecedent is located for a lexical anaphor , the one with the highest salience weight is determined to be the actual antecedent .", "Once an antecedent has been located , the anaphor is added to the COREF class associated with the antecedent , and the salience of the COREF class is recalculatec accordingly .", "The final step is the interpretation f pronouns .", "The basic resolution heuristic , as noted above , is quite sim ple generate a set of candidate antecedents , then es tablish coreference with the candidate which has the greatest salience weight in the event of a tie , the clos est candidateis chosen .", "In order to generate the candi date set , however , those discourse referents with which 115 a pronoun cannot refer must be eliminated from consid eration .", "This is accomplished by running the overall candidate pool the set of interpreted iscourse ref erents whose salience values exceed an arbitrarily set threshold through two sets of filters a set of morpho logical agreement filters , which eliminate from consid eration any discourse referent which disagrees in per son , numbeb or gender with the pronoun , and a set of disjoint reference filters .", "The determination f disjoint reference represents a significant point of divergence between our algorithm and the Lappin Leass algorithm , because , as is well known , configurational relations play a prominent role in determining which constituents in a sentence a pro noun may refer to .", "Three conditions are of particular relevance to the anaphora resolution algorithm Condition A pronoun cannot corefer with a coargument .", "Condition 2 A pronoun cannot corefer with a nonpronominal constituent which it both commands and precedes .", "Condition 3 A pronoun cannot corefer with a constituent which contains it .", "In the absence of configurafional information , our al gorithm relies on inferences from grammatical func tion and precedence todetermine disjoint reference .", "In practice , even without accurate information about con stituent structure , the syntactic filters described below are extremely accurate see the discussion of this point in section 4 .", "Condition i is implemented bylocating all discourse referents with GFUN value direct object , indirect object , or oblique which follow a pronoun with GFUN value subject or direct object , as long as no subject intervenes the hypothesis being that a subject indicates the beginning of the next clause .", "Discourse referents which satisfy these conditions are identified as disjoint .", "Condition 2 is implemented by locating for ev ery non adjunct and non embedded pronoun the set of non pronominal discourse referents in its sentence which follow it , and eliminating these as potential an tecedents .", "In effect , the command relation is inferred from precedence and the information provided by the syntactic patterns an argument which is neither con tained in an adjunct nor embedded in another nominal commands those expressions which it precedes .", "Condition 3 makes use of the observation that a dis course referent contains every object o its right with a non nil EMBED value .", "The algorithm identifies as dis joint a discourse referent and every pronoun which fol lows it and has a non nil EMBED value , until a discourse referent with EMBED value NIL is located marking the end of the containment domain .", "Condiditon 3 also rules out coreference between a genitive pronoun and the NP it modifies .", "After the morphological nd syntactic filters have been applied , the set of discourse referents that remain constitute the set of candidate antecedents for the pro noun .", "The candidate set is subjected to a final evalu ation procedure which performs two functions it de creases the salience of candidates which the pronoun precedes cataphora is penalized , and it increases the sa li ence of candida tes which satisfy either a locality or a parallelism condition described below , both of which apply to intrasentential c ndidates .", "The h cality heuristic isdesigned to negate the effects of subordinationwhen both candidate and anaphor ap pear in the same subordinate context , the assumption being that the prominence of a candidate should be de termined with respect o the position of the anaphor .", "This is a point of difference between our algorithm and the one described in Lappin and Leass , 1994 .", "The salience of a candidate which is determined tobe in the same subordinate context as a pronoun determined as a function of precedence r lations and EMBED and ADJUNCT values is temporarily increased to the level it would have were the candidate not in the subordi nate context ; the level is returned to normal after the anaphor is resolved .", "The parallelism heuristic rewards candidates which are such that the pair consisting of the GFUN values of candidate and anaphor are identical to GFUN values of a previously identified anaphor antecedent pair .", "This parallelism heuristic differs from a similar one used by the Lappin Leass algorithm , which rewards candi dates whose grammatical function is identical to that of an anaphor .", "Once the generation and evaluation of the candidate set is complete , the candidates are ranked according to salience weight , and the candidate with the high est salience weight is determined tobe the antecedent of the pronoun under consideration .", "In the event of a tie , the candidate which most immediately precedes the anaphor is selected as the antededent where prece dence is determined by comparing offset values .", "The COREF value of the pronoun is set to that of the an tecedent , adding it to the the antecedent's COREF class , and the salience of the class is recalculated accordingly .", "The larger context from which the sample analysis in the beginning of Section 2 was taken is as follows . . . while Apple and its PowerPC partners claimed some prime real estate on the show floor , Apple's most interesting offerings de buted behind the scenes .", "Gone was the nar row corner booth that Apple shoehorned its products into last year .", "For 1995 the com pany set up its headquarters in Hall 11 , the newest and most prestigious of CeNT's 23 halls .", "The anaphora resolution algorithm generates the fol lowing analysis for the first italicized pronoun .", "For each candidate , the annotation i square brackets in dicates its offset value , and the number to the right indicates its salience weight at the point of interpreta tkm of the pronoun .", "ANA its off 33 CND Apple of 1 131 432 Apple aol f 10 352 its off . 03 352 App e's offf I 5 1352 prilne real estat !", "off 08 165 show f loor aof f 1 2 l 55 year o f 137 I 310 3 The candidate set illustrates several important points .", "First , the equality in salience weights of the candi dates at offsets 101 , 103 , and 115 is a consequence of 2Note that our syntactic filters are quite capable of discarding a number of configurationally inappropriate antecedents , which appear to satisfy the precedence r lation .", "116 the fact that these discourse referents are members of the same COP , Et ' class .", "Their unification into a single class indicates both successful anaphora resolution of the pronoun at offset 103 , as well as the operation of higherqevel discourse processing designed to identify all references to a particular COREF class , not just the anaphoric ones cf .", "Kennedy and Boguraev , 1996 .", "The higher salience of the optimal candidate which ix also a member of this COREF class shows the effect of the locality heuristic described in section 2 . 2 . 3 .", "Both the pronoun and the candidate appear in the same sub ordinate context within a relative clause ; as a result the salience of the candidate but not of the class to which it bekmgs is temporarily boosted to negate the effect of subordinatkm .", "An abbreviated candidate set for the second itali cized pronoun is given below ANA i t s 61of f 145 CND company , ot I 142 H , 0 App l e , ! of 17 13 192 it ; aof I I 3 192 This set is interesting because it illustrates the promi nent role of SENT S in controlling salience company ix correctly identified as the antecedent of the pronotm , despite the frequency of mention of members of the COREF class containing Apple and its , because it occurs in the same sentence as the anaphor .", "Of course , this ex ample also indicates the need fl r additional heuristics designed to connect company with Apple , since these discourse referents clearly make reference to the same object .", "We are currentlyworking towards this goal ; see Kennedy and Boguraev , 996 for discussion .", "'l'he following text segment illust rates the resolution of in tersen ten tia l a napho ra .", "Sun's prototype lntemet access device uses a 1 10 Mhz MicroSPARCprocesso ; and is diskless .", "Its dimensions are 5 . 5 inches x 9 inches x 2inches .", "ANA its aol f 347 CNI IAlte ileL access devic , ! o 33 i 180 M i c KOf ; PARCI rOC e ! s sot 4oEI 34 16 ! i ; un 's 4o1 f 3 t3 I 40 The first sentence in this fl'agment introduces three dis course referents bearing different grammatical func tions , none of which appear in subordinate contexts .", "Since the sentence in which the anaphor occurs does not contain any candidates the discourse referent in troduced by dimensions ix eliminated from considera tion by both the morphok gical nct disjoint reference filters , only those from the previous entence are con sidered each is compatible with the morphological requirements of the anaphor .", "These are ranked ac cording to salience weight , where the crucial factor is grammatical function value .", "The result of the ranking is that Internet access device the candidate which satis fies the highest weighted salience facto1 , SUBl S is the optimal candidate , and so correctly identified as the an tecedent", "Quantitative evaluation shows the anaphora resolution algorithm described here to run at a rate of 75'70 accu racy .", "The data set on which the evaluatkm was based consisted of 27 texts , taken from a random selection of genres , including press releases , product annotmce meats , news stories , magazine articles , and other doc uments existing as World Wide Web pages .", "Within these texts , we counted 3 16 third person anaphoric pro nouns ; of these , 231l were correctly resolved to the dis course referent identified as the antecedent by the first author .", "3 This rate of accuracy is clearly comparable to that of the Lappin Leass algorithm , which Lappin and Leass , 994 report as 85 ? , , .", "Several observations about he results and the com parison with lmppin and I , eass , 1994 are in order .", "First , and most obviously , some deterioratkm in qual ity is to be expected , given the relatively impoverished linguistic base we start with .", "Second , it is important to note that this is not just a matter of simple comparison .", "The results in l . appin and Leass , 1994 describe the output of the procedttre applied to a singh , ' text genre computer manuals .", "Ar guably , this is an example of a particularly well be haved text ; in any case , it is not clear how the figure would be normalized over a wide range of text types , some of them not completely 'clean' , as is the case with our data .", "Third , close analysis of the most common types of error our algorithm currently makes reveals two spe cific configurations in the input which confuse the pro cedure and contribute to the error rate gender mis match 35 of errors and certain long range contextttal stylistic phenomena , best exemplified by text contain ing quoted passages in line 14 of errors .", "Implementing a gender dis agreement fil er is not technically complex ; as noted above , the current algo rithrn contains one .", "The persistence of gender mis matches in the output simply reflects the lack of a con sistent gender slot in the I , NGSOFT tagger output .", "Aug menting the algorithm with a lexical database which includes more detailed gender information will result in improved accuracy .", "Ensuring proper interpretatkm of anaphors both within and outside of quoted text requires , in effect , a method of evaluating quoted speech separately from its surrotmdingcnntext .", "Al hough acomplex problem , we feel that this is possible , given that our input data stream embodies a richer notkm of position and con text , as a resu t of an independent text segmentation procedure adapted from learst , 1994 and discussed above in section 2 . 2 . 2 .", "What is worth noting is the small number of errors which can be directly attributed to the absence of con figurational inh rmation .", "Of the 75 misinterpreted pro nouns , only 2 inw lved a failure to establish configu ratkmally determined disjoint reference both of these inw lved Condition 3 , and only an additional several errors could be tmambiguously traced to a failure to correctly identify the syntactic ontext in which a dis course referent appeared as determined by a misfireof the salience factors ensitive to syntactic context , I lEAD S and ARC S .", "Overall , these considerations lead to two conchl .", "sions .", "First , with the incorporation of more explicit morphological nd contextual information , it should 3The set of 306 anaphoric pronouns excluded 30 occurrences of expletive it not identified by the expletive patterns prhnari ly occurrences in object position , as well as 6 occurrences of it which referred to a VP or propositional constituent .", "We are currently mfinin g the existing expletive patterns for improved accuracy .", "117 be possible to increase the overall quality of our out put , bringing it much closer in line with Lappin and Leass' results .", "Again , straight comparison would not be trivial , as e . g . quoted text passages are not a natural part of computer manuals , and are , on the other hand , an extremely common occurrence in the types of text we are dealing with .", "Second , and most importantly , the absence of ex plicit configurational information does not result in a substantial degradation i the accuracy of an anaphora resolution algorithm that is otherwise similar to that described in Lappin and Leass , 1994 .", "Lappin and Leass' algorithm for pronominal anaphora resolution is capable of high accuracy , but requires in depth , full , syntactic parsing of text .", "The modifications of that algorithm that we have developed make it avail able to a larger set of text processing frameworks , as we assume a considerably 'poorer' analysis ubstrate .", "While adaptations to the input format and interpreta tion procedures have necessarily addressed the issues of coping with a less rich level of linguistic analysis , there is only a small compromise in the quality of the results .", "Our evaluation indicates that the problems with the current implementation donot stem from the absence of a parse , but rather from factors which can be addressed within the constraints imposed by the shallow base analysis .", "The overall success of the algo rithm is important , then , not only for the immediate utility of the particular modifications , but also because the strategy we have developed for circumventing the need for full syntactic analysis is applicable to other in terpretation tasks which , like the problem of anaphora resolution , lie in the space of higher level semantic and discourse analysis ."], "summary_lines": ["Anaphora For Everyone: Pronominal Anaphora Resolution Without A Parser\n", "We present an algorithm for anaphora resolution which is a modified and extended version of that developed by (Lappin and Leass, 1994).\n", "In contrast to that work, our algorithm does not require in-depth, full, syntactic parsing of text.\n", "Instead, with minimal compromise in output quality, the modifications enable the resolution process to work from tile output of a part of speech tagger, enriched only with annotations of grammatical function of lexical items in the in-put text stream.\n", "Evaluation of the results of our implementation demonstrates that accurate anaphora resolution can be realized within natural language processing frameworks which do not -- cannot -- employ robust and reliable parsing components.\n", "We also suggest that anaphora resolution is part of the discourse referents resolution.\n"]}
{"article_lines": ["Learning A Translation Lexicon From Monolingual Corpora", "This paper presents work on the task of constructing a word level translation lexicon purely from unrelated monolingual corpora .", "We combine various clues such as cognates , similar context , preservation of word similarity , and word frequency .", "Experimental results for the construction of a German English noun lexicon are reported .", "Noun translation accuracy of 39 scored against a parallel test corpus could be achieved .", "Recently , there has been a surge in research in machine translation that is based on empirical methods .", "The seminal work by Brown et al . 1990 at IBM on the Candide system laid the foundation for much of the current work in Statistical Machine Translation SMT .", "Some of this work has been re implemented and is freely available for research purposes AlOnaizan et al . , 1999 .", "Roughly speaking , SMT divides the task of translation into two steps a word level translation model and a model for word reordering during the translation process .", "The statistical models are trained on parallel corpora large amounts of text in one language along with their translation in another .", "Various parallel texts have recently become available , mostly from government sources such as parliament proceedings the Canadian Hansard , the minutes of the European parliament1 or law texts from Hong Kong .", "Still , for most language pairs , parallel texts are hard to come by .", "This is clearly the case for low density languages such as Tamil , Swahili , or Tetun .", "Furthermore , texts derived from parliament speeches may not be appropriate for a particular targeted domain .", "Specific parallel texts can be constructed by hand for the purpose of training an SMT system , but this is a very costly endeavor .", "On the other hand , the digital revolution and the wide spread use of the World Wide Web have proliferated vast amounts of monolingual corpora .", "Publishing text in one language is a much more natural human activity than producing parallel texts .", "To illustrate this point The world wide web alone contains currently over two billion pages , a number that is still growing exponentially .", "According to Google , 2 the word directory occurs 61 million times , empathy 383 , 000 times , and reflex 787 , 000 times .", "In the Hansard , each of these words occurs only once .", "The objective of this research to build a translation lexicon solely from monolingual corpora .", "Specifically , we want to automatically generate a one to one mapping of German and English nouns .", "We are testing our mappings against a bilingual lexicon of 9 , 206 German and 10 , 645 English nouns .", "The two monolingual corpora should be in a fairly comparable domain .", "For our experiments we use the 1990 1992 Wall Street Journal corpus on the English side and the 1995 1996 German news wire DPA corpus on the German side .", "Both corpora are news sources in the general sense .", "However , they span different time periods and have a different orientation the World Street Journal covers mostly business news , the German news wire mostly German politics .", "For experiments on training probabilistic translation lexicons from parallel corpora and similar tasks on the same test corpus , refer to our earlier work Koehn and Knight , 2000 , 2001 .", "This section will describe clues that enable us to find translations of words of the two monolingual corpora .", "We will examine each clue separately .", "The following clues are considered more frequent than flower , as its translation Regierung is more frequent than Blume .", "We will now look in detail how these clues may contribute to building a German English translation lexicon .", "Due to cultural exchange , a large number of words that originate in one language are adopted by others .", "Recently , this phenomenon can be seen with words such as Internet , or Aids .", "These terms may be adopted verbatim , or changed by well established rules .", "For instance , immigration German and English has the Portuguese translation immigra c ao , as many words ending in tion have translations with the same spelling except for the ending changed to c ao .", "We examined the German words in our lexicon and tried to find English words that have the exact same spelling .", "Surprisingly , we could count a total of 976 such words .", "When checking them against a benchmark lexicon , we found these mappings to be 88 correct .", "The correctness of word mappings acquired in this fashion depends highly on word length .", "This is illustrated in Table 1 While identical 3letter words are only translations of each other 60 of the time , this is true for 98 of 10 letter words .", "Clearly , for shorter words , the accidental existence of an identically spelled word in the other language word is much higher .", "This includes words such as fee , ton , art , and tag . spelled words are in fact translations of each other The accuracy of this assumption depends highly on the length of the words see Section 2 . 1 Knowing this allows us to restrict the word length to be able to increase the accuracy of the collected word pairs .", "For instance , by relying only on words at least of length 6 , we could collect 622 word pairs with 96 accuracy .", "In our experiments , however , we included all the words pairs .", "As already mentioned , there are some wellestablished transformation rules for the adoption of words from a foreign language .", "For German to English , this includes replacing the letters k and z by c and changing the ending t at by ty .", "Both these rules can be observed in the word pair Elektrizit at and electricity .", "By using these two rules , we can gather 363 additional word pairs of which 330 , or 91 , are in fact translations of each other .", "The combined total of 1339 976 363 word pairs are separated and form the seed for some of the following steps .", "When words are adopted into another language , their spelling might change slightly in a manner that can not be simply generalized in a rule .", "Observe , for instance website and Webseite .", "This is even more the case for words that can be traced back to common language roots , such as friend and Freund , or president and Pr asident .", "Still , these words often called cognates maintain a very similar spelling .", "This can be defined as differing in very few letters .", "This measurement can be formalized as the number of letters common in sequence between the two words , divided by the length of the longer word .", "The example word pair friend and freund shares 5 letters fr e nd , and both words have length 6 , hence there spelling similarity is 5 6 , or 0 . 83 .", "This measurement is called longest common subsequence ratio Melamed , 1995 .", "In related work , string edit distance or , Levenshtein distance has been used Mann and Yarowski , 2001 .", "With this computational means at hand , we can now measure the spelling similarity between every German and English word , and sort possible word pairs accordingly .", "By going through this list starting at the top we can collect new word pairs .", "We do this is in a greedy fashion once a word is assigned to a word pair , we do not look for another match .", "Table 2 gives the top 24 generated word pairs by this algorithm . ing words with most similar spelling in a greedy fashion .", "The applied measurement of spelling similarity does not take into account that certain letter changes such as z to s , or dropping of the final e are less harmful than others .", "Tiedemann 1999 explores the automatic construction of a string similarity measure that learns which letter changes occur more likely between cognates of two languages .", "This measure is trained , however , on parallel sentence aligned text , which is not available here .", "Obviously , the vast majority of word pairs can not be collected this way , since their spelling shows no resemblance at all .", "For instance , Spiegel and mirror share only one vowel , which is rather accidental .", "If our monolingual corpora are comparable , we can assume a word that occurs in a certain context should have a translation that occurs in a similar context .", "Context , as we understand it here , is defined by the frequencies of context words in surrounding positions .", "This local context has to be translated into the other language , and we can search the word with the most similar context .", "This idea has already been investigated in earlier work .", "Rapp 1995 , 1999 proposes to collect counts over words occurring in a four word window around the target word .", "For each occurrence of a target word , counts are collected over how often certain context words occur in the two positions directly ahead of the target word and the two following positions .", "The counts are collected separately for each position and then entered into in a context vector with an dimension for each context word in each position .", "Finally , the raw counts are normalized , so that for each of the four word positions the vector values add up to one .", "Vector comparison is done by adding all absolute differences of all components .", "Fung and Yee 1998 propose a similar approach They count how often another word occurs in the same sentence as the target word .", "The counts are then normalized by a using the tf idf method which is often used in information retrieval Jones , 1979 .", "The need for translating the context poses a chicken and egg problem If we already have a translation lexicon we can translate the context vectors .", "But we can only construct a translation lexicon with this approach if we are already able to translate the context vectors .", "Theoretically , it is possible to use these methods to build a translation lexicon from scratch Rapp , 1995 .", "The number of possible mappings has complexity O n !", ", and the computing cost of each mapping has quadratic complexity O n2 .", "For a large number of words n at least more than 10 , 000 , maybe more than 100 , 000 the combined complexity becomes prohibitively expensive .", "Because of this , both Rapp and Fung focus on expanding an existing large lexicon to add a few novel terms .", "Clearly , a seed lexicon to bootstrap these methods is needed .", "Fortunately , we have outlined in Section 2 . 1 how such a seed lexicon can be obtained by finding words spelled identically in both languages .", "We can then construct context vectors that contain information about how a new unmapped word co occurs with the seed words .", "This vector can be translated into the other language , since we already know the translations of the seed words .", "Finally , we can look for the best matching context vector in the target language , and decide upon the corresponding word to construct a word mapping .", "Again , as in Section 2 . 2 , we have to compute all possible word or context vector matches .", "We collect then the best word matches in a greedy fashion .", "Table 3 displays the top 15 generated word pairs by this algorithm .", "The context vectors are constructed in the way proposed by Rapp 1999 , with the difference that we collect counts over a four noun window , not a four word window , by dropping all intermediate words .", "Intuitively it is obvious that pairs of words that are similar in one language should have translations that are similar in the other language .", "For instance , Wednesday is similar to Thursday as Mittwoch is similar to Donnerstag .", "Or dog is similar to cat in English , as Hund is similar to Katze in German .", "The challenge is now to come up with a quantifiable measurement of word similarity .", "One strategy is to define two words as similar if they occur in a similar context .", "Clearly , this is the case for Wednesday and Thursday , as well as for dog and cat .", "Exactly this similarity measurement is used in the work by Diab and Finch 2000 .", "Their approach to constructing and comparing context vectors differs significantly from methods discussed in the previous section .", "For each word in the lexicon , the context vector consists of co occurrence counts in respect to 150 so called peripheral tokens , basically the most frequent words .", "These counts are collected for each position in a 4 word window around the word in focus .", "This results in a 600 dimensional vector .", "Instead of comparing these co occurrence counts directly , the Spearman rank order correlation is applied For each position the tokens are compared in frequency and the frequency count is replaced by the frequency rank the most frequent token count is replaced by 1 , the least frequent by n 150 .", "The similarity of two context vectors a ai and b bi is then defined by 3 The result of all this is a matrix with similarity scores between all German words , and second one with similarity scores between all English words .", "Such matrices could also be constructed using the definitions of context we reviewed in the previous section .", "The important point here is that we have generated a similarity matrix , which we will use now to find new translation word pairs .", "Again , as in the previous Section 2 . 3 , we as3In the given formula we fixed two mistakes of the original presentation Diab and Finch , 2000 The square of the differences is used , and the denominator contains the additional factor 4 , since essentially 4 150 word vectors are compared . sume that we will already have a seed lexicon .", "For a new word we can look up its similarity scores to the seed words , thus creating a similarity vector .", "Such a vector can be translated into the other language recall that dimensions of the vector are the similarity scores to seed words , for which we already have translations .", "The translated vector can be compared to other vectors in the second language .", "As before , we search greedily for the best matching similarity vectors and add the corresponding words to the lexicon .", "Finally , another simple clue is the observation that in comparable corpora , the same concepts should be used with similar frequencies .", "Even if the most frequent word in the German corpus is not necessarily the translation of the most frequent English word , it should also be very frequent .", "Table 4 illustrates the situation with our corpora .", "It contains the top 10 German and English words , together with the frequency ranks of their best translations .", "For both languages , 4 of the 10 words have translations that also rank in the top 10 .", "Clearly , simply aligning the nth frequent German word with the nth frequent English word is not a viable strategy .", "In our case , this is additionally hampered by the different orientation of the news sources .", "The frequent financial terms in the English WSJ corpus stock , bank , sales , etc . are rather rare in the German corpus .", "For most words , especially for more comparable corpora , there is a considerable correlation between the frequency of a word and its translation .", "Our frequency measurement is defined as ratio of the word frequencies , normalized by the corpus sizes .", "This section provides more detail on the experiments we have carried out to test the methods just outlined . quent German and English words and their translations .", "We are trying to build a one to one GermanEnglish translation lexicon for the use in a machine translation system .", "To evaluate this performance we use two different measurements Firstly , we record how many correct word pairs we have constructed .", "This is done by checking the generated wordpairs against an existing bilingual lexicon . 4 In essence , we try to recreate this lexicon , which contains 9 , 206 distinct German and 10 , 645 distinct English nouns and 19 , 782 lexicon entries .", "For a machine translation system , it is often more important to get more frequently used words right than obscure ones .", "Thus , our second evaluation measurement tests the word translations proposed by the acquired lexicon against the actual word level translations in a 5 , 000 sentence aligned parallel corpus . 5 The starting point to extending the lexicon is the seed lexicon of identically spelled words , as described in Section 2 . 1 .", "It consists of 1339 entries , of which are 88 . 9 correct according to the existing bilingual lexicon .", "Due to computational constraints , 6 we focus on the additional mapping of only 1 , 000 German and English words .", "These 1 , 000 words are chosen from the 1 , 000 most frequent lexicon entries in the dictionary , without duplications of words .", "This frequency is defined by the sum of two word frequencies of the words in the entry , as found in the monolingual corpora .", "We did not collect statistics of the actual use of lexical entries in , say , a parallel corpus .", "In a different experimental set up we also simply tried to match the 1 , 000 most frequent German words with the 1 , 000 most frequent English words .", "The results do not differ significantly .", "Each of the four clues described in the Sections 2 . 2 to 2 . 5 provide a matching score between a German and an English word .", "The likelihood of these two words being actual translations of each other should correlate to these scores .", "There are many ways one could search for the best set of lexicon entries based on these scores .", "We could perform an exhaustive search construct all possible mappings and find the highest combined score of all entries .", "Since there are O n ! possible mappings , a brute force approach to this is practically impossible .", "We therefore employed a greedy search First we search for the highest score for any word pair .", "We add this word pair to the lexicon , and drop word pairs that include either the German and English word from further search .", "Again , we search for the highest score and add the corresponding word pair , drop these words from further search , and so on .", "This is done iteratively , until all words are used up .", "Tables 2 and 3 illustrate this process for the spelling and context similarity clues , when applied separately .", "The results are summarized in Table 5 .", "Recall that for each word that we are trying to map to the other language , a thousand possible target words exist , but only one is correct .", "The baseline for this task , choosing words at random , results on average in only 1 correct mapping in the entire lexicon .", "A perfect lexicon , of course , contains 1000 correct entries .", "The starting point for the corpus score is the 15 . 8 that are already achieved with the seed lexicon from Section 2 . 1 .", "In an experiment where we identified the best lexical entries using a very large parallel corpus , we could achieve 89 accuracy on this test corpus . many correct lexicon entries where added Entries , and how well the resulting translation lexicon performs compared to the actual wordlevel translations in a parallel corpus Corpus .", "For all experiments the starting point was the seed lexicon of 1339 identical spelled words described in Section 2 . 1 . which achieve 15 . 8 Corpus score .", "Taken alone , both the context and spelling clues learn over a hundred lexicon entries correctly .", "The similarity and frequency clues , however , seem to be too imprecise to pinpoint the search to the correct translations .", "A closer look of the spelling and context scores reveals that while the spelling clue allows to learn more correct lexicon entries 140 opposed to 107 , the context clue does better with the more frequently used lexicon entries , as found in the test corpus accuracy of 31 . 9 opposed to 25 . 4 .", "Combining different clues is quite simple We can simply add up the matching scores .", "The scores can be weighted .", "Initially we simply weighted all clues equally .", "We then changed the weights to see , if we can obtain better results .", "We found that there is generally a broad range of weights that result in similar performance .", "When using the spelling clue in combination with others , we found it useful to define a cutoff .", "If two words agree in 30 of their letters this is generally as bad as if they do not agree in any the agreements are purely coincidental .", "Therefore we counted all spelling scores below 0 . 3 as 0 . 3 .", "Combining the context and the spelling clues yields a significantly better result than using each clue by itself .", "A total of 185 correct lexical entries are learned with a corpus score of 38 . 6 .", "Adding in the other scores , however , does not seem to be beneficial only adding the frequency clue to the spelling clue provides some improvement .", "In all other cases , these scores are not helpful .", "Besides this linear combination of scores from the different clues , more sophisticated methods may be possible Koehn , 2002 .", "We have attempted to learn a one to one translation lexicon purely from unrelated monolingual corpora .", "Using identically spelled words proved to be a good starting point .", "Beyond this , we examined four different clues .", "Two of them , matching similar spelled words and words with the same context , helped us to learn a significant number of additional correct lexical entries .", "Our experiments have been restricted to nouns .", "Verbs , adjectives , adverbs and other part of speech may be tackled in a similar way .", "They might also provide useful context information that is beneficial to building a noun lexicon .", "These methods may be also useful given a different starting point For efforts in building machine translation systems , some small parallel text should be available .", "From these , some high quality lexical entries can be learned , but there will always be many words that are missing .", "These may be learned using the described methods ."], "summary_lines": ["Learning A Translation Lexicon From Monolingual Corpora\n", "This paper presents work on the task of constructing a word-level translation lexicon purely from unrelated mono-lingual corpora.\n", "We combine various clues such as cognates, similar context, preservation of word similarity, and word frequency.\n", "Experimental results for the construction of a German-English noun lexicon are reported.\n", "Noun translation accuracy of 39% scored against a parallel test corpus could be achieved.\n", "We automatically induce the initial seed bilingual dictionary by using identical spelling features such as cognates and similar contexts.\n"]}
{"article_lines": ["Coreference Resolution Using Competition Learning Approach", "In this paper we propose a competition learning approach to coreference resolution .", "Traditionally , supervised machine learning approaches adopt the singlecandidate model .", "Nevertheless the preference relationship between the antecedent candidates cannot be determined accurately in this model .", "By contrast , our approach adopts a twin candidate learning model .", "Such a model can present the competition criterion for antecedent candidates reliably , and ensure that the most preferred candidate is selected .", "Furthermore , our approach applies a candidate filter to reduce the computational cost and data noises during training and resolution .", "The experimental results on MUC 6 and MUC 7 data set show that our approach can outperform those based on the singlecandidate model .", "Coreference resolution is the process of linking together multiple expressions of a given entity .", "The key to solve this problem is to determine the antecedent for each referring expression in a document .", "In coreference resolution , it is common that two or more candidates compete to be the antecedent of an anaphor Mitkov , 1999 .", "Whether a candidate is coreferential to an anaphor is often determined by the competition among all the candidates .", "So far , various algorithms have been proposed to determine the preference relationship between two candidates .", "Mitkov s knowledge poor pronoun resolution method Mitkov , 1998 , for example , uses the scores from a set of antecedent indicators to rank the candidates .", "And centering algorithms Brennan et al . , 1987 ; Strube , 1998 ; Tetreault , 2001 , sort the antecedent candidates based on the ranking of the forward looking or backwardlooking centers .", "In recent years , supervised machine learning approaches have been widely used in coreference resolution Aone and Bennett , 1995 ; McCarthy , 1996 ; Soon et al . , 2001 ; Ng and Cardie , 2002a , and have achieved significant success .", "Normally , these approaches adopt a single candidate model in which the classifier judges whether an antecedent candidate is coreferential to an anaphor with a confidence value .", "The confidence values are generally used as the competition criterion for the antecedent candidates .", "For example , the Best First selection algorithms Aone and Bennett , 1995 ; Ng and Cardie , 2002a link the anaphor to the candidate with the maximal confidence value above 0 . 5 .", "One problem of the single candidate model , however , is that it only takes into account the relationships between an anaphor and one individual candidate at a time , and overlooks the preference relationship between candidates .", "Consequently , the confidence values cannot accurately represent the true competition criterion for the candidates .", "In this paper , we present a competition learning approach to coreference resolution .", "Motivated by the research work by Connolly et al . 1997 , our approach adopts a twin candidate model to directly learn the competition criterion for the antecedent candidates .", "In such a model , a classifier is trained based on the instances formed by an anaphor and a pair of its antecedent candidates .", "The classifier is then used to determine the preference between any two candidates of an anaphor encountered in a new document .", "The candidate that wins the most comparisons is selected as the antecedent .", "In order to reduce the computational cost and data noises , our approach also employs a candidate filter to eliminate the invalid or irrelevant candidates .", "The layout of this paper is as follows .", "Section 2 briefly describes the single candidate model and analyzes its limitation .", "Section 3 proposes in details the twin candidate model and Section 4 presents our coreference resolution approach based on this model .", "Section 5 reports and discusses the experimental results .", "Section 6 describes related research work .", "Finally , conclusion is given in Section 7 .", "The main idea of the single candidate model for coreference resolution is to recast the resolution as a binary classification problem .", "During training , a set of training instances is generated for each anaphor in an annotated text .", "An instance is formed by the anaphor and one of its antecedent candidates .", "It is labeled as positive or negative based on whether or not the candidate is tagged in the same coreferential chain of the anaphor .", "After training , a classifier is ready to resolve the NPs1 encountered in a new document .", "For each NP under consideration , every one of its antecedent candidates is paired with it to form a test instance .", "The classifier returns a number between 0 and 1 that indicates the likelihood that the candidate is coreferential to the NP .", "The returned confidence value is commonly used as the competition criterion to rank the candidate .", "Normally , the candidates with confidences less than a selection threshold e . g .", "0 . 5 are discarded .", "Then some algorithms are applied to choose one of the remaining candidates , if any , as the antecedent .", "For example , Closest First Soon et al . , 2001 selects the candidate closest to the anaphor , while Best First Aone and Bennett , 1995 ; Ng and Cardie , 2002a selects the candidate with the maximal confidence value .", "One limitation of this model , however , is that it only considers the relationships between a NP encountered and one of its candidates at a time during its training and testing procedures .", "The confidence value reflects the probability that the candidate is coreferential to the NP in the overall distribution2 , but not the conditional probability when the candidate is concurrent with other competitors .", "Consequently , the confidence values are unreliable to represent the true competition criterion for the candidates .", "To illustrate this problem , just suppose a data set where an instance could be described with four exclusive features F1 , F2 , F3 and F4 .", "The ranking of candidates obeys the following rule Here CSFi 1 _ i _ 4 is the set of antecedent candidates with the feature Fi on .", "The mark of denotes the preference relationship , that is , the candidates in CSF1 is preferred to those in CSF2 , and to those in CSF3 and CSF4 .", "Let CF2 and CF3 denote the class value of a leaf node F2 1 and F3 1 , respectively .", "It is possible that CF2 CF3 , if the anaphors whose candidates all belong to CSF3 or CSF4 take the majority in the training data set .", "In this case , a candidate in CSF3 would be assigned a larger confidence value than a candidate in CSF2 .", "This nevertheless contradicts the ranking rules .", "If during resolution , the candidates of an anaphor all come from CSF2 or CSF3 , the anaphor may be wrongly linked to a candidate in CSF3 rather than in CSF2 .", "Different from the single candidate model , the twin candidate model aims to learn the competition criterion for candidates .", "In this section , we will introduce the structure of the model in details .", "Consider an anaphor ana and its candidate set candidate_set , C1 , C2 , . . . , Ck , where Cj is closer to ana than Ci if j i .", "Suppose positive_set is the set of candidates that occur in the coreferential chain of ana , and negative_set is the set of candidates not in the chain , that is , negative_set candidate_set positive_set .", "The set of training instances based on ana , inst_set , is defined as follows From the above definition , an instance is formed by an anaphor , one positive candidate and one negative candidate .", "For each instance , inst ci , cj , ana , the candidate at the first position , Ci , is closer to the anaphor than the candidate at the second position , Cj .", "A training instance inst ci , cj , ana is labeled as positive if Ci positive set and Cj negative set ; or negative if Ci negative set and Cj positiveset .", "See the following example Any design to link China's accession to the WTO with the missile tests1 was doomed to failure .", "If some countries2 try to block China TO accession , that will not be popular and will fail to win the support of other countries3 she said .", "Although no governments4 have suggested formal sanctions5 on China over the missile tests6 , the United States has called them7 provocative and reckless and other countries said they could threaten Asian stability .", "In the above text segment , the antecedent candidate set of the pronoun them ? consists of six candidates highlighted in Italics .", "Among the candidates , Candidate 1 and 6 are in the coreferential chain of them ? , while Candidate 2 , 3 , 4 , 5 are not .", "Thus , eight instances are formed for them ? Here the instances in the first line are negative , while those in the second line are all positive .", "A feature vector is specified for each training or testing instance .", "Similar to those in the singlecandidate model , the features may describe the lexical , syntactic , semantic and positional relationships of an anaphor and any one of its candidates .", "Besides , the feature set may also contain intercandidate features characterizing the relationships between the pair of candidates , e . g . the distance between the candidates in the number distances or paragraphs .", "Based on the feature vectors generated for each anaphor encountered in the training data set , a classifier can be trained using a certain machine learning algorithm , such as C4 . 5 , RIPPER , etc .", "Given the feature vector of a test instance inst ci , cj , ana i j , the classifier returns the positive class indicating that Ci is preferred to Cj as the antecedent of ana ; or negative indicating that Cj is preferred .", "Let CR inst ci , cj , ana denote the classification result for an instance inst ci , cj , ana .", "The antecedent of an anaphor is identified using the algorithm shown in Figure 1 .", "Input ana the anaphor under consideration candidate_set the set of antecedent candidates of ana , C1 , C2 , . . . , Ck for i 1 to K do Score i 0 ; for i K downto 2 do for j i 1 downto 1 do if CR inst ci , cj , ana positive then While the realization and the structure of the twincandidate model are significantly different from the single candidate model , the single candidate model in fact can be regarded as a special case of the twin candidate model .", "To illustrate this , just consider a virtual blank candidate C0 such that we could convert an instance inst ci , ana in the single candidate model to an instance inst ci , c0 , ana in the twin candidate model .", "Let inst ci , c0 , ana have the same class label as inst ci , ana , that is , inst ci , c0 , ana is positive if Ci is the antecedent of ana ; or negative if not .", "Apparently , the classifier trained on the instance set inst ci , ana , T1 , is equivalent to that trained on inst ci , c0 , ana , T2 .", "T1 and T2 would assign the same class label for the test instances inst ci , ana and inst ci , c0 , ana , respectively .", "That is to say , determining whether Ci is coreferential to ana by T1 in the single candidate model equals to determining whether Ci is better than C0 w . r . t ana by T2 in the twin candidate model .", "Here we could take C0 as a standard candidate .", "While the classification in the single candidate model can find its interpretation in the twincandidate model , it is not true vice versa .", "Consequently , we can safely draw the conclusion that the twin candidate model is more powerful than the single candidate model in characterizing the relationships among an anaphor and its candidates .", "Our competition learning approach adopts the twin candidate model introduced in the Section 3 .", "The main process of the approach is as follows To determine the boundary of the noun phrases , a pipeline of Nature Language Processing components are applied to an input raw text Among them , named entity recognition , part ofspeech tagging and text chunking apply the same Hidden Markov Model HMM based engine with error driven learning capability Zhou and Su , 2000 2002 .", "The named entity recognition component recognizes various types of MUC style named entities , i . e . , organization , location , person , date , time , money and percentage .", "For our study , in this paper we only select those features that can be obtained with low annotation cost and high reliability .", "All features are listed in Table 1 together with their respective possible values .", "For a NP under consideration , all of its preceding NPs could be the antecedent candidates .", "Nevertheless , since in the twin candidate model the number of instances for a given anaphor is about the square of the number of its antecedent candidates , the computational cost would be prohibitively large if we include all the NPs in the candidate set .", "Moreover , many of the preceding NPs are irrelevant or even invalid with regard to the anaphor .", "These data noises may hamper the training of a goodperformanced classifier , and also damage the accuracy of the antecedent selection too many comparisons are made between incorrect candidates .", "Therefore , in order to reduce the computational cost and data noises , an effective candidate filtering strategy must be applied in our approach .", "During training , we create the candidate set for each anaphor with the following filtering algorithm Features describing the two candidates During resolution , we filter the candidates for each encountered pronoun in the same way as during training .", "That is , we only consider the NPs in the current and the preceding 2 sentences .", "Such a context window is reasonable as the distance between a pronominal anaphor and its antecedent is generally short .", "In the MUC 6 data set , for example , the immediate antecedents of 95 pronominal anaphors can be found within the above distance .", "Comparatively , candidate filtering for nonpronouns during resolution is complicated .", "A potential problem is that for each non pronoun under consideration , the twin candidate model always chooses a candidate as the antecedent , even though all of the candidates are low qualified , that is , unlikely to be coreferential to the non pronoun under consideration .", "In fact , the twin candidate model in itself can identify the qualification of a candidate .", "We can compare every candidate with a virtual standard candidate , C0 .", "Only those better than C0 are deemed qualified and allowed to enter the round robin , whereas the losers are eliminated .", "As we have discussed in Section 3 . 5 , the classifier on the pairs of a candidate and C0 is just a singlecandidate classifier .", "Thus , we can safely adopt the single candidate classifier as our candidate filter .", "The candidate filtering algorithm during resolution is as follows", "Our coreference resolution approach is evaluated on the standard MUC 6 1995 and MUC 7 1998 data set .", "For MUC 6 , 30 dry run documents annotated with coreference information could be used as training data .", "There are also 30 annotated training documents from MUC 7 .", "For testing , we utilize the 30 standard test documents from MUC 6 and the 20 standard test documents from MUC 7 .", "In the experiment we compared our approach with the following research works Among them , S List , a version of centering algorithm , uses well defined heuristic rules to rank the antecedent candidates ; Ng and Cardie s approach employs the standard single candidate model and Best First rule to select the antecedent ; Connolly et al . s approach also adopts the twin candidate model , but their approach lacks of candidate filtering strategy and uses greedy linear search to select the antecedent See Related work for details .", "We constructed three baseline systems based on the above three approaches , respectively .", "For comparison , in the baseline system 2 and 3 , we used the similar feature set as in our system see table 1 .", "Table 2 and 3 show the performance of different approaches in the pronoun and non pronoun resolution , respectively .", "In these tables we focus on the abilities of different approaches in resolving an anaphor to its antecedent correctly .", "The recall measures the number of correctly resolved anaphors over the total anaphors in the MUC test data set , and the precision measures the number of correct anaphors over the total resolved anaphors .", "The F measure F 2 RP R P is the harmonic mean of precision and recall .", "The experimental result demonstrates that our competition learning approach achieves a better performance than the baseline approaches in resolving pronominal anaphors .", "As shown in Table 2 , our approach outperforms Ng and Cardie s singlecandidate based approach by 3 . 7 and 5 . 4 in Fmeasure for MUC 6 and MUC 7 , respectively .", "Besides , compared with Strube s S list algorithm , our approach also achieves gains in the F measure by 3 . 2 MUC 6 , and 1 . 6 MUC 7 .", "In particular , our approach obtains significant improvement 21 . 1 for MUC 6 , and 13 . 1 for MUC 7 over Connolly et al . s twin candidate based approach .", "Compared with the gains in pronoun resolution , the improvement in non pronoun resolution is slight .", "As shown in Table 3 , our approach resolves non pronominal anaphors with the recall of 51 . 3 39 . 7 and the precision of 90 . 4 87 . 6 for MUC 6 MUC 7 .", "In contrast to Ng and Cardie s approach , the performance of our approach improves only 0 . 3 0 . 6 in recall and 0 . 5 1 . 2 in precision .", "The reason may be that in non pronoun resolution , the coreference of an anaphor and its candidate is usually determined only by some strongly indicative features such as alias , apposition , string matching , etc this explains why we obtain a high precision but a low recall in non pronoun resolution .", "Therefore , most of the positive candidates are coreferential to the anaphors even though they are not the best .", "As a result , we can only see comparatively slight difference between the performances of the two approaches .", "Although Connolly et al . s approach also adopts the twin candidate model , it achieves a poor performance for both pronoun resolution and nonpronoun resolution .", "The main reason is the absence of candidate filtering strategy in their approach this is why the recall equals to the precision in the tables .", "Without candidate filtering , the recall may rise as the correct antecedents would not be eliminated wrongly .", "Nevertheless , the precision drops largely due to the numerous invalid NPs in the candidate set .", "As a result , a significantly low Fmeasure is obtained in their approach .", "Table 4 summarizes the overall performance of different approaches to coreference resolution .", "Different from Table 2 and 3 , here we focus on whether a coreferential chain could be correctly identified .", "For this purpose , we obtain the recall , the precision and the F measure using the standard MUC scoring program Vilain et al . 1995 for the coreference resolution task .", "Here the recall means the correct resolved chains over the whole coreferential chains in the data set , and precision means the correct resolved chains over the whole resolved chains .", "In line with the previous experiments , we see reasonable improvement in the performance of the coreference resolution compared with the baseline approach based on the single candidate model , the F measure of approach increases from 69 . 4 to 71 . 3 for MUC 6 , and from 58 . 7 to 60 . 2 for MUC 7 .", "A similar twin candidate model was adopted in the anaphoric resolution system by Connolly et al . 1997 .", "The differences between our approach and theirs are 1 In Connolly et al . s approach , all the preceding NPs of an anaphor are taken as the antecedent candidates , whereas in our approach we use candidate filters to eliminate invalid or irrelevant candidates .", "2 The antecedent identification in Connolly et al . s approach is to apply the classifier to successive pairs of candidates , each time retaining the better candidate .", "However , due to the lack of strong assumption of transitivity , the selection procedure is in fact a greedy search .", "By contrast , our approach evaluates a candidate according to the times it wins over the other competitors .", "Comparatively this algorithm could lead to a better solution .", "3 Our approach makes use of more indicative features , such as Appositive , Name Alias , String matching , etc .", "These features are effective especially for non pronoun resolution .", "In this paper we have proposed a competition learning approach to coreference resolution .", "We started with the introduction of the singlecandidate model adopted by most supervised machine learning approaches .", "We argued that the confidence values returned by the single candidate classifier are not reliable to be used as ranking criterion for antecedent candidates .", "Alternatively , we presented a twin candidate model that learns the competition criterion for antecedent candidates directly .", "We introduced how to adopt the twincandidate model in our competition learning approach to resolve the coreference problem .", "Particularly , we proposed a candidate filtering algorithm that can effectively reduce the computational cost and data noises .", "The experimental results have proved the effectiveness of our approach .", "Compared with the baseline approach using the single candidate model , the F measure increases by 1 . 9 and 1 . 5 for MUC 6 and MUC 7 data set , respectively .", "The gains in the pronoun resolution contribute most to the overall improvement of coreference resolution .", "Currently , we employ the single candidate classifier to filter the candidate set during resolution .", "While the filter guarantees the qualification of the candidates , it removes too many positive candidates , and thus the recall suffers .", "In our future work , we intend to adopt a looser filter together with an anaphoricity determination module Bean and Riloff , 1999 ; Ng and Cardie , 2002b .", "Only if an encountered NP is determined as an anaphor , we will select an antecedent from the candidate set generated by the looser filter .", "Furthermore , we would like to incorporate more syntactic features into our feature set , such as grammatical role or syntactic parallelism .", "These features may be helpful to improve the performance of pronoun resolution ."], "summary_lines": ["Coreference Resolution Using Competition Learning Approach\n", "In this paper we propose a competition learning approach to coreference resolution.\n", "Traditionally, supervised machine learning approaches adopt the single-candidate model.\n", "Nevertheless the preference relationship between the antecedent candidates cannot be determined accurately in this model.\n", "By contrast, our approach adopts a twin-candidate learning model.\n", "Such a model can present the competition criterion for antecedent candidates reliably, and ensure that the most preferred candidate is selected.\n", "Furthermore, our approach applies a candidate filter to reduce the computational cost and data noises during training and resolution.\n", "The experimental results on MUC-6 and MUC-7 data set show that our approach can outperform those based on the single-candidate model.\n", "We make use of non-anaphors to create a special class of training instances in the twin-candidate model (Yang et al 2003) and improve the performance by 2.9 and 1.6 to 67.3 and 67.2 in F1-measure on the MUC-6 and MUC-7corpora, respectively.\n"]}
{"article_lines": ["Experiments Using Stochastic Search For Text Planning", "Marcu has characterised an important and difficult problem in text planning given a set of facts to convey and a set of rhetorical relations that can be used to link them together , how can one arrange this material so as to yield the best possible text ?", "We describe experiments with a number of heuristic search methods for this task .", "This paper presents some initial experiments using stochastic search methods for aspects of text planning .", "The work was motivated by the needs of the ILEX system for generating descriptions of museum artefacts in particular , 20th Century jewellery Mellish et al 98 .", "We present results on examples semi automatically generated from datastructures that exist within ILEX .", "Forming a set of facts about a piece of jewellery into a structure that yields a coherent text is a non trivial problem .", "Rhetorical Structure Theory Mann and Thompson 87 claims that a text is coherent just in case it can be analysed hierarchically in terms of relations between text spans .", "Much work in NLG makes the assumption that constructing something like an RS tree is a necessary step in the planning of a text .", "This work takes as its starting point Marcu's Marcu 97 excellent formalisation of RST and the problem of building legal RST trees , and for the purposes of this paper the phrase quot ; text planning quot ; will generally denote the task characterised by him .", "In this task , one is given a set of facts all of which should be included in a text and a set of relations between facts , some of which can be included in the text .", "The task is to produce a legal RS tree using the facts and some relations oi . the quot ; best quot ; such tree .", "Following the original work on RST and assumptions that have been commonly made in subsequent work , we will assume that there is a fixed set of possible relations we include quot ; joint quot ; as a second class relation which can be applied to any two facts , but whose use is not preferred .", "Each relation has a nucleus and a satellite we don't consider multiple nuclei or satellites here , apart from the case of quot ; joint quot ; , which is essentially multinuclear .", "Each relation may be indicated by a distinctive quot ; cue phrase quot ; , with the nucleus and satellite being realised in some fashion around it .", "Each relation has applicability conditions which can be tested between two atomic facts .", "For two complex text spans , a relation holds exactly when that relation holds between the nuclei of those spans .", "Relations can thus hold between text spans of arbitrary size .", "Figure 1 shows an example of the form of the input that is used for the experiments reported here .", "Each primitive quot ; fact quot ; is represented in terms of a subject , verb and complement as well as a unique identifier .", "The quot ; subject quot ; is assumed to be the entity that the fact is quot ; about quot ; .", "The approaches reported here have not yet been linked to a realisation component , and so the entities fact 'this item' , 'is' , 'a figurative jewel' , f6 . rel contrast , f7 , f3 , 0 . fact bleufort , 'was' , 'a french designer' , f3 . rel elab , F1 , F2 , 0 fact shiltredge , 'was' , 'a british designer' , f7 . mentions F1 , 0 , fact 'this item' , 'was made by' , bleufort , f8 . mentions F2 , 0 , fact titanium , 'is' , 'a refractory metal' , f4 .", "F1 F2 . are represented simply by canned phrases for readability it is assumed that each entity in the domain has a fixed distinctive phrase that is always used for it .", "Relations are represented in terms of the relation name , the nucleus and satellite facts and a list in this example , empty of precondition facts which need to have been assimilated before the relation can be used this represents an extension to Marcu's chcracterisation .", "This example uses the definition of objectattribute quot ; elaboration quot ; that we will be using consistently , namely that one fact can elaborate another if they have an entity in common of course , there are other kinds of elaborations , but we would want to model them differently .", "There seem to be three main approaches to controlling the search for a good RS tree or something similar .", "One is to restrict what relations can appear in the nucleus and satellite of others for instance , using Hovy's Hovy 90 idea of quot ; growth points quot ; .", "This is a step towards creating quot ; schemas quot ; for larger pieces of text .", "It can therefore be expected that it will produce very good results in restricted domains where limited text patterns are used , but that it will be hard to extend it to freer text types .", "The second idea is to use information about goals to limit possibilities .", "This is an element of Hovy's work but is more apparent in the planning work of Moore and Paris Moore and Paris 93 .", "This second approach will work well if there are strong goals in the domain which really can influence textual decisions .", "This is not always the case .", "For instance , in our ILEX domain Mellish et al 98 the system's goal is something very general like quot ; say interesting things about item X , ' subject to length and coherence constraints quot ; .", "The third approach , most obviously exemplified by Marcu 97 , is to use some form of explicit search through possible trees , guided by heuristics about tree quality .", "Marcu first of all attempts to find the best ordering of the facts .", "For every relation that could be indicated , constraints are generated saying what the order of the two facts involved should be and that the facts should be adjacent .", "The constraints are weighted according to attributes of rhetorical relations that have been determined empirically .", "A standard constraint satisfaction algorithm is used to find the linear sequence such that the total weight of the satisfied constraints is maximal .", "Once the sequence of facts is known , a general algorithm Marcu 96 is used to construct all possible RS trees based on those facts .", "It is not clear how the best such tree is selected , though clearly the adjacency and order constraints could in principle be reapplied in some way possibly with other heuristics that Marcu has used in rhetorical parsing to select a tree .", "We are interested in further developing the ideas of Marcu , but seek to address the following problems and having weighted constraints seems to make matters worse .", "Enumerating all RS trees that can be built on a given sequence of facts also has combinatorical problems .", "Marcu's approach may not be much better than one that builds all possible trees .", "Yet if there are enough relations to link any pair of facts which , given the existence of elaboration , may often be nearly the case , the number of trees whose top nucleus are a specified fact grows from 336 to 5040 to 95040 as the number of facts grows from 5 to 6 to 7 .", "In our examples , we have more like 20 30 facts .", "As Marcu points out , the constraints on linear order only indirectly reflect requirements on the tree because related facts need not appear consecutively .", "Though in fact we will use the idea of planning via a linear sequence later , we would like to experiment using measures of quality that are applied directly to the trees .", "We also have a number of factors that we would like to take account of in the evaluation see section 3 below .", "Building a good RS tree is a search problem .", "Stochastic search methods are a form of heuristic search that use the following generic algorithm Use these to generate one or more new random variations .", "Add these to the set , possibly removing less preferred items in order to keep the size constant .", "Examples of stochastic search approaches are stochastic hillclimbing , simulated annealing and evolutionary algorithms .", "The approaches differ according to factors like the size of the population of possible solutions that is maintained , the operations for generating new possibilities and any special mechanisms for avoiding local maxima .", "They are similar to one another and different from constraint satisfaction and enumeration approaches in that they are heuristic not guaranteed to find optimal solutions and they are quot ; anytime quot ; .", "That is , such an algorithm can be stopped at any point and it will be able to yield at that point a result which is the best it has found so far .", "This is important for NLG applications where interface considerations mean that texts have to be produced within a limited time .", "A key requirement for the use of any stochastic search approach is the ability to assess the quality of a possible solution .", "Thus we are forced to confront directly the task of evaluating RST trees .", "We assign a candidate tree a score which is the sum of scores for particular features the tree may have .", "A positive score here indicates a good feature and a negative one indicates a bad one .", "We cannot make any claims to have the best way of evaluating RS trees .", "The problem is far too complex and our knowledge of the issues involved so meagre that only a token gesture can be made at this point .", "We offer the following evaluation scheme merely so that the basis of our experiments is clear and because we believe that some of the ideas are starting in the right direction .", "Here are the features that we score for Topic and Interestingness We assume that the entity that the text is quot ; about quot ; is specified with the input .", "It is highly desirable that the quot ; top nucleus quot ; most important nucleus of the text be about this entity .", "Also we prefer texts that use interesting relations .", "We score as follows 4 for each fact that will come textually between a satellite and its nucleus Constraints on Information Ordering Our relations have preconditions which are facts that should be conveyed before them .", "We score as follows 20 for an unsatisfied precondition for a relation Focus Movement We do not have a complex model of focus development through the text , though development of such a model would be worthwhile .", "As McKeown and others have done , we prefer certain transitions over others .", "If consecutive facts mention the same entities or verb , the prospects for aggregation are greater , and this is usually desirable .", "We score as follows 9 for a fact apart from the first not mentioning any previously mentioned entity 3 for a fact not mentioning any entity in the previous fact , but whose subject is a previously mentioned entity 3 for a fact retaining the subject of the last fact as its subject 3 for a fact using the same verb as the previous one Object Introduction When an entity is first introduced as the subject of a fact , it is usual for that to be a very general statement about the entity .", "Preferring this introduces a mild schema like influence to the system .", "We score as follows 3 for the first fact with a given entity as subject having verb quot ; is quot ;", "Using the above evaluation metric for RS trees , we have experimented with a range of stochastic search methods .", "Space does not permit us to discuss more than one initial experiment in this section .", "In the next section , we describe a couple of methods based on genetic algorithms which proved more productive .", "The subtree swapping approach produces new trees by swapping random subtrees in a candidate solution .", "It works as follows When two subtrees are swapped over in an RS tree , some of the relations indicated in the tree no longer apply i . e . those higher relations that make use of the nuclei of the subtrees .", "These are quot ; repaired quot ; by in each case selecting the quot ; best quot ; valid relation that really relates the top nuclei i . e . a non elaboration relation is chosen if possible , otherwise an elaboration if that is valid , with quot ; joint quot ; as a last resort .", "We investigated variations on this algorithm , including having initial random balanced trees including the quot ; best quot ; relation at each point and focussing the subtree swapping on subtrees that contributed to bad scores , but the above algorithm was the one that seemed most successful .", "Figure 2 shows an example text generated by subtree swapping .", "Note that we have taken liberties in editing by hand the surface text for instance , by introducing better referring expressions and aggregation .", "For clarity , coreference has been indicated by subscripts .", "The ordering of the material and the use of rhetorical relations 'are the only things which are determined by the algorithm .", "Results for subtree swapping are shown together with later results in Figure 5 the example text shown for subtree swapping is for the item named j 342540 .", "The most obvious feature of these results is the huge variability of the results , which suggests that there are many local maxima in the search space .", "Looking at the texts produced , we can see a number of problems .", "If there is only one way smoothly to include a fact in the text , the chance of finding it by random subtree swapping is very low .", "The same goes for fixing other local problems in the text .", "The introduction of quot ; the previous jewel quot ; is an example of this .", "This entity can only be introduced elegantly through the fact that it , like the current item , is encrusted with jewels .", "The text is also still suffering from material getting between a satellite and its nucleus .", "For instance , there is a relation indicated by the colon between quot ; It is encrusted with jewels quot ; and quot ; it has silver links encrusted asymmetrically . . . quot ; , but this is weakened by the presence of quot ; and is an Organic style jewel quot ; in the middle .", "The trouble is that subtree swapping needs incrementally to acquire all good features not present in whichever initial tree develops into the best solution .", "It can only acquire these features quot ; accidentally quot ; and the chances of stumbling on them are small .", "Different initial trees will contain different good fragments , and it seems desirable to be able to combine the good parts of different solutions .", "This motivates using some sort of crossover operation that can combine elements of two solutions into a new one Goldberg 89 .", "But it is not immediately clear how crossover could work on two RS trees .", "In particular , two chosen trees will rarely have non trivial subtrees with equal fringes .", "Their way of breaking up the material may be so different that it is hard to imagine how one could combine elements of both .", "This jewel i is 72 . 0 cm long .", "The previous jewel .", "; has little diamonds scattered around its edges and has an encrusted bezel .", "Iti is encrusted with jewels iti features diamonds encrusted on a natural shell .", "As a way of making a crossover operation conceivable , our first step has been to reduce the planning problem . to that of planning the sequential order of the facts in a way that echoes Marcu's approach to some extent .", "We have done this by making certain restrictions on the RS trees that we are prepared to build .", "In particular , we make the following assumptions With these assumptions , an RS tree is characterised almost by the sequence of facts at its leaves .", "Indeed , we have an algorithm that almost deterministically builds a tree from a sequence of facts , according to these principles .", "The algorithm is not completely deterministic , because there may be more than one non elaboration relation that can be used with two given facts as nucleus and satellite our evaluation function won't , of course , differentiate between these .", "The algorithm for building a tree from a sequence essentially makes a tree that can be processed by a reader with minimal short term memory .", "The tree will be right branching and if the reader just remembers the last fact at any point , then they can follow the connection between the text so far and the next fact2 Interestingly , Marcu uses quot ; right skew quot ; to b disambiguate between alternative trees produced in rhetorical parsing .", "Here we are setting it as a much harder constraint .", "The only 21n fact , there is local left branching for non nested relations whose satellite is presented first .", "Such relations are often presented using embedded clauses in a way that signals the deviation from right branching clearly to the reader . exception is quot ; joint quot ; relations , which can join together texts of any size , but since there is no real The first two assumptions above make fundamental use of the order in which facts will appear in the text .", "For simplicity , we assume that every relation has a fixed order of nucleus and satellite though this assumption could be relaxed .", "The approach is controversial in that it takes into account realisation order in the criterion for a legal tree .", "It is likely that the above assumptions will not apply equally well to all types of text .", "Still , they mean that the planning problem can be reduced to that of planning a sequence .", "The next experiments were an attempt to evaluate this idea .", "The genetic algorithm we used takes the following form Notice that although the algorithm manipulates sequences , the evaluation is one that operates on trees .", "Mutation is a unary operation which , given one sequence , generates a new one .", "Crossover is binary in that it generates new solution s based on two existing ones .", "The choice of mutation and crossover operations depends on how the sequences are internally represented and should facilitate the exchange of useful subparts of solutions .", "Two different representations have been tried so far .", "The relevant features are summarised in Figure 3 .", "The ordinal representation Michalewicz 92 assumes that there is an initial canonical sequence of facts in the figure , this is assumed to be 1 , 2 , 3 , 4 .", "A given sequence is represented by a sequence of numbers , where the ith element indicates the position of the ith element of the sequence in that canonical sequence with all previous elements deleted .", "So the ith element is always a number between 1 and n 1 i , where n is the length of the sequence .", "Mutation is implemented by a change of a random element to a random legal value .", "Crossover here is implemented by two point crossover the material between two random points of the sequences the same points for both is swapped over , yielding two new sequences .", "The ordinal representation has been used extensively for tasks such as the travelling salesman problem , and it has the advantage that the crossover operation is particularly simple .", "In many ways , this is a more obvious encoding , though the operations are chosen to reflect the intuition that order and adjacency information should generally be maintained from old solution s Figure 4 shows an example text produced using the path encoding operations for j 342540 , after 2000 iterations , just under 2 minutes , score 180 .", "The same remarks about hand editing apply as before .", "Figure 5 summarises the results for subtree swapping and the two genetic algorithms on a set of examples .", "These results summarise the mean and standard deviations of the scores of the system run 10 times .", "The system was tried with a limit of 2000 and 4000 iterations around the main loop of the algorithm .", "These took about 2 and 4 minutes respectively .", "With each example problem we have specified the number of facts , the number of elaboration relations and the number of non elaboration relations .", "Note that there is not a very clear basis for comparison between This jeweli is made from diamonds and yellow metals .", "Iti was made by Flockinger , who was an English designer .", "Flockinger lived in London , which is a city .", "This jeweli was made in London .", "Iti is a necklace .", "Iti is made from oxidized white metal , pearls and opals . hi is set with jewels .", "This jeweli is encrusted with jewels it has silver links encrusted asymetrically with pearls and diamonds .", "This jeweli was made in 1976 . hi is an Organic style jewel and is 72 . 0 cm long .", "Iti draws on natural themes for inspiration iti uses natural pearls .", "Indeed , Organic style jewels usually draw on natural themes for inspiration .", "Organic style jewels usually have a coarse texture , are usually made up of asymmetrical shapes and are usually encrusted with jewels .", "The previous jeweli is encrusted with jewels it features diamonds encrusted on a natural shell .", "It , has little diamonds scattered around its edges and an encrusted bezel . algorithms , since each algorithm performs different operations during an quot ; iteration quot ; .", "Nevertheless , since iterations take roughly the same amount of time one can get a rough idea of the relative performance .", "The example text is now in a single paragraph , with a clear link from each sentence to the previous ones .", "From the numerical results , one can see that there is much less variability than before .", "This is mainly because the rigid tree building constraints prevent really bad trees being built and so the worst results are less bad .", "The results are also significantly better than for subtree swapping , with the edge sensitive representation clearly winning .", "It is necessary to be careful in evaluating these results , which are only as good as the evaluation function .", "This is certainly flawed in major ways .", "The texts are of a specific type , there are only three of them and we have not used all rhetorical relations .", "Some independent evaluation by human readers is imperative at this point .", "The texts are especially limited by the fact that there is no account taken of the possibilities for aggregation , embedding etc . in the trees that are produced .", "Nevertheless the approach looks promising enough that it is a real candidate to be used with the ILEX system .", "Future work needs to look at improving the characterisation of good trees and if possible introducing more natural crossover mutation operations .", "Future work could also consider extending the scope of the algorithm to deal with aspects of content determination as well as structuring .", "The ILEX project is supported by EPSRC grant GR K53321 .", "We acknowledge the valuable assistence of the National Museums of Scotland and the useful advice of Andrew Tuson ."], "summary_lines": ["Experiments Using Stochastic Search For Text Planning\n", "Marcu has characterised an important and difficult problem in text planning: given a set of facts to convey and a set of rhetorical relations that can be used to link them together, how can one arrange this material so as to yield the best possible text?\n", "We describe experiments with a number of heuristic Search methods for this task.\n", "We investigate the problem of determining a discourse tree for a set of elementary speech acts which are partially constrained by rhetorical relations.\n", "We advocate genetic algorithms as an alternative to exhaustively searching for the optimal ordering of descriptions of museum artefacts.\n"]}
{"article_lines": ["ROUGE A Package For Automatic Evaluation Of Summaries", "for Recall Oriented Understudy for Gisting Evaluation .", "It includes measures to automatically determine the quality of a summary by comparing it to other ideal summaries created by humans .", "The measures count the number of overlapping units such as n gram , word sequences , and word pairs between the computer generated summary to be evaluated and the ideal summaries created by humans .", "This paper introduces four different included in the summarization evaluation package and their evaluations .", "Three of them have been used in the Document Understanding Conference DUC 2004 , a large scale summarization evaluation sponsored by NIST .", "Traditionally evaluation of summarization involves human judgments of different quality metrics , for example , coherence , conciseness , grammaticality , readability , and content Mani , 2001 .", "However , even simple manual evaluation of summaries on a large scale over a few linguistic quality questions and content coverage as in the Document Understanding Conference DUC Over and Yen , 2003 would require over 3 , 000 hours of human efforts .", "This is very expensive and difficult to conduct in a frequent basis .", "Therefore , how to evaluate summaries automatically has drawn a lot of attention in the summarization research community in recent years .", "For example , Saggion et al . 2002 proposed three content based evaluation methods that measure similarity between summaries .", "These methods are cosine similarity , unit overlap i . e . unigram or bigram , and longest common subsequence .", "However , they did not show how the results of these automatic evaluation methods correlate to human judgments .", "Following the successful application of automatic evaluation methods , such as BLEU Papineni et al . , 2001 , in machine translation evaluation , Lin and Hovy 2003 showed that methods similar to BLEU , i . e . n gram co occurrence statistics , could be applied to evaluate summaries .", "In this paper , we introduce a package , ROUGE , for automatic evaluation of summaries and its evaluations .", "ROUGE stands for Recall Oriented Understudy for Gisting Evaluation .", "It includes several automatic evaluation methods that measure the similarity between summaries .", "We describe ROUGE N in Section 2 , ROUGE L in Section 3 , ROUGE W in Section 4 , and ROUGE S in Section 5 .", "Section 6 shows how these measures correlate with human judgments using DUC 2001 , 2002 , and 2003 data .", "Section 7 concludes this paper and discusses future directions .", "Formally , ROUGE N is an n gram recall between a candidate summary and a set of reference summaries .", "ROUGE N is computed as follows Where n stands for the length of the n gram , gramn , and Countmatch gramn is the maximum number of n grams co occurring in a candidate summary and a set of reference summaries .", "It is clear that ROUGE N is a recall related measure because the denominator of the equation is the total sum of the number of n grams occurring at the reference summary side .", "A closely related measure , BLEU , used in automatic evaluation of machine translation , is a precision based measure .", "BLEU measures how well a candidate translation matches a set of reference translations by counting the percentage of n grams in the candidate translation overlapping with the references .", "Please see Papineni et al . 2001 for details about BLEU .", "Note that the number of n grams in the denominator of the ROUGE N formula increases as we add more references .", "This is intuitive and reasonable because there might exist multiple good summaries .", "Every time we add a reference into the pool , we expand the space of alternative summaries .", "By controlling what types of references we add to the reference pool , we can design evaluations that focus on different aspects of summarization .", "Also note that the numerator sums over all reference summaries .", "This effectively gives more weight to matching n grams occurring in multiple references .", "Therefore a candidate summary that contains words shared by more references is favored by the ROUGE N measure .", "This is again very intuitive and reasonable because we normally prefer a candidate summary that is more similar to consensus among reference summaries .", "So far , we only demonstrated how to compute ROUGE N using a single reference .", "When multiple references are used , we compute pairwise summarylevel ROUGE N between a candidate summary s and every reference , ri , in the reference set .", "We then take the maximum of pairwise summary level ROUGE N scores as the final multiple reference ROUGE N score .", "This can be written as follows This procedure is also applied to computation of ROUGE L Section 3 , ROUGE W Section 4 , and ROUGE S Section 5 .", "In the implementation , we use a Jackknifing procedure .", "Given M references , we compute the best score over M sets of M 1 references .", "The final ROUGE N score is the average of the M ROUGE N scores using different M 1 references .", "The Jackknifing procedure is adopted since we often need to compare system and human performance and the reference summaries are usually the only human summaries available .", "Using this procedure , we are able to estimate average human performance by averaging M ROUGE N scores of one reference vs . the rest M 1 references .", "Although the Jackknifing procedure is not necessary when we just want to compute ROUGE scores using multiple references , it is applied in all ROUGE score computations in the ROUGE evaluation package .", "In the next section , we describe a ROUGE measure based on longest common subsequences between two summaries .", "A sequence Z z1 , z2 , . . . , zn is a subsequence of another sequence X x1 , x2 , . . . , xm , if there exists a strict increasing sequence i1 , i2 , . . . , ik of indices of X such that for all j 1 , 2 , . . . , k , we have xij zj Cormen et al . , 1989 .", "Given two sequences X and Y , the longest common subsequence LCS of X and Y is a common subsequence with maximum length .", "LCS has been used in identifying cognate candidates during construction of N best translation lexicon from parallel text .", "Melamed 1995 used the ratio LCSR between the length of the LCS of two words and the length of the longer word of the two words to measure the cognateness between them .", "He used LCS as an approximate string matching algorithm .", "Saggion et al . 2002 used normalized pairwise LCS to compare similarity between two texts in automatic summarization evaluation .", "To apply LCS in summarization evaluation , we view a summary sentence as a sequence of words .", "The intuition is that the longer the LCS of two summary sentences is , the more similar the two summaries are .", "We propose using LCS based Fmeasure to estimate the similarity between two summaries X of length m and Y of length n , assuming X is a reference summary sentence and Y is a candidate summary sentence , as follows Where LCS X , Y is the length of a longest common subsequence of X and Y , and \u00df Plcs Rlcs when ? Flcs ? Rlcs ? Flcs ? Plcs .", "In DUC , \u00df is set to a very big number ?", "Therefore , only Rlcs is considered .", "We call the LCS based F measure , i . e .", "Equation 4 , ROUGE L . Notice that ROUGE L is 1 when X Y ; while ROUGE L is zero when LCS X , Y 0 , i . e . there is nothing in common between X and Y . Fmeasure or its equivalents has been shown to have met several theoretical criteria in measuring accuracy involving more than one factor Van Rijsbergen , 1979 .", "The composite factors are LCS based recall and precision in this case .", "Melamed et al . 2003 used unigram F measure to estimate machine translation quality and showed that unigram Fmeasure was as good as BLEU .", "One advantage of using LCS is that it does not require consecutive matches but in sequence matches that reflect sentence level word order as n grams .", "The other advantage is that it automatically includes longest in sequence common n grams , therefore no predefined n gram length is necessary .", "ROUGE L as defined in Equation 4 has the property that its value is less than or equal to the minimum of unigram F measure of X and Y . Unigram recall reflects the proportion of words in X reference summary sentence that are also present in Y candidate summary sentence ; while unigram precision is the proportion of words in Y that are also in X . Unigram recall and precision count all cooccurring words regardless their orders ; while ROUGE L counts only in sequence co occurrences .", "By only awarding credit to in sequence unigram matches , ROUGE L also captures sentence level structure in a natural way .", "Consider the following example S1 . police killed the gunman S2 . police kill the gunman S3 . the gunman kill police We only consider ROUGE 2 , i . e .", "N 2 , for the purpose of explanation .", "Using S1 as the reference and S2 and S3 as the candidate summary sentences , S2 and S3 would have the same ROUGE 2 score , since they both have one bigram , i . e .", "the gunman .", "However , S2 and S3 have very different meanings .", "In the case of ROUGE L , S2 has a score of 3 4 0 . 75 and S3 has a score of 2 4 0 . 5 , with \u00df 1 .", "Therefore S2 is better than S3 according to ROUGE L .", "This example also illustrated that ROUGE L can work reliably at sentence level .", "However , LCS suffers one disadvantage that it only counts the main in sequence words ; therefore , other alternative LCSes and shorter sequences are not reflected in the final score .", "For example , given the following candidate sentence S4 . the gunman police killed Using S1 as its reference , LCS counts either the gunman or police killed , but not both ; therefore , S4 has the same ROUGE L score as S3 .", "ROUGE 2 would prefer S4 than S3 .", "Previous section described how to compute sentence level LCS based F measure score .", "When applying to summary level , we take the union LCS matches between a reference summary sentence , ri , and every candidate summary sentence , cj .", "Given a reference summary of u sentences containing a total of m words and a candidate summary of v sentences containing a total of n words , the summary level LCS based F measure can be computed as follows Again \u00df is set to a very big number ?", "8 in DUC , i . e . only Rlcs is considered .", "LCS ri , C is the LCS score of the union longest common subsequence between reference sentence ri and candidate summary C . For example , if ri w1 w2 w3 w4 w5 , and C contains two sentences c1 w1 w2 w6 w7 w8 and c2 w1 w3 w8 w9 w5 , then the longest common subsequence of ri and c1 is w1 w2 and the longest common subsequence of ri and c2 is w1 w3 w5 .", "The union longest common subsequence of ri , c1 , and c2 is w1 w2 w3 w5 and LCS ri , C 4 5 .", "The normalized pairwise LCS proposed by Radev et al . page 51 , 2002 between two summaries S1 and S2 , LCS S1 , S2 MEAD , is written as follows Assuming S1 has m words and S2 has n words , Equation 8 can be rewritten as Equation 9 due to symmetry We then define MEAD LCS recall Rlcs MEAD and MEAD LCS precision Plcs MEAD as follows We can rewrite Equation 9 in terms of Rlcs MEAD and Plcs MEAD with a constant parameter \u00df 1 as follows Equation 12 shows that normalized pairwise LCS as defined in Radev et al .", "2002 and implemented in MEAD is also a F measure with \u00df 1 .", "Sentencelevel normalized pairwise LCS is the same as ROUGE L with \u00df 1 .", "Besides setting \u00df 1 , summary level normalized pairwise LCS is different from ROUGE L in how a sentence gets its LCS score from its references .", "Normalized pairwise LCS takes the best LCS score while ROUGE L takes the union LCS score .", "LCS has many nice properties as we have described in the previous sections .", "Unfortunately , the basic LCS also has a problem that it does not differentiate LCSes of different spatial relations within their embedding sequences .", "For example , given a reference sequence X and two candidate sequences Y1 and Y2 as follows Y1 and Y2 have the same ROUGE L score .", "However , in this case , Y1 should be the better choice than Y2 because Y1 has consecutive matches .", "To improve the basic LCS method , we can simply remember the length of consecutive matches encountered so far to a regular two dimensional dynamic program table computing LCS .", "We call this weighted LCS WLCS and use k to indicate the length of the current consecutive matches ending at words xi and yj .", "Given two sentences X and Y , the WLCS score of X and Y can be computed using the following dynamic programming procedure For j 1 ; j n ; j If xi yj Then the length of consecutive matches at position i 1 and j 1 Where c is the dynamic programming table , c i , j stores the WLCS score ending at word xi of X and yj of Y , w is the table storing the length of consecutive matches ended at c table position i and j , and f is a function of consecutive matches at the table position , c i , j .", "Notice that by providing different weighting function f , we can parameterize the WLCS algorithm to assign different credit to consecutive in sequence matches .", "The weighting function f must have the property that f x y f x f y for any positive integers x and y .", "In other words , consecutive matches are awarded more scores than non consecutive matches .", "For example , f k ak b when k 0 , and a , b 0 .", "This function charges a gap penalty of b for each non consecutive n gram sequences .", "Another possible function family is the polynomial family of the form ka where a 1 .", "However , in order to normalize the final ROUGE W score , we also prefer to have a function that has a close form inverse function .", "For example , f k k2 has a close form inverse function f 1 k k1 2 .", "F measure based on WLCS can be computed as follows , given two sequences X of length m and Y of length n Where f 1 is the inverse function of f . In DUC , \u00df is set to a very big number ?", "Therefore , only Rwlcs is considered .", "We call the WLCS based Fmeasure , i . e .", "Equation 15 , ROUGE W .", "Using Equation 15 and f k k2 as the weighting function , the ROUGE W scores for sequences Y1 and Y2 are 0 . 571 and 0 . 286 respectively .", "Therefore , Y1 would be ranked higher than Y2 using WLCS .", "We use the polynomial function of the form ka in the ROUGE evaluation package .", "In the next section , we introduce the skip bigram co occurrence statistics .", "Skip bigram is any pair of words in their sentence order , allowing for arbitrary gaps .", "Skip bigram cooccurrence statistics measure the overlap of skipbigrams between a candidate translation and a set of reference translations .", "Using the example given in Section 3 . 1 each sentence has C 4 , 2 1 6 skip bigrams .", "For example , S1 has the following skip bigrams police killed , police the , police gunman , killed the , killed gunman , the gunman S2 has three skip bigram matches with S1 police the , police gunman , the gunman , S3 has one skip bigram match with S1 the gunman , and S4 has two skip bigram matches with S1 police killed , the gunman .", "Given translations X of length m and Y of length n , assuming X is a reference translation and Y is a candidate translation , we compute skip bigram based F measure as follows Where SKIP2 X , Y is the number of skip bigram matches between X and Y , \u00df controlling the relative importance of Pskip2 and Rskip2 , and C is the combination function .", "We call the skip bigram based Fmeasure , i . e .", "Equation 18 , ROUGE S .", "Using Equation 18 with \u00df 1 and S1 as the reference , S2 s ROUGE S score is 0 . 5 , S3 is 0 . 167 , and S4 is 0 . 333 .", "Therefore , S2 is better than S3 and S4 , and S4 is better than S3 .", "This result is more intuitive than using BLEU 2 and ROUGE L . One advantage of skip bigram vs . BLEU is that it does not require consecutive matches but is still sensitive to word order .", "Comparing skip bigram with LCS , skip bigram counts all in order matching word pairs while LCS only counts one longest common subsequence .", "Applying skip bigram without any constraint on the distance between the words , spurious matches such as the the or of in might be counted as valid matches .", "To reduce these spurious matches , we can limit the maximum skip distance , dskip , between two in order words that is allowed to form a skip bigram .", "For example , if we set dskip to 0 then ROUGE S is equivalent to bigram overlap Fmeasure .", "If we set dskip to 4 then only word pairs of at most 4 words apart can form skip bigrams .", "Adjusting Equations 16 , 17 , and 18 to use maximum skip distance limit is straightforward we only count the skip bigram matches , SKIP2 X , Y , within the maximum skip distance and replace denominators of Equations 16 , C m , 2 , and 17 , C n , 2 , with the actual numbers of within distance skip bigrams from the reference and the candidate respectively .", "One potential problem for ROUGE S is that it does not give any credit to a candidate sentence if the sentence does not have any word pair co occurring with its references .", "For example , the following sentence has a ROUGE S score of zero S5 . gunman the killed police S5 is the exact reverse of S1 and there is no skip bigram match between them .", "However , we would like to differentiate sentences similar to S5 from sentences that do not have single word cooccurrence with S1 .", "To achieve this , we extend ROUGE S with the addition of unigram as counting unit .", "The extended version is called ROUGE SU .", "We can also obtain ROUGE SU from ROUGE S by adding a begin of sentence marker at the beginning of candidate and reference sentences .", "To assess the effectiveness of ROUGE measures , we compute the correlation between ROUGE assigned summary scores and human assigned summary scores .", "The intuition is that a good evaluation measure should assign a good score to a good summary and a bad score to a bad summary .", "The ground truth is based on human assigned scores .", "Acquiring human judgments are usually very expensive ; fortunately , we have DUC 2001 , 2002 , and 2003 evaluation data that include human judgments for the following Besides these human judgments , we also have 3 sets of manual summaries for DUC 2001 , 2 sets for DUC 2002 , and 4 sets for DUC 2003 .", "Human judges assigned content coverage scores to a candidate summary by examining the percentage of content overlap between a manual summary unit , i . e . elementary discourse unit or sentence , and the candidate summary using Summary Evaluation Environment3 SEE developed by the University of Southern California s Information Sciences Institute ISI .", "The overall candidate summary score is the average of the content coverage scores of all the units in the manual summary .", "Note that human judges used only one manual summary in all the evaluations although multiple alternative summaries were available .", "With the DUC data , we computed Pearson s product moment correlation coefficients , Spearman s rank order correlation coefficients , and Kendall s correlation coefficients between systems average ROUGE scores and their human assigned average coverage scores using single reference and multiple references .", "To investigate the effect of stemming and inclusion or exclusion of stopwords , we also ran experiments over original automatic and manual summaries CASE set , stemmed4 version of the summaries STEM set , and stopped version of the summaries STOP set .", "For example , we computed ROUGE scores for the 12 systems participated in the DUC 2001 single document summarization evaluation using the CASE set with single reference and then calculated the three correlation scores for these 12 systems ROUGE scores vs . human assigned average coverage scores .", "After that we repeated the process using multiple references and then using STEM and STOP sets .", "Therefore , 2 multi or single x 3 CASE , STEM , or STOP x 3 Pearson , Spearman , or Kendall 18 data points were collected for each ROUGE measure and each DUC task .", "To assess the significance of the results , we applied bootstrap resampling technique Davison and Hinkley , 1997 to estimate 95 confidence intervals for every correlation computation .", "17 ROUGE measures were tested for each run using ROUGE evaluation package v1 . 2 . 1 ROUGE N with N 1 to 9 , ROUGE L , ROUGE W with weighting factor a 1 . 2 , ROUGE S and ROUGE SU with maximum skip distance d , 1 , , o 1 , 4 , and 9 .", "Due to limitation of space , we only report correlation analysis results based on Pearson s correlation coefficient .", "Correlation analyses based on Spearman s and Kendall s correlation coefficients are tracking Pearson s very closely and will be posted later at the ROUGE website5 for reference .", "The critical value6 for Pearson s correlation is 0 . 632 at 95 confidence with 8 degrees of freedom .", "Table 1 shows the Pearson s correlation coefficients of the 17 ROUGE measures vs . human judgments on DUC 2001 and 2002 100 words single document summarization data .", "The best values in each column are marked with dark green color and statistically equivalent values to the best values are marked with gray .", "We found that correlations were not affected by stemming or removal of stopwords in this data set , ROUGE 2 performed better among the ROUGE N variants , ROUGE L , ROUGE W , and ROUGE S were all performing well , and using multiple references improved performance though not much .", "All ROUGE measures achieved very good correlation with human judgments in the DUC 2002 data .", "This might due to the double sample size in DUC 2002 295 vs . 149 in DUC 2001 for each system .", "Table 2 shows the correlation analysis results on the DUC 2003 single document very short summary data .", "We found that ROUGE 1 , ROUGE L , ROUGESU4 and 9 , and ROUGE W were very good measures in this category , ROUGE N with N 1 performed significantly worse than all other measures , and exclusion of stopwords improved performance in general except for ROUGE 1 .", "Due to the large number of samples 624 in this data set , using multiple references did not improve correlations .", "In Table 3 A1 , A2 , and A3 , we show correlation analysis results on DUC 2001 , 2002 , and 2003 100 words multi document summarization data .", "The results indicated that using multiple references improved correlation and exclusion of stopwords usually improved performance .", "ROUGE 1 , 2 , and 3 performed fine but were not consistent .", "ROUGE 1 , ROUGE S4 , ROUGE SU4 , ROUGE S9 , and ROUGESU9 with stopword removal had correlation above 0 . 70 .", "ROUGE L and ROUGE W did not work well in this set of data .", "Table 3 C , D1 , D2 , E1 , E2 , and F show the correlation analyses using multiple references on the rest of DUC data .", "These results again suggested that exclusion of stopwords achieved better performance especially in multi document summaries of 50 words .", "Better correlations 0 . 70 were observed on long summary tasks , i . e .", "200 and 400 words summaries .", "The relative performance of ROUGE measures followed the pattern of the 100 words multi document summarization task .", "Comparing the results in Table 3 with Tables 1 and 2 , we found that correlation values in the multidocument tasks rarely reached high 90 except in long summary tasks .", "One possible explanation of this outcome is that we did not have large amount of samples for the multi document tasks .", "In the single document summarization tasks we had over 100 samples ; while we only had about 30 samples in the multi document tasks .", "The only tasks that had over 30 samples was from DUC 2002 and the correlations of ROUGE measures with human judgments on the 100 words summary task were much better and more stable than similar tasks in DUC 2001 and 2003 .", "Statistically stable human judgments of system performance might not be obtained due to lack of samples and this in turn caused instability of correlation analyses .", "In this paper , we introduced ROUGE , an automatic evaluation package for summarization , and conducted comprehensive evaluations of the automatic measures included in the ROUGE package using three years of DUC data .", "To check the significance of the results , we estimated confidence intervals of correlations using bootstrap resampling .", "We found that 1 ROUGE 2 , ROUGE L , ROUGE W , and ROUGE S worked well in single document summarization tasks , 2 ROUGE 1 , ROUGE L , ROUGE W , ROUGE SU4 , and ROUGE SU9 performed great in evaluating very short summaries or headline like summaries , 3 correlation of high 90 was hard to achieve for multi document summarization tasks but ROUGE 1 , ROUGE 2 , ROUGE S4 , ROUGE S9 , ROUGE SU4 , and ROUGE SU9 worked reasonably well when stopwords were excluded from matching , 4 exclusion of stopwords usually improved correlation , and 5 correlations to human judgments were increased by using multiple references .", "In summary , we showed that the ROUGE package could be used effectively in automatic evaluation of summaries .", "In a separate study Lin and Och , 2004 , ROUGE L , W , and S were also shown to be very effective in automatic evaluation of machine translation .", "The stability and reliability of ROUGE at different sample sizes was reported by the author in Lin , 2004 .", "However , how to achieve high correlation with human judgments in multi document summarization tasks as ROUGE already did in single document summarization tasks is still an open research topic .", "The author would like to thank the anonymous reviewers for their constructive comments , Paul Over at NIST , U . S . A , and ROUGE users around the world for testing and providing useful feedback on earlier versions of the ROUGE evaluation package , and the DARPA TIDES project for supporting this research ."], "summary_lines": ["ROUGE: A Package For Automatic Evaluation Of Summaries\n", "ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation.\n", "It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans.\n", "The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans.\n", "This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summarization evaluation package and their evaluations.\n", "Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST.\n"]}
{"article_lines": ["ECOGNI ZING F XT GENII . ES Wl r l l S lb , l ll , I , ; I , ; II I S USING DISCII .", "IMINANT ANAI , YS IS J USSI AIt I , C , ILEN juss i sics . se Swedish Insl ; il , ute of 2 ml , tter Science Box 1263 , S 164 28 K SrA , Stockholm , Sw . den I OUOI , ASS UTIIN cut t ing apple , com hl l le Compel , or Cupe . rl . lno , CA 95014 , USA Abstract A siml le method for d , egorizing texts into pre deturmincd text gem e c ; ttcgorics using tit st ; tti . , t , icM sl . utd ud tcch nique of discriminatH , amdysis is demonstrated wil . h appli cation to the Brown orpus .", "I is rimina . ut analysis makes it possibh tl , qC it , la , rge l l l l l l lber of l . Xla Ill l , rs Lh ; tL llHl , y 1 !", "SI 1 cific for a . certain corpus or inlormation stream , and combine I . henl into t small tmmber ol functions , wiLh t . he pa . ram i rs weighted oil basis of how usehd they ; u e for discritniml . t ing text genres .", "An a . ppli tl . ion to inforuta . tiott retrieval is discussed .", "Text Types Thor ; are .", "different types of l ; exL exl . s al oui , l . he sa . me th ing m ty be in differing geurcs , of difl rem .", "I y I eS , ; rod of v ; trying quality .", "Texts vary along st . ver ; d param .", "el . ers , a . ll relcwull , for l , he gcuera . l inlortlu tiol rel , ri wal problem of real . thing rea lcr needs m . I texts .", "liven this variat ion , in a text retrieval eonl . ext , the l rol lems arc i i Mttifying ; cures , and ii choosing criteria t , o ch , s ter texts of the smnc gem e , wit , h l redictal le l recision aml rcca . ll .", "This should uot he eonfused with t , he issue of idenl . ifying topics , m , d choosiug criW ria that .", "diserinl inatc on topic from auother .", "All . hough u t orthogonal to gem , del endent ; wu iat , ion , the wuiat , ioll i , hat , rela , l , es dirc t . ly to onW . uI ; and topic is Moug or , her litu . usions .", "Na . l , ura . lly , there is ; o va . riancc . . Iexl . s al oul .", "rl . aitl topics ula , y only occur iu ; rt ; ailt g ! tll ! s , alt . exl . s ill eertaiu ge . nres may only t . rea . t c ql . ain topics ; mosl .", "l . ol ics do , however , occur iu several ; cures , which is what inl ; erests us here .", "Douglas I il et has sl , udied l ; exl , variat . ion along scv eral l aranmtcrs , and found that t , cxt . s can I , , cousidcrcd to wvry along live ditnensious .", "In his st , udy , he clush . rs ai . ures according t . o eowuiauce , t . o find tmderlyiug di mens ions 198 ! .", "We wish to liud a method for idenl . ifv in ; easily eomput . al h ; I tl al , et . cHs t . hat ra . l idly classify previously IlllS ?", "ll texts in gell r ql classes and along a smal l set smalh r 1 , tmn I , il ers ivl .", "of dimm , siot , s , s , , ch that l . hcy can bc cxplai , , d in i , , t , tit . iwdy siml le terms to l . hc , , so , of a . n informal . ion rel . riewd Hq liea tion .", "m a im is 1 , o t ; ke set of texts that .", "has b ei , select , ed I y sotne sort of crude semant ic analysis uch as is typica . lly performexl I y an iufornmtion rel , ri ! vM sys l , em and I art . il . ion il , flulher I y genre or . cxl .", "t ; yl e , aud Experiment I I , xperiment 2 l xperiment 3 .", "l ? w Lc at e g ? zies . . .", "Informa . tive 1 .", "Press report ; tge B .", "Press editoriaJ L Press reviews 4 .", "Mis l Ileligion I , , .", "Skills and lIohhies 1 . .", "I olml u Lore C . Belles , cttr s , cl . c .", "21 Non . tiction l lT h v . doc .", ", estr n . d II .", "magin ; ttivu 3 .", "Fiction K . , eneral ietion I , .", "Mystery N . Adv .", "Wes elll P . tloma . nce i ii i , h . ; i .", "Table 1 , al , egories iu the I rowu ; orpus t . o display this wuiat . iou as si luply as possible in oue or l . wo dilu msions .", "Method vVe st , art by using catmes similar go those firsl , hlw ! s . igat d by iber , but wc eonc . ul , rate on ; hose t , hat ; arc easy 1 . o comput assuming we have a parl , of speech tag get hll , l . ing e l , 1992 ; hureh , 1988 , such ; Is , queh as i , Jlhd l lSOll l FoIIOllIl oeeull Ci , C l ; atc ; 18 o l obed 1 . o geucral hedges l iher , 1989 .", "More mid more of I ihcrs egtlaileS will be awfilahle with tim advent of more prolieieut aua . lysis programs , for iusl , a . nce if eom plel . e surface syntaet . ic l a . rsing were performed hefore catl ! gorizat . iotl Voul ; ilaiueu , Talmnai lu u , 1993 .", "W then use l iscr iuduant analysis , a . technique from descriptive . tatist . ics .", "atmlysis tak , s a set of l rCcat . egorized iml iv iduals and I ; ta ou t , hcir vm . m l iOl , Oil iI lltllIlb 21 o1 plLr lliiCl . elS lLlld WOlks olll .", "a s ! t discriminant Juuctions which dist ; ingnishes hetw . etl t . he groups .", "These l uuetious can l . llen l e used I . o predicl , the ca l . egory mlmd ershil s of new iudiv iduals based on tJmir ara . met ! r scores Tal . sluoka , 1971 ; M ustouen , 1965 .", "Evaluat ion or data .", "we used the Browu corpus of English text sn , i , hs of uuifolnt length , ca . l , cgorized ht se , cral cal . cgorh s I07 Variable Range Adverb count 19 157 Character count 7601 12143 , ong word count 6 chars 168 838 Preposit ion count 151 433 Seeond person pronoun count 0 89 Therefore count 0 11 Words per sentence average 8 . 2 5a . 2 Chars sentence average 34 . 6 266 . 3 First person pronoun count 0 156 Me count 0 3 1 Present part iciple count 6 1 11 Sentence count 40 236 Type token rat io 14 . 3 53 . 0 I count 0 120 Character per word average 3 . 8 5 . 8 It count 1 53 Noun count 243 75 l Present verb count 0 79 That count 1 72 Which count 0 40 Fable 2 Parameters for l i sc r iminant Ana lys i s Category I tems Er rors .", "ln fo rmat iw .", "374 16 4 I I .", "Imag inat ive 126 6 5 qbtM 500 22 4 Tab le 3 Categor i za t ion in Two Categor ies as seen in tab le 1 .", "We ran d i sc r iminant ana lys i s on the texts in the cor l us us ing seve . ral d i f ferent features as seen in tab le 2 .", "We used the SPSS sys tem for sta t i s t i ca l data ana lys i s , wh ich has as one of i ts fcatm . es a complete d i sc r iminant ana lys i s SPSS , 1990 .", "The d i ser iminant f lmct ion ext rac ted t ? om the data by the ana lys i s is a l inear combinat ion of t l le parameters .", "To categor i ze a set in to N categor ies N 1 funct ions need to be determined , l l owever , if we are content w i th be ing able to p lot al l ca tegor ies on a two d imens iona l p lane , wh id l p robab ly is what we want to do , for ease of ex pos i t ion , we on ly use the two f irst and most s ign i f i cant funct ions .", "2 categor ies In the ease of two categor ies , on ly one funct ion is nec essary foe determin ing the category of an i tenl .", "The f lmct ion c lassi f ied 478 cases cor rec t ly and mise lass i l l ed 22 , out of the 500 cases , as shown in tab le 3 and f igure I .", "4 categor ies Us ing the three funct ions ext rac ted , 366 cases were cor rect ly c lassi f ied , and 134 eases were misc lass i f ied , out of t i le 500 cases , as can be seen in tab le 4 and f igure 2 .", "M isce l laneous , the most p rob lemat ic category , is a loose group ing of d i f ferent in fo rmat ive texts .", "The s ing le most p rob lemat ic subsubset of texts is a subset of eigh teen non f ic t ion texts labe led learned humalfities .", "S ix teen of them were eniselassit ied , th i r teen as mis eell eleotls .", "40 I I I 20 I I I X .", "X I 11 I 111 1111 I I i i i i I 11111111 I 211111111 11111111111 2 11111111111112212 2 2 22 11111111111111111112222222222221 x .", "x 2 . 0 0 . 0 2 . 0 Cent ro ids F igure 1 D is t r ibut ion , 2 Categor ies Category J Errors 2 .", "Non Iiction 28 25 3 .", "12 ; I 4 .", "176 I 68 47 focal L ?", "l 134 27 ? T Tab le 4 Categor i za t ion in Four Categor ies .", "I 223 I 23 I 233 I 22433 244433 I 224 44333 I 1 244 44433 I 224 44333 I I 244 44433 I 0 . 0 224 4433 2244 44333 2444 44433 I 22211444444444444444433 I 221111111111111111111443333 I I 2211 111111333 22211 1113 I 22111 i i I I 2211 I I 22211 .", "2 . 0 0 . 0 2 . 0 F igure 2 l i s t r ibut ion , 4 Categor ies 1072 15 or 10 cat ; gorh . s Using th0 Oill l ; eell funetions extracted , 258 cases w we correctly classified and 242 cases inischlssilied out of the 500 cases , as shown in table 5 .", "Trying to distin .", "guish I eLween the di ferenL types of fiction is exl en sive .", "hi tornis of errors .", "the tiction subcategories were collapsed there only wouht be ten categories , and the error rate R r the c . atogorizal , ion would iniprove as showil ill th0 revis0d totM record of the tal le .", "The learned humanities nubcal ; egory is , as I erore , prol lematic only two of the .", "eighteen itomn were correctly classified .", "others were irlost often misclassilied as l , cl igion or Belles l . ettre . s .", "Va l idat ion o f the Techn ique It is i inl ortant o note that this exl erinlent does not claim to show how geHrc , s ill fact ditfer .", "What we show is tha . t this sort of teellnique can .", "bc used t . o determine which l aramcters to line , given set of them .", "We did not use a test set disjoint from I , he training set , and we do not claiul I ; hat the functions we had the method extract fi onl the data are useful iu theulselves .", "We dis cuss how well this meJ , hod categorizes a set texl , given a set of categories , alld given a net of paralllCl . ers .", "The error rates clinlt steelfly with the iiunlher of categories tested Ibr in the rims we used .", "This , m , y have to do with how the categories are chosen aud de fined .", "For iustance , distinguishing between dill rein .", "types of l iction by fornlal or stylistic criteria of this kind may just he sonicthing we shouht not a . tteml t the fictiou types are naturally delined ill ternln o1 their content , a . fter all .", "Fhc statistical tcchni luc of factor anM qsi , can be used to discover categories , like l iher has done .", "The prol lenl with using automatically lerived categories is that even if they are iu a sense tea . l , lneaniug that they are SUl l orted by data , i . hey may t e di l l icult to C ? l lain for l he uuenthusiastic l ltyl l ial l if l . he ahii is to tlS !", "the techlii lUe in retrieval tooln .", "Other criteria that shouhl be studied are second alld higher order statistics on the rospeoLivc l aranle ters .", "Jorl , ain l aranieterst robal ly varG lnor ill certahl text types than other aild they may have a s c ? lJcd dislribulion as well .", "This is i iot dillicull , to deterli i ine , although l . h !", "standard methods do llOt nupl orl , illltO lnatic detcr ininat ion of staudard devial , iou or skl wness as discrinl ination criteria .", "lT gethcr with iJle hwesti .", "gation of sew ; ra hil , herto Ultl . ried l aranlcters , this is a 11 7 .", "Readab i l i ty Index ing Not unrel Lted to the study of genre is the study of rcadabilily which aims to categorize texts aecoMing to their suital ility for assumed sets of assumed readers .", "There ix a weall , h of formula to couqmte readahilil . y .", "Most commonly l , hey combine easily computed text measures typically average or Saml led averag , s n t ; ncc leugth couibiucd with siulihMy couqluled woM length , or in ides , of words not on a sl ecified easy word lint , hall , 1948 ; K late , 1963 .", "hi spite of C , halln warnings al out inj , . ticious application to writing tasks , readal ility measurement has naively come to be used as a l l escriptive metric of good wr i t iug as a tool for writers , md has thus COllie into some disrepute , among text researchers Our small study conlirms the I asie findings of the early readal ility studies the most im i ortant fa . cl . ors of tim ones we tested are .", "word length , sentence length , and different derivatives of these two parameters .", "As long as readM ility indexing nchemes are used iT , descriptive at l lications they work well to discrinlilml ; e between text types .", "App l i ca t ion The technique shows practical promise .", "The territo rial nial s showu in ligmes 1 , 2 , and 3 are intuitively une ul tools for lisplayiug what type a particular text is , compared with other existing texts .", "The technique denionstrated above has au obvious application in in formatiol retrieval , for l ieking out interesting texts , if cutest based methods select a too large set for easy nlanipulation and browning Cutting c al , 1992 .", "In any specific application area it will be unlikely t , hat the text datM ase to be accessed will be completely free form .", "The texts uuder consideration will probably he speciiic in some way .", "C , encral text tyl eS may be useful , but quite l rohably there wil l be a domain or liehl sl ecilic text typology .", "In till envisioned apl lica tics , a user will employ a cascade of filters starting with filtering by topic , and continuing with filters by genre or text , l . yl e , aim ending by filters for text quality , or other t mtal , iv ; liner grained quMilieal , ionn .", "The In tF i l te r P ro jec t The Ntl , ilter Froject at the departments of Computer aml Systems Sciences , C , omputational , inguistics , md Psychology at Stockhohn University is at present stiMy . . ing texts on the USli . NIi ; T News contercncing system , The project at present studies texts which appear on several different types of USF . Nt ; T News coll erences , a , ml investigates how well the classilieation criteria and categories tllat exl erienced USENI , 71 News users report using lutl ilter , 1993 can be used by a newsreader systeni .", "To do this the l roject apl lics the method described here .", "The project uses categories uch as l tuery lCCOI I l l l e l l t l 1 l l kL l l I l l lC l l le l l t l l 1 FAQ , a l l l so orth , categorizing theui I , sing paranieters such ; is dif ti rent ypes of length tneanurcs , form word content , quote level , lereentage quoted text and other USEN I ; T News Sl ecific parameters .", "Acknowledgements Thanl , s to Hans Karlgrcu , Gumml K , iJlgren , _h c , ff Nun berg , Jau l ederscn , and the , ling re . ferees , who all have colH ril uted with suggestions and method logical discussious .", "70 , 7 . 7 Category A .", "Press reportage B .", "Press editorial C . Press reviews l .", "Religion E . Skills and Hobbies I .", "Popular Lore G . Belles Lettres , 13iogral hies re .", "Government documents misc .", "d . Learned K . General Fiction L . Mystery M . Science Fiction N . Adventure and Western P . Romance R . Ilumor Total l ! ietion From prev ious table Revised total Items 44 27 17 17 36 48 75 3O 80 29 24 6 29 29 9 500 126 500 Errors Miss 1 25 l , 8 ao A 4 4 I 8 47 G 17 47 , I 32 67 ? , 1 49 65 I , B , A 9 3o J 32 40 II , I , G , F 16 55 fiction 12 50 l 17 18 62 22 76 a aa 242 4s i , 5 178 35 Table 5 Categorization i 15 , at go les .", "4 2 L J J 0 2 JHH LL J J JH LLP J J JH LLLPKF J J JHH LLLPKKKFF J J J J Jn LLLPKKKKFFFFF J J J JHH LLLPKKK KF FFF J J J JH L LNPRK KF FF J J J JH LLLLNNNKKK KKF FF J J J J I I t t I LLLLNNNNKKK KFF FFF J J J JH LLLLNNNNNRKK KF FFF J J J J JHH LLLLNNNNNNNKK KKF FFFGGGGGJ J J J J t l LNNNN NNNKKK KK RFFFFFFFFFFGGGG GGGJ J J J JH INN NNKKK KKRRRBBBBBBB BBBBBGGGGGGGGGJJJJ JHH NNNKK KKKRR RB BBBBBBGGGGGGGJJ J J JJll I NNNKKK KKKRRR RRB BBAAAAAAAAAJJ J JHH NNKKK KKKRRR RBB BBA AAAAJJHH NKK KKRRR RB BBAA AAAAHI KK KKKRR RRB BAA AA KKKRRR RBB BBA KKKRRR RRB BBHBBBAAAAAA KKRRR RBBBBBBBBBBHBBBCCCCCCCCCCAAAAAAAAAAAAAA KKKRR RRBBBCCCCCCCCCCCC CCCCCCCCCCCCCCC KKKRRR RRCCCCC IKKRRR RRCC I .", "Figure 3 l istribution , 15 C . ategories Indicates a group eentroid .", "7074 l eferences Doug las B i tmr 1989 .", "A typology of English texts , I , iu guistics , 27 3 43 .", "Jeanne S . Cha l l 1948 .", "I cadnbility , Ohio Stal , c Univ .", "Ke lnmth C lmrch 1988 .", "A Stochastic Parts of 5 ; fmc h aJtd Noun Pitrasa Pa rser for Unrestricted Text , lbocs .", "2rid ANLP , Austbt .", "Douglas , Cut th tg , Ju l ian Kupi w . , Jan l .", "hns m , an 1 l . N . n h l e S ibun 1992 .", "A Ihact . ical lbn t of Stmech 13 . gger , lbocs .", "2rd A NLP , Trcnto .", "Doug lass Cu ? , t . lng , D . Karger , Jan Pedersml , m d John Tuk , . y 1992 .", "Scatl . e , ather A Jh , sl . cr lmst d Al l roa h to Browsing , arge ocument 2olhx l . ions Irocs .", "I n tF i l te r 199 1 .", "Working Papers of the lnll illcr Project , available I , y gopher from dsv . su . se pub In tF i te r .", "George R . K la re 1963 .", "ThcMcasurcmcntoft adabi l i tg , owa .", "W . N . 5ancis rod F . Ku i era 1982 . l rcq cm g An , ! sis of J tglish Usage , loughton MilllilL .", "Sept o Mus l , onmi 1965 .", "M ultiple t iscriminsu l Analy sis in Linguistic Problems , 5t ttislical Methods i t , in lui . slics , t 37 1 , t .", "M . M . Tatsuoka 7197l .", "Multivariate Analgsis , New York . lohn Wiley Sons .", "A t ro Vout i la inen and pas l 5 ? 1I all thlelt I993 .", "Ambi guity resoh , l . ion in a , reduct . ionistic parser , Procs .", "6lh uropcan A CL , t ltrcchl . . SPSS 1990 .", "The , 5 , 5 . b Ib ercncc ; id , . qdca go , qP , q5 I I IC 7075"], "summary_lines": ["Recognizing Text Genres With Simple Metrics Using Discriminant Analysis\n", "A simple method for categorizing texts into pre-determined text genre categories using the statistical standard technique of discriminant analysis is demonstrated with application to the Brown corpus.\n", "Discriminant analysis makes it possible to use a large number of parameters taht may be specific for a certain corpus or information streatm, and combine them into a small number of function, with the parameters weighted on bais of how useful they are for discriminating text genres.\n", "An application to information retrieval is discussed.\n", "We word length as an indicator of formality for applications such as genre classification.\n"]}
{"article_lines": ["Language Model Adaptation For Statistical Machine Translation Via Structured Query Models", "We explore unsupervised language model adaptation techniques for Statistical Machine Translation .", "The hypotheses from the machine translation output are converted into queries at different levels of representation power and used to extract similar sentences from very large monolingual text collection .", "Specific language models are then build from the retrieved data and interpolated with a general background model .", "Experiments show significant improvements when translating with these adapted language models .", "Language models LM are applied in many natural language processing applications , such as speech recognition and machine translation , to encapsulate syntactic , semantic and pragmatic information .", "For systems which learn from given data we frequently observe a severe drop in performance when moving to a new genre or new domain .", "In speech recognition a number of adaptation techniques have been developed to cope with this situation .", "In statistical machine translation we have a similar situation , i . e . estimate the model parameter from some data , and use the system to translate sentences which may not be well covered by the training data .", "Therefore , the potential of adaptation techniques needs to be explored for machine translation applications .", "Statistical machine translation is based on the noisy channel model , where the translation hypothesis is searched over the space defined by a translation model and a target language Brown et al , 1993 .", "Statistical machine translation can be formulated as follows maxarg maxarg tPtsPstPt tt ? where t is the target sentence , and s is the source sentence .", "P t is the target language model and P s t is the translation model .", "The argmax operation is the search , which is done by the decoder .", "In the current study we modify the target language model P t , to represent the test data better , and thereby improve the translation quality .", "Janiszek , et al 2001 list the following approaches to language model adaptation ? Linear interpolation of a general and a domain specific model Seymore , Rosenfeld , 1997 .", "Back off of domain specific probabilities with those of a specific model Besling , Meier , 1995 .", "Retrieval of documents pertinent to the new domain and training a language model on line with those data Iyer , Ostendorf , 1999 , Mahajan et . al . 1999 .", "Maximum entropy , minimum discrimination adaptation Chen , et . al . , 1998 .", "Adaptation by linear transformation of vectors of bigram counts in a reduced space DeMori , Federico , 1999 .", "Smoothing and adaptation in a dual space via latent semantic analysis , modeling long term semantic dependencies , and trigger combinations .", "J . Bellegarda , 2000 .", "Our approach can be characterized as unsupervised data augmentation by retrieval of relevant documents from large monolingual corpora , and interpolation of the specific language model , build from the retrieved data , with a background language model .", "To be more specific , the following steps are carried out to do the language model adaptation .", "First , a baseline statistical machine translation system , using a large general language model , is applied to generate initial translations .", "Then these translations hypotheses are reformulated as queries to retrieve similar sentences from a very large text collection .", "A small domain specific language model is build using the retrieved sentences and linearly interpolated with the background language model .", "This new interpolated language model in applied in a second decoding run to produce the final translations .", "There are a number of interesting questions pertaining to this approach ? Which information can and should used to generate the queries the first best translation only , or also translation alternatives .", "How should we construct the queries , just as simple bag of words , or can we incorporate more structure to make them more powerful .", "How many documents should be retrieved to build the specific language models , and on what granularity should this be done , i . e . what is a document in the information retrieval process .", "The paper is structured as follows section 2 outlines the sentence retrieval approach , and three bag of words query models are designed and explored ; structured query models are introduced in section 3 .", "In section 4 we present translation experiments are presented for the different query .", "Finally , summary is given in section 5 .", "Our language model adaptation is an unsupervised data augmentation approach guided by query models .", "Given a baseline statistical machine translation system , the language model adaptation is done in several steps shown as follows ? Generate a set of initial translation hypotheses H h1 ? hn for source sentences s , using either the baseline MT system with the background language model or only the translation model ? Use H to build query ? Use query to retrieve relevant sentences from the large corpus ? Build specific language models from retrieved sentences ? Interpolate the specific language model with the background language ? Re translate sentences s with adapted language model Figure 1 Adaptation Algorithm The specific language model hwP iA and the general background model hwP iB are combined using linear interpolation 1 ?", "hwPhwPhwP iAiBi ? ?", "? 1 The interpolation factor ? can be simply estimated using cross validation or a grid search .", "As an alternative to using translations for the baseline system , we will also describe an approach , which uses partial translations of the source sentence , using the translation model only .", "In this case , no full translation needs to be carried out in the first step ; only information from the translation model is used .", "Our approach focuses on query model building , using different levels of knowledge representations from the hypothesis set or from the translation model itself .", "The quality of the query models is crucial to the adapted language model ? s performance .", "Three bag of words query models are proposed and explained in the following sections .", "2 . 1 Sentence Retrieval Process .", "In our sentence retrieval process , the standard tf idf term frequency and inverse document frequency term weighting scheme is used .", "The queries are built from the translation hypotheses .", "We follow Eck , et al , 2004 in considering each sentence in the monolingual corpus as a document , as they have shown that this gives better results compared to retrieving entire news stories .", "Both the query and the sentences in the text corpus are converted into vectors by assigning a term weight to each word .", "Then the cosine similarity is calculated proportional to the inner product of the two vectors .", "All sentences are ranked according to their similarity with the query , and the most similar sentences are used as the data for building the specific language model .", "In our experiments we use different numbers of similar sentences , ranting from one to several thousand .", "2 . 2 Bag of words Query Models .", "Different query models are designed to guide the data augmentation efficiently .", "We first define ? bag of words ?", "models , based on different levels of knowledge collected from the hypotheses of the statistical machine translation engine .", "2 . 2 . 1 First best Hypothesis as a Query Model The first best hypothesis is the Viterbi path in the search space returned from the statistical machine translation decoder .", "It is the optimal hypothesis the statistical machine translation system can generate using the given translation and language model , and restricted by the applied pruning strategy .", "Ignoring word order , the hypothesis is converted into a bag of words representation , which is then used as a query , , , 1211 TiiilT VwfwwwwQ ? L where iw is a word in the vocabulary 1TV of the Top 1 hypothesis .", "if is the frequency of iw ? s occurrence in the hypothesis .", "The first best hypothesis is the actual translation we want to improve , and usually it captures enough correct word translations to secure a sound adaptation process .", "But it can miss some informative translation words , which could lead to better adapted language models .", "2 . 2 . 2 N Best Hypothesis List as a Query Model Similar to the first best hypothesis , the n best hypothesis list is converted into a bag of words representation .", "Words which occurred in several translation hypotheses are simply repeated in the bag of words representations .", ", , , ; ; , , , 2 , 1 , , 12 , 11 , 1 1 TNiii lNNNlTN Vwfw wwwwwwQ N ? LLL where TNV is the combined vocabulary from all n best hypotheses and if is the frequency of iw ? s occurrence in the n best hypothesis list .", "TNQ has several good characteristics First it contains translation candidates , and thus is more informative than 1TQ . In addition , the confidently translated words usually occur in every hypothesis in the n best list , therefore have a stronger impact on the retrieval result due to the higher term frequency tf in the query .", "Thirdly , most of the hypotheses are only different from each other in one word or two .", "This means , there is not so much noise and variance introduced in this query model .", "2 . 2 . 3 Translation Model as a Query Model To fully leverage the available knowledge from the translation system , the translation model can be used to guide the language model adaptation process .", "As introduced in section 1 , the translation model represents the full knowledge of translating words , as it encodes all possible translations candidates for a given source sentence .", "Thus the query model based on the translation model , has potential advantages over both 1TQ and TNQ . To utilize the translation model , all the n grams from the source sentence are extracted , and the corresponding candidate translations are collected from the translation model .", "These are then converted into a bag of words representation as follows , , , ; ; , , , 2 , 1 , , 2 , 1 , 1111 TMiii nsssnsssTM Vwfw wwwwwwQ IIII ? LLL where is is a source n gram , and I is the number of n grams in the source sentence .", "jsiw , is a candidate target word as translation of is . Thus the translation model is converted into a collection of target words as a bag of word query model .", "There is no decoding process involved to build TMQ . This means TMQ does not incorporate any background language model information at all , while both 1TQ and TNQ implicitly use the background language model to prune the words in the query .", "Thus TMQ is a generalization , and 1TQ and TNQ are pruned versions .", "This also means TMQ is subject to more noise .", "Word proximity and word order is closely related to syntactic and semantic characteristics .", "However , it is not modeled in the query models presented so far , which are simple bag of words representations .", "Incorporating syntactic and semantic information into the query models can potentially improve the effectiveness of LM adaptation .", "The word proximity and word ordering information can be easily extracted from the first best hypothesis , the n best hypothesis list , and the translation lattice built from the translation model .", "After extraction of the information , structured query models are proposed using the structured query language , described in the Section 3 . 1 .", "3 . 1 Structured Query Language .", "This query language essentially enables the use of proximity operators ordered and unordered windows in queries , so that it is possible to model the syntactic and semantic information encoded in phrases , n grams , and co occurred word pairs .", "The InQuery implementation Lemur 2003 is applied .", "So far 16 operators are defined in InQuery to model word proximity ordered , unordered , phrase level , and passage level .", "Four of these operators are used specially for our language model adaptation Sum Operator sum 1t ? nt The terms or nodes 1t ? nt are treated as having equal influence on the final retrieval result .", "The belief values provided by the arguments of the sum are averaged to produce the belief value of the sum node .", "Weighted Sum Operator wsum 11 tw , ? The terms or nodes 1t ? nt contribute unequally to the final result according to the weight iw associated with each it . Ordered Distance Operator N 1t ? nt The terms must be found within N words of each other in the text in order to contribute to the document's belief value .", "An n gram phrase can be modeled as an ordered distance operator with N n . Unordered Distance Operator uwN 1t ? nt The terms contained must be found in any order within a window of N words in order for this operator to contribute to the belief value of the document .", "3 . 2 Structured Query Models .", "Given the representation power of the structured query language , the Top 1 hypothesis , Top N Best hypothesis list , and the translation lattice can be converted into three Structured Query Models respectively .", "For first best and n best hypotheses , we collect related target n grams of a given source word according to the alignments generated in the Viterbi decoding process .", "While for the translation lattice , similar to the construction of TMQ , we collect all the source n grams , and translate them into target n grams .", "In either case , we get a set of target n grams for each source word .", "The structured query model for the whole source sentence is a collection of such subsets of target n grams .", ", , , 21 Isssst .", "tttQ vLvv is t v is a set of target n grams for the source word is ; , ; , 311211 LLL v gramiiigramiigramis ttttttt i ? ? ? ? In our experiments , we consider up to trigram for better retrieval efficiency , but higher order n grams could be used as will .", "The second simplification is that every source word is equally important , thus each n gram subset is t v will have an equal contribution to the final retrieval results .", "The last simplification is each n gram within the set of is t v has an equal weight , i . e . we do not use the translation probabilities of the translation model .", "If the system is a phrase based translation system , we can encode the phrases using the ordered distance operator N with N equals to the number of the words of that phrase , which is denoted as the phrase operator in InQuery implementation .", "The 2 grams and 3 grams can be encoded using this operator too .", "Thus our final structured query model is a sum operator over a set of nodes .", "Each node corresponds to a source word .", "Usually each source word has a number of translation candidates unigrams or phrases .", "Each node is a weighted sum over all translation candidates weighted by their frequency in the hypothesis set .", "An example is shown below , where phrase indicates the use of the ordered distance operator with varying n q sum wsum 2 eu 2 phrase european union wsum 12 phrase the united states 1 american 1 phrase an american wsum 4 are 1 is wsum 8 markets 3 market wsum 7 phrase the main 5 primary ;", "Experiments are carried out on a standard statistical machine translation task defined in the NIST evaluation in June 2002 .", "There are 878 test sentences in Chinese , and each sentence has four human translations as references .", "NIST score NIST 2002 and Bleu score Papineni et . al . 2002 of mteval version 9 are reported to evaluate the translation quality .", "4 . 1 Baseline Translation System .", "Our baseline system Vogel et al , 2003 gives scores of 7 . 80 NIST and 0 . 1952 Bleu for Top 1 hypothesis , which is comparable to the best results reported on this task .", "For the baseline system , we built a translation model using 284K parallel sentence pairs , and a trigram language model from a 160 million words general English news text collection .", "This LM is the background model to be adapted .", "With the baseline system , the n best hypotheses list and the translation lattice are extracted to build the query models .", "Experiments are carried out on the adapted language model using the three bag of words query models 1TQ , TNQ and TMQ , and the corresponding structured query models .", "4 . 2 Data GigaWord Corpora .", "The so called GigaWord corpora LDC , 2003 are very large English news text collections .", "There are four distinct international sources of English newswire AFE Agence France Press English Service APW Associated Press Worldstream English Service NYT The New York Times Newswire Service XIE The Xinhua News Agency English Service Table 1 shows the size of each part in word counts .", "AFE APW NYT XIE 170 , 969K 539 , 665K 914 , 159K 131 , 711K Table 1 Number of words in the different GigaWord corpora As the Lemur toolkit could not handle the two large corpora APW and NYT we used only 200 million words from each of these two corpora .", "In the preprocessing all words are lowercased and punctuation is separated .", "There is no explicit removal of stop words as they usually fade out by tf . idf weights , and our experiments showed not positive effects when removing stop words .", "4 . 3 Bag of Words Query Models .", "Table 2 shows the size of 1TQ , TNQ and TMQ in terms of number of tokens in the 878 queries 1TQ TNQ TMQ Q 25 , 861 231 , 834 3 , 412 , 512 Table 2 Query size in number of tokens As words occurring several times are reduced to word frequency pairs , the size of the queries generated from the 100 best translation lists is only 9 times as big as the queries generated from the first best translations .", "The queries generated from the translation model contain many more translation alternatives , summing up to almost 3 . 4 million tokens .", "Using the lattices the whole information of the translation model is kept .", "4 . 3 . 1 Results for Query 1TQ In the first experiment we used the first best translations to generate the queries .", "For each of the 4 corpora different numbers of similar sentences 1 , 10 , 100 , and 1000 were retrieved to build specific language models .", "Figure 2 shows the language model adaptation after tuning the interpolation factor ? by a grid search over 0 , 1 .", "Typically ? is around 0 . 80 .", "1 Best NIST Scores 7 . 7500 7 . 8000 7 . 8500 7 . 9000 7 . 9500 8 . 0000 AFE APW NYT XIE Top1 Top10 Top100 Top1000 Baseline 1 Best BLEU Scores 0 . 1900 0 . 1920 0 . 1940 0 . 1960 0 . 1980 0 . 2000 0 . 2020 0 . 2040 AFE APW NYT XIE Top1 Top10 Top100 Top1000 Baseline Figure 2 NIST and Bleu scores 1TQ We see that each corpus gives an improvement over the baseline .", "The best NIST score is 7 . 94 , and the best Bleu score is 0 . 2018 .", "Both best scores are realized using top 100 relevant sentences corpus per source sentence mined from the AFE .", "4 . 3 . 2 Results for Query TNQ Figure 3 shows the results for the query model TNQ . The best results are 7 . 99 NIST score , and 0 . 2022 Bleu score .", "These improvements are statistically significant .", "Both scores are achieved at the same settings as those in 1TQ , i . e . using top 100 retrieved relevant sentences mined from the AFE corpus .", "100 Best NIST Scores 7 . 7500 7 . 8000 7 . 8500 7 . 9000 7 . 9500 8 . 0000 AFE APW NYT XIE Top1 Top10 Top100 Top1000 Baseline 100 Best BLEU Scores 0 . 1900 0 . 1920 0 . 1940 0 . 1960 0 . 1980 0 . 2000 0 . 2020 0 . 2040 AFE APW NYT XIE Top1 Top10 Top100 Top1000 Baseline Figure 3 NIST and Bleu scores from TNQ Using the translation alternatives to retrieve the data for language model adaptation gives an improvement over using the first best translation only for query construction .", "Using only one translation hypothesis to build an adapted language model has the tendency to reinforce that translation .", "4 . 3 . 3 Results for Query TMQ The third bag of words query model uses all translation alternatives for source words and source phrases .", "Figure 4 shows the results of this query model TMQ . The best results are 7 . 91 NIST score and 0 . 1995 Bleu .", "For this query model best results were achieved using the top 1000 relevant sentences mined from the AFE corpus per source sentence .", "The improvement is not as much as the other two query models .", "The reason is probably that all translation alternatives , even wrong translations resulting from errors in the word and phrase alignment , contribute alike to retrieve similar sentences .", "Thereby , an adapted language model is built , which reinforces not only good translations , but also bad translations .", "All the three query models showed improvements over the baseline system in terms of NIST and Bleu scores .", "The best bag of words query model is TNQ built from the N Best list .", "It provides a good balance between incorporating translation alternatives in the language model adaptation process and not reinforcing wrong translations .", "Lattice NIST Scores 7 . 7500 7 . 8000 7 . 8500 7 . 9000 7 . 9500 8 . 0000 AFE APW NYT XIE Top1 Top10 Top100 Top1000 Baseline Lattice BLEU Scores 0 . 1900 0 . 1920 0 . 1940 0 . 1960 0 . 1980 0 . 2000 0 . 2020 0 . 2040 AFE APW NYT XIE Top1 Top10 Top100 Top1000 Baseline Figure 4 NIST and Bleu scores from TMQ 4 . 4 Structured Query Models .", "The next series of experiments was done to study if using word order information in constructing the queries could help to generate more effective adapted language models .", "By using the structured query language we converted the same first best hypothesis , the 100 best list , and the translation lattice into structured query models .", "Results are reported for the AFE corpus only , as this corpus gave best translation scores .", "Figure 5 shows the results for all three structured query models , built from the first best hypothesis ? 1 Best ? , the 100 best hypotheses list ? 100 Best ? , and translation lattice ? TM Lattice ? .", "Using these query models , different numbers of most similar sentences , ranging from 100 to 4000 , where retrieved from the AFE corpus .", "The given baseline results are the best results achieved from the corresponding bag of words query models .", "Consistent improvements were observed on NIST and Bleu scores .", "Again , optimal interpolation factors to interpolate the specific language models with the background language model were used , which typically were in the range of 0 . 6 , 0 . 7 .", "Structured query models give most improvements when using more sentences for language model adaptation .", "The effect is more pronounced for Bleu then for NIST score .", "Structured query NIST Scores 7 . 7500 7 . 8000 7 . 8500 7 . 9000 7 . 9500 8 . 0000 8 . 0500 8 . 1000 8 . 1500 Baseline Top100 Top500 Top1000 Top2000 Top4000 1 Best 100 Best TM Lattice Structured query BLEU Scores 0 . 1920 0 . 1940 0 . 1960 0 . 1980 0 . 2000 0 . 2020 0 . 2040 0 . 2060 0 . 2080 Baseline Top100 Top500 Top1000 Top2000 Top4000 1 Best 100 Best TM Lattice Figure 5 NIST and Bleu scores from the structured query models The really interesting result is that the structured query model TMQ gives now the best translation results .", "Adding word order information to the queries obviously helps to reduce the noise in the retrieved data by selecting sentences , which are closer to the good translations , The best results using the adapted language models are NIST score 8 . 12 for using the 2000 most similar sentences , whereas Bleu score goes up to 0 . 2068 when using 4000 sentences for language model adaptation .", "4 . 5 Example .", "Table 3 shows translation examples for the 17th Chinese sentence in the test set .", "We applied the baseline system Base , the bag of word query model Hyp1 , and the structured query model Hyp2 using AFE corpus .", "Ref The police has already blockade the scene of the explosion .", "Base At present , the police had cordoned off the explosion .", "Hyp1 At present , police have sealed off the explosion .", "Hyp2 Currently , police have blockade on the scene of the explosion .", "Table 3 Translation examples 4 . 6 Oracle Experiment Finally , we run an oracle experiments to see how much improvement could be achieved if we only selected better data for the specific language models .", "We converted the four available reference translations into structured query models and retrieved the top 4000 relevant sentences from AFE corpus for each source sentence .", "Using these language models , interpolated with the background language model gave a NIST score of 8 . 67 , and a Bleu score of 0 . 2228 .", "This result indicates that there is room for further improvements using this language model adaptation technique .", "The oracle experiment suggests that better initial translations lead to better language models and thereby better 2nd iteration translations .", "This lead to the question if we can iterate the retrieval process several times to get further improvement , or if the observed improvement results form using for good translations , which have more diversity than the translations in an n best list .", "On the other side the oracle experiment also shows that the optimally expected improvement is limited by the translation model and decoding algorithm used in the current SMT system .", "In this paper , we studied language model adaptation for statistical machine translation .", "Extracting sentences most similar to the initial translations , building specific language models for each sentence to be translated , and interpolating those with the background language models gives significant improvement in translation quality .", "Using structured query models , which capture word order information , leads to better results that plain bag of words models .", "The results obtained suggest a number of extensions of this work The first question is if more data to retrieve similar sentences from will result in even better translation quality .", "A second interesting question is if the translation probabilities can be incorporated into the queries .", "This might be especially useful for structured query models generated from the translation lattices ."], "summary_lines": ["Language Model Adaptation For Statistical Machine Translation Via Structured Query Models\n", "We explore unsupervised language model adaptation techniques for Statistical Machine Translation.\n", "The hypotheses from the machine translation output are converted into queries at different levels of representation power and used to extract similar sentences from very large monolingual text collection.\n", "Specific language models are then build from the retrieved data and interpolated with a general background model.\n", "Experiments show significant improvements when translating with these adapted language models.\n", "We apply a slightly different sentence-level strategy to language model adaptation, first generating an nbest list with a baseline system, then finding similar sentences in a monolingual target language corpus.\n", "We construct specific language models by using machine translation output as queries to extract similar sentences from large monolingual corpora.\n", "We convert initial SMT hypotheses to queries and retrieved similar sentences from a large monolingual collection.\n"]}
{"article_lines": ["Unsupervised Part Of Speech Tagging Employing Efficient Graph Clustering", "An unsupervised part of speech POS tagging system that relies on graph clustering methods is described .", "Unlike in current state of the art approaches , the kind and number of different tags is generated by the method itself .", "We compute and merge two partitionings of word graphs one based on context similarity of high frequency words , another on log likelihood statistics for words of lower frequencies .", "Using the resulting word clusters as a lexicon , a Viterbi POS tagger is trained , which is refined by a morphological component .", "The approach is evaluated on three different languages by measuring agreement with existing taggers .", "Assigning syntactic categories to words is an important pre processing step for most NLP applications .", "Essentially , two things are needed to construct a tagger a lexicon that contains tags for words and a mechanism to assign tags to running words in a text .", "There are words whose tags depend on their use .", "Further , we also need to be able to tag previously unseen words .", "Lexical resources have to offer the possible tags , and our mechanism has to choose the appropriate tag based on the context .", "Given a sufficient amount of manually tagged text , several approaches have demonstrated the ability to learn the instance of a tagging mechanism from manually labelled data and apply it successfully to unseen data .", "Those highquality resources are typically unavailable for many languages and their creation is labourintensive .", "We will describe an alternative needing much less human intervention .", "In this work , steps are undertaken to derive a lexicon of syntactic categories from unstructured text without prior linguistic knowledge .", "We employ two different techniques , one for highand medium frequency terms , one for mediumand low frequency terms .", "The categories will be used for the tagging of the same text where the categories were derived from .", "In this way , domain or language specific categories are automatically discovered .", "There are a number of approaches to derive syntactic categories .", "All of them employ a syntactic version of Harris distributional hypothesis Words of similar parts of speech can be observed in the same syntactic contexts .", "Contexts in that sense are often restricted to the most frequent words .", "The words used to describe syntactic contexts will be called feature words in the remainder .", "Target words , as opposed to this , are the words that are to be grouped into syntactic clusters .", "The general methodology Finch and Chater , 1992 ; Sch\u00fctze , 1995 ; inter al . for inducing word class information can be outlined as follows Throughout , feature words are the 150 250 words with the highest frequency .", "Contexts are the feature words appearing in the immediate neighbourhood of a word .", "The word s global context is the sum of all its contexts .", "For clustering , a similarity measure has to be defined and a clustering algorithm has to be chosen .", "Finch and Chater 1992 use the Spearman Rank Correlation Coefficient and a hierarchical clustering , Sch\u00fctze 1995 uses the cosine between vector angles and Buckshot clustering .", "An extension to this generic scheme is presented in Clark , 2003 , where morphological Proceedings of the COLING ACL 2006 Student Research Workshop , pages 7 12 , Sydney , July 2006 . c 2006 Association for Computational Linguistics information is used for determining the word class of rare words .", "Freitag 2004 does not sum up the contexts of each word in a context vector , but the most frequent instances of four word windows are used in a co clustering algorithm .", "Regarding syntactic ambiguity , most approaches do not deal with this issue while clustering , but try to resolve ambiguities at the later tagging stage .", "A severe problem with most clustering algorithms is that they are parameterised by the number of clusters .", "As there are as many different word class schemes as tag sets , and the exact amount of word classes is not agreed upon intra and interlingually , inputting the number of desired clusters beforehand is clearly a drawback .", "In that way , the clustering algorithm is forced to split coherent clusters or to join incompatible sub clusters .", "In contrast , unsupervised part of speech induction means the induction of the tag set , which implies finding the number of classes in an unguided way .", "This work constructs an unsupervised POS tagger from scratch .", "Input to our system is a considerable amount of unlabeled , monolingual text bar any POS information .", "In a first stage , we employ a clustering algorithm on distributional similarity , which groups a subset of the most frequent 10 , 000 words of a corpus into several hundred clusters partitioning 1 .", "Second , we use similarity scores on neighbouring co occurrence profiles to obtain again several hundred clusters of medium and low frequency words partitioning 2 .", "The combination of both partitionings yields a set of word forms belonging to the same derived syntactic category .", "To gain on text coverage , we add ambiguous high frequency words that were discarded for partitioning 1 to the lexicon .", "Finally , we train a Viterbi tagger with this lexicon and augment it with an affix classifier for unknown words .", "The resulting taggers are evaluated against outputs of supervised taggers for various languages .", "The method employed here follows the coarse methodology as described in the introduction , but differs from other works in several respects .", "Although we use 4 word context windows and the top frequency words as features as in Sch\u00fctze 1995 , we transform the cosine similarity values between the vectors of our target words into a graph representation .", "Additionally , we provide a methdology to identify and incorporate POS ambiguous words as well as low frequency words into the lexicon .", "Let us consider a weighted , undirected graph G V , E v V vertices , vi , vj , wij E edges with weights wij .", "Vertices represent entities here words ; the weight of an edge between two vertices indicates their similarity .", "As the data here is collected in feature vectors , the question arises why it should be transformed into a graph representation .", "The reason is , that graph clustering algorithms such as e . g .", "van Dongen , 2000 ; Biemann 2006 , find the number of clusters automatically1 .", "Further , outliers are handled naturally in that framework , as they are represented as singleton nodes without edges and can be excluded from the clustering .", "A threshold s on similarity serves as a parameter to influence the number of non singleton nodes in the resulting graph .", "For assigning classes , we use the Chinese Whispers CW graph clustering algorithm , which has been proven useful in NLP applications as described in Biemann 2006 .", "It is time linear with respect to the number of edges , making its application viable even for graphs with several million nodes and edges .", "Further , CW is parameter free , operates locally and results in a partitioning of the graph , excluding singletons i . e . nodes without edges .", "Partitioning 1 High and medium frequency words Four steps are executed in order to obtain partitioning 1 partitioning as one member clusters .", "The graph construction in step 2 is conducted by adding an edge between two words a and b with weight w 1 1 cos a , b , if w exceeds a similarity threshold s . The latter influences the number of words that actually end up in the graph and get clustered .", "It might be desired to cluster fewer words with higher confidence as opposed to running in the danger of joining two unrelated clusters because of too many ambiguous words that connect them .", "After step 3 , we already have a partition of a subset of our target words .", "The distinctions are normally more fine grained than existing tag sets .", "As feature words form the bulk of tokens in corpora , it is clearly desired to make sure that they appear in the final partitioning , although they might form word classes of their own2 .", "This is done in step 4 .", "We argue that assigning separate word classes for high frequency words is a more robust choice then trying to disambiguate them while tagging .", "Lexicon size for partitioning 1 is limited by the computational complexity of step 2 , which is time quadratic in the number of target words .", "For adding words with lower frequencies , we pursue another strategy .", "Partitioning 2 Medium and low frequency words As noted in Dunning , 1993 , log likelihood statistics are able to capture word bi gram regularities .", "Given a word , its neighbouring cooccurrences as ranked by the log likelihood reflect the typical immediate contexts of the word .", "Regarding the highest ranked neighbours as the profile of the word , it is possible to assign similarity scores between two words A and B according to how many neighbours they share , i . e . to what extent the profiles of A and B overlap .", "This directly induces a graph , which can be again clustered by CW .", "This procedure is parametrised by a loglikelihood threshold and the minimum number of left and right neighbours A and B share in order to draw an edge between them in the resulting graph .", "For experiments , we chose a minimum log likelihood of 3 . 84 corresponding to statistical dependence on 5 level , and at least four shared neighbours of A and B on each side .", "Only words with a frequency rank higher than 2 , 000 are taken into account .", "Again , we obtain several hundred clusters , mostly of open word classes .", "For computing partitioning 2 , an efficient algorithm like CW is crucial the graphs 2 This might even be desired , e . g . for English not . as used for the experiments consisted of 52 , 857 691 , 241 English , 85 , 827 702 , 349 Finnish and 137 , 951 1 , 493 , 571 German nodes edges .", "The procedure to construct the graphs is faster than the method used for partitioning 1 , as only words that share at least one neighbour have to be compared and therefore can handle more words with reasonable computing time .", "Combination of partitionings 1 and 2 Now , we have two partitionings of two different , yet overlapping frequency bands .", "A large portion of these 8 , 000 words in the overlapping region is present in both partitionings .", "Again , we construct a graph , containing the clusters of both partitionings as nodes ; weights of edges are the number of common elements , if at least two elements are shared .", "And again , CW is used to cluster this graph of clusters .", "This results in fewer clusters than before for the following reason While the granularities of partitionings 1 and 2 are both high , they capture different aspects as they are obtained from different sources .", "Nodes of large clusters which usually consist of open word classes have many edges to the other partitioning s nodes , which in turn connect to yet other clusters of the same word class .", "Eventually , these clusters can be grouped into one .", "Clusters that are not included in the graph of clusters are treated differently , depending on their origin clusters of partition 1 are added to the result , as they are believed to contain important closed word class groups .", "Dropouts from partitioning 2 are left out , as they mostly consist of small , yet semantically motivated word sets .", "Combining both partitionings in this way , we arrive at about 200 500 clusters that will be further used as a lexicon for tagging .", "A lexicon is constructed from the merged partitionings , which contains one possible tag the cluster ID per word .", "To increase text coverage , it is possible to include those words that dropped out in the distributional step for partitioning 1 into the lexicon .", "It is assumed that these words dropped out because of ambiguity .", "From a graph with a lower similarity threshold s here such that the graph contained 9 , 500 target words , we obtain the neighbourhoods of these words one at a time .", "The tags of those neighbours if known provide a distribution of possible tags for these words .", "Unlike in supervised scenarios , our task is not to train a tagger model from a small corpus of hand tagged data , but from our clusters of derived syntactic categories and a considerably large , yet unlabeled corpus .", "We decided to use a simple trigram model without re estimation techniques .", "Adopting a standard POS tagging framework , we maximize the probability of the joint occurrence of tokens ti and categories ci for a sequence of length n The transition probability P ci ci 1 , ci 2 is estimated from word trigrams in the corpus whose elements are all present in our lexicon .", "The last term of the product , namely P ci ti , is dependent on the lexicon3 .", "If the lexicon does not contain ti , then ci only depends on neighbouring categories .", "Words like these are called out of vocabulary OOV words .", "Morphologically motivated add ons are used e . g . in Clark , 2003 and Freitag 2004 to guess a more appropriate category distribution based on a word s suffix or its capitalization for OOV words .", "Here , we examine the effects of Compact Patricia Trie classifiers CPT trained on prefixes and suffixes .", "We use the implementation of Witschel and Biemann , 2005 .", "For OOV words , the category wise product of both classifier s distributions serve as probabilities P ci ti Let w ab cd be a word , a be the longest common prefix of w that can be found in all lexicon words , and d be the longest common suffix of w that can be found in all lexicon words .", "Then", "We adopt the methodology of Freitag 2004 and measure cluster conditional tag perplexity PP as the average amount of uncertainty to predict the tags of a POS tagged corpus , given the tagging with classes from the unsupervised method .", "Let be the mutual information between two random variables X and Y .", "Then the clusterconditional tag perplexity for a gold standard tagging T and a tagging resulting from clusters C is computed as Minimum PP is 1 . 0 , connoting a perfect congruence on gold standard tags .", "In the experiment section we report PP on lexicon words and OOV words separately .", "The objective is to minimize the total PP .", "For this study , we chose three corpora the British National Corpus BNC for English , a 10 Million sentences newspaper corpus from Projekt Deutscher Wortschatz4 for German , and 3 million sentences from a Finnish web corpus from the same source .", "Table 1 summarizes some characteristics . lang . sent . tok . tagger nr .", "200 10K tags cov . cov .", "CPTs do not only smoothly serve as a substitute lexicon component , they also realize capitalization , camel case and suffix endings naturally .", "Since a high coverage is reached with few words in English , a strategy that assigns only the most frequent words to sensible clusters will take us very far here .", "In the Finnish case , we can expect a high OOV rate , hampering performance of strategies that cannot cope well with low frequency or unseen words . value , we found the best performance averaged over all corpora .", "To put our results in perspective , we computed the following baselines on random samples of the same 1000 randomly chosen sentences that we used for evaluation Table 2 summarizes the baselines .", "We give PP figures as well as tag conditional cluster perplexity PPG uncertainty to predict the clustering from the gold standard tags , inverse direction of PP We measured the quality of the resulting taggers for combinations of several substeps Figure 2 illustrates the influence of the similarity threshold s for O , OM and OMA for German the other languages showed similar results .", "Varying s influences coverage on the 10 , 000 target words .", "When clustering very few words , tagging performance on these words reaches a PP as low as 1 . 25 but the high OOV rate impairs the total performance .", "Clustering too many words results in deterioration of results most words end up in one big partition .", "In the medium ranges , higher coverage and lower known PP compensate each other , optimal total PPs were observed at target coverages 4 , 0008 , 000 .", "Adding ambiguous words results in a worse performance on lexicon words , yet improves overall performance , especially for high thresholds .", "For all further experiments we fixed the threshold in a way that partitioning 1 consisted of 5 , 000 words , so only half of the top 10 , 000 words are considered unambiguous .", "At this Fig 2 .", "Influence of threshold s on tagger performance cluster conditional tag perplexity PP as a function of target word coverage . oov is the fraction of non lexicon words .", "Overall results are presented in table 3 .", "The combined strategy TMA reaches the lowest PP for all languages .", "The morphology extension M always improves the OOV scores .", "Adding ambiguous words A hurts the lexicon performance , but largely reduces the OOV rate , which in turn leads to better overall performance .", "Combining both partitionings T does not always decrease the total PP a lot , but lowers the number of tags significantly .", "Finnish figures are generally worse than for the other languages , akin to higher baselines .", "The high OOV perplexities for English in experiment TM and TMA can be explained as follows The smaller the OOV rate gets , the more likely it is that the corresponding words were also OOV in the gold standard tagger .", "A remedy would be to evaluate on hand tagged data .", "Differences between languages are most obvious when comparing OMA and TM whereas for English it pays off much more to add ambiguous words than to merge the two partitionings , it is the other way around in the German and Finnish experiments .", "To wrap up all steps undertaken improve the performance , yet their influence's strength varies .", "As a flavour of our system's output , consider the example in table 4 that has been tagged by our English TMA model as in the introductory example , quot ; saw quot ; is disambiguated correctly .", "We compare our results to Freitag , 2004 , as most other works use different evaluation techniques that are only indirectly measuring what we try to optimize here .", "Unfortunately , Freitag 2004 does not provide a total PP score for his 200 tags .", "He experiments with an handtagged , clean English corpus we did not have access to the Penn Treebank .", "Freitag reports a PP for known words of 1 . 57 for the top 5 , 000 words 91 corpus coverage , baseline 1 at 23 . 6 , a PP for unknown words without morphological extension of 4 . 8 .", "Using morphological features the unknown PP score is lowered to 4 . 0 .", "When augmenting the lexicon with low frequency words via their distributional characteristics , a PP as low as 2 . 9 is obtained for the remaining 9 of tokens .", "His methodology , however , does not allow for class ambiguity in the lexicon , the low number of OOV words is handled by a Hidden Markov Model .", "We presented a graph based approach to unsupervised POS tagging .", "To our knowledge , this is the first attempt to leave the decision on tag granularity to the tagger .", "We supported the claim of language independence by validating the output of our system against supervised systems in three languages .", "The system is not very sensitive to parameter changes the number of feature words , the frequency cutoffs , the log likelihood threshold and all other parameters did not change overall performance considerably when altered in reasonable limits .", "In this way it was possbile to arrive at a one size fits all configuration that allows the parameter free unsupervised tagging of large corpora .", "To really judge the benefit of an unsupervised tagging system , it should be evaluated in an application based way .", "Ideally , the application should tell us the granularity of our tagger e . g . semantic class learners could greatly benefit from the high granular word sets arising in both of our partitionings , which we endeavoured to lump into a coarser tagset here ."], "summary_lines": ["Unsupervised Part-Of-Speech Tagging Employing Efficient Graph Clustering\n", "An unsupervised part-of-speech (POS) tagging system that relies on graph clustering methods is described.\n", "Unlike in current state-of-the-art approaches, the kind and number of different tags is generated by the method itself.\n", "We compute and merge two partitionings of word graphs: one based on context similarity of high frequency words, another on log-likelihood statistics for words of lower frequencies.\n", "Using the resulting word clusters as a lexicon, a Viterbi POS tagger is trained, which is refined by a morphological component.\n", "The approach is evaluated on three different languages by measuring agreement with existing taggers.\n", "We directly compare the tagger output to supervised taggers for English, German and Finnish via information-theoretic measures.\n", "We conceptualize a network of words that capture the word co-occurrence patterns.\n", "We cluster the most frequent 10,000 words using contexts formed from the most frequent 150-200 words.\n"]}
{"article_lines": ["A Hierarchical Phrase Based Model For Statistical Machine Translation", "We present a statistical phrase based translamodel that uses phrases that contain subphrases .", "The model is formally a synchronous context free grammar but is learned from a bitext without any syntactic information .", "Thus it can be seen as shift to the of syntaxtranslation systems without any lin In our experiments using BLEU as a metric , the hierarchical phrasebased model achieves a relative improvement of 7 . 5 over Pharaoh , a state of the art phrase based system .", "The alignment template translation model Och and Ney , 2004 and related phrase based models advanced the previous state of the art by moving from words to phrases as the basic unit of translation .", "Phrases , which can be any substring and not necessarily phrases in any syntactic theory , allow these models to learn local reorderings , translation of short idioms , or insertions and deletions that are sensitive to local context .", "They are thus a simple and powerful mechanism for machine translation .", "The basic phrase based model is an instance of the noisy channel approach Brown et al . , 1993 , 1 in which the translation of a French sentence f into an from position i to position j inclusive , and similarly for eji .", "English sentence e is modeled as The translation model P f e encodes e into f by the following steps Other phrase based models model the joint distribution P e , f Marcu and Wong , 2002 or made P e and P f e into features of a log linear model Och and Ney , 2002 .", "But the basic architecture of phrase segmentation or generation , phrase reordering , and phrase translation remains the same .", "Phrase based models can robustly perform translations that are localized to substrings that are common enough to have been observed in training .", "But Koehn et al . 2003 find that phrases longer than three words improve performance little , suggesting that data sparseness takes over for longer phrases .", "Above the phrase level , these models typically have a simple distortion model that reorders phrases independently of their content Och and Ney , 2004 ; Koehn et al . , 2003 , or not at all Zens and Ney , 2004 ; Kumar et al . , 2005 .", "But it is often desirable to capture translations whose scope is larger than a few consecutive words . de shaoshu guojia zhiyi that few countries one of Australia is one of the few countries that have diplomatic relations with North Korea If we count zhiyi , lit .", "of one , as a single token , then translating this sentence correctly into English requires reversing a sequence of five elements .", "When we run a phrase based system , Pharaoh Koehn et al . , 2003 ; Koehn , 2004a , on this sentence using the experimental setup described below , we get the following phrases with translations 4 Aozhou shi yu Bei Han you bangjiao 1 de shaoshu guojia zhiyi Australia is dipl . rels .", "1 with North Korea is one of the few countries where we have used subscripts to indicate the reordering of phrases .", "The phrase based model is able to order diplomatic . . . Korea correctly using phrase reordering and one . . . countries correctly using a phrase translation , but does not accomplish the necessary inversion of those two groups .", "A lexicalized phrase reordering model like that in use in ISI s system Och et al . , 2004 might be able to learn a better reordering , but simpler distortion models will probably not .", "We propose a solution to these problems that does not interfere with the strengths of the phrasebased approach , but rather capitalizes on them since phrases are good for learning reorderings of words , we can use them to learn reorderings of phrases as well .", "In order to do this we need hierarchical phrases that consist of both words and subphrases .", "For example , a hierarchical phrase pair that might help with the above example is 5 yu 1 you 2 , have 2 with 1 where 1 and 2 are placeholders for subphrases .", "This would capture the fact that Chinese PPs almost always modify VP on the left , whereas English PPs usually modify VP on the right .", "Because it generalizes over possible prepositional objects and direct objects , it acts both as a discontinuous phrase pair and as a phrase reordering rule .", "Thus it is considerably more powerful than a conventional phrase pair .", "Similarly , 6 1 de 2 , the 2 that 1 would capture the fact that Chinese relative clauses modify NPs on the left , whereas English relative clauses modify on the right ; and 7 1 zhiyi , one of 1 would render the construction zhiyi in English word order .", "These three rules , along with some conventional phrase pairs , suffice to translate the sentence correctly 8 Aozhou shi yu Bei Han 1 you bangjiao 2 de shaoshu guojia 3 zhiyi Australia is one of the few countries 3 that have dipl . rels .", "2 with North Korea 1 The system we describe below uses rules like this , and in fact is able to learn them automatically from a bitext without syntactic annotation .", "It translates the above example almost exactly as we have shown , the only error being that it omits the word that from 6 and therefore 8 .", "These hierarchical phrase pairs are formally productions of a synchronous context free grammar defined below .", "A move to synchronous CFG can be seen as a move towards syntax based MT ; however , we make a distinction here between formally syntax based and linguistically syntax based MT .", "A system like that of Yamada and Knight 2001 is both formally and linguistically syntax based formally because it uses synchronous CFG , linguistically because the structures it is defined over are on the English side informed by syntactic theory via the Penn Treebank .", "Our system is formally syntaxbased in that it uses synchronous CFG , but not necessarily linguistically syntax based , because it induces a grammar from a parallel text without relying on any linguistic annotations or assumptions ; the result sometimes resembles a syntactician s grammar but often does not .", "In this respect it resembles Wu s bilingual bracketer Wu , 1997 , but ours uses a different extraction method that allows more than one lexical item in a rule , in keeping with the phrasebased philosophy .", "Our extraction method is basically the same as that of Block 2000 , except we allow more than one nonterminal symbol in a rule , and use a more sophisticated probability model .", "In this paper we describe the design and implementation of our hierarchical phrase based model , and report on experiments that demonstrate that hierarchical phrases indeed improve translation .", "Our model is based on a weighted synchronous CFG Aho and Ullman , 1969 .", "In a synchronous CFG the elementary structures are rewrite rules with aligned pairs of right hand sides where X is a nonterminal , \u03b3 and \u03b1 are both strings of terminals and nonterminals , and is a one to one correspondence between nonterminal occurrences in \u03b3 and nonterminal occurrences in \u03b1 . Rewriting begins with a pair of linked start symbols .", "At each step , two coindexed nonterminals are rewritten using the two components of a single rule , such that none of the newly introduced symbols is linked to any symbols already present .", "Thus the hierarchical phrase pairs from our above example could be formalized in a synchronous CFG as where we have used boxed indices to indicate which occurrences of X are linked by .", "Note that we have used only a single nonterminal symbol X instead of assigning syntactic categories to phrases .", "In the grammar we extract from a bitext described below , all of our rules use only X , except for two special glue rules , which combine a sequence of Xs to form an S These give the model the option to build only partial translations using hierarchical phrases , and then combine them serially as in a standard phrase based model .", "For a partial example of a synchronous CFG derivation , see Figure 1 .", "Following Och and Ney 2002 , we depart from the traditional noisy channel approach and use a more general log linear model .", "The weight of each rule is where the \u03c6i are features defined on rules .", "For our experiments we used the following features , analogous to Pharaoh s default feature set model to learn a preference for longer or shorter derivations , analogous to Koehn s phrase penalty Koehn , 2003 .", "The exceptions to the above are the two glue rules , 13 , which has weight one , and 14 , which has weight 16 w S _ S 1 X 2 , S 1 X 2 exp \u03bbg the idea being that \u03bbg controls the model s preference for hierarchical phrases over serial combination of phrases .", "Let D be a derivation of the grammar , and let f D and e D be the French and English strings generated by D . Let us represent D as a set of triples r , i , j , each of which stands for an application of a grammar rule r to rewrite a nonterminal that spans f D ji on the French side . 3 Then the weight of D hS 1 , S 1 i hS 2 X 3 , S 2 X 3 i hS 4 X 5 X 3 , S 4 X 5 X 3 i hX 6 X 5 X 3 , X 6 X 5 X 3 i hAozhou X 5 X 3 , Australia X 5 X 3 i hAozhou shi X 3 , Australia is X 3 i hAozhou shi X 7 zhiyi , Australia is one of X 7 i hAozhou shi X 8 de X 9 zhiyi , Australia is one of the X 9 that X 8 i hAozhou shi yu X 1 you X 2 de X 9 zhiyi , Australia is one of the X 9 that have X 2 with X 1 i is the product of the weights of the rules used in the translation , multiplied by the following extra factors where plm is the language model , and exp \u03bbwp e , the word penalty , gives some control over the length of the English output .", "We have separated these factors out from the rule weights for notational convenience , but it is conceptually cleaner and necessary for polynomial time decoding to integrate them into the rule weights , so that the whole model is a weighted synchronous CFG .", "The word penalty is easy ; the language model is integrated by intersecting the English side CFG with the language model , which is a weighted finitestate automaton .", "The training process begins with a word aligned corpus a set of triples hf , e , i , where f is a French sentence , e is an English sentence , and is a manyto many binary relation between positions of f and positions of e . We obtain the word alignments using the method of Koehn et al . 2003 , which is based on that of Och and Ney 2004 .", "This involves running GIZA Och and Ney , 2000 on the corpus in both directions , and applying refinement rules the variant they designate final and to obtain a single many to many word alignment for each sentence .", "Then , following Och and others , we use heuristics to hypothesize a distribution of possible derivations of each training example , and then estimate the phrase translation parameters from the hypothesized distribution .", "To do this , we first identify initial phrase pairs using the same criterion as previous systems Och and Ney , 2004 ; Koehn et al . , 2003 Definition 1 .", "Given a word aligned sentence pair hf , e , i , a rule hfij , ej0 i0 i is an initial phrase pair of hf , e , i iff Next , we form all possible differences of phrase pairs Definition 2 .", "The set of rules of hf , e , i is the smallest set satisfying the following is a rule , where k is an index not used in r . The above scheme generates a very large number of rules , which is undesirable not only because it makes training and decoding very slow , but also because it creates spurious ambiguity a situation where the decoder produces many derivations that are distinct yet have the same model feature vectors and give the same translation .", "This can result in nbest lists with very few different translations or feature vectors , which is problematic for the algorithm we use to tune the feature weights .", "Therefore we filter our grammar according to the following principles , chosen to balance grammar size and performance on our development set which simplifies the decoder implementation .", "Moreover , we prohibit nonterminals that are adjacent on the French side , a major cause of spurious ambiguity .", "A rule must have at least one pair of aligned words , making translation decisions always based on some lexical evidence .", "Now we must hypothesize weights for all the derivations .", "Och s method gives equal weight to all the extracted phrase occurences .", "However , our method may extract many rules from a single initial phrase pair ; therefore we distribute weight equally among initial phrase pairs , but distribute that weight equally among the rules extracted from each .", "Treating this distribution as our observed data , we use relativefrequency estimation to obtain P y \u03b1 and P \u03b1 y .", "Our decoder is a CKY parser with beam search together with a postprocessor for mapping French derivations to English derivations .", "Given a French sentence f , it finds the best derivation or n best derivations , with little overhead that generates f , e for some e . Note that we find the English yield of the highest probability single derivation and not necessarily the highest probability e , which would require a more expensive summation over derivations .", "We prune the search space in several ways .", "First , an item that has a score worse than 3 times the best score in the same cell is discarded ; second , an item that is worse than the bth best item in the same cell is discarded .", "Each cell contains all the items standing for X spanning fi .", "We choose b and 3 to balance speed and performance on our development set .", "For our experiments , we set b 40 , 3 10 1 for X cells , and b 15 , 3 10 1 for S cells .", "We also prune rules that have the same French side b 100 .", "The parser only operates on the French side grammar ; the English side grammar affects parsing only by increasing the effective grammar size , because there may be multiple rules with the same French side but different English sides , and also because intersecting the language model with the English side grammar introduces many states into the nonterminal alphabet , which are projected over to the French side .", "Thus , our decoder s search space is many times larger than a monolingual parser s would be .", "To reduce this effect , we apply the following heuristic when filling a cell if an item falls outside the beam , then any item that would be generated using a lowerscoring rule or a lower scoring antecedent item is also assumed to fall outside the beam .", "This heuristic greatly increases decoding speed , at the cost of some search errors .", "Finally , the decoder has a constraint that prohibits any X from spanning a substring longer than 10 on the French side , corresponding to the maximum length constraint on initial rules during training .", "This makes the decoding algorithm asymptotically linear time .", "The decoder is implemented in Python , an interpreted language , with C code from the SRI Language Modeling Toolkit Stolcke , 2002 .", "Using the settings described above , on a 2 . 4 GHz Pentium IV , it takes about 20 seconds to translate each sentence average length about 30 .", "This is faster than our Python implementation of a standard phrase based decoder , so we expect that a future optimized implementation of the hierarchical decoder will run at a speed competitive with other phrase based systems .", "Our experiments were on Mandarin to English translation .", "We compared a baseline system , the state of the art phrase based system Pharaoh Koehn et al . , 2003 ; Koehn , 2004a , against our system .", "For all three systems we trained the translation model on the FBIS corpus 7 . 2M 9 . 2M words ; for the language model , we used the SRI Language Modeling Toolkit to train a trigram model with modified Kneser Ney smoothing Chen and Goodman , 1998 on 155M words of English newswire text , mostly from the Xinhua portion of the Gigaword corpus .", "We used the 2002 NIST MT evaluation test set as our development set , and the 2003 test set as our test set .", "Our evaluation metric was BLEU Papineni et al . , 2002 , as calculated by the NIST script version 11a with its default settings , which is to perform case insensitive matching of n grams up to n 4 , and to use the shortest as opposed to nearest reference sentence for the brevity penalty .", "The results of the experiments are summarized in Table 1 .", "The baseline system we used for comparison was Pharaoh Koehn et al . , 2003 ; Koehn , 2004a , as publicly distributed .", "We used the default feature set language model same as above , p f e , p e f , lexical weighting both directions , distortion model , word penalty , and phrase penalty .", "We ran the trainer with its default settings maximum phrase length 7 , and then used Koehn s implementation of minimumerror rate training Och , 2003 to tune the feature weights to maximize the system s BLEU score on our development set , yielding the values shown in Table 2 .", "Finally , we ran the decoder on the test set , pruning the phrase table with b 100 , pruning the chart with b 100 , 3 10 5 , and limiting distortions to 4 .", "These are the default settings , except for the phrase table s b , which was raised from 20 , and the distortion limit .", "Both of these changes , made by Koehn s minimum error rate trainer by default , improve performance on the development set .", "We ran the training process of Section 3 on the same data , obtaining a grammar of 24M rules .", "When filtered for the development set , the grammar has 2 . 2M rules see Figure 2 for examples .", "We then ran the minimum error rate trainer with our decoder to tune the feature weights , yielding the values shown in Table 2 .", "Note that Ag penalizes the glue rule much less than App does ordinary rules .", "This suggests that the model will prefer serial combination of phrases , unless some other factor supports the use of hierarchical phrases e . g . , a better language model score .", "We then tested our system , using the settings described above . 4 Our system achieves an absolute improvement of 0 . 02 over the baseline 7 . 5 relative , without using any additional training data .", "This difference is statistically significant p 0 . 01 . 5 See Table 1 , which also shows that the relative gain is higher for higher n grams .", "4Note that we gave Pharaoh wider beam settings than we used on our own decoder ; on the other hand , since our decoder s chart has more cells , its b limits do not need to be as high .", "The use of hierarchical structures opens the possibility of making the model sensitive to syntactic structure .", "Koehn et al . 2003 mention German es gibt , there is as an example of a good phrase pair which is not a syntactic phrase pair , and report that favoring syntactic phrases does not improve accuracy .", "But in our model , the rule would indeed respect syntactic phrases , because it builds a pair of Ss out of a pair of NPs .", "Thus , favoring subtrees in our model that are syntactic phrases might provide a fairer way of testing the hypothesis that syntactic phrases are better phrases .", "This feature adds a factor to 17 , 1 if fij is a constituent 0 otherwise as determined by a statistical tree substitutiongrammar parser Bikel and Chiang , 2000 , trained on the Penn Chinese Treebank , version 3 250k words .", "Note that the parser was run only on the test data and not the much larger training data .", "Rerunning the minimum error rate trainer with the new feature yielded the feature weights shown in Table 2 .", "Although the feature improved accuracy on the development set from 0 . 314 to 0 . 322 , it gave no statistically significant improvement on the test set .", "Hierarchical phrase pairs , which can be learned without any syntactically annotated training data , improve translation accuracy significantly compared with a state of the art phrase based system .", "They also facilitate the incorporation of syntactic information , which , however , did not provide a statistically significant gain .", "Our primary goal for the future is to move towards a more syntactically motivated grammar , whether by automatic methods to induce syntactic categories , or by better integration of parsers trained on annotated data .", "This would potentially improve both accuracy and efficiency .", "Moreover , reducing the grammar size would allow more ambitious training settings .", "The maximum initial phrase length is currently 10 ; preliminary experiments show that increasing this limit to as high as 15 does improve accuracy , but requires more memory .", "On the other hand , we have successfully trained on almost 30M 30M words by tightening the initial phrase length limit for part of the data .", "Streamlining the grammar would allow further experimentation in these directions .", "In any case , future improvements to this system will maintain the design philosophy proven here , that ideas from syntax should be incorporated into statistical translation , but not in exchange for the strengths of the phrase based approach .", "I would like to thank Philipp Koehn for the use of the Pharaoh software ; and Adam Lopez , Michael Subotin , Nitin Madnani , Christof Monz , Liang Huang , and Philip Resnik .", "This work was partially supported by ONR MURI contract FCPO . 810548265 and Department of Defense contract RD 02 5700 .", "S . D . G ."], "summary_lines": ["A Hierarchical Phrase-Based Model For Statistical Machine Translation\n", "We present a statistical phrase-based translation model that uses hierarchical phrases - phrases that contain subphrases.\n", "The model is formally a synchronous context-free grammar but is learned from a bitext without any syntactic information.\n", "Thus it can be seen as a shift to the formal machinery of syntax-based translation systems without any linguistic commitment.\n", "In our experiments using BLEU as a metric, the hierarchical phrase-based model achieves a relative improvement of 7.5% over Pharaoh, a state-of-the-art phrase-based system.\n", "We use the k-best parsing algorithm in a CFG-based log-linear translation model in order to learn feature weights which maximize BLEU.\n", "We note that whenever we combine two dynamic programming items, we need to score the fluency of their concatentation by incorporating the score of any language model features which cross the target side boundaries of the two concatenated items.\n", "To better leverage syntactic constraint yet still allow non-syntactic translations, we introduce a count for each hypothesis and accumulates it whenever the hypothesis exactly matches syntactic boundaries on the source side.\n", "Our hierarchical phrase models for machine translation is an evolution from the traditional word (Brown et al, 1993) and phrase based (Koehn et al, 2003a) models.\n"]}
{"article_lines": ["Pseudo Projectivity A Polynomially Parsable Non Projective Dependency Grammar", "Dependency grammar has a long tradition in syntactic theory , dating back to at least Tesniere's work from the thirties . '", "Recently , it has gained renewed attention as empirical methods in parsing are discovering the importance of relations between words see , e . g . , Collins , 1997 , which is what dependency grammars model explicitly do , but context free phrasestructure grammars do not .", "One problem that has posed an impediment to more wide spread acceptance of dependency grammars is the fact that there is no computationally tractable version of dependency grammar which is not restricted to projective analyses .", "However , it is well known that there are some syntactic phenomena such as wh movement in English or clitic climbing in Romance that require nonprojective analyses .", "In this paper , we present a form of projectivity which we call pseudoprojectivity , and we present a generative stringrewriting formalism that can generate pseudoprojective analyses and which is polynomially parsable .", "The paper is structured as follows .", "In Section 2 , we introduce our notion of pseudoprojectivity .", "We briefly review a previously proposed formalization of projective dependency grammars in Section 3 .", "In Section 4 , we extend this formalism to handle pseudo projectivity .", "We informally present a parser in Section 5 .", "We will use the following terminology and notation in this paper .", "The hierarchical order dominance between the nodes of a tree T will be represented with the symbol T and Whenever they are unambiguous , the notations and will be used .", "When x y , we will say that x is a descendent of y and y an ancestor of x .", "The projection of a node x , belonging to a tree T , is the set of the nodes y of T such that y x .", "An arc between two nodes y and x of a tree T , directed from y to x will be noted either y , x or The node x will be referred to as the dependent and y as the governor .", "The latter will be noted , when convenient , X T X when unambiguous .", "The notations t and x are unambiguous because a node x has at most one governor in a tree .", "As usual , an ordered tree is a tree enriched with a linear order over the set of its nodes .", "Finally , if 1 is an arc of an ordered tree T , then Supp 1 represents the support of 1 , i . e . the set of the nodes of T situated between the extremities of 1 , extremities included .", "We will say that the elements of Supp 1 are covered by 1 .", "The notion of projectivity was introduced by Lecerf , 1960 and has received several different definitions since then .", "The definition given here is borrowed from Marcus , 1965 and Robinson , 1970 Definition An arc t is projective if and only if for every y covered by y x .", "A tree T is projective if and only if every arc of T is projective A projective tree has been represented in Figure 1 .", "A projective dependency tree can be associated with a phrase structure tree whose constituents are the projections of the nodes of the dependency tree .", "Projectivity is therefore equivalent , in phrase structure markers , to continuity of constituent .", "The strong constraints introduced by the projectivity property on the relationship between hierarchical order and linear order allow us to describe word order of a projective dependency tree at a local level in order to describe the linear position of a node , it is sufficient to describe its position towards its governor and sister nodes .", "The domain of locality of the linear order rules is therefore limited to a subtree of depth equal to one .", "It can be noted that this domain of locality is equal to the domain of locality of sub categorization rules .", "Both rules can therefore be represented together as in Gaffman , 1965 or separately as will be proposed in 3 .", "Although most linguistic structures can be represented as projective trees , it is well known that projectivity is too strong a constraint for dependency trees , as shown by the example of Figure 2 , which includes a non projective arc marked with a star .", "Who do you think she invited ?", "The non projective structures found in linguistics represent a small subset of the potential non projective structures .", "We will define a property more exactly a family of properties , weaker than projectivity , called pseudo projectivity , which describes a subset of the set of ordered dependency trees , containing the non projective linguistic structures .", "In order to define pseudo projectivity , we introduce an operation on dependency trees called lifting .", "When applied to a tree , this operation leads to the creation of a second tree , a lift of the first one .", "An ordered tree T' is a lift of the ordered tree T if and only if T and T' have the same nodes in the same order and for every node X , X T TX T .", "We will say that the node x has been lifted from X T its syntactic governor to X Ti its linear governor .", "Recall that the linear position of a node in a projective tree can be defined relative to its governor and its sisters .", "In order to define the linear order in a non projective tree , we will use a projective lift of the tree .", "In this case , the position of a node can be defined only with regards to its governor and sisters in the lift , i . e . , its linear governor and sisters .", "Definition An ordered tree T is said pseudo projective if there exists a lift T' of tree T which is projective .", "If there is no restriction on the lifting , the previous definition is not very interesting since we can in fact take any non projective tree and lift all nodes to the root node and obtain a projective tree .", "We will therefore constrain the lifting by a set of rules , called lifting rules .", "Consider a set of syntactic categories .", "The following definitions make sense only for trees whose nodes are labeled with categories . '", "The lifting rules are of the following form LD , SG and LG are categories and w is a regular expression on the set of categories This rule says that a node of category LD can be lifted from its syntactic governor of category SG to its linear governor of category LG through a path consisting of nodes of category C1 , , Cn , where the string belongs to L w .", "Every set of lifting rules defines a particular property of pseudo projectivity by imposing particular constraints on the lifting .", "A 21t is possible to define pseudo projectivity purely structurally i . e . without referring to the labeling .", "For example , we can impose that each node x is lifted to the highest ancestor of x covered by t Nasr , 1996 .", "The resulting pseudo projectivity is a fairly weak extension to projectivity , which nevertheless covers major nonprojective linguistic structures .", "However , we do not pursue a purely structural definition of pseudo projectivity in this paper . linguistic example of lifting rule is given in Section 4 .", "The idea of building a projective tree by means of lifting appears in Kunze , 1968 and is used by Hudson , 1990 and Hudson , unpublished .", "This idea can also be compared to the notion of word order domain Reape , 1990 ; Broker and Neuhaus , 1997 , to the Slash feature of GPSG and HPSG , to the functional uncertainty of LFG , and to the Move a of GB theory .", "We informally define a projective Dependency Grammar as a string rewriting system3 by giving a set of categories such as N , V and Adv , 4 a set of distinguished start categories the root categories of well formed trees , a mapping from strings to categories , and two types of rules dependency rules which state hierarchical order dominance and LP rules which state linear order .", "The dependency rules are further subdivided into subcategorization rules or s rules and modification rules or m rules .", "Here are some sample s rules LP rules are represented as regular expressions actually , only a limited form of regular expressions associated with each category .", "We use the hash sign to denote the position of the governor head .", "For example 3We follow Gaifman , 1965 throughout this paper by modeling a dependency grammar with a string rewriting system .", "However , we will identify a derivation with its representation as a tree , and we will sometimes refer to symbols introduced in a rewrite step as quot ; dependent nodes quot ; .", "For a model of a DG based on tree rewriting in the spirit of Tree Adjoining Grammar Joshi et al . , 1975 , see Nasr , 1995 .", "We will call this system generative dependency grammar or GDG for short .", "Derivations in GDG are defined as follows .", "In a rewrite step , we choose a multiset of dependency rules i . e . , a set of instances of dependency rules which contains exactly one srule and zero or more m rules .", "The left hand side nonterminal is the same as that we want to rewrite .", "Call this multiset the rewrite multiset .", "In the rewriting operation , we introduce a multiset of new nonterminals and exactly one terminal symbol the head .", "The rewriting operation then must meet the following three conditions As an example , consider a grammar containing the three dependency rules di rule 2 , d2 rule 3 , and d3 rule 4 , as well as the LP rule pi rule 5 .", "In addition , we have some lexical mappings they are obvious from the example , and the start symbol is Vfinite , .", "A sample derivation is shown in Figure 3 , with the sentential form representation on top and the corresponding tree representation below .", "Using this kind of representation , we can derive a bottom up parser in the following straightforward manner . 5 Since syntactic and linear governors coincide , we can derive deterministic finite state machines which capture both the dependency and the LP rules for a given governor category .", "We will refer to these FSMs as rule FSMs , and if the governor is of category C , we will refer to a C rule FSM .", "In a rule FSM , the transitions are labeled by categories , and the transition corresponding to the governor labeled by its category and a special mark such as .", "This transition is called the quot ; head transition quot ; .", "The entries in the parse matrix M are of the form in , q , where in is a rule FSM and q a state of it , except for the entries in squares M i , i , 1 j n , which also contain category labels .", "Let WO wn be the input word .", "We initialize the parse matrix as follows .", "Let C be a category of word wi .", "First , we add C to M i , i .", "Then , we add to M i , i every pair 7n , q such that m is a rule FSM with a transition labeled C from a start state and q the state reached after that transition . 6 Embedded in the usual three loops on i , j , k , we add an entry mi , q to M i , j if m1 , qi is in M k , j , m2 , q2 is in M i , k 1 , q2 is a final state of m2 , m2 is a C rule FSM , and mi transitions from qi to q on C a non head transition .", "There is a special case for the head transitions in mi if k i 1 , C is in M i , i , mi is a Crule FSM , and there is a head transition from qi to q in ml , then we add mi , q to M i , j .", "The time complexity of the algorithm is 0 n3IGIQmax , where G is the number of ruleFSMs derived from the dependency and LP rules in the grammar and Qmax is the maximum number of states in any of the rule FSMs .", "Recall that in a pseudo projective tree , we make a distinction between a syntactic governor and a linear governor .", "A node can be quot ; lifted quot ; along a lifting path from being a dependent of its syntactic governor to being a dependent of its linear 'This type of parser has been proposed previously .", "See for example Lombardi , 1996 ; Eisner , 1996 , who also discuss Early style parsers for projective dependency grammars .", "'We can use pre computed top down prediction to limit the number of pairs added . governor , which must be an ancestor of the governor .", "In defining a formal rewriting system for pseudo projective trees , we will not attempt to model the quot ; lifting quot ; as a transformational step in the derivation .", "Rather , we will directly derive the quot ; lifted quot ; version of the tree , where a node is dependent of its linear governor .", "Thus , the derived structure resembles more a unistratal dependency representation like those used by Hudson , 1990 than the multistratal representations of , for example , Mel'euk , 1988 .", "However , from a formal point of view , the distinction is not significant .", "In order to capture pseudo projectivity , we will interpret rules of the form 2 for subcategorization of arguments by a head and 4 for selection of a head by an adjunct as introducing syntactic dependents which may lift to a higher linear governor .", "An LP rule of the form 5 orders all linear dependents of the linear governor , no matter whose syntactic dependents they are .", "In addition , we need a third type of rule , namely a lifting rule , or 1 rule see 2 . 3 .", "The 1 rule 1 can be rewrited on the following form This rule resembles normal dependency rules but instead of introducing syntactic dependents of a category , it introduces a lifted dependent .", "Besides introducing a linear dependent LD , a 1 rule should make sure that the syntactic governor of LD will be introduced at a later stage of the derivation , and prevent it to introduce LD as its syntactic dependent , otherwise non projective nodes would be introduced twice , a first time by their linear governor and a second time by their syntactic governor .", "This condition is represented in the rule by means of a constraint on the categories found along the lifting path .", "This condition , which we call the lifting condition , is represented by the regular expression LG w SG .", "The regular expression representing the lifting condition is enriched with a dot separating , on its left , the part of the lifting path which has already been introduced during the rewriting and on its right the part which is still to be introduced for the rewriting to be valid .", "The dot is an unperfect way of representing the current state in a finite state automaton equivalent to the regular expression .", "We can further notice that the lifting condition ends with a repetition of LD for reasons which will be made clear when discussing the rewriting process .", "A sentential form contains terminal strings and categories paired with a multiset of lifting conditions , called the lift multiset .", "The lift multiset associated to a category C contains 'transiting' lifting conditions introduced by ancestors of C and passing across C . Three cases must be distinguished when rewriting a category C and its lifting multiset LM LM contains a single lifting condition which dot is situated to its right LG w SG C . .", "In such a case , C must be rewritten by the empty string .", "The situation of the dot at the right of the lifting condition indicates that C has been introduced by its syntactic governor although it has already been introduced by its linear governor earlier in the rewriting process .", "This is the reason why C has been added at the end of the lifting condition . of the 1 rules used in the rewriting operation .", "The lifting conditions contained in the lift multiset of all the newly introduced dependents D should be compatible with D , with the dot advanced appropriately .", "In addition , we require that , when we rewrite a category as a terminal , the lift multiset is empty .", "Let us consider an example .", "Suppose we have have a grammar containing the dependency rules di rule 2 , d2 rule 3 , and d3 rule 4 ; the LP rule pi rule 5 and p2 This rule says that an objective wh noun with feature top which depends on a verb with no further restrictions the third V in the lifting path can raise to any verb that dominates its immediate governor as long as the raising paths contains only verb with feature bridge , i . e . , bridge verbs .", "A sample derivation is shown in Figure 4 , with the sentential form representation on top and the corresponding tree representation below .", "We start our derivation with the start symbol Klause and rewrite it using dependency rules d2 and d3 , and the lifting rule 1 which introduces an objective NP argument .", "The lifting condition of I is passed to the V dependent but the dot remains at the left of Vbridge because of the Kleene star .", "When we rewrite the embedded V , we choose to rewrite again with Klause , and the lifting condition is passed on to the next verb .", "This verb is a V .", "rans which requires a Nobj .", "The lifting condition is passed to Nobj and the dot is moved to the right of the regular expression , therefore Nobj is rewritten as the empty string .", "In this section , we show that pseudo projective dependency grammars as defined in Section 2 . 3 are polynomially parsable .", "We can extend the bottom up parser for GDG to a parser for PP GDG in the following manner .", "In PP GDG , syntactic and linear governors do not necessarily coincide , and we must keep track separately of linear precedence and of lifting i . e . , quot ; long distance quot ; syntactic dependence .", "The entries in the parse matrix M are of the form m , q , LM , where m is a rule FSM , q a state of m , and LM is a multiset of lifting conditions as defined in Section 4 .", "An entry m , q , LM in a square M i , j of the parse matrix means that the sub word wi wj of the entry can be analyzed by in up to state q i . e . , it matches the beginning of an LP rule , but that nodes corresponding to the lifting rules in LM are being lifted from the subtrees spanning wz wj .", "Put differently , in this bottomup view LM represents the set of nodes which have a syntactic governor in the subtree spanning ID , w3 and a lifting rule , but are still looking for a linear governor .", "Suppose we have an entry in the parse matrix M of the form m , q , L .", "As we traverse the Crule FSM m , we recognize one by one the linear dependents of a node of category C . Call this governor n . The action of adding a new entry to the parse matrix corresponds to adding a single new linear dependent to n . While we are working on the C rule FSM 771 and are not yet in a final state , we have not yet recognized n itself .", "Each new dependent 71' brings with it a multiset of nodes being lifted from the , subtree it is the root of .", "Call this multiset LM .", "The new entry will be m , q' , LMULM where q is the state that m transitions to when n' is recognized as the next linear dependent .", "When we have reached a final state q of the rule FSM m , we have recognized a complete subtree rooted in the new governor , 7 .", "Some of the dependent nodes of ? I will be both syntactic and linear dependents of n , and the others will be linear dependents of 7 , but lifted from a descendent of i .", "In addition , i may have syntactic dependents which are not realized as its own linear dependent and are lifted away .", "No other options are possible .", "Therefore , when we have reached the final state of a rule FSM , we must connect up all nodes and lifting conditions before we can proceed to put an entry in , q , L in the parse matrix .", "This involves these steps 1 .", "For every lifting condition in LM , we ensure that it is compatible with the category of 77 .", "This is done by moving the dot leftwards in accordance with the category of The dot is moved leftwards since we are doing bottom up recognition .", "The obvious special provisions deal with the Kleene star and optional elements .", "If the category matches a catgeory with Kleene start in the lifting condition , we do not move the dot .", "If the category matches a category which is to the left of an optional category , or to the left of category with Kleene star , then we can move the dot to the left of that category .", "If the dot cannot be placed in accordance with the category of ij , then no new entry is made in the parse matrix for n . 2 .", "We then choose a multiset of s , m , and 1rules whose left hand side is the category of n . For every dependent of n introduced by an 1 rule , the dependent must be compatible with an instance of a lifting condition in LM whose dot must be at its beginning , or seperated from the beginning by optional or categories only ; the lifting condition is then removed from L . 3 .", "If , after the above repositioning of the dot and the linking up of all linear dependents to lifting conditions , there are still lifting conditions in LM such that the dot is at the beginning of the lifting condition , then no new entry is made in the parse matrix for n . 4 .", "For every syntactic dependent of rh we determine if it is a linear dependent of n which has not yet been identified as lifted .", "For each syntactic dependents which is not also a linear dependent , we check whether there is an applicable lifting rule .", "If not , no entry is made in the parse matrix for 77 .", "If yes , we add the lifting rule to LM .", "This procedure determines a new multiset LM so we can add entry in , q , LM in the parse matrix .", "In fact , it may determine several possible new multisets , resulting in multiple new entries .", "The parse is complete if there is an entry m , qm , 0 in square M n , 1 of the parse matrix , where in is a C rule FSM for a start category and qm is a final state of M . If we keep backpointers at each step in the algorithm , we have a compact representation of the parse forest .", "The maximum number of entries in each square of the parse matrix is 0 GQ4 , where G is the number of rule FSMs corresponding to LP rules in the grammar , Q is the maximum number of states in any of the rule FSMs , and L is the maximum number of states that the lifting rules can be in i . e . , the number of lifting conditions in the grammar multiplied by the maximum number of dot positions of any lifting condition .", "Note that the exponent is a grammar constant , but this number can be rather small since the lifting rules are not lexicalized they are construction specific , not lexemespecific .", "The time complexity of the algorithm is therefore 0 GQn3 2ILI ."], "summary_lines": ["Pseudo-Projectivity A Polynomially Parsable Non-Projective Dependency Grammar\n", "The pseudo-projective grammar we propose can be parsed in polynomial time and captures non-local dependencies through a form of gap-threading, but the structures generated by the grammar are strictly projective.\n"]}
{"article_lines": ["Automatic sense prediction for implicit discourse relations in text", "We present a series of experiments on auidentifying the sense of imrelations , i . e . relations that are not marked with a discourse connective such as but or because .", "We work with a corpus of implicit relations present in newspaper text and report results on a test set that is representative of the naturally occurring distribution of senses .", "We use several linguistically informed features , including polarity tags , Levin verb classes , length of verb phrases , modality , context , and lexical features .", "In addition , we revisit past approaches using lexical pairs from unannotated text as features , explain some of their shortcomings and propose modifications .", "Our best combination of features outperforms the baseline from data intensive approaches by 4 for comparison and 16 for contingency .", "Implicit discourse relations abound in text and readers easily recover the sense of such relations during semantic interpretation .", "But automatic sense prediction for implicit relations is an outstanding challenge in discourse processing .", "Discourse relations , such as causal and contrast relations , are often marked by explicit discourse connectives also called cue words such as because or but .", "It is not uncommon , though , for a discourse relation to hold between two text spans without an explicit discourse connective , as the example below demonstrates 1 The 101 year old magazine has never had to woo advertisers with quite so much fervor before .", "because It largely rested on its hard to fault demographics .", "In this paper we address the problem of automatic sense prediction for discourse relations in newspaper text .", "For our experiments , we use the Penn Discourse Treebank , the largest existing corpus of discourse annotations for both implicit and explicit relations .", "Our work is also informed by the long tradition of data intensive methods that rely on huge amounts of unannotated text rather than on manually tagged corpora Marcu and Echihabi , 2001 ; Blair Goldensohn et al . , 2007 .", "In our analysis , we focus only on implicit discourse relations and clearly separate these from explicits .", "Explicit relations are easy to identify .", "The most general senses comparison , contingency , temporal and expansion can be disambiguated in explicit relations with 93 accuracy based solely on the discourse connective used to signal the relation Pitler et al . , 2008 .", "So reporting results on explicit and implicit relations separately will allow for clearer tracking of progress .", "In this paper we investigate the effectiveness of various features designed to capture lexical and semantic regularities for identifying the sense of implicit relations .", "Given two text spans , previous work has used the cross product of the words in the spans as features .", "We examine the most informative word pair features and find that they are not the semantically related pairs that researchers had hoped .", "We then introduce several other methods capturing the semantics of the spans polarity features , semantic classes , tense , etc . and evaluate their effectiveness .", "This is the first study which reports results on classifying naturally occurring implicit relations in text and uses the natural distribution of the various senses .", "Experiments on implicit and explicit relations Previous work has dealt with the prediction of discourse relation sense , but often for explicits and at the sentence level .", "Soricut and Marcu 2003 address the task of parsing discourse structures within the same sentence .", "They use the RST corpus Carlson et al . , 2001 , which contains 385 Wall Street Journal articles annotated following the Rhetorical Structure Theory Mann and Thompson , 1988 .", "Many of the useful features , syntax in particular , exploit the fact that both arguments of the connective are found in the same sentence .", "Such features would not be applicable to the analysis of implicit relations that occur intersententially .", "Wellner et al . 2006 used the GraphBank Wolf and Gibson , 2005 , which contains 105 Associated Press and 30 Wall Street Journal articles annotated with discourse relations .", "They achieve 81 accuracy in sense disambiguation on this corpus .", "However , GraphBank annotations do not differentiate between implicits and explicits , so it is difficult to verify success for implicit relations .", "Experiments on artificial implicits Marcu and Echihabi 2001 proposed a method for cheap acquisition of training data for discourse relation sense prediction .", "Their idea is to use unambiguous patterns such as Arg1 , but Arg2 . to create synthetic examples of implicit relations .", "They delete the connective and use Arg1 , Arg2 as an example of an implicit relation .", "The approach is tested using binary classification between relations on balanced data , a setting very different from that of any realistic application .", "For example , a question answering application that needs to identify causal relations i . e . as in Girju 2003 , must not only differentiate causal relations from comparison relations , but also from expansions , temporal relations , and possibly no relation at all .", "In addition , using equal numbers of examples of each type can be misleading because the distribution of relations is known to be skewed , with expansions occurring most frequently .", "Causal and comparison relations , which are most useful for applications , are less frequent .", "Because of this , the recall of the classification should be the primary metric of success , while the Marcu and Echihabi 2001 experiments report only accuracy .", "Later work Blair Goldensohn et al . , 2007 ; Sporleder and Lascarides , 2008 has discovered that the models learned do not perform as well on implicit relations as one might expect from the test accuracies on synthetic data .", "For our experiments , we use the Penn Discourse Treebank PDTB ; Prasad et al . , 2008 , the largest available annotated corpora of discourse relations .", "The PDTB contains discourse annotations over the same 2 , 312 Wall Street Journal WSJ articles as the Penn Treebank .", "For each explicit discourse connective such as but or so , annotators identified the two text spans between which the relation holds and the sense of the relation .", "The PDTB also provides information about local implicit relations .", "For each pair of adjacent sentences within the same paragraph , annotators selected the explicit discourse connective which best expressed the relation between the sentences and then assigned a sense to the relation .", "In Example 1 above , the annotators identified because as the most appropriate connective between the sentences , and then labeled the implicit discourse relation Contingency .", "In the PDTB , explicit and implicit relations are clearly distinguished , allowing us to concentrate solely on the implicit relations .", "As mentioned above , each implicit and explicit relation is annotated with a sense .", "The senses are arranged in a hierarchy , allowing for annotations as specific as Contingency . Cause . reason .", "In our experiments , we use only the top level of the sense annotations Comparison , Contingency , Expansion , and Temporal .", "Using just these four relations allows us to be theory neutral ; while different frameworks Hobbs , 1979 ; McKeown , 1985 ; Mann and Thompson , 1988 ; Knott and Sanders , 1998 ; Asher and Lascarides , 2003 include different relations of varying specificities , all of them include these four core relations , sometimes under different names .", "Each relation in the PDTB takes two arguments .", "Example 1 can be seen as the predicate Contingency which takes the two sentences as arguments .", "For implicits , the span in the first sentence is called Arg1 and the span in the following sentence is called Arg2 .", "Cross product of words Discourse connectives are the most reliable predictors of the semantic sense of the relation Marcu , 2000 ; Pitler et al . , 2008 .", "However , in the absence of explicit markers , the most easily accessible features are the words in the two text spans of the relation .", "Intuitively , one would expect that there is some relationship that holds between the words in the two arguments .", "Consider for example the following sentences The recent explosion of country funds mirrors the closedend fund mania of the 1920s , Mr .", "Foot says , when narrowly focused funds grew wildly popular .", "They fell into oblivion after the 1929 crash .", "The words popular and oblivion are almost antonyms , and one might hypothesize that their occurrence in the two text spans is what triggers the contrast relation between the sentences .", "Similarly , a pair of words such as rain , rot might be indicative of a causal relation .", "If this hypothesis is correct , pairs of words w1 , w2 such that w1 appears in the first sentence and w2 appears in the second sentence would be good features for identifying contrast relations .", "Indeed , word pairs form the basic feature of most previous work on classifying implicit relations Marcu and Echihabi , 2001 ; BlairGoldensohn et al . , 2007 ; Sporleder and Lascarides , 2008 or the simpler task of predicting which connective should be used to express a relation Lapata and Lascarides , 2004 .", "Semantic relations vs . function word pairs If the hypothesis for word pair triggers of discourse relations were true , the analysis of unambiguous relations can be used to discover pairs of words with causal or contrastive relations holding between them .", "Yet , feature analysis has not been performed in prior studies to establish or refute this possibility .", "At the same time , feature selection is always necessary for word pairs , which are numerous and lead to data sparsity problems .", "Here , we present a meta analysis of the feature selection work in three prior studies .", "One approach for reducing the number of features follows the hypothesis of semantic relations between words .", "Marcu and Echihabi 2001 considered only nouns , verbs and and other cue phrases in word pairs .", "They found that even with millions of training examples , prediction results using all words were superior to those based on only pairs of non function words .", "However , since the learning curve is steeper when function words were removed , they hypothesize that using only non function words will outperform using all words once enough training data is available .", "In a similar vein , Lapata and Lascarides 2004 used pairings of only verbs , nouns and adjectives for predicting which temporal connective is most suitable to express the relation between two given text spans .", "Verb pairs turned out to be one of the best features , but no useful information was obtained using nouns and adjectives .", "Blair Goldensohn et al . 2007 proposed several refinements of the word pair model .", "They show that i stemming , ii using a small fixed vocabulary size consisting of only the most frequent stems which would tend to be dominated by function words and iii a cutoff on the minimum frequency of a feature , all result in improved performance .", "They also report that filtering stopwords has a negative impact on the results .", "Given these findings , we expect that pairs of function words are informative features helpful in predicting discourse relation sense .", "In our work that we describe next , we use feature selection to investigate the word pairs in detail .", "For the analysis of word pair features , we use a large collection of automatically extracted explicit examples from the experiments in BlairGoldensohn et al . 2007 .", "The data , from now on referred to as TextRels , has explicit contrast and causal relations which were extracted from the English Gigaword Corpus Graff , 2003 which contains over four million newswire articles .", "The explicit cue phrase is removed from each example and the spans are treated as belonging to an implicit relation .", "Besides cause and contrast , the TextRels data include a no relation category which consists of sentences from the same text that are separated by at least three other sentences .", "To identify features useful for classifying comparison vs other relations , we chose a random sample of 5000 examples for Contrast and 5000 Other relations 2500 each of Cause and No relation .", "For the complete set of 10 , 000 examples , word pair features were computed .", "After removing word pairs that appear less than 5 times , the remaining features were ranked by information gain using the MALLET toolkit1 .", "Table 1 lists the word pairs with highest information gain for the Contrast vs . Other and Cause vs . Other classification tasks .", "All contain very frequent stop words , and interestingly for the Con1mallet . cs . umass . edu trast vs . Other task , most of the word pairs contain discourse connectives .", "This is certainly unexpected , given that word pairs were formed by deleting the discourse connectives from the sentences expressing Contrast .", "Word pairs containing but as one of their elements in fact signal the presence of a relation that is not Contrast .", "Consider the example shown below The government says it has reached most isolated townships by now , but because roads are blocked , getting anything but basic food supplies to people remains difficult .", "Following Marcu and Echihabi 2001 , the pair The government says it has reached most isolated townships by now , but and roads are blocked , getting anything but basic food supplies to people remains difficult . is created as an example of the Cause relation .", "Because of examples like this , but but is a very useful word pair feature indicating Cause , as the but would have been removed for the artifical Contrast examples .", "In fact , the top 17 features for classifying Contrast versus Other all contain the word but , and are indications that the relation is Other .", "These findings indicate an unexpected anomalous effect in the use of synthetic data .", "Since relations are created by removing connectives , if an unambiguous connective remains , its presence is a reliable indicator that the example should be classified as Other .", "Such features might work well and lead to high accuracy results in identifying synthetic implicit relations , but are unlikely to be useful in a realistic setting of actual implicits . the but s but the in of but for but but but in but was but it but to but that but the it and and the the in in to the of and a of said but they but of in in and in of s and Also note that the only two features predictive of the comparison class indicated by in Table 1 the it and to it , contain only function words rather than semantically related nonfunction words .", "This ranking explains the observations reported in Blair Goldensohn et al . 2007 where removing stopwords degraded classifier performance and why using only nouns , verbs or adjectives Marcu and Echihabi , 2001 ; Lapata and Lascarides , 2004 is not the best option2 .", "The contrast between the popular oblivion example we started with above can be analyzed in terms of lexical relations near antonyms , but also could be explained by different polarities of the two words popular is generally a positive word , while oblivion has negative connotations .", "While we agree that the actual words in the arguments are quite useful , we also define several higher level features corresponding to various semantic properties of the words .", "The words in the two text spans of a relation are taken from the gold standard annotations in the PDTB .", "Polarity Tags We define features that represent the sentiment of the words in the two spans .", "Each word s polarity was assigned according to its entry in the Multi perspective Question Answering Opinion Corpus Wilson et al . , 2005 .", "In this resource , each sentiment word is annotated as positive , negative , both , or neutral .", "We use the number of negated and non negated positive , negative , and neutral sentiment words in the two text spans as features .", "If a writer refers to something as nice in Arg1 , that counts towards the positive sentiment count Arg1Positive ; not nice would count towards Arg1NegatePositive .", "A sentiment word is negated if a word with a General Inquirer Stone et al . , 1966 Negate tag precedes it .", "We also have features for the cross products of these polarities between Arg1 and Arg2 .", "We expected that these features could help Comparison examples especially .", "Consider the following example wasn t a good one .", "The venture , formed in 1986 , was supposed to be Time s low cost , safe entry into women s magazines .", "The word good is annotated with positive polarity , however it is negated .", "Safe is tagged as having positive polarity , so this opposition could indicate the Comparison relation between the two sentences .", "Inquirer Tags To get at the meanings of the spans , we look up what semantic categories each word falls into according to the General Inquirer lexicon Stone et al . , 1966 .", "The General Inquirer has classes for positive and negative polarity , as well as more fine grained categories such as words related to virtue or vice .", "The Inquirer even contains a category called Comp that includes words that tend to indicate Comparison , such as optimal , other , supreme , or ultimate .", "Several of the categories are complementary Understatement versus Overstatement , Rise versus Fall , or Pleasure versus Pain .", "Pairs where one argument contains words that indicate Rise and the other argument indicates Fall might be good evidence for a Comparison relation .", "The benefit of using these tags instead of just the word pairs is that we see more observations for each semantic class than for any particular word , reducing the data sparsity problem .", "For example , the pair rose fell often indicates a Comparison relation when speaking about stocks .", "However , occasionally authors refer to stock prices as jumping rather than rising .", "Since both jump and rise are members of the Rise class , new jump examples can be classified using past rise examples .", "Development testing showed that including features for all words tags was not useful , so we include the Inquirer tags of only the verbs in the two arguments and their cross product .", "Just as for the polarity features , we include features for both each tag and its negation .", "Money Percent Num If two adjacent sentences both contain numbers , dollar amounts , or percentages , it is likely that a comparison relation might hold between the sentences .", "We included a feature for the count of numbers , percentages , and dollar amounts in Arg1 and Arg2 .", "We also included the number of times each combination of number percent dollar occurs in Arg1 and Arg2 .", "For example , if Arg1 mentions a percentage and Arg2 has two dollar amounts , the feature Arg1Percent Arg2Money would have a count of 2 .", "This feature is probably genre dependent .", "Numbers and percentages often appear in financial texts but would be less frequent in other genres .", "WSJ LM This feature represents the extent to which the words in the text spans are typical of each relation .", "For each sense , we created unigram and bigram language models over the implicit examples in the training set .", "We compute each example s probability according to each of these language models .", "The features are the ranks of the spans likelihoods according to the various language models .", "For example , if of the unigram models , the most likely relation to generate this example was Contingency , then the example would include the feature ContingencyUnigram1 .", "If the third most likely relation according to the bigram models was Expansion , then it would include the feature ExpansionBigram3 .", "Expl LM This feature ranks the text spans according to language models derived from the explicit examples in the TextRels corpus .", "However , the corpus contains only Cause , Contrast and Norelation , hence we expect the WSJ language models to be more helpful .", "Verbs These features include the number of pairs of verbs in Arg1 and Arg2 from the same verb class .", "Two verbs are from the same verb class if each of their highest Levin verb class Levin , 1993 levels in the LCS Database Dorr , 2001 are the same .", "The intuition behind this feature is that the more related the verbs , the more likely the relation is an Expansion .", "The verb features also include the average length of verb phrases in each argument , as well as the cross product of this feature for the two arguments .", "We hypothesized that verb chunks that contain more words , such as They are allowed to proceed often contain rationales afterwards signifying Contingency relations , while short verb phrases like They proceed might occur more often in Expansion or Temporal relations .", "Our final verb features were the part of speech tags gold standard from the Penn Treebank of the main verb .", "One would expect that Expansion would link sentences with the same tense , whereas Contingency and Temporal relations would contain verbs with different tenses .", "First Last , First3 The first and last words of a relation s arguments have been found to be particularly useful for predicting its sense Wellner et al . , 2006 .", "Wellner et al . 2006 suggest that these words are such predictive features because they are often explicit discourse connectives .", "In our experiments on implicits , the first and last words are not connectives .", "However , some implicits have been found to be related by connective like expressions which often appear in the beginning of the second argument .", "In the PDTB , these are annotated as alternatively lexicalized relations AltLexes .", "To capture such effects , we included the first and last words of Arg1 as features , the first and last words of Arg2 , the pair of the first words of Arg1 and Arg2 , and the pair of the last words .", "We also add two additional features which indicate the first three words of each argument .", "Modality Modal words , such as can , should , and may , are often used to express conditional statements i . e .", "If I were a wealthy man , I wouldn t have to work hard . thus signaling a Contingency relation .", "We include a feature for the presence or absence of modals in Arg1 and Arg2 , features for specific modal words , and their cross products .", "Context Some implicit relations appear immediately before or immediately after certain explicit relations far more often than one would expect due to chance Pitler et al . , 2008 .", "We define a feature indicating if the immediately preceding or following relation was an explicit .", "If it was , we include the connective trigger of the relation and its sense as features .", "We use oracle annotations of the connective sense , however , most of the connectives are unambiguous .", "One might expect a different distribution of relation types in the beginning versus further in the middle of a paragraph .", "We capture paragraphposition information using a feature which indicates if Arg1 begins a paragraph .", "Word pairs Four variants of word pair models were used in our experiments .", "All the models were eventually tested on implicit examples from the PDTB , but the training set up was varied .", "Wordpairs TextRels In this setting , we trained a model on word pairs derived from unannotated text TextRels corpus .", "Wordpairs PDTBImpl Word pairs for training were formed from the cross product of words in the textual spans Arg1 x Arg2 of the PDTB implicit relations .", "Wordpairs selected Here , only word pairs from Wordpairs PDTBImpl with non zero information gain on the TextRels corpus were retained .", "Wordpairs PDTBExpl In this case , the model was formed by using the word pairs from the explicit relations in the sections of the PDTB used for training .", "For all experiments , we used sections 2 20 of the PDTB for training and sections 21 22 for testing .", "Sections 0 1 were used as a development set for feature design .", "We ran four binary classification tasks to identify each of the main relations from the rest .", "As each of the relations besides Expansion are infrequent , we train using equal numbers of positive and negative examples of the target relation .", "The negative examples were chosen at random .", "We used all of sections 21 and 22 for testing , so the test set is representative of the natural distribution .", "The training sets contained Comparison 1927 positive , 1927 negative , Contingency 3500 each , Expansion3 6356 each , and Temporal 730 each .", "The test set contained 151 examples of Comparison , 291 examples of Contingency , 986 examples of Expansion , 82 examples of Temporal , and 13 examples of No relation .", "We used Naive Bayes , Maximum Entropy MaxEnt , and AdaBoost Freund and Schapire , 1996 classifiers implemented in MALLET .", "The performance using only our semantically informed features is shown in Table 7 .", "Only the Naive Bayes classification results are given , as space is limited and MaxEnt and AdaBoost gave slightly lower accuracies overall .", "The table lists the f score for each of the target relations , with overall accuracy shown in brackets .", "Given that the experiments are run on natural distribution of the data , which are skewed towards Expansion relations , the f score is the more important measure to track .", "Our random baseline is the f score one would achieve by randomly assigning classes in proportion to its true distribution in the test set .", "The best results for all four tasks are considerably higher than random prediction , but still low overall .", "Our features provide 6 to 18 absolute improvements in f score over the baseline for each of the four tasks .", "The largest gain was in the Contingency versus Other prediction task .", "The least improvement was for distinguishing Expansion versus Other .", "However , since Expansion forms the largest class of relations , its f score is still the highest overall .", "We discuss the results per relation class next .", "Comparison We expected that polarity features would be especially helpful for identifying Comparison relations .", "Surprisingly , polarity was actually one of the worst classes of features for Comparison , achieving an f score of 16 . 33 in contrast to using the first , last and first three words of the sentences as features , which leads to an f score of 21 . 01 .", "We examined the prevalence of positivenegative or negative positive polarity pairs in our training set .", "30 of the Comparison examples contain one of these opposite polarity pairs , while 31 of the Other examples contain an opposite polarity pair .", "To our knowledge , this is the first study to examine the prevalence of polarity words in the arguments of discourse relations in their natural distributions .", "Contrary to popular belief , Comparisons do not tend to have more opposite polarity pairs .", "The two most useful classes of features for recognizing Comparison relations were the first , last and first three words in the sentence and the context features that indicate the presence of a paragraph boundary or of an explicit relation just before or just after the location of the hypothesized implicit relation 19 . 32 f score .", "Contingency The two best features for the Contingency vs . Other distinction were verb information 36 . 59 f score and first , last and first three words in the sentence 36 . 75 f score .", "Context again was one of the features that led to improvement .", "This makes sense , as Pitler et al . 2008 found that implicit contingencies are often found immediately following explicit comparisons .", "We were surprised that the polarity features were helpful for Contingency but not Comparison .", "Again we looked at the prevalence of opposite polarity pairs .", "While for Comparison versus Other there was not a significant difference , for Contingency there are quite a few more opposite polarity pairs 52 than for not Contingency 41 .", "The language model features were completely useless for distinguishing contingencies from other relations .", "Expansion As Expansion is the majority class in the natural distribution , recall is less of a problem than precision .", "The features that help achieve the best f score are all features that were found to be useful in identifying other relations .", "Polarity tags , Inquirer tags and context were the best features for identifying expansions with f scores around 70 .", "Temporal Implicit temporal relations are relatively rare , making up only about 5 of our test set .", "Most temporal relations are explicitly marked with a connective like when or after .", "Yet again , the first and last words of the sentence turned out to be useful indicators for temporal relations 15 . 93 f score .", "The importance of the first and last words for this distinction is clear .", "It derives from the fact that temporal implicits often contain words like yesterday or Monday at the end of the sentence .", "Context is the next most helpful feature for temporal relations .", "For Comparison and Contingency , we analyze the behavior of word pair features under several different settings .", "Specifically we want to address two important related questions raised in recent work by others i is unannotated data from explicits useful for training models that disambiguate implicit discourse relations and ii are explicit and implicit relations intrinsically different from each other .", "Wordpairs TextRels is the worst approach .", "The best use of word pair features is Wordpairsselected .", "This model gives 4 better absolute fscore for Comparison and 14 for Contingency over Wordpairs TextRels .", "In this setting the TextRels data was used to choose the word pair features , but the probabilities for each feature were estimated using the training portion of the PDTB implicit examples .", "We also confirm that even within the PDTB , information from annotated explicit relations Wordpairs PDTBExpl is not as helpful as information from annotated implicit relations Wordpairs PDTBImpl .", "The absolute difference in f score between the two models is close to 2 for Comparison , and 6 for Contingency .", "Adding other features to word pairs leads to improved performance for Contingency , Expansion and Temporal relations , but not for Comparison .", "For contingency detection , the best combination of our features included polarity , verb information , first and last words , modality , context with Wordpairs selected .", "This combination led to a definite improvement , reaching an f score of 47 . 13 16 absolute improvement in f score over Wordpairs TextRels .", "For detecting expansions , the best combination of our features polarity Inquirer tags context outperformed Wordpairs PDTBImpl by a wide margin , close to 13 absolute improvement fscores of 76 . 42 and 63 . 84 respectively .", "Our results from the previous section show that classification of implicits benefits from information about nearby relations , and so we expected improvements using a sequence model , rather than classifying each relation independently .", "We trained a CRF classifier Lafferty et al . , 2001 over the sequence of implicit examples from all documents in sections 02 to 20 .", "The test set is the same as used for the 2 way classifiers .", "We compare against a 6 way4 Naive Bayes classifier .", "Only word pairs were used as features for both .", "Overall 6 way prediction accuracy is 43 . 27 for the Naive Bayes model and 44 . 58 for the CRF model .", "We have presented the first study that predicts implicit discourse relations in a realistic setting distinguishing a relation of interest from all others , where the relations occur in their natural distributions .", "Also unlike prior work , we separate the task from the easier task of explicit discourse prediction .", "Our experiments demonstrate that features developed to capture word polarity , verb classes and orientation , as well as some lexical features are strong indicators of the type of discourse relation .", "We analyze word pair features used in prior work that were intended to capture such semantic oppositions .", "We show that the features in fact do not capture semantic relation but rather give information about function word co occurrences .", "However , they are still a useful source of information for discourse relation prediction .", "The most beneficial application of such features is when they are selected from a large unannotated corpus of explicit relations , but then trained on manually annotated implicit relations .", "Context , in terms of paragraph boundaries and nearby explicit relations , also proves to be useful for the prediction of implicit discourse relations .", "It is helpful when added as a feature in a standard , instance by instance learning model .", "A sequence model also leads to over 1 absolute improvement for the task .", "This work was partially supported by NSF grants IIS 0803159 , IIS 0705671 and IGERT 0504487 .", "We would like to thank Sasha Blair Goldensohn for providing us with the TextRels data and for the insightful discussion in the early stages of our work ."], "summary_lines": ["Automatic sense prediction for implicit discourse relations in text\n", "We present a series of experiments on automatically identifying the sense of implicit discourse relations, i.e. relations that are not marked with a discourse connective such as \u201cbut\u201d or \u201cbecause\u201d.\n", "We work with a corpus of implicit relations present in newspaper text and report results on a test set that is representative of the naturally occurring distribution of senses.\n", "We use several linguistically informed features, including polarity tags, Levin verb classes, length of verb phrases, modality, context, and lexical features.\n", "In addition, we revisit past approaches using lexical pairs from unannotated text as features, explain some of their shortcomings and propose modifications.\n", "Our best combination of features outperforms the baseline from data intensive approaches by 4% for comparison and 16% for contingency.\n", "Although on Comparison relation there is only as light improvement (+1.07%), our two best systems both got around 10% improvements of f score over a state-of-the-art system in (Pitler et al, 2009a).\n", "Our analysis shows that the top word pairs (ranked by information gain) all contain common functional words, and are not at all the semantically-related content words that were imagined.\n"]}
{"article_lines": ["Incremental Integer Linear Programming For Non Projective Dependency Parsing", "Integer Linear Programming has recently been used for decoding in a number of probabilistic models in order to enforce global constraints .", "However , in certain applications , such as non projective dependency parsing and machine translation , the complete formulation of the decoding problem as an integer linear program renders solving intractable .", "We present an approach which solves the problem incrementally , thus we avoid creating intractable integer linear programs .", "This approach is applied to Dutch dependency parsing and we show how the addition of linguistically motivated constraints can yield a significant improvement over stateof the art .", "Many inference algorithms require models to make strong assumptions of conditional independence between variables .", "For example , the Viterbi algorithm used for decoding in conditional random fields requires the model to be Markovian .", "Strong assumptions are also made in the case of McDonald et al . s 2005b non projective dependency parsing model .", "Here attachment decisions are made independently of one another' .", "However , often such assumptions can not be justified .", "For example in dependency parsing , if a subject has already been identified for a given verb , then the probability of attaching a second subject to the verb is zero .", "Similarly , if we find that one coordination argument is a noun , then the other argu'If we ignore the constraint that dependency trees must be cycle free see sections 2 and 3 for details . ment cannot be a verb .", "Thus decisions are often co dependent .", "Integer Linear Programming ILP has recently been applied to inference in sequential conditional random fields Roth and Yih , 2004 , this has allowed the use of truly global constraints during inference .", "However , it is not possible to use this approach directly for a complex task like non projective dependency parsing due to the exponential number of constraints required to prevent cycles occurring in the dependency graph .", "To model all these constraints explicitly would result in an ILP formulation too large to solve efficiently Williams , 2002 .", "A similar problem also occurs in an ILP formulation for machine translation which treats decoding as the Travelling Salesman Problem Germann et al . , 2001 .", "In this paper we present a method which extends the applicability of ILP to a more complex set of problems .", "Instead of adding all the constraints we wish to capture to the formulation , we first solve the program with a fraction of the constraints .", "The solution is then examined and , if required , additional constraints are added .", "This procedure is repeated until all constraints are satisfied .", "We apply this dependency parsing approach to Dutch due to the language s non projective nature , and take the parser of McDonald et al . 2005b as a starting point for our model .", "In the following section we introduce dependency parsing and review previous work .", "In Section 3 we present our model and formulate it as an ILP problem with a set of linguistically motivated constraints .", "We include details of an incremental algorithm used to solve this formulation .", "Our experimental set up is provided in Section 4 and is followed by results in Section 5 along with runtime experiments .", "We finally discuss future research and potential improvements to our approach .", "Dependency parsing is the task of attaching words to their arguments .", "Figure 1 shows a dependency graph for the Dutch sentence I ll come at twelve and then you ll get what you deserve taken from the Alpino Corpus van der Beek et al . , 2002 .", "In this dependency graph the verb kom is attached to its subject ik .", "kom is referred to as the head of the dependency and ik as its child .", "In labelled dependency parsing edges between words are labelled with the relation captured .", "In the case of the dependency between kom and ik the label would be subject .", "In a dependency tree every token must be the child of exactly one other node , either another token or the dummy root node .", "By definition , a dependency tree is free of cycles .", "For example , it must not contain dependency chains such as en __ kom __ ik en .", "For a more formal definition see previous work Nivre et al . , 2004 .", "An important distinction between dependency trees is whether they are projective or nonprojective .", "Figure 1 is an example of a projective dependency tree , in such trees dependencies do not cross .", "In Dutch and other flexible word order languages such as German and Czech we also encounter non projective trees , in these cases the trees contain crossing dependencies .", "Dependency parsing is useful for applications such as relation extraction Culotta and Sorensen , 2004 and machine translation Ding and Palmer , 2005 .", "Although less informative than lexicalised phrase structures , dependency structures still capture most of the predicate argument information needed for applications .", "It has the advantage of being more efficient to learn and parse .", "McDonald et al . 2005a introduce a dependency parsing framework which treats the task as searching for the projective tree that maximises the sum of local dependency scores .", "This frameFigure 2 An incorrect partial dependency tree .", "The verb krijg is incorrectly coordinated with the preposition om . work is efficient and has also been extended to non projective trees McDonald et al . , 2005b .", "It provides a discriminative online learning algorithm which when combined with a rich feature set reaches state of the art performance across multiple languages .", "However , within this framework one can only define features over single attachment decisions .", "This leads to cases where basic linguistic constraints are not satisfied e . g . verbs with two subjects or incompatible coordination arguments .", "An example of this for Dutch is illustrated in Figure 2 which was produced by the parser of McDonald et al . 2005b .", "Here the parse contains a coordination of incompatible word classes a preposition and a verb .", "Our approach is able to include additional constraints which forbid configurations such as those in Figure 2 .", "While McDonald and Pereira 2006 address the issue of local attachment decisions by defining scores over attachment pairs , our solution is more general .", "Furthermore , it is complementary in the sense that we could formulate their model using ILP and then add constraints .", "The method we present is not the only one that can take global constraints into account .", "Deterministic dependency parsing Nivre et al . , 2004 ; Yamada and Matsumoto , 2003 can apply global constraints by conditioning attachment decisions on the intermediate parse built .", "However , for efficiency a greedy search is used which may produce sub optimal solutions .", "This is not the case when using ILP .", "Our underlying model is a modified labelled version2 of McDonald et al . 2005b where x is a sentence , y is a set of labelled dependencies , f i , j , l is a multidimensional feature vector representation of the edge from token i to token j with label l and w the corresponding weight vector .", "For example , a feature f101 in f could be where t i is the word at token i and p j the partof speech tag at token j . Decoding in this model amounts to finding the y for a given x that maximises s x , y while fulfilling the following constraints T1 For every non root token in x there exists exactly one head ; the root token has no head .", "T2 There are no cycles .", "Thus far , the formulation follows McDonald et al . 2005b and corresponds to the Maximum Spanning Tree MST problem .", "In addition to T1 and T2 , we include a set of linguistically motivated constraints A1 Heads are not allowed to have more than one outgoing edge labelled l for all l in a set of labels U . C1 In a symmetric coordination there is exactly one argument to the right of the conjunction and at least one argument to the left .", "C2 In an asymmetric coordination there are no arguments to the left of the conjunction and at least two arguments to the right .", "C3 There must be at least one comma between subsequent arguments to the left of a symmetric coordination .", "C4 Arguments of a coordination must have compatible word classes .", "P1 Two dependencies must not cross if one of their labels is in a set of labels P . A1 covers constraints such as there can only be one subject if U contains subject see Section 4 . 4 for more details of U .", "C1 applies to configurations which contain conjunctions such as en , of or maar and , or and but .", "C2 will rule out settings where a conjunction such as zowel translates as both having arguments to its left .", "C3 forces coordination arguments to the left of a conjunction to have commas in between .", "C4 avoids parses in which incompatible word classes are coordinated , such as nouns and verbs .", "Finally , P1 allows selective projective parsing we can , for instance , forbid the crossing of Noun Determiner dependencies if we add the corresponding label type to P see Section 4 . 4 for more details of P .", "If we extend P to contain all labels we forbid any type of crossing dependencies .", "This corresponds to projective parsing .", "McDonald et al . 2005b use the Chu LiuEdmonds CLE algorithm to solve the maximum spanning tree problem .", "However , global constraints cannot be incorporated into the CLE algorithm McDonald et al . , 2005b .", "We alleviate this problem by presenting an equivalent Integer Linear Programming formulation which allows us to incorporate global constraints naturally .", "Before giving full details of our formulation we first introduce some of the concepts of linear and integer linear programming for a more thorough introduction see Winston and Venkataramanan 2003 .", "Linear Programming LP is a tool for solving optimisation problems in which the aim is to maximise or minimise a given linear function with respect to a set of linear constraints .", "The function to be maximised or minimised is referred to as the objective function .", "A number of decision variables are under our control which exert influence on the objective function .", "Specifically , they have to be optimised in order to maximise or minimise the objective function .", "Finally , a set of constraints restrict the values that the decision variables can take .", "Integer Linear Programming is an extension of linear programming where all decision variables must take integer values .", "There are several explicit formulations of the MST problem as an integer linear program in the literature Williams , 2002 .", "They are based on the concept of eliminating subtours cycles , cuts disconnections or requiring intervertex flows paths .", "However , in practice these formulations cause long solve times as the first two methAlgorithm 1 Incremental Integer Linear Programming ods yield an exponential number of constraints .", "Although the latter scales cubically , it produces non fractional solutions in its relaxed version ; this causes long runtimes for the branch and bound algorithm Williams , 2002 commonly used in integer linear programming .", "We found out experimentally that dependency parsing models of this form do not converge on a solution after multiple hours of solving , even for small sentences .", "As a workaround for this problem we follow an incremental approach akin to the work of Warme 1998 .", "Instead of adding constraints which forbid all possible cycles in advance this would result in an exponential number of constraints we first solve the problem without any cycle constraints .", "The solution is then examined for cycles , and if cycles are found we add constraints to forbid these cycles ; the solver is then run again .", "This process is repeated until no more violated constraints are found .", "The same procedure is used for other types of constraints which are too expensive to add in advance e . g . the constraints of P1 .", "Algorithm 1 outlines our approach .", "For a sentence x , Bx is the set of constraints that we add in advance and Ix are the constraints we add incrementally .", "Ox is the objective function and Vx is a set of variables including integer declarations . solve C , O , V maximises the objective function O with respect to the set of constraints C and variables V . violated y , I inspects the proposed solution y and returns all constraints in I which are violated .", "The number of iterations required in this approach is at most polynomial with respect to the number of variables Gr otschel et al . , 1981 .", "In practice , this technique converges quickly less than 20 iterations in 99 of approximately 12 , 000 sentences , yielding average solve times of less than 0 . 5 seconds .", "Our approach converges quickly due to the quality of the scoring function .", "Its weights have been trained on cycle free data , thus it is more likely to guide the search to a cycle free solution .", "In the following section we present the objective function Ox , variables Vx and linear constraints Bx and Ix needed for parsing x using Algorithm 1 .", "Vx contains a set of binary variables to represent labelled edges where n is the number of tokens and the index 0 represents the root token . bestk i , j is the set of k labels with highest s i , j , l . ei , j , l equals 1 if there is a edge i . e . , a dependency with the label l between token i head and j child , 0 otherwise . k depends on the type of constraints we want to add .", "For the plain MST problem it is sufficient to set k 1 and only take the best scoring label for each token pair .", "However , if we want a constraint which forbids duplicate subjects we need to provide additional labels to fall back on .", "Vx also contains a set of binary auxiliary variables which represent the existence of a dependency between tokens i and j .", "We connect these to the ei , j , l variables by the constraint Given the above variables our objective function Ox can be represented as using a suitable k We first introduce a set of base constraints Bx which we add in advance .", "Only One Head T1 Every token has exactly one head for non root tokens j 0 in x .", "An exception is made for the artificial root node Label Uniqueness A1 To enforce uniqueness of children with labels in U we augment our model with the constraint for every token i in x and label l in U . Symmetric Coordination C1 For each conjunction token i which forms a symmetric coordination we add and Asymmetric Coordination C2 For each conjunction token i which forms an asymmetric coordination we add and", "Now we present the set of constraints IX we add incrementally .", "The constraints are chosen based on the two criteria 1 adding them to the base constraints those added in advance would result in an extremely large program , and 2 it must be efficient to detect whether the constraint is violated in y .", "No Cycles T2 For every possible cycle c for the sentence x we have a constraint which forbids the case where all edges in c are active simultaneously Comma Coordination C3 For each symmetric conjunction token i which forms a symmetric coordination and each set of tokens A in x to the left of i with no comma between each pair of successive tokens we add which forbids configurations where i has the argument tokens A .", "Compatible Coordination Arguments C4 For each conjunction token i and each set of tokens A in x with incompatible POS tags , we add a constraint to forbid configurations where i has the argument tokens A .", "Selective Projective Parsing P1 For each pair of triplets i , j , l1 and m , n , l2 we add the constraint For training we use single best MIRA McDonald et al . , 2005a .", "This is an online algorithm that learns by parsing each sentence and comparing the result with a gold standard .", "Training is complete after multiple passes through the whole corpus .", "Thus we decode using the Chu Liu Edmonds CLE algorithm due to its speed advantage over ILP see Section 5 . 2 for a detailed comparison of runtimes .", "The fact that we decode differently during training using CLE and testing using ILP may degrade performance .", "In the presence of additional constraints weights may be able to capture other aspects of the data .", "Our experiments were designed to answer the following questions Before we try to answer these questions we briefly describe our data , features used , settings for U and P in our parametric constraints , our working environment and our implementation .", "We use the Alpino treebank van der Beek et al . , 2002 , taken from the CoNLL shared task of multilingual dependency parsing3 .", "The CoNLL data differs slightly from the original Alpino treebank as the corpus has been part of speech tagged using a Memory Based Tagger Daelemans et al . , 1996 .", "It consists of 13 , 300 sentences with an average length of 14 . 6 tokens .", "The data is non projective , more specifically 5 . 4 of all dependencies are crossed by at least one other dependency .", "It contains approximately 6000 sentences more than the Alpino corpus used by Malouf and van Noord 2004 .", "The training set was divided into a 10 development set dev while the remaining 90 is used as a training and cross validation set cross .", "Feature sets , constraints and training parameters were selected through training on cross and optimising against dev .", "Our final accuracy scores and runtime evaluations were acquired using a nine fold crossvalidation on cross All our experiments were conducted on a Intel Xeon with 3 . 8 Ghz and 4Gb of RAM .", "We used the open source Mixed Integer Programming library lp solve4 to solve the Integer Linear Programs .", "Our code ran in Java and called the JNIwrapper around the lp solve library .", "Our feature set was determined through experimentation with the development set .", "The features are based upon the data provided within the Alpino treebank .", "Along with POS tags the corpus contains several additional attributes such as gender , number and case .", "Our best results on the development set were achieved using the feature set of McDonald et al . 2005a and a set of features based on the additional attributes .", "These features combine the attributes of the head with those of the child .", "For example , if token i has the attributes a1 and a2 , and token j has the attribute a3 then we created the features a1 a3 and a2 a3 .", "All the constraints presented in Section 3 were used in our model .", "The set U of unique labels constraints contained su , obj1 , obj2 , sup , ld , vc , predc , predm , pc , pobj1 , obcomp and body .", "Here su stands for subject and obj1 for direct object for full details see Moortgat et al . 2000 .", "The set of projective labels P contained cnj , for coordination dependencies ; and det , for determiner dependencies .", "One exception was added for the coordination constraint dependencies can cross when coordinated arguments are verbs .", "One drawback of hard deterministic constraints is the undesirable effect noisy data can cause .", "We see this most prominently with coordination argument compatibility .", "Words ending in en are typically wrongly tagged and cause our coordination argument constraint to discard correct coordinations .", "As a workaround we assigned words ending in en a wildcard POS tag which is compatible with all POS tags .", "In this section we report our results .", "We not only present our accuracy but also provide an empirical evaluation of the runtime behaviour of this approach and show how parsing can be accelerated using a simple approximation .", "An important question to answer when using global constraints is How much of a performance boost is gained when using global constraints ?", "We ran the system without any linguistic constraints as a baseline bl and compared it to a system with the additional constraints cnstr .", "To evaluate our systems we use the accuracy over labelled attachment decisions where Nl is the number of tokens with correct head and label and Nt is the total number of tokens .", "For completeness we also report the unlabelled accuracy where Nu is the number of tokens with correct head . curacy using nine fold cross validation on cross for baseline bl and constraint based constr system .", "LC and UC are the percentages of sentences with 100 labelled and unlabelled accuracy , respectively .", "Table 1 shows our results using nine fold crossvalidation on the cross set .", "The baseline system no additional constraints gives an unlabelled accuracy of 84 . 6 and labelled accuracy of 88 . 9 .", "When we add our linguistic constraints the performance increases by 0 . 5 for both labelled and unlabelled accuracy .", "This increase is significant p 0 . 001 according to Dan Bikel s parse comparison script and using the Sign test p 0 . 001 .", "Now we give a little insight into how our results compare with the rest of the community .", "The reported state of the art parser of Malouf and van Noord 2004 achieves 84 . 4 labelled accuracy which is very close numerically to our baseline .", "However , they use a subset of the CoNLL Alpino treebank with a higher average number of tokens per sentences and also evaluate control relations , thus results are not directly comparable .", "We have also run our parser on the relatively small approximately 400 sentences CoNNL test data .", "The best performing system McDonald et al . 2006 ; note this system is different to our baseline achieves 79 . 2 labelled accuracy while our baseline system achieves 78 . 6 and our constrained version 79 . 8 .", "However , a significant difference is only observed between our baseline and our constraintbased system .", "Examining the errors produced using the dev set highlight two key reasons why we do not see a greater improvement using our constraint based system .", "Firstly , we cannot improve on coordinations that include words ending with en based on the workaround present in Section 4 . 4 .", "This problem can only be solved by improving POS taggers for Dutch or by performing POS tagging within the dependency parsing framework .", "Secondly , our system suffers from poor next best solutions .", "That is , if the best solution violates some constraints , then we find the next best solution is typically worse than the best solution with violated constraints .", "This appears to be a consequence of inaccurate local score distributions as opposed to inaccurate best local scores .", "For example , suppose we attach two subjects , t1 and t2 , to a verb , where t1 is the actual subject while t2 is meant to be labelled as object .", "If we forbid this configuration two subjects and if the score of labelling t1 object is higher than that for t2 being labelled subject , then the next best solution will label t1 incorrectly as object and t2 incorrectly as subject .", "This is often the case , and thus results in a drop of accuracy .", "We now concentrate on the runtime of our method .", "While we expect a longer runtime than using the Chu Liu Edmonds as in previous work McDonald et al . , 2005b , we are interested in how large the increase is .", "Table 2 shows the average solve time ST for sentences with respect to the number of tokens in each sentence for our system with constraints cnstr and the Chu Liu Edmonds CLE algorithm .", "All solve times do not include feature extraction as this is identical for all systems .", "For cnstr we also show the number of sentences that could not be parsed after two minutes , the average number of iterations and the average duration of the first iteration .", "The results show that parsing using our generic approach is still reasonably fast although significantly slower than using the Chu Liu Edmonds algorithm .", "Also , only a small number of sentences take longer than two minutes to parse .", "Thus , in practice we would not see a significant degradation in performance if we were to fall back on the CLE algorithm after two minutes of solving .", "When we examine the average duration of the first iteration it appears that the majority of the solve time is spent within this iteration .", "This could be used to justify using the CLE algorithm to find a initial solution as starting point for the ILP solver see Section 6 .", "Despite the fact that our parser can parse all sentences in a reasonable amount of time , it is still significantly slower than the CLE algorithm .", "While this is not crucial during decoding , it does make discriminative online training difficult as training requires several iterations of parsing the whole corpus . time ST for the cross dataset using varying q values and the Chu Liu Edmonds algorithm CLE Thus we investigate if it is possible to speed up our inference using a simple approximation .", "For each token we now only consider the q variables in VX with the highest scoring edges .", "For example , if we set q 2 the set of variables for a token j will contain two variables , either both for the same head i but with different labels variables ei , j , l1 and ei , j , l2 or two distinct heads i1 and i2 variables ei1 , j , l1 and ei2 , j , l2 where labels l1 and l2 may be identical .", "Table 3 shows the effect of different q values on solve time for the full corpus cross roughly 12 , 000 sentences and overall accuracy .", "We see that solve time can be reduced by 80 while only losing a marginal amount of accuracy when we set q to 10 .", "However , we are unable to reach the 20 seconds solve time of the CLE algorithm .", "Despite this , when we add the time for preprocessing and feature extraction , the CLE system parses a corpus in around 15 minutes whereas our system with q 10 takes approximately 25 minutes' .", "When we view the total runtime of each system we see our system is more competitive .", "While we have presented significant improvements using additional constraints , one may won5Even when caching feature extraction during training McDonald et al . 2005a still takes approximately 10 minutes to train . der whether the improvements are large enough to justify further research in this direction ; especially since McDonald and Pereira 2006 present an approximate algorithm which also makes more global decisions .", "However , we believe that our approach is complementary to their model .", "We can model higher order features by using an extended set of variables and a modified objective function .", "Although this is likely to increase runtime , it may still be fast enough for real world applications .", "In addition , it will allow exact inference , even in the case of non projective parsing .", "Also , we argue that this approach has potential for interesting extensions and applications .", "For example , during our runtime evaluations we find that a large fraction of solve time is spent in the first iteration of our incremental algorithm .", "After the first iteration the solver uses its last state to efficiently search for solutions in the presence of new constraints .", "Some solvers allow the specification of an initial solution as a starting point , thus it is expected that significant improvements in terms of speed can be made by using the CLE algorithm to provide an initial solution .", "Our approach uses a generic algorithm to solve a complex task .", "Thus other applications may benefit from it .", "For instance , Germann et al . 2001 present an ILP formulation of the Machine Translation MT decoding task in order to conduct exact inference .", "However , their model suffers from the same type of exponential blow up we observe when we add all our cycle constraints in advance .", "In fact , the constraints which cause the exponential explosion in their graphically formulation are of the same nature as our cycle constraints .", "We hope that the incremental approach will allow exact MT decoding for longer sentences .", "In this paper we have presented a novel approach for inference using ILP .", "While previous approaches which use ILP for decoding have solved each integer linear program in one run , we incrementally add constraints and solve the resulting program until no more constraints are violated .", "This allows us to efficiently use ILP for dependency parsing and add constraints which provide a significant improvement over the current stateof the art parser McDonald et al . , 2005b on the Dutch Alpino corpus see bl row in Table 1 .", "Although slower than the baseline approach , our method can still parse large sentences more than 50 tokens in a reasonable amount of time less than a minute .", "We have shown that parsing time can be significantly reduced using a simple approximation which only marginally degrades performance .", "Furthermore , we believe that the method has potential for further extensions and applications .", "Thanks to Ivan Meza Ruiz , Ruken C ak\u0131c\u0131 , Beata Kouchnir and Abhishek Arun for their contribution during the CoNLL shared task and to Mirella Lapata for helpful comments and suggestions ."], "summary_lines": ["Incremental Integer Linear Programming For Non-Projective Dependency Parsing\n", "Integer Linear Programming has recently been used for decoding in a number of probabilistic models in order to enforce global constraints.\n", "However, in certain applications, such as non-projective dependency parsing and machine translation, the complete formulation of the decoding problem as an integer linear program renders solving intractable.\n", "We present an approach which solves the problem incrementally, thus we avoid creating intractable integer linear programs.\n", "This approach is applied to Dutch dependency parsing and we show how the addition of linguistically motivated constraints can yield a significant improvement over state-of-the-art.\n", "For dependency parsing, we study a method using integer linear programming which can incorporate global linguistic constraints.\n", "Our work in dependency parsing demonstrate that it is possible to use ILP to perform efficient inference for very large programs when used in an incremental manner.\n", "We show that even exponentially large decoding problems may be solved efficiently using ILP solvers if a Cutting-Plane Algorithm (Dantzig et al, 1954) is used.\n", "We tackle the MAP problem for dependency parsing by an incremental approach that starts with a relaxation of the problem, solves it, and adds additional constraints only if they are violated.\n"]}
{"article_lines": ["Corpus Based Identification Of Non Anaphoric Noun Phrases", "Coreference resolution involves finding antecedents for anaphoric discourse entities , such as definite noun phrases .", "But many definite noun phrases are not anaphoric because their meaning can be understood from general world knowledge e . g . , quot ; the White House quot ; or quot ; the news media quot ; .", "We have developed a corpus based algorithm for automatically identifying definite noun phrases that are non anaphoric , which has the potential to improve the efficiency and accuracy of coreference resolution systems .", "Our algorithm generates lists of nonanaphoric noun phrases and noun phrase patterns from a training corpus and uses them to recognize non anaphoric noun phrases in new texts .", "Using 1600 MUC 4 terrorism news articles as the training corpus , our approach achieved 78 recall and 87 precision at identifying such noun phrases in 50 test documents .", "Most automated approaches to coreference resolution attempt to locate an antecedent for every potentially coreferent discourse entity DE in a text .", "The problem with this approach is that a large number of DE's may not have antecedents .", "While some discourse entities such as pronouns are almost always referential , definite descriptions quot ; may not be .", "Earlier work found that nearly 50 of definite descriptions had no prior referents Vieira and Poesio , 1997 , and we found that number to be even higher , 63 , in our corpus .", "Some non anaphoric definite descriptions can be identified by looking for syntactic clues like attached prepositional phrases or restrictive relative clauses .", "But other definite descriptions are non anaphoric because readers understand their meaning due to common knowledge .", "For example , readers of this 'In this work , we define a definite description to be a noun phrase beginning with the . paper will probably understand the real world referents of quot ; the F . B . I . , quot ; quot ; the White House , quot ; and quot ; the Golden Gate Bridge . quot ; These are instances of definite descriptions that a coreference resolver does not need to resolve because they each fully specify a cognitive representation of the entity in the reader's mind .", "One way to address this problem is to create a list of all non anaphoric NPs that could be used as a filter prior to coreference resolution , but hand coding such a list is a daunting and intractable task .", "We propose a corpusbased mechanism to identify non anaphoric NPs automatically .", "We will refer to non anaphoric definite noun phrases as existential NPs Allen , 1995 .", "Our algorithm uses statistical methods to generate lists of existential noun phrases and noun phrase patterns from a training corpus .", "These lists are then used to recognize existential NPs in new texts .", "Computational coreference resolvers fall into two categories systems that make no attempt to identify non anaphoric discourse entities prior to coreference resolution , and those that apply a filter to discourse entities , identifying a subset of them that are anaphoric .", "Those that do not practice filtering include decision tree models Aone and Bennett , 1996 , McCarthy and Lehnert , 1995 that consider all possible combinations of potential anaphora and referents .", "Exhaustively examining all possible combinations is expensive and , we believe , unnecessary .", "Of those systems that apply filtering prior to coreference resolution , the nature of the filtering varies .", "Some systems recognize when an anaphor and a candidate antecedent are incompatible .", "In SRI's probabilistic model Kehler , The ARCE battalion command has reported that about 50 peasants of various ages have been kidnapped by terrorists of the Farabundo Marti National Liberation Front FMLN in San Miguel Department .", "According to that garrison , the mass kidnapping took place on 30 December in San Luis de la Reina .", "The source added that the terrorists forced the individuals , who were taken to an unknown location , out of their residences , presumably to incorporate them against their will into clandestine groups .", "1997 , a pair of extracted templates may be removed from consideration because an outside knowledge base indicates contradictory features .", "Other systems look for particular constructions using certain trigger words .", "For example , pleonastic2 pronouns are identified by looking for modal adjectives e . g .", "quot ; necessary quot ; or cognitive verbs e . g .", "quot ; It is thought that . . . quot ; in a set of patterned constructions Lappin and Leass , 1994 , Kennedy and Boguraev , 1996 .", "A more recent system Vieira and Poesio , 1997 recognizes a large percentage of nonanaphoric definite noun phrases NPs during the coreference resolution process through the use of syntactic cues and case sensitive rules .", "These methods were successful in many instances , but they could not identify them all .", "The existential NPs that were missed were existential to the reader , not because they were modified by particular syntactic constructions , but because they were part of the reader's general world knowledge .", "Definite noun phrases that do not need to be resolved because they are understood through world knowledge can represent a significant portion of the existential noun phrases in a text .", "In our research , we found that existential NPs account for 63 of all definite NPs , and 24 of them could not be identified by syntactic or lexical means .", "This paper details our method for identifying existential NPs that are understood through general world knowledge .", "Our system requires no hand coded information and can recognize a larger portion of existential NPs than Vieira and Poesio's system .", "To better understand what makes an NP anaphoric or non anaphoric , we found it useful to classify definite NPs into a taxonomy .", "We first classified definite NPs into two broad categories , referential NPs , which have prior referents in the texts , and existential NPs , which do not .", "In Figure 1 , examples of referential NPs are quot ; the mass kidnapping , quot ; quot ; the terrorists quot ; and quot ; the individuals . quot ; , while examples of existential NPs are quot ; the ARCE battalion command quot ; and quot ; the Farabundo Marti National Liberation Front . quot ; The full taxonomy can be found in Figure 2 .", "We should clarify an important point .", "When we say that a definite NP is existential , we say this because it completely specifies a cognitive representation of the entity in the reader's mind .", "That is , suppose quot ; the F . B . I . quot ; appears in both sentence 1 and sentence 7 of a text .", "Although there may be a cohesive relationship between the noun phrases , because they both completely specify independently , we consider them to be non anaphoric .", "Definite Noun Phrases We further classified existential NPs into two categories , independent and associative , which are distinguished by their need for context .", "Independent existentials can be understood in isolation .", "Associative existentials are inherently associated with an event , action , object or other context3 .", "In a text about a basketball game , for example , we might find quot ; the score , quot ; quot ; the hoop quot ; and quot ; the bleachers . quot ; Although they may that our independent existentials roughly equate to her new class , our associative existentials to her inferable class , and our referentials to her evoked class . not have direct antecedents in the text , we understand what they mean because they are all associated with basketball games .", "In isolation , a reader would not necessarily understand the meaning of quot ; the score quot ; because context is needed to disambiguate the intended word sense and provide a complete specification .", "Because associative NPs represent less than 10 of the existential NPs in our corpus , our efforts were directed at automatically identifying independent existentials .", "Understanding how to identify independent existential NPs requires that we have an understanding of why these NPs are existential .", "We classified independent existentials into two groups , semantic and syntactic .", "Semantically independent NPs are existential because they are understood by readers who share a collective understanding of current events and world knowledge .", "For example , we understand the meaning of quot ; the F . B . I . quot ; without needing any other information .", "Syntactically independent NPs , on the other hand , gain this quality because they are modified structurally .", "For example , in quot ; the man who shot Liberty Valence , quot ; quot ; the man quot ; is existential because the relative clause uniquely identifies its referent .", "Our goal is to build a system that can identify independent existential noun phrases automatically .", "In the previous section , we observed that quot ; existentialism quot ; can be granted to a definite noun phrase either through syntax or semantics .", "In this section , we introduce four methods for recognizing both classes of existentials .", "We began by building a set of syntactic heuristics that look for the structural cues of restrictive premodification and restrictive postmodification .", "Restrictive premodification is often found in noun phrases in which a proper noun is used as a modifier for a head noun , for example , quot ; the U . S . president . quot ; quot ; The president quot ; itself is ambiguous , but quot ; the U . S . president quot ; is not .", "Restrictive postmodification is often represented by restrictive relative clauses , prepositional phrases , and appositives .", "For example , quot ; the president of the United States quot ; and quot ; the president who governs the U . S . quot ; are existential due to a prepositional phrase and a relative clause , respectively .", "We also developed syntactic heuristics to recognize referential NPs .", "Most NPs of the form quot ; the number noun quot ; e . g . , quot ; the 12 men quot ; have an antecedent , so we classified them as referential .", "Also , if the head noun of the NP appeared earlier in the text , we classified the NP as referential .", "This method , then , consists of two groups of syntactic heuristics .", "The first group , which we refer to as the rule in heuristics , contains seven heuristics that identify restrictive premodification or postmodification , thus targeting existential NPs .", "The second group , referred to as the rule out heuristics , contains two heuristics that identify referential NPs .", "Most referential NPs have antecedents that precede them in the text .", "This observation is the basis of our first method for identifying semantically independent NPs .", "If a definite NP occurs in the first sentence4 of a text , we assume the NP is existential .", "Using a training corpus , we create a list of presumably existential NPs by collecting the first sentence of every text and extracting all definite NPs that were not classified by the syntactic heuristics .", "We call this list the 51 extractions .", "While examining the Si extractions , we found many similar NPs , for example quot ; the Salvadoran Government , quot ; quot ; the Guatemalan Government , quot ; and quot ; the U . S . Government . quot ; The similarities indicate that some head nouns , when premodified , represent existential entities .", "By using the Si extractions as input to a pattern generation algorithm , we built a set of Existential Head Patterns EHPs that identify such constructions .", "These patterns are of the form quot ; the x 5 nounl . . . nounN quot ; such as quot ; the x government quot ; or quot ; the x Salvadoran government . quot ; Figure 3 shows the algorithm for creating EHPs .", "It also became clear that some existentials never appear in indefinite constructions .", "quot ; The F . B . I . , quot ; quot ; the contrary , quot ; quot ; the National Guard quot ; are definite NPs which are rarely , if ever , seen in indefinite constructions .", "The chances that a reader will encounter quot ; an F . B . I . quot ; are slim to none .", "These NPs appeared to be perfect candidates for a corpus based approach .", "To locate quot ; definite only quot ; NPs we made two passes over the corpus .", "The first pass produced a list of every definite NP and its frequency .", "The second pass counted indefinite uses of all NPs cataloged during the first pass .", "Knowing how often an NP was used in definite and indefinite constructions allowed us to sort the NPs , first by the probability of being used as a definite its definite probability , and second by definite use frequency .", "For example , quot ; the contrary quot ; appeared high on this list because its head noun occurred 15 times in the training corpus , and every time it was in a definite construction .", "From this , we created a definite only list by selecting those NPs which occurred at least 5 times and only in definite constructions .", "Examples from the three methods can be found in the Appendix .", "Our methods for identifying existential NPs are all heuristic based and therefore can be incorrect in certain situations .", "We identified two types of common errors .", "To address these problems , we developed a vaccine .", "It was clear that we had a number of infections in our Si list , including quot ; the base , quot ; quot ; the For every definite NP in a text individuals , quot ; quot ; the attack , quot ; and quot ; the banks . quot ; We noticed , however , that many of these incorrect NPs also appeared near the bottom of our definite indefinite list , indicating that they were often seen in indefinite constructions .", "We used the definite probability measure as a way of detecting errors in the Si and EHP lists .", "If the definite probability of an NP was above an upper threshold , the NP was allowed to be classified as existential .", "If the definite probability of an NP fell below a lower threshold , it was not allowed to be classified by the Si or EHP method .", "Those NPs that fell between the two thresholds were considered occasionally existential .", "Occasionally existential NPs were handled by observing where the NPs first occurred in the text .", "For example , if the first use of quot ; the guerrillas quot ; was in the first few sentences of a text , it was usually an existential use .", "If the first use was later , it was usually a referential use because a prior definition appeared in earlier sentences .", "We applied an early allowance threshold of three sentences occasionally existential NPs occuring under this threshold were classified as existential , and those that occurred above were left unclassified .", "Figure 4 details the vaccine's algorithm .", "We trained and tested our methods on the Latin American newswire articles from MUC4 MUC 4 Proceedings , 1992 .", "The training set contained 1 , 600 texts and the test set contained 50 texts .", "All texts were first parsed by SUNDANCE , our heuristic based partial parser developed at the University of Utah .", "We generated the Si extractions by processing the first sentence of all training texts .", "This produced 849 definite NPs .", "Using these NPs as input to the existential head pattern algorithm , we generated 297 EHPs .", "The DO list was built by using only those NPs which appeared at least 5 times in the corpus and 100 of the time as definites .", "We generated the DO list in two iterations , once for head nouns alone and once for full NPs , resulting in a list of 65 head nouns and 321 full NPs6 .", "Once the methods had been trained , we classified each definite NP in the test set as referential or existential using the algorithm in Figure 5 .", "Figure 6 graphically represents the main elements of the algorithm .", "Note that we applied vaccines to the Si and EHP lists , but not to the DO list because gaining entry to the DO list is much more difficult an NP must occur at least 5 times in the training corpus , and every time it must occur in a definite construction .", "To evaluate the performance of our algorithm , we hand tagged each definite NP in the 50 test texts as a syntactically independent existential , a semantically independent existential , an associative existential or a referential NP .", "Figure 8 shows the distribution of definite NP types in the test texts .", "Of the 1 , 001 definite NPs tested , 63 were independent existentials , so removing these NPs from the coreference resolution process could have substantial savings .", "We measured the accuracy of our classifications using recall and precision metrics .", "Results are shown in Figure 7 .", "As a baseline measurement , we considered the accuracy of classifying every definite NP as existential .", "Given the distribution of definite NP types in our test set , this would result in recall of 100 and precision of 72 .", "Note that we are more interested in high measures of precision than recall because we view this method to be the precursor to a coreference resolution algorithm .", "Incorrectly removing an anaphoric NP means that the coreference resolver would never have a chance to resolve it , on the other hand , non anaphoric NPs that slip through can still be ruled as non anaphoric by the coreference resolver .", "We first evaluated our system using only the syntactic heuristics , which produced only 43 recall , but 92 precision .", "Although the syntactic heuristics are a reliable way to identify existential definite NPs , they miss 57 of the true existentials .", "We expected the Si , EHP , and DO methods to increase coverage .", "First , we evaluated each method independently on top of the syntactic heuristics .", "The results appear in rows 2 4 of Figure 7 .", "Each method increased recall to between 61 69 , but decreased precision to 8487 .", "All of these methods produced a substantial gain in recall at some cost in precision .", "Next , we tried combining the methods to make sure that they were not identifying exactly the same set of existential NPs .", "When we combined the Si and EHP heuristics , recall increased to 80 with precision dropping only slightly to 82 .", "When we combined all three methods Si , EHP , and DO , recall increased to 82 without any corresponding loss of precision .", "These experiments show that these heuristics substantially increase recall and are identifying different sets of existential NPs .", "Finally , we tested our vaccine algorithm to see if it could increase precision without sacrificing much recall .", "We experimented with two variations Va used an upper definite probability threshold of 70 and VI , used an upper definite probability threshold of 50 .", "Both variations used a lower definite probability threshold of 25 .", "The results are shown in rows 7 8 of Figure 7 .", "Both vaccine variations increased precision by several percentage points with only a slight drop in recall .", "In previous work , the system developed by Vieria Poesio achieved 74 recall and 85 precision for identifying quot ; larger situation and unfamiliar use quot ; NPs .", "This set of NPs does not correspond exactly to our definition of existential NPs because we consider associative NPs to be existential and they do not .", "Even so , our results are slightly better than their previous results .", "A more equitable comparison is to measure our system's performance on only the independent existential noun phrases .", "Using this measure , our algorithm achieved 81 . 8 recall with 85 . 6 precision using Va , and achieved 82 . 9 recall with 83 . 5 precision using Vb .", "We have developed several methods for auto matically identifying existential noun phrases using a training corpus .", "It accomplishes this task with recall and precision measurements that exceed those of the earlier Vieira Rz Poesio system , while not exploiting full parse trees , appositive constructions , hand coded lists , or case sensitive text7 .", "In addition , because the system is fully automated and corpus based , it is suitable for applications that require portability across domains .", "Given the large percentage of non anaphoric discourse entities handled by most coreference resolvers , we believe that using a system like ours to filter existential NPs has the potential to reduce processing time and complexity and improve the accuracy of coreference resolution ."], "summary_lines": ["Corpus-Based Identification Of Non-Anaphoric Noun Phrases\n", "Coreference resolution involves finding antecedents for anaphoric discourse entities, such as definite noun phrases.\n", "But many definite noun phrases are not anaphoric because their meaning can be understood from general world knowledge (e.g., \"the White House\" or \"the news media\").\n", "We have developed a corpus-based algorithm for automatically identifying definite noun phrases that are non-anaphoric, which has the potential to improve the efficiency and accuracy of coreference resolution systems.\n", "Our algorithm generates lists of non-anaphoric noun phrases and noun phrase patterns from a training corpus and uses them to recognize non-anaphoric noun phrases in new texts.\n", "Using 1600 MUC-4 terrorism news articles as the training corpus, our approach achieved 78% recall and 87% precision at identifying such noun phrases in 50 test documents.\n", "We develop a system for identifying discourse-new DDs that incorporates, in addition to syntax-based heuristics aimed at recogznizing predicative and established DDs, additional techniques for mining from corpora unfamiliar DDS including proper names, larger situation and semantically functional.\n", "We develop an unsupervised learning algorithm that automatically recognizes definite NPs that are existential without syntactic modification because their meaning is universally understood.\n"]}
{"article_lines": ["Word Sense Disambiguation Using Decomposable Models", "is composed of interdependent variables .", "The test used to evaluate a model gives preference to those that have the fewest number of interdependencies , thereby selecting models expressing only the most systematic variable interactions .", "To summarize the method , one first identifies informative contextual features where quot ; informative quot ; is a well defined notion , discussed in Section 2 .", "Then , out of all possible decomposable models characterizing interdependency relationships among the selected variables , those that are found to produce good approximations to the data are identified using the test mentioned above and one of those models is used to perform disambiguation .", "Thus , we are able to use multiple contextual features without the need for untested assumptions regarding the form of the model .", "Further , approximating the joint distribution of all variables with a model identifying only the most important systematic interactions among variables limits the number of parameters to be estimated , supports computational efficiency , and provides an understanding of the data .", "The biggest limitation associated with this method is the need for large amounts of sense tagged data .", "Because asymptotic distributions of the test statistics are used , the validity of the results obtained using this approach are compromised when it is applied to sparse data this point is discussed further in Section 2 .", "To test the method of model selection presented in this paper , a case study of the disambiguation of the performed . selected because it has been shown in previous studies to be a difficult word to disambiguate .", "We selected as the set of tags all non idiomatic noun senses of defined in the electronic version of Longman's Dictionary of Contemporary English LDOCE 23 .", "Using the models produced in this study , we are able to assign an sense tag to every usage of a heldout test set with 78 accuracy .", "Although it is difficult to compare our results to those reported for previous disambiguation experiments , as will be discussed later , we feel these results are encouraging .", "The remainder of the paper is organized as follows .", "Section 2 provides a more complete definition of the Abstract Most probabilistic classifiers used for word sense disambiguation have either been based on only one contextual feature or have used a model that is simply assumed to characterize the interdependencies among multiple contextual features .", "In this paper , a different approach to formulating a probabilistic model is presented along with a case study of the performance of models produced in this manner for the disambiguation of the noun describe a method for formulating probabilistic models that use multiple contextual features for word sense disambiguation , without requiring untested assumptions regarding the form of the model .", "Using this approach , the joint distribution of all variables is described by only the most systematic variable interactions , thereby limiting the number of parameters to be estimated , supporting computational efficiency , and providing an understanding of the data .", "This paper presents a method for constructing probabilistic classifiers for word sense disambiguation that offers advantages over previous approaches .", "Most previous efforts have not attempted to systematically identify the interdependencies among contextual features such as collocations that can be used to classify the meaning of an ambiguous word .", "Many researchers have performed disambiguation on the basis of only a single feature , while others who do consider multiple contextual features assume that all contextual features are either conditionally independent given the sense of the word or fully independent .", "Of course , all contextual features could be treated as interdependent , but , if there are several features , such a model could have too many parameters to estimate in practice .", "We present a method for formulating probabilistic models that describe the relationships among all variables in terms of only the most important interdependencies , that is , models of a certain class that are good approximations to the joint distribution of contextual features and word meanings .", "This class is the set of decomposable models models that can be expressed as a product of marginal distributions , where each marginal methodology used for formulating decomposable models and Section 3 describes the details of the case study performed to test the approach .", "The results of the disambiguation case study are discussed and contrasted with similar efforts in Sections 4 and 5 .", "Section 6 is the conclusion .", "In this Section , we address the problem of finding the models that generate good approximations to a given discrete probability distribution , as selected from among the class of decomposable models .", "Decomposable models are a subclass of log linear models and , as such , can be used to characterize and study the structure of data 21 , that is , the interactions among variables as evidenced by the frequency with which the values of the variables co occur .", "Given a data sample of objects , where each object is described by d discrete variables , let x xi , x2 , . . . , xg be a q dimensional vector of counts , where each xi is the frequency with which one of the possible combinations of the values of the d variables occurs in the data sample and the frequencies of all such possible combinations are included in x .", "The log linear model expresses the logarithm of E x the mean of x as a linear sum of the contributions of the quot ; effects quot ; of the variables and the interactions among the variables .", "Assume that a random sample consisting of N independent and identical trials i . e . , all trials are described by the same probability density function is drawn from a discrete d variate distribution .", "In such a situation , the outcome of each trial must be an event corresponding to a particular combination of the values of the d variables .", "Let pi be the probability that the ith event i . e . , the jth possible combination of the values of all variables occurs on any trial and let xi be the number of times that the i event occurs in the random sample .", "Then xl , x2 , . . , xg has a multinomial distribution with parameters N and pi , , p9 .", "For a given sample size , N , the likelihood of selecting any particular random sample is defined once the population parameters , that is , the pi's or , equivalently , the E xi 's where E xi is the mean frequency of event i , are known .", "Log linear models express the value of the logarithm of each E xi or pi as a linear sum of a smaller i . e . , less than q number of new population parameters that characterize the effects of individual variables and their interactions .", "The theory of log linear models specifies the sufficient statistics functions of x for estimating the effects of each variable and of each interaction among variables on E x .", "The sufficient statistics are the sample counts from the highest order marginals composed of only interdependent variables .", "These statistics are the maximum likelihood estimates of the mean values of the corresponding marginals distributions .", "Consider , for example , a random sample taken from a population in which four contextual features are used to characterize each occurrence of an ambiguous word .", "The sufficient statistics for the model describing contextual features one and two as independent but all other variables as interdependent are , for all i , j , k , in , n in this and all subsequent equations , f is an abbreviation for feature Within the class of decomposable models , the maximum likelihood estimate for E x reduces to the product of the sufficient statistics divided by the sample counts defined in the marginals composed of the common elements in the sufficient statistics .", "As such , decomposable models are models that can be expressed as a product of marginals , 1 where each marginal consists of only interdependent variables .", "Returning to our previous example , the maximum likelihood estimate for E x is , for all i , j , k , m , n Expressing the population parameters as probabilities instead of expected counts , the equation above can be rewritten as follows , where the sample marginal relative frequencies are the maximum likelihood estimates of the population marginal probabilities .", "For all The degree to which the data is approximated by a model is called the fit of the model .", "In this work , the likelihood ratio statistic , G2 , is used as the measure of the goodness of fit of a model .", "It is distributed asymptotically as X2 with degrees of freedom corresponding to the number of interactions and or variables omitted from unconstrained in the model .", "Accessing the fit 'The marginal distributions can be represented in terms of counts or relative frequencies , depending on whether the parameters are expressed as expected frequencies or probabilities , respectively . of a model in terms of the significance of its G2 statistic gives preference to models with the fewest number of interdependencies , thereby assuring the selection of a model specifying only the most systematic variable interactions .", "Within the framework described above , the process of model selection becomes one of hypothesis testing , where each pattern of dependencies among variables expressible in terms of a decomposable model is postulated as a hypothetical model and its fit to the data is evaluated .", "The quot ; best fitting quot ; models are identified , in the sense that the significance of their reference x2 values are large , and , from among this set , a conceptually appealing model is chosen .", "The exhaustive search of decomposable models can be conducted as described in 12 .", "What we have just described is a method for approximating the joint distribution of all variables with a model containing only the most important systematic interactions among variables .", "This approach to model formulation limits the number of parameters to be estimated , supports computational efficiency , and provides an understanding of the data .", "The single biggest limitation remaining in this day of large memory , high speed computers results from reliance on asymptotic theory to describe the distribution of the maximum likelihood estimates and the likelihood ratio statistic .", "The effect of this reliance is felt most acutely when working with large sparse multinomials , which is exactly when this approach to model construction is most needed .", "When the data is sparse , the usual asymptotic properties of the distribution of the likelihood ratio statistic and the maximum likelihood estimates may not hold .", "In such cases , the fit of the model will appear to be too good , indicating that the model is in fact over constrained for the data available .", "In this work , we have limited ourselves to considering only those models with sufficient statistics that are not sparse , where the significance of the reference x2 is not unreasonable ; most such models have sufficient statistics that are lower order marginal distributions .", "In the future , we will investigate other goodness of fit tests 18 , 1 , 22 that are perhaps more appropriate for sparse data .", "Unlike several previous approaches to word sense disambiguation 29 , 5 , 7 , 10 , nothing in this approach limits the selection of sense tags to a particular number or type of meaning distinctions .", "In this study , our goal was to address a non trivial case of ambiguity , but one that would allow some comparison of results with previous work .", "As a result of these considerations , the word interest was chosen as a test case , and the six non idiomatic noun senses of interest defined in LDOCE were selected as the tag set .", "The only restriction limiting the choice of corpus is the need for large amounts of on line data .", "Due to availability , the Penn Treebank Wall Street Journal corpus was selected .", "In total , 2 , 476 usages2 of interest as a noun3 were automatically extracted from the corpus and manually assigned sense tags corresponding to the LDOCE definitions .", "During tagging , 107 usages were removed from the data set due to the authors' inability to classify them in terms of the set of LDOCE senses .", "Of the rejected usages , 43 are metonymic , and the rest are hybrid meanings specific to the domain , such as public interest group .", "Because our sense distinctions are not merely between two or three clearly defined core senses of a word , the task of hand tagging the tokens of interest required subtle judgments , a point that has also been observed by other researchers disambiguating with respect to the full set of LDOCE senses 6 , 28 .", "Although this undoubtedly degraded the accuracy of the manually assigned sense tags and thus the accuracy of the study as well , this problem seems unavoidable when making semantic distinctions beyond clearly defined core senses of a word 17 , 11 , 14 , 15 .", "Of the 2 , 369 sentences containing the sense tagged usages of interest , 600 were randomly selected and set aside to serve as the test set .", "The distribution of sense tags in the data set is presented in Table 1 .", "We now turn to the selection of individually informative contextual features .", "In our approach to disambiguation , a contextual feature is judged to be informative i . e . , correlated with the sense tag of the ambiguous word if the model for independence between that feature and the sense tag is judged to have an extremely poor fit using the test described in Section 2 .", "The worse the fit , the more informative the feature is judged to be similar to the approach suggested in 9 .", "Only features whose values can be automatically determined were considered , and preference was given to features that intuitively are not specific to interest but see the discussion of collocational features below .", "An additional criterion was that the features not have too many possible values , in order to curtail sparsity in the resulting data matrix .", "We considered three different types of contextual features morphological , collocation specific , and classbased , with part of speech POS categories serving as the word classes .", "Within these classes , we choose a number of specific features , each of which was judged to be informative as described above .", "We used one morphological feature a dichotomous variable indicating the presence or absence of the plural form .", "The values of the class based variables are a set of twenty five POS tags formed , with one exception , from the first letter of the tags used in the Penn Treebank corpus .", "Two different sets of class based variables were selected .", "The first set contained only the POS tags of the word immediately preceding and the word immediately succeeding the ambiguous word , while the second set was extended to include the FOS tags of the two immediately preceding and two succeeding words .", "A limited number of collocation specific variables were selected , where the term collocation is used loosely to refer to a specific spelling form occurring in the same sentence as the ambiguous word .", "All of our collocational variables are dichotomous , indicating the presence or absence of the associated spelling form .", "While collocation specific variables are , by definition , specific to the word being disambiguated , the procedure used to select them is general .", "The search for collocationspecific variables was limited to the 400 most frequent spelling forms in a data sample composed of sentences containing interest .", "Out of these 400 , the five spelling forms found to be the most informative using the test described above were selected as the collocational variables .", "It is not enough to know that each of the features described above is highly correlated with the meaning of the ambiguous word .", "In order to use the features in concert to perform disambiguation , a model describing the interactions among them is needed .", "Since we had no reason to prefer , a priori , one form of model over another , all models describing possible interactions among the features were generated , and a model with good fit was selected .", "Models were generated and tested as described in Section 2 .", "Both the form and the performance of the model selected for each set of variables is presented in Table 2 .", "Performance is measured in terms of the total percentage of the test set tagged correctly by a classifier using the specified model .", "This measure combines both precision and recall .", "Portions of the test set that are not covered by the estimates of the parameters made from the training set are not tagged and , therefore , counted as wrong .", "The form of the model describes the interactions among the variables by expressing the joint distribution of the values of all contextual features and sense tags as a product of conditionally independent marginals , with each marginal being composed of non independent variables .", "Models of this form describe a markov field 8 , 21 that can be represented graphically as is shown in Figure 1 for Model 4 of Table 2 .", "In both Figures 1 and 2 , each of the variables short , in , pursue , rate s , percent i . e . , the sign quot ; 70' is the presence or absence of that spelling form .", "Each of the variables rlpos , r2pos , Ilpos , and 12pos is the POS tag of the word 1 or 2 positions to the left or right r .", "The variable ending is whether interest is in the singular or plural , and the variable tag is the sense tag assigned to interest .", "The graphical representation of Model 4 is such that there is a one to one correspondence between the nodes of the graph and the sets of conditionally independent variables in the model .", "The semantics of the graph topology is that all variables that are not directly connected in the graph are conditionally independent given the values of the variables mapping to the connecting nodes .", "For example , if node a separates node b from node c in the graphical representation of a markov field , then the variables mapping to node b are conditionally independent of the variables mapping to node c given the values of the variables mapping to node a .", "In the case of Model 4 , Figure 1 graphically depicts the fact that the value of the morphological variable ending is conditionally independent of the values of all other contextual features given the sense tag of the ambiguous word .", "The Markov field depicted in Figure 1 is represented by an undirected graph because conditional independence is a symmetric relationship .", "But decomposable models can also be characterized by directed graphs and interpreted according to the semantics of a Bayesian network 21 ; also described as quot ; recursive causal models quot ; in 27 and 16 .", "In a Bayesian network , the notions of causation and influence replace the notion of conditional independence in a Markov field .", "The parents of a variable or set of variables V are those variables judged to be the direct causes or to have direct influence on the value of V ; V is called a quot ; response quot ; to those causes or influences .", "The Bayesian network representation of a decomposable model embodies an explicit ordering of the n variables in the model such that variable i may be considered a response to some or all of variables i 1 , , n , but is not thought of as a response to any one of the variables 1 , , i 1 .", "In all models presented in this paper , the sense tag of the ambiguous word causes or influences the values of all other variables in the model .", "The Bayesian network representation of Model 4 is presented in Figure 2 .", "In Model 4 , the variables in and percent are treated as influencing the values of rate , short , and pursue in order to achieve an ordering of variables as described above .", "Many researchers have avoided characterizing the interactions among multiple contextual features by considering only one feature in determining the sense of an ambiguous word .", "Techniques for identifying the optimum feature to use in disambiguating a word are presented in 7 , 30 and 5 .", "Other works consider multiple contextual features in performing disambiguation without formally characterizing the relationships among the features .", "The majority of these efforts 13 , 31 weight each feature in predicting the sense of an ambiguous word in accordance with frequency information , without considering the extent to which the features cooccur with one another .", "Gale , Church and Yarowsky 10 and Yarowsky 29 formally characterize the interactions that they consider in their model , but they simply assume that their model fits the data .", "Other researchers have proposed approaches to systematically combining information from multiple contextual features in determining the sense of an ambiguous word .", "Schutze 26 derived contextual features from a singular value decomposition of a matrix of letter four gram co occurrence frequencies , thereby assuring the independence of all features .", "Unfortunately , interpreting a contextual feature that is a weighted combination of letter four grams is difficult .", "Further , the clustering procedure used to assign word meaning based on these features is such that the resulting sense clusters do not have known statistical properties .", "This makes it impossible to generalize the results to other data sets .", "Black 3 used decision trees 4 to define the relationships among a number of pre specified contextual features , which he called quot ; contextual categories quot ; , and the sense tags of an ambiguous word .", "The tree construction process used by Black partitions the data according to the values of one contextual feature before considering the values of the next , thereby treating all features incorporated in the tree as interdependent .", "The method presented here for using information from multiple contextual features is more flexible and makes better use of a small data set by eliminating the need to treat all features as interdependent .", "The work that bears the closest resemblance to the work presented here is the maximum entropy approach to developing language models 24 , 25 , 19 and 20 .", "Although this approach has not been applied to wordsense disambiguation , there is a strong similarity between that method of model formulation and our own .", "A maximum entropy model for multivariate data is the likelihood function with the highest entropy that satisfies a pre defined set of linear constraints on the underlying probability estimates .", "The constraints describe interactions among variables by specifying the expected frequency with which the values of the constrained variables co occur .", "When the expected frequencies specified in the constraints are linear combinations of the observed frequencies in the training data , the resulting maximum entropy model is equivalent to a maximum likelihood model , which is the type of model used here .", "To date , in the area of natural language processing , the principles underlying the formulation of maximum entropy models have been used only to estimate the parameters of a model .", "Although the method described in this paper for finding a good approximation to the joint distribution of a set of discrete variables makes use of maximum likelihood models , the scope of the technique we are describing extends beyond parameter estimation to include selecting the form of the model that approximates the joint distribution .", "Several of the studies mentioned in this Section have used interest as a test case , and all of them with the exception of Schutze 26 considered four possible meanings for that word .", "In order to facilitate comparison of our work with previous studies , we re estimated the parameters of our best model and tested it using data containing only the four LDOCE senses corresponding to those used by others usages not tagged as being one of these four senses were removed from both the test and training data sets .", "The results of the modified experiment along with a summary of the published results of previous studies are presented in Table 3 .", "While it is true that all of the studies reported in Table 3 used four senses of interest , it is not clear that any of the other experimental parameters were held constant in all studies .", "Therefore , this comparison is only suggestive .", "In order to facilitate more meaningful comparisons in the future , we are donating the data used in this experiment to the Consortium for Lexical Research ftp site clr . nmsu . edu where it will be available to all interested parties .", "In this paper , we presented a method for formulating probabilistic models that use multiple contextual features for word sense disambiguation without requiring untested assumptions regarding the form of the model .", "In this approach , the joint distribution of all variables is described by only the most systematic variable interactions , thereby limiting the number of parameters to be estimated , supporting computational efficiency , and providing an understanding of the data .", "Further , different types of variables , such as class based and collocation specific ones , can be used in combination with one another .", "We also presented the results of a study testing this approach .", "The results suggest that the models produced in this study perform as well as or better than previous efforts on a difficult test case .", "We are investigating several extensions to this work .", "In order to reasonably consider doing large scale wordsense disambiguation , it is necessary to eliminate the need for large amounts of manually sense tagged data .", "In the future , we hope to develop a parametric model or models applicable to a wide range of content words and to estimate the parameters of those models from untagged data .", "To those ends , we are currently investigating a means of obtaining maximum likelihood estimates of the parameters of decomposable models from untagged data .", "The procedure we are using is a variant of the EM algorithm that is specific to models of the form produced in this study .", "Preliminary results are mixed , with performance being reasonably good on models with low order marginals e . g . , 63 of the test set was tagged correctly with Model 1 using parameters estimated in this manner but poorer on models with higher order marginals , such as Model 4 .", "Work is needed to identify and constrain the parameters that cannot be estimated from the available data and to determine the amount of data needed for this procedure .", "We also hope to integrate probabilistic disambiguation models , of the type described in this paper , with a constraint based knowledge base such as WordNet .", "In the past , there have been two types of approaches to word sense disambiguation 1 a probabilistic approach such as that described here which bases the choice of sense tag on the observed joint distribution of the tags and contextual features , and 2 a symbolic knowledge based approach that postulates some kind of relational or constraint structure among the words to be tagged .", "We hope to combine these methodologies and thereby derive the benefits of both .", "Our approach to combining these two paradigms hinges on the network representations of our probabilistic models as described in Section 4 and will make use of the methods presented in 21 .", "The authors would like to thank Gerald Rogers for sharing his expertise in statistics , Ted Dunning for advice and support on software development , and the members of the NLP group in the CRL for helpful discussions .", "1 Baglivo , J . , Olivier , D . , and Pagano , M . 1992 .", "Methods for Exact Goodness of Fit Tests .", "Journal of the American Statistical Association , Vol .", "87 , No .", "418 , June 1992 .", "2 Bishop , Y . M . ; Fienberg , S . ; and Holland , P 1975 .", "Discrete Multivariate Analysis Theory and Practice .", "Cambridge The MIT Press .", "3 Black , Ezra 1988 .", "An Experiment in Computational Discrimination of English Word Senses ."], "summary_lines": ["Word-Sense Disambiguation Using Decomposable Models\n", "Most probabilistic classifiers used for word-sense disambiguation have either been based on only one contextual feature or have used a model that is simply assumed to characterize the interdependencies among multiple contextual features.\n", "In this paper, a different approach to formulating a probabilistic model is presented along with a case study of the performance of models produced in this manner for the disambiguation of the noun interest.\n", "We describe a method for formulating probabilistic models that use multiple contextual features for word-sense disambiguation, without requiring untested assumptions regarding the form of the model.\n", "Using this approach, the joint distribution of all variables is described by only the most systematic variable interactions, thereby limiting the number of parameters to be estimated, supporting computational efficiency, and providing an understanding of the data.\n", "We manually assign 2,476 usages of interest with sense tags from the Longman Dictionary of Contemporary English.\n"]}
{"article_lines": ["Learning Semantic Correspondences with Less Supervision", "A central problem in grounded language acquisition is learning the correspondences between a rich world state and a stream of text which references that world state .", "To deal with the high degree of ambiguity present in this setting , we present a generative model that simultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state .", "We show that our model generalizes across three domains of increasing difficulty Robocup sportscasting , weather forecasts a new domain , and NFL recaps .", "Recent work in learning semantics has focused on mapping sentences to meaning representations e . g . , some logical form given aligned sentence meaning pairs as training data Ge and Mooney , 2005 ; Zettlemoyer and Collins , 2005 ; Zettlemoyer and Collins , 2007 ; Lu et al . , 2008 .", "However , this degree of supervision is unrealistic for modeling human language acquisition and can be costly to obtain for building large scale , broadcoverage language understanding systems .", "A more flexible direction is grounded language acquisition learning the meaning of sentences in the context of an observed world state .", "The grounded approach has gained interest in various disciplines Siskind , 1996 ; Yu and Ballard , 2004 ; Feldman and Narayanan , 2004 ; Gorniak and Roy , 2007 .", "Some recent work in the NLP community has also moved in this direction by relaxing the amount of supervision to the setting where each sentence is paired with a small set of candidate meanings Kate and Mooney , 2007 ; Chen and Mooney , 2008 .", "The goal of this paper is to reduce the amount of supervision even further .", "We assume that we are given a world state represented by a set of records along with a text , an unsegmented sequence of words .", "For example , in the weather forecast domain Section 2 . 2 , the text is the weather report , and the records provide a structured representation of the temperature , sky conditions , etc .", "In this less restricted data setting , we must resolve multiple ambiguities 1 the segmentation of the text into utterances ; 2 the identification of relevant facts , i . e . , the choice of records and aspects of those records ; and 3 the alignment of utterances to facts facts are the meaning representations of the utterances .", "Furthermore , in some of our examples , much of the world state is not referenced at all in the text , and , conversely , the text references things which are not represented in our world state .", "This increased amount of ambiguity and noise presents serious challenges for learning .", "To cope with these challenges , we propose a probabilistic generative model that treats text segmentation , fact identification , and alignment in a single unified framework .", "The parameters of this hierarchical hidden semi Markov model can be estimated efficiently using EM .", "We tested our model on the task of aligning text to records in three different domains .", "The first domain is Robocup sportscasting Chen and Mooney , 2008 .", "Their best approach KRISPER obtains 67 F1 ; our method achieves 76 . 5 .", "This domain is simplified in that the segmentation is known .", "The second domain is weather forecasts , for which we created a new dataset .", "Here , the full complexity of joint segmentation and alignment arises .", "Nonetheless , we were able to obtain reasonable results on this task .", "The third domain we considered is NFL recaps Barzilay and Lapata , 2005 ; Snyder and Barzilay , 2007 .", "The language used in this domain is richer by orders of magnitude , and much of it does not reference the world state .", "Nonetheless , taking the first unsupervised approach to this problem , we were able to make substantial progress We achieve an F1 of 53 . 2 , which closes over half of the gap between a heuristic baseline 26 and supervised systems 68 80 .", "Our goal is to learn the correspondence between a text w and the world state s it describes .", "We use the term scenario to refer to such a w , s pair .", "The text is simply a sequence of words w wi , . . . , w w .", "We represent the world state s as a set of records , where each record r E s is described by a record type r . t E T and a tuple of field values r . v r . vi , . . . , r . vm . 1 For example , temperature is a record type in the weather domain , and it has four fields time , min , mean , and max .", "The record type r . t E T specifies the field type r . tf E INT , STR , CAT of each field value r . vf , f 1 , . . . , m . There are three possible field types integer INT , string STR , and categorical CAT which are assumed to be known and fixed .", "Integer fields represent numeric properties of the world such as temperature , string fields represent surface level identifiers such as names of people , and categorical fields represent discrete concepts such as score types in football touchdown , field goal , and safety .", "The field type determines the way we expect the field value to be rendered in words integer fields can be numerically perturbed , string fields can be spliced , and categorical fields are represented by open ended word distributions , which are to be learned .", "See Section 3 . 3 for details .", "In this domain , a Robocup simulator generates the state of a soccer game , which is represented by a set of event records .", "For example , the record pass arg1 pink1 , arg2 pink5 denotes a passing event ; this type of record has two fields arg1 the actor and arg2 the recipient .", "As the game is progressing , humans interject commentaries about notable events in the game , e . g . , pink passes back to pink5 near the middle of the field .", "All of the fields in this domain are categorical , which means there is no a priori association between the field value pink1 and the word pink .", "This degree of flexibility is desirable because pink is sometimes referred to as pink goalie , a mapping which does not arise from string operations but must instead be learned .", "We used the dataset created by Chen and Mooney 2008 , which contains 1919 scenarios from the 2001 2004 Robocup finals .", "Each scenario consists of a single sentence representing a fragment of a commentary on the game , paired with a set of candidate records .", "In the annotation , each sentence corresponds to at most one record possibly one not in the candidate set , in which case we automatically get that sentence wrong .", "See Figure 1 a for an example and Table 1 for summary statistics on the dataset .", "In this domain , the world state contains detailed information about a local weather forecast and the text is a short forecast report see Figure 1 b for an example .", "To create the dataset , we collected local weather forecasts for 3 , 753 cities in the US those with population at least 10 , 000 over three days February 7 9 , 2009 from www . weather . gov .", "For each city and date , we created two scenarios , one for the day forecast and one for the night forecast .", "The forecasts consist of hour by hour measurements of temperature , wind speed , sky cover , chance of rain , etc . , which represent the underlying world state .", "This world state is summarized by records which aggregate measurements over selected time intervals .", "For example , one of the records states the minimum , average , and maximum temperature from 5pm to 6am .", "This aggregation process produced 22 , 146 scenarios , each containing s 36 multi field records .", "There are 12 record types , each consisting of only integer and categorical fields .", "To annotate the data , we split the text by punctuation into lines and labeled each line with the records to which the line refers .", "These lines are used only for evaluation and are not part of the model see Section 5 . 1 for further discussion .", "The weather domain is more complex than the Robocup domain in several ways The text w is longer , there are more candidate records , and most notably , w references multiple records 5 . 8 on average , so the segmentation of w is unknown .", "See Table 1 for a comparison of the two datasets .", "In this domain , each scenario represents a single NFL football game see Figure 1 c for an example .", "The world state the things that happened during the game is represented by database tables , e . g . , scoring summary , team comparison , drive chart , play by play , etc .", "Each record is a database entry , for instance , the receiving statistics for a certain player .", "The text is the recap of the game an article summarizing the game highlights .", "The dataset we used was collected by Barzilay and Lapata 2005 .", "The data includes 466 games during the 2003 2004 NFL season .", "78 of these games were annotated by Snyder and Barzilay 2007 , who aligned each sentence to a set of records .", "This domain is by far the most complicated of the three .", "Many records corresponding to inconsequential game statistics are not mentioned .", "Conversely , the text contains many general remarks e . g . , it was just that type of game which are not present in any of the records .", "Furthermore , the complexity of the language used in the recap is far greater than what we can represent using our simple model .", "Fortunately , most of the fields are integer fields or string fields generally names or brief descriptions , which provide important anchor points for learning the correspondences .", "Nonetheless , the same names and numbers occur in multiple records , so there is still uncertainty about which record is referenced by a given sentence .", "To learn the correspondence between a text w and a world state s , we propose a generative model p w I s with latent variables specifying this correspondence .", "Our model combines segmentation with alignment .", "The segmentation aspect of our model is similar to that of Grenager et al . 2005 and Eisenstein and Barzilay 2008 , but in those two models , the segments are clustered into topics rather than grounded to a world state .", "The alignment aspect of our model is similar to the HMM model for word alignment Ney and Vogel , 1996 .", "DeNero et al . 2008 perform joint segmentation and word alignment for machine translation , but the nature of that task is different from ours .", "The model is defined by a generative process , which proceeds in three stages Figure 2 shows the corresponding graphical model 1 .", "Record choice choose a sequence of records r r1 , . . . , r r to describe , where each ri E s . 2 .", "Field choice for each chosen record ri , select a sequence of fields fi fi1 , .", ", fi fi , where each fij E 11 , . . . , m .", "Word choice for each chosen field fij , choose a number cij 0 and generate a sequence of cij words .", "The observed text w is the terminal yield formed by concatenating the sequences of words of all fields generated ; note that the segmentation of w provided by c 1cij is latent .", "Think of the words spanned by a record as constituting an utterance with a meaning representation given by the record and subset of fields chosen .", "Formally , our probabilistic model places a distribution over r , f , c , w and factorizes according to the three stages as follows p r , f , c , w s p r s p f r p c , w r , f , s The following three sections describe each of these stages in more detail .", "The record choice model specifies a distribution over an ordered sequence of records r r1 , . . . , r r , where each record ri E s . This model is intended to capture two types of regularities in the discourse structure of language .", "The first is salience , that is , some record types are simply more prominent than others .", "For example , in the NFL domain , 70 of scoring records are mentioned whereas only 1 of punting records are mentioned .", "The second is the idea of local coherence , that is , the order in which one mentions records tend to follow certain patterns .", "For example , in the weather domain , the sky conditions are generally mentioned first , followed by temperature , and then wind speed .", "To capture these two phenomena , we define a Markov model on the record types and given the record type , a record is chosen uniformly from the set of records with that type where s t def 1r E s r . t t and r0 . t is a dedicated START record type . 2 We also model the transition of the final record type to a designated STOP record type in order to capture regularities about the types of records which are described last .", "More sophisticated models of coherence could also be employed here Barzilay and Lapata , 2008 .", "We assume that s includes a special null record whose type is NULL , responsible for generating parts of our text which do not refer to any real records .", "Each record type t E T has a separate field choice model , which specifies a distribution over a sequence of fields .", "We want to capture salience and coherence at the field level like we did at the record level .", "For instance , in the weather domain , the minimum and maximum fields of a temperature record are mentioned whereas the average is not .", "In the Robocup domain , the actor typically precedes the recipient in passing event records .", "Formally , we have a Markov model over the fields 3 Each record type has a dedicated null field with its own multinomial distribution over words , intended to model words which refer to that record type in general e . g . , the word passes for passing records .", "We also model transitions into the first field and transitions out of the final field with special START and STOP fields .", "This Markov structure allows us to capture a few elements of rudimentary syntax .", "We arrive at the final component of our model , which governs how the information about a particular field of a record is rendered into words .", "For each field fij , we generate the number of words cij from a uniform distribution over 11 , 2 , . . . , Cmax , where Cmax is set larger than the length of the longest text we expect to see .", "Conditioned on the fields f , the words w are generated independently 4 where r k and f k are the record and field responsible for generating word wk , as determined by the segmentation c . The word choice model pw w I t , v specifies a distribution over words given the field type t and field value v . This distribution is a mixture of a global backoff distribution over words and a field specific distribution which depends on the field type t . Although we designed our word choice model to be relatively general , it is undoubtedly influenced by the three domains .", "However , we can readily extend or replace it with an alternative if desired ; this modularity is one principal benefit of probabilistic modeling .", "Integer Fields t INT For integer fields , we want to capture the intuition that a numeric quantity v is rendered in the text as a word which is possibly some other numerical value w due to stylistic factors .", "Sometimes the exact value v is used e . g . , in reporting football statistics .", "Other times , it might be customary to round v e . g . , wind speeds are typically rounded to a multiple of 5 .", "In other cases , there might just be some unexplained error , where w deviates from v by some noise c w v 0 or c v w 0 .", "We model c and c as geometric distributions . 5 In summary , we allow six possible ways of generating the word w given v v rv15 LvI5 round5 v v c v c Separate probabilities for choosing among these possibilities are learned for each field type see Figure 3 for an example .", "String Fields t STR Strings fields are intended to represent values which we expect to be realized in the text via a simple surface level transformation .", "For example , a name field with value v Moe Williams is sometimes referenced in the text by just Williams .", "We used a simple generic model of rendering string fields Let w be a word chosen uniformly from those in v . Categorical Fields t CAT Unlike string fields , categorical fields are not tied down to any lexical representation ; in fact , the identities of the categorical field values are irrelevant .", "For each categorical field f and possible value v , we have a , clear mostly sunny partly , cloudy increasing mostly cloudy , partly of inch an possible new a rainfall herence structure at both the record and field levels .", "To quantify the benefits of incorporating these two aspects , we compare our full model with two simpler variants . skyCover . mode in the weather domain .", "It is interesting to note that skyCover 75 100 is so highly correlated with rain that the model learns to connect an overcast sky in the world to the indication of rain in the text . separate multinomial distribution over words from which w is drawn .", "An example of a categorical field is skyCover . mode in the weather domain , which has four values 0 25 , 25 50 , 50 75 , and 75 100 .", "Table 2 shows the top words for each of these field values learned by our model .", "Our learning and inference methodology is a fairly conventional application of Expectation Maximization EM and dynamic programming .", "The input is a set of scenarios D , each of which is a text w paired with a world state s . We maximize the marginal likelihood of our data , summing out the latent variables r , f , c where 0 are the parameters of the model all the multinomial probabilities .", "We use the EM algorithm to maximize 3 , which alternates between the E step and the M step .", "In the E step , we compute expected counts according to the posterior p r , f , c w , s ; 0 .", "In the M step , we optimize the parameters 0 by normalizing the expected counts computed in the E step .", "In our experiments , we initialized EM with a uniform distribution for each multinomial and applied add 0 . 1 smoothing to each multinomial in the M step .", "As with most complex discrete models , the bulk of the work is in computing expected counts under p r , f , c w , s ; 0 .", "Formally , our model is a hierarchical hidden semi Markov model conditioned on s . Inference in the E step can be done using a dynamic program similar to the inside outside algorithm .", "Two important aspects of our model are the segmentation of the text and the modeling of the coIn the annotated data , each text w has been divided into a set of lines .", "These lines correspond to clauses in the weather domain and sentences in the Robocup and NFL domains .", "Each line is annotated with a possibly empty set of records .", "Let A be the gold set of these line record alignment pairs .", "To evaluate a learned model , we compute the Viterbi segmentation and alignment argmaxr , f , c p r , f , c w , s .", "We produce a predicted set of line record pairs A' by aligning a line to a record ri if the span of the utterance corresponding to ri overlaps the line .", "The reason we evaluate indirectly using lines rather than using utterances is that it is difficult to annotate the segmentation of text into utterances in a simple and consistent manner .", "We compute standard precision , recall , and F1 of A' with respect to A .", "Unless otherwise specified , performance is reported on all scenarios , which were also used for training .", "However , we did not tune any hyperparameters , but rather used generic values which worked well enough across all three domains .", "We ran 10 iterations of EM on Models 1 3 .", "Table 3 shows that performance improves with increased model sophistication .", "We also compare our model to the results of Chen and Mooney 2008 in Table 4 .", "Figure 4 provides a closer look at the predictions made by each of our three models for a particular example .", "Model 1 easily mistakes pink10 for the recipient of a pass record because decisions are made independently for each word .", "Model 2 chooses the correct record , but having no model of the field structure inside a record , it proposes an incorrect field segmentation although our evaluation is insensitive to this .", "Equipped with the ability to prefer a coherent field sequence , Model 3 fixes these errors .", "Many of the remaining errors are due to the garbage collection phenomenon familiar from word alignment models Moore , 2004 ; Liang et al . , 2006 .", "For example , the ballstopped record occurs frequently but is never mentioned in the text .", "At the same time , there is a correlation between ballstopped and utterances such as pink2 holds onto the ball , which are not aligned to any record in the annotation .", "As a result , our model incorrectly chooses to align the two .", "For the weather domain , staged training was necessary to get good results .", "For Model 1 , we ran 15 iterations of EM .", "For Model 2 , we ran 5 iterations of EM on Model 1 , followed by 10 iterations on Model 2 .", "For Model 3 , we ran 5 iterations of Model 1 , 5 iterations of a simplified variant of Model 3 where records were chosen independently , and finally , 5 iterations of Model 3 .", "When going from one model to another , we used the final posterior distributions of the former to initialize the parameters of the latter . 6 We also prohibited utterances in Models 2 and 3 from crossing punctuation during inference .", "Table 5 shows that performance improves substantially in the more sophisticated models , the gains being greater than in the Robocup domain .", "Figure 5 shows the predictions of the three models on an example .", "Model 1 is only able to form isolated but not completely inaccurate associations .", "By modeling segmentation , Model 2 accounts for the intermediate words , but errors are still made due to the lack of Markov structure .", "Model 3 remedies this .", "However , unexpected structures are sometimes learned .", "For example , the temperature . time 6 21 field indicates daytime , which happens to be perfectly correlated with the word high , although high intuitively should be associated with the temperature . max field .", "In these cases of high correlation Table 2 provides another example , it is very difficult to recover the proper alignment without additional supervision .", "In order to scale up our models to the NFL domain , we first pruned for each sentence the records which have either no numerical values e . g . , 23 , 23 10 , 2 4 nor name like words e . g . , those that appear only capitalized in the text in common .", "This eliminated all but 1 . 5 of the record candidates per sentence , while maintaining an oracle alignment F1 score of 88 . 7 .", "Guessing a single random record for each sentence yields an F1 of 12 . 0 .", "A reasonable heuristic which uses weighted number and string matching achieves 26 . 7 .", "Due to the much greater complexity of this domain , Model 2 was easily misled as it tried without success to find a coherent segmentation of the fields .", "We therefore created a variant , Model 2 , where we constrained each field to generate exactly one word .", "To train Model 2 , we ran 5 iterations of EM where each sentence is assumed to have exactly one record , followed by 5 iterations where the constraint was relaxed to also allow record boundaries at punctuation and the word and .", "We did not experiment with Model 3 since the discourse structure on records in this domain is not at all governed by a simple Markov model on record types indeed , most regions do not refer to any records at all .", "We also fixed the backoff probability to 0 . 1 instead of learning it and enforced zero numerical deviation on integer field values .", "Model 2 achieved an F1 of 39 . 9 , an improvement over Model 1 , which attained 32 . 8 .", "Inspection of the errors revealed the following problem The alignment task requires us to sometimes align a sentence to multiple redundant records e . g . , play and score referenced by the same part of the text .", "However , our model generates each part of text from only one record , and thus it can only allow an alignment to one record . 7 To cope with this incompatibility between the data and our notion of semantics , we used the following solution We divided the records into three groups by type play , score , and other .", "Each group has a copy of the model , but we enforce that they share the same segmentation .", "We also introduce a potential that couples the presence or absence of records across groups on the same segment to capture regular cooccurrences between redundant records .", "Table 6 shows our results .", "With groups , we achieve an F1 of 53 . 2 .", "Though we still trail supervised techniques , which attain numbers in the 68 80 range , we have made substantial progress over our baseline using an unsupervised method .", "Furthermore , our model provides a more detailed analysis of the correspondence between the world state and text , rather than just producing a single alignment decision .", "Most of the remaining errors made by our model are due to a lack of calibration .", "Sometimes , our false positives are close calls where a sentence indirectly references a record , and our model predicts the alignment whereas the annotation standard does not .", "We believe that further progress is possible with a richer model .", "We have presented a generative model of correspondences between a world state and an unsegmented stream of text .", "By having a joint model of salience , coherence , and segmentation , as well as a detailed rendering of the values in the world state into words in the text , we are able to cope with the increased ambiguity that arises in this new data setting , successfully pushing the limits of unsupervision ."], "summary_lines": ["Learning Semantic Correspondences with Less Supervision\n", "A central problem in grounded language acquisition is learning the correspondences between a rich world state and a stream of text which references that world state.\n", "To deal with the high degree of ambiguity present in this setting, we present a generative model that simultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state.\n", "We show that our model generalizes across three domains of increasing difficulty--Robocup sportscasting, weather forecasts (a new domain), and NFL recaps.\n", "We propose a probabilistic generative approach to produce a Viterbi alignment between NL and MRs. We use a hierarchical semi-Markov generative model that first determines which facts to discuss and then generates words from the predicates and arguments of the chosen facts.\n"]}
{"article_lines": ["Manual And Automatic Evaluation Of Machine Translation Between European Languages", "Adequacy rank Fluency rank BLEU rank upc jmc 1 7 1 8 1 6 lcc 1 6 1 7 1 4 utd 1 7 1 6 2 7 upc mr 1 8 1 6 1 7 nrc 1 7 2 6 8 ntt 1 8 2 8 1 7 cmu 3 7 4 8 2 7 rali 5 8 3 9 3 7 systran 9 8 9 10 upv 10 10 9 Spanish English In Domain Adequacy rank Fluency rank BLEU rank upc jmc 1 7 1 6 1 5 ntt 1 7 1 8 1 5 lcc 1 8 2 8 1 4 utd 1 8 2 7 1 5 nrc 2 8 1 9 6 upc mr 1 8 1 6 7 uedin birch 1 8 2 10 8 rali 3 9 3 9 2 5 upc jg 7 9 6 9 9 upv 10 9 10 10 German English In Domain Adequacy rank Fluency rank BLEU rank uedin phi 1 2 1 1 lcc 2 7 2 7 2 nrc 2 7 2 6 5 7 utd 3 7 2 8 3 4 ntt 2 9 2 8 3 4 upc mr 3 9 6 9 8 rali 4 9 3 9 5 7 upc jmc 2 9 3 9 5 7 systran 3 9 3 9 10 upv 10 10 9 Figure 7 Evaluation of translation to English on in domain test data 112 English French In Domain Adequacy rank Fluency rank BLEU rank nrc 1 5 1 5 1 6 upc mr 1 4 1 5 1 6 upc jmc 1 6 1 6 1 5 systran 2 7 1 6 7 utd 3 7 3 7 3 6 rali 1 7 2 7 1 6 ntt 4 7 4 7 1 5 English Spanish In Domain Adequacy rank Fluency rank BLEU rank ms 1 5 1 7 7 8 upc mr 1 4 1 5 1 4 utd 1 5 1 6 1 4 nrc 2 7 1 6 5 6 ntt 3 7 1 6 1 4 upc jmc 2 7 2 7 1 4 rali 5 8 6 8 5 6 uedin birch 6 9 6 10 7 8 upc jg 9 8 10 9 upv 9 10 8 10 10 English German In Domain Adequacy rank Fluency rank BLEU rank upc mr 1 3 1 5 3 5 ntt 1 5 2 6 1 3 upc jmc 1 5 1 4 1 3 nrc 2 4 1 5 4 5 rali 3 6 2 6 1 4 systran 5 6 3 6 7 upv 7 7 6 Figure 8 Evaluation of translation from English on in domain test data 113 French English Out of Domain Adequacy rank Fluency rank BLEU rank upc jmc 1 5 1 8 1 4 cmu 1 8 1 9 4 7 systran 1 8 1 7 9 lcc 1 9 1 9 1 5 upc mr 2 8 1 7 1 3 utd 1 9 1 8 3 7 ntt 3 9 1 9 3 7 nrc 3 8 3 9 3 7 rali 4 9 5 9 8 upv 10 10 10 Spanish English Out of Domain Adequacy rank Fluency rank BLEU rank upc jmc 1 2 1 6 1 3 uedin birch 1 7 1 6 5 8 nrc 2 8 1 8 5 7 ntt 2 7 2 6 3 4 upc mr 2 8 1 7 5 8 lcc 4 9 3 7 1 4 utd 2 9 2 8 1 3 upc jg 4 9 7 9 9 rali 4 9 6 9 6 8 upv 10 10 10 German English Out of Domain Adequacy rank Fluency rank BLEU rank systran 1 4 1 4 7 9 uedin phi 1 6 1 7 1 lcc 1 6 1 7 2 3 utd 2 7 2 6 4 6 ntt 1 9 1 7 3 5 nrc 3 8 2 8 7 8 upc mr 4 8 6 8 4 6 upc jmc 4 8 3 9 2 5 rali 8 9 8 9 8 9 upv 10 10 10 Figure 9 Evaluation of translation to English on out of domain test data 114 English French Out of Domain Adequacy rank Fluency rank BLEU rank systran 1 1 1 upc jmc 2 5 2 4 2 6 upc mr 2 4 2 4 2 6 utd 2 6 2 6 7 rali 4 7 5 7 2 6 nrc 4 7 4 7 2 5 ntt 4 7 4 7 3 6 English Spanish Out of Domain Adequacy rank Fluency rank BLEU rank upc mr 1 3 1 6 1 2 ms 1 7 1 8 6 7 utd 2 6 1 7 3 5 nrc 1 6 2 7 3 5 upc jmc 2 7 1 6 3 5 ntt 2 7 1 7 1 2 rali 6 8 4 8 6 8 uedin birch 6 10 5 9 7 8 upc jg 8 9 9 10 9 upv 9 8 9 10 English German Out of Domain Adequacy rank Fluency rank BLEU rank systran 1 1 2 1 6 upc mr 2 3 1 3 1 5 upc jmc 2 3 3 6 1 6 rali 4 6 4 6 1 6 nrc 4 6 2 6 2 6 ntt 4 6 3 5 1 6 upv 7 7 7 Figure 10 Evaluation of translation from English on out of domain test data 115 French English In domain Out of Domain Adequacy Adequacy 0 . 3 0 . 3 0 . 2 0 . 2 0 . 1 0 . 1 0 . 0 0 . 0 0 . 1 0 . 1 0 . 2 0 . 2 0 . 3 0 . 3 0 . 4 0 . 4 0 . 5 0 . 5 0 . 6 0 . 6 0 . 7 0 . 7 upv 0 . 8 0 . 8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 upv systran upcntt rali upc jmc cc Fluency 0 . 2 0 . 1 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 upv 0 . 5 systran upv upc jmc Fluency 0 . 2 0 . 1 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 td t cc upc rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11 Correlation between manual and automatic scores for French English 116 Spanish English Figure 12 Correlation between manual and automatic scores for Spanish English 0 . 3 0 . 4 0 . 2 0 . 1 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 upv 0 . 4 upv 0 . 3 In Domain upc jg Adequacy 0 . 3 0 . 2 0 . 1 0 . 0 0 . 1 0 . 2 Out of Domain upc jmc nrc ntt Adequacy upc jmc lcc rali rali 0 . 7 0 . 5 0 . 6 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 rali Fluency 0 . 2 0 . 1 0 . 0 0 . 1 0 . 2 ntt upc mr lcc utd upc jg rali Fluency 0 . 2 0 . 1 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 upc jmc uedin birch 0 . 5 0 . 5 upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German English 15 16 17 18 19 20 21 22 23 24 25 26 27 0 . 4 0 . 3 0 . 2 0 . 1 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 lcc upc jmc systran upv Fluency ula upc mr lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0 . 4 0 . 3 0 . 2 0 . 1 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 systran upv uedin phi jmc rali systran 0 . 3 0 . 4 0 . 5 0 . 6 upv 12 13 14 15 16 17 18 19 20 0 . 4 0 . 3 0 . 2 0 . 1 0 . 0 0 . 1 0 . 2 Fluency uedin phi utd upc jmc upc mr 0 . 4 rali 0 . 3 0 . 4 0 . 5 upv 12 13 14 15 16 17 18 19 20 0 . 3 0 . 2 0 . 1 0 . 0 0 . 1 0 . 2 English French In Domain Out of Domain Adequacy Adequacy .", "0 . 2 0 . 1 0 . 0 0 . 1 25 26 27 28 29 30 31 32 0 . 2 0 . 3 systran ntt 0 . 5 0 . 4 0 . 3 0 . 2 0 . 1 0 . 0 0 . 1 0 . 2 0 . 3 20 21 22 23 24 25 26 Fluency Fluency systran nrc rali 25 26 27 28 29 30 31 32 0 . 2 0 . 1 0 . 0 0 . 1 0 . 2 0 . 3 cme p 20 21 22 23 24 25 26 0 . 5 0 . 4 0 . 3 0 . 2 0 . 1 0 . 0 0 . 1 0 . 2 0 . 3 Figure 14 Correlation between manual and automatic scores for English French 119 In Domain Out of Domain upv Adequacy 0 . 9 0 . 3 0 . 2 0 . 1 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 upv 23 24 25 26 27 28 29 30 31 32 upc mr utd upc jmc uedin birch ntt rali uedin birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy upc mr 0 . 4 0 . 3 0 . 2 0 . 1 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 . 0 1 . 1 English Spanish Fluency ntt nrc rali uedin birch 0 . 2 0 . 3 0 . 5 upv 16 17 18 19 20 21 22 23 24 25 26 27 0 . 4 nr rali Fluency 0 . 4 upc mr utd upc jmc 0 . 5 0 . 6 upv 23 24 25 26 27 28 29 30 31 32 0 . 2 0 . 1 0 . 0 0 . 1 0 . 2 0 . 3 0 . 3 0 . 2 0 . 1 0 . 0 0 . 1 0 . 6 0 . 7 Figure 15 Correlation between manual and automatic scores for English Spanish 120 English German In Domain Out of Domain Adequacy Adequacy 0 . 3 0 . 2 0 . 1 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 upv 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 upv 0 . 5 0 . 4 systran upc mr rali 0 . 3 ntt 0 . 2 0 . 1 0 . 0 0 . 1 systran upc mr 0 . 9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0 . 2 0 . 1 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 upv 0 . 5 upv systran upc mr Fluency 0 . 4 0 . 3 0 . 2 0 . 1 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 systran ntt", "was done by the participants .", "This revealed interesting clues about the properties of automatic and manual scoring .", "We evaluated translation from English , in addition to into English .", "English was again paired with German , French , and Spanish .", "We dropped , however , one of the languages , Finnish , partly to keep the number of tracks manageable , partly because we assumed that it would be hard to find enough Finnish speakers for the manual evaluation .", "The evaluation framework for the shared task is similar to the one used in last year s shared task .", "Training and testing is based on the Europarl corpus .", "Figure 1 provides some statistics about this corpus .", "To lower the barrier of entrance to the competition , we provided a complete baseline MT system , along with data resources .", "To summarize , we provided The performance of the baseline system is similar to the best submissions in last year s shared task .", "We are currently working on a complete open source implementation of a training and decoding system , which should become available over the summer . pus , from which also the in domain test set is taken .", "There is twice as much language modelling data , since training data for the machine translation system is filtered against sentences of length larger than 40 words .", "Out of domain test data is from the Project Syndicate web site , a compendium of political commentary .", "The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000 , which is excluded from the training data .", "Participants were also provided with two sets of 2 , 000 sentences of parallel text to be used for system development and tuning .", "In addition to the Europarl test set , we also collected 29 editorials from the Project Syndicate website2 , which are published in all the four languages of the shared task .", "We aligned the texts at a sentence level across all four languages , resulting in 1064 sentence per language .", "For statistics on this test set , refer to Figure 1 .", "The out of domain test set differs from the Europarl data in various ways .", "The text type are editorials instead of speech transcripts .", "The domain is general politics , economics and science .", "However , it is also mostly political content even if not focused on the internal workings of the European Union and opinion .", "We received submissions from 14 groups from 11 institutions , as listed in Figure 2 .", "Most of these groups follow a phrase based statistical approach to machine translation .", "Microsoft s approach uses dependency trees , others use hierarchical phrase models .", "Systran submitted their commercial rule based system that was not tuned to the Europarl corpus .", "About half of the participants of last year s shared task participated again .", "The other half was replaced by other participants , so we ended up with roughly the same number .", "Compared to last year s shared task , the participants represent more long term research efforts .", "This may be the sign of a maturing research environment .", "While building a machine translation system is a serious undertaking , in future we hope to attract more newcomers to the field by keeping the barrier of entry as low as possible .", "For more on the participating systems , please refer to the respective system description in the proceedings of the workshop .", "For the automatic evaluation , we used BLEU , since it is the most established metric in the field .", "The BLEU metric , as all currently proposed automatic metrics , is occasionally suspected to be biased towards statistical systems , especially the phrase based systems currently in use .", "It rewards matches of n gram sequences , but measures only at most indirectly overall grammatical coherence .", "The BLEU score has been shown to correlate well with human judgement , when statistical machine translation systems are compared Doddington , 2002 ; Przybocki , 2004 ; Li , 2005 .", "However , a recent study Callison Burch et al . , 2006 , pointed out that this correlation may not always be strong .", "They demonstrated this with the comparison of statistical systems against a manually post edited MT output , and b a rule based commercial system .", "The development of automatic scoring methods is an open field of research .", "It was our hope that this competition , which included the manual and automatic evaluation of statistical systems and one rulebased commercial system , will give further insight into the relation between automatic and manual evaluation .", "At the very least , we are creating a data resource the manual annotations that may the basis of future research in evaluation metrics .", "We computed BLEU scores for each submission with a single reference translation .", "For each sentence , we counted how many n grams in the system output also occurred in the reference translation .", "By taking the ratio of matching n grams to the total number of n grams in the system output , we obtain the precision pn for each n gram order n . These values for n gram precision are combined into a BLEU score The formula for the BLEU metric also includes a brevity penalty for too short output , which is based on the total number of words in the system output c and in the reference r . BLEU is sensitive to tokenization .", "Because of this , we retokenized and lowercased submitted output with our own tokenizer , which was also used to prepare the training and test data .", "Confidence Interval Since BLEU scores are not computed on the sentence level , traditional methods to compute statistical significance and confidence intervals do not apply .", "Hence , we use the bootstrap resampling method described by Koehn 2004 .", "Following this method , we repeatedly say , 1000 times sample sets of sentences from the output of each system , measure their BLEU score , and use these 1000 BLEU scores as basis for estimating a confidence interval .", "When dropping the top and bottom 2 . 5 the remaining BLEU scores define the range of the confidence interval .", "Pairwise comparison We can use the same method to assess the statistical significance of one system outperforming another .", "If two systems scores are close , this may simply be a random effect in the test data .", "To check for this , we do pairwise bootstrap resampling Again , we repeatedly sample sets of sentences , this time from both systems , and compare their BLEU scores on these sets .", "If one system is better in 95 of the sample sets , we conclude that its higher BLEU score is statistically significantly better .", "The bootstrap method has been critized by Riezler and Maxwell 2005 and Collins et al . 2005 , as being too optimistic in deciding for statistical significant difference between systems .", "We are therefore applying a different method , which has been used at the 2005 DARPA NIST evaluation .", "We divide up each test set into blocks of 20 sentences 100 blocks for the in domain test set , 53 blocks for the out of domain test set , check for each block , if one system has a higher BLEU score than the other , and then use the sign test .", "The sign test checks , how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance .", "Let say , if we find one system doing better on 20 of the blocks , and worse on 80 of the blocks , is it significantly worse ?", "We check , how likely only up to k 20 better scores out of n 100 would have been generated by two equal systems , using the binomial distribution If p 0 . . k ; n , p 0 . 05 , or p 0 . . k ; n , p 0 . 95 then we have a statistically significant difference between the systems .", "While automatic measures are an invaluable tool for the day to day development of machine translation systems , they are only a imperfect substitute for human assessment of translation quality , or as the acronym BLEU puts it , a bilingual evaluation understudy .", "Many human evaluation metrics have been proposed .", "Also , the argument has been made that machine translation performance should be evaluated via task based evaluation metrics , i . e . how much it assists performing a useful task , such as supporting human translators or aiding the analysis of texts .", "The main disadvantage of manual evaluation is that it is time consuming and thus too expensive to do frequently .", "In this shared task , we were also confronted with this problem , and since we had no funding for paying human judgements , we asked participants in the evaluation to share the burden .", "Participants and other volunteers contributed about 180 hours of labor in the manual evaluation .", "We asked participants to each judge 200 300 sentences in terms of fluency and adequacy , the most commonly used manual evaluation metrics .", "We settled on contrastive evaluations of 5 system outputs for a single test sentence .", "See Figure 3 for a screenshot of the evaluation tool .", "Presenting the output of several system allows the human judge to make more informed judgements , contrasting the quality of the different systems .", "The judgements tend to be done more in form of a ranking of the different systems .", "We assumed that such a contrastive assessment would be beneficial for an evaluation that essentially pits different systems against each other .", "While we had up to 11 submissions for a translation direction , we did decide against presenting all 11 system outputs to the human judge .", "Our initial experimentation with the evaluation tool showed that this is often too overwhelming .", "Making the ten judgements 2 types for 5 systems takes on average 2 minutes .", "Typically , judges initially spent about 3 minutes per sentence , but then accelerate with experience .", "Judges where excluded from assessing the quality of MT systems that were submitted by their institution .", "Sentences and systems were randomly selected and randomly shuffled for presentation .", "We collected around 300 400 judgements per judgement type adequacy or fluency , per system , per language pair .", "This is less than the 694 judgements 2004 DARPA NIST evaluation , or the 532 judgements in the 2005 DARPA NIST evaluation .", "This decreases the statistical significance of our results compared to those studies .", "The number of judgements is additionally fragmented by our breakup of sentences into in domain and out of domain .", "The human judges were presented with the following definition of adequacy and fluency , but no additional instructions", "Judges varied in the average score they handed out .", "The average fluency judgement per judge ranged from 2 . 33 to 3 . 67 , the average adequacy judgement ranged from 2 . 56 to 4 . 13 .", "Since different judges judged different systems recall that judges were excluded to judge system output from their own institution , we normalized the scores .", "The normalized judgement per judge is the raw judgement plus 3 minus average raw judgement for this judge .", "In words , the judgements are normalized , so that the average normalized judgement per judge is 3 .", "Another way to view the judgements is that they are less quality judgements of machine translation systems per se , but rankings of machine translation systems .", "In fact , it is very difficult to maintain consistent standards , on what say an adequacy judgement of 3 means even for a specific language pair .", "The way judgements are collected , human judges tend to use the scores to rank systems against each other .", "If one system is perfect , another has slight flaws and the third more flaws , a judge is inclined to hand out judgements of 5 , 4 , and 3 .", "On the other hand , when all systems produce muddled output , but one is better , and one is worse , but not completely wrong , a judge is inclined to hand out judgements of 4 , 3 , and 2 .", "The judgement of 4 in the first case will go to a vastly better system output than in the second case .", "We therefore also normalized judgements on a per sentence basis .", "The normalized judgement per sentence is the raw judgement plus 0 minus average raw judgement for this judge on this sentence .", "Systems that generally do better than others will receive a positive average normalizedjudgement per sentence .", "Systems that generally do worse than others will receive a negative one .", "One may argue with these efforts on normalization , and ultimately their value should be assessed by assessing their impact on inter annotator agreement .", "Given the limited number of judgements we received , we did not try to evaluate this .", "Confidence Interval To estimate confidence intervals for the average mean scores for the systems , we use standard significance testing .", "Given a set of n sentences , we can compute the sample mean x and sample variance s2 of the individual sentence judgements xi The extend of the confidence interval x d , x df can be computed by d 1 . 96 n 6 Pairwise Comparison As for the automatic evaluation metric , we want to be able to rank different systems against each other , for which we need assessments of statistical significance on the differences between a pair of systems .", "Unfortunately , we have much less data to work with than with the automatic scores .", "The way we cant distinction between system performance .", "Automatic scores are computed on a larger tested than manual scores 3064 sentences vs . 300 400 sentences . collected manual judgements , we do not necessarily have the same sentence judged for both systems judges evaluate 5 systems out of the 8 10 participating systems .", "Still , for about good number of sentences , we do have this direct comparison , which allows us to apply the sign test , as described in Section 2 . 2 .", "The results of the manual and automatic evaluation of the participating system translations is detailed in the figures at the end of this paper .", "The scores and confidence intervals are detailed first in the Figures 7 10 in table form including ranks , and then in graphical form in Figures 11 16 .", "In the graphs , system scores are indicated by a point , the confidence intervals by shaded areas around the point .", "In all figures , we present the per sentence normalized judgements .", "The normalization on a per judge basis gave very similar ranking , only slightly less consistent with the ranking from the pairwise comparisons .", "The confidence intervals are computed by bootstrap resampling for BLEU , and by standard significance testing for the manual scores , as described earlier in the paper .", "Pairwise comparison is done using the sign test .", "Often , two systems can not be distinguished with a confidence of over 95 , so there are ranked the same .", "This actually happens quite frequently more below , so that the rankings are broad estimates .", "For instance if 10 systems participate , and one system does better than 3 others , worse then 2 , and is not significant different from the remaining 4 , its rank is in the interval 3 7 .", "At first glance , we quickly recognize that many systems are scored very similar , both in terms of manual judgement and BLEU .", "There may be occasionally a system clearly at the top or at the bottom , but most systems are so close that it is hard to distinguish them .", "In Figure 4 , we displayed the number of system comparisons , for which we concluded statistical significance .", "For the automatic scoring method BLEU , we can distinguish three quarters of the systems .", "While the Bootstrap method is slightly more sensitive , it is very much in line with the sign test on text blocks .", "For the manual scoring , we can distinguish only half of the systems , both in terms of fluency and adequacy .", "More judgements would have enabled us to make better distinctions , but it is not clear what the upper limit is .", "We can check , what the consequences of less manual annotation of results would have been With half the number of manual judgements , we can distinguish about 40 of the systems , 10 less .", "The test set included 2000 sentences from the Europarl corpus , but also 1064 sentences out ofdomain test data .", "Since the inclusion of out ofdomain test data was a very late decision , the participants were not informed of this .", "So , this was a surprise element due to practical reasons , not malice .", "All systems except for Systran , which was not tuned to Europarl did considerably worse on outof domain training data .", "This is demonstrated by average scores over all systems , in terms of BLEU , fluency and adequacy , as displayed in Figure 5 .", "The manual scores are averages over the raw unnormalized scores .", "It is well know that language pairs such as EnglishGerman pose more challenges to machine translation systems than language pairs such as FrenchEnglish .", "Different sentence structure and rich target language morphology are two reasons for this .", "Again , we can compute average scores for all systems for the different language pairs Figure 6 .", "The differences in difficulty are better reflected in the BLEU scores than in the raw un normalized manual judgements .", "The easiest language pair according to BLEU English French 28 . 33 received worse manual scores than the hardest English German 14 . 01 .", "This is because different judges focused on different language pairs .", "Hence , the different averages of manual scores for the different language pairs reflect the behaviour of the judges , not the quality of the systems on different language pairs .", "Given the closeness of most systems and the wide over lapping confidence intervals it is hard to make strong statements about the correlation between human judgements and automatic scoring methods such as BLEU .", "We confirm the finding by Callison Burch et al . 2006 that the rule based system of Systran is not adequately appreciated by BLEU .", "In domain Systran scores on this metric are lower than all statistical systems , even the ones that have much worse human scores .", "Surprisingly , this effect is much less obvious for out of domain test data .", "For instance , for out ofdomain English French , Systran has the best BLEU and manual scores .", "Our suspicion is that BLEU is very sensitive to jargon , to selecting exactly the right words , and not synonyms that human judges may appreciate as equally good .", "This is can not be the only explanation , since the discrepancy still holds , for instance , for out of domain French English , where Systran receives among the best adequacy and fluency scores , but a worse BLEU score than all but one statistical system .", "This data set of manual judgements should provide a fruitful resource for research on better automatic scoring methods .", "So , who won the competition ?", "The best answer to this is many research labs have very competitive systems whose performance is hard to tell apart .", "This is not completely surprising , since all systems use very similar technology .", "For some language pairs such as GermanEnglish system performance is more divergent than for others such as English French , at least as measured by BLEU .", "The statistical systems seem to still lag behind the commercial rule based competition when translating into morphological rich languages , as demonstrated by the results for English German and English French .", "The predominate focus of building systems that translate into English has ignored so far the difficult issues of generating rich morphology which may not be determined solely by local context .", "This is the first time that we organized a large scale manual evaluation .", "While we used the standard metrics of the community , the we way presented translations and prompted for assessment differed from other evaluation campaigns .", "For instance , in the recent IWSLT evaluation , first fluency annotations were solicited while withholding the source sentence , and then adequacy annotations .", "Almost all annotators reported difficulties in maintaining a consistent standard for fluency and adequacy judgements , but nevertheless most did not explicitly move towards a ranking based evaluation .", "Almost all annotators expressed their preference to move to a ranking based evaluation in the future .", "A few pointed out that adequacy should be broken up into two criteria a are all source words covered ?", "b does the translation have the same meaning , including connotations ?", "Annotators suggested that long sentences are almost impossible to judge .", "Since all long sentence translation are somewhat muddled , even a contrastive evaluation between systems was difficult .", "A few annotators suggested to break up long sentences into clauses and evaluate these separately .", "Not every annotator was fluent in both the source and the target language .", "While it is essential to be fluent in the target language , it is not strictly necessary to know the source language , if a reference translation was given .", "However , ince we extracted the test corpus automatically from web sources , the reference translation was not always accurate due to sentence alignment errors , or because translators did not adhere to a strict sentence by sentence translation say , using pronouns when referring to entities mentioned in the previous sentence .", "Lack of correct reference translations was pointed out as a short coming of our evaluation .", "One annotator suggested that this was the case for as much as 10 of our test sentences .", "Annotators argued for the importance of having correct and even multiple references .", "It was also proposed to allow annotators to skip sentences that they are unable to judge .", "We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs .", "While many systems had similar performance , the results offer interesting insights , especially about the relative performance of statistical and rule based systems .", "Due to many similarly performing systems , we are not able to draw strong conclusions on the question of correlation of manual and automatic evaluation metrics .", "The bias of automatic methods in favor of statistical systems seems to be less pronounced on out of domain test data .", "The manual evaluation of scoring translation on a graded scale from 1 5 seems to be very hard to perform .", "Replacing this with an ranked evaluation seems to be more suitable .", "Human judges also pointed out difficulties with the evaluation of long sentences .", "This work was supported in part under the GALE program of the Defense Advanced Research Projects Agency , Contract No .", "HR0011 06 C 0022 ."], "summary_lines": ["Manual And Automatic Evaluation Of Machine Translation Between European Languages\n", "We evaluated machine translation performance for six European language pairs that participated in a shared task: translating French, German, Spanish texts to English and back.\n", "Evaluation was done automatically using the BLEU score and manually on fluency and adequacy.\n", "The results of the workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems.\n", "We report and analyze several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric.\n"]}
{"article_lines": ["An Algorithm For Finding Noun Phrase Correspondences In Bilingual Corpora", "The paper describes an algorithm that employs English and French text taggers to associate noun phrases in an aligned bilingual corpus .", "The taggers provide part of speech categories which are used by finite state recognizers to extract simple noun phrases for both languages .", "Noun phrases are then mapped to each other using an iterative re estimation algorithm that bears similarities to the Baum Welch algorithm which is used for training the taggers .", "The algorithm provides an alternative to other approaches for finding word correspondences , with the advantage that linguistic structure is incorporated .", "Improvements to the basic algorithm are described , which enable context to be accounted for when constructing the noun phrase mappings .", "Areas of investigation using bilingual corpora have included the following The work described here makes use of the aligned Canadian Hansards Gale and Church , 1991b to obtain noun phrase correspondences between the English and French text .", "The term quot ; correspondence quot ; is used here to signify a mapping between words in two aligned sentences .", "Consider an English sentence Ei and a French sentence Fi which are assumed to be approximate translations of each other .", "The subscript i denotes the i'th alignment of sentences in both languages .", "A word sequence in Ei is defined here as the correspondence of another sequence in Fi if the words of one sequence are considered to represent the words in the other .", "Single word correspondences have been investigated Gale and Church , 1991a using a statistic operating on contingency tables .", "An algorithm for producing collocational correspondences has also been described Smadja , 1992 .", "The algorithm involves several steps .", "English collocations are first extracted from the English side of the corpus .", "Instances of the English collocation are found and the mutual information is calculated between the instances and various single word candidates in aligned French sentences .", "The highest ranking candidates are then extended by another word and the procedure is repeated until a corresponding French collocation having the highest mutual information is found .", "An alternative approach is described here , which employs simple iterative re estimation .", "It is used to make correspondences between simple noun phrases that have been isolated in corresponding sentences of each language using finitestate recognizers .", "The algorithm is applicable for finding single or multiple word correspondences and can accommodate additional kinds of phrases .", "In contrast to the other methods that have been mentioned , the algorithm can be extended in a straightforward way to enable correct correspondences to be made in circumstances where numerous low frequency phrases are involved .", "This is important consideration because in large text corpora roughly a third of the word types only occur once .", "Several applications for bilingual correspondence information have been suggested .", "They can be used in bilingual concordances , for automatically constructing bilingual lexicons , and probabilistically quantified correspondences may be useful for statistical translation methods .", "Figure 1 illustrates how the corpus is analyzed .", "The words in sentences are first tagged with their corresponding part of speech categories .", "Each tagger contains a hidden Markov model HMM , which is trained using samples of raw text from the Hansards for each language .", "The taggers are robust and operate with a low error rate Kupiec , 19921 .", "Simple noun phrases excluding pronouns and digits are then extracted from the sentences by finite state recognizers that are specified by regular expressions defined in terms of part ofspeech categories .", "Simple noun phrases are identified because they are most reliably recognized ; it is also assumed that they can be identified unambiguously .", "The only embedding that is allowed is by prepositional phrases involving quot ; of' in English and quot ; de quot ; in French , as noun phrases involving them can be identified with relatively low error revisions to this restriction are considered later .", "Noun phrases are placed in an index to associate a unique identifier with each one .", "A noun phrase is defined by its word sequence , excluding any leading determiners .", "Singular and plural forms of common nouns are thus distinct and assigned different positions in the index .", "For each sentence corresponding to an alignment , the index positions of all noun phrases in the sentence are recorded in a separate data structure , providing a compact representation of the corpus .", "So far it has been assumed for the sake of simplicity that there is always a one to one mapping between English and French sentences .", "In practice , if an alignment program produces blocks of several sentences in one or both languages , this can be accommodated by treating the block instead as a single bigger quot ; compound sentence quot ; in which noun phrases have a higher number of possible correspondences .", "Some terminology is necessary to describe the algorithm concisely .", "Let there be L total alignments in the corpus ; then Ei is the English sentence for alignment i .", "Let the function 0 E2 be the number of noun phrases identified in the sentence .", "If there are k of them , k cb Ei , and they can be referenced by j 1 . . . k . Considering the j'th noun phrase in sentence Ei , the function p Ei , j produces an identifier for the phrase , which is the position of the phrase in the English index .", "If this phrase is at position s , then p Ei , j s . In turn , the French sentence Fi will contain 0 Fi noun phrases and given the p'th one , its position in the French index will be given by p Fi , p .", "It will also be assumed that there are a total of VE and VE phrases in the English and French indexes respectively .", "Finally , the indicator function 0 has the value unity if its argument is true , and zero otherwise .", "Assuming these definitions , the algorithm is stated in Figure 2 .", "The equations assume a directionality finding French quot ; target quot ; correspondences for English quot ; source quot ; phrases .", "The algorithm is reversible , by swapping E with F . The model for correspondence is that a source noun phrase in Ei is responsible for producing the various different target noun phrases in Fi with correspondingly different probabilities .", "Two quantities are calculated ; C , .", "s , t and Pr s , t .", "Computation proceeds by evaluating Equation 1 , Equation 2 and then iteratively applying Equations 3 and 2 ; r increasing with each successive iteration .", "The argument s refers to the English noun phrase npE s having position s in the English index , and the argument t refers to the French noun phrase npF t at position t in the French index .", "Equation 1 assumes that each English noun phrase in Ei is initially equally likely to correspond to each French noun phrase in Fi .", "All correspondences are thus equally weighted , reflecting a state of ignorance .", "Weights are summed over the corpus , so noun phrases that co occur in several sentences will have larger sums .", "The weights Co s , t can be interpreted as the mean number of times that npF t corresponds to npE s given the corpus and the initial assumption of equiprobable correspondences .", "These weights can be used to form a new estimate of the probability that npF t corresponds to npE s , by considering the mean number of times npF t corresponds to npE s as a fraction of the total mean number of correspondences for npE s , as in Equation 2 .", "The procedure is then iterated using Equations 3 , and 2 to obtain successively refined , convergent estimates of the probability that npF t corresponds to npE s .", "The probability of correspondences can be used as a method of ranking them occurrence counts can be taken into account as an indication of the reliability of a correspondence .", "Although Figure 2 defines the coefficients simply , the algorithm is not implemented literally from it .", "The algorithm employs a compact representation of the correspondences for efficient operation .", "An arbitrarily large corpus can be accommodated by segmenting it appropriately .", "The algorithm described here is an instance of a general approach to statistical estimation , represented by the EM algorithm Dempster et al . , 1977 .", "In contrast to reservations that have been expressed Gale and Church , 1991a about using the EM algorithm to provide word correspondences , there have been no indications that prohibitive amounts of memory might be required , or that the approach lacks robustness .", "Unlike the other methods that have been mentioned , the approach has the capability to accommodate more context to improve performance .", "A sample of the aligned corpus comprising 2 , 600 alignments was used for testing the algorithm not all of the alignments contained sentences .", "4 , 900 distinct English noun phrases and 5 , 100 distinct French noun phrases were extracted from the sample .", "When forming correspondences involving long sentences with many clauses , it was observed that the position at which a noun phrase occurred in Ei was very roughly proportional to the corresponding noun phrase in F . .", "In such cases it was not necessary to form correspondences with all noun phrases in Fi for each noun phrase in E . Instead , the location of a phrase in Ei was mapped linearly to a position in Fi and correspondences were formed for noun phrases occurring in a window around that position .", "This resulted in a total of 34 , 000 correspondences .", "The mappings are stable within a few 2 4 iterations .", "In discussing results , a selection of examples will be presented that demonstrates the strengths and weaknesses of the algorithm .", "To give an indication of noun phrase frequency counts in the sample , the highest ranking correspondences are shown in Table 1 .", "The figures in columns 1 and 3 indicate the number of instances of the noun phrase to their right .", "To give an informal impression of overall performance , the hundred highest ranking correspondences were inspected and of these , ninety were completely correct .", "Less frequently occurring noun phrases are also of interest for purposes of evaluation ; some of these are shown in Table 2 .", "The table also illustrates an unembedded English noun phrase having multiple prepositional phrases in its French correspondent .", "Organizational acronyms which may be not be available in general purpose dictionaries are also extracted , as the taggers are robust .", "Even when a noun phrase only occurs once , a correct correspondence can be found if there are only single noun phrases in each sentence of the alignment .", "This is demonstrated in the last row of Table 2 , which is the result of the following alignment Ei quot ; The whole issue of free trade has been mentioned . quot ; quot ; On a mentionne la question du libreechange . quot ; Table 3 shows some incorrect correspondences produced by the algorithm in the table , quot ; usine quot ; means quot ; factory quot ; .", "The sentences that are responsible for these correspondences illustrate some of the problems associated with the correspondence model Ei quot ; They use what is known as the dual system in which there is a mix of on the job and offthe job training . quot ; Fi quot ; Ils ont recours a une formation mixte , partie en usine et partie hors usine . quot ; The first problem is that the conjunctive modifiers in the English sentence cannot be accommodated by the noun phrase recognizer .", "The tagger also assigned quot ; on the job quot ; as a noun when adjectival use would be preferred .", "If verb correspondences were included , there is a mismatch between the three that exist in the English sentence and the single one in the French .", "If the English were to reflect the French for the correspondence model to be appropriate , the noun phrases would perhaps be quot ; part in the factory quot ; and quot ; part out of the factory quot ; .", "Considered as a translation , this is lame .", "The majority of errors that occur are not the result of incorrect tagging or noun phrase recognition , but are the result of the approximate nature of the correspondence model .", "The correspondences in Table 4 are likewise flawed in the table , quot ; souris quot ; means quot ; mouse quot ; and quot ; tigre de papier quot ; means quot ; paper tiger quot ; These correspondences are the result of the following sentences Ei quot ; It is a roaring rabbit , a toothless tiger . quot ; Fi quot ; C' est un tigre de papier , un souris qui rugit . quot ; In the case of the alliterative English phrase quot ; roaring rabbit quot ; , the presumably rhetorical aspect is preserved as a rhyme in quot ; souris qui rugit quot ; ; the result being that quot ; rabbit quot ; corresponds to quot ; souris quot ; mouse .", "Here again , even if the best correspondence were made the result would be wrong because of the relatively sophisticated considerations involved in the translation .", "As regards future possibilities , the algorithm lends itself to a range of improvements and applications , which are outlined next .", "Finding Word Correspondences The algorithm finds corresponding noun phrases but provides no information about word level correspondences within them .", "One possibility is simply to eliminate the tagger and noun phrase recognizer treating all words as individual phrases of length unity and having a larger number of correspondences .", "Alternatively , the following strategy can be adopted , which involves fewer total correspondences .", "First , the algorithm is used to build noun phrase correspondences , then the phrase pairs that are produced are themselves treated as a bilingual noun phrase corpus .", "The algorithm is then employed again on this corpus , treating all words as individual phrases .", "This results in a set of single word correspondences for the internal words in noun phrases .", "Reducing Ambiguity The basic algorithm assumes that noun phrases can be uniquely identified in both languages , which is only true for simple noun phrases .", "The problem of prepositional phrase attachment is exemplified by the following correspondences The correct English and French noun phrases are quot ; Secretary of State for External Affairs quot ; and quot ; secretaire d' Etat aux Affaires exterieures quot ; .", "If prepositional phrases involving quot ; for quot ; and quot ; a quot ; were also permitted , these phrases would be correctly identified ; however many other adverbial prepositional phrases would also be incorrectly attached to noun phrases .", "If all embedded prepositional phrases were permitted by the noun phrase recognizer , the algorithm could be used to reduce the degree of ambiguity between alternatives .", "Consider a sequence npePPe of an unembedded English noun phrase npe followed by a prepositional phrase ppe , and likewise a corresponding French sequence npf ppf .", "Possible interpretations of this are 1 .", "The prepositional phrase attaches to the noun phrase in both languages .", "The prepositional phrase attaches to the noun phrase in one language and does not in the other .", "The prepositional phrase does not attach to the noun phrase in either language .", "If the prepositional phrases attach to the noun phrases in both languages , they are likely to be repeated in most instances of the noun phrase ; it is less likely that the same prepositional phrase will be used adverbially with each instance of the noun phrase .", "This provides a heuristic method for reducing ambiguity in noun phrases that occur several times .", "The only modifications required to the algorithm are that the additional possible noun phrases and correspondences between them must be included .", "Given thresholds on the number of occurrences and the probability of the correspondence , the most likely correspondence can be predicted .", "Including Context In the algorithm , correspondences between source and target noun phrases are considered irrespectively of other correspondences in an alignment .", "This does not make the best use of the information available , and can be improved upon .", "For example , consider the following alignment quot ; The Bill was introduced just before Christmas . quot ; Fi quot ; Le projet de loi a ete presente juste avant le conge des Fetes . quot ; Here it is assumed that there are many instances of the correspondence quot ; Bill quot ; and quot ; projet de loi quot ; , but only one instance of quot ; Christmas quot ; and quot ; conge des Fetes quot ; .", "This suggests that quot ; Bill quot ; corresponds to quot ; projet de loi quot ; with a high probability and that quot ; Christmas quot ; likewise corresponds strongly to quot ; conge des Fetes quot ; .", "However , the model will assert that quot ; Christmas quot ; corresponds to quot ; projet de loi quot ; and to quot ; conge des Fetes quot ; with equal probability , no matter how likely the correspondence between quot ; Bill quot ; and quot ; projet de loi quot ; .", "The model can be refined to reflect this situation by considering the joint probability that a target npF t corresponds to a source npE s and all the other possible correspondences in the alignment are produced .", "This situation is very similar to that involved in training HMM text taggers , where joint probabilities are computed that a particular word corresponds to a particular part ofspeech , and the rest of the words in the sentence are also generated e . g .", "Cutting et al . , 1992 .", "The algorithm described in this paper provides a practical means for obtaining correspondences between noun phrases in a bilingual corpus .", "Linguistic structure is used in the form of noun phrase recognizers to select phrases for a stochastic model which serves as a means of minimizing errors due to the approximations inherent in the correspondence model .", "The algorithm is robust , and extensible in several ways ."], "summary_lines": ["An Algorithm For Finding Noun Phrase Correspondences In Bilingual Corpora\n", "The paper describes an algorithm that employs English and French text taggers to associate noun phrases in an aligned bilingual corpus.\n", "The taggets provide part-of-speech categories which are used by finite-state recognizers to extract simple noun phrases for both languages.\n", "Noun phrases are then mapped to each other using an iterative re-estimation algorithm that bears similarities to the Baum-Welch algorithm which is used for training the taggers.\n", "The algorithm provides an alternative to other approaches for finding word correspondences, with the advantage that linguistic structure is incorporated.\n", "Improvements to the basic algorithm are described, which enable context to be accounted for when constructing the noun phrase mappings.\n", "We attempt to find noun phrase correspondence in parallel corpora using part-of-speech tagging and noun phrase recognition methods.\n"]}
{"article_lines": ["Domain Adaptation with Active Learning for Word Sense Disambiguation", "When a word sense disambiguation WSD system is trained on one domain but applied to a different domain , a drop in accuracy is frequently observed .", "This highlights the importance of domain adaptation for word sense disambiguation .", "In this paper , we first show that an active learning approach can be successfully used to perform domain adaptation of WSD systems .", "Then , by using the predominant sense predicted by expectation maximization EM and adopting a count merging technique , we improve the effectiveness of the original adaptation process achieved by the basic active learning approach .", "In natural language , a word often assumes different meanings , and the task of determining the correct meaning , or sense , of a word in different contexts is known as word sense disambiguation WSD .", "To date , the best performing systems in WSD use a corpus based , supervised learning approach .", "With this approach , one would need to collect a text corpus , in which each ambiguous word occurrence is first tagged with its correct sense to serve as training data .", "The reliance of supervised WSD systems on annotated corpus raises the important issue of domain dependence .", "To investigate this , Escudero et al . 2000 and Martinez and Agirre 2000 conducted experiments using the DSO corpus , which 49 contains sentences from two different corpora , namely Brown Corpus BC and Wall Street Journal WSJ .", "They found that training a WSD system on one part BC or WSJ of the DSO corpus , and applying it to the other , can result in an accuracy drop of more than 10 , highlighting the need to perform domain adaptation of WSD systems to new domains .", "Escudero et al . 2000 pointed out that one of the reasons for the drop in accuracy is the difference in sense priors i . e . , the proportions of the different senses of a word between BC and WSJ .", "When the authors assumed they knew the sense priors of each word in BC and WSJ , and adjusted these two datasets such that the proportions of the different senses of each word were the same between BC and WSJ , accuracy improved by 9 .", "In this paper , we explore domain adaptation of WSD systems , by adding training examples from the new domain as additional training data to a WSD system .", "To reduce the effort required to adapt a WSD system to a new domain , we employ an active learning strategy Lewis and Gale , 1994 to select examples to annotate from the new domain of interest .", "To our knowledge , our work is the first to use active learning for domain adaptation for WSD .", "A similar work is the recent research by Chen et al . 2006 , where active learning was used successfully to reduce the annotation effort for WSD of 5 English verbs using coarse grained evaluation .", "In that work , the authors only used active learning to reduce the annotation effort and did not deal with the porting of a WSD system to a new domain .", "Domain adaptation is necessary when the training and target domains are different .", "In this paper , we perform domain adaptation for WSD of a set of its BC and WSJ parts to investigate the domain denouns using fine grained evaluation .", "The contribu pendence of several WSD algorithms .", "Following the tion of our work is not only in showing that active setup of Escudero et al . , 2000 , we similarly made learning can be successfully employed to reduce the use of the DSO corpus to perform our experiments annotation effort required for domain adaptation in on domain adaptation . a fine grained WSD setting .", "More importantly , our Among the few currently available manually main focus and contribution is in showing how we sense annotated corpora for WSD , the SEMCOR can improve the effectiveness of a basic active learn SC corpus Miller et al . , 1994 is the most widely ing approach when it is used for domain adaptation . used .", "SEMCOR is a subset of BC which is senseIn particular , we explore the issue of different sense annotated .", "Since BC is a balanced corpus , and since priors across different domains .", "Using the sense performing adaptation from a general corpus to a priors estimated by expectation maximization EM , more specific corpus is a natural scenario , we focus the predominant sense in the new domain is pre on adapting a WSD system trained on BC to WSJ in dicted .", "Using this predicted predominant sense and this paper .", "Henceforth , out of domain data will readopting a count merging technique , we improve the fer to BC examples , and in domain data will refer to effectiveness of the adaptation process .", "WSJ examples .", "In the next section , we discuss the choice of corpus and nouns used in our experiments .", "We then introduce active learning for domain adaptation , followed by count merging .", "Next , we describe an EMbased algorithm to estimate the sense priors in the new domain .", "Performance of domain adaptation using active learning and count merging is then presented .", "Next , we show that by using the predominant sense of the target domain as predicted by the EM based algorithm , we improve the effectiveness of the adaptation process .", "Our empirical results show that for the set of nouns which have different predominant senses between the training and target domains , we are able to reduce the annotation effort by 71 .", "2 Experimental Setting In this section , we discuss the motivations for choosing the particular corpus and the set of nouns to conduct our domain adaptation experiments .", "2 . 1 Choice of Corpus The DSO corpus Ng and Lee , 1996 contains 192 , 800 annotated examples for 121 nouns and 70 verbs , drawn from BC and WSJ .", "While the BC is built as a balanced corpus , containing texts in various categories such as religion , politics , humanities , fiction , etc , the WSJ corpus consists primarily of business and financial news .", "Exploiting the difference in coverage between these two corpora , Escudero et al . 2000 separated the DSO corpus into 50 2 . 2 Choice of Nouns The WordNet Domains resource Magnini and Cavaglia , 2000 assigns domain labels to synsets in WordNet .", "Since the focus of the WSJ corpus is on business and financial news , we can make use of WordNet Domains to select the set of nouns having at least one synset labeled with a business or finance related domain label .", "This is similar to the approach taken in Koeling et al . , 2005 where they focus on determining the predominant sense of words in corpora drawn from finance versus sports domains . 1 Hence , we select the subset of DSO nouns that have at least one synset labeled with any of these domain labels commerce , enterprise , money , finance , banking , and economy .", "This gives a set of 21 nouns book , business , center , community , condition , field , figure , house , interest , land , line , money , need , number , order , part , power , society , term , use , value . 2 For each noun , all the BC examples are used as out of domain training data .", "One third of the WSJ examples for each noun are set aside as evaluation 1Note however that the coverage of the WordNet Domains resource is not comprehensive , as about 31 of the synsets are simply labeled with factotum , indicating that the synset does not belong to a specific domain .", "225 nouns have at least one synset labeled with the listed domain labels .", "In our experiments , 4 out of these 25 nouns have an accuracy of more than 90 before adaptation i . e . , training on just the BC examples and accuracy improvement is less than 1 after all the available WSJ adaptation examples are added as additional training data .", "To obtain a clearer picture of the adaptation process , we discard these 4 nouns , leaving a set of data , and the rest of the WSJ examples are designated as in domain adaptation data .", "The row 21 nouns in Table 1 shows some information about these 21 nouns .", "For instance , these nouns have an average of 6 . 7 senses in BC and 6 . 8 senses in WSJ .", "This is slightly higher than the 5 . 8 senses per verb in Chen et al . , 2006 , where the experiments were conducted using coarse grained evaluation .", "Assuming we have access to an oracle which determines the predominant sense , or most frequent sense MFS , of each noun in our WSJ test data perfectly , and we assign this most frequent sense to each noun in the test data , we will have achieved an accuracy of 61 . 1 as shown in the column MFS accuracy of Table 1 .", "Finally , we note that we have an average of 310 BC training examples and 406 WSJ adaptation examples per noun .", "For our experiments , we use naive Bayes as the learning algorithm .", "The knowledge sources we use include parts of speech , local collocations , and surrounding words .", "These knowledge sources were effectively used to build a state of the art WSD program in one of our prior work Lee and Ng , 2002 .", "In performing WSD with a naive Bayes classifier , the sense s assigned to an example with features f1 , . . . , fn is chosen so as to maximize In our domain adaptation study , we start with a WSD system built using training examples drawn from BC .", "We then investigate the utility of adding additional in domain training data from WSJ .", "In the baseline approach , the additional WSJ examples are randomly selected .", "With active learning Lewis and Gale , 1994 , we use uncertainty sampling as shown in Figure 1 .", "In each iteration , we train a WSD system on the available training data and apply it on the WSJ adaptation examples .", "Among these WSJ examples , the example predicted with the lowest confidence is selected and removed from the adaptation data .", "The correct label is then supplied for this example and it is added to the training data .", "Note that in the experiments reported in this paper , all the adaptation examples are already preannotated before the experiments start , since all the WSJ adaptation examples come from the DSO corpus which have already been sense annotated .", "Hence , the annotation of an example needed during each adaptation iteration is simulated by performing a lookup without any manual annotation .", "We also employ a technique known as countmerging in our domain adaptation study .", "Countmerging assigns different weights to different examples to better reflect their relative importance .", "Roark and Bacchiani 2003 showed that weighted count merging is a special case of maximum a posteriori MAP estimation , and successfully used it for probabilistic context free grammar domain adaptation Roark and Bacchiani , 2003 and language model adaptation Bacchiani and Roark , 2003 .", "Count merging can be regarded as scaling of counts obtained from different data sets .", "We let c denote the counts from out of domain training data , c denote the counts from in domain adaptation data , and p denote the probability estimate by count merging .", "We can scale the out of domain and in domain counts with different factors , or just use a single weight parameter \u03b2 Obtaining an optimum value for \u03b2 is not the focus of this work .", "Instead , we are interested to see if assigning a higher weight to the in domain WSJ adaptation examples , as compared to the out of domain BC examples , will improve the adaptation process .", "Hence , we just use a \u03b2 value of 3 in our experiments involving count merging .", "In this section , we describe an EM based algorithm that was introduced by Saerens et al . 2002 , which can be used to estimate the sense priors , or a priori probabilities of the different senses in a new dataset .", "We have recently shown that this algorithm is effective in estimating the sense priors of a set of nouns Chan and Ng , 2005 .", "Most of this section is based on Saerens et al . , 2002 .", "Assume we have a set of labeled data DL with n classes and a set of N independent instances x1 , . . . , xN from a new data set .", "The likelihood of these N instances can be defined as Assuming the within class densities p xk \u03c9i , i . e . , the probabilities of observing xk given the class \u03c9i , do not change from the training set DL to the new data set , we can define p xk \u03c9i pL xk \u03c9i .", "To determine the a priori probability estimates bp \u03c9i of the new data set that will maximize the likelihood of 3 with respect to p \u03c9i , we can apply the iterative procedure of the EM algorithm .", "In effect , through maximizing the likelihood of 3 , we obtain the a priori probability estimates as a by product .", "Let us now define some notations .", "When we apply a classifier trained on DL on an instance xk drawn from the new data set DU , we get bpL \u03c9i xk , which we define as the probability of instance xk being classified as class \u03c9i by the classifier trained on DL .", "Further , let us define bpL \u03c9i as the a priori probability of class \u03c9i in DL .", "This can be estimated by the class frequency of \u03c9i in DL .", "We also define bp s \u03c9i and bp s \u03c9i xk as estimates of the new a priori and a posteriori probabilities at step s of the iterative EM procedure .", "Assuming we initialize bp \u03c9i bpL \u03c9i , then for each instance xk in DU and each class \u03c9i , the EM algorithm provides the following iterative steps where Equation 4 represents the expectation Estep , Equation 5 represents the maximization Mstep , and N represents the number of instances in DU .", "Note that the probabilities bpL \u03c9i xk and bpL \u03c9i in Equation 4 will stay the same throughout the iterations for each particular instance xk and class \u03c9i .", "The new a posteriori probabilities bp s \u03c9i xk at step s in Equation 4 are simply the a posteriori probabilities in the conditions of the labeled data , bpL \u03c9i xk , weighted by the ratio of the new priors bp s \u03c9i to the old priors bpL \u03c9i .", "The denominator in Equation 4 is simply a normalizing factor .", "The a posteriori bp s \u03c9i xk and a priori probabilities bp s \u03c9i are re estimated sequentially during each iterations for each new instance xk and each class \u03c9i , until the convergence of the estimated probabilities bp s \u03c9i , which will be our estimated sense priors .", "This iterative procedure will increase the likelihood of 3 at each step .", "For each adaptation experiment , we start off with a classifier built from an initial training set consisting of the BC training examples .", "At each adaptation iteration , WSJ adaptation examples are selected one at a time and added to the training set .", "The adaptation process continues until all the adaptation examples are added .", "Classification accuracies averaged over 3 random trials on the WSJ test examples at each iteration are calculated .", "Since the number of WSJ adaptation examples differs for each of the 21 nouns , the learning curves we will show in the various figures are plotted in terms of different percentage of adaptation examples added , varying from 0 to 100 percent in steps of 1 percent .", "To obtain these curves , we first calculate for each noun , the WSD accuracy when different percentages of adaptation examples are added .", "Then , for each percentage , we calculate the macro average WSD accuracy over all the nouns to obtain a single learning curve representing all the nouns .", "In Figure 2 , the curve r represents the adaptation process of the baseline approach , where additional WSJ examples are randomly selected during each adaptation iteration .", "The adaptation process using active learning is represented by the curve a , while applying count merging with active learning is represented by the curve a c .", "Note that random selection r achieves its highest WSD accuracy after all the adaptation examples are added .", "To reach the same accuracy , the a approach requires the addition of only 57 of adaptation examples .", "The a c approach is even more effective and requires only 42 of adaptation examples .", "This demonstrates the effectiveness of count merging in further reducing the annotation effort , when compared to using only active learning .", "To reach the MFS accuracy of 61 . 1 as shown earlier in Table 1 , a c requires just 4 of the adaptation examples .", "To determine the utility of the out of domain BC examples , we have also conducted three active learning runs using only WSJ adaptation examples .", "Using 10 , 20 , and 30 of WSJ adaptation examples to build a classifier , the accuracy of these runs is lower than the active learning a curve and paired t tests show that the difference is statistically significant at the level of significance 0 . 01 .", "As mentioned in section 1 , research in Escudero et al . , 2000 noted an improvement in accuracy when they adjusted the BC and WSJ datasets such that the proportions of the different senses of each word were the same between BC and WSJ .", "We can similarly choose BC examples such that the sense priors in the BC training data adhere to the sense priors in the WSJ evaluation data .", "To gauge the effectiveness of this approach , we first assume that we know the true sense priors of each noun in the WSJ evaluation data .", "We then gather BC training examples for a noun to adhere as much as possible to the sense priors in WSJ .", "Assume sense si is the predominant sense in the WSJ evaluation data , si has a sense prior of pi in the WSJ data and has ni BC training examples .", "Taking ni examples to represent a sense prior of pi , we proportionally determine the number of BC examples to gather for other senses s according to their respective sense priors in WSJ .", "If there are insufficient training examples in BC for some sense s , whatever available examples of s are used .", "This approach gives an average of 195 BC training examples for the 21 nouns .", "With this new set of training examples , we perform adaptation using active learning and obtain the a truePrior curve in Figure 2 .", "The a truePrior curve shows that by ensuring that the sense priors in the BC training data adhere as much as possible to the sense priors in the WSJ data , we start off with a higher WSD accuracy .", "However , the performance is no different from the a curve after 35 of adaptation examples are added .", "A possible reason might be that by strictly adhering to the sense priors in the WSJ data , we have removed too many BC training examples , from an average of 310 examples per noun as shown in Table 1 , to an average of 195 examples .", "Research by McCarthy et al . 2004 and Koeling et al .", "2005 pointed out that a change of predominant sense is often indicative of a change in domain .", "For example , the predominant sense of the noun interest in the BC part of the DSO corpus has the meaning a sense of concern with and curiosity about someone or something .", "In the WSJ part of the DSO corpus , the noun interest has a different predominant sense with the meaning a fixed charge for borrowing money , which is reflective of the business and finance focus of the WSJ corpus .", "Instead of restricting the BC training data to adhere strictly to the sense priors in WSJ , another alternative is just to ensure that the predominant sense in BC is the same as that of WSJ .", "Out of the 21 nouns , 12 nouns have the same predominant sense in both BC and WSJ .", "The remaining 9 nouns that have different predominant senses in the BC and WSJ data are center , field , figure , interest , line , need , order , term , value .", "The row 9 nouns in Table 1 gives some information for this set of 9 nouns .", "To gauge the utility of this approach , we conduct experiments on these nouns by first assuming that we know the true predominant sense in the WSJ data .", "Assume that the WSJ predominant sense of a noun is sz and sz has nz examples in the BC data .", "We then gather BC examples for a noun to adhere to this WSJ predominant sense , by gathering only up to nz BC examples for each sense of this noun .", "This approach gives an average of 190 BC examples for the 9 nouns .", "This is higher than an average of 83 BC examples for these 9 nouns if BC examples are selected to follow the sense priors of WSJ evaluation data as described in the last subsection 6 . 2 .", "For these 9 nouns , the average KL divergence between the sense priors of the original BC data and WSJ evaluation data is 0 . 81 .", "This drops to 0 . 51 after ensuring that the predominant sense in BC is the same as that of WSJ , confirming that the sense priors in the newly gathered BC data more closely follow the sense priors in WSJ .", "Using this new set of training examples , we perform domain adaptation using active learning to obtain the curve a truePred in Figure 3 .", "For comparison , we also plot the curves a and a truePrior for this set of 9 nouns in Figure 3 .", "Results in Figure 3 show that a truePred starts off at a higher accuracy and performs consistently better than the a curve .", "In contrast , though a truePrior starts at a high accuracy , its performance is lower than a truePred and a after 50 of adaptation examples are added .", "The approach represented by atruePred is a compromise between ensuring that the sense priors in the training data follow as closely as possible the sense priors in the evaluation data , while retaining enough training examples .", "These results highlight the importance of striking a balance between these two goals .", "In McCarthy et al . , 2004 , a method was presented to determine the predominant sense of a word in a corpus .", "However , in Chan and Ng , 2005 , we showed that in a supervised setting where one has access to some annotated training data , the EMbased method in section 5 estimates the sense priors more effectively than the method described in McCarthy et al . , 2004 .", "Hence , we use the EM based algorithm to estimate the sense priors in the WSJ evaluation data for each of the 21 nouns .", "The sense with the highest estimated sense prior is taken as the predominant sense of the noun .", "For the set of 12 nouns where the predominant sense remains unchanged between BC and WSJ , the EM based algorithm is able to predict that the predominant sense remains unchanged for all 12 nouns .", "Hence , we will focus on the 9 nouns which have different predominant senses between BC and WSJ for our remaining adaptation experiments .", "For these 9 nouns , the EM based algorithm correctly predicts the WSJ predominant sense for 6 nouns .", "Hence , the algorithm is able to predict the correct predominant sense for 18 out of 21 nouns overall , representing an accuracy of 86 .", "Figure 4 plots the curve a estPred , which is similar to a truePred , except that the predominant sense is now estimated by the EM based algorithm .", "Employing count merging with a estPred produces the curve a c estPred .", "For comparison , the curves r , a , and a truePred are also plotted .", "The results show that a estPred performs consistently better than a , and a c estPred in turn performs better than aestPred .", "Hence , employing the predicted predominant sense and count merging , we further improve the effectiveness of the active learning based adaptation process .", "With reference to Figure 4 , the WSD accuracies of the r and a curves before and after adaptation are 43 . 7 and 78 . 4 respectively .", "Starting from the mid point 61 . 1 accuracy , which represents a 50 accuracy increase from 43 . 7 , we show in Table 2 the percentage of adaptation examples required by the various approaches to reach certain levels of WSD accuracies .", "For instance , to reach the final accuracy of 78 . 4 , r , a , a estPred , and ac estPred require the addition of 100 , 51 , 38 , and 29 adaptation examples respectively .", "The numbers in brackets give the ratio of adaptation examples needed by a , a estPred , and a c estPred versus random selection r . For instance , to reach a WSD accuracy of 78 . 4 , a c estPred needs only 29 adaptation examples , representing a ratio of 0 . 29 and an annotation saving of 71 .", "Note that this represents a more effective adaptation process than the basic active learning a approach , which requires 51 adaptation examples .", "Hence , besides showing that active learning can be used to reduce the annotation effort required for domain adaptation , we have further improved the effectiveness of the adaptation process by using the predicted predominant sense of the new domain and adopting the count merging technique .", "In applying active learning for domain adaptation , Zhang et al . 2003 presented work on sentence boundary detection using generalized Winnow , while Tur et al .", "2004 performed language model adaptation of automatic speech recognition systems .", "In both papers , out of domain and indomain data were simply mixed together without MAP estimation such as count merging .", "For WSD , Fujii et al . 1998 used selective sampling for a Japanese language WSD system , Chen et al .", "2006 used active learning for 5 verbs using coarse grained evaluation , and H . T . Dang 2004 employed active learning for another set of 5 verbs .", "However , their work only investigated the use of active learning to reduce the annotation effort necessary for WSD , but did not deal with the porting of a WSD system to a different domain .", "Escudero et al . 2000 used the DSO corpus to highlight the importance of the issue of domain dependence of WSD systems , but did not propose methods such as active learning or countmerging to address the specific problem of how to perform domain adaptation for WSD .", "Domain adaptation is important to ensure the general applicability of WSD systems across different domains .", "In this paper , we have shown that active learning is effective in reducing the annotation effort required in porting a WSD system to a new domain .", "Also , we have successfully used an EM based algorithm to detect a change in predominant sense between the training and new domain .", "With this information on the predominant sense of the new domain and incorporating count merging , we have shown that we are able to improve the effectiveness of the original adaptation process achieved by the basic active learning approach .", "Yee Seng Chan is supported by a Singapore Millennium Foundation Scholarship ref no .", "SMF 20041076 ."], "summary_lines": ["Domain Adaptation with Active Learning for Word Sense Disambiguation\n", "When a word sense disambiguation (WSD) system is trained on one domain but applied to a different domain, a drop in accuracy is frequently observed.\n", "This highlights the importance of domain adaptation for word sense disambiguation.\n", "In this paper, we first show that an active learning approach can be successfully used to perform domain adaptation of WSD systems.\n", "Then, by using the predominant sense predicted by expectation-maximization (EM) and adopting a count-merging technique, we improve the effectiveness of the original adaptation process achieved by the basic active learning approach.\n", "We perform supervised domain adaptation on a manually selected subset of 21 nouns from the DSO corpus.\n", "We notably show that detecting changes in predominant sense as modeled by domain sense priors can improve sense disambiguation, even after performing adaptation using active learning.\n"]}
{"article_lines": ["Noun Phrase Coreference As Clustering", "This paper introduces a new , unsupervised algorithm for noun phrase coreference resolution .", "It differs from existing methods in that it views coreference resolution as a clustering task .", "In an evaluation on the MUC 6 coreference resolution corpus , the algorithm achieves an F measure of 53 . 6 , placing it firmly between the worst 40 and best 65 systems in the MUC 6 evaluation .", "More importantly , the clustering approach outperforms the only MUC 6 system to treat coreference resolution as a learning problem .", "The clustering algorithm appears to provide a flexible mechanism for coordinating the application of context independent and context dependent constraints and preferences for accurate partitioning of noun phrases into coreference equivalence classes .", "Many natural language processing NLP applications require accurate noun phrase coreference resolution They require a means for determining which noun phrases in a text or dialogue refer to the same real world entity .", "The vast majority of algorithms for noun phrase coreference combine syntactic and , less often , semantic cues via a set of hand crafted heuristics and filters .", "All but one system in the MUC 6 coreference performance evaluation MUC , 1995 , for example , handled coreference resolution in this manner .", "This same reliance on complicated hand crafted algorithms is true even for the narrower task of pronoun resolution .", "Some exceptions exist , however .", "Ge et al . 1998 present a probabilistic model for pronoun resolution trained on a small subset of the Penn Treebank Wall Street Journal corpus Marcus et al . , 1993 .", "Dagan and Itai 1991 develop a statistical filter for resolution of the pronoun quot ; it quot ; that selects among syntactically viable antecedents based on relevant subject verb object cooccurrences .", "Aone and Bennett 1995 and McCarthy and Lehnert 1995 employ decision tree algorithms to handle a broader subset of general noun phrase coreference problems .", "This paper presents a new corpus based approach to noun phrase coreference .", "We believe that it is the first such unsupervised technique developed for the general noun phrase coreference task .", "In short , we view the task of noun phrase coreference resolution as a clustering task .", "First , each noun phrase in a document is represented as a vector of attribute value pairs .", "Given the feature vector for each noun phrase , the clustering algorithm coordinates the application of context independent and context dependent coreference constraints and preferences to partition the noun phrases into equivalence classes , one class for each real world entity mentioned in the text .", "Context independent coreference constraints and preferences are those that apply to two noun phrases in isolation .", "Context dependent coreference decisions , on the other hand , consider the relationship of each noun phrase to surrounding noun phrases .", "In an evaluation on the MUC 6 coreference resolution corpus , our clustering approach achieves an F measure of 53 . 6 , placing it firmly between the worst 40 and best 65 systems in the MUC6 evaluation .", "More importantly , the clustering approach outperforms the only MUC 6 system to view coreference resolution as a learning problem The RESOLVE system McCarthy and Lehnert , 1995 employs decision tree induction and achieves an Fmeasure of 47 on the MUC 6 data set .", "Furthermore , our approach has a number of important advantages over existing learning and non learning methods for coreference resolution As a result , we believe that viewing noun phrase coreference as clustering provides a promising framework for corpus based coreference resolution .", "The remainder of the paper describes the details of our approach .", "The next section provides a concrete specification of the noun phrase coreference resolution task .", "Section 3 presents the clustering algorithm .", "Evaluation of the approach appears in Section 4 .", "Qualitative and quantitative comparisons to related work are included in Section 5 .", "It is commonly observed that a human speaker or author avoids repetition by using a variety of noun phrases to refer to the same entity .", "While human audiences have little trouble mapping a collection of noun phrases onto the same entity , this task of noun phrase NP coreference resolution can present a formidable challenge to an NLP system .", "Figure 1 depicts a typical coreference resolution system , which takes as input an arbitrary document and produces as output the appropriate coreference equivalence classes .", "The subscripted noun phrases in the sample output constitute two noun phrase coreference equivalence classes Class JS contains the five noun phrases that refer to John Simon , and class PC contains the two noun phrases that represent Prime Corp .", "The figure also visually links neighboring coreferent noun phrases .", "The remaining unbracketed noun phrases have no coreferent NPs and are considered singleton equivalence classes .", "Handling the JS class alone requires recognizing coreferent NPs in appositive and genitive constructions as well as those that occur as proper names , possessive pronouns , and definite NPs .", "Our approach to the coreference task stems from the observation that each group of coreferent noun phrases defines an equivalence classl .", "Therefore , it is natural to view the problem as one of partitioning , or clustering , the noun phrases .", "Intuitively , all of the noun phrases used to describe a specific concept will be quot ; near quot ; or related in some way , i . e . their conceptual quot ; distance quot ; will be small .", "Given a description of each noun phrase and a method for measuring the distance between two noun phrases , a clustering algorithm can then group noun phrases together Noun phrases with distance greater than a clustering radius r are not placed into the same partition and so are not considered coreferent .", "The subsections below describe the noun phrase representation , the distance metric , and the clustering algorithm in turn .", "Given an input text , we first use the Empire noun phrase finder Cardie and Pierce , 1998 to locate all noun phrases in the text .", "Note that Empire identifies only base noun phrases , i . e . simple noun phrases that contain no other smaller noun phrases within them .", "For example , Chief Financial Officer of Prime Corp . is too complex to be a base noun phrase .", "It contains two base noun phrases Chief Financial Officer and Prime Corp . Each noun phrase in the input text is then represented as a set of 11 features as shown in Table 1 .", "This noun phrase representation is a first approximation to the feature vector that would be required for accurate coreference resolution .", "All feature values are automatically generated and , therefore , are not always perfect .", "In particular , we use very simple heuristics to approximate the behavior of more complex feature value computations Individual Words .", "The words contained in the noun phrase are stored as a feature .", "Head noun .", "The last word in the noun phrase is considered the head noun .", "Position .", "Noun phrases are numbered sequentially , starting at the beginning of the document .", "Pronoun Type .", "Pronouns are marked as one of Nominative , Accusative , POSSessive , or AMBiguous you and it .", "All other noun phrases obtain the value NONE for this feature .", "Article .", "Each noun phrase is marked INDEFinite contains a or an , DEFinite contains the , or NONE .", "Appositive .", "Here we use a simple , overly restrictive heuristic to determine whether or not the noun phrase is in a post posed appositive construction If the noun phrase is surrounded by commas , contains an article , and is immediately preceded by another noun phrase , then it is marked as an appositive .", "Number .", "If the head noun ends in an 's' , then the noun phrase is marked PLURAL ; otherwise , it is considered siNGular .", "Expressions denoting money , numbers , or percentages are also marked as PLURAL .", "Proper Name .", "Proper names are identified by looking for two adjacent capitalized words , optionally containing a middle initial .", "Semantic Class .", "Here we use WordNet Fellbaum , 1998 to obtain coarse semantic information for the head noun .", "The head noun is characterized as one of TIME , CITY , ANIMAL , HUMAN , or OBJECT .", "If none of these classes pertains to the head noun , its immediate parent in the class hierarchy is returned as the semantic class , e . g .", "PAYMENT for the head noun pay in NP6 of Table 1 .", "A separate algorithm identifies NUMBERS , MONEY , and COMPANYS .", "Gender .", "Gender mAsculine , FEMinine , EITHER , or NEUTER is determined using WordNet and for proper names a list of common first names .", "Animacy .", "Noun phrases classified as HUMAN or ANIMAL are marked ANIM ; all other NPs are considered INANIM .", "Next , we define the following distance metric between two noun phrases where F corresponds to the NP feature set described above ; incompatibility f is a function that returns a value between 0 and 1 inclusive and indicates the degree of incompatibility of f for N Pi and NP ; and w1 denotes the relative importance of compatibility w . r . t . feature f . The incompatibility functions and corresponding weights are listed in Table 2 . 2 In general , weights are chosen to represent linguistic knowledge about coreference .", "Terms with a weight of oo represent filters that rule out impossible antecedents Two noun phrases can never corefer when they have incompatible values for that term's feature .", "In the current version of our system , the NUMBER , PROPER NAME , SEMANTIC CLASS , GENDER , and ANIMACY features operate as coreference filters .", "Conversely , terms with a weight of oo force coreference between two noun phrases with compatible values for that term's feature .", "The APPOSITIVE and WORDS SUBSTRING terms operate in this fashion in the current distance metric .", "Terms with a weight of r the clustering radius threshold implement a preference that two NPs not be coreferent if they are incompatible w . r . t . that term's feature .", "As will be explained below , however , two such NPs can be merged into the same equivalence class by the clustering algorithm if there is enough other evidence that they are similar i . e . there are other , coreferent noun phrase s sufficiently close to both .", "All other terms obtain weights selected using the development corpus .", "Although additional testing is required , our current results indicate that these weights are sensitive to the distance metric , but probably not to the corpus .", "When computing a sum that involves both oo and oo , we choose , the more conservative route , and the oo distance takes precedence i . e . the two noun phrases are not Considered coreferent .", "An example of where this might occur is in the following sentence i Reardon Steel Co . manufactures several thousand tons of 2 steel each week .", "Here , NPi subsumes NP2 , giving them a distance of oo via the word substring term ; however , NPi's semantic class is COMPANY , and NP2's class is OBJECT , generating a distance of oo via the semantic class feature .", "Therefore , dist NP1 , NP2 oo and the two noun phrases are not considered coreferent .", "The coreference distance metric is largely contextindependent in that it determines the distance between two noun phrases using very little , if any , of their intervening or surrounding context .", "The clustering algorithm described below is responsible for coordinating these local coreference decisions across arbitrarily long contexts and , thus , implements a series of context dependent coreference decisions .", "The clustering algorithm is given in Figure 2 .", "Because noun phrases generally refer to noun phrases that precede them , we start at the end of the document and work backwards .", "Each noun phrase is compared to all preceding noun phrases .", "If the distance between two noun phrases is less than the clustering radius r , then their classes are considered for possible merging .", "Two coreference equivalence classes can be merged unless there exist any incompatible NPs in the classes to be merged .", "It is useful to consider the application of our algorithm to an excerpt from a document i The chairman spoke with 2 Ms . White yesterday .", "3 He . . .", "The noun phrase instances for this fragment are shown in Table 3 .", "Initially , NPI , NP2 , and NP3 are all singletons and belong to coreference classes c2 , and c3 , respectively .", "We begin by considering NP3 .", "Dist N P2 , N P3 oo due to a mismatch on gender , so they are not considered for possible merging .", "Next , we calculate the distance from NPi to NP3 .", "Pronouns are not expected to match when the words of two noun phrases are compared , so there is no penalty here for word or head noun mismatches .", "The penalty for their difference in position is dependent on the length of the document .", "For illustration , assume that this is less than r . Thus , dist NPi , NP3 r . Their coreference classes , c1 and c3 , are then considered for merging .", "Because they are singleton classes , there is no additional possibility for conflict , and both noun phrases are merged into cl .", "We developed and evaluated the clustering approach to coreference resolution using the quot ; dry run quot ; and quot ; formal evaluation quot ; MUC 6 coreference corpora .", "Each corpus contains 30 documents that have been annotated with NP coreference links .", "We used the dryrun data for development of the distance measure and selection of the clustering radius r and reserved the formal evaluation materials for testing .", "All results are reported using the standard measures of recall and precision or F measure which combines recall and precision equally .", "They were calculated automatically using the MUC 6 scoring program Vilain et al . , 1995 .", "Table 4 summarizes our results and compares them to three baselines .", "For each algorithm , we show the F measure for the dryrun evaluation column 2 and the formal evaluation column 4 .", "The quot ; adjusted quot ; results are described below .", "For the dryrun data set , the clustering algorithm obtains 48 . 8 recall and 57 . 4 precision .", "The formal evaluation produces similar scores 52 . 7 recall and 54 . 6 precision .", "Both runs use r 4 , which was obtained by testing different values on the dryrun corpus .", "Table 5 summarizes the results on the dryrun data set for r values from 1 . 0 to 10 . 0 . 3 As expected , increasing r also increases recall , but decreases precision .", "Subsequent tests with different values for r on the formal evaluation data set also obtained optimal performance with r 4 .", "This provides partial support for our hypothesis that r need not be recalculated for new corpora .", "The remaining rows in Table 4 show the performance of the three baseline algorithms .", "The first baseline marks every pair of noun phrases as coreferent , i . e . all noun phrases in the document form one class .", "This baseline is useful because it establishes an upper bound for recall on our clustering algorithm 67 for the dryrun and 69 for the formal evaluation .", "The second baseline marks as coreferent any two noun phrases that have a word in common .", "The third baseline marks as coreferent any two noun phrases whose head nouns match .", "Although the baselines perform better one might expect they outperform one MUC 6 system , the clustering algorithm performs significantly better .", "In part because we rely on base noun phrases , our Figure 2 Clustering Algorithm The algorithm then considers NP2 .", "Dist NPi , NP2 11 . 0 plus a small penalty for their difference in position .", "If this distance is r , they will not be considered coreferent , and the resulting equivalence classes will be The chairman , he , Ms . White .", "Otherwise , the distance is r , and the algorithm considers c and c2 for merging .", "However , c1 contains NP3 , and , as calculated above , the distance from NP2 to NP3 is oo .", "This incompatibility prevents the merging of ci and 02 , so the resulting equivalence classes would still be The chairman , he , Ms . White .", "In this way , the equivalence classes grow in a flexible manner .", "In particular , the clustering algorithm automatically computes the transitive closure of the coreference relation .", "For instance , if dist NP , , NP3 r and dist NP3 , NPk r then assuming no incompatible NPs , NP , NP3 , and NPk will be in the same class and considered mutually coreferent .", "In fact , it is possible that dist NPz , NPk r , according to the distance measure ; but as long as that distance is not oo , NP can be in the same class as NPk .", "The distance measure operates on two noun phrases in isolation , but the clustering algorithm can and does make use of intervening NP information intervening noun phrases can form a chain that links otherwise distant NPs .", "By separating context independent and recall levels are fairly low .", "The quot ; adjusted quot ; figures of Table 4 reflect this upper bound on recall .", "Considering only coreference links between base noun phrases , the clustering algorithm obtains a recall of 72 . 4 on the dryrun , and 75 . 9 on the formal evaluation .", "Another source of error is inaccurate and inadequate NP feature vectors .", "Our procedure for computing semantic class values , for example , is responsible for many errors it sometimes returns incorrect values and the coarse semantic class distinctions are often inadequate .", "Without a better named entity finder , computing feature vectors for proper nouns is difficult .", "Other errors result from a lack of thematic and grammatical role information .", "The lack of discourse related topic and focus information also limits system performance .", "In addition , we currently make no special attempt to handle reflexive pronouns and pleonastic quot ; it quot ; .", "Lastly , errors arise from the greedy nature of the clustering algorithm .", "Noun phrase NP . , is linked to every preceding noun phrase N13 , that is compatible and within the radius r , and that link can never be undone .", "We are considering three possible ways to make the algorithm less aggressively greedy .", "First , for each NP3 , instead of considering every previous noun phrase , the algorithm could stop on finding the first compatible antecedent .", "Second , for each NPJ , the algorithm could rank all possible antecedents and then choose the best one and link only to that one .", "Lastly , the algorithm could rank all possible coreference links all pairs of noun phrases in the document and then proceed through them in ranked order , thus progressing from the links it is most confident about to those it is less certain of .", "Future work will include a more detailed error analysis .", "Existing systems for noun phrase coreference resolution can be broadly characterized as learning and non learning approaches .", "All previous attempts to view coreference as a learning problem treat coreference resolution as a classification task the algorithms classify a pair of noun phrases as coreferent or not .", "Both MLR Aone and Bennett , 1995 and RESOLVE McCarthy and Lehnert , 1995 , for example , apply the C4 . 5 decision tree induction algorithm Quinlan , 1992 to the task .", "As supervised learning algorithms , both systems require a fairly large amount of training data that has been annotated with coreference resolution information .", "Our approach , on the other hand , uses unsupervised learning4 and requires no training data . 5 In addition , both MLR and RESOLVE require an additional mechanism to coordinate the collection of pairwise coreference decisions .", "Without this mechanism , it is possible that the decision tree classifies NP i and NP i as coreferent , and NP i and NPk as coreferent , but NP i and NPk as not coreferent .", "In an evaluation on the MUC 6 data set see Table 6 , RESOLVE achieves an F measure of 47 .", "The MUC 6 evaluation also provided results for a large number of non learning approaches to coreference resolution .", "Table 6 provides a comparison of our results to the best and worst of these systems .", "Most implemented a series of linguistic constraints similar in spirit to those employed in our system .", "The main advantage of our approach is that all constraints and preferences are represented neatly in the distance metric and radius r , allowing for simple modification of this measure to incorporate new knowledge sources .", "In addition , we anticipate being able to automatically learn the weights used in the distance metric .", "There is also a growing body of work on the narrower task of pronoun resolution .", "Azzam et al . 1998 , for example , describe a focus based approach that incorporates discourse information when resolving pronouns .", "Lappin and Leass 1994 make use of a series of filters to rule out impossible antecedents , many of which are similar to our ooincompatibilities .", "They also make use of more extensive syntactic information such as the thematic role each noun phrase plays , and thus require a fuller parse of the input text .", "Ge et al . 1998 present a supervised probabilistic algorithm that assumes a full parse of the input text .", "Dagan and Itai 1991 present a hybrid full parse unsupervised learning approach that focuses on resolving quot ; it quot ; .", "Despite a large corpus 150 million words , their approach suffers from sparse data problems , but works well when enough relevant data is available .", "Lastly , Cardie 1992a ; 1992b presents a case based learning approach for relative pronoun disambiguation .", "Our clustering approach differs from this previous work in several ways .", "First , because we only require the noun phrases in any input text , we do not require a full syntactic parse .", "Although we would expect increases in performance if complex noun phrases were used , our restriction to base NPs does not reflect a limitation of the clustering algorithm or the distance metric , but rather a self imposed limitation on the preprocessing requirements of the approach .", "Second , our approach is unsupervised and requires no annotation of training data , nor a large corpus for computing statistical occurrences .", "Finally , we handle a wide array of noun phrase coreference , beyond just pronoun resolution .", "We have presented a new approach to noun phrase coreference resolution that treats the problem as a clustering task .", "In an evaluation on the MUC6 coreference resolution data set , the approach achieves very promising results , outperforming the only other corpus based learning approach and producing recall and precision scores that place it firmly between the best and worst coreference systems in the evaluation .", "In contrast to other approaches to coreference resolution , ours is unsupervised and offers several potential advantages over existing methods no annotated training data is required , the distance metric can be easily extended to account for additional linguistic information as it becomes available to the NLP system , and the clustering approach provides a flexible mechanism for combining a variety of constraints and preferences to impose a partitioning on the noun phrases in a text into coreference equivalence classes .", "Nevertheless , the approach can be improved in a number of ways .", "Additional analysis and evaluation on new corpora are required to determine the generality of the approach .", "Our current distance metric and noun phrase instance representation are only first , and admittedly very coarse , approximations to those ultimately required for handling the wide variety of anaphoric expressions that comprise noun phrase coreference .", "We would also like to make use of cues from centering theory and plan to explore the possibility of learning the weights associated with each term in the distance metric .", "Our methods for producing the noun phrase feature vector are also overly simplistic .", "Nevertheless , the relatively strong performance of the technique indicates that clustering constitutes a powerful and natural approach to noun phrase coreference resolution .", "This work was supported in part by NSF Grant IRI9624639 and a National Science Foundation Graduate fellowship .", "We would like to thank David Pierce for his formatting and technical advice ."], "summary_lines": ["Noun Phrase Coreference As Clustering\n", "This paper introduces a new, unsupervised algorithm for noun phrase coreference resolution.\n", "It differs from existing methods in that it views coreference resolution as a clustering task.\n", "In an evaluation on the MUC-6 coreference resolution corpus, the algorithm achieves an F-measure of 53.6%, placing it firmly between the worst (40%) and best (65%) systems in the MUC-6 evaluation.\n", "More importantly, the clustering approach outperforms the only MUC-6 system to treat coreference resolution as a learning problem.\n", "The clustering algorithm appears to provide a flexible mechanism for coordinating the application of context-independent and context-dependent constraints and preferences for accurate partitioning of noun phrases into coreference equivalence classes.\n", "We combine the use of WordNet with proper name gazetteers in order to obtain information on the compatibility of coreferential NPs in their clustering algorithm.\n", "Approaches to coreference resolution that rely only on clustering can easily enforce transitivity.\n", "We use pairwise NP distances to cluster document mentions.\n", "Our system uses the node distance in WordNet (with an upper limit of 4) as one component in the distance measure that guides their clustering algorithm.\n", "Coreference resolution is performed in two phases: a binary classification phase, in which the likelihood of coreference for each pair of noun phrases is assessed; and a partitioning phase, in which the clusters of mutually coreferring NPs are formed, maximizing some global criterion.\n"]}
{"article_lines": ["Domain Adaptation for Statistical Machine Translation with Monolingual Resources", "Domain adaptation has recently gained interest in statistical machine translation to cope with the performance drop observed when testing conditions deviate from training conditions .", "The basic idea is that in domain training data can be exploited to adapt all components of an already developed system .", "Previous work showed small performance gains by adapting from limited in domain bilingual data .", "Here , we aim instead at significant performance gains by exploiting large but cheap monolingual in domain data , either in the source or in the target language .", "We propose to synthesize a bilingual corpus by translating the monolingual adaptation data into the counterpart language .", "Investigations were conducted on a stateof the art phrase based system trained on the Spanish English part of the UN corpus , and adapted on the corresponding Europarl data .", "Translation , re ordering , and language models were estimated after translating in domain texts with the baseline .", "By optimizing the interpolation of these models on a development set the BLEU score was improved from 22 . 60 to 28 . 10 on a test set .", "A well known problem of Statistical Machine Translation SMT is that performance quickly degrades as soon as testing conditions deviate from training conditions .", "The very simple reason is that the underlying statistical models always tend to closely approximate the empirical distributions of the training data , which typically consist of bilingual texts and monolingual target language texts .", "The former provide a means to learn likely translations pairs , the latter to form correct sentences with translated words .", "Besides the general difficulties of language translation , which we do not consider here , there are two aspects that make machine learning of this task particularly hard .", "First , human language has intrinsically very sparse statistics at the surface level , hence gaining complete knowledge on translation phrase pairs or target language n grams is almost impractical .", "Second , language is highly variable with respect to several dimensions , style , genre , domain , topics , etc .", "Even apparently small differences in domain might result in significant deviations in the underlying statistical models .", "While data sparseness corroborates the need of large language samples in SMT , linguistic variability would indeed suggest to consider many alternative data sources as well .", "By rephrasing a famous saying we could say that no data is better than more and assorted data .", "The availability of language resources for SMT has dramatically increased over the last decade , at least for a subset of relevant languages and especially for what concerns monolingual corpora .", "Unfortunately , the increase in quantity has not gone in parallel with an increase in assortment , especially for what concerns the most valuable resource , that is bilingual corpora .", "Large parallel data available to the research community are for the moment limited to texts produced by international organizations European Parliament , United Nations , Canadian Hansard , press agencies , and technical manuals .", "The limited availability of parallel data poses challenging questions regarding the portability of SMT across different application domains and language pairs , and its adaptability with respect to language variability within the same application domain .", "This work focused on the second issue , namely the adaptation of a Spanish to English phrasebased SMT system across two apparently close domains the United Nation corpus and the European Parliament corpus .", "Cross domain adaptation is faced under the assumption that only monolingual texts are available , either in the source language or in the target language .", "The paper is organized as follows .", "Section 2 presents previous work on the problem of adaptation in SMT ; Section 3 introduces the exemplar task and research questions we addressed ; Section 4 describes the SMT system and the adaptation techniques that were investigated ; Section 5 presents and discusses experimental results ; and Section 6 provides conclusions .", "Domain adaptation in SMT has been investigated only recently .", "In Eck et al . , 2004 adaptation is limited to the target language model LM .", "The background LM is combined with one estimated on documents retrieved from the WEB by using the input sentence as query and applying crosslanguage information retrieval techniques .", "Refinements of this approach are described in Zhao et al . , 2004 .", "In Hildebrand et al . , 2005 information retrieval techniques are applied to retrieve sentence pairs from the training corpus that are relevant to the test sentences .", "Both the language and the translation models are retrained on the extracted data .", "In Foster and Kuhn , 2007 two basic settings are investigated cross domain adaptation , in which a small sample of parallel in domain text is assumed , and dynamic adaptation , in which only the current input source text is considered .", "Adaptation relies on mixture models estimated on the training data through some unsupervised clustering method .", "Given available adaptation data , mixture weights are re estimated ad hoc .", "A variation of this approach was also recently proposed in Finch and Sumita , 2008 .", "In Civera and Juan , 2007 mixture models are instead employed to adapt a word alignment model to in domain parallel data .", "In Koehn and Schroeder , 2007 cross domain adaptation techniques were applied on a phrasebased SMT trained on the Europarl task , in order to translate news commentaries , from French to English .", "In particular , a small portion of indomain bilingual data was exploited to adapt the Europarl language model and translation models by means of linear interpolation techniques .", "Ueffing et al . 2007 proposed several elaborate adaptation methods relying on additional bilingual data synthesized from the development or test set .", "Our work is mostly related to Koehn and Schroeder , 2007 but explores different assumptions about available adaptation data i . e . only monolingual in domain texts are available .", "The adaptation of the translation and re ordering models is performed by generating synthetic bilingual data from monolingual texts , similarly to what proposed in Schwenk , 2008 .", "Interpolation of multiple phrase tables is applied in a more principled way than in Koehn and Schroeder , 2007 all entries are merged into one single table , corresponding feature functions are concatenated and smoothing is applied when observations are missing .", "The approach proposed in this paper has many similarities with the simplest technique in Ueffing et al . , 2007 , but it is applied to a much larger monolingual corpus .", "Finally , with respect to previous work we also investigate the behavior of the minimum error training procedure to optimize the combination of feature functions on a small in domain bilingual sample .", "This paper addresses the issue of adapting an already developed phrase based translation system in order to work properly on a different domain , for which almost no parallel data are available but only monolingual texts . 1 The main components of the SMT system are the translation model , which aims at porting the content from the source to the target language , and the language model , which aims at building fluent sentences in the target language .", "While the former is trained with bilingual data , the latter just needs monolingual target texts .", "In this work , a lexicalized re ordering model is also exploited to control re ordering of target words .", "This model is also learnable from parallel data .", "Assuming some large monolingual in domain texts are available , two basic adaptation approaches are pursued here i generating synthetic bilingual data with an available SMT system and use this data to adapt its translation and re ordering models ; ii using synthetic or provided target texts to also , or only , adapt its language model .", "The following research questions summarize our basic interest in this work", "The investigation presented in this paper was carried out with the Moses toolkit Koehn et al . , 2007 , a state of the art open source phrase based SMT system .", "We trained Moses in a standard configuration , including a 4 feature translation model , a 7 feature lexicalized re ordering model , one LM , word and phrase penalties .", "The translation and the re ordering model relied on grow diag final symmetrized word toword alignments built using GIZA Och and Ney , 2003 and the training script of Moses .", "A 5 gram language model was trained on the target side of the training parallel corpus using the IRSTLM toolkit Federico et al . , 2008 , exploiting Modified Kneser Ney smoothing , and quantizing both probabilities and backoff weights .", "Decoding was performed applying cube pruning with a poplimit of 6000 hypotheses .", "Log linear interpolations of feature functions were estimated with the parallel version of minimum error rate training procedure distributed with Moses .", "The standard procedure of Moses for the estimation of the translation and re ordering models from a bilingual corpus consists in three main steps Recently , we enhanced Moses decoder to also output the word to word alignment between the input sentence and its translation , given that they have been added to the phrase table at training time .", "Notice that the additional information introduces an overhead in disk usage of about 70 , but practically no overhead at decoding time .", "However , when training translation and re ordering models from synthetic data generated by the decoder , this feature allows to completely skip the time expensive step 1 . 2 We tested the efficiency of this solution for training a translation model on a synthesized corpus of about 300K Spanish sentences and 8 . 8M running words , extracted from the EuroParl corpus .", "With respect to the standard procedure , the total training time was reduced by almost 50 , phrase extraction produced 10 more phrase pairs , and the final translation system showed a loss in translation performance BLEU score below 1 relative .", "Given this outcome we decided to apply the faster procedure in all experiments .", "Once monolingual adaptation data is automatically translated , we can use the synthetic parallel corpus to estimate new language , translation , and re ordering models .", "Such models can either replace or be combined with the original models of the SMT system .", "There is another simple option which is to concatenate the synthetic parallel data with the original training data and re build the system .", "We did not investigate this approach because it does not allow to properly balance the contribution of different data sources , and also showed to underperform in preliminary work .", "Concerning the combination of models , in the following we explain how Moses was extended to manage multiple translation models TMs and multiple re ordering models RMs .", "In Moses , a TM is provided as a phrase table , which is a set S f , e of phrase pairs associated with a given number of features values h f , E ; S .", "In our configuration , 5 features for the TM the phrase penalty is included are taken into account .", "In the first phase of the decoding process , Moses generates translation options for all possible input phrases f through a lookup into S ; it simply extracts alternative phrase pairs f , E for a specific f and optionally applies pruning based on the feature values and weights to limit the number of such pairs .", "In the second phase of decoding , it creates translation hypotheses of the full input sentence by combining in all possible ways satisfying given re ordering constraints the prefetched translation options .", "In this phase the hypotheses are scored , according to all features functions , ranked , and possibly pruned .", "When more TMs Sj are available , Moses can behave in two different ways in pre fetching the translation options .", "It searches a given f in all sets and keeps a phrase pair f , E if it belongs to either i their intersection or ii their union .", "The former method corresponds to building one new TM SI , whose set is the intersection of all given sets phrase based and lexical based direct features are defined as follows Here , \u03c6 ek fh is the probability of ek given fh provided by the word to word lexicon computed on Sj .", "The inverted features are defined similarly .", "The phrase penalty is trivially set to 1 .", "The same approach has been applied to build the union of re ordering models .", "In this case , however , the smoothing value is constant and set to 0 . 001 .", "As concerns as the use of multiple LMs , Moses has a very easy policy , consisting of querying each of them to get the likelihood of a translation hypotheses , and uses all these scores as features .", "It is worth noting that the exploitation of multiple models increases the number of features of the whole system , because each model adds its set of features .", "Furthermore , the first approach of Moses for model combination shrinks the size of the phrase table , while the second one enlarges it .", "The set of features of the new TM is the union of the features of all single TMs .", "Straightforwardly , all feature values are well defined .", "The second method corresponds to building one new TM SU , whose set is the union of all given sets SU f , e j f , E E Sj Again , the set of features of the new TM is the union of the features of all single TMs ; but for a phrase pair f , E belonging to SU Sj , the feature values h f , E ; Sj are undefined .", "In these undefined situations , Moses provides a default value of 0 , which is the highest available score , as the feature values come from probabilistic distributions and are expressed as logarithms .", "Henceforth , a phrase pair belonging to all original sets is penalized with respect to phrase pairs belonging to few of them only .", "To address this drawback , we proposed a new method3 to compute a more reliable and smoothed score in the undefined case , based on the IBM model 1 Brown et al . , 1993 .", "If f f1 , . . . , fl , e e1 , . . . , el E SU Sj for any j the In this work , the background domain is given by the Spanish English portion of the UN parallel corpus , 4 composed by documents coming from the Office of Conference Services at the UN in New York spanning the period between 1988 and 1993 .", "The adaptation data come from the European Parliament corpus Koehn , 2002 EP as provided for the shared translation task of the 2008 Workshop on Statistical Machine Translation . 5 Development and test sets for this task , namely dev2006 and test2008 , are supplied as well , and belong to the European Parliament domain .", "We use the symbol S E to denote synthetic Spanish English data .", "Spanish to English and English to Spanish systems trained on UN data were exploited to generate English and Spanish synthetic portions of the original EP corpus , respectively .", "In this way , we created two synthetic versions of the EP corpus , named S E EP and SEEP , respectively .", "All presented translation systems were optimized on the dev2006 set with respect to the BLEU score Papineni et al . , 2002 , and tested on test2008 .", "Notice that one reference translation is available for both sets .", "Table 1 reports statistics of original and synthetic parallel corpora , as well of the employed development and evaluation data sets .", "All the texts were just tokenized and mixed case was kept .", "Hence , all systems were developed to produce case sensitive translations .", "Three Spanish to English baseline systems were trained by exploiting different parallel or monolingual corpora summarized in the first three lines in Table 2 .", "For each system , the table reports the perplexity and out of vocabulary OOV percentage of their LM , and its translation performance achieved on the test set in terms of BLEU score , NIST score , WER word error rate and PER position independent error rate .", "The distance in style , genre , jargon , etc . between the UN and the EP corpora is made evident by the gap in perplexity Federico and De Mori , 1998 and OOV percentage between their English LMs 286 vs 74 and 1 . 12 vs 0 . 15 , respectively .", "Performance of the system trained on the EP corpus third row can be taken as an upper bound for any adaptation strategy trying to exploit parts of the EP corpus , while those of the first line clearly provide the corresponding lower bound .", "The system in the second row can instead be consider as the lower bound when only monolingual English adaptation data are assumed .", "The synthesis of the S E EP corpus was performed with the system trained just on the UN training data first row of Table 2 , because we had assumed that the in domain data were only monolingual Spanish and thus not useful for neither the TM , RM nor target LM estimation .", "Similarly , the system in the last row of Table 2 was developed on the UN corpus to translate the English part of the EP data to generate the synthetic SE EP corpus .", "Again , any in domain data were exploited to train this sytem .", "Of course , this system cannot be compared with any other because of the different translation direction .", "In order to compare reported performance with the state of the art , Table 2 also reports results of the best system published in the EuroMatrix project website6 and of the Google online translation engine . 7 It is well known that tuning the SMT system is fundamental to achieve good performance .", "The standard tuning procedure consists of a minimum error rate training mert Och and Ney , 2003 which relies on the availability of a development data set .", "On the other hand , the most important assumption we make is that almost no parallel indomain data are available . the tuning process and BLEU score achieved on the test set by the uniform interpolation weights first row , and by the optimal weights with different configurations of the tuning parameters .", "In a preliminary phase , we investigated different settings of the tuning process in order to understand how much development data is required to perform a reliable weight optimization .", "Our models were trained on the S E EP parallel corpus and by using uniform interpolation weights the system achieved a BLEU score of 22 . 28 on the test set see Table 3 .", "We assumed to dispose of either a regular in domain development set of 2 , 000 sentences dev2006 , or a small portion of it of just 200 sentences .", "Moreover , we tried to employ either 1 , 000best or 200 best translation candidates during the mert process .", "From a theoretical point of view , computational effort of the tuning process is proportional to the square of the number of translation alternatives generated at each iteration times the number of iterations until convergence .", "Figure 1 reports incremental tuning time and translation performance on the test set at each iteration .", "Notice that the four tuning configurations are ranked in order of complexity .", "Table 3 summaries the final performance of each tuning process , after convergence was reached .", "Notice that decoding time is not included in this plot , as Moses allows to perform this step in parallel on a computer cluster .", "Hence , to our view the real bottleneck of the tuning process is actually related to the strictly serial part of the mert implementation of Moses .", "As already observed in previous literature Macherey et al . , 2008 , first iterations of the tuning process produces very bad weights even close to 0 ; this exceptional performance drop is attributed to an over fitting on the candidate repository .", "Configurations exploiting the small development set c , d show a slower and more unstable convergence ; however , their final performance in Table 3 result only slightly lower than that obtained with the standard dev sets a , b .", "Due to the larger number of iterations they needed , both configurations are indeed more time consuming than the intermediate configuration b , which seems the best one .", "In conclusion , we found that the size of the n best list has essentially no effect on the quality of the final weights , but it impacts significantly on the computational time .", "Moreover , using the regular development set with few translation alternatives ends up to be the most efficient configuration in terms of computational effort , robustness , and performance .", "Our analysis suggests that it is important to dispose of a sufficiently large development set although reasonably good weights can be obtained even if such data are very few .", "A set of experiments was devoted to the adaptation of the LM only .", "We trained three different LMs on increasing portions of the EP and we employed them either alone or in combination with the background LM trained on the UN corpus .", "Percentage of monolingual English adaptation data systems .", "The absolute gain with respect to the baseline is fairly high , even with the smallest amount of adaptation data 4 . 02 .", "The benefit of using the background data together with indomain data is very small , and rapidly vanishes as the amount of such data increases .", "If English synthetic texts are employed to adapt the LM component , the increase in performance is significantly lower but still remarkable see Table 2 .", "By employing all the available data , the gain in BLEU score was of 4 relative , that is from 22 . 60 to 23 . 52 . opment set as usual .", "Results of these experiments are reported in Figure 3 .", "Results suggest that regardless of the used bilingual corpora the in domain TMs and RMs work better alone than combined with the original models .", "We think that this behavior can be explained by a limited disciminative power of the resulting combined model .", "The background translation model could contain phrases which either do or do not fit the adaptation domain .", "As the weights are optimized to balance the contribution of all phrases , the system is not able to well separate the positive examples from the negative ones .", "In addition to it , system tuning is much more complex because the number of features increases from 14 to 26 .", "Finally , TMs and RMs estimated from synthetic data show to provide smaller , but consistent , contributions than the corresponding LMs .", "When English in domain data is provided , BLEU score increases from 22 . 60 to 28 . 10 ; TM and RM contribute by about 5 relative , by covering the gap from 27 . 83 to 28 . 10 .", "When Spanish in domain data is provided BLEU score increases from 22 . 60 to 23 . 68 ; TM and RM contribute by about 15 relative , by covering the gap from 23 . 52 to 23 . 68 .", "Summarizing , the most important role in the domain adaptation is played by the LM ; nevertheless the adaptation of the TM and RM gives a small further improvement . . Another set of experiments relates to the adaptation of the TM and the RM .", "In domain TMs and RMs were estimated on three different versions of the full parallel EP corpus , namely EP , S E EP , and SE EP .", "In domain LMs were trained on the corresponding English side .", "All in domain models were either used alone or combined with the baseline models according to multiple model paradigm explained in Section 4 . 3 .", "Tuning of the interpolation weights was performed on the standard devel", "This paper investigated cross domain adaptation of a state of the art SMT system Moses , by exploiting large but cheap monolingual data .", "We proposed to generate synthetic parallel data by translating monolingual adaptation data with a background system and to train statistical models from the synthetic corpus .", "We found that the largest gain 25 relative is achieved when in domain data are available for the target language .", "A smaller performance improvement is still observed 5 relative if source adaptation data are available .", "We also observed that the most important role is played by the LM adaptation , while the adaptation of the TM and RM gives consistent but small improvement .", "We also showed that a very tiny development set of only 200 parallel sentences is adequate enough to get comparable performance as a 2000 sentence set .", "Finally , we described how to reduce the time for training models from a synthetic corpus generated through Moses by 50 at least , by exploiting word alignment information provided during decoding ."], "summary_lines": ["Domain Adaptation for Statistical Machine Translation with Monolingual Resources\n", "Domain adaptation has recently gained interest in statistical machine translation to cope with the performance drop observed when testing conditions deviate from training conditions.\n", "The basic idea is that in-domain training data can be exploited to adapt all components of an already developed system.\n", "Previous work showed small performance gains by adapting from limited in-domain bilingual data.\n", "Here, we aim instead at significant performance gains by exploiting large but cheap monolingual in-domain data, either in the source or in the target language.\n", "We propose to synthesize a bilingual corpus by translating the monolingual adaptation data into the counterpart language.\n", "Investigations were conducted on a state-of-the-art phrase-based system trained on the Spanish\u2013English part of the UN corpus, and adapted on the corresponding Europarl data.\n", "Translation, re-ordering, and language models were estimated after translating in-domain texts with the baseline.\n", "By optimizing the interpolation of these models on a development set the BLEU score was improved from 22.60% to 28.10% on a test set.\n", "In order to use source-side monolingual data, we employ the transductive learning to first translate the source-side monolingual data using the best configuration (baseline+in-domain lexicon+indomain language model) and obtain 1-best translation for each source-side sentence.\n", "We adapt an SMT system with automatic translations and trained the translation and reordering models on the word alignment used by moses.\n"]}
{"article_lines": ["A Discriminative Matching Approach To Word Alignment", "We present a discriminative , large margin approach to feature based matching for word alignment .", "In thisframework , pairs of word tokens re ceive a matching score , which is basedon features of that pair , including mea sures of association between the words , distortion between their positions , sim ilarity of the orthographic form , and soon .", "Even with only 100 labeled train ing examples and simple features whichincorporate counts from a large unlabeled corpus , we achieve AER perfor mance close to IBM Model 4 , in muchless time .", "Including Model 4 predic tions as features , we achieve a relativeAER reduction of 22 in over inter sected Model 4 alignments .", "The standard approach to word alignment from sentence aligned bitexts has been to constructmodels which generate sentences of one language from the other , then fitting those genera tive models with EM Brown et al , 1990 ; Och and Ney , 2003 .", "This approach has two primary advantages and two primary drawbacks .", "In itsfavor , generative models of alignment are wellsuited for use in a noisy channel translation system .", "In addition , they can be trained in an un supervised fashion , though in practice they do require labeled validation alignments for tuning model hyper parameters , such as null counts orsmoothing amounts , which are crucial to pro ducing alignments of good quality .", "A primarydrawback of the generative approach to alignment is that , as in all generative models , explicitly incorporating arbitrary features of the in put is difficult .", "For example , when considering whether to align two words in the IBM models Brown et al , 1990 , one cannot easily include information about such features as orthographic similarity for detecting cognates , presence of the pair in various dictionaries , similarity of the frequency of the two words , choices made by other alignment systems on this sentence pair , and so on .", "While clever models can implicitly capture some of these information sources , ittakes considerable work , and can make the resulting models quite complex .", "A second draw back of generative translation models is that , since they are learned with EM , they require extensive processing of large amounts of data to achieve good performance .", "While tools likeGIZA Och and Ney , 2003 do make it eas ier to build on the long history of the generativeIBM approach , they also underscore how com plex high performance generative models can , and have , become . In this paper , we present a discriminative ap proach to word alignment .", "Word alignment is cast as a maximum weighted matching problem Cormen et al , 1990 in which each pair of words e j , f k in a sentence pair e , f is associated with a score s jk e , f reflecting the desirability of the alignment of that pair .", "The alignment 73 for the sentence pair is then the highest scoring matching under some constraints , for example the requirement that matchings be one to one .", "This view of alignment as graph matching isnot , in itself , new Melamed 2000 uses com petitive linking to greedily construct matchingswhere the pair score is a measure of word to word association , and Matusov et al 2004 find exact maximum matchings where the pair scores come from the alignment posteriors of generative models .", "Tiedemann 2003 proposes incorporating a variety of word association ? clues ?", "into a greedy linking algorithm . What we contribute here is a principled ap proach for tractable and efficient learning of the alignment score s jk e , f as a function of arbitrary features of that token pair .", "This con tribution opens up the possibility of doing the kind of feature engineering for alignment that has been so successful for other NLP tasks .", "Wefirst present the algorithm for large margin es timation of the scoring function .", "We then showthat our method can achieve AER rates com parable to unsymmetrized IBM Model 4 , usingextremely little labeled data as few as 100 sen tences and a simple feature set .", "Remarkably , by including bi directional IBM Model 4 predic tions as features , we achieve an absolute AER of 5 . 4 on the English French Hansards alignmenttask , a relative reduction of 22 in AER over intersected Model 4 alignments and , to our knowl edge , the best AER result published on this task .", "We model the alignment prediction task as a maximum weight bipartite matching problem , where nodes correspond to the words in the two sentences .", "For simplicity , we assume here that each word aligns to one or zero words in the other sentence .", "The edge weight s jkrepre sents the degree to which word j in one sentencecan translate into the word k in the other sen tence .", "Our goal is to find an alignment that maximizes the sum of edge scores .", "We represent a matching using a set of binary variables y jk that are set to 1 if word j is assigned to word k in the other sentence , and 0 otherwise .", "The score of an assignment is the sum of edge scores s y ? jk s jk y jk . The maximum weight bi .", "partite matching problem , arg maxy ? Y s y , canbe solved using well known combinatorial algo rithms or the following linear program max z ? jk s jk z jk 1 s . t . ? j z jk ? 1 , ? k z jk ? 1 , 0 ? z jk ? 1 , where the continuous variables z jk correspond to the binary variables y jk . This LP is guaranteed .", "to have integral and hence optimal solutions for any scoring function s y Schrijver , 2003 .", "Note that although the above LP can be used to compute alignments , combinatorial algorithms are generally more efficient .", "However , we use the LP to develop the learning algorithm below .", "For a sentence pair x , we denote position pairs by x jk and their scores as s jk . We let .", "s jk wf x jk for some user provided fea ture mapping f and abbreviate wf x , y ? jk y jk wf x jk .", "We can include in the fea ture vector the identity of the two words , their relative positions in their respective sentences , their part of speech tags , their string similarity for detecting cognates , and so on .", "At this point , one can imagine estimating alinear matching model in multiple ways , includ ing using conditional likelihood estimation , anaveraged perceptron update see which matchings are proposed and adjust the weights ac cording to the difference between the guessed and target structures Collins , 2002 , or inlarge margin fashion .", "Conditional likelihood es timation using a log linear model P y x 1 Z w x exp wf x , y requires summing over all matchings to compute the normalization Zw x , which is P complete Valiant , 1979 .", "In ourexperiments , we therefore investigated the aver aged perceptron in addition to the large margin method outlined below .", "2 . 1 Large margin estimation .", "We follow the large margin formulation of Taskar et al 2005a .", "Our input is a set of training instances x i , y i m i 1 , where each in stance consists of a sentence pair x i and a target 74 alignment y i . We would like to find parameters .", "w that predict correct alignments on the train ing data y i arg max ? y i ? Y i wf x i , y ?", "i , ? i , where Y i is the space of matchings appropriate for the sentence pair i . In standard classification problems , we typi cally measure the error of prediction ,", "y i , y ?", "i , using the simple 0 1 loss .", "In structured prob lems , where we are jointly predicting multiple variables , the loss is often more complex .", "While the F measure is a natural loss function for this task , we instead chose a sensible surrogate that fits better in our framework Hamming distance between y i and y ?", "i , which simply counts the number of edges predicted incorrectly .", "We use an SVM like hinge upper bound on the loss", "y i , y ?", "i , given by max ? y i ? Y i wf i y ? i", "i y ? i ? wf i y i , where", "i y ? i", "y i , y ?", "i , and f i y ? i f x i , y ?", "i .", "Minimizing this upper bound encourages the true alignment y i to be optimal with respect to w for each instance i min w ? ?", "i max ? y i ? Y i wf i y ? i", "i y ? i ? wf i y i , where ? is a regularization parameter . In this form , the estimation problem is a mixture of continuous optimization over w and com binatorial optimization over y i . In order to .", "transform it into a more standard optimization problem , we need a way to efficiently handle the loss augmented inference , max ? y i ? Y i wf i y ? i", "i y ? i .", "This optimization problem has precisely the same form as the prediction prob lem whose parameters we are trying to learn ? max ? y i ? Y i wf i y ? i ? but with an additionalterm corresponding to the loss function .", "Our as sumption that the loss function decomposes over the edges is crucial to solving this problem .", "In particular , we use weighted Hamming distance , which counts the number of variables in which a candidate solution y ?", "i differs from the target output y i , with different cost for false positives c and false negatives c", "i y ? i ? jk c y i , jk 1 ? y ? i , jk c y ? i , jk 1 ? y i , jk ? jk c y i , jk ? jk c ?", "c c y i , jk y ? i , jk . The loss augmented matching problem can thenbe written as an LP similar to Equation 1 with out the constant term ? jk c y i , jk max z ? jk z i , jk wf x i , jk c ?", "c c y i , jk s . t . ? j z i , jk ? 1 , ? k z i , jk ? 1 , 0 ? z i , jk ? 1 .", "Hence , without any approximations , we have a continuous optimization problem instead of a combinatorial one max ? y i ? Y i wf i y ? i", "i y ? i d i max z i ? Z i wF i c i z i , where d i ? jk c y i , jk is the constant term , F i is the appropriate matrix that has a column of features f x i , jk for each edge jk , c i is the vector of the loss terms c ?", "c c y i , jk and finally Z i z i ? j z i , jk ? 1 , ? k z i , jk ? 1 , 0 ? z i , jk ? 1 .", "Plugging this LP back into our estimation problem , we have min w ? ?", "max z ? Z ? i wF i z i c i z i ? wF i y i , 2 where z z 1 , . . .", ", z m , Z Z 1 ? .", ". ? Z m . In .", "stead of the derivation in Taskar et al 2005a , which produces a joint convex optimization problem using Lagrangian duality , here we tackle the problem in its natural saddle point form .", "2 . 2 The extragradient method .", "For saddle point problems , a well known solution strategy is the extragradient method Ko rpelevich , 1976 , which is closely related to projected gradient methods .", "The gradient of the objective in Equation 2 is given by ? i F i z i ? y i with respect to w and F i w c i with respect to each z i .", "We de note the Euclidean projection of a vector onto Z i as P Z i v arg minu ? Z i v ? u and pro jection onto the ball w ? ?", "as P ?", "w ? w max ? , w .", "75An iteration of the extragradient method con sists of two very simple steps , prediction w ? t 1 P ?", "wt ? k ? i F i y i ? zt i ; z ? t 1 i P Z i zt i ? k F i wt c i ; and correction wt 1 P ?", "wt ? k ? i F i y i ? z ? t 1 i ; zt 1 i P Z i zt i ? k F i w ? t 1 c i , where ? k are appropriately chosen step sizes .", "The method is guaranteed to converge linearly to a solution w ? , z ?", "Korpelevich , 1976 ; He and Liao , 2002 ; Taskar et al , 2005b .", "Please see www . cs . berkeley . edu taskar extragradient . pdf for more details . The key subroutine of the algorithm is Eu clidean projection onto the feasible sets Z i . In .", "case of word alignment , Z i is the convex hull of bipartite matchings and the problem reduces to the much studied minimum cost quadratic flow problem Bertsekas et al , 1997 .", "The projection problem P Z i z ? i is given by min z ? jk 1 2 z ? i , jk ? z i , jk 2 s . t . ? j z i , jk ? 1 , ? k z i , jk ? 1 , 0 ? z i , jk ? 1 . We can now use a standard reduction of bipar tite matching to min cost flow by introducing a source node connected to all the words in one sentence and a sink node connected to all thewords in the other sentence , using edges of ca pacity 1 and cost 0 .", "The original edges jk have a quadratic cost 1 2 z ? i , jk ? z i , jk 2 and capacity 1 .", "Now the minimum cost flow from the source to the sink computes projection of z ?", "i onto Z i We use standard , publicly available code for solving this problem Guerriero and Tseng , 2002 .", "We applied this matching algorithm to word level alignment using the English French Hansards data from the 2003 NAACL shared task Mihalcea and Pedersen , 2003 .", "This corpus consists of 1 . 1M automatically aligned sentences , and comes with a validation set of 39 sentence pairs and a test set of 447 sentences .", "The validation and test sentences have been hand aligned see Och and Ney 2003 and are marked with both sure and possible alignments .", "Using these alignments , alignment error rate AER is calculated as AER A , S , P 1 ? A ? S A ? P A S Here , A is a set of proposed index pairs , S is the sure gold pairs , and P is the possible goldpairs .", "For example , in Figure 1 , proposed align ments are shown against gold alignments , with open squares for sure alignments , rounded open squares for possible alignments , and filled black squares for proposed alignments .", "Since our method is a supervised algorithm , we need labeled examples .", "For the training data , we split the original test set into 100 trainingexamples and 347 test examples .", "In all our ex periments , we used a structured loss function", "y i , y ?", "i that penalized false negatives 3 times more than false positives , where 3 was picked bytesting several values on the validation set .", "In stead of selecting a regularization parameter ? and running to convergence , we used early stopping as a cheap regularization method , by set ting ? to a very large value 10000 and running the algorithm for 500 iterations .", "We selected a stopping point using the validation set by simply picking the best iteration on the validation set in terms of AER ignoring the initial ten iterations , which were very noisy in our experiments .", "All selected iterations turned out to be in the first 50 iterations , as the algorithm converged fairly rapidly .", "3 . 1 Features and Results .", "Very broadly speaking , the classic IBM mod els of word level translation exploit four primary sources of knowledge and constraint association of words all IBM models , competition betweenalignments all models , zero or first order preferences of alignment positions 2 , 4 , and fer tility 3 .", "We model all of these in some way , 76 on e of th e ma jo r ob je ct iv es of th es e co ns ul ta ti on s is to ma ke su re th at th e re co ve ry be ne fi ts al l . le un de les grands objectifs de les consultations est de faire en sorte que la relance profite e ? galement a tous . on e of th e ma jo r ob je ct iv es of th es e co ns ul ta ti on s is to ma ke su re th at th e re co ve ry be ne fi ts al l . le un de les grands objectifs de les consultations est de faire en sorte que la relance profite e ? galement a tous .", "a Dice only b Dice and Distance on e of th e ma jo r ob je ct iv es of th es e co ns ul ta ti on s is to ma ke su re th at th e re co ve ry be ne fi ts al l . le un de les grands objectifs de les consultations est de faire en sorte que la relance profite e ? galement a tous . on e of th e ma jo r ob je ct iv es of th es e co ns ul ta ti on s is to ma ke su re th at th e re co ve ry be ne fi ts al l . le un de les grands objectifs de les consultations est de faire en sorte que la relance profite e ? galement a tous .", "c Dice , Distance , Orthographic , and BothShort d All features Figure 1 Example alignments for each successive feature set .", "except fertility . 1First , and , most importantly , we want to include information about word association ; trans lation pairs are likely to co occur together in a bitext .", "This information can be captured , among many other ways , using a feature whose 1In principle , we can model also model fertility , by allowing 0 k matches for each word rather than 0 1 , and having bias features on each word .", "However , we did not explore this possibility .", "value is the Dice coefficient Dice , 1945 Dice e , f 2CEF e , f C E e C F f Here , C E and C F are counts of word occurrences in each language , while C EF is the number of co occurrences of the two words .", "With just this feature on a pair of word tokens which depends only on their types , we can already make a stab 77 at word alignment , aligning , say , each English word with the French word or null with thehighest Dice value see Melamed , 2000 , sim ply as a matching free heuristic model .", "With Dice counts taken from the 1 . 1M sentences , thisgives and AER of 38 . 7 with English as the tar get , and 36 . 0 with French as the target in line with the numbers from Och and Ney 2003 .", "As observed in Melamed 2000 , this use ofDice misses the crucial constraint of competition a candidate source word with high asso ciation to a target word may be unavailable for alignment because some other target has an even better affinity for that source word .", "Melameduses competitive linking to incorporate this con straint explicitly , while the IBM style models get this effect via explaining away effects in EM training .", "We can get something much like the combination of Dice and competitive linking by running with just one feature on each pair the Dice value of that pair ? s words . 2 With just a Dice feature ? meaning no learning is needed yet ? we achieve an AER of 29 . 8 , between the Dice with competitive linking result of 34 . 0 and Model 1 of 25 . 9 given in Och and Ney 2003 .", "An example of the alignment at this stage is shown in Figure 1 a .", "Note that most errors lie off the diagonal , for example the often correct to a match .", "IBM Model 2 , as usually implemented , addsthe preference of alignments to lie near the di agonal .", "Model 2 is driven by the product of a word to word measure and a usually Gaussian distribution which penalizes distortion from thediagonal .", "We can capture the same effect using features which reference the relative posi tions j and k of a pair e j , f k .", "In addition to aModel 2 style quadratic feature referencing relative position , we threw in the following proximity features absolute difference in relative posi tion abs j e ? k f , and the square and squareroot of this value .", "In addition , we used a con junction feature of the dice coefficient times the proximity .", "Finally , we added a bias feature on each edge , which acts as a threshold that allows 2This isn ? t quite competitive linking , because we use a non greedy matching .", "in 19 78 Am er ic an s di vo rc ed 1 , 12 2 , 00 0 ti me s . en 1978 , on a enregistre ?", "1 , 122 , 000 divorces sur le continent . in 19 78 Am er ic an s di vo rc ed 1 , 12 2 , 00 0 ti me s . en 1978 , on a enregistre ?", "1 , 122 , 000 divorces sur le continent .", "a b Figure 2 Example alignments showing the ef fects of orthographic cognate features .", "a Dice and Distance , b With Orthographic Features .", "sparser , higher precision alignments .", "With these features , we got an AER of 15 . 5 compare to 19 . 5 for Model 2 in Och and Ney , 2003 .", "Note that we already have a capacity that Model 2 does not we can learn a non quadratic penalty with linear mixtures of our various components ? this gives a similar effect to learning the variance of the Gaussian for Model 2 , but is , at least in principle , more flexible . 3 These features fix the to a error in Figure 1 a , giving the alignment in Figure 1 b .", "On top of these features , we included other kinds of information , such as word similarityfeatures designed to capture cognate and ex act match information .", "We added a feature forexact match of words , exact match ignoring accents , exact matching ignoring vowels , and frac tion overlap of the longest common subsequence .", "Since these measures were only useful for long words , we also added a feature which indicatesthat both words in a pair are short .", "These or thographic and other features improved AER to14 . 4 .", "The running example now has the align ment in Figure 1 c , where one improvement may be attributable to the short pair feature ? it has stopped proposing the de , partially because the short pair feature downweights the score of that pair .", "A clearer example of these features making a difference is shown in Figure 2 , whereboth the exact match and character overlap fea 3The learned response was in fact close to a Gaussian , but harsher near zero displacement .", "78 tures are used .", "One source of constraint which our model stilldoes not explicitly capture is the first order de pendency between alignment positions , as in theHMM model Vogel et al , 1996 and IBM models 4 .", "The the le error in Figure 1 c is symp tomatic of this lack .", "In particular , it is a slightly better pair according to the Dice value than the correct the les .", "However , the latter alignment has the advantage that major grands follows it .", "To use this information source , we included a feature which gives the Dice value of the wordsfollowing the pair . 4 We also added a word frequency feature whose value is the absolutedifference in log rank of the words , discourag ing very common words from translating to very rare ones .", "Finally , we threw in bilexical features of the pairs of top 5 non punctuation words ineach language . 5 This helped by removing spe cific common errors like the residual tendency for French de to mistakenly align to English the the two most common words .", "The resulting model produces the alignment in Figure 1 d .", "It has sorted out the the le the les confusion , and is also able to guess to de , which is not the most common translation for either word , but which is supported by the good Dice value on the following pair make faire .", "With all these features , we got a final AER of 10 . 7 , broadly similar to the 8 . 9 or 9 . 7 AERs of unsymmetrized IBM Model 4 trained on the same data that the Dice counts were takenfrom . 6 Of course , symmetrizing Model 4 by in tersecting alignments from both directions does yield an improved AER of 6 . 9 , so , while ourmodel does do surprisingly well with cheaply ob tained count based features , Model 4 does still outperform it so far .", "However , our model can4It is important to note that while our matching algo rithm has no first order effects , the features can encode such effects in this way , or in better ways ? e . g . using as features posteriors from the HMM model in the style of Matusov et al 2004 .", "5The number of such features which can be learned depends on the number of training examples , and since some of our experiments used only a few dozen training examples we did not make heavy use of this feature .", "6Note that the common word pair features affectedcommon errors and therefore had a particularly large im pact on AER .", "Model AER Dice without matching 38 . 7 36 . 0 Model 4 E F , F E , intersected 8 . 9 9 . 7 6 . 9 Discriminative Matching Dice Feature Only 29 . 8 Distance Features 15 . 5 Word Shape and Frequency 14 . 4 Common Words and Next Dice 10 . 7 Model 4 Predictions 5 . 4 Figure 3 AER on the Hansards task .", "also easily incorporate the predictions of Model 4 as additional features .", "We therefore added three new features for each edge the prediction of Model 4 in the English French direction , the prediction in the French English direction , and the intersection of the two predictions .", "With these powerful new features , our AER dropped dramatically to 5 . 4 , a 22 improvement over the intersected Model 4 performance . Another way of doing the parameter estima tion for this matching task would have been to use an averaged perceptron method , as in Collins 2002 .", "In this method , we merely run our matching algorithm and update weights based on the difference between the predictedand target matchings .", "However , the perfor mance of the average perceptron learner on the same feature set is much lower , only 8 . 1 , not even breaking the AER of its best single feature the intersected Model 4 predictions .", "3 . 2 Scaling Experiments .", "We explored the scaling of our method by learn ing on a larger training set , which we created by using GIZA intersected bi directional Model 4 alignments for the unlabeled sentence pairs .", "We then took the first 5K sentence pairs from these 1 . 1M Model 4 alignments .", "This gave us more training data , albeit with noisier labels .", "On a 3 . 4GHz Intel Xeon CPU , GIZA took 18 hours to align the 1 . 1M words , while ourmethod learned its weights in between 6 min utes 100 training sentences and three hours 5K sentences .", "We have presented a novel discriminative , large margin method for learning word alignment models on the basis of arbitrary features of wordpairs .", "We have shown that our method is suitable for the common situation where a moder ate number of good , fairly general features must be balanced on the basis of a small amount of labeled data .", "It is also likely that the method will be useful in conjunction with a large labeled alignment corpus should such a set be created .", "We presented features capturing a few separate sources of information , producing alignments on the order of those given by unsymmetrized IBM Model 4 using labeled training data of about the size others have used to tune generative models .", "In addition , when given bi directional Model 4 predictions as features , our method provides a 22 AER reduction over intersected Model 4 predictions alone .", "The resulting 5 . 4 AER on the English French Hansarks task is , to our knowledge , the best published AER fig ure for this training scenario though since we use a subset of the test set , evaluations are not problem free .", "Finally , our method scales to large numbers of training sentences and trains in minutes rather than hours or days for thehigher numbered IBM models , a particular ad vantage when not using features derived from those slower models ."], "summary_lines": ["A Discriminative Matching Approach To Word Alignment\n", "We present a discriminative, large-margin approach to feature-based matching for word alignment.\n", "In this framework, pairs of word tokens receive a matching score, which is based on features of that pair, including measures of association between the words, distortion between their positions, similarity of the orthographic form, and so on.\n", "Even with only 100 labeled training examples and simple features which incorporate counts from a large unlabeled corpus, we achieve AER performance close to IBM Model 4, in much less time.\n", "Including Model 4 predictions as features, we achieve a relative AER reduction of 22% in over intersected Model 4 alignments.\n", "We use a large margin approach by factoring the structure level constraints to constraints at the level of an alignment link.\n", "We use a one-to-one constraint, where words in either sentence can participate in at most one link.\n", "We cast the problem of alignment as a maximum weight bipartite matching problem, where nodes correspond to the words in the two sentences.\n"]}
{"article_lines": ["Statistical Sentence Condensation Using Ambiguity Packing And Stochastic Disambiguation Methods For Lexical Functional Grammar", "We present an application of ambiguity packing and stochastic disambiguation techniques for Lexical Functional Grammars LFG to the domain of sentence condensation .", "Our system incorporates a linguistic parser generator for LFG , a transfer component for parse reduction operating on packed parse forests , and a maximum entropy model for stochastic output selection .", "Furthermore , we propose the use of standard parser evaluation methods for automatically evaluating the summarization quality of sentence condensation systems .", "An experimental evaluation of summarization quality shows a close correlation between the automatic parse based evaluation and a manual evaluation of generated strings .", "Overall summarization quality of the proposed system is state of the art , with guaranteed grammaticality of the system output due to the use of a constraint based parser generator .", "Recent work in statistical text summarization has put forward systems that do not merely extract and concatenate sentences , but learn how to generate new sentences from Summary , Text tuples .", "Depending on the chosen task , such systems either generate single sentence headlines for multi sentence text Witbrock and Mittal , 1999 , or they provide a sentence condensation module designed for combination with sentence extraction systems Knight and Marcu , 2000 ; Jing , 2000 .", "The challenge for such systems is to guarantee the grammaticality and summarization quality of the system output , i . e . the generated sentences need to be syntactically wellformed and need to retain the most salient information of the original document .", "For example a sentence extraction system might choose a sentence like The UNIX operating system , with implementations from Apples to Crays , appears to have the advantage . from a document , which could be condensed as UNIX appears to have the advantage .", "In the approach of Witbrock and Mittal 1999 , selection and ordering of summary terms is based on bagof words models and n grams .", "Such models may well produce summaries that are indicative of the original s content ; however , n gram models seem to be insufficient to guarantee grammatical well formedness of the system output .", "To overcome this problem , linguistic parsing and generation systems are used in the sentence condensation approaches of Knight and Marcu 2000 and Jing 2000 .", "In these approaches , decisions about which material to include delete in the sentence summaries do not rely on relative frequency information on words , but rather on probability models of subtree deletions that are learned from a corpus of parses for sentences and their summaries .", "A related area where linguistic parsing systems have been applied successfully is sentence simplification .", "Grefenstette 1998 presented a sentence reduction method that is based on finite state technology for linguistic markup and selection , and Carroll et al . 1998 present a sentence simplification system based on linguistic parsing .", "However , these approaches do not employ statistical learning techniques to disambiguate simplification decisions , but iteratively apply symbolic reduction rules , producing a single output for each sentence .", "The goal of our approach is to apply the fine grained tools for stochastic Lexical Functional Grammar LFG parsing to the task of sentence condensation .", "The system presented in this paper is conceptualized as a tool that can be used as a standalone system for sentence condensation or simplification , or in combination with sentence extraction for text summarization beyond the sentence level .", "In our system , to produce a condensed version of a sentence , the sentence is first parsed using a broad coverage LFG grammar for English .", "The parser produces a set of functional f structures for an ambiguous sentence in a packed format .", "It presents these to the transfer component in a single packed data structure that represents in one place the substructures shared by several different interpretations .", "The transfer component operates on these packed representations and modifies the parser output to produce reduced f structures .", "The reduced f structures are then filtered by the generator to determine syntactic well formedness .", "A stochastic disambiguator using a maximum entropy model is trained on parsed and manually disambiguated f structures for pairs of sentences and their condensations .", "Using the disambiguator , the string generated from the most probable reduced f structure produced by the transfer system is chosen .", "In contrast to the approaches mentioned above , our system guarantees the grammaticality of generated strings through the use of a constraint based generator for LFG which uses a slightly tighter version of the grammar than is used by the parser .", "As shown in an experimental evaluation , summarization quality of our system is high , due to the combination of linguistically fine grained analysis tools and expressive stochastic disambiguation models .", "A second goal of our approach is to apply the standard evaluation methods for parsing to an automatic evaluation of summarization quality for sentence condensation systems .", "Instead of deploying costly and non reusable human evaluation , or using automatic evaluation methods based on word error rate or n gram match , summarization quality can be evaluated directly and automatically by matching the reduced f structures that were produced by the system against manually selected f structures that were produced by parsing a set of manually created condensations .", "Such an evaluation only requires human labor for the construction and manual structural disambiguation of a reusable gold standard test set .", "Matching against the test set can be done automatically and rapidly , and is repeatable for development purposes and system comparison .", "As shown in an experimental evaluation , a close correspondence can be established for rankings produced by the f structure based automatic evaluation and a manual evaluation of generated strings .", "In this section , each of the system components will be described in more detail .", "In this project , a broad coverage LFG grammar and parser for English was employed see Riezler et al . 2002 .", "The parser produces a set of context free constituent c structures and associated functional f structures for each input sentence , represented in packed form see Maxwell and Kaplan 1989 .", "For sentence condensation we are only interested in the predicate argument structures encoded in f structures .", "For example , Fig .", "1 shows an f structure manually selected out of the 40 f structures for the sentence A prototype is ready for testing , and Leary hopes to set requirements for a full system by the end of the year .", "The transfer component for the sentence condensation system is based on a component previously used in a machine translation system see Frank 1999 .", "It consists of an ordered set of rules that rewrite one f structure into another .", "Structures are broken down into flat lists of facts , and rules may add , delete , or change individual facts .", "Rules may be optional or obligatory .", "In the case of optional rules , transfer of a single input structure may lead to multiple alternate output structures .", "The transfer component is designed to operate on packed input from the parser and can also produce packed representations of the condensation alternatives , using methods adapted from parse packing . 1 An example rule that optionally removes an adjunct is shown below This rule eliminates an adjunct , Z , by deleting the fact that Z is contained within the set of adjuncts , Y , associated with the expression X .", "The before the adjunct X , Y fact marks this fact as one that needs to be present for the rule to be applied , but which is left unaltered by the rule application .", "The in set Z , Y fact is deleted .", "Two new facts are added . delete node Z , r1 indicates that the structure rooted at node Z is to be deleted , and rule trace r1 , del Z , X adds a trace of this rule to an accumulating history of rule applications .", "This history records the relation of transferred f structures to the original f structure and is available for stochastic disambiguation . slept .", ", the optional deletion of parts of coordinate structures e . g . , They laughed and giggled . can become They giggled .", ", and certain simplifications e . g .", "It is clear that the earth is round . can become The earth is round . but It seems that he is asleep . cannot become He is asleep . .", "For example , one possible post transfer output of the sentence in Fig .", "1 is shown in Fig .", "The transfer rules are independent of the grammar and are not constrained to preserve the grammaticality or wellformedness of the reduced f structures .", "Some of the reduced structures therefore may not correspond to any English sentence , and these are eliminated from future consideration by using the generator as a filter .", "The filtering is done by running each transferred structure through the generator to see whether it produces an output string .", "If it does not , the structure is rejected .", "For example , for the f structure in Fig .", "1 , the transfer system proposed 32 possible reductions .", "After filtering these structures by generation , 16 reduced f structures comprising possible condensations of the input sentence survive .", "The 16 wellformed structures correspond to the following strings that were outputted by the generator note that a single structure may correspond to more than one string and a given string may correspond to more than one structure A prototype is ready .", "A prototype is ready for testing .", "Leary hopes to set requirements for a full system .", "A prototype is ready and Leary hopes to set requirements for a full system .", "A prototype is ready for testing and Leary hopes to set requirements for a full system .", "Leary hopes to set requirements for a full system by the end of the year .", "A prototype is ready and Leary hopes to set requirements for a full system by the end of the year .", "A prototype is ready for testing and Leary hopes to set requirements for a full system by the end of the year .", "In order to guarantee non empty output for the overall condensation system , the generation component has to be fault tolerant in cases where the transfer system operates on a fragmentary parse , or produces non valid fstructures from valid input f structures .", "Robustness techniques currently applied to the generator include insertion and deletion of features in order to match invalid transferoutput to the grammar rules and lexicon .", "Furthermore , repair mechanisms such as repairing subject verb agreement from the subject s number value are employed .", "As a last resort , a fall back mechanism to the original uncondensed f structure is used .", "These techniques guarantee that a non empty set of reduced f structures yielding grammatical strings in generation is passed on to the next system component .", "In case of fragmentary input to the transfer component , grammaticaliy of the output is guaranteed for the separate fragments .", "In other words , strings generated from a reduced fragmentary f structure will be as grammatical as the string that was fed into the parsing component .", "After filtering by the generator , the remaining fstructures were weighted by the stochastic disambiguation component .", "Similar to stochastic disambiguation for constraint based parsing Johnson et al . , 1999 ; Riezler et al . , 2002 , an exponential a . k . a . log linear or maximumentropy probability model on transferred structures is estimated from a set of training data .", "The data for estimation consists of pairs of original sentences y and goldstandard summarized f structures s which were manually selected from the transfer output for each sentence .", "For training data I sj , yj Imj 1 and a set of possible summarized structures S y for each sentence y , the objective was to maximize a discriminative criterion , namely the conditional likelihood L A of a summarized f structure given the sentence .", "Optimization of the function shown below was performed using a conjugate gradient optiAt the core of the exponential probability model is a vector of property functions f to be weighted by parameters A .", "For the application of sentence condensation , 13 , 000 property functions of roughly three categories were used A trained probability model is applied to unseen data by selecting the most probable transferred f structure , yielding the string generated from the selected structure as the target condensation .", "The transfered f structure chosen for our current example is shown in Fig .", "This structure was produced by the following set of transfer rules , where var refers to the indices in the representation of the f structure rtrace r13 , keep var 98 , of , rtrace r161 , keep system , var 85 , rtrace r1 , del var 91 , set , by , rtrace r1 , del var 53 , be , for , rtrace r20 , equal var 1 , and , rtrace r20 , equal var 2 , and , rtrace r2 , del var 1 , hope , and , rtrace r22 , delb var 0 , and .", "These rules delete the adjunct of the first conjunct for testing , the adjunct of the second conjunct by the end of the year , the rest of the second conjunct Leary hopes to set requirements for a full system , and the conjunction itself and .", "Evaluation of quality of sentence condensation systems , and of text summarization and simplification systems in general , has mostly been conducted as intrinsic evaluation by human experts .", "Recently , Papineni et al . s 2001 proposal for an automatic evaluation of translation systems by measuring n gram matches of the system output against reference examples has become popular for evaluation of summarization systems .", "In addition , an automatic evaluation method based on context free deletion decisions has been proposed by Jing 2000 .", "However , for summarization systems that employ a linguistic parser as an integral system component , it is possible to employ the standard evaluation techniques for parsing directly to an evaluation of summarization quality .", "A parsingbased evaluation allows us to measure the semantic aspects of summarization quality in terms of grammaticalfunctional information provided by deep parsers .", "Furthermore , human expertise was necessary only for the creation of condensed versions of sentences , and for the manual disambiguation of parses assigned to those sentences .", "Given such a gold standard , summarization quality of a system can be evaluated automatically and repeatedly by matching the structures of the system output against the gold standard structures .", "The standard metrics of precision , recall , and F score from statistical parsing can be used as evaluation metrics for measuring matching quality Precision measures the number of matching structural items in the parses of the system output and the gold standard , out of all structural items in the system output s parse ; recall measures the number of matches , out of all items in the gold standard s parse .", "F score balances precision and recall as 2 precision recall precision recall .", "For the sentence condensation system presented above , the structural items to be matched consist of relation predicate , argument triples .", "For example , the goldstandard f structure of Fig .", "2 corresponds to 23 dependency relations , the first 14 of which are shared with the reduced f structure chosen by the stochastic disambiguation system Matching these f structures against each other corresponds to a precision of 1 , recall of . 61 , and F score of . 76 .", "The fact that our method does not rely on a comparison of the characteristics of surface strings is a clear advantage .", "Such comparisons are bad at handling examples which are similar in meaning but differ in word order or vary structurally , such as in passivization or nominalization .", "Our method handles such examples straightforwardly .", "Fig .", "4 shows two serialization variants of the condensed sentence of Fig .", "The f structures for these examples are similar to the f structure assigned to the gold standard condensation shown in Fig .", "2 except for the relations ADJUNT TYPE parenthetical versus ADV TYPE vpadv versus ADV TYPE sadv .", "An evaluation of summarization quality that is based on matching f structures will treat these examples equally , whereas an evaluation based on string matching will yield different quality scores for different serializations .", "quot ; A prototype , for testing , is ready . quot ; quot ; For testing , a prototype is ready . quot ; In the next section , we present experimental results of an automatic evaluation of the sentence condensation system described above .", "These results show a close correspondence between automatically produced evaluation results and human judgments on the quality of generated condensed strings .", "The sentences and condensations we used are taken from data for the experiments of Knight and Marcu 2000 , which were provided to us by Daniel Marcu .", "These data consist of pairs of sentences and their condensed versions that have been extracted from computer news articles and abstracts of the Ziff Davis corpus .", "Out of these data , we parsed and manually disambiguated 500 sentence pairs .", "These included a set of 32 sentence pairs that were used for testing purposes in Knight and Marcu 2000 .", "In order to control for the small corpus size of this test set , we randomly extracted an additional 32 sentence pairs from the 500 parsed and disambiguated examples as a second test set .", "The rest of the 436 randomly selected sentence pairs were used to create training data .", "For the purpose of discriminative training , a gold standard of transferred f structures was created from the transfer output and the manually selected f structures for the condensed strings .", "This was done automatically by selecting for each example the transferred f structure that best matched the fstructure annotated for the condensed string .", "In the automatic evaluation of f structure match , three different system variants were compared .", "Firstly , randomly chosen transferred f structures were matched against the manually selected f structures for the manually created condensations .", "This evaluation constitutes a lower bound on the F score against the given gold standard .", "Secondly , matching results for transferred fstructures yielding the maximal F score against the gold standard were recorded , giving an upper bound for the system .", "Thirdly , the performance of the stochastic model within the range of the lower bound and upper bound was measured by recording the F score for the f structure that received highest probability according to the learned distribution on transferred structures .", "In order to make our results comparable to the results of Knight and Marcu 2000 and also to investigate the correspondence between the automatic evaluation and human judgments , a manual evaluation of the strings generated by these system variants was conducted .", "Two human judges were presented with the uncondensed surface string and five condensed strings that were displayed in random order for each test example .", "The five condensed strings presented to the human judges contained 1 strings generated from three randomly selected fstructures , 2 the strings generated from the f structures which were selected by the stochastic model , and 3 the manually created gold standard condensations extracted from the Ziff Davis abstracts .", "The judges were asked to judge summarization quality on a scale of increasing quality from 1 to 5 by assessing how well the generated strings retained the most salient information of the original uncondensed sentences .", "Grammaticality of the system output is optimal and not reported separately .", "Results for both evaluations are reported for two test corpora of 32 examples each .", "Testset I contains the sentences and condensations used to evaluate the system described in Knight and Marcu 2000 .", "Testset II consists of another randomly extracted 32 sentence pairs from the same domain , prepared in the same way .", "Fig .", "5 shows evaluation results for a sentence condensation run that uses manually selected f structures for the original sentences as input to the transfer component .", "These results demonstrate how the condenstation system performs under the optimal circumstances when the parse chosen as input is the best available .", "Fig .", "6 applies the same evaluation data and metrics to a sentence condensation experiment that performs transfer from packed fstructures , i . e . transfer is performed on all parses for an ambiguous sentence instead of on a single manually selected parse .", "Alternatively , a single input parse could be selected by stochastic models such as the one described in Riezler et al . 2002 .", "A separate phase of parse disambiguation , and perhaps the effects of any errors that this might introduce , can be avoided by transferring from all parses for an ambiguous sentence .", "This approach is computationally feasible , however , only if condensation can be carried all the way through without unpacking .", "Our technology is not yet able to do this in particular , as mentioned earlier , we have not yet implemented a method for stochastic disambiguation on packed f structures .", "However , we conducted a preliminary assessment of this possibility by unpacking and enumerating the transferred fstructures .", "For many sentences this resulted in more candidates than we could operate on in the available time and space , and in those cases we arbitrarily set a cut off on the number of transferred f structures we considered .", "Since transferred f structures are produced according to the number of rules applied to transfer them , in this setup the transfer system produces smaller f structures first , and cuts off less condensed output .", "The result of this experiment , shown in Fig .", "6 , thus provides a conservative estimate on the quality of the condensations we might achieve with a full packing implementation .", "In Figs .", "5 and 6 , the first row shows F scores for a random selection , the system selection , and the best possible selection from the transfer output against the gold standard .", "The second rows show summarization quality scores for generations from a random selection and the system selection , and for the human written condensation .", "The third rows report compression ratios .", "As can be seen from these tables , the ranking of system variants produced by the automatic and manual evaluation confirm a close correlation between the automatic evaluation and human judgments .", "A comparison of evaluation results across colums , i . e . across selection variants , shows that a stochastic selection of transferred f structures is indeed important .", "Even if all f structures are transferred from the same linguistically rich source , and all generated strings are grammatical , a reduction in error rate of around 50 relative to the upper bound can be achieved by stochastic selection .", "In contrast , a comparison between transfer runs with and without perfect disambiguation of the original string shows a decrease of about 5 in F score , and of only . 1 points for summarization quality when transferring from packed parses instead of from the manually selected parse .", "This shows that it is more important to learn what a good transferred f structure looks like than to have a perfect f structure to transfer from .", "The compression rates associated with the systems that used stochastic selection is around 60 , which is acceptable , but not as aggressive as human written condensations .", "Note that in our current implementation , in some cases the transfer component was unable to operate on the packed representation .", "In those cases a parse was chosen at random as a conservative estimate of transfer from all parses .", "This fall back mechanism explains the drop in F score for the upper bound in comparing Figs .", "5 and 6 .", "We presented an approach to sentence condensation that employs linguistically rich LFG grammars in a parsing generation based stochastic sentence condensation system .", "Fine grained dependency structures are output by the parser , then modified by a highly expressive transfer system , and filtered by a constraint based generator .", "Stochastic selection of generation filtered reduced structures uses a powerful Maximum Entropy model .", "As shown in an experimental evaluation , summarization quality of the system output is state of the art , and grammaticality of condensed strings is guaranteed .", "Robustness techniques for parsing and generation guarantee that the system produces non empty output for unseen input .", "Overall , the summarization quality achieved by our system is similar to the results reported in Knight and Marcu 2000 .", "This might seem disappointing considering the more complex machinery employed in our approach .", "It has to be noted that these results are partially due to the somewhat artificial nature of the data that were used in the experiments of Knight and Marcu 2000 and therefore in our experiments The human written condensations in the data set extracted from the Ziff Davis corpus show the same word order as the original sentences and do not exhibit any structural modification that are common in humanwritten summaries .", "For example , humans tend to make use of structural modifications such as nominalization and verb alternations such as active passive or transitive intransitive alternations in condensation .", "Such alternations can easily be expressed in our transfer based approach , whereas they impose severe problems to approaches that operate only on phrase structure trees .", "In the given test set , however , the condensation task restricted to the operation of deletion .", "A creation of additional condensations for the original sentences other than the condensed versions extracted from the human written abstracts would provide a more diverse test set , and furthermore make it possible to match each system output against any number of independent human written condensations of the same original sentence .", "This idea of computing matching scores to multiple reference examples was proposed by Alshawi et al . 1998 , and later by Papineni et al .", "2001 for evaluation of machine translation systems .", "Similar to these proposals , an evaluation of condensation quality could consider multiple reference condensations and record the matching score against the most similar example .", "Another desideratum for future work is to carry condensation all the way through without unpacking at any stage .", "Work on employing packing techniques not only for parsing and transfer , but also for generation and stochastic selection is currently underway see Geman and Johnson 2002 .", "This will eventually lead to a system whose components work on packed representations of all or n best solutions , but completely avoid costly unpacking of representations ."], "summary_lines": ["Statistical Sentence Condensation Using Ambiguity Packing And Stochastic Disambiguation Methods For Lexical-Functional Grammar\n", "We present an application of ambiguity pack- ing and stochastic disambiguation techniques for Lexical-Functional Grammars (LFG) to the domain of sentence condensation.\n", "Our system incorporates a linguistic parser/generator for LFG, a transfer component for parse reduction operating on packed parse forests, and a maximum-entropy model for stochastic output selection.\n", "Furthermore, we propose the use of standard parser evaluation methods for automatically evaluating the summarization quality of sentence condensation systems.\n", "An experimental evaluation of summarization quality shows a close correlation between the automatic parse-based evaluation and a manual evaluation of generated strings.\n", "Overall summarization quality of the proposed system is state-of-the-art, with guaranteed grammaticality of the system output due to the use of a constraint-based parser/generator.\n", "We present a discriminative sentence compressor over the output of an LFG parser that is a packed representation of possible compressions.\n", "We apply linguistically rich LFG grammars to a sentence compression system.\n"]}
{"article_lines": ["Dependency Parsing by Belief Propagation", "We formulate dependency parsing as a graphical model with the novel ingredient of global constraints .", "We show how to apply loopy belief propagation BP , a simple and tool for and inference .", "As a parsing algorithm , BP is both asymptotically and empirically efficient .", "Even with second order features or latent variables , which would make exact parsing considerslower or NP hard , BP needs only with a small constant factor .", "Furthermore , such features significantly improve parse accuracy over exact first order methods .", "Incorporating additional features would increase the runtime additively rather than multiplicatively .", "Computational linguists worry constantly about runtime .", "Sometimes we oversimplify our models , trading linguistic nuance for fast dynamic programming .", "Alternatively , we write down a better but intractable model and then use approximations .", "The CL community has often approximated using heavy pruning or reranking , but is beginning to adopt other methods from the machine learning community , such as Gibbs sampling , rejection sampling , and certain variational approximations .", "We propose borrowing a different approximation technique from machine learning , namely , loopy belief propagation BP .", "In this paper , we show that BP can be used to train and decode complex parsing models .", "Our approach calls a simpler parser as a subroutine , so it still exploits the useful , well studied combinatorial structure of the parsing problem . 1", "We wish to make a dependency parse s score depend on higher order features , which consider arbitrary interactions among two or more edges in the parse and perhaps also other latent variables such as part of speech tags or edge labels .", "Such features can help accuracy as we show .", "Alas , they raise the polynomial runtime of projective parsing , and render non projective parsing NP hard .", "Hence we seek approximations .", "We will show how BP s message passing discipline offers a principled way for higher order features to incrementally adjust the numerical edge weights that are fed to a fast first order parser .", "Thus the first order parser is influenced by higher order interactions among edges but not asymptotically slowed down by considering the interactions itself .", "BP s behavior in our setup can be understood intuitively as follows .", "Inasmuch as the first order parser finds that edge e is probable , the higher order features will kick in and discourage other edges e' to the extent that they prefer not to coexist with e . 2 Thus , the next call to the first order parser assigns lower probabilities to parses that contain these e' .", "The method is approximate because a first order parser must equally penalize all parses containing e' , even those that do not in fact contain e . This behavior is somewhat similar to parser stacking Nivre and McDonald , 2008 ; Martins et al . , 2008 , in which a first order parser derives some of its input features from the full 1 best output of another parser .", "In our method , a first order parser derives such input features from its own previous full output but probabilistic output rather than just 1best .", "This circular process is iterated to convergence .", "Our method also permits the parse to interact cheaply with other variables .", "Thus first order parsing , part of speech tagging , and other tasks on a common input could mutually influence one another .", "Our method and its numerical details emerge naturally as an instance of the well studied loopy BP algorithm , suggesting several potential future improvements to accuracy Yedidia et al . , 2004 ; Braunstein et al . , 2005 and efficiency Sutton and McCallum , 2007 .", "Loopy BP has occasionally been used before in NLP , with good results , to handle non local features Sutton and McCallum , 2004 or joint decoding Sutton et al . , 2004 .", "However , our application to parsing requires an innovation to BP that we explain in 5 a global constraint to enforce that the parse is a tree .", "The tractability of some such global constraints points the way toward applying BP to other computationally intensive NLP problems , such as syntax based alignment of parallel text .", "To apply BP , we must formulate dependency parsing as a search for an optimal assignment to the variables of a graphical model .", "We encode a parse using the following variables Sentence .", "The n word input sentence W is fully observed not a lattice .", "Let W W0W1 Wn , where W0 is always the special symbol ROOT .", "Tags .", "If desired , the variables T T1T2 Tn may specify tags on the n words , drawn from some tagset T e . g . , parts of speech .", "These variables are needed iff the tags are to be inferred jointly with the parse .", "Links .", "The O n2 boolean variables Lij 0 i n , 1 j n , i j correspond to the possible links in the dependency parse . 3 Lij true is interpreted as meaning that there exists a dependency link from parent i child j . 4 Link roles , etc .", "It would be straightforward to add other variables , such as a binary variable Lij that is true iff there is a link i j labeled with role r e . g . , AGENT , PATIENT , TEMPORAL ADJUNCT .", "We wish to define a probability distribution over all configurations , i . e . , all joint assignments A to these variables .", "Our distribution is simply an undirected graphical model , or Markov random field MRF 5 specified by the collection of factors Fm A H R 'O .", "Each factor is a function that consults only a subset of A .", "We say that the factor has degree d if it depends on the values of d variables in A , and that it is unary , binary , ternary , or global if d is respectively 1 , 2 , 3 , or unbounded grows with n .", "A factor function Fm A may also depend freely on the observed variables the input sentence W and a known learned parameter vector 0 .", "For notational simplicity , we suppress these extra arguments when writing and drawing factor functions , and when computing their degree .", "In this treatment , these observed variables are not specified by A , but instead are absorbed into the very definition of Fm .", "In defining a factor Fm , we often define the circumstances under which it fires .", "These are the only circumstances that allow Fm A 1 .", "When Fm does not fire , Fm A 1 and does not affect the product in equation 1 .", "A hard factor Fm fires only on parses A that violate some specified condition .", "It has value 0 on those parses , acting as a hard constraint to rule them out .", "TREE .", "A hard global constraint on all the Lij variables at once .", "It requires that exactly n of these variables be true , and that the corresponding links form a directed tree rooted at position 0 .", "PTREE .", "This stronger version of TREE requires further that the tree be projective .", "That is , it prohibits Lij and LH from both being true if i j crosses k E . These links are said to cross if one of k , E is strictly between i and j while the other is strictly outside that range .", "EXACTLY1 .", "A family of O n hard global constraints , indexed by 1 j n . EXACTLY1j requires that j have exactly one parent , i . e . , exactly one of the Lij variables must be true .", "Note that EXACTLY1 is implied by TREE or PTREE .", "ATMOST1 .", "A weaker version .", "ATMOST1j requires j to have one or zero parents .", "NAND .", "A family of hard binary constraints .", "NANDij , kt requires that Lij and Lkt may not both be true .", "We will be interested in certain subfamilies .", "NOT2 .", "Shorthand for the family of O n3 binary constraints NANDij , kj .", "These are collectively equivalent to ATMOST1 , but expressed via a larger number of simpler constraints , which can make the BP approximation less effective footnote 30 .", "NO2CYCLE .", "Shorthand for the family of O n2 binary constraints NANDij , ji .", "A soft factor Fm acts as a soft constraint that prefers some parses to others .", "In our experiments , it is always a log linear function returning positive values where \u03b8 is a learned , finite collection of weights and f is a corresponding collection of feature functions , some of which are used by Fm .", "Note that fh is permitted to consult the observed input W . It also sees which factor Fm it is scoring , to support reuse of a single feature function fh and its weight \u03b8h by unboundedly many factors in a model .", "LINK .", "A family of unary soft factors that judge the links in a parse A individually .", "LINKij fires iff Lij true , and then its value depends on i , j , W , and \u03b8 .", "Our experiments use the same features as McDonald et al . 2005 .", "A first order or edge factored parsing model McDonald et al . , 2005 contains only LINK factors , along with a global TREE or PTREE factor .", "Though there are O n2 link factors one per Lij , only n of them fire on any particular parse , since the global factor ensures that exactly n are true .", "We ll consider various higher order soft factors PAIR .", "The binary factor PAIRij , kt fires with some value iff Lij and Lkt are both true .", "Thus , it penalizes or rewards a pair of links for being simultaneously present .", "This is a soft version of NAND .", "GRAND .", "Shorthand for the family of O n3 binary factors PAIRij , jk , which evaluate grandparentparent child configurations , i j k . For example , whether preposition j attaches to verb i might depend on its object k . In non projective parsing , we might prefer but not require that a parent and child be on the same side of the grandparent .", "SIB .", "Shorthand for the family of O n3 binary factors PAIRij , ik , which judge whether two children of the same parent are compatible .", "E . g . , a given verb may not like to have two noun children both to its left . 6 The children do not need to be adjacent .", "CHILDSEQ .", "A family of O n global factors .", "CHILDSEQi scores i s sequence of children ; hence it consults all variables of the form Lij .", "The scoring follows the parametrization of a weighted split head automaton grammar Eisner and Satta , 1999 .", "If 5 has children 2 , 7 , 9 under A , then CHILDSEQi is a product of subfactors of the form PAIR5 , 57 , PAIR57 , 59 , PAIR59 , 5 right child sequence and PAIR5 , 52 , PAIR52 , 5 left child sequence .", "NOCROSS .", "A family of O n2 global constraints .", "If the parent to j link crosses the parent to link , then NOCROSSjt fires with a value that depends only on j and .", "If j and do not each have exactly one parent , NOCROSSjt fires with value 0 ; i . e . , it incorporates EXACTLY1j and EXACTLY1t .", "7 TAGi is a unary factor that evaluates whether Ti s value is consistent with W especially Wi .", "TAGLINKij is a ternary version of the LINKij factor whose value depends on Lij , Ti and Tj i . e . , its feature functions consult the tag variables to decide whether a link is likely .", "One could similarly enrich the other features above to depend on tags and or link roles ; TAGLINK is just an illustrative example .", "TRIGRAM is a global factor that evaluates the tag sequence T according to a trigram model .", "It is a product of subfactors , each of which scores a trigram of adjacent tags Ti_2 , Ti_1 , Ti , possibly also considering the word sequence W as in CRFs .", "MacKay 2003 , chapters 16 and 26 provides an excellent introduction to belief propagation , a generalization of the forward backward algorithm that is deeply studied in the graphical models literature Yedidia et al . , 2004 , for example .", "We briefly sketch the method in terms of our parsing task .", "The basic BP idea is simple .", "Variable L34 maintains a distribution over values true and false a belief that is periodically recalculated based on the current distributions at other variables . 8 Readers familiar with Gibbs sampling can regard this as a kind of deterministic approximation .", "In Gibbs sampling , L34 s value is periodically resampled based on the current values of other variables .", "Loopy BP works not with random samples but their expectations .", "Hence it is approximate but tends to converge much faster than Gibbs sampling will mix .", "It is convenient to visualize an undirected factor graph Fig .", "1 , in which each factor is connected to the variables it depends on .", "Many factors may connect to and hence influence a given variable such as L34 .", "If X is a variable or a factor , N X denotes its set of neighbors .", "Given an input sentence W and a parameter vector \u03b8 , the collection of factors Fm defines a probability distribution 1 .", "The parser should determine the values of the individual variables .", "In other words , we would like to marginalize equation 1 to obtain the distribution p L34 over L34 true vs . false , the distribution p T4 over tags , etc .", "If the factor graph is acyclic , then BP computes these marginal distributions exactly .", "Given 8Or , more precisely this is the tricky part based on versions of those other distributions that do not factor in L34 s reciprocal influence on them .", "This prevents e . g .", "L34 and T3 from mutually reinforcing each other s existing beliefs . an HMM , for example , BP reduces to the forwardbackward algorithm .", "BP s estimates of these distributions are called beliefs about the variables .", "BP also computes beliefs about the factors , which are useful in learning \u03b8 see 7 .", "E . g . , if the model includes the factor TAGLINKij , which is connected to variables Lij , Ti , Tj , then BP will estimate the marginal joint distribution p Lij , Ti , Tj over boolean , tag , tag triples .", "When the factor graph has loops , BP s beliefs are usually not the true marginals of equation 1 which are in general intractable to compute .", "Indeed , BP s beliefs may not be the true marginals of any distribution p A over assignments , i . e . , they may be globally inconsistent .", "All BP does is to incrementally adjust the beliefs till they are at least locally consistent e . g . , the beliefs at factors TAGLINKij and TAGLINKik must both imply9 the same belief about variable Ti , their common neighbor .", "This iterated negotiation among the factors is handled by message passing along the edges of the factor graph .", "A message to or from a variable is a possibly unnormalized probability distribution over the values of that variable .", "The variable V sends a message to factor F , saying My other neighboring factors G jointly suggest that I have posterior distribution qV , F assuming that they are sending me independent evidence . Meanwhile , factor F sends messages to V , saying , Based on my factor function and the messages received from my other neighboring variables U about their values and assuming that those messages are independent , I suggest you have posterior distribution rF , V over your values . To be more precise , BP at each iteration k until convergence updates two kinds of messages from factors to variables .", "Each message is a probability distribution over values v of V , normalized by a scaling constant n . Alternatively , messages may be left as unnormalized distributions , choosing n 1 only as needed to prevent over or underflow .", "Messages are initialized to uniform distributions .", "Whenever we wish , we may compute the beliefs at V and F These beliefs do not truly characterize the expected behavior of Gibbs sampling 4 . 1 , since the products in 5 6 make conditional independence assumptions that are valid only if the factor graph is acyclic .", "Furthermore , on cyclic loopy graphs , BP might only converge to a local optimum Weiss and Freedman , 2001 , or it might not converge at all .", "Still , BP often leads to good , fast approximations .", "One iteration of standard BP simply updates all the messages as in equations 3 4 one message per edge of the factor graph .", "Therefore , adding new factors to the model increases the runtime per iteration additively , by increasing the number of messages to update .", "We believe this is a compelling advantage over dynamic programming in which new factors usually increase the runtime and space multiplicatively by exploding the number of distinct items . 10 But how long does updating each message take ?", "The runtime of summing over all assignments EA in 10For example , with unknown tags T , a model with PTREE TAGLINK will take only O n3 n2g2 time for BP , compared to O n3g2 time for dynamic programming Eisner Satta 1999 .", "Adding TRIGRAM , which is string local rather than tree local , will increase this only to O n3 n2g2 ng3 , compared to O n3g6 for dynamic programming .", "Even more dramatic , adding the SIB family of O n3 PAIRij , ik factors will add only O n3 to the runtime of BP Table 1 .", "By contrast , the runtime of dynamic programming becomes exponential , because each item must record its headword s full set of current children . equation 4 may appear prohibitive .", "Crucially , however , F A only depends on the values in A of F s its neighboring variables N F .", "So this sum is proportional to a sum over restricted assignments to just those variables . 11 For example , computing a message from TAGLINKij Ti only requires iterating over all boolean , tag , tag triples . 12 The runtime to update that message is therefore O 2 T T .", "The above may be tolerable for a ternary factor .", "But how about global factors ?", "EXACTLY1j has n neighboring boolean variables surely we cannot iterate over all 2n assignments to these !", "TREE is even worse , with 2O n2 assignments to consider .", "We will give specialized algorithms for handling these summations more efficiently .", "A historical note is in order .", "Traditional constraint satisfaction corresponds to the special case of 1 where all factors Fm are hard constraints with values in 0 , 1 .", "In that case , loopy BP reduces to an algorithm for generalized arc consistency Mackworth , 1977 ; Bessi ere and R egin , 1997 ; Dechter , 2003 , and updating a factor s outgoing messages is known as constraint propagation .", "R egin 1994 famously introduced an efficient propagator for a global constraint , ALLDIFFERENT , by adapting combinatorial bipartite matching algorithms .", "In the same spirit , we will demonstrate efficient propagators for our global constraints , e . g . by adapting combinatorial algorithms for weighted parsing .", "We are unaware of any previous work on global factors in sum product BP , although for max product BP , 13 Duchi et al . 2007 independently showed that a global 1 to 1 alignment constraint a kind of weighted ALLDIFFERENT permits an efficient propagator based on weighted bipartite matching .", "Table 1 shows our asymptotic runtimes for all factors in 3 . 3 3 . 4 .", "Remember that if several of these factors are included , the total runtime is additive . 14 Propagating the local factors is straightforward 5 . 1 .", "We now explain how to handle the global factors .", "Our main trick is to work backwards from marginal beliefs .", "Let F be a factor and V be one of its neighboring variables .", "At any time , F has a marginal belief about V see footnote 9 , A s . t .", "A V v a sum over 6 s products of incoming messages .", "By the definition of rF V in 4 , and distributivity , we can also express the marginal belief 7 as a pointwise product of outgoing and incoming messages15 up to a constant .", "If we can quickly sum up the marginal belief 7 , then 8 says we can divide out each particular incoming message q V F to obtain its corresponding outgoing message r 1 14We may ignore the cost of propagators at the variables .", "Each outgoing message from a variable can be computed in time proportional to its size , which may be amortized against the cost of generating the corresponding incoming message .", "15E . g . , the familiar product of forward and backward messages that is used to extract posterior marginals from an HMM .", "Note that the marginal belief and both messages are unnormalized distributions over values v of V .", "F and k are clear from context below , so we simplify the notation so that 7 8 become TRIGRAM must sum over assignments to the tag sequence T . The belief 6 in a given assignment is a product of trigram scores which play the role of transition weights and incoming messages qTj playing the role of emission weights .", "The marginal belief 7 needed above , b Ti t , is found by summing over assignments where Ti t . All marginal beliefs are computed together in O ng3 total time by the forward backward algorithm . 16 EXACTLY1j is a sparse hard constraint .", "Even though there are 2n assignments to its n neighboring variables Lij , the factor function returns 1 on only n assignments and 0 on the rest .", "In fact , for a given i , b Lij true in 7 is defined by 6 to have exactly one non zero summand , in which A puts Lij true and all other Ligj false .", "We compute the marginal beliefs for all i together in O n total time TREE and PTREE must sum over assignments to the O n2 neighboring variables Lij .", "There are now exponentially many non zero summands , those in which A corresponds to a valid tree .", "Nonetheless , 16Which is itself an exact BP algorithm , but on a different graph a junction tree formed from the graph of TRIGRAM subfactors .", "Each variable in the junction tree is a bigram .", "If we had simply replaced the global TRIGRAM factor with its subfactors in the full factor graph , we would have had to resort to Generalized BP Yedidia et al . , 2004 to obtain the same exact results .", "17But taking it 1 gives the same results , up to a constant .", "18As a matter of implementation , this odds ratio qL , , can be used to represent the incoming message qL . , everywhere . we can follow the same approach as for EXACTLY1 .", "Steps 1 and 4 are modified to iterate over all i , j such that Lij is a variable .", "In step 3 , the partition function PA b A is now 7r times the total weight of all trees , where the weight of a given tree is the product of the gLij values of its n edges .", "In step 2 , the marginal belief b Lij true is now 7r times the total weight of all trees having edge i j .", "We perform these combinatorial sums by calling a first order parsing algorithm , with edge weights qij .", "Thus , as outlined in 2 , a first order parser is called each time we propagate through the global TREE or PTREE constraint , using edge weights that include the first order LINK factors but also multiply in any current messages from higher order factors .", "The parsing algorithm simultaneously computes the partition function b , and all O n2 marginal beliefs b Lij true .", "For PTREE projective , it is the inside outside version of a dynamic programming algorithm Eisner , 1996 .", "For TREE nonprojective , Koo et al . 2007 and Smith and Smith 2007 show how to employ the matrix tree theorem .", "In both cases , the total time is O n3 . 19 NOCROSSj must sum over assignments to O n neighboring variables Lij and Lk .", "The nonzero summands are assignments where j and E each have exactly one parent .", "At step 1 , 7r def Qi qLij false Qk qLke false .", "At step 2 , the marginal belief b Lij true sums over the n nonzero assignments containing i j .", "It is 7r gLij Pk qLke PAIRij , k , where PAIRij , k is xj if i j crosses k E and is 1 otherwise . xj is some factor value defined by equation 2 to penalize or reward the crossing .", "Steps 3 4 are just as in EXACTLY1j .", "The question is how to compute b Lij true for each i in only O 1 time , 20 so that we can propagate each of the O n2 NOCROSSj in O n time .", "This is why we allowed xj to depend only on j , E . We can rewrite the sum b Lij true as crossing k noncrossing k 19A dynamic algorithm could incrementally update the outgoing messages if only a few incoming messages have changed as in asynchronous BP .", "In the case of TREE , dynamic matrix inverse allows us to update any row or column i . e . , messages from all parents or children of a given word and find the new inverse in O n2 time Sherman and Morrison , 1950 .", "20Symmetrically , we compute b Lke true for each k . To find this in O 1 time , we precompute for each E an array of partial sums Q s , t def Ps k t qLke .", "Since Q s , t Q s , t 1 qLte , we can compute each entry in O 1 time .", "The total precomputation time over all E , s , t is then O n3 , with the array Q shared across all factors NOCROSSjq .", "The crossing sum is respectively Q 0 , i 1 Q j 1 , n , Q i 1 , j 1 , or 0 according to whether E i , j , E i , j , or E i . 21 The non crossing sum is Q 0 , n minus the crossing sum .", "CHILDSEQi , like TRIGRAM , is propagated by a forward backward algorithm .", "In this case , the algorithm is easiest to describe by replacing CHILDSEQi in the factor graph by a collection of local subfactors , which pass messages in the ordinary way . 22 Roughly speaking , 23 at each j 1 , n , we introduce a new variable Cij a hidden state whose value is the position of i s previous child , if any so 0 Cij j .", "So the ternary subfactor on Cij , Lij , Ci , j 1 has value 1 if Lij false and Ci , j 1 Ci , j ; a sibling bigram score PAIRiCij , iCi , j 1 if Lij true and Ci , j 1 j ; and 0 otherwise .", "The sparsity of this factor , which is 0 almost everywhere , is what gives CHILDSEQi a total runtime of O n2 rather than O n3 .", "It is equivalent to forward backward on an HMM with n observations the Lij and n states per observation the Cj , with a deterministic thus sparse transition function .", "BP computes local beliefs , e . g . the conditional probability that a link Lij is present .", "But if we wish to output a single well formed dependency tree , we need to find a single assignment to all the Lij that satisfies the TREE or PTREE constraint .", "Our final belief about the TREE factor is a distribution over such assignments , in which a tree s probability is proportional to the probability of its edge weights gLij incoming messages .", "We could simply return the mode of this distribution found by using a 1 best first order parser or the k best trees , or take samples .", "21There are no NOCROSSje factors with f j .", "22We still treat CHILDSEQi as a global factor and compute all its correct outgoing messages on a single BP iteration , via serial forward and backward sweeps through the subfactors .", "Handling the subfactors in parallel , 3 4 , would need O n iterations .", "23Ignoring the treatment of boundary symbols see 3 . 4 .", "In our experiments , we actually take the edge weights to be not the messages qLij from the links , def bLij log bLij true bLij false .", "These are passed into a fast algorithm for maximum spanning tree Tarjan , 1977 or maximum projective spanning tree Eisner , 1996 .", "This procedure is equivalent to minimum Bayes risk MBR parsing Goodman , 1996 with a dependency accuracy loss function .", "Notice that the above decoding approaches do not enforce any hard constraints other than TREE in the final output .", "In addition , they only recover values of the Lij variables .", "They marginalize over other variables such as tags and link roles .", "This solves the problem of nuisance variables which merely fragment probability mass among refinements of a parse .", "On the other hand , it may be undesirable for variables whose values we desire to recover . 24", "Our training method also uses beliefs computed by BP , but at the factors .", "We choose the weight vector 0 by maximizing the log probability of training data 24An alternative is to attempt to find the most probable MAP assignment to all variables using the max product algorithm footnote 13 or one of its recent variants .", "The estimated marginal beliefs become max marginals , which assess the 1 best assignment consistent with each value of the variable .", "We can indeed build max product propagators for our global constraints .", "PTREE still propagates in O n3 time simply change the first order parser s semiring Goodman , 1999 to use max instead of sum .", "TREE requires O n4 time it seems that the O n2 max marginals must be computed separately , each requiring a separate call to an O n2 maximum spanning tree algorithm Tarjan , 1977 .", "If max product BP converges , we may simply output each variable s favorite value according to its belief , if unique .", "However , max product BP tends to be unstable on loopy graphs , and we may not wish to wait for full convergence in any case .", "A more robust technique for extracting an assignment is to mimic Viterbi decoding , and follow backpointers of the max product computation along some spanning subtree of the factor graph .", "A slower but potentially more stable alternative is deterministic annealing .", "Replace each factor Fm A with Fm A 1 T , where T 0 is a temperature .", "As T 0 quenches , the distribution 1 retains the same mode the MAP assignment , but becomes more sharply peaked at the mode , and sum product BP approaches max product BP .", "Deterministic annealing runs sum product BP while gradually reducing T toward 0 as it iterates .", "By starting at a high T and reducing T slowly , it often manages in practice to find a good local optimum .", "We may then extract an assignment just as we do for max product . under equation 1 , regularizing only by early stopping .", "If all variables are observed in training , this objective function is convex as for any log linear model .", "The difficult step in computing the gradient of our objective is finding V\u03b8 log Z , where Z in equation 1 is the normalizing constant partition function that sums over all assignments A .", "Recall that Z , like each Fm , depends implicitly on W and 0 .", "As usual for log linear models , Since V\u03b8Fm A only depends on the assignment A s values for variables that are connected to Fm in the factor graph , its expectation under p A depends only on the marginalization of p A to those variables jointly .", "Fortunately , BP provides an estimate of that marginal distribution , namely , its belief about the factor Fm , given W and 0 4 . 2 . 25 Note that the hard constraints do not depend on 0 at all ; so their summands in equation 10 will be 0 .", "We employ stochastic gradient descent Bottou , 2003 , since this does not require us to compute the objective function itself but only to approximately estimate its gradient as explained above .", "Alternatively , given any of the MAP decoding procedures from 6 , we could use an error driven learning method such as the perceptron or MIRA . 26", "We asked 1 For projective parsing , where higherorder factors have traditionally been incorporated into slow but exact dynamic programming DP , what are the comparative speed and quality of the BP approximation ?", "2 How helpful are such higherorder factors particularly for non projective parsing , where BP is needed to make them tractable ?", "3 Do our global constraints e . g . , TREE contribute to the goodness of BP s approximation ?", "We built a first order projective parser one that uses only factors PTREE and LINK and then compared the cost of incorporating second order factors , GRAND and CHILDSEQ , by BP versus DP . 28 Under DP , the first order runtime of O n3 is increased to O n4 with GRAND , and to O n5 when we add CHILDSEQ as well .", "BP keeps runtime down to O n3 although with a higher constant factor , since it takes several rounds to converge , and since it computes more than just the best parse . 29 Figures 2 3 compare the empirical runtimes for various input sentence lengths .", "With only the GRAND factor , exact DP can still find the Viterbi parse though not the MBR parse29 faster than ten iterations of the asymptotically better BP Fig .", "2 , at least for sentences with n 75 .", "However , once we add the CHILDSEQ factor , BP is always faster dramatically so for longer sentences Fig .", "More complex models would widen BP s advantage .", "Fig .", "4 shows the tradeoff between runtime and search error of BP in the former case GRAND only .", "To determine BP s search error at finding the MBR parse , we measured its dependency accuracy not against the gold standard , but against the optimal MBR parse under the model , which DP is able to find .", "After 10 iterations , the overall macro averaged search error compared to O n4 DP MBR is 0 . 4 ; compared to O n5 not shown , 2 . 4 .", "More BP iterations may help accuracy .", "In future work , we plan to compare BP s speed accuracy curve on more complex projective models with the speed accuracy curve of pruned or reranked DP .", "The BP approximation can be used to improve the accuracy of non projective parsing by adding higher order features .", "These would be NP hard to incorporate exactly ; DP cannot be used .", "We used BP with a non projective TREE factor to train conditional log linear parsing models of two highly non projective languages , Danish and Dutch , as well as slightly non projective English 8 . 1 .", "In all three languages , the first order non projective parser greatly overpredicts the number of crossing links .", "We thus added NOCROSS factors , as well as GRAND and CHILDSEQ as before .", "All of these significantly improve the first order baseline , though not necessarily cumulatively Table 2 .", "Finally , Table 2 compares loopy BP to a previously proposed hill climbing method for approximate inference in non projective parsing McDonald and Pereira 2006 .", "Hill climbing decodes our richest non projective model by finding the best projective parse under that model using slow , higherorder DP and then greedily modifies words parents until the parse score 1 stops improving . with TREE , decoding it with weaker constraints is asymptotically faster except for NOT2 but usually harmful .", "Parenthetical numbers show that the harm is compounded if the weaker constraints are used in training as well ; even though this matches training to test conditions , it may suffer more from BP s approximate gradients .", "Decoding the TREE model with the even stronger PTREE constraint can actually be helpful for a more projective language .", "All results use 5 iterations of BP .", "BP for non projective languages is much faster and more accurate than the hill climbing method .", "Also , hill climbing only produces an approximate 1 best parse , but BP also obtains approximate marginals of the distribution over all parses .", "Given the BP architecture , do we even need the hard TREE constraint ?", "Or would it suffice for more local hard constraints to negotiate locally via BP ?", "We investigated this for non projective first order parsing .", "Table 3 shows that global constraints are indeed important , and that it is essential to use TREE during training .", "At test time , the weaker but still global EXACTLY1 may suffice followed by MBR decoding to eliminate cycles , for total time O n2 .", "Table 3 includes NOT2 , which takes O n3 time , merely to demonstrate how the BP approximation becomes more accurate for training and decoding when we join the simple NOT2 constraints into more global ATMOST1 constraints .", "This does not change the distribution 1 , but makes BP enforce stronger local consistency requirements at the factors , relying less on independence assumptions .", "In general , one can get better BP approximations by replacing a group of factors F , , t A with their product . 30 The above experiments concern gold standard 30In the limit , one could replace the product 1 with a single all purpose factor ; then BP would be exact but slow .", "In constraint satisfaction , joining constraints similarly makes arc consistency slower but better at eliminating impossible values . accuracy under a given first order , non projective model .", "Flipping all three of these parameters for Danish , we confirmed the pattern by instead measuring search error under a higher order , projective model PTREE LINK GRAND , when PTREE was weakened during decoding .", "Compared to the MBR parse under that model , the search errors from decoding with weaker hard constraints were 2 . 2 for NOT2 , 2 . 1 for EXACTLY1 , 1 . 7 for EXACTLY1 NO2CYCLE , and 0 . 0 for PTREE .", "Belief propagation improves non projective dependency parsing with features that would make exact inference intractable .", "For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase structure and lattice parsing , and in trying other higher order features , such as those used in parse reranking Charniak and Johnson , 2005 ; Huang , 2008 and history based parsing Nivre and McDonald , 2008 .", "We could also introduce new variables , e . g . , nonterminal refinements Matsuzaki et al . , 2005 , or secondary links Mid not constrained by TREE PTREE that augment the parse with representations of control , binding , etc .", "Sleator and Temperley , 1993 ; Buch Kromann , 2006 .", "Other parsing like problems that could be attacked with BP appear in syntax based machine translation .", "Decoding is very expensive with a synchronous grammar composed with an n gram language model Chiang , 2007 but our footnote 10 suggests that BP might incorporate a language model rapidly .", "String alignment with synchronous grammars is quite expensive even for simple synchronous formalisms like ITG Wu , 1997 but Duchi et al . 2007 show how to incorporate bipartite matching into max product BP .", "Finally , we can take advantage of improvements to BP proposed in the context of other applications .", "For example , instead of updating all messages in parallel at every iteration , it is empirically faster to serialize updates using a priority queue Elidan et al . , 2006 ; Sutton and McCallum , 2007 . 31 31These methods need alteration to handle our global propagators , which do update all their outgoing messages at once ."], "summary_lines": ["Dependency Parsing by Belief Propagation\n", "We formulate dependency parsing as a graphical model with the novel ingredient of global constraints.\n", "We show how to apply loopy belief propagation (BP), a simple and effective tool for approximate learning and inference.\n", "As a parsing algorithm, BP is both asymptotically and empirically efficient.\n", "Even with second-order features or latent variables, which would make exact parsing considerably slower or NP-hard, BP needs only O(n3) time with a small constant factor.\n", "Furthermore, such features significantly improve parse accuracy over exact first-order methods.\n", "Incorporating additional features would increase the runtime additively rather than multiplicatively.\n", "We can encapsulate common dynamic programming algorithms within special-purpose factors to efficiently globally constrain variable configurations.\n", "DEP-TREE is a global combinatorial factor which attaches to all Link (i, j) variables and similarly contributes a factor of 1 iff the configuration of Link variables forms a valid projective dependency graph.\n"]}
{"article_lines": ["TextRunner Open Information Extraction on the Web", "Traditional information extraction systems have focused on satisfying precise , narrow , pre specified requests from small , homogeneous corpora .", "In contrast , the TEXTRUNNER system demonstrates a new kind of information extraction , called Open Information Extraction OIE , in which the system makes a single , data driven pass over the entire corpus and extracts a large set of relational tuples , without requiring any human input .", "Banko et al . , 2007 TEXTRUNNER is a fullyimplemented , highly scalable example of OIE .", "TEXTRUNNER's extractions are indexed , allowing a fast query mechanism .", "Our first public demonstration of the TEXTRUNNER system shows the results of performing OIE on a set of 117 million web pages .", "It demonstrates the power of TEXTRUNNER in terms of the raw number of facts it has extracted , as well as its precision using our novel assessment mechanism .", "And it shows the ability to automatically determine synonymous relations and objects using large sets of extractions .", "We have built a fast user interface for querying the results .", "The bulk of previous information extraction work uses hand labeled data or hand crafted patterns to enable relation specific extraction e . g . , Culotta et al . , 2006 .", "OIE seeks to avoid these requirements for human input .", "Shinyama and Sekine Shinyama and Sekine , 2006 describe an approach to unrestricted relation discovery that does away with many of the requirements for human input .", "However , it requires clustering of the documents used for extraction , and thus scales in quadratic time in the number of documents .", "It does not scale to the size of the Web .", "For a full discussion of previous work , please see Banko et al . , 2007 , or see Yates and Etzioni , 2007 for work relating to synonym resolution .", "OIE presents significant new challenges for information extraction systems , including Automation of relation extraction , which in traditional information extraction uses handlabeled inputs .", "Corpus Heterogeneity on the Web , which makes tools like parsers and named entity taggers less accurate because the corpus is different from the data used to train the tools .", "Scalability and efficiency of the system .", "Open IE systems are effectively restricted to a single , fast pass over the data so that they can scale to huge document collections .", "In response to these challenges , TEXTRUNNER includes several novel components , which we now summarize see Banko et al . , 2007 for details .", "The TEXTRUNNER extractor makes a single pass over all documents , tagging sentences with part of speech tags and nounphrase chunks as it goes .", "For each pair of noun phrases that are not too far apart , and subject to several other constraints , it applies a classifier described below to determine whether or not to extract a relationship .", "If the classifier deems the relationship trustworthy , a tuple of the form t ei , rj , ek is extracted , where ei , ek are entities and rj is the relation between them .", "For example , TEXTRUNNER might extract the tuple Edison , invented , light bulbs .", "On our test corpus a 9 million document subset of our full corpus , it took less than 68 CPU hours to process the 133 million sentences .", "The process is easily parallelized , and took only 4 hours to run on our cluster .", "While full parsing is too expensive to apply to the Web , we use a parser to generate training examples for extraction .", "Using several heuristic constraints , we automatically label a set of parsed sentences as trustworthy or untrustworthy extractions positive and negative examples , respectively .", "The classifier is trained on these examples , using features such as the part of speech tags on the words in the relation .", "The classifier is then able to decide whether a sequence of POS tagged words is a correct extraction with high accuracy .", "Because TEXTRUNNER has no pre defined relations , it may extract many different strings representing the same relation .", "Also , as with all information extraction systems , it can extract multiple names for the same object .", "The RESOLVER system performs an unsupervised clustering of TEXTRUNNER's extractions to create sets of synonymous entities and relations .", "RESOLVER uses a novel , unsupervised probabilistic model to determine the probability that any pair of strings is co referential , given the tuples that each string was extracted with .", "Yates and Etzioni , 2007", "TEXTRUNNER builds an inverted index of the extracted tuples , and spreads it across a cluster of machines .", "This architecture supports fast , interactive , and powerful relational queries .", "Users may enter words in a relation or entity , and TEXTRUNNER quickly returns the entire set of extractions matching the query .", "For example , a query for Newton will return tuples like Newton , invented , calculus .", "Users may opt to query for all tuples matching synonyms of the keyword input , and may also opt to merge all tuples returned by a query into sets of tuples that are deemed synonymous .", "On our test corpus of 9 million Web documents , TEXTRUNNER extracted 7 . 8 million well formed tuples .", "On a randomly selected subset of 400 tuples , 80 . 4 were deemed correct by human reviewers .", "We performed a head to head comparison with a state of the art traditional information extraction system , called KNOWITALL .", "Etzioni et al . , 2005 On a set of ten high frequency relations , TEXTRUNNER found nearly as many correct extractions as KNOWITALL 11 , 631 to 11 , 476 , while reducing the error rate of KNOWITALL by 33 18 to 12 .", "This research was supported in part by NSF grants IIS 0535284 and IIS 0312988 , DARPA contract NBCHD030010 , ONR grant N0001405 1 0185 as well as gifts from Google , and carried out at the University of Washington s Turing Center ."], "summary_lines": ["TextRunner: Open Information Extraction on the Web\n", "Traditional information extraction systems have focused on satisfying precise, narrow, pre-specified requests from small, homogeneous corpora.\n", "In contrast, the TEXTRUNNER system demonstrates a new kind of information extraction, called Open Information Extraction (OIE), in which the system makes a single, data-driven pass over the entire corpus and extracts a large set of relational tuples, without requiring any human input.\n", "(Banko et al., 2007) TEXTRUNNER is a fully-implemented, highly scalable example of OIE.\n", "TEXTRUNNER's extractions are indexed, allowing a fast query mechanism.\n", "Our first public demonstration of the TEXTRUNNER system shows the results of performing OIE on a set of 117 million web pages.\n", "It demonstrates the power of TEXTRUNNER in terms of the raw number of facts it has extracted, as well as its precision using our novel assessment mechanism.\n", "And it shows the ability to automatically determine synonymous relations and objects using large sets of extractions.\n", "We have built a faster user interface for querying the results.\n", "We provide an online demo of TextRunner.\n"]}
{"article_lines": ["What's In A Translation Rule ?", "We propose a theory that gives formal semantics to word level alignments defined over parallel corpora .", "We use our theory to introduce a linear algorithm that can be used to derive from word aligned , parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data .", "In a very interesting study of syntax in statistical machine translation , Fox 2002 looks at how well proposed translation models fit actual translation data .", "One such model embodies a restricted , linguistically motivated notion of word re ordering .", "Given an English parse tree , children at any node may be reordered prior to translation .", "Nodes are processed independently .", "Previous to Fox 2002 , it had been observed that this model would prohibit certain re orderings in certain language pairs such as subjectVP verb object into verb subject object , but Fox carried out the first careful empirical study , showing that many other common translation patterns fall outside the scope of the child reordering model .", "This is true even for languages as similar as English and French .", "For example , English adverbs tend to move outside the local parent children in environment .", "The English word not translates to the discontiguous pair ne . . . pas . English parsing errors also cause trouble , as a normally well behaved re ordering environment can be disrupted by wrong phrase attachment .", "For other language pairs , the divergence is expected to be greater .", "In the face of these problems , we may choose among several alternatives .", "The first is to abandon syntax in statistical machine translation , on the grounds that syntactic models are a poor fit for the data .", "On this view , adding syntax yields no improvement over robust phrasesubstitution models , and the only question is how much does syntax hurt performance .", "Along this line , Koehn et al . , 2003 present convincing evidence that restricting phrasal translation to syntactic constituents yields poor translation performance the ability to translate nonconstituent phrases such as there are , note that , and according to turns out to be critical and pervasive .", "Another direction is to abandon conventional English syntax and move to more robust grammars that adapt to the parallel training corpus .", "One approach here is that of Wu 1997 , in which word movement is modeled by rotations at unlabeled , binary branching nodes .", "At each sentence pair , the parse adapts to explain the translation pattern .", "If the same unambiguous English sentence were to appear twice in the corpus , with different Chinese translations , then it could have different learned parses .", "A third direction is to maintain English syntax and investigate alternate transformation models .", "After all , many conventional translation systems are indeed based on syntactic transformations far more expressive than what has been proposed in syntax based statistical MT .", "We take this approach in our paper .", "Of course , the broad statistical MT program is aimed at a wider goal than the conventional rule based program it seeks to understand and explain human translation data , and automatically learn from it .", "For this reason , we think it is important to learn from the model data explainability studies of Fox 2002 and to extend her results .", "In addition to being motivated by rule based systems , we also see advantages to English syntax within the statistical framework , such as marrying syntax based translation models with syntaxbased language models Charniak et al . , 2003 and other potential benefits described by Eisner 2003 .", "Our basic idea is to create transformation rules that condition on larger fragments of tree structure .", "It is certainly possible to build such rules by hand , and we have done this to formally explain a number of humantranslation examples .", "But our main interest is in collecting a large set of such rules automatically through corpus analysis .", "The search for these rules is driven exactly by the problems raised by Fox 2002 cases of crossing and divergence motivate the algorithms to come up with better explanations of the data and better rules .", "Section 2 of this paper describes algorithms for the acquisition of complex rules for a transformation model .", "Section 3 gives empirical results on the explanatory power of the acquired rules versus previous models .", "Section 4 presents examples of learned rules and shows the various types of transformations lexical and nonlexical , contiguous and noncontiguous , simple and complex that the algorithms are forced by the data to invent .", "Section 5 concludes .", "Due to space constraints , all proofs are omitted .", "Suppose that we have a French sentence , its translation into English , and a parse tree over the English translation , as shown in Figure 1 .", "Generally one defines an alignment as a relation between the words in the French sentence and the words in the English sentence .", "Given such an alignment however , what kinds of rules are we entitled to learn from this instance ?", "How do we know when it is valid to extract a particular rule , especially in the presence of numerous crossings in the alignment ?", "In this section , we give principled answers to these questions , by constructing a theory that gives formal semantics to word alignments .", "We are going to define a generative process through which a string from a source alphabet is mapped to a rooted tree whose nodes are labeled from a target alphabet .", "Henceforth we will refer to symbols from our source alphabet as source symbols and symbols from our target alphabet as target symbols .", "We define a symbol tree over an alphabet A as a rooted , directed tree , the nodes of which are each labeled with a symbol of A .", "We want to capture the process by which a symbol tree over the target language is derived from a string of source symbols .", "Let us refer to the symbol tree that we want to derive as the target tree .", "Any subtree of this tree will be called a target subtree .", "Furthermore , we define a derivation string as an ordered sequence of elements , each of which is either a source symbol or a target subtree .", "Now we are ready to define the derivation process .", "Given a derivation string S , a derivation step replaces a substring S' of S with a target subtree T that has the following properties Moreover , a derivation from a string S of source symbols to the target tree T is a sequence of derivation steps that produces T from S . Moving away from the abstract for a moment , let us revisit the example from Figure 1 .", "Figure 2 shows three derivations of the target tree from the source string il ne va pas , which are all consistent with our definitions .", "However , it is apparent that one of these derivations seems much more wrong than the other .", "Specifically , in the second derivation , pas is replaced by the English word he , which makes no sense .", "Given the vast space of possible derivations according to the definition above , how do we distinguish between good ones and bad ones ?", "Here is where the notion of an alignment becomes useful .", "Let S be a string of source symbols and let T be a target tree .", "First observe the following facts about derivations from S to T these follow directly from the definitions 1 .", "Each element of S is replaced at exactly one step of the derivation .", "Thus for each element s of S , we can define replaced s , D to be the step of the derivation D during which s is replaced .", "For instance , in the leftmost derivation of Figure 2 , va is replaced by the second step of the derivation , thus replaced va , D 2 .", "Similarly , for each node t of T , we can define created t , D to be the step of derivation D during which t is created .", "For instance , in the same derivation , the nodes labeled by AUX and VP are created during the third step of the derivation , thus created AUX , D 3 and created VP , D 3 .", "Given a string S of source symbols and a target tree T , an alignment A with respect to S and T is a relation between the leaves of T and the elements of S . Choose some derivation D from S to T . The alignment A induced by D is created as follows an element s of S is aligned with a leaf node t of T iff replaced s , D created t , D .", "In other words , a source word is aligned with a target word if the target word is created during the same step in which the source word is replaced .", "Figure 3 shows the alignments induced by the derivations of Figure 2 .", "Now , say that we have a source string , a target tree , and an alignment A .", "A key observation is that the set of good derivations according to A is precisely the set of derivations that induce alignments A' such that A is a subalignment of A' .", "By subalignment , we mean that A C_ A' recall that alignments are simple mathematical relations .", "In other words , A is a subalignment of A' if A aligns two elements only if A' also aligns them .", "We can see this intuitively by examining Figures 2 and 3 .", "Notice that the two derivations that seem right the first and the third are superalignments of the alignment given in Figure 1 , while the derivation that is clearly wrong is not .", "Hence we now have a formal definition of the derivations that we are interested in .", "We say that a derivation is admitted by an alignment A if it induces a superalignment of A .", "The set of derivations from source string S to target tree T that are admitted by alignment A can be denoted 6A S , T .", "Given this , we are ready to obtain a formal characterization of the set of rules that can be inferred from the source string , target tree , and alignment .", "In essence , a derivation step can be viewed as the application of a rule .", "Thus , compiling the set of derivation steps used in any derivation of 6A S , T gives us , in a meaningful sense , all relevant rules that can be extracted from the triple S , T , A .", "In this section , we show in concrete terms how to convert a derivation step into a usable rule .", "Consider the second last derivation step of the first derivation in Figure 2 .", "In it , we begin with a source symbol ne , followed by a target subtree rooted at V B , followed by another source symbol pas . These three elements of the derivation string are replaced with a target subtree rooted at V P that discards the source symbols and contains the target subtree rooted at V B .", "In general , this replacement process can be captured by the rule depicted in Figure 4 .", "The input to the rule are the roots of the elements of the derivation string that are replaced where we define the root of a symbol to be simply the symbol itself , whereas the output of the rule is a symbol tree , except that some of the leaves are labeled with variables instead of symbols from the target alphabet .", "These variables correspond to elements of the input to the rule .", "For instance , the leaf labeled x2 means that when this rule is applied , x2 is replaced by the target subtree rooted at V B since V B is the second element of the input .", "Observe that the second rule induced in Figure 4 is simply a CFG rule expressed in the opposite direction , thus this rule format can and should be viewed as a strict generalization of CFG rules .", "Every derivation step can be mapped to a rule in this way .", "Hence given a source string S , a target tree T , and an alignment A , we can define the set PA S , T as the set of rules in any derivation D E 6A S , T .", "We can regard this as the set of rules that we are entitled to infer from the triple S , T , A .", "Now we have a precise problem statement learn the set PA S , T .", "It is not immediately clear how such a set can be learned from the triple S , T , A .", "Fortunately , we can infer these rules directly from a structure called an alignment graph .", "In fact , we have already seen numerous examples of alignment graphs .", "Graphically , we have been depicting the triple S , T , A as a rooted , directed , acyclic graph where direction is top down in the diagrams .", "We refer to such a graph as an alignment graph .", "Formally , the alignment graph corresponding to S , T , and A is just T , augmented with a node for each element of S , and edges from leaf node t E T to element s E S iff A aligns s with t . Although there is a difference between a node of the alignment graph and its label , we will not make a distinction , to ease the notational burden .", "To make the presentation easier to follow , we assume throughout this section that the alignment graph is connected , i . e . there are no unaligned elements .", "All of the results that follow have generalizations to deal with unaligned elements , but unaligned elements incur certain procedural complications that would cloud the exposition .", "It turns out that it is possible to systematically convert certain fragments of the alignment graph into rules of PA S , T .", "We define a fragment of a directed , acyclic graph G to be a nontrivial i . e . not just a single node subgraph G' of G such that if a node n is in G' then either n is a sink node of G' i . e . it has no children or all of its children are in G' and it is connected to all of them .", "In Figure 6 , we show two examples of graph fragments of the alignment graph of Figure 5 .", "The span of a node n of the alignment graph is the subset of nodes from S that are reachable from n . Note that this definition is similar to , but not quite the same as , the definition of a span given by Fox 2002 .", "We say that a span is contiguous if it contains all elements of a contiguous substring of S . The closure of span n is the shortest contiguous span which is a superset of span n .", "For instance , the closure of 1s2 , s3 , s5 , s71 would be 1s2 , s3 , s4 , s5 , s6 , s71 The alignment graph in Figure 5 is annotated with the span of each node .", "Take a look at the graph fragments in Figure 6 .", "These fragments are special they are examples of frontier graph fragments .", "We first define the frontier set of an alignment graph to be the set of nodes n that satisfy the following property for every node n' of the alignment graph that is connected to n but is neither an ancestor nor a descendant of n , span n' n closure span n 0 .", "We then define a frontier graph fragment of an alignment graph to be a graph fragment such that the root and all sinks are in the frontier set .", "Frontier graph fragments have the property that the spans of the sinks of the fragment are each contiguous and form a partition of the span of the root , which is also contiguous .", "This allows the following transformation process take the tree part of the fragment i . e . project the fragment on T .", "This forms the output of the rule .", "Figure 6 shows the rules derived from the given graph fragments .", "We have the following result .", "Theorem 1 Rules constructed according to the above procedure are in PA S , T .", "Rule extraction Algorithm 1 .", "Thus we now have a simple method for extracting rules of PA S , T from the alignment graph search the space of graph fragments for frontier graph fragments .", "Unfortunately , the search space of all fragments of a graph is exponential in the size of the graph , thus this procedure can also take a long time to execute .", "To arrive at a much faster procedure , we take advantage of the following provable facts minimal frontier graph fragment rooted at n observe that for any node n' not in the frontier set , there is no frontier graph fragment rooted at n' , by definition .", "By minimal , we mean that the frontier graph fragment is a subgraph of every other frontier graph fragment with the same root .", "Clearly , for an alignment graph with k nodes , there are at most k minimal frontier graph fragments .", "In Figure 7 , we show the seven minimal frontier graph fragments of the alignment graph of Figure 5 .", "Furthermore , all other frontier graph fragments can be created by composing 2 or more minimal graph fragments , as shown in Figure 8 .", "Thus , the entire set of frontier graph fragments and all rules derivable from these fragments can be computed systematically as follows compute the set of minimal frontier graph fragments , compute the set of graph fragments resulting from composing 2 minimal frontier graph fragments , compute the set of graph fragments resulting from composing 3 minimal graph fragments , etc .", "In this way , the rules derived from the minimal frontier graph fragments can be regarded as a basis for all other rules derivable from frontier graph fragments .", "Furthermore , we conjecture that the set of rules derivable from frontier graph fragments is in fact equivalent to PA S , T .", "Thus we have boiled down the problem of extracting complex rules to the following simple problem find the set of minimal frontier graph fragments of a given alignment graph .", "The algorithm is a two step process , as shown below .", "Step 1 can be computed in a single traversal of the alignment graph .", "This traversal annotates each node with its span and its complement span .", "The complement span is computed as the union of the complement span of its parent and the span of all its siblings siblings are nodes that share the same parent .", "A node n is in the frontier set iff complement span n n closure span n 0 .", "Notice that the complement span merely summarizes the spans of all nodes that are neither ancestors nor descendents of n . Since this step requires only a single graph traversal , it runs in linear time .", "Step 2 can also be computed straightforwardly .", "For each node n of the frontier set , do the following expand n , then as long as there is some sink node n' of the resulting graph fragment that is not in the frontier set , expand n' .", "Note that after computing the minimal graph fragment rooted at each node of the frontier set , every node of the alignment graph has been expanded at most once .", "Thus this step also runs in linear time .", "For clarity of exposition and lack of space , a couple of issues have been glossed over .", "Briefly results of the next two sections are all based on implementations that handle unaligned elements .", "This theory can be generalized quite cleanly to include derivations for which substrings are replaced by sets of trees , rather than one single tree .", "This corresponds to allowing rules that do not require the output to be a single , rooted tree .", "Such a generalization gives some nice power to effectively explain certain linguistic phenomena .", "For instance , it allows us to immediately translate va as does go instead of delaying the creation of the auxiliary word does until later in the derivation .", "We evaluated the coverage of our model of transformation rules with two language pairs English French and English Chinese .", "These two pairs clearly contrast by the underlying difficulty to understand and model syntactic transformations among pairs while there is arguably a fair level of cohesion between English and French , English and Chinese are syntactically more distant languages .", "We also chose French to compare our study with that of Fox 2002 .", "The additional language pair provides a good means of evaluating how our transformation rule extraction method scales to more problematic language pairs for which child reordering models are shown not to explain the data well .", "We performed experiments with two corpora , the FBIS English Chinese Parallel Text and the Hansard FrenchEnglish corpus . We parsed the English sentences with a state of the art statistical parser Collins , 1999 .", "For the FBIS corpus representing eight million English words , we automatically generated word alignments using GIZA Och and Ney , 2003 , which we trained on a much larger data set 150 million words .", "Cases other than one to one sentence mappings were eliminated .", "For the Hansard corpus , we took the human annotation of word alignment described in Och and Ney , 2000 .", "The corpus contains two kinds of alignments S sure for unambiguous cases and P possible for unclear cases , e . g . idiomatic expressions and missing function words S C_ P .", "In order to be able to make legitimate comparisons between the two language pairs , we also used GIZA to obtain machine generated word alignments for Hansard we trained it with the 500 sentences and additional data representing 13 . 7 million English words taken from the Hansard and European parliament corpora .", "From a theoretical point of view , we have shown that our model can fully explain the transformation of any parse tree of the source language into a string of the target language .", "The purpose of this section is twofold to provide quantitative results confirming the full coverage of our model and to analyze some properties of the transformation rules that support these derivations linguistic analyses of these rules are presented in the next section .", "Figure 9 summarizes the coverage of our model with respect to the Hansard and FBIS corpora .", "For the former , we present results for the three alignments S alignments , P alignments , and the alignments computed by GIZA .", "Each plotted value represents a percentage of parse trees in a corpus that can be transformed into a target sentence using transformation rules .", "The x axis represents different restrictions on the size of these rules if we use a model that restrict rules to a single expansion of a non terminal into a sequence of symbols , we are in the scope of the child reordering model of Yamada and Knight , 2001 ; Fox , 2002 .", "We see that its explanatory power is quite poor , with only 19 . 4 , 14 . 3 , 16 . 5 , and 12 . 1 for the respective corpora .", "Allowing more expansions logically expands the coverage of the model , until the point where it is total transformation rules no larger than 17 , 18 , 23 , and 43 in number of rule expansions respectively provide enough coverage to explain the data at 100 for each of the four cases .", "It appears from the plot that the quality of alignments plays an important role .", "If we compare the three kinds of alignments available for the Hansard corpus , we see that much more complex transformation rules are extracted from noisy GIZA alignments .", "It also appears that the language difference produces quite contrasting results .", "Rules acquired for the English Chinese pair have , on average , many more nodes .", "Note that the language difference in terms of syntax might be wider than what the plot seems to indicate , since word alignments computed for the Hansard corpus are likely to be more errorful than the ones for FBIS because the training data used to induce the latter is more than ten times larger than for the former .", "In Figure 10 , we show the explanatory power of our model at the node level .", "At each node of the frontier set , we determine whether it is possible to extract a rule that doesn t exceed a given limit k on its size .", "The plotted values represent the percentage of frontier set internal nodes that satisfy this condition .", "These results appear more promising for the child reordering model , with coverage ranging from 72 . 3 to 85 . 1 of the nodes , but we should keep in mind that many of these nodes are low in the tree e . g . base NPs ; extraction of 1 level transformation rules generally present no difficulties when child nodes are pre terminals , since any crossings can be resolved by lexicalizing the elements involved in it .", "However , higher level syntactic constituents are more problematic for child reordering models , and the main reasons they fail to provide explanation of the parses at the sentence level .", "Table 1 shows that the extraction of rules can be performed quite efficiently .", "Our first algorithm , which has an exponential running time , cannot scale to process large corpora and extract a sufficient number of rules that a syntax based statistical MT system would require .", "The second algorithm , which runs in linear time , is on the other hand barely affected by the size of rules it extracts .", "In this section , we present some syntactic transformation rules that our system learns .", "Fox 2002 identified three major causes of crossings between English and French the ne . . . pas construct , modals and adverbs , which a child reordering model doesn t account for .", "In section 2 , we have already explained how we learn syntactic rules involving ne . . . pas .", "Here we describe the other two problematic cases .", "Figure 11 presents a frequent cause of crossings between English and French adverbs in French often appear after the verb , which is less common in English .", "Parsers generally create nested verb phrases when adverbs are present , thus no child reordering can allow a verb and an adverb to be permuted .", "Multi level reodering as the rule in the figure can prevent crossings .", "Fox s solution to the problem of crossings is to flatten verb phrases .", "This is a solution for this sentence pair , since this accounts for adverb verb reorderings , but flattening the tree structure is not a general solution .", "Indeed , it can only apply to a very limited number of syntactic categories , for which the advantage of having a deep syntactic structure is lost .", "Figure 12 dotted lines are P alignments shows an interesting example where flattening the tree structure cannot resolve all crossings in node reordering models .", "In these models , a crossing remains between MD and AUX no matter how VPs are flattened .", "Our transformation rule model creates a lexicalized rule as shown in the figure , where the transformation of will be into sera is the only way to resolve the crossing .", "In the Chinese English domain , the rules extracted by our algorithm often have the attractive quality that they are the kind of common sense constructions that are used in Chinese language textbooks to teach students .", "For instance , there are several that illustrate the complex reorderings that occur around the Chinese marker word de .", "The fundamental assumption underlying much recent work in statistical machine translation Yamada and Knight , 2001 ; Eisner , 2003 ; Gildea , 2003 is that local transformations primarily child node re orderings of one level parent children substructures are an adequate model for parallel corpora .", "Our empirical results suggest that this may be too strong of an assumption .", "To explain the data in two parallel corpora , one English French , and one English Chinese , we are often forced to learn rules involving much larger tree fragments .", "The theory , algorithms , and transformation rules we learn automatically from data have several interesting aspects .", "Our rules provide a good , realistic indicator of the complexities inherent in translation .", "We believe that these rules can inspire subsequent developments of generative statistical models that are better at explaining parallel data than current ones .", "Our rules put at the fingertips of linguists a very rich source of information .", "They encode translation transformations that are both syntactically and lexically motivated some of our rules are purely syntactic ; others are lexically grounded .", "A simple sort on the counts of our rules makes explicit the transformations that occur most often .", "A comparison of the number of rules extracted from parallel corpora specific to multiple language pairs provide a quantitative estimator of the syntactic closeness between various language pairs .", "The theory we proposed in this paper is independent of the method that one uses to compute the wordlevel alignments in a parallel corpus .", "The theory and rule extraction algorithm are also well suited to deal with the errors introduced by the word level alignment and parsing programs one uses .", "Our theory makes no a priori assumptions about the transformations that one is permitted to learn .", "If a parser , for example , makes a systematic error , we expect to learn a rule that can nevertheless be systematically used to produce correct translations .", "In this paper , we focused on providing a well founded mathematical theory and efficient , linear algorithms for learning syntactically motivated transformation rules from parallel corpora .", "One can easily imagine a range of techniques for defining probability distributions over the rules that we learn .", "We suspect that such probabilistic rules could be also used in conjunction with statistical decoders , to increase the accuracy of statistical machine translation systems .", "This work was supported by DARPA contract N6600100 1 9814 and MURI grant N00014 00 1 0617 ."], "summary_lines": ["What's In A Translation Rule?\n", "We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora.\n", "We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data.\n", "We describe how to learn hundreds of millions of tree transformation rules from a parsed, aligned Chinese/English corpus, and we describe probability estimators for those rules.\n", "Our translation rules are learned from word-aligned bilingual texts whose source side has been parsed by using a syntactic parser.\n"]}
{"article_lines": ["Online Large Margin Training Of Dependency Parsers", "We present an effective training algorithm for linearly scored dependency parsers that implements online largemargin multi class training Crammer and Singer , 2003 ; Crammer et al . , 2003 on top of efficient parsing techniques for dependency trees Eisner , 1996 .", "The trained parsers achieve a competitive dependency accuracy for both English and Czech with no language specific enhancements .", "Research on training parsers from annotated data has for the most part focused on models and training algorithms for phrase structure parsing .", "The best phrase structure parsing models represent generatively the joint probability P x , y of sentence x having the structure y Collins , 1999 ; Charniak , 2000 .", "Generative parsing models are very convenient because training consists of computing probability estimates from counts of parsing events in the training set .", "However , generative models make complicated and poorly justified independence assumptions and estimations , so we might expect better performance from discriminatively trained models , as has been shown for other tasks like document classification Joachims , 2002 and shallow parsing Sha and Pereira , 2003 .", "Ratnaparkhi s conditional maximum entropy model Ratnaparkhi , 1999 , trained to maximize conditional likelihood P y x of the training data , performed nearly as well as generative models of the same vintage even though it scores parsing decisions in isolation and thus may suffer from the label bias problem Lafferty et al . , 2001 .", "Discriminatively trained parsers that score entire trees for a given sentence have only recently been investigated Riezler et al . , 2002 ; Clark and Curran , 2004 ; Collins and Roark , 2004 ; Taskar et al . , 2004 .", "The most likely reason for this is that discriminative training requires repeatedly reparsing the training corpus with the current model to determine the parameter updates that will improve the training criterion .", "The reparsing cost is already quite high for simple context free models with O n3 parsing complexity , but it becomes prohibitive for lexicalized grammars with O n5 parsing complexity .", "Dependency trees are an alternative syntactic representation with a long history Hudson , 1984 .", "Dependency trees capture important aspects of functional relationships between words and have been shown to be useful in many applications including relation extraction Culotta and Sorensen , 2004 , paraphrase acquisition Shinyama et al . , 2002 and machine translation Ding and Palmer , 2005 .", "Yet , they can be parsed in O n3 time Eisner , 1996 .", "Therefore , dependency parsing is a potential sweet spot that deserves investigation .", "We focus here on projective dependency trees in which a word is the parent of all of its arguments , and dependencies are non crossing with respect to word order see Figure 1 .", "However , there are cases where crossing dependencies may occur , as is the case for Czech Haji\u02c7c , 1998 .", "Edges in a dependency tree may be typed for instance to indicate grammatical function .", "Though we focus on the simpler non typed case , all algorithms are easily extendible to typed structures .", "The following work on dependency parsing is most relevant to our research .", "Eisner 1996 gave a generative model with a cubic parsing algorithm based on an edge factorization of trees .", "Yamada and Matsumoto 2003 trained support vector machines SVM to make parsing decisions in a shift reduce dependency parser .", "As in Ratnaparkhi s parser , the classifiers are trained on individual decisions rather than on the overall quality of the parse .", "Nivre and Scholz 2004 developed a history based learning model .", "Their parser uses a hybrid bottom up topdown linear time heuristic parser and the ability to label edges with semantic types .", "The accuracy of their parser is lower than that of Yamada and Matsumoto 2003 .", "We present a new approach to training dependency parsers , based on the online large margin learning algorithms of Crammer and Singer 2003 and Crammer et al . 2003 .", "Unlike the SVM parser of Yamada and Matsumoto 2003 and Ratnaparkhi s parser , our parsers are trained to maximize the accuracy of the overall tree .", "Our approach is related to those of Collins and Roark 2004 and Taskar et al . 2004 for phrase structure parsing .", "Collins and Roark 2004 presented a linear parsing model trained with an averaged perceptron algorithm .", "However , to use parse features with sufficient history , their parsing algorithm must prune heuristically most of the possible parses .", "Taskar et al . 2004 formulate the parsing problem in the large margin structured classification setting Taskar et al . , 2003 , but are limited to parsing sentences of 15 words or less due to computation time .", "Though these approaches represent good first steps towards discriminatively trained parsers , they have not yet been able to display the benefits of discriminative training that have been seen in namedentity extraction and shallow parsing .", "Besides simplicity , our method is efficient and accurate , as we demonstrate experimentally on English and Czech treebank data .", "In what follows , the generic sentence is denoted by x possibly subscripted ; the ith word of x is denoted by xi .", "The generic dependency tree is denoted by y .", "If y is a dependency tree for sentence x , we write i , j E y to indicate that there is a directed edge from word xi to word xj in the tree , that is , xi is the parent of xj .", "T xt , yt t_1 denotes the training data .", "We follow the edge based factorization method of Eisner 1996 and define the score of a dependency tree as the sum of the score of all edges in the tree , where f i , j is a high dimensional binary feature representation of the edge from xi to xj .", "For example , in the dependency tree of Figure 1 , the following feature would have a value of 1 In general , any real valued feature may be used , but we use binary features for simplicity .", "The feature weights in the weight vector w are the parameters that will be learned during training .", "Our training algorithms are iterative .", "We denote by w i the weight vector after the ith training iteration .", "Finally we define dt x as the set of possible dependency trees for the input sentence x and bestk x ; w as the set of k dependency trees in dt x that are given the highest scores by weight vector w , with ties resolved by an arbitrary but fixed rule .", "Three basic questions must be answered for models of this form how to find the dependency tree y with highest score for sentence x ; how to learn an appropriate weight vector w from the training data ; and finally , what feature representation f i , j should be used .", "The following sections address each of these questions .", "Given a feature representation for edges and a weight vector w , we seek the dependency tree or trees that maximize the score function , s x , y .", "The primary difficulty is that for a given sentence of length n there are exponentially many possible dependency trees .", "Using a slightly modified version of a lexicalized CKY chart parsing algorithm , it is possible to generate and represent these sentences in a forest that is O n5 in size and takes O n5 time to create .", "Eisner 1996 made the observation that if the head of each chart item is on the left or right periphery , then it is possible to parse in O n3 .", "The idea is to parse the left and right dependents of a word independently and combine them at a later stage .", "This removes the need for the additional head indices of the O n5 algorithm and requires only two additional binary variables that specify the direction of the item either gathering left dependents or gathering right dependents and whether an item is complete available to gather more dependents .", "Figure 2 shows the algorithm schematically .", "As with normal CKY parsing , larger elements are created bottom up from pairs of smaller elements .", "Eisner showed that his algorithm is sufficient for both searching the space of dependency parses and , with slight modification , finding the highest scoring tree y for a given sentence x under the edge factorization assumption .", "Eisner and Satta 1999 give a cubic algorithm for lexicalized phrase structures .", "However , it only works for a limited class of languages in which tree spines are regular .", "Furthermore , there is a large grammar constant , which is typically in the thousands for treebank parsers .", "Figure 3 gives pseudo code for the generic online learning setting .", "A single training instance is considered on each iteration , and parameters updated by applying an algorithm specific update rule to the instance under consideration .", "The algorithm in Figure 3 returns an averaged weight vector an auxiliary weight vector v is maintained that accumulates Training data T xt , yt Tt 1 the values of w after each iteration , and the returned weight vector is the average of all the weight vectors throughout training .", "Averaging has been shown to help reduce overfitting Collins , 2002 .", "Crammer and Singer 2001 developed a natural method for large margin multi class classification , which was later extended by Taskar et al . 2003 to structured classification where L y , y' is a real valued loss for the tree y' relative to the correct tree y .", "We define the loss of a dependency tree as the number of words that have the incorrect parent .", "Thus , the largest loss a dependency tree can have is the length of the sentence .", "Informally , this update looks to create a margin between the correct dependency tree and each incorrect dependency tree at least as large as the loss of the incorrect tree .", "The more errors a tree has , the farther away its score will be from the score of the correct tree .", "In order to avoid a blow up in the norm of the weight vector we minimize it subject to constraints that enforce the desired margin between the correct and incorrect trees1 .", "The Margin Infused Relaxed Algorithm MIRA Crammer and Singer , 2003 ; Crammer et al . , 2003 employs this optimization directly within the online framework .", "On each update , MIRA attempts to keep the norm of the change to the parameter vector as small as possible , subject to correctly classifying the instance under consideration with a margin at least as large as the loss of the incorrect classifications .", "This can be formalized by substituting the following update into line 4 of the generic online algorithm , This is a standard quadratic programming problem that can be easily solved using Hildreth s algorithm Censor and Zenios , 1997 .", "Crammer and Singer 2003 and Crammer et al . 2003 provide an analysis of both the online generalization error and convergence properties of MIRA .", "In equation 1 , s x , y is calculated with respect to the weight vector after optimization , w Z 1 .", "To apply MIRA to dependency parsing , we can simply see parsing as a multi class classification problem in which each dependency tree is one of many possible classes for a sentence .", "However , that interpretation fails computationally because a general sentence has exponentially many possible dependency trees and thus exponentially many margin constraints .", "To circumvent this problem we make the assumption that the constraints that matter for large margin optimization are those involving the incorrect trees y' with the highest scores s x , y' .", "The resulting optimization made by MIRA see Figure 3 , line 4 would then be reducing the number of constraints to the constant k . We tested various values of k on a development data set and found that small values of k are sufficient to achieve close to best performance , justifying our assumption .", "In fact , as k grew we began to observe a slight degradation of performance , indicating some overfitting to the training data .", "All the experiments presented here use k 5 .", "The Eisner 1996 algorithm can be modified to find the k best trees while only adding an additional O k log k factor to the runtime Huang and Chiang , 2005 .", "A more common approach is to factor the structure of the output space to yield a polynomial set of local constraints Taskar et al . , 2003 ; Taskar et al . , 2004 .", "One such factorization for dependency trees It is trivial to show that if these O n2 constraints are satisfied , then so are those in 1 .", "We implemented this model , but found that the required training time was much larger than the k best formulation and typically did not improve performance .", "Furthermore , the k best formulation is more flexible with respect to the loss function since it does not assume the loss function can be factored into a sum of terms for each dependency .", "Finally , we need a suitable feature representation f i , j for each dependency .", "The basic features in our model are outlined in Table 1a and b .", "All features are conjoined with the direction of attachment as well as the distance between the two words being attached .", "These features represent a system of backoff from very specific features over words and partof speech tags to less sparse features over just partof speech tags .", "These features are added for both the entire words as well as the 5 gram prefix if the word is longer than 5 characters .", "Using just features over the parent child node pairs in the tree was not enough for high accuracy , because all attachment decisions were made outside of the context in which the words occurred .", "To solve this problem , we added two other types of features , which can be seen in Table 1c .", "Features of the first type look at words that occur between a child and its parent .", "These features take the form of a POS trigram the POS of the parent , of the child , and of a word in between , for all words linearly between the parent and the child .", "This feature was particularly helpful for nouns identifying their parent , since node . p pos POS of parent node . c pos POS of child node . p pos 1 POS to the right of parent in sentence . p pos 1 POS to the left of parent . c pos 1 POS to the right of child . c pos 1 POS to the left of child . b pos POS of a word in between parent and child nodes . it would typically rule out situations when a noun attached to another noun with a verb in between , which is a very uncommon phenomenon .", "The second type of feature provides the local context of the attachment , that is , the words before and after the parent child pair .", "This feature took the form of a POS 4 gram The POS of the parent , child , word before after parent and word before after child .", "The system also used back off features to various trigrams where one of the local context POS tags was removed .", "Adding these two features resulted in a large improvement in performance and brought the system to state of the art accuracy .", "Besides performance see Section 3 , the approach to dependency parsing we described has several other advantages .", "The system is very general and contains no language specific enhancements .", "In fact , the results we report for English and Czech use identical features , though are obviously trained on different data .", "The online learning algorithms themselves are intuitive and easy to implement .", "The efficient O n3 parsing algorithm of Eisner allows the system to search the entire space of dependency trees while parsing thousands of sentences in a few minutes , which is crucial for discriminative training .", "We compare the speed of our model to a standard lexicalized phrase structure parser in Section 3 . 1 and show a significant improvement in parsing times on the testing data .", "The major limiting factor of the system is its restriction to features over single dependency attachments .", "Often , when determining the next dependent for a word , it would be useful to know previous attachment decisions and incorporate these into the features .", "It is fairly straightforward to modify the parsing algorithm to store previous attachments .", "However , any modification would result in an asymptotic increase in parsing complexity .", "We tested our methods experimentally on the English Penn Treebank Marcus et al . , 1993 and on the Czech Prague Dependency Treebank Haji\u02c7c , 1998 .", "All experiments were run on a dual 64 bit AMD Opteron 2 . 4GHz processor .", "To create dependency structures from the Penn Treebank , we used the extraction rules of Yamada and Matsumoto 2003 , which are an approximation to the lexicalization rules of Collins 1999 .", "We split the data into three parts sections 02 21 for training , section 22 for development and section 23 for evaluation .", "Currently the system has 6 , 998 , 447 features .", "Each instance only uses a tiny fraction of these features making sparse vector calculations possible .", "Our system assumes POS tags as input and uses the tagger of Ratnaparkhi 1996 to provide tags for the development and evaluation sets .", "Table 2 shows the performance of the systems that were compared .", "Y M2003 is the SVM shiftreduce parsing model of Yamada and Matsumoto 2003 , N S2004 is the memory based learner of Nivre and Scholz 2004 and MIRA is the the system we have described .", "We also implemented an averaged perceptron system Collins , 2002 another online learning algorithm for comparison .", "This table compares only pure dependency parsers that do identified their parent in the tree .", "Root is the number of trees in which the root word was correctly identified .", "For Czech this is f measure since a sentence may have multiple roots .", "Complete is the number of sentences for which the entire dependency tree was correct . not exploit phrase structure .", "We ensured that the gold standard dependencies of all systems compared were identical .", "Table 2 shows that the model described here performs as well or better than previous comparable systems , including that of Yamada and Matsumoto 2003 .", "Their method has the potential advantage that SVM batch training takes into account all of the constraints from all training instances in the optimization , whereas online training only considers constraints from one instance at a time .", "However , they are fundamentally limited by their approximate search algorithm .", "In contrast , our system searches the entire space of dependency trees and most likely benefits greatly from this .", "This difference is amplified when looking at the percentage of trees that correctly identify the root word .", "The models that search the entire space will not suffer from bad approximations made early in the search and thus are more likely to identify the correct root , whereas the approximate algorithms are prone to error propagation , which culminates with attachment decisions at the top of the tree .", "When comparing the two online learning models , it can be seen that MIRA outperforms the averaged perceptron method .", "This difference is statistically significant , p 0 . 005 McNemar test on head selection accuracy .", "In our Czech experiments , we used the dependency trees annotated in the Prague Treebank , and the predefined training , development and evaluation sections of this data .", "The number of sentences in this data set is nearly twice that of the English treebank , leading to a very large number of features 13 , 450 , 672 .", "But again , each instance uses just a handful of these features .", "For POS tags we used the automatically generated tags in the data set .", "Though we made no language specific model changes , we did need to make some data specific changes .", "In particular , we used the method of Collins et al . 1999 to simplify part of speech tags since the rich tags used by Czech would have led to a large but rarely seen set of POS features .", "The model based on MIRA also performs well on Czech , again slightly outperforming averaged perceptron .", "Unfortunately , we do not know of any other parsing systems tested on the same data set .", "The Czech parser of Collins et al . 1999 was run on a different data set and most other dependency parsers are evaluated using English .", "Learning a model from the Czech training data is somewhat problematic since it contains some crossing dependencies which cannot be parsed by the Eisner algorithm .", "One trick is to rearrange the words in the training set so that all trees are nested .", "This at least allows the training algorithm to obtain reasonably low error on the training set .", "We found that this did improve performance slightly to 83 . 6 accuracy .", "It is well known that dependency trees extracted from lexicalized phrase structure parsers Collins , 1999 ; Charniak , 2000 typically are more accurate than those produced by pure dependency parsers Yamada and Matsumoto , 2003 .", "We compared our system to the Bikel re implementation of the Collins parser Bikel , 2004 ; Collins , 1999 trained with the same head rules of our system .", "There are two ways to extract dependencies from lexicalized phrase structure .", "The first is to use the automatically generated dependencies that are explicit in the lexicalization of the trees , we call this system Collinsauto .", "The second is to take just the phrase structure output of the parser and run the automatic head rules over it to extract the dependencies , we call this system Collins rules .", "Table 3 shows the results comparing our system , MIRA Normal , to the Collins parser for English .", "All systems are implemented in Java and run on the same machine .", "Interestingly , the dependencies that are automatically produced by the Collins parser are worse than those extracted statically using the head rules .", "Arguably , this displays the artificialness of English dependency parsing using dependencies automatically extracted from treebank phrase structure trees .", "Our system falls in between , better than the automatically generated dependency trees and worse than the head rule extracted trees .", "Since the dependencies returned from our system are better than those actually learnt by the Collins parser , one could argue that our model is actually learning to parse dependencies more accurately .", "However , phrase structure parsers are built to maximize the accuracy of the phrase structure and use lexicalization as just an additional source of information .", "Thus it is not too surprising that the dependencies output by the Collins parser are not as accurate as our system , which is trained and built to maximize accuracy on dependency trees .", "In complexity and run time , our system is a huge improvement over the Collins parser .", "The final system in Table 3 takes the output of Collins rules and adds a feature to MIRA Normal that indicates for given edge , whether the Collins parser believed this dependency actually exists , we call this system MIRA Collins .", "This is a well known discriminative training trick using the suggestions of a generative system to influence decisions .", "This system can essentially be considered a corrector of the Collins parser and represents a significant improvement over it .", "However , there is an added complexity with such a model as it requires the output of the O n5 Collins parser .", "One question that can be asked is how justifiable is the k best MIRA approximation .", "Table 4 indicates the accuracy on testing and the time it took to train models with k 1 , 2 , 5 , 10 , 20 for the English data set .", "Even though the parsing algorithm is proportional to O k log k , empirically , the training times scale linearly with k . Peak performance is achieved very early with a slight degradation around k 20 .", "The most likely reason for this phenomenon is that the model is overfitting by ensuring that even unlikely trees are separated from the correct tree proportional to their loss .", "We described a successful new method for training dependency parsers .", "We use simple linear parsing models trained with margin sensitive online training algorithms , achieving state of the art performance with relatively modest training times and no need for pruning heuristics .", "We evaluated the system on both English and Czech data to display state of theart performance without any language specific enhancements .", "Furthermore , the model can be augmented to include features over lexicalized phrase structure parsing decisions to increase dependency accuracy over those parsers .", "We plan on extending our parser in two ways .", "First , we would add labels to dependencies to represent grammatical roles .", "Those labels are very important for using parser output in tasks like information extraction or machine translation .", "Second , we are looking at model extensions to allow nonprojective dependencies , which occur in languages such as Czech , German and Dutch .", "J . Eisner and G . Satta .", "Efficient parsing for bilexical context free grammars and head automaton grammars .", "In Proc .", "ACL .", "Acknowledgments We thank Jan Haji\u02c7c for answering queries on the Prague treebank , and Joakim Nivre for providing the Yamada and Matsumoto 2003 head rules for English that allowed for a direct comparison with our systems .", "This work was supported by NSF ITR grants 0205456 , 0205448 , and 0428193 ."], "summary_lines": ["Online Large-Margin Training Of Dependency Parsers\n", "We present an effective training algorithm for linearly-scored dependency parsers that implements online large-margin multi-class training (Crammer and Singer, 2003; Crammer et al., 2003) on top of efficient parsing techniques for dependency trees (Eisner, 1996).\n", "The trained parsers achieve a competitive dependency accuracy for both English and Czech with no language specific enhancements.\n", "We have achieved parsers with O(n3) time complexity without the grammar constant.\n", "We use the prefix of each word form instead of word form itself as features.\n", "Our dependency parser achieves accuracy as good as Charniak (2000) with speed ten times faster than Collins (1997) and four times faster than Charniak (2000).\n"]}
{"article_lines": ["Findings of the 2012 Workshop on Statistical Machine Translation", "This paper presents the results of the WMT12 shared tasks , which included a translation task , a task for machine translation evaluation metrics , and a task for run time estimation of machine translation quality .", "We conducted a large scale manual evaluation of 103 machine translation systems submitted by 34 teams .", "We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 12 evaluation metrics .", "We introduced a new quality estimation task this year , and evaluated submissions from 11 teams .", "This paper presents the results of the shared tasks of the Workshop on statistical Machine Translation WMT , which was held at NAACL 2012 .", "This workshop builds on six previous WMT workshops Koehn and Monz , 2006 ; Callison Burch et al . , 2007 ; Callison Burch et al . , 2008 ; Callison Burch et al . , 2009 ; Callison Burch et al . , 2010 ; CallisonBurch et al . , 2011 .", "In the past , the workshops have featured a number of shared tasks a translation task between English and other languages , a task for automatic evaluation metrics to predict human judgments of translation quality , and a system combination task to get better translation quality by combining the outputs of multiple translation systems .", "This year we discontinued the system combination task , and introduced a new task in its place ficulty is not uniform across all input types .", "It would thus be useful to have some measure of confidence in the quality of the output , which has potential usefulness in a range of settings , such as deciding whether output needs human post editing or selecting the best translation from outputs from a number of systems .", "This shared task focused on sentence level estimation , and challenged participants to rate the quality of sentences produced by a standard Moses translation system on an EnglishSpanish news corpus in one of two tasks ranking and scoring .", "Predictions were scored against a blind test set manually annotated with relevant quality judgments .", "The primary objectives of WMT are to evaluate the state of the art in machine translation , to disseminate common test sets and public training data with published performance numbers , and to refine evaluation methodologies for machine translation .", "As with previous workshops , all of the data , translations , and collected human judgments are publicly available . 1 We hope these datasets form a valuable resource for research into statistical machine translation , system combination , and automatic evaluation or automatic prediction of translation quality .", "2 Overview of the Shared Translation Task The recurring task of the workshop examines translation between English and four other languages German , Spanish , French , and Czech .", "We created a test set for each language pair by translating newspaper articles .", "We additionally provided training data and two baseline systems .", "The test data for this year s task was created by hiring people to translate news articles that were drawn from a variety of sources from November 15 , 2011 .", "A total of 99 articles were selected , in roughly equal amounts from a variety of Czech , English , French , German , and Spanish news sites 2 Czech Blesk 1 , CTK 1 , E15 1 , den \u0131k 4 , iDNES . cz 3 , iHNed . cz 3 , Ukacko 2 , Zheny 1 French Canoe 3 , Croix 3 , Le Devoir 3 , Les Echos 3 , Equipe 2 , Le Figaro 3 , Liberation 3 Spanish ABC . es 4 , Milenio 4 , Noroeste 4 , Nacion 3 , El Pais 3 , El Periodico 3 , Prensa Libre 3 , El Universal 4 English CNN 3 , Fox News 2 , Los Angeles Times 3 , New York Times 3 , Newsweek 1 , Time 3 , Washington Post 3 German Berliner Kurier 1 , FAZ 3 , Giessener Allgemeine 2 , Morgenpost 3 , Spiegel 3 , Welt 3 The translations were created by the professional translation agency CEET . 3 All of the translations were done directly , and not via an intermediate language .", "Although the translations were done professionally , we observed a number of errors .", "These errors ranged from minor typographical mistakes I was terrible . . . instead of It was terrible . . . to more serious errors of incorrect verb choices and nonsensical constructions .", "An example of the latter is the French sentence translated from German Il a gratt e une planche de b eton , perdit des pi eces du v ehicule .", "He scraped against a concrete crash barrier and lost parts of the car .", "Here , the French verb gratter is incorrect , and the phrase planche de b eton does not make any sense .", "We did not quantify errors , but collected a number of examples during the course of the manual evaluation .", "These errors were present in the data available to all the systems and therefore did not bias the results , but we suggest that next year a manual review of the professionally collected translations be taken prior to releasing the data in order to correct mistakes and provide feedback to the translation agency .", "As in past years we provided parallel corpora to train translation models , monolingual corpora to train language models , and development sets to tune system parameters .", "Some statistics about the training materials are given in Figure 1 .", "We received submissions from 34 groups across 18 institutions .", "The participants are listed in Table 1 .", "We also included two commercial off the shelf MT systems , three online statistical MT systems , and three online rule based MT systems .", "Not all systems supported all language pairs .", "We note that the eight companies that developed these systems did not submit entries themselves , but were instead gathered by translating the test data via their interfaces web or PC . 4 They are therefore anonymized in this paper .", "The data used to construct these systems is not subject to the same constraints as the shared task participants .", "It is possible that part of the reference translations that were taken from online news sites could have been included in the systems models , for instance .", "We therefore categorize all commercial systems as unconstrained when evaluating the results .", "As with past workshops , we placed greater emphasis on the human evaluation than on the automatic evaluation metric scores .", "It is our contention that automatic measures are an imperfect substitute for human assessment of translation quality .", "Therefore , we define the manual evaluation to be primary , and distinct words case insensitive is based on the provided tokenizer . use the human judgments to validate automatic metrics .", "Manual evaluation is time consuming , and it requires a large effort to conduct on the scale of our workshop .", "We distributed the workload across a number of people , beginning with shared task participants and interested volunteers .", "This year , we also opened up the evaluation to non expert annotators hired on Amazon Mechanical Turk CallisonBurch , 2009 .", "To ensure that the Turkers provided high quality annotations , we used controls constructed from the machine translation ranking tasks from prior years .", "Control items were selected such that there was high agreement across the system developers who completed that item .", "In all , there were 229 people who participated in the manual evaluation , with 91 workers putting in more than an hour s worth of effort , and 21 putting in more than four hours .", "After filtering Turker rankings against the controls to discard Turkers who fell below a threshold level of agreement on the control questions , there was a collective total of 336 hours of usable labor .", "This is similar to the total of 361 hours of labor collected for WMT11 .", "We asked annotators to evaluate system outputs by ranking translated sentences relative to each other .", "This was our official determinant of translation quality .", "The total number of judgments collected for each of the language pairs is given in Table 2 .", "Ranking translations relative to each other is a reasonably intuitive task .", "We therefore kept the instructions simple You are shown a source sentence followed by several candidate translations .", "Your task is to rank the translations from best to worst ties are allowed .", "Each screen for this task involved judging translations of three consecutive source segments .", "For each source segment , the annotator was shown the outputs of five submissions , and asked to rank them .", "We refer to each of these as ranking tasks or sometimes blocks .", "Every language task had more than five participating systems up to a maximum of 16 for the German English task .", "Rather than attempting to get a complete ordering over the systems in each ranking task , we instead relied on random selection and a reasonably large sample size to make the comparisons fair .", "We use the collected rank labels to assign each system a score that reflects how highly that system was usually ranked by the annotators .", "The score for some system A reflects how frequently it was judged to be better than other systems .", "Specifically , each block in which A appears includes four implicit pairwise comparisons against the other presented systems .", "A is rewarded once for each of the four comparisons in which A wins , and its score is the number of such winning pairwise comparisons , divided by the total number of non tying pairwise comparisons involving A .", "This scoring metric is different from that used in prior years in two ways .", "First , the score previously included ties between system rankings .", "In that case , the score for A reflected how often A was rated as better than or equal to other systems , and was normalized by all comparisons involving A .", "However , this approach unfairly rewards systems that are similar and likely to be ranked as tied .", "This is problematic since many of the systems use variations of the same underlying decoder Bojar et al . , 2011 .", "A second difference is that this year we no longer include comparisons against reference translations .", "In the past , reference translations were included among the systems to be ranked as controls , and the pairwise comparisons were used in determining the best system .", "However , workers have a very clear preference for reference translations , so including them unduly penalized systems that , through un luck of the draw , were pitted against the references more often .", "These changes are part of a broader discussion of the best way to produce the system ranking , which we discuss at length in Section 4 .", "The system scores are reported in Section 3 . 3 .", "Appendix A provides detailed tables that contain pairwise head to head comparisons between pairs of systems .", "Each year we calculate the inter and intra annotator agreement for the human evaluation , since a reasonable degree of agreement must exist to support our process as a valid evaluation setup .", "To ensure we had enough data to measure agreement , we occasionally showed annotators items that were repeated from previously completed items .", "These repeated items were drawn from ones completed by the same annotator and from different annotators .", "We measured pairwise agreement among annotators using Cohen s kappa coefficient n Cohen , 1960 , which is defined as where P A is the proportion of times that the annotators agree , and P E is the proportion of time that they would agree by chance .", "Note that n is basically a normalized version of P A , one which takes into account how meaningful it is for annotators to agree with each other , by incorporating P E .", "Note also that n has a value of at most 1 and could possibly be negative , with higher rates of agreement resulting in higher n . We calculate P A by examining all pairs of systems which had been judged by two or more judges , and calculating the proportion of time that they agreed that A B , A B , or A B .", "In other words , P A is the empirical , observed rate at which annotators agree , in the context of pairwise comparisons .", "P A is computed similarly for intraannotator agreement i . e . self consistency , but over pairwise comparisons that were annotated more than once by a single annotator .", "As for P E , it should capture the probability that two annotators would agree randomly .", "Therefore Note that each of the three probabilities in P E s definition are squared to reflect the fact that we are considering the chance that two annotators would agree by chance .", "Each of these probabilities is computed empirically , by observing how often annotators actually rank two systems as being tied .", "We note here that this empirical computation is a departure from previous years analyses , where we had assumed that the three categories are equally likely yielding P E 19 19 19 1 .", "We believe that this is a more principled approach , which faithfully reflects the motivation of accounting for P E in the first place .", "Table 3 gives n values for inter annotator and intra annotator agreement .", "These give an indication of how often different judges agree , and how often single judges are consistent for repeated judgments , respectively .", "The exact interpretation of the kappa coefficient is difficult , but according to Landis and Koch 1977 , 0 0 . 2 is slight , 0 . 2 0 . 4 is fair , 0 . 4 0 . 6 is moderate , 0 . 6 0 . 8 is substantial , and 0 . 8 1 . 0 is almost perfect .", "Based on these interpretations , the agreement for sentencelevel ranking is fair for inter annotator and moderate for intra annotator agreement .", "Consistent with previous years , intra annotator agreement is higher than inter annotator agreement , except for English Czech .", "An important difference from last year is that the evaluations were not constrained only to workshop participants , but were made available to all Turkers .", "The workshop participants were trusted to complete the tasks in good faith , and we have multiple years of data establishing general levels of inter and intra annotator agreement .", "Their HITs were unpaid , and access was limited with the use of a qualification .", "The Turkers completed paid tasks , and we used controls to filter out fraudulent and unconscientious workers .", "Agreement rates vary widely across languages .", "For inter annotator agreements , the range is 0 . 176 to 0 . 336 , while intra annotator agreement ranges from 0 . 279 to 0 . 648 .", "We note in particular the low agreement rates among judgments in the English Spanish task , which is reflected in the relative lack of statistical significance Table 4 .", "The agreement rates for this year were somewhat lower than last year .", "We used the results of the manual evaluation to analyze the translation quality of the different systems that were submitted to the workshop .", "In our analysis , we aimed to address the following questions Table 4 shows the system ranking for each of the translation tasks .", "For each language pair , we define a system as winning if no other system was found statistically significantly better using the Sign Test , at p 0 . 10 .", "In some cases , multiple systems are listed as winners , either due to a large number of participants or a low number of judgments per system pair , both of which are factors that make it difficult to achieve statistical significance .", "As in prior years , unconstrained online systems A and B are among the best for many tasks , with a few notable exceptions .", "CU DEPFIX , which postprocesses the output of ONLINE B , was judged as the best system for English Czech .", "For the FrenchEnglish and English French tasks , constrained systems came out on top , with LIMSI appearing both times .", "Consistent with prior years , the rule based systems performed very well on the English German task .", "A rule based system also had a good showing for English Spanish , but not really anywhere else .", "Among the systems competing in all tasks , no single system consistently appeared among the top entrants .", "Participants that competed in all tasks tended to fair worse , with the exception of UEDIN .", "Additionally , KIT appeared in four tasks and was a constrained winner each time .", "Last year one of the long papers published at WMT criticized our method for compiling the overall ranking for systems in the translation task Bojar et al . , 2011 .", "This year another paper shows some additional potential inconsistencies in the rankings Lopez , 2012 .", "In this section we delve into a detailed analysis of a variety of methods that use the human evaluation to create an overall ranking of systems .", "In the human evaluation , we collect ranking judgments for output from five systems at a time .", "We in5 2 4 pairwise judgments over terpret them as 10 systems and use these to analyze how each system faired compared against each of the others .", "Not all pairwise comparisons detect statistical significantly superior quality of either system , and we note this accordingly .", "It is desirable to additionally produce an overall ranking .", "In the past evaluation campaigns , we used two different methods to obtain such a ranking , and this year we use yet another one .", "In this section , we discuss each of these overall ranking methods and a few more .", "In the first human evaluation , we use fluency and adequacy judgments on a scale from 1 to 5 Koehn and Monz , 2006 .", "We normalized the scores on a per sentence basis , thus converting them to a relative ranking in a 5 system comparison .", "We listed systems by the average of these scores over all sentences , in which they were judged .", "We did not report ranks , but rank ranges .", "To give an example if a system scored neither statistically significantly better nor statistically significantly worse than 3 other systems , we assign it the rank range 1 4 .", "The given evidence is not sufficient to rank it exactly , but it does rank somewhere in the top 4 .", "In subsequent years , we did not continue the reporting of rank ranges although they can be obtained by examining the pairwise comparison tables , but we continued to report systems as winners whenever there was not statistically significantly outperformed by any other system .", "In the following years Callison Burch et al . , 2007 ; Callison Burch et al . , 2008 ; Callison Burch et al . , 2009 ; Callison Burch et al . , 2010 ; Callison Burch et al . , 2011 , we abandoned the idea of using fluency and adequacy judgments , since they showed to be less reliable than simple ranking of system translations .", "We also started to interpret the 5 system comparison as a set of pairwise comparisons .", "Systems were then ranked by the ratio of how often they were ranked better or equal to any of the other systems .", "Given a set J of sentence level judgments s1 , s2 , c where s1 E S and s2 E S are two systhen we can count the total number of wins and ties of a system s as and rank systems by the ratio This ratio was used for the official rankings over the last five years .", "Bojar et al . 2011 present a persuasive argument that our ranking scheme is biased towards systems that are similar to many other systems .", "Given that most of the systems are based on phrase based models trained on the same training data , this is indeed a valid concern .", "They suggest ignoring ties , and using as ranking score instead the following ratio Lopez 2012 , in this volume argues against using aggregate statistics over a set of very diverse judgments .", "Instead , a ranking that has the least number of pairwise ranking violations is said to be preferred .", "If we define the number of pairwise wins as then we define a count function for pairwise order violations as ranking of three systems as Given a bijective ranking function R s i with the codomain of consecutive integers starting at 1 , the total number of pairwise ranking violations is defined as Finding the optimal ranking R that minimizes this score is not trivial , but given the number of systems involved in this evaluation campaign , it is quite manageable .", "We now introduce a variant to Lopez s ranking method .", "We motivate it first .", "Consider the following scenario Since this constitutes a circle , there are three rankings with the minimum number of 20 violation ABC , BCA , CAB .", "However , we may want to take the ratio of wins and losses for each pairwise ranking into account .", "Using maximum likelihood estimation , we can define the probability that system s1 is better than system s2 on a randomly drawn sentence as This function scores the three rankings in the example above as follows One disadvantage of this and the previous ranking method is that they do not take advantage of all available evidence .", "Consider the example Here , system A is clearly ahead , but how about B and C ?", "They are tied in their pairwise comparison .", "So , both ABC and ACB have no pairwise ranking violations and their most probable ranking score , as defined above , is the same .", "B is clearly worse than A , but C has a fighting chance , and this should be reflected in the ranking .", "The following two overall ranking methods overcome this problem .", "The sports world is accustomed to the problem of finding a ranking of sports teams , but being only able to have pairwise competitions think basketball or football .", "One strategy is to stage playoffs .", "Let s say there are 4 systems A , B , C , and D . As in well known play off fashion , they are first seeded .", "In our case , this happens randomly , say , 1 A , 2 B , 3 C , 4 D for simplicity s sake .", "First round A plays against D , B plays against C . How do they play ?", "We randomly select a sentence on which they were compared no ties .", "If A is better according to human judgment than D , then A wins .", "Let s say , A wins against D , and B loses against C . This leads us to the final A against C and the 3rd place game D against B , in which , say , A and D win .", "The resulting final ranking is ACDB .", "We repeat this a million times with a different random seeding every time , and compute the average rank , which is then used for overall ranking .", "In European national football competitions , each team plays against each other team , and at the end the number of wins decides the rankings . 6 We can simulate this type of tournament as well with Monte Carlo methods .", "However , in the limit , each team will be on average ranked based on its expected number of wins in the competition .", "We can compute the expected number of wins straightforward as j Note that this is very similar to Bojar s method of ranking systems , with one additional and important twist .", "We can rewrite Equation 4 , the variant that ignores ties , as The difference is that the new overall ranking method normalizes the win ratios per pairwise ranking .", "And this makes sense , since it overcomes one problem with our traditional and Bojar s ranking method .", "Previously , some systems were put at an disadvantage , if they are compared more frequently against good systems than against bad systems .", "This could happen , if participants were not allowed to rank their own systems a constraint we enforced in the past , but no longer .", "This was noticed by judges a few years ago , when we had instant reporting of rankings during the evaluation period .", "If you have one of the best systems and carry out a lot of human judgments , then competitors systems will creep up higher , since they are not compared against your own very good system anymore , but more frequently against bad systems .", "Table 5 shows the different rankings for English German , a rather typical example .", "The table displays the ranking of the systems according to five different methods , alongside with system scores according to the ranking method the win ratio Bojar , the average rank MC Playoffs , and the expected win ratio Expected Wins .", "For the latter , we performed bootstrap resampling and computed rank ranges that lie in a 95 confidence interval .", "You can find the tables for the other language pairs in the annex .", "The win based methods Bojar , MC Playoffs , Expected Wins give very similar rankings exhibiting mostly just the occasional pairwise flip or for many language pairs the ranking is identical .", "The same is true for the two methods based on pairwise rankings Lopez , Most Probable .", "However , the two types of ranking lead to significantly different outcomes .", "For instance , the win based methods are pretty sure that ONLINE B and RBMT 3 are the two top performers .", "Bootstrap resampling of rankings according to Expected Wins ranking draws a clear line between them and the rest .", "However , Lopez s method ranks RBMT 4 first .", "Why ?", "In direct comparison of the three systems , RBMT 4 beats statistically insignificantly ONLINE B 45 wins against 42 wins and essentially ties with RBMT 3 41 wins against 41 wins ONLINE B beats RBMT 3 49 35 , p G 0 . 01 .", "We use Bojar s method as our official method for ranking in Table 4 and as the human judgments that we used when calculating how well automatic evaluation metrics correlate with human judgments .", "In general , there are not enough judgments to rank systems unambiguously .", "How many judgments do we need ?", "We may extrapolate this number from the number of judgments we have .", "Figure 2 provides some hints .", "The outlier is Czech English , for which only 6 systems were submitted and we can separate them almost completely even at p level 0 . 01 .", "For all the other language pairs , we can only draw for around 40 of the pairwise comparisons conclusions with that level of statistical significance .", "Since the plots also contains the ratio of significant conclusions when sub sampling the number of judgments , we obtain curves with a clear upward slope .", "For English Czech , for which we were able to collect much more judgments , we can draw over 60 significant conclusions .", "The curve for this language pair does not look much different than the other languages , suggesting that doubling the number of judgments should allow similar levels for them as well .", "In addition to allowing us to analyze the translation quality of different systems , the data gathered during the manual evaluation is useful for validating automatic evaluation metrics .", "Table 6 lists the participants in this task , along with their metrics .", "A total of 12 metrics and their variants were submitted to the metrics task by 8 research groups .", "We provided BLEU and TER scores as baselines .", "We asked metrics developers to score the outputs of the machine translation systems and system combinations at the system level and at the segmentlevel .", "The system level metrics scores are given in the Appendix in Tables 29 36 .", "The main goal of the metrics shared task is not to score the systems , but instead to validate the use of automatic metrics by measuring how strongly they correlate with human judgments .", "We used the human judgments collected during the manual evaluation for the translation task and the system combination task to calculate how well metrics correlate at system level and at the segment level .", "We measured the correlation of the automatic metrics with the human judgments of translation quality at the system level using Spearman s rank correlation coefficient \u03c1 .", "We converted the raw scores assigned to each system into ranks .", "We assigned a human ranking to the systems based on the percent of time that their translations were judged to be better than the translations of any other system in the manual evaluation Equation 4 .", "When there are no ties , \u03c1 can be calculated using the simplified equation System level correlation for translations into English where di is the difference between the rank for systemi and n is the number of systems .", "The possible values of p range between 1 where all systems are ranked in the same order and 1 where the systems are ranked in the reverse order .", "Thus an automatic evaluation metric with a higher absolute value for p is making predictions that are more similar to the human judgments than an automatic evaluation metric with a lower absolute p . The system level correlations are shown in Table 7 for translations into English , and Table 8 out of English , sorted by average correlation across the language pairs .", "The highest correlation for each language pair and the highest overall average are bolded .", "Once again this year , many of the metrics had stronger correlation with human judgments than BLEU .", "The metrics that had the strongest correlation this year were SEMPOS for the into English direction and SIMPBLEU for the out of English direction .", "We measured the metrics segment level scores with the human rankings using Kendall s tau rank correSegment level correlation for translations into English lation coefficient .", "We calculated Kendall s tau as num concordant pairs num discordant pairs T total pairs where a concordant pair is a pair of two translations of the same segment in which the ranks calculated from the same human ranking task and from the corresponding metric scores agree ; in a discordant pair , they disagree .", "In order to account for accuracy vs . error based metrics correctly , counts of concordant vs . discordant pairs were calculated specific to these two metric types .", "The possible values of T range between 1 where all pairs are concordant and 1 where all pairs are discordant .", "Thus an automatic evaluation metric with a higher value for T is making predictions that are more similar to the human judgments than an automatic evaluation metric with a lower T . We did not include cases where the human ranking was tied for two systems .", "As the metrics produce absolute scores , compared to five relative ranks in the human assessment , it would be potentially unfair to the metric to count a slightly different metric score as discordant with a tie in the relative human rankings .", "A tie in automatic metric rank for two translations was counted as discordant with two corresponding non tied human judgments .", "The correlations are shown in Table 9 for translations into English , and Table 10 out of English , sorted by average correlation across the four language pairs .", "The highest correlation for each language pair and the highest overall average are bolded .", "For the into English direction SPEDE and METEOR tied for the highest segment level correlation .", "METEOR performed the best for the out of English direction , with AMBER doing admirably well in both the into and the out of English directions .", "Quality estimation aims to provide a quality indicator for machine translated sentences at various granularity levels .", "It differs from MT evaluation , because quality estimation techniques do not rely on reference translations .", "Instead , quality estimation is generally addressed using machine learning techniques to predict quality scores .", "Potential applications of quality estimation include This shared task provides a first common ground for development and comparison of quality estimation systems , focusing on sentence level estimation .", "It provides training and test datasets , along with evaluation metrics and a baseline system .", "The goals of this shared task are The task provides datasets for a single language pair , text domain and MT system English Spanish news texts produced by a phrase based SMT system Moses trained on Europarl and News Commentaries corpora provided in the WMT10 translation task .", "As training data , translations were manually annotated for quality in terms of post editing effort 1 5 scores and were provided together with their source sentences , reference translations , and post edited translations Section 6 . 1 .", "The sharedtask consisted on automatically producing qualityestimations for a blind test set , where English source sentences and their MT translations were used as inputs .", "Hidden and subsequently publicly released manual effort annotations of those translations obtained in the same fashion as for the training data were used as reference labels to evaluate the performance of the participating systems Section 6 . 1 .", "Participants also had full access to the translation engine related resources Section 6 . 1 and could use any additional external resources .", "We have also provided a software package to extract baseline quality estimation features Section 6 . 3 .", "Participants could submit up to two systems for two variations of the task ranking , where participants submit a ranking of translations no ties allowed , without necessarily giving any explicit scores for translations , and scoring , where participants submit a score for each sentence in the 1 , 5 range .", "Each of these subtasks is evaluated using specific metrics Section 6 . 2 .", "The training data used was selected from data available from previous WMT shared tasks for machine translation a subset of the WMT10 English Spanish test set , and a subset of the WMT09 English Spanish test set , for a total of 1832 sentences .", "The training data consists of the following resources The guidelines used by the PE effort judges to assign scores 1 5 for each of the source , MT output , PE output triplets are the following 1 The MT output is incomprehensible , with little or no information transferred accurately .", "It cannot be edited , needs to be translated from scratch .", "Providing reliable effort estimates turned out to be a difficult task for the PE effort judges , even in the current set up with post edited outputs available for consultation .", "To eliminate some of the noise from these judgments , we performed an intermediate cleaning step , in which we eliminated the sentences for which the difference between the maximum score and the minimum score assigned between the three judges was 1 .", "We started the data creation process from a total of 2000 sentences for the training set , and the final 1832 sentences we selected as training data were the ones that passed through this intermediate cleaning step .", "Besides score disagreement , we noticed another trend on the human judgements of PE effort .", "Some judges tend to give more moderate scores in the middle of available range , while others like to commit also to scores that are more in the extremes of the available range .", "Since the quality estimation task would be negatively influenced by having most of the scores in the middle of the range , we have chosen to compute the final effort scores as an weighted average between the three PE effort scores , with more weight given to the judges with higher standard deviation from their own mean score .", "We have used weights 3 , 2 , and 1 for the three PE effort judges according to this criterion .", "There is an additional advantage resulting from this weighted average score instead of obtaining average numbers only at values x . 0 , x . 33 , and x . 66 for unweighted average 7 , the weighted averages are spread more evenly in the range 1 , 5 .", "A few variations of the training data were provided , including version with cases restored and a version detokenized .", "In addition , engine internal information from Moses such as phrase and word alignments , detailed model scores , etc .", "parameter trace , n best lists and stack information from the search graph as a word graph parameter outputword graph as produced by the Moses engine were provided .", "The rationale behind releasing this engineinternal data was to make it possible for this sharedtask to address quality estimation using a glass box approach , that is , making use of information from the internal workings of the MT engine .", "The test data was a subset of the WMT12 EnglishSpanish test set , consisting of 442 sentences .", "The test data consists of the following files The first two files were the input for the qualityestimation shared task participating systems .", "Since the Moses engine used to create the MT outputs was the same as the one used for generating the training data , the engine internal resources are the same as the ones we released as part of the training data package .", "The effort scores were released after the participants submitted their shared task submission , and were solely used to evaluate the submissions according to the established metrics .", "The guidelines used by the PE effort judges to assign 1 5 scores were the same as the ones used for creating the training data .", "We have used the same criteria to ensure the consistency of the human judgments .", "The initial set of candidates consisted of 604 sentences , of which only 442 met this criteria .", "The final scores used as goldvalues have been obtained using the same weightedaverage scheme as for the training data .", "In addition to the training and test materials , we made several additional resources that were used for the baseline QE system and or the SMT system that produced the training and test datasets For the ranking task , we defined a novel metric that provides some advantages over a more traditional ranking metrics like Spearman correlation .", "Our metric , called DeltaAvg , assumes that the reference test set has a number associated with each entry that represents its extrinsic value .", "For instance , using the effort scale we described in Section 6 . 1 , we associate a value between 1 and 5 with each sentence , representing the quality of that sentence .", "Given these values , our metric does not need an explicit reference ranking , the way the Spearman ranking correlation does . 9 The goal of the DeltaAvg metric is to measure how valuable a proposed ranking which we call a hypothesis ranking is according to the extrinsic values associated with the test entries .", "We first define a parameterized version of this metric , called DeltaAvg n .", "The following notations are used for a given entry sentence s , V s represents the function that associates an extrinsic value to that entry ; we extend this notation to a set S , with V S representing the average of all V s , s S . Intuitively , V S is a quantitative measure of the quality of the set S , as induced by the extrinsic values associated with the entries in S . For a set of ranked entries S and a parameter n , we denote by S1 the first quantile of set S the highest ranked entries , S2 the second quantile , and so on , for n quantiles of equal sizes . 10 We also use the notation Si , vk i Sk .", "Using these notations , we define quantile top half S1 and the overall quality represented by V S .", "For n 3 , DeltaAvg 3 V S1 V S1 , 2 2 V S V S1 V S V S1 , 2 V S 2 , hence it measures an average difference across two cases between the quality of the top quantile top third and the overall quality , and between the quality of the top two quantiles S1 S2 , top two thirds and the overall quality .", "In general , DeltaAvg n measures an average difference in quality across n 1 cases , with each case measuring the impact in quality of adding an additional quantile , from top to bottom .", "Finally , we define where N S 2 .", "As before , we write DeltaAvg for DeltaAvgV when the valuation function V is clear from the context .", "The DeltaAvg metric is an average across all DeltaAvg n values , for those n values for which the resulting quantiles have at least 2 entries no singleton quantiles .", "The DeltaAvg metric has some important properties that are desired for a ranking metric see Section 6 . 4 for the results of the shared task that substantiate these claims DeltaAvgV n En 1 V S 14 it measures the quality of a hypothesis rankk 1 V S1 , k ing from an extrinsic perspective as offered by n 1 function V When the valuation function V is clear from the context , we write DeltaAvg n for DeltaAvgV n .", "The parameter n represents the number of quantiles we want to split the set S into .", "For instance , n 2 gives DeltaAvg 2 V S1 V S , hence it measures the difference between the quality of the top In the rest of this paper , we present results for DeltaAvg using as valuation function V the PostEditing effort scores , as defined in Section 6 . 1 .", "We also report the results of the ranking task using the more traditional Spearman correlation .", "For the scoring task , we use two metrics that have been traditionally used for measuring performance for regression tasks Mean Absolute Error MAE as a primary metric , and Root of Mean Squared Error RMSE as a secondary metric .", "For a given test set 5 with entries si71 i 5 , we denote by H si the proposed score for entry si hypothesis , and by V si the reference value for entry si gold standard value .", "We formally define our metrics as follows where N 5 .", "Both these metrics are nonparametric , automatic and deterministic and therefore consistent , and extrinsically interpretable .", "For instance , a MAE value of 0 . 5 means that , on average , the absolute difference between the hypothesized score and the reference score value is 0 . 5 .", "The interpretation of RMSE is similar , with the difference that RMSE penalizes larger errors more via the square function .", "Eleven teams listed in Table 11 submitted one or more systems to the shared task , with most teams submitting for both ranking and scoring subtasks .", "Each team was allowed up to two submissions for each subtask .", "In the descriptions below participation in the ranking is denoted R and scoring is denoted S .", "Baseline system R , S the baseline system used the feature extraction software also provided to all participants .", "It analyzed the source and translation files and the SMT training corpus to extract the following 17 system independent features that were found to be relevant in previous work Specia et al . , 2009 These features are used to train a Support Vector Machine SVM regression algorithm using a radial basis function kernel with the LIBSVM package Chang and Lin , 2011 .", "They , E and C parameters were optimized using a grid search and 5 fold cross validation on the training set .", "We note that although the system is referred to as a baseline , it is in fact a strong system .", "Although it is simple it has proved to be robust across a range of language pairs , MT systems , and text domains .", "It is a simpler variant of the system used in Specia , 2011 .", "The rationale behind having such a strong baseline was to push systems to exploit alternative sources of information and combination learning approaches .", "SDLLW R , S Both systems use 3 sets of features the 17 baseline features , 8 systemdependent features from the decoder logs of Moses , and 20 features developed internally .", "Some of these features made use of additional data and or resources , such as a secondary MT system that was used as pseudo reference for the hypothesis , and POS taggers for both languages .", "Feature selection algorithms were used to select subsets of features that directly optimize the metrics used in the task .", "System SDLLW M5PbestAvgDelta uses a resulting 15 feature set optimized towards the AvgDelta metric .", "It employs an M5P model to learn a decision tree with only two linear equations .", "System SDLLW SVM uses a 20 feature set and an SVM epsilon regression model with radial basis function kernel with parameters C , gamma , and epsilon tuned on a development set 305 training instances .", "The model was trained with 10 fold cross validation and the tuning process was restarted several times using different starting points and step sizes to avoid overfitting .", "The final model was selected based on its performance on the development set and the number of support vectors .", "UU R , S System UU best uses the 17 baseline features , plus 82 features from Hardmeier 2011 with some redundancy and some overlap with baseline features , and constituency trees over input sentences generated by the Stanford parser and dependency trees over both input and output sentences generated by the MaltParser .", "System UU bltk uses only the 17 baseline features plus constituency and dependency trees as above .", "The machine learning component in both cases is SVM regression SVMlight software .", "For the ranking task , the ranking induced by the regression output is used .", "The system uses polynomial kernels of degree 2 UU best and 3 UU bltk as well as two different types of tree kernels for constituency and dependency trees , respectively .", "The SVM margin error trade off , the mixture proportion between tree kernels and polynomial kernels and the degree of the polynomial kernels were optimised using grid search with 5 fold cross validation over the training set .", "TCD R , S TCD M5P resources only uses only the baseline features , while TCD M5Pall uses the baseline and additional features .", "A number of metrics used as features in TCD M5P all were proposed which work in the following way given a sentence to evaluate source sentence for complexity or target sentence for fluency , it is compared against some reference data using similarity measures various metrics which compare distributions of n grams .", "The training data was used as reference , along with the Google ngrams dataset .", "Several learning methods were tested using Weka on the training data 10fold cross validation .", "The system submission uses the M5P regression with decision trees algorithm which performed best .", "Contrary to what had been observed on the training data using cross validation , TCD M5P resourcesonly performs better than TCD M5P all on the test data .", "PRHLT UPV R , S The system addresses the task using a regression algorithm with 475 features , including the 17 the baseline features .", "Most of the features are defined as word scores .", "Among them , the features obtained form a smoothed naive Bayes classifier have shown to be particularly interesting .", "Different methods to combine word level scores into sentencelevel features were investigated .", "For model building , SVM regression was used .", "Given the large number of features , the training data provided as part of the task was insufficient yielding unstable systems with not so good performance .", "Different feature selection methods were implemented to determine a subset of relevant features .", "The final submission used these relevant features to train an SVM system whose parameters were optimized with respect to the final evaluation metrics .", "UEDIN R , S The system uses the baseline features along with some additional features binary features for named entities in source using Stanford NER Tagger ; binary indicators for occurrence of quotes or parenthetical segments , words in upper case and numbers ; geometric mean of target word probabilities and probability of worst scoring word under a Discriminative Word Lexicon Model ; Sparse Neural Network directly mapping from source to target using the vector space model with source and target side either filtered to relevant words or hashed to reduce dimensionality ; number of times at least a 3 gram is seen normalized by sentence length ; and Levenshtein distance of either source or translation to closest entry of the SMT training corpus on word or character level .", "An ensemble of neural networks optimized for RMSE was used for prediction scoring and ranking .", "The contribution of new features was tested by adding them to the baseline features using 5 fold cross validation .", "Most features did not result in any improvement over the baseline .", "The final submission was a combination of all feature sets that showed improvement .", "SJTU R , S The task is treated as a regression problem using the epsilon SVM method .", "All features are extracted from the official data , involving no external NLP tools resources .", "Most of them come from the phrase table , decoding data and SMT training data .", "The focus is on special word relations and special phrase patterns , thus several feature templates on this topic are extracted .", "Since the training data is not large enough to assign weights to all features , methods for estimating common strings or sequences of words are used .", "The training data is divided in 3 4 for training and 1 4 for development to filter ineffective features .", "Besides the baseline features , the final submission contains 18 feature templates and about 4 million features in total .", "WLV SHEF R , S The systems integrates novel linguistic features from the source and target texts in an attempt to overcome the limitations of existing shallow features for quality estimation .", "These linguistically informed features include part of speech information , phrase constituency , subject verb agreement and target lexicon analysis , which are extracted using parsers , corpora and auxiliary resources .", "Systems are built using epsilon SVM regression with parameters optimised using 5 fold crossvalidation on the training set and two different feature sets WLV SHEF BL uses the 17 baseline features plus 70 linguistically inspired features , while WLV SHEF FS uses a larger set of 70 linguistic plus 77 shallow features including the baseline .", "Although results indicate that the models fall slightly below the baseline , further analysis shows that linguistic information is indeed informative and complementary to shallow indicators .", "DFKI R , S DFKI morphPOSibm1LM R is a simple linear interpolation of POS 6 gram language model scores , morpheme 6 gram language model scores , IBM 1 scores both direct and inverse for POS 4 grams and for morphemes .", "The parallel News corpora from WMT10 is used as extra data to train the language model and the IBM 1 model .", "DFKI cfsplsreg and DFKI grcfs mars S use a collection of 264 features generated containing the baseline features and additional resources .", "Numerous methods of feature selection were tested using 10 fold cross validation on the training data , reducing these to 23 feature sets .", "Several regression and discretized classification algorithms were employed to train prediction models .", "The best performing models included features derived from PCFG parsing , language quality checking and LM scoring , of both source and target , besides features from the SMT search graph and a few baseline features .", "DFKI cfs plsreg uses a Best First correlation based feature selection technique , trained with Partial Least Squares Regression , while DFKI grcfs mars uses a Greedy Stepwise correlation based feature selection technique , trained with multivariate adaptive regression splines .", "DCU SYMC R , S Systems are based on a classification approach using a set of features that includes the baseline features .", "The manually assigned quality scores provided for each MT output in the training set were rounded in order to apply classification algorithms on a limited set of classes integer values from 1 to 5 .", "Three classifiers were combined by averaging the predicted classes SVM using sequential minimal optimization and RBF kernel parameters optimized by grid search , Naive Bayes and Random Forest .", "DCU SYMC constrained is based on a set of 70 features derived only from the data provided for the task .", "These include a set of features which attempt to model translation adequacy using a bilingual topic model built using Latent Dirichlet Allocation .", "DCUSYMC unconstrained is based on 308 features including the constrained ones and others extracted using external tools grammaticality features extracted from the source segments using the TreeTagger part of speech tagger , an English precision grammar , the XLE parser and the Brown re ranking parser and features based on part of speech tag counts extracted from the MT output using a Spanish TreeTagger model .", "Loria S Several numerical or boolean features are computed from the source and target sentences and used to train an SVM regression algorithm with linear Loria SVMlinear and radial basis function Loria SVMrbf as kernel .", "For the radial basis function , a grid search is performed to optimise the parameter y .", "The official submission use the baseline features and a number of features proposed in previous work Raybaud et al . , 2011 , amounting to 66 features .", "A feature selection algorithm is used in order to remove non informative features .", "No additional data other than that provided for the shared task is considered .", "The training data is split into a training part 1000 sentences and a development part 832 sentences to learn the regression model and optimise the parameters of the regression and for feature selection .", "UPC R , S The systems use several features on top of the baseline features .", "These are mostly based on different language models estimated on reference and automatic Spanish translations of the news v7 corpus .", "The automatic translations are generated by the system used for the shared task .", "N gram LMs are estimated on word forms , POS tags , stop words interleaved by POS tags , stop word patterns , plus variants in which the POS tags are replaced with the stem or root of each target word .", "The POS tags on the target side are obtained by projecting source side annotations via automatic alignments .", "The resulting features are the perplexity of each additional language model , according to the two translations , and the ratio between the two perplexities .", "Additionally , features that estimate the likelihood of the projection of dependency parses on the two translations are encoded .", "For learning , linear SVM regression is used .", "Optimization was done via 5 fold cross validation on a development data .", "Features are encoded by means of their z scores , i . e . how many standard deviations the observed value is above or below the mean .", "A variant of the system , UPC 2 uses an option of SVMLight that removes inconsistent points from the training set and retrains the model until convergence .", "Here we give the official results for the ranking and scoring subtasks followed by a discussion that highlights the main findings of the task .", "Table 12 gives the results for the ranking subtask .", "The table is sorted from best to worse using the DeltaAvg metric scores Equation 15 as primary key and the Spearman correlation scores as secondary key .", "The winning submissions for the ranking subtask are SDLLW s M5PbestDeltaAvg and SVM entries , which have DeltaAvg scores of 0 . 63 and 0 . 61 , respectively .", "The difference with respect to all the other submissions is statistically significant at p 0 . 05 , using pairwise bootstrap resampling Koehn , 2004 .", "The state of the art baseline system has a DeltaAvg score of 0 . 55 Spearman rank correlation of 0 . 58 .", "Five other submissions have performances that are not different from the baseline at a statistically significant level p 0 . 05 , as shown by the gray area in the middle of Table 12 .", "Three submissions scored higher than the baseline system at p 0 . 05 systems above the middle gray area , which indicates that this shared task succeeded in pushing the state of the art performance to new levels .", "The range of performance for the submissions in the ranking task varies from a DeltaAvg of 0 . 65 down to a DeltaAvg of 0 . 15 with Spearman values varying from 0 . 64 down to 0 . 19 .", "In addition to the performance of the official submission , we report here results obtained by various oracle methods .", "The oracle methods make use of various metrics that are associated in a oracle manner to the test input the gold label Effort metric for Oracle Effort , the HTER metric computed against the post edited translations as reference for Oracle HTER , and the BLEU metric computed against the same post edited translations as reference for Oracle H BLEU . 11 The Oracle Effort DeltaAvg score of 0 . 95 gives an upperbound in terms of DeltaAvg for the test set used in this evaluation .", "It basically indicates that , for this set , 11We use the H BLEU notation to underscore the use of Post Edited translations as reference , as opposed to using references that are not the product of a Post Editing process , as for the traditional BLEU metric . the difference in PE effort between the top quality quantiles and the overall quality is 0 . 95 on average .", "We would like to emphasize here that the DeltaAvg metric does not have any a priori range for its values .", "The upperbound , for instance , is test dependent , and therefore an Oracle Effort score is useful for understanding the performance level of real systemsubmissions .", "The Oracle HTER DeltaAvg score of 0 . 77 is a more realistic upperbound for the current set .", "Since the HTER metric is considered a good approximation for the effort required in postediting , ranking the test set based on the HTER scores from lowest HTER to highest HTER provides a good oracle comparison point .", "The oracle based on H BLEU gives a lower DeltaAvg score , which can be interpreted to mean that the BLEU metric provides a lower correlation to post editing effort compared to HTER .", "We also note here that there is room for improvement between the highestscoring submission at DeltaAvg 0 . 63 and the Oracle HTER DeltaAvg score of 0 . 77 .", "We are not sure if this difference can be bridged completely , but having measured a quantitative difference between the current best performance and a realistic upperbound is an important achievement of this shared task .", "The results for the scoring task are presented in Table 13 , sorted from best to worse by using the MAE metric scores Equation 16 as primary key and the RMSE metric scores Equation 17 as secondary key .", "The winning submission is SDLLW s M5PbestDeltaAvg , with an MAE of 0 . 61 and an RMSE of 0 . 75 the difference with respect to all the other submissions is statistically significant at p 0 . 05 , using pairwise bootstrap resampling Koehn , 2004 .", "The strong , state of the art quality estimation baseline system is measured to have an MAE of 0 . 69 and RMSE of 0 . 82 , with six other submissions having performances that are not different from the baseline at a statisticallysignificant level p 0 . 05 , as shown by the gray area in the middle of Table 13 .", "Five submissions scored higher than the baseline system at p 0 . 05 systems above the middle gray area , which indicates that this shared task also succeeded in pushing the state of the art performance to new levels in terms of absolute scoring .", "The range of performance for the submissions in the scoring task varies from an MAE of 0 . 61 up to an MAE of 0 . 87 the outlier MAE of 2 . 09 is reportedly due to bugs .", "We also calculate scoring Oracles using the methods used for the ranking Oracles .", "The difference is that the HTER and H BLEU oracles need a way of mapping their scores which are usually in the 0 , 100 range into the 1 , 5 range .", "For the comparison here , we did the mapping by excluding the 5 top and bottom outlier scores , and then linearly mapping the remaining range into the 1 . 5 , 5 range .", "The Oracle Effort scores are not very indicative in this case .", "However , the Oracle HTER MAE score of 0 . 56 is a somewhat realistic lowerbound for the current set although the score could be decreased by a smarter mapping from the HTER range to the Effort range .", "We argue that since the HTER metric is considered a good approximation for the effort required in post editing , effort like scores derived from the HTER score provide a good way to compute oracle scores in a deterministic manner .", "Note that again the oracle based on H BLEU gives a worse MAE score at 0 . 61 , which support the interpretation that the H BLEU metric provides a lower correlation to post editing effort compared to H TER .", "Overall , we consider the MAE values for these HTER and H BLEU based oracles to indicate high error margins .", "Most notably the performance of the best system gets the same MAE score as the H BLEU oracle , at 0 . 61 MAE .", "We take this to mean that the scoring task is more difficult compared to the ranking task , since even oracle based solutions get high error scores .", "When looking back at the goals that we identified for this shared task , most of them have been successfully accomplished .", "In addition , we have achieved additional ones that were not explicitly stated from the beginning .", "In this section , we discuss the accomplishments of this shared task in more detail , starting from the defined goals and beyond .", "Identify new and effective quality indicators The vast majority of the participating systems use external resources in addition to those provided for the task , such as parsers , part of speech taggers , named entity recognizers , etc .", "This has resulted in a wide variety of features being used .", "Many of the novel features have tried to exploit linguisticallyoriented features .", "While some systems did not achieve improvements over the baseline while exploiting such features , others have the UU submissions , for instance , exploiting both constituency and dependency trees .", "Another significant set of features that has been previously overlooked is the feature set of the MT decoder .", "Considering statistical engines , these features are immediately available for quality prediction from the internal trace of the MT decoder in a glass box prediction scenario , and its contribution is significant .", "These features , which reflect the confidence of the SMT system on the translations it produces , have been shown to be complementary to other , system independent black box features .", "For example , the SDLLW submissions incorporate these features , and their feature selection strategy consistently favored this feature set .", "The power of this set of features alone is enough to yield when used with an M5P model outputs that would have been placed 4th in the ranking task and 5th in the scoring task , a remarkable achievement .", "Another interesting feature used by the SDLLW submissions rely on pseudo references , i . e . , translations produced by other MT systems for the same input sentence .", "Identify alternative machine learning techniques Although SVM regression was used to compute the baseline performance , the baseline system provided for the task consisted solely of a software to extract features , as opposed to a model built using the regression algorithm .", "The rationale behind this decision was to encourage participants to experiment with alternative methods for combining different quality indicators .", "This was achieved to a large extent .", "The best performing machine learning techniques were found to be the M5P Regression Trees and the SVM Regression SVR models .", "The merit of the M5P Regression Trees is that it provides compact models that are less prone to overfitting .", "In contrast , the SVR models can easily overfit given the small amount of training data available and the large numbers of features commonly used .", "Indeed , many of the submissions that fell below the baseline performance can blame overfitting for part of their suboptimal performance .", "However , SVR models can achieve high performance through the use of tuning and feature selection techniques to avoid overfitting .", "Structured learning techniques were successfully used by the UU submissions the second best performing team to represent parse trees .", "This seems an interesting direction to encode other sorts of linguistic information about source and translation texts .", "Other interesting learning techniques have been tried , such as Neural Networks , Partial Least Squares Regression , or multivariate adaptive regression splines , but their performance does not suggest they are strong candidates for learning highly performing quality estimation models .", "Test the suitability of evaluation metrics for quality estimation DeltaAvg , our proposed metric for measuring ranking performance , proved suitable for scoring the ranking subtask .", "Its high correlation with the Spearman ranking metric , coupled with its extrinsic interpretability , makes it a preferred choice for future measurements .", "It is also versatile , in the sense that the its valuation function V can change to reflect different extrinsic measures of quality .", "Establish the state of the art performance The results on both the ranking and the scoring subtasks established new state of the art levels on the test set used in this shared task .", "In addition to these levels , the oracle performance numbers also help understand the current performance level , and how much of a gap in performance there still exists .", "Additional data points regarding quality estimation performance are needed to establish how stable this measure of the performance gap is .", "Contrast the performance of regression and ranking techniques Most of the submissions in the ranking task used the results provided by a regression solution submitted for the scoring task to infer the rankings .", "Also , optimizing for ranking performance via a regression solution seems to result in regression models that perform very well , as in the case of the top ranked submission .", "There appear to be significant differences between considering the quality estimation task as a ranking problem versus a scoring problem .", "The rankingbased approach appears to be somewhat simpler and more easily amenable to automatic solutions , and at the same time provides immediate benefits when integrated into larger applications see , for instance , the post editing application described in Specia 2011 .", "The scoring based approach is more difficult , as the high error rate even of oracle based solutions indicates .", "It is also well known from human evaluations of MT outputs that human judges also have a difficult time agreeing on absolute number judgements to translations .", "Our experience in creating the current datasets confirms that , even with highly trained professionals , it is difficult to arrive at consistent judgements .", "We plan to have future investigations on how to achieve more consistent ways of generating absolute number scores that reflect the quality of automated translations .", "As in previous incarnations of this workshop we carried out an extensive manual and automatic evaluation of machine translation performance , and we used the human judgements that we collected to validate automatic metrics of translation quality .", "This year was also the debut of a new quality estimation task , which tries to predict the effort involved in having post editors correct MT output .", "The quality estimation task differs from the metrics task in that it does not involve reference translations .", "As in previous years , all data sets generated by this workshop , including the human judgments , system translations and automatic scores , are publicly available for other researchers to analyze . 12", "This work was supported in parts by the EuroMatrixPlus project funded by the European Commission 7th Framework Programme , the GALE program of the US Defense Advanced Research Projects Agency , Contract No .", "HR0011 06 C 0022 , the US National Science Foundation under grant IIS 0713448 , and the CoSyne project FP7 ICT 4248531 funded by the European Commission .", "The views and findings are the authors alone .", "Thanks for Adam Lopez for discussions about alternative ways of ranking the overall system scores .", "The Quality Estimation shared task organizers thank Wilker Aziz for his help with the SMT models and resources , and Mariano Felice for his help with the system for the extraction of baseline features ."], "summary_lines": ["Findings of the 2012 Workshop on Statistical Machine Translation\n", "This paper presents the results of the WMT12 shared tasks, which included a translation task, a task for machine translation evaluation metrics, and a task for run-time estimation of machine translation quality.\n", "We conducted a large-scale manual evaluation of 103 machine translation systems submitted by 34 teams.\n", "We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 12 evaluation metrics.\n", "We introduced a new quality estimation task this year, and evaluated submissions from 11 teams.\n", "We report for several automatic metrics on the whole WMT12 English-to-Czech dataset.\n"]}
{"article_lines": ["? EMLWLWEMY ? GJU ? ? EMCB ? YBIK 0e ? ? ?", "? ? ? ? ? V ? ? ? ? ? ? ? s ? ? ? 4 ?", "? ? ? ? V ? ? ? ? ? ? ? ? ? ? ? a ? ? ? D ? fl ? a ? ? ? ? ? V ? ? ? ? ?", "? s ? ? ? ? ? 4 ? Z ? ? ? ? ? ? ? ? ? ? s ? ? ?", "? H ? ? ? ? ? Z ? ? ? ? ? ? s ? ! ? Z ?", "? p ? 0 ? ? ? M ? W ? T ? ? ? V ? ? ?", "? B ? ? ? ? V ? ? Z ? V ? ? ? ? ? ? ? W ? T ? Z ? ? ? ? V ?", "? ? ? Z ? ? ? fl ? 5 ? Z ? ? ? V ? ? ? Z ? V ?", "? ? ? V ? ? ? s ? ? ? ? ? V ? ? ? ? ? ? ? ? ? ? ? s ? ? ? ? ? 4 ? Z ? ? fl ? Z ? V ? ? ?", "? ? ? Z ? fl ? D ? ? ? s ? ! ? ? ? D ? ? ? ? ? ? ? ? ? B ? V ? ? ? ?", "? V ? 0 ? ? ? ? ? ? ? D ? ? ? ? ? ? ? ? D ? V ? ? ? ? ?", "? ? ? ? ? ? ? D ? D ? Z ? D ? s ? ? ? ? ? Z ? s ? ? ? V ?", "? ? ? , ? ? ? Z ? Q ? ? ? a ? ? ? ? ? ? ? fl ? ? ?", "? a ? ? ? ? ? Z ? ? ? s ? ? ? V ? 2 ? ! ? D ? ? ? 4 ? B ? ! ?", "? ? ? a ? s ? 4 ? ? ? ? ? D ?", "? ? 2 ? a ? 9 ? V ? ? ? ? 9 ? ? ? ? ? H ?", "? B ? ? ? ? ? 9 ? ? s ? 2 ? ? ? ?", "? ? ? V ? ? 5 ? ? ? ? ? ? ? s ? a ? 9 ?", "? B ? ? ? ? ? 9 ? ? ? ? ? a ? X ? a ? 2 ? ? ? ? Z ? ? ? ? 9 ? ? ?", "? s ? ? ? ? ? ? a ? ? ? ? ? ? s ? 9 ?", "? ? ? ? ? 4 ? Z ? ? ? ? D ? ? ? ?", "? ? ? ? ? ? a ? s ? ? ? ? ? ? ? a ?", "? ? s ? ? ? ? ? ? ? 4 ? Z ? ? ? a ? D ? ? ? ? ? ? ? s ? ? F ? ? ? ? 2 ? ? fiF ? ? ? ? ? ? 2 ?", "? s ? 9 ? T ? Z ? a ? ? ? ? ?", "? a ? Pfi ? ? ? 9 ? ? ? ? ?", "? V ? a ? 9 ? ? ? ? ? ? a ? ?", "? ? ? ? ? s ? a ? 9 ? ? ? Z ? V ?", "? ? ? s ? ? F ? ? ? ? 2 ? ? 0 ? 0 ? a ? ? 9 ? 9 ? ? ? ?", "? 9 ? ? ? Z ? V ? ? ? 2 ? a ? s ? s ? ? ? a ? B ? ? ? s ?", "? V ? C ? ? Z ? 2 ? ? ? 2 ? ? ? B ?", "? ? 2 ? ? ? V ? 9 ? ? ? fl ? ? Z ? ? ? ? ? ? ?", "Question PATTERN How can X be detected ?", "Question FOCUS X biological weapons program TOPIC MODEL stockpile ? ?", "weapons , deliver ? ?", "missiles Topic relations develop ? ?", "program , produce ? ?", "bilogical agents Possible paths of action Predicate ? argument structure PREDICATE detect Arg0 detector Answer 1 Arg1 detected biological weapons program Arg2 instrument ; Answer 2 2 development ?", "? acquisition ?", "? stockpiling ?", "? delivery 1 development ?", "? production ?", "? stockpiling ?", "? a ? ? ? M ? V ? ? ? ? s ? ? ? V ? ? ? a ? 9 ? ? 9 ? , ? a ? s ? ? ? a ? ? ? 2 ? ? ? ? ?", "? a ? s ? M ? a ? ? ? ? ? ? ? ? 9 ?", "? s ? 9 ? ? ? 2 ? a ? s ? s ? ? ? a ? B ?", "? ? ? ? ? ? ? 9 ? ? ? s ? Z ?", "? ? ? ? ? ? ? ? ? 5 ? F ? a ?", "? a ? V ? 9 ? ? ? ? ? ? a ?", "? 4 ? ? s ? 2 ? ? ? s ? 9 ? ? B ?", "fi ? ? ? 9 ? ? ? ? ? ? ? ? ? Z ? ?", "? a ? V ? 9 ? s ? 2 ? ? ?", "? s ? a ? V ? 5 ? ? ? s ? a ?", "? ? ? 2 ? 9 ? ? ? ? ? ? ? ? ? ? a ? F ?", "? ? ? s ? 2 ? ? ? 0 ? ? ? , ? ? D ? 9 ?", "? s ? V ? ? ? s ? ? ? Z ? ? ? ? ? ? ? ? F ? ? ? ?", "? 4 ? ? ? ? ? 9 ? Z ? V ? ? ? fl ?", "? ? 2 ? ? B ? ? ? F ? ? ? ? ? J ? V ?", "? 5 ? 2 ? Z ? 4 ? ? ? ? a ?", "Q3 What kind of nuclear materials were stolen from the Russian navy ?", "FS Q3 What GOODS kind of nuclear materials were PAS Q3 What Arg1 kind of nuclear materials were Predicate stolen target ? Predicate stolen VICTIM from the Russian navy ?", "? 2 ? a ? s ? a ? a ? ? ? ? ? 9 ? 2 ?", "? 9 ? ? ? ? ? 9 ? ? ? ffifi ? ? s ? 9 ? fl ?", "PAS A Q3 Arg1 P1 Russia ? s Pacific Fleet has ArgM ? DIS P1 also ArgM ? TMP P2 in 1 96 , Arg1 P2 approximately 7 kg of HEU Predicate P1 fallen Arg1 P1 prey to nuclear theft ; was ArgM ? ADV P2 reportedly Predicate P2 stolen Arg2 P2 from a naval base Arg3 P2 in Sovetskaya Gavan FS A Q3 VICTIM Russia ? s Pacific Fleet has also fallen prey to GOODS nuclear target ? Predicate P1 theft ; in 1 96 , target ? Predicate P2 stolen VICTIM P2 from a naval base GOODS P2 approximately 7 kg of HEU was reportedly SOURCE P2 in Sovetskaya Gavan 9 ? ?", "? 9 ? V ? ? ? ? ? ? 2 ? ? ?", "? ? Z ? ? ? ? ? ? ? ? a ?", "? V ? a ? 9 ? ? ? ? ? ? a ? Q ? ? ?", "? ? ? ? ? ? 9 ? s ? ? ? Z ? 2 ? ? ? 9 ? ? ? Z ?", "? D ? ! ? 2 ? ? ? ? 9 ? ! ? fl ?", "? 2 ? ? ? ? ? ? a ? ? ? a ? ; ? ? ? Z ? ? ? s ? 9 ? s ? Z ? ? V ? ? ? ?", "? ? ? ? ? ? V ? ? ? ? ? ? ? ? a ?", "? s ? 9 ? ? ? ? V ? 9 ? ? ? fl ? ? ? ? a ? 2 ? ? ? ? 9 ? s ? a ?", "? W5M ? ? ? ? ? ? ? 9 ? ? ? a ? D ? ? V ? ? ? V ? a ? ? a ? ? V ?", "? Z ? V ? ? ? ? ? Z ? ? ? ? ? ? ? ? a ? ? ? ? ? ? ? ? ? ?", "? ? V ? 9 ? ? ? ? ? ? Z ? ? ? ? ?", "? s ? 9 ? ? ? 9 ? ? ? s ? ? ? V ?", "? ? ? ? a ? ? ? ? ? ? ? ? ? ? s ?", "? ? Z ? ? ? a ? D ? ? ? ? ? ?", "? ? 9 ? 5 ? 9 ? 2 ? 4 ? ? s ? 2 ? ?", "3Z ? ? ? ? ? ? a ? T ? ? ?", "? s ? 2 ? ? ? ? 9 ? 2 ? 2 ? a ?", "? ? 9 ? ? ? s ? a ? 9 ? ? ?", "? ? ? ? ? ? F ? 4 ? ? ? ? ? s ? Z ? ? ? ? ? ? ? ? ? F ? B ?", "? a ? 2 ? ? ? ? ? ? ?", "fi ? ? ? ? ? 2 ? ? B ? ? ? ?", "? ? ? ? ? ? ? ? ? ? a ? ? ? ? ? ? ? a ? ? ? ! ? s ? Z ? ? F ? B ? ? ? ? ?", "? M ? V ? s ? ? ? ? ? 0 ? ? ? ? ? B ? ? ? s ? 9 ? ? 2 ? ? fl ? ? ? ? ? ? h ?", "? a ? ? ? ? ? ? ? F ? 4 ? ? ? ? ? ?", "7KG HEU MEANS ? m SOURCE in Sovetskaya Gavan GOODS approx .", "7 KG of HEU VICTIM Russian Navy , Pacific Fleet , Naval Base PERPETRATOR ? x AGENT SOURCE in Sovetskaya Gavan GOODS approx .", "7 KG of HEU A Q3 Russia ? s Pacific Fleet has also fallen prey to nuclear theft ; in 1 96 , approximately 7 kg of HEU was reportedly stolen from a naval base in Sovetskaya Gavan .", "? ? ? ? ! ? 9 ? ? ? ? ? ? ? ? a ? 8 ? ?", "B ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 2 ?", "? ? ? ? ? ? V ? ? ? ? ?", "? ? 9 ? ? ? ? ? ? ? ? ? 9 ? ? ? ? ? s ? ? ? V ?", "? ? ? ? ? ? V ? ? ? ? ?", "? a ? F ? 4 ? 9 ? ? ? ? s ? ? ? V ? ? ? ? ?", "B ? ? B ? 4 ? 2 ? ? ? ? ? fl ? ? ? ? ? D ? ? ? ? 2 ?", "? 9 ? s ? 9 ? eM KPOe ?", "? s ? a ? ? ? ? ? J ? 4 ? F ? ? ? ?", "? ? 9 ? dMN KPO ? 9 ?", "? ? ? ? ? s ? a ? 9 ? ? ? Z ? V ? ? ? s ? p ? ? ?", "? Z ? s ? T ? ? ? ? ? Z ? ? ? ? ? ?", "? ? s ? ? ? 2 ? ? ? ? 2 ? ? ? a ?", "? ? ? ? ? V ? 9 ? V ? 4 ? ? s ? 9 ? 5 ? ? ? ? ? F ? V ? 4 ? ? ? ? ? D ? ? ? ? ? ? ? ? Z ? ? Z ? 2 ?", "5 ? ? ? 2 ? ? ? ? ? ? ? Z ?", "? s ? Z ? ? Z ? ? ? ? ? ? ? ? ? ? ? ? M ? ? ? s ? ? ? 4 ? Z ? 2 ? 4 ? V ? ? ? ? ? ? ? ? s ? a ? ? ?", "? V ? D ? ? a ? ? ? a ? V ? ? ? B ? ? V ? ? ? s ? 9 ? ? ? 4 ? Z ? ? ? ? D ? ? ? ?", "? ? Z ? ? ? s ? ? ? V ? T ? V ? ? ? ? ? ? ? 9 ? Z ? , ? s ? 9 ? ?", "9 ? ? ? ? ? Z ? ? Z ? 2 ?", "? ? 2 ? ? ? ? ? a ? ? ? ? ? ? ? ? ? ? ? 9 ?", "? ? ? 9 ? ? ? ? ? s ? ? ? ?", "? 9 ? ? s ? ? s ? 9 ? ? M ? Z ? 4 ? ? a ? s ? 4 ? ? ? a ? 9 ? ? ? Z ? V ? ! ? ? ?", "? V ? 9 ? D ? ? Z ? 4 ? ? ? V ? ? ? ? F ? ? ? ?", "? s ? a ? ? ? ? ? ? ? ? ? ? ? ? 9 ? ? ? a ? ? ? V ? a ? s ? ? ? s ? 9 ?", "? a ? M ? X ? a ? fl ? ? 9 ? ? ? T ? 2 ? ?", "? ? ? ? ? a ? V ? ? ? ? ? 2 ? ?", "? MxLz u ? x ? ? ?", "t . ? A ? J ? ay ? ? ? ? ? 5t ? ?", "? Z ? ? ? Z ? Z ? ? ? ? ? ? ? D ? 8 ? D ? ? ? ? ? ? ? ? ? fl ? ? ? D ? ? ?", "? ? ? K ? Z ? ? ? ? ? s ? ? ? ? ? Z ? ? ? ; ? ? ? ? ? ? ? ? ? ?", "? ? ? ? ? Z ? 9 ? ? ? ? ? ? ? Z ? ? ? ? ? ? ? ? ? s ? ? ? ? ? D ? ? ? 9 ?", "? ? ? fl ? ? ? Z ? ? ? D ? ? ? V ? ! ? ? ?", "? V ? ? ? ? ? a ? ? ? s ? ? ? ? ? H ?", "? ? ? ? ? ? V ? ? ? s ? ? ? ? ? ? ? ? ? 0 ? ? ? ? ? ? ? ? ? ? ? ? 5 ? ? ? ? ? ? ? s ? ? ? B ? ? ? ? ? ? ? ? ? Z ? ? ? ? ? ? ? ? ? ? ? D ? ? ?", "? ? ? a ? ? Z ? ? ? D ? ? ? ? ? ? ? ? Z ? K ? ? ? Z ? V ?", "? B ? ? ? H ? s ? ? ? ? ? ? ? ? ? H ?", "GJI 7 3 ? 91 ? K7c ? M ? ?", "? ? ? ? 4 ? B ? 5 ? ? 2 ? ? ?", "? ? ? M ? . ? ? ? J ? fl ? ? ? ? ? ? ? fl ? ? ?", "? V ? 2 ? 4 ? ? ? 4 ? ? ? ? 9 ? ? ? ? ?", "1M ? c ? Z ? ? ? O ? ? ? M ?", "? ? 2 ? , ? s ? Z ? ? 9 ? ?", "? ? ? 2 ? ? ? 0 ? ? ? ? ? 4 ? ? ? ? ? ? ? 4 ? s ? ? s ? 9 ? 0 ? 9 ? ? B ?", "? ? ? 4 ? ? ? a ? ! ? ? ? K ?", "fiD ? s ? 9 ? 5 ? s ? Z ? Z ? ? ? ? ? 9 ? ? ? V ? , ? a ? ? V ? ?", "? ? ? ? ? a ? fl ? ? B ? 4 ? 9 ? ? ? s ? ? ? ? ? V ? ? s ? Z ?", "? ? ? a ? s ? ? ? ? ? ? ? ? ? ? 9 ?", "? 4 ? Z ? ? ? ? D ? ? ? ? ? ? ? V ? ? ? B ?", "? ? ? ? 9 ? Z ? ? ? ? ? ? s ? ? ? 9 ?", "? s ? Z ? ? F ? B ? ? ? ? ? s ? ? ? ? 2 ? X ?", "? Z ? ? ? ? ? ? ? 9 ? ? ? ? ? s ?", "? 9 ? ? ? ? ? , ? V ? ? ? V ? 9 ?", "? ? ? B ? zfia ? , ? ? ? ? ? ? ? ? 2 ? ? ? ? ? ? ? ! ?", "? V ? ? ? ? ? 9 ? Z ? a ? s ? ? ? a ?", "? ? a ? D ? ? ? ? ? , ? ? 9 ? T ?", "? ? ? ? ? ? Z ? ? Z ? 2 ?", "? ? 2 ? J ? V ? s ? ? ? ? ? H ? ? ? ? ?", "? ? ? V ? ? ? 0 ? W ? a ? ?", "? ? ? ? ? ? ? F ? B ? ? ? ? ? fl ? ? ? ? ?", "? ? ? ? F ? ? ? ? ? ? ? ? ? ? ? s ?", "? H ? 0 ? ? ? s ? ? ? ? ? s ? X ? s ? 2 ? ? ?", "? a ? ? ? B ? ? ? ? ? s ? a ?", "? ? ? ? 2 ? a ? ? ? ? ? 9 ? 2 ? ? ? ? ?", "? ? ? ? ? a ? , ? a ? ? ? a ?", "? ? 9 ? s ? ? ? ? ? ? ? ? Z ? 4 ?", "? ? 2 ? A flffiflQ ? a ? V ?", "? a ? s ? s ? Z ? ? ? ? ? ? ? ? ? ? Z ? 4 ?", "? ? ? ? ? ? ? D ? ? ? ? ? ? X ? ?", "? ? ? ? ? ? ? D ? ? ? fl ? ? ? V ? ? ?", "? V ? ? ? ? ? ? B ? ? ? ? ? ? ? ? ? B ?", "1243 7 563 7 3 7 3 ffi !", "? s ? ? ? J ? V ? ? s ? 0 ? ? ? V ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "? 4 ? ? ? ? ? ? ? 0 ? M ? ? ? ? ? D ?", "? ? ? B ? D ? fl ? s ? ? ? , ? ? ? ? ? ? ? D ? s ? ? ? a ? ? ? ! ? ? ? 9 ?", "? ? ? a ? ? ? ? ? ? ? 9 ?", "? V ? 9 ? ? ? ? ? a ? ? ? 9 ?", "? ? ? a ? ? ? ? ? B ? a ? ? a ? 2 ? a ? 9 ? ? ?", "? ? ? B ? a ? ? 9 ? ? ? ? 9 ? , ? ? ? ! ?", "? ? ? s ? 9 ? s ? a ? 9 ? V ?", "? ? ? V ? ! ? s ? 9 ? ? ? ? ? 2 ?", "? a ? ? ? 9 ? s ? ? ? 4 ? ? ? 9 ? ? ? ? ? a ? s ? 0 ? ? 2 ? a ?"], "summary_lines": ["Question Answering Based On Semantic Structures\n", "The ability to answer complex questions posed in Natural Language depends on (1) the depth of the available semantic representations and (2) the inferential mechanisms they support.\n", "In this paper we describe a QA architecture where questions are analyzed and candidate answers generated by 1) identifying predicate argument structures and semantic frames from the input and 2) performing structured probabilistic inference using the extracted relations in the context of a domain and scenario model.\n", "A novel aspect of our system is a scalable and expressive representation of actions and events based on Coordinated Probabilistic Relational Models (CPRM).\n", "In this paper we report on the ability of the implemented system to perform several forms of probabilistic and temporal inferences to extract answers to complex questions.\n", "The results indicate enhanced accuracy over current state-of-the-art Q/A systems.\n", "We explore the role of semantic structures in question answering.\n", "We demonstrate that question answering can stand to benefit from broad coverage semantic processing.\n", "Our question answering system takes PropBank/FrameNet annotations as input, uses the PropBank targets to indicate which actions are being described with which arguments and produces an answer using probabilistic models of actions as the tools of inference.\n"]}
{"article_lines": ["CoNLL 2008 Proceedings of the 12th Conference on Computational Natural Language Learning , pages 183 ? 187 Manchester , August 2008 Dependency based Syntactic ? Semantic Analysis with PropBank and NomBank Richard Johansson and Pierre Nugues Lund University , Sweden richard , pierre cs . lth . se Abstract This paper presents our contribution in the closed track of the 2008 CoNLL Shared Task Surdeanu et al . , 2008 .", "To tackle the problem of joint syntactic ? semantic anal ysis , the system relies on a syntactic and a semantic subcomponent .", "The syntactic model is a bottom up projective parser us ing pseudo projective transformations , and the semantic model uses global inference mechanisms on top of a pipeline of clas sifiers .", "The complete syntactic ? semantic output is selected from a candidate pool generated by the subsystems .", "The system achieved the top score in the closed challenge a labeled syntactic accu racy of 89 . 32 , a labeled semantic F1 of 81 . 65 , and a labeled macro F1 of 85 . 49 .", "1 Introduction Syntactic ? Semantic Analysis Intuitively , semantic interpretation should help syntactic disambiguation , and joint syntactic ?", "semantic analysis has a long tradition in linguis tic theory .", "This motivates a statistical modeling of the problem of finding a syntactic tree y ?", "syn and a semantic graph y ?", "sem for a sentence x as maximiz ing a function F that scores the joint syntactic ?", "semantic structure ? y ?", "arg max y syn , y sem F x , y syn , y sem The dependencies in the feature representation used to compute F determine the tractability of the search procedure needed to perform the maximiza tion .", "To be able to use complex syntactic features c ?", "Licensed under the Creative Commons Attribution Noncommercial Share Alike 3 . 0 Unported li cense http creativecommons . org licenses by nc sa 3 . 0 .", "Some rights reserved .", "such as paths when predicting semantic structures , exact search is clearly intractable .", "This is true even with simpler feature representations ?", "the problem is a special case of multi headed dependency anal ysis , which is NP hard even if the number of heads is bounded Chickering et al . , 1994 .", "This means that we must resort to a simplifica tion such as an incremental method or a reranking approach .", "We chose the latter option and thus cre ated syntactic and semantic submodels .", "The joint syntactic ? semantic prediction is selected from a small list of candidates generated by the respective subsystems .", "2 Syntactic Submodel We model the process of syntactic parsing of a sentence x as finding the parse tree y ?", "syn argmax y F x , y that maximizes a scoring func tion F .", "The learning problem consists of fitting this function so that the cost of the predictions is as low as possible according to a cost function ? .", "In this work , we consider linear scoring functions of the following form F x , y w ? ?", "x , y is a numeric feature representation of the pair x , y andw a vector of feature weights .", "We defined the syntactic cost ?", "as the sum of link costs , where the link cost was 0 for a correct de pendency link with a correct label , 0 . 5 for a correct link with an incorrect label , and 1 for an incorrect link .", "A widely used framework for fitting the weight vector is the max margin model Taskar et al . , 2003 , which is a generalization of the well known support vector machines to general cost based prediction problems .", "Since the large num ber of training examples and features in our case make an exact solution of the max margin opti mization problem impractical , we used the on line passive ? aggressive algorithm Crammer et al . , 183 2006 , which approximates the optimization pro cess in two ways ?", "The weight vector w is updated incremen tally , one example at a time .", "For each example , only the most violated con straint is considered .", "The algorithm is a margin based variant of the per ceptron preliminary experiments show that it out performs the ordinary perceptron on this task .", "Al gorithm 1 shows pseudocode for the algorithm .", "Algorithm 1 The Online PA Algorithm input Training set T x t , y t T t 1 Number of iterations N Regularization parameter C Initialize w to zeros repeat N times for x t , y t in T let y ?", "t argmax y F x t , y ?", "y t , y let ?", "t ? F x t , y t ?", "x , y t ? ? x , y ?", "t returnwaverage We used a C value of 0 . 01 , and the number of iterations was 6 .", "2 . 1 Features and Search The feature function ?", "is a second order edge factored representation McDonald and Pereira , 2006 ; Carreras , 2007 .", "The second order repre sentation allows us to express features not only of head ? dependent links , but also of siblings and chil dren of the dependent .", "This feature set forces us to adopt the expensive search procedure by Car reras 2007 , which extends Eisner ? s span based dynamic programming algorithm 1996 to allow second order feature dependencies .", "Since the cost function ?", "is based on the cost of single links , this procedure can also be used to find the maximizer of F x i , y ij ?", "y i , y ij , which is needed at train ing time .", "The search was constrained to disallow multiple root links .", "2 . 2 Handling Nonprojective Links Although only 0 . 4 of the links in the training set are nonprojective , 7 . 6 of the sentences contain at least one nonprojective link .", "Many of these links represent long range dependencies ?", "such as wh movement ?", "that are valuable for semantic pro cessing .", "Nonprojectivity cannot be handled by span based dynamic programming algorithms .", "For parsers that consider features of single links only , the Chu Liu Edmonds algorithm can be used in stead .", "However , this algorithm cannot be gen eralized to the second order setting ?", "McDonald and Pereira 2006 proved that this problem is NP hard , and described an approximate greedy search algorithm .", "To simplify implementation , we instead opted for the pseudo projective approach Nivre and Nilsson , 2005 , in which nonprojective links are lifted upwards in the tree to achieve projectivity , and special trace labels are used to enable recovery of the nonprojective links at parse time .", "The use of trace labels in the pseudo projective transfor mation leads to a proliferation of edge label types from 69 to 234 in the training set , many of which occur only once .", "Since the running time of our parser depends on the number of labels , we used only the 20 most frequent trace labels .", "3 Semantic Submodel Our semantic model consists of three parts ?", "A SRL classifier pipeline that generates a list of candidate predicate ? argument structures .", "A constraint system that filters the candidate list to enforce linguistic restrictions on the global configuration of arguments .", "A global classifier that rescores the predicate ?", "argument structures in the filtered candidate list .", "Rather than training the models on gold standard syntactic input , we created an automati cally parsed training set by 5 fold cross validation .", "Training on automatic syntax makes the semantic classifiers more resilient to parsing errors , in par ticular adjunct labeling errors .", "3 . 1 SRL Pipeline The SRL pipeline consists of classifiers for predi cate identification , predicate disambiguation , sup port identification for noun predicates , argument identification , and argument classification .", "We trained one set of classifiers for verb predicates and another for noun predicates .", "For the pred icate disambiguation classifiers , we trained one subclassifier for each lemma .", "All classifiers in the pipeline were L2 regularized linear logistic regres sion classifiers , implemented using the efficient LIBLINEAR package Lin et al . , 2008 .", "For multi class problems , we used the one vs all binarization 184 method , which makes it easy to prevent outputs not allowed by the PropBank or NomBank frame .", "Since our classifiers were logistic , their output values could be meaningfully interpreted as prob abilities .", "This allowed us to combine the scores from subclassifiers into a score for the complete predicate ? argument structure .", "To generate the can didate lists used by the global SRL models , we ap plied beam search based on these scores using a beam width of 4 .", "The features used by the classifiers are listed in Tables 1 and 2 .", "In the tables , the features used by the classifiers for noun and verb predicates are indicated by N and V , respectively .", "We selected the feature sets by greedy forward subset selection .", "Feature PredId PredDis PREDWORD N , V N , V PREDLEMMA N , V N , V PREDPARENTWORD POS N , V N , V CHILDDEPSET N , V N , V CHILDWORDSET N , V N , V CHILDWORDDEPSET N , V N , V CHILDPOSSET N , V N , V CHILDPOSDEPSET N , V N , V DEPSUBCAT N , V N , V PREDRELTOPARENT N , V N , V Table 1 Classifier features in predicate identifica tion and disambiguation .", "Feature Supp ArgId ArgCl PREDPARENTWORD POS N N , V CHILDDEPSET N N , V N , V PREDLEMMASENSE N N , V N , V VOICE V V POSITION N N , V N , V ARGWORD POS N N , V N , V LEFTWORD POS N N , V RIGHTWORD POS N N , V N , V LEFTSIBLINGWORD POS N , V RIGHTSIBLINGWORD POS N N PREDPOS N N , V V RELPATH N N , V N , V POSPATH N RELPATHTOSUPPORT N N VERBCHAINHASSUBJ V V CONTROLLERHASOBJ V N PREDRELTOPARENT N N , V N , V FUNCTION N , V Table 2 Classifier features in argument identifica tion and classification and support detection .", "Features Used in Predicate Identification and Disambiguation PREDWORD , PREDLEMMA .", "The lexical form and lemma of the predicate .", "PREDPARENTWORD and PREDPARENTPOS .", "Form and part of speech tag of the parent node of the predicate .", "CHILDDEPSET , CHILDWORDSET , CHILD WORDDEPSET , CHILDPOSSET , CHILD POSDEPSET .", "These features represent the set of dependents of the predicate using combinations of dependency labels , words , and parts of speech .", "Subcategorization frame the con catenation of the dependency labels of the predicate dependents .", "PREDRELTOPARENT .", "Dependency relation be tween the predicate and its parent .", "Features Used in Argument Identification and Classification PREDLEMMASENSE .", "The lemma and sense number of the predicate , e . g .", "For verbs , this feature is Active or Pas sive .", "For nouns , it is not defined .", "Position of the argument with respect to the predicate Before , After , or On .", "ARGWORD and ARGPOS .", "Lexical form and part of speech tag of the argument node .", "LEFTWORD , LEFTPOS , RIGHTWORD , RIGHT POS .", "Form part of speech tag of the left most rightmost dependent of the argument .", "LEFTSIBLINGWORD , LEFTSIBLINGPOS , RIGHTSIBLINGWORD , RIGHTSIBLING POS .", "Form part of speech tag of the left right sibling of the argument .", "Part of speech tag of the predicate .", "A representation of the complex grammatical relation between the predicate and the argument .", "It consists of the sequence of dependency relation labels and link direc tions in the path between predicate and argu ment , e . g .", "An alternative view of the grammat ical relation , which consists of the POS tags passed when moving from predicate to argu ment , e . g .", "RELPATHTOSUPPORT .", "The RELPATH from the argument to a support chain .", "VERBCHAINHASSUBJ .", "Binary feature that is set to true if the predicate verb chain has a sub ject .", "The purpose of this feature is to resolve verb coordination ambiguity as in Figure 1 .", "CONTROLLERHASOBJ .", "Binary feature that is true if the link between the predicate verb chain and its parent is OPRD , and the parent has an object .", "This feature is meant to resolve control ambiguity as in Figure 2 .", "The grammatical function of the ar gument node .", "For direct dependents of the predicate , this is identical to the RELPATH .", "I SBJ eat drinkyouand COORD SBJ CONJROOT SBJ COORD ROOT drinkandeatI CONJ Figure 1 Coordination ambiguity The subject I is in an ambiguous position with respect to drink .", "I to IMSBJ want sleephim OBJ OPRD ROOT IM sleepI SBJ want ROOT to OPRD Figure 2 Subject object control ambiguity I is in an ambiguous position with respect to sleep .", "3 . 2 Linguistically Motivated Global Constraints The following three global constraints were used to filter the candidates generated by the pipeline .", "CORE ARGUMENT CONSISTENCY .", "Core argu ment labels must not appear more than once .", "DISCONTINUITY CONSISTENCY .", "If there is a la bel C X , it must be preceded by a label X .", "REFERENCE CONSISTENCY .", "If there is a label R X and the label is inside a relative clause , it must be preceded by a label X .", "3 . 3 Global SRL Model Toutanova et al .", "2005 have showed that a global model that scores the complete predicate ?", "argument structure can lead to substantial perfor mance gains .", "We therefore created a global SRL classifier using the following global features in ad dition to the features from the pipeline CORE ARGUMENT LABEL SEQUENCE .", "The complete sequence of core argument labels .", "The sequence also includes the predicate and voice , for instance A0 break . 01 Active A1 .", "MISSING CORE ARGUMENT LABELS .", "The set of core argument labels declared in the Prop Bank NomBank frame that are not present in the predicate ? argument structure .", "Similarly to the syntactic submodel , we trained the global SRL model using the online passive ?", "aggressive algorithm .", "The cost function ?", "was defined as the number of incorrect links in the predicate ? argument structure .", "The number of it erations was 20 and the regularization parameter C was 0 . 01 .", "Interestingly , we noted that the global SRL model outperformed the pipeline even when no global features were added .", "This shows that the global learning model can correct label bias prob lems introduced by the pipeline architecture .", "4 Syntactic ? Semantic Integration Our baseline joint feature representation contained only three features the log probability of the syn tactic tree and the log probability of the semantic structure according to the pipeline and the global model , respectively .", "This model was trained on the complete training set using cross validation .", "The probabilities were obtained using the multinomial logistic function ? softmax ? .", "We carried out an initial experiment with a more complex joint feature representation , but failed to improve over the baseline .", "Time prevented us from exploring this direction conclusively .", "5 Results The submitted results on the development and test corpora are presented in the upper part of Table 3 .", "After the submission deadline , we corrected a bug in the predicate identification method .", "This re sulted in improved results shown in the lower part .", "Corpus Syn acc Sem F1 Macro F1 Development 88 . 47 80 . 80 84 . 66 Test WSJ 90 . 13 81 . 75 85 . 95 Test Brown 82 . 81 69 . 06 75 . 95 Test WSJ Brown 89 . 32 80 . 37 84 . 86 Development 88 . 47 81 . 86 85 . 17 Test WSJ 90 . 13 83 . 75 86 . 61 Test Brown 82 . 84 69 . 85 76 . 34 Test WSJ Brown 89 . 32 81 . 65 85 . 49 Table 3 Results .", "5 . 1 Syntactic Results Table 4 shows the effect of adding second order features to the parser in terms of accuracy as well as training and parsing time on a Mac Pro , 3 . 2 GHz .", "The training times were measured on the complete training set and the parsing time and ac curacies on the development set .", "Similarly to Car reras 2007 , we see that these features have a very large impact on parsing accuracy , but also that the parser pays dearly in terms of efficiency as the search complexity increases fromO n3 toO n4 .", "186 Since the low efficiency of the second order parser restricts its use to batch applications , we see an in teresting research direction to find suitable com promises between the two approaches , for instance by sacrificing the exact search procedure .", "System Training Parse Labeled Unlabeled 1st order 65 min 28 sec 85 . 78 89 . 51 2nd order 60 hours 34 min 88 . 33 91 . 43 Table 4 Impact of second order features .", "Table 5 shows the dependency types most af fected by the addition of second order features to the parser when ordered by the increase in F1 .", "As can be seen , they are all verb adjunct categories , which demonstrates the effect of grandchild fea tures on PP attachment and labeling .", "Label ? R ? P ? F 1 TMP 14 . 7 12 . 9 13 . 9 DTV 0 19 . 9 10 . 5 LOC 7 . 8 12 . 3 9 . 9 PRP 12 . 4 6 . 7 9 . 6 DIR 5 . 9 7 . 2 6 . 5 Table 5 Labels affected by second order features .", "5 . 2 Semantic Results To assess the effect of the components in the se mantic submodel , we tested their performance on the top scoring parses from the syntactic model .", "Table 6 shows the results .", "The baseline system consists of the SRL pipeline only P .", "Adding lin guistic constraints C results in a more precision oriented system with slightly lower recall , but sig nificantly higher F1 .", "Even higher performance is obtained when adding the global SRL model G .", "System P R F1 P 80 . 74 77 . 98 79 . 33 P C 82 . 42 77 . 66 79 . 97 P C G 83 . 64 78 . 14 80 . 40 Table 6 SRL results on the top scoring parse trees .", "5 . 3 Syntactic ? Semantic Integration The final experiment concerned the integration of syntactic and semantic analysis .", "In this setting , the system chooses the output that maximizes the joint syntactic ? semantic score , based on the top N syntactic trees .", "Table 7 shows the results on the development set .", "We see that syntactic ? semantic integration improves both syntactic accuracy and semantic F1 .", "This holds for the constraint based SRL system as well as for the full system .", "Sem model N Syn acc Sem F1 Macro F1 P C 1 88 . 33 79 . 97 84 . 17 P C 16 88 . 42 80 . 42 84 . 44 P C G 1 88 . 33 80 . 40 84 . 39 P C G 16 88 . 47 80 . 80 84 . 66 Table 7 Syntactic ? semantic integration .", "6 Conclusion We have described a system1 for syntactic and se mantic dependency analysis based on PropBank and NomBank , and detailed the implementation of its subsystems .", "Crucial to our success was the high performance of the syntactic parser , which achieved a high accuracy .", "In addition , we recon firmed the benefits of global inference in semantic analysis both constraint based and learning based methods resulted in improvements over a baseline .", "Finally , we showed that integration of syntactic and semantic analysis is beneficial for both sub tasks .", "We hope that this shared task will spur fur ther research that leads to new feature representa tions and search procedures to handle the problem of joint syntactic and semantic analysis .", "References Carreras , Xavier .", "Experiments with a higher order pro jective dependency parser .", "In Proceedings of CoNLL .", "Chickering , David M . , Dan Geiger , and David Heckerman .", "Learning Bayesian networks The combination of knowledge and statistical data .", "Technical Report MSR TR 94 09 , Microsoft Research .", "Crammer , Koby , Ofer Dekel , Joseph Keshet , Shai Shalev Schwartz , and Yoram Singer .", "Online passive aggressive algorithms .", "JMLR , 2006 7 551 ? 585 .", "Eisner , Jason M . 1996 .", "Three new probabilistic models for dependency parsing An exploration .", "Lin , Chih Jen , Ruby C . Weng , and S . Sathiya Keerthi .", "Trust region Newton method for large scale logistic regres sion .", "JMLR , 2008 9 627 ? 650 .", "McDonald , Ryan and Fernando Pereira .", "Online learn ing of approximate dependency parsing algorithms .", "In Proceedings of EACL 2006 .", "Nivre , Joakim and Jens Nilsson .", "Pseudo projective dependency parsing .", "In Proceedings of ACL 2005 .", "Surdeanu , Mihai , Richard Johansson , Adam Meyers , Llu ? s M ? rquez , and Joakim Nivre .", "The CoNLL ? 2008 shared task on joint parsing of syntactic and semantic de pendencies .", "In Proceedings of CoNLL ? 2008 .", "Taskar , Ben , Carlos Guestrin , and Daphne Koller .", "Max margin Markov networks .", "In Proceedings of NIPS .", "Toutanova , Kristina , Aria Haghighi , and Christopher D . Man ning .", "Joint learning improves semantic role label ing .", "In Proceedings of ACL 2005 .", "1Our system is freely available for download at http nlp . cs . lth . se lth_srl ."], "summary_lines": ["Dependency-based Syntactic&#x2013;Semantic Analysis with PropBank and NomBank\n", "This paper presents our contribution in the closed track of the 2008 CoNLL Shared Task (Surdeanu et al., 2008).\n", "To tackle the problem of joint syntactic-semantic analysis, the system relies on a syntactic and a semantic subcomponent.\n", "The syntactic model is a bottom-up projective parser using pseudo-projective transformations, and the semantic model uses global inference mechanisms on top of a pipeline of classifiers.\n", "The complete syntactic-semantic output is selected from a candidate pool generated by the subsystems.\n", "The system achieved the top score in the closed challenge: a labeled syntactic accuracy of 89.32%, a labeled semantic F1 of 81.65, and a labeled macro F1 of 85.49.\n", "Our system use two 30 different subsystems to handle verbal and nominal predicates, respectively.\n", "We present importance of capturing non-local dependencies of core arguments in predicate-argument structure analysis.\n", "In our work, the impact of different grammatical representations on the task of frame-based shallow semantic parsing is studied and the poor lexical generalization problem is outlined.\n"]}
{"article_lines": ["Reliable Measures For Aligning Japanese English News Articles And Sentences", "We have aligned Japanese and English news articles and sentences to make a large parallel corpus .", "We first used a method based on cross language information retrieval CLIR to align the Japanese and English articles and then used a method based on dynamic programming DP matching to align the Japanese and English sentences in these articles .", "However , the results included many incorrect alignments .", "To remove these , we propose two measures scores that evaluate the validity of alignments .", "The measure for article alignment uses similarities in sentences aligned by DP matching and that for sentence alignment uses similarities in articles aligned by CLIR .", "They enhance each other to improve the accuracy of alignment .", "Using these measures , we have successfully constructed a largescale article and sentence alignment corpus available to the public .", "A large scale Japanese English parallel corpus is an invaluable resource in the study of natural language processing NLP such as machine translation and cross language information retrieval CLIR .", "It is also valuable for language education .", "However , no such corpus has been available to the public .", "We recently have obtained a noisy parallel corpus of Japanese and English newspapers consisting of issues published over more than a decade and have tried to align their articles and sentences .", "We first aligned the articles using a method based on CLIR Collier et al . , 1998 ; Matsumoto and Tanaka , 2002 and then aligned the sentences in these articles by using a method based on dynamic programming DP matching Gale and Church , 1993 ; Utsuro et al . , 1994 .", "However , the results included many incorrect alignments due to noise in the corpus .", "To remove these , we propose two measures scores that evaluate the validity of article and sentence alignments .", "Using these , we can selectively extract valid alignments .", "In this paper , we first discuss the basic statistics on the Japanese and English newspapers .", "We next explain methods and measures used for alignment .", "We then evaluate the effectiveness of the proposed measures .", "Finally , we show that our aligned corpus has attracted people both inside and outside the NLP community .", "The Japanese and English newspapers used as source data were the Yomiuri Shimbun and the Daily Yomiuri .", "They cover the period from September 1989 to December 2001 .", "The number of Japanese articles per year ranges from 100 , 000 to 350 , 000 , while English articles ranges from 4 , 000 to 13 , 000 .", "The total number of Japanese articles is about 2 , 000 , 000 and the total number of English articles is about 110 , 000 .", "The number of English articles represents less than 6 percent that of Japanese articles .", "Therefore , we decided to search for the Japanese articles corresponding to each of the English articles .", "The English articles as of mid July 1996 have tags indicating whether they are translated from Japanese articles or not , though they don t have explicit links to the original Japanese articles .", "Consequently , we only used the translated English articles for the article alignment .", "The number of English articles used was 35 , 318 , which is 68 percent of all of the articles .", "On the other hand , the English articles before mid July 1996 do not have such tags .", "So we used all the articles for the period .", "The number of them was 59 , 086 .", "We call the set of articles before mid July 1996 1989 1996 and call the set of articles after mid July 1996 1996 2001 . If an English article is a translation of a Japanese article , then the publication date of the Japanese article will be near that of the English article .", "So we searched for the original Japanese articles within 2 days before and after the publication of each English article , i . e . , the corresponding article of an English article was searched for from the Japanese articles of 5 days issues .", "The average number of English articles per day was 24 and that of Japanese articles per 5 days was 1 , 532 for 1989 1996 .", "For 1996 2001 , the average number of English articles was 18 and that of Japanese articles was 2 , 885 .", "As there are many candidates for alignment with English articles , we need a reliable measure to estimate the validity of article alignments to search for appropriate Japanese articles from these ambiguous matches .", "Correct article alignment does not guarantee the existence of one to one correspondence between English and Japanese sentences in article alignment because literal translations are exceptional .", "Original Japanese articles may be restructured to conform to the style of English newspapers , additional descriptions may be added to fill cultural gaps , and detailed descriptions may be omitted .", "A typical example of a restructured English and Japanese article pair is Part of an English article he1i Two bullet holes were found at the home of Kengo Tanaka , 65 , president of Bungei Shunju , in Akabane , Tokyo , by his wife Kimiko , 64 , at around 9 a . m . Monday . h e1i he2i Police suspect right wing activists , who have mounted criticism against articles about the Imperial family appearing in the Shukan Bunshun , the publisher s weekly magazine , were responsible for the shooting . h e2i he3i Police received an anonymous phone call shortly after 1 a . m . Monday by a caller who reported hearing gunfire near Tanaka s residence . h e3i he4i Police found nothing after investigating the report , but later found a bullet in the Tanakas bedroom , where they were sleeping at the time of the shooting . h e4i Part of a literal translation of a Japanese article hj1i At about 8 55 a . m . on the 29th , Kimiko Tanaka , 64 , the wife of Bungei Shunju s president Kengo Tanaka , 65 , found bullet holes on the eastern wall of their two story house at 4 Akabane Nishi , Kitaku , Tokyo . h j1i hj2i As a result of an investigation , the officers of the Akabane police station found two holes on the exterior wall of the bedroom and a bullet in the bedroom . h j2i hj3i After receiving an anonymous phone call shortly after 1 a . m . saying that two or three gunshots were heard near Tanaka s residence , police officers hurried to the scene for investigation , but no bullet holes were found . h j3i hj4i When gunshots were heard , Mr . and Mrs . Tanaka were sleeping in the bedroom . h j4i hj5i Since Shukan Bunshun , a weekly magazine published by Bungei Shunju , recently ran an article criticizing the Imperial family , Akabane police suspect rightwing activists who have mounted criticism against the recent article to be responsible for the shooting and have been investigating the incident . h j5i where there is a three to four correspondence between e1 , e3 , e4 and j1 , j2 , j3 , j4 , together with a one to one correspondence between e2 and j5 .", "Such sentence matches are of particular interest to researchers studying human translations and or stylistic differences between English and Japanese newspapers .", "However , their usefulness as resources for NLP such as machine translation is limited for the time being .", "It is therefore important to extract sentence alignments that are as literal as possible .", "To achieve this , a reliable measure of the validity of sentence alignments is necessary .", "We adopt a standard strategy to align articles and sentences .", "First , we use a method based on CLIR to align Japanese and English articles Collier et al . , 1998 ; Matsumoto and Tanaka , 2002 and then a method based on DP matching to align Japanese and English sentences Gale and Church , 1993 ; Utsuro et al . , 1994 in these articles .", "As each of these methods uses existing NLP techniques , we describe them briefly focusing on basic similarity measures , which we will compare with our proposed measures in Section 5 .", "We first convert each of the Japanese articles into a set of English words .", "We use ChaSen1 to segment each of the Japanese articles into words .", "We next extract content words , which are then translated into English words by looking them up in the EDR Japanese English bilingual dictionary , 2 EDICT , and ENAMDICT , 3 which have about 230 , 000 , 100 , 000 , and 180 , 000 entries , respectively .", "We select two English words for each of the Japanese words using simple heuristic rules based on the frequencies of English words .", "We use each of the English articles as a query and search for the Japanese article that is most similar to the query article .", "The similarity between an English article and a word based English translation of Japanese article is measured by BM25 Robertson and Walker , 1994 .", "BM25 and its variants have been proven to be quite efficient in information retrieval .", "Readers are referred to papers by the Text REtrieval Conference TREC 4 , for example .", "The definition of BM25 is where J is the set of translated English words of a Japanese article and E is the set of words of an English article .", "The words are stemmed and stop words are removed .", "N is the number of Japanese articles to be searched . n is the number of articles containing T . Kis k1 1 b b dl avdl . k1 , b and k3 are parameters set to 1 , 1 , and 1000 , respectively . dl is the document length of J and avdl is the average document length in words . tf is the frequency of occurrence of T in J . qtf is the frequency of T in E . To summarize , we first translate each of the Japanese articles into a set of English words .", "We then use each of the English articles as a query and search for the most similar Japanese article in terms of BM25 and assume that it corresponds to the English article .", "The sentences5 in the aligned Japanese and English articles are aligned by a method based on DP matching Gale and Church , 1993 ; Utsuro et al . , 1994 .", "We allow 1 to n or n to 1 1 n 6 alignments when aligning the sentences .", "Readers are referred to Utsuro et al . 1994 for a concise description of the algorithm .", "Here , we only discuss the similarities between Japanese and English sentences for alignment .", "Let JZ and EZ be the words of Japanese and English sentences for i th alignment .", "The similarity6 between JZ and EZ is where a one to one correspondence between Japanese and English words .", "JZ and EZ are obtained as follows .", "We use ChaSen to morphologically analyze the Japanese sentences and extract content words , which consists of JZ .", "We use Brill s tagger Brill , 1992 to POS tag the English sentences , extract content words , and use WordNet s library7 to obtain lemmas of the words , which consists of EZ .", "We use simple heuristics to obtain JZ x EZ , i . e . , a one to one correspondence between the words in JZ and EZ , by looking up JapaneseEnglish and English Japanese dictionaries made up by combining entries in the EDR Japanese English bilingual dictionary and the EDR English Japanese bilingual dictionary .", "Each of the constructed dictionaries has over 300 , 000 entries .", "We evaluated the implemented program against a corpus consisting of manually aligned Japanese and English sentences .", "The source texts were Japanese white papers JEIDA , 2000 .", "The style of translation was generally literal reflecting the nature of government documents .", "We used 12 pairs of texts for evaluation .", "The average number of Japanese sentences per text was 413 and that of English sentences was 495 .", "The recall , R , and precision , P , of the program against this corpus were R 0 . 982 and P 0 . 986 , respectively , where The number of pairs in a one to n alignment is n . For example , if sentences J11 and E1 , E2 , E31 are aligned , then three pairs J1 , E1 , J1 , E2 , and J1 , E3 are obtained .", "This recall and precision are quite good considering the relatively large differences in the language structures between Japanese and English .", "We use BM25 and SIM to evaluate the similarity in articles and sentences , respectively .", "These measures , however , cannot be used to reliably discriminate between correct and incorrect alignments as will be discussed in Section 5 .", "This motivated us to devise more reliable measures based on basic similarities .", "BM25 measures the similarity between two bags of words .", "It is not sensitive to differences in the order of sentences between two articles .", "To remedy this , we define a measure that uses the similarities in sentence alignments in the article alignment .", "We define AVSIM J , E as the similarity between Japanese article , J , and English article , E where J1 , E1 , J2 , E2 , . . . Jm , Em are the sentence alignments obtained by the method described in Section 3 . 2 .", "The sentence alignments in a correctly aligned article alignment should have more similarity than the ones in an incorrectly aligned article alignment .", "Consequently , article alignments with high AVSIM are likely to be correct .", "Our sentence alignment program aligns sentences accurately if the English sentences are literal translations of the Japanese as discussed in Section 3 . 2 .", "However , the relation between English news sentences and Japanese news sentences are not literal translations .", "Thus , the results for sentence alignments include many incorrect alignments .", "To discriminate between correct and incorrect alignments , we take advantage of the similarity in article alignments containing sentence alignments so that the sentence alignments in a similar article alignment will have a high value .", "We define SntScore JZ , EZ AVSIM J , E x SIM JZ , EZ SntScore JZ , EZ is the similarity in the i th alignment , JZ , EZ , in article alignment J and E . When we compare the validity of two sentence alignments in the same article alignment , the rank order of sentence alignments obtained by applying SntScore is the same as that of SIM because they share a common AVSIM .", "However , when we compare the validity of two sentence alignments in different article alignments , SntScore prefers the sentence alignment with the more similar high AVSIM article alignment even if their SIM has the same value , while SIM cannot discriminate between the validity of two sentence alignments if their SIM has the same value .", "Therefore , SntScore is more appropriate than SIM if we want to compare sentence alignments in different article alignments , because , in general , a sentence alignment in a reliable article alignment is more reliable than one in an unreliable article alignment .", "The next section compares the effectiveness of AVSIM to that of BM25 , and that of SntScore to that of SIM .", "Here , we discuss the results of evaluating article and sentence alignments .", "We first estimate the precision of article alignments by using randomly sampled alignments .", "Next , we sort them in descending order of BM25 and AVSIM to see whether these measures can be used to provide correct alignments with a high ranking .", "Finally , we show that the absolute values of AVSIM correspond well with human judgment .", "Each English article was aligned with a Japanese article with the highest BM25 .", "We sampled 100 article alignments from each of 1996 2001 and 19891996 .", "We then classified the samples into four categories A , B , C , and D .", "A means that there was more than 50 to 60 overlap in the content of articles .", "B means more than 20 to 30 and less than 50 to 60 overlap .", "D means that there was no overlap at all .", "C means that alignment was not included in A , B or D .", "We regard alignments that were judged to be A or B to be suitable for NLP because of their relatively large overlap .", "The results of evaluations are in Table 1 . 8 Here , ratio means the ratio of the number of articles judged to correspond to the respective category against the total number of articles .", "For example , 0 . 59 in line A of 1996 2001 means that 59 out of 100 samples were evaluated as A .", "Lower and upper mean the lower and upper bounds of the 95 confidence interval for ratio .", "The table shows that the precision sum of the ratios of A and B for 1996 2001 was higher than that for 1989 1996 .", "They were 0 . 71 for 1996 2001 and 0 . 44 for 1989 1996 .", "This is because the English articles from 1996 2001 were translations of Japanese articles , while those from 1989 1996 were not necessarily translations as explained in Section 2 .", "Although the precision for 1996 2001 was higher than that for 1989 1996 , it is still too low to use them as NLP resources .", "In other words , the article alignments included many incorrect alignments .", "We want to extract alignments which will be evaluated as A or B from these noisy alignments .", "To do this , we have to sort all alignments according to some measures that determine their validity and extract highly ranked ones .", "To achieve this , AVSIM is more reliable than BM25 as is explained below .", "8The evaluations were done by the authors .", "We double checked the sample articles from 1996 2001 .", "Our second checks are presented in Table 1 .", "The ratio of categories in the first check were A 0 . 62 , B 0 . 09 , C 0 . 09 , and D 0 . 20 .", "Comparing these figures with those in Table 1 , we concluded that first and second evaluations were consistent .", "Sorted alignments AVSIM vs . BM25 We sorted the same alignments in Table 1 in decreasing order of AVSIM and BM25 .", "Alignments judged to be A or B were regarded as correct .", "The number , N , of correct alignments and precision , P , up to each rank are shown in Table 2 .", "From the table , we can conclude that AVSIM ranks correct alignments higher than BM25 .", "Its greater accuracy indicates that it is important to take similarities in sentence alignments into account when estimating the validity of article alignments .", "Table 2 shows that AVSIM is reliable in ranking correct and incorrect alignments .", "This section reveals that not only rank order but also absolute values of AVSIM are reliable for discriminating between correct and incorrect alignments .", "That is , they correspond well with human evaluations .", "This means that a threshold value is set for each of 19962001 and 1989 1996 so that valid alignments can be extracted by selecting alignments whose AVSIM is larger than the threshold .", "We used the same data in Table 1 to calculate statistics on AVSIM .", "They are shown in Tables 3 and 4 for 1996 2001 and 1989 1996 , respectively .", "In these tables , N means the number of alignments against the corresponding human judgment .", "Av . means the average value of AVSIM .", "Lower and upper mean the lower and upper bounds of the 95 confidence interval for the average .", "Th . means the threshold for AVSIM that can be used to discriminate between the alignments estimated to be the corresponding evaluations .", "For example , in Table 3 , evaluations A and B are separated by 0 . 168 .", "These thresholds were identified through linear discriminant analysis .", "The asterisks and in the sig . column mean that the difference in averages for AVSIM is statistically significant at 1 and 5 based on a one sided Welch test .", "In these tables , except for the differences in the averages for B and C in Table 4 , all differences in averages are statistically significant .", "This indicates that AVSIM can discriminate between differences in judgment .", "In other words , the AVSIM values correspond well with human judgment .", "We then tried to determine why B and C in Table 4 were not separated by inspecting the article alignments and found that alignments evaluated as C in Table 4 had relatively large overlaps compared with alignments judged as C in Table 3 .", "It was more difficult to distinguish B or C in Table 4 than in Table 3 .", "We next classified all article alignments in 19962001 and 1989 1996 based on the thresholds in Tables 3 and 4 .", "The numbers of alignments are in Table 5 .", "It shows that the number of alignments estimated to be A or B was 46738 31495 15243 .", "We regard about 47 , 000 article alignments to be sufficiently large to be useful as a resource for NLP such as bilingual lexicon acquisition and for language education .", "In summary , AVSIM is more reliable than BM25 and corresponds well with human judgment .", "By using thresholds , we can extract about 47 , 000 article alignments which are estimated to be A or B evaluations .", "Sentence alignments in article alignments have many errors even if they have been obtained from correct article alignments due to free translation as discussed in Section 2 .", "To extract only correct alignments , we sorted whole sentence alignments in whole article alignments in decreasing order of SntScore and selected only the higher ranked sentence alignments so that the selected alignments would be sufficiently precise to be useful as NLP resources .", "The number of whole sentence alignments was about 1 , 300 , 000 .", "The most important category for sentence alignment is one to one .", "Thus , we want to discard as many errors in this category as possible .", "In the first step , we classified whole oneto one alignments into two classes the first consisted of alignments whose Japanese and English sentences ended with periods , question marks , exclamation marks , or other readily identifiable characteristics .", "We call this class one to one .", "The second class consisted of the one to one alignments not belonging to the first class .", "The alignments in this class , together with the whole one to n alignments , are called one to many .", "One to one had about 640 , 000 alignments and one to many had about 660 , 000 alignments .", "We first evaluated the precision of one to one alignments by sorting them in decreasing order of SntScore .", "We randomly extracted 100 samples from each of 10 blocks ranked at the top 300 , 000 alignments .", "A block had 30 , 000 alignments .", "We classified these 1000 samples into two classes The first was match A , the second was not match D .", "We judged a sample as A if the Japanese and English sentences of the sample shared a common event approximately a clause .", "D consisted of the samples not belonging to A .", "The results of evaluation are in Table 6 . 9 This table shows that the number of A s decreases rapidly as the rank increases .", "This means that SntScore ranks appropriate one to one alignments highly .", "The table indicates that the top 150 , 000 oneto one alignments are sufficiently reliable . 10 The ratio of A s in these alignments was 0 . 982 .", "We then evaluated precision for one to many alignments by sorting them in decreasing order of SntScore .", "We classified one to many into three categories 1 90000 , 90001 180000 , and 180001270000 , each of which was covered by the range of SntScore of one to one that was presented in Table 6 .", "We randomly sampled 100 one to many alignments from these categories and judged them to be A or D see Table 7 .", "Table 7 indicates that the 38 , 090 alignments in the range from 1 90000 are sufficiently reliable .", "Tables 6 and 7 show that we can extract valid alignments by sorting alignments according to SntScore and by selecting only higher ranked sentence alignments .", "Overall , evaluations between the first and second check were consistent .", "10The notion of appropriate correct sentence alignment depends on applications .", "Machine translation , for example , may require more precise literal alignment .", "To get literal alignments beyond a sharing of a common event , we will select a set of alignments from the top of the sorted alignments that satisfies the required literalness .", "This is because , in general , higher ranked alignments are more literal translations , because those alignments tend to have many one to one corresponding words and to be contained in highly similar article alignments .", "We compared SntScore with SIM and found that SntScore is more reliable than SIM in discriminating between correct and incorrect alignments .", "We first sorted the one to one alignments in decreasing order of SIM and randomly sampled 100 alignments from the top 150 , 000 alignments .", "We classified the samples into A or D . The number of A s was 93 , and that of D s was 7 .", "The precision was 0 . 93 .", "However , in Table 6 , the number of A s was 491 and D s was 9 , for the 500 samples extracted from the top 150 , 000 alignments .", "The precision was 0 . 982 .", "Thus , the precision of SntScore was higher than that of SIM and this difference is statistically significant at 1 based on a one sided proportional test .", "We then sorted the one to many alignments by SIM and sampled 100 alignments from the top 38 , 090 and judged them .", "There were 89 A s and 11 D s .", "The precision was 0 . 89 .", "However , in Table 7 , there were 98 A s and 2 D s for samples from the top 38 , 090 alignments .", "The precision was 0 . 98 .", "This difference is also significant at 1 based on a one sided proportional test .", "Thus , SntScore is more reliable than SIM .", "This high precision in SntScore indicates that it is important to take the similarities of article alignments into account when estimating the validity of sentence alignments .", "Much work has been done on article alignment .", "Collier et al . 1998 compared the use of machine translation MT with the use of bilingual dictionary term lookup DTL for news article alignment in Japanese and English .", "They revealed that DTL is superior to MT at high recall levels .", "That is , if we want to obtain many article alignments , then DTL is more appropriate than MT .", "In a preliminary experiment , we also compared MT and DTL for the data in Table 1 and found that DTL was superior to MT . 11 These 11We translated the English articles into Japanese with an MT system .", "We then used the translated English articles as queries and searched the database consisting of Japanese articles .", "The direction of translation was opposite to the one described in Section 3 . 1 .", "Therefore this comparison is not as objective as it could be .", "However , it gives us some idea into a comparison of MT and DTL .", "12http www . crl . go . jp jt a132 members mutiyama jea index . html experimental results indicate that DTL is more appropriate than MT in article alignment .", "Matsumoto and Tanaka 2002 attempted to align Japanese and English news articles in the Nikkei Industrial Daily .", "Their method achieved a 97 precision in aligning articles , which is quite high .", "They also applied their method to NHK broadcast news .", "However , they obtained a lower precision of 69 . 8 for the NHK corpus .", "Thus , the precision of their method depends on the corpora .", "Therefore , it is not clear whether their method would have achieved a high accuracy in the Yomiuri corpus treated in this paper .", "There are two significant differences between our work and previous works .", "1 We have proposed AVSIM , which uses similarities in sentences aligned by DP matching , as a reliable measure for article alignment .", "Previous works , on the other hand , have used measures based on bag of words .", "2 A more important difference is that we have actually obtained not only article alignments but also sentence alignments on a large scale .", "In addition to that , we are distributing the alignment data for research and educational purposes .", "This is the first attempt at a Japanese English bilingual corpus .", "As of late October 2002 , we have been distributing the alignment data discussed in this paper for research and educational purposes . 12 All the information on the article and sentence alignments are numerically encoded so that users who have the Yomiuri data can recover the results of alignments .", "The data also contains the top 150 , 000 one to one sentence alignments and the top 30 , 000 one to many sentence alignments as raw sentences .", "The Yomiuri Shimbun generously allowed us to distribute them for research and educational purposes .", "We have sent over 30 data sets to organizations on their request .", "About half of these were NLPrelated .", "The other half were linguistics related .", "A few requests were from high school and junior highschool teachers of English .", "A psycho linguist was also included .", "It is obvious that people from both inside and outside the NLP community are interested in this Japanese English alignment data .", "We have proposed two measures for extracting valid article and sentence alignments .", "The measure for article alignment uses similarities in sentences aligned by DP matching and that for sentence alignment uses similarities in articles aligned by CLIR .", "They enhance each other and allow valid article and sentence alignments to be reliably extracted from an extremely noisy Japanese English parallel corpus .", "We are distributing the alignment data discussed in this paper so that it can be used for research and educational purposes .", "It has attracted the attention of people both inside and outside the NLP community .", "We have applied our measures to a Japanese and English bilingual corpus and these are language independent .", "It is therefore reasonable to expect that they can be applied to any language pair and still retain good performance , particularly since their effectiveness has been demonstrated in such a disparate language pair as Japanese and English ."], "summary_lines": ["Reliable Measures For Aligning Japanese-English News Articles And Sentences\n", "We have aligned Japanese and English news articles and sentences to make a large parallel corpus.\n", "We first used a method based on cross-language information retrieval (CLIR) to align the Japanese and English articles and then used a method based on dynamic programming (DP) matching to align the Japanese and English sentences in these articles.\n", "However, the results included many incorrect alignments.\n", "To remove these, we propose two measures (scores) that evaluate the validity of alignments.\n", "The measure for article alignment uses similarities in sentences aligned by DP matching and that for sentence alignment uses similarities in articles aligned by CLIR.\n", "They enhance each other to improve the accuracy of alignment.\n", "Using these measures, we have successfully constructed a large-scale article and sentence alignment corpus available to the public.\n", "We build an automatically sentence aligned Japanese/English Yomiuri newspaper corpus consisting of 180K sentence pairs.\n", "We use the BM25 similarity measure.\n"]}
{"article_lines": ["Indirect HMM based Hypothesis Alignment for Combining Outputs from Machine Translation Systems", "This paper presents a new hypothesis alignment method for combining outputs of multiple machine translation MT systems .", "An indirect hidden Markov model IHMM is proposed to address the synonym matching and word ordering issues in hypothesis alignment .", "Unlike traditional HMMs whose parameters are trained via maximum likelihood estimation MLE , the of the IHMM are estimated a variety of sources including word semantic similarity , word surface similarity , and a distance based distortion penalty .", "The IHMM based method significantly outperforms the state of the art TER based alignment model in our experiments on NIST benchmark datasets .", "Our combined SMT system using the proposed method achieved the best Chinese to English translation result in the constrained training track of the", "System combination has been applied successfully to various machine translation tasks .", "Recently , confusion network based system combination algorithms have been developed to combine outputs of multiple machine translation MT systems to form a consensus output Bangalore , et al . 2001 , Matusov et al . , 2006 , Rosti et al . , 2007 , Sim et al . , 2007 .", "A confusion network comprises a sequence of sets of alternative words , possibly including null s , with associated scores .", "The consensus output is then derived by selecting one word from each set of alternatives , to produce the sequence with the best overall score , which could be assigned in various ways such as by voting , by using posterior probability estimates , or by using a combination of these measures and other features .", "Constructing a confusion network requires choosing one of the hypotheses as the backbone also called skeleton in the literature , and other hypotheses are aligned to it at the word level .", "High quality hypothesis alignment is crucial to the performance of the resulting system combination .", "However , there are two challenging issues that make MT hypothesis alignment difficult .", "First , different hypotheses may use different synonymous words to express the same meaning , and these synonyms need to be aligned to each other .", "Second , correct translations may have different word orderings in different hypotheses and these words need to be properly reordered in hypothesis alignment .", "In this paper , we propose an indirect hidden Markov model IHMM for MT hypothesis alignment .", "The HMM provides a way to model both synonym matching and word ordering .", "Unlike traditional HMMs whose parameters are trained via maximum likelihood estimation MLE , the parameters of the IHMM are estimated indirectly from a variety of sources including word semantic similarity , word surface similarity , and a distancebased distortion penalty , without using large amount of training data .", "Our combined SMT system using the proposed method gave the best result on the Chinese to English test in the constrained training track of the 2008 NIST Open MT Evaluation MT08 .", "The current state of the art is confusion networkbased MT system combination as described by Rosti and colleagues Rosti et al . , 2007a , Rosti et al . , 2007b .", "The major steps are illustrated in Figure 1 .", "In Fig .", "1 a , hypotheses from different MT systems are first collected .", "Then in Fig .", "1 b , one of the hypotheses is selected as the backbone for hypothesis alignment .", "This is usually done by a sentence level minimum Bayes risk MBR method which selects a hypothesis that has the minimum average distance compared to all hypotheses .", "The backbone determines the word order of the combined output .", "Then as illustrated in Fig .", "1 c , all other hypotheses are aligned to the backbone .", "Note that in Fig .", "1 c the symbol \u03b5 denotes a null word , which is inserted by the alignment normalization algorithm described in section 3 . 4 .", "Fig .", "1 c also illustrates the handling of synonym alignment e . g . , aligning car to sedan , and word re ordering of the hypothesis .", "Then in Fig .", "1 d , a confusion network is constructed based on the aligned hypotheses , which consists of a sequence of sets in which each word is aligned to a list of alternative words including null in the same set .", "Then , a set of global and local features are used to decode the confusion network .", "In confusion network based system combination for SMT , a major difficulty is aligning hypotheses to the backbone .", "One possible statistical model for word alignment is the HMM , which has been widely used for bilingual word alignment Vogel et al . , 1996 , Och and Ney , 2003 .", "In this paper , we propose an indirect HMM method for monolingual hypothesis alignment .", "Let denote the backbone , ' _ , . . . , dj a hypothesis to be aligned to e ; , and the alignment that specifies the position of the backbone word aligned to each hypothesis word .", "We treat each word in the backbone as an HMM state and the words in the hypothesis as the observation sequence .", "We use a first order HMM , assuming that the emission probability depends only on the backbone word , and the transition probability p aj I aj_ , , I depends only on the position of the last state and the length of the backbone .", "Treating the alignment as hidden variable , the conditional probability that the hypothesis is generated by the backbone is given by As in HMM based bilingual word alignment Och and Ney , 2003 , we also associate a null with each backbone word to allow generating hypothesis words that do not align to any backbone word .", "In HMM based hypothesis alignment , emission probabilities model the similarity between a backbone word and a hypothesis word , and will be referred to as the similarity model .", "The transition probabilities model word reordering , and will be called the distortion model .", "The similarity model , which specifies the emission probabilities of the HMM , models the similarity between a backbone word and a hypothesis word .", "Since both words are in the same language , the similarity model can be derived based on both semantic similarity and surface similarity , and the overall similarity model is a linear interpolation of the two where and reflect the semantic and surface similarity between and e ; , respectively , and \u03b1 is the interpolation factor .", "Since the semantic similarity between two target words is source dependent , the semantic similarity model is derived by using the source word sequence as a hidden layer where is the source sentence .", "Moreover , in order to handle the case that two target words are synonyms but neither of them has counter part in the source sentence , a null is introduced on the source side , which is represented by f0 .", "The last step in 3 assumes that first ei generates all source words including null .", "Then ej is generated by all source words including null .", "In the common SMT scenario where a large amount of bilingual parallel data is available , we can estimate the translation probabilities from a source word to a target word and vice versa via conventional bilingual word alignment .", "Then both p fk I e ; and in 3 can be derived where is the translation model from the source to target word alignment model , and p fk I e ; , which enforces the sum to 1 constraint over all words in the source sentence , takes the following form , where A2s fk I e ; is the translation model from the target to source word alignment model .", "In our method , A2s null I e ; for all target words is simply a constant pnull , whose value is optimized on held out data 1 .", "The surface similarity model can be estimated in several ways .", "A very simple model could be based on exact match the surface similarity model , per , , .", "elj I , would take the value 1 . 0 if e e , and 0 otherwise 2 .", "However , a smoothed surface similarity model is used in our method .", "If the target language uses alphabetic orthography , as English does , we treat words as letter sequences and the similarity measure can be the length of the longest matched prefix LMP or the length of the longest common subsequence LCS between them .", "Then , this raw similarity measure is transformed to a surface similarity score between 0 and 1 through an exponential mapping , where is computed as and is the raw similarity measure of ej ei , which is the length of the LMP or LCS of ej and ei . and p is a smoothing factor that characterizes the mapping , Thus as p approaches infinity , backs off to the exact match model .", "We found the smoothed similarity model of 4 yields slightly better results than the exact match model .", "Both LMP and LCS based methods achieve similar performance but the computation of LMP is faster .", "Therefore , we only report results of the LMP based smoothed similarity model .", "The distortion model , which specifies the transition probabilities of the HMM , models the first order dependencies of word ordering .", "In bilingual HMM based word alignment , it is commonly assumed that transition probabilities Following Och and Ney 2003 , we use a fixed value p0 for the probability of jumping to a null state , which can be optimized on held out data , and the overall distortion model becomes As suggested by Liang et al . 2006 , we can group the distortion parameters c d , d i i' , into a few buckets .", "In our implementation , 11 buckets are used for c 4 , c 3 , . . . c 0 , . . . , c 5 , c 6 .", "The probability mass for transitions with jump distance larger than 6 and less than 4 is uniformly divided .", "By doing this , only a handful of c d parameters need to be estimated .", "Although it is possible to estimate them using the EM algorithm on a small development set , we found that a particularly simple model , described below , works surprisingly well in our experiments .", "Since both the backbone and the hypothesis are in the same language , It seems intuitive that the distortion model should favor monotonic alignment and only allow non monotonic alignment with a certain penalty .", "This leads us to use a distortion model of the following form , where K is a tuning factor optimized on held out data .", "As shown in Fig .", "2 , the value of distortion score peaks at d 1 , i . e . , the monotonic alignment , and decays for non monotonic alignments depending on how far it diverges from the monotonic alignment .", "Given an HMM , the Viterbi alignment algorithm can be applied to find the best alignment between the backbone and the hypothesis , However , the alignment produced by the algorithm cannot be used directly to build a confusion network .", "There are two reasons for this .", "First , the alignment produced may contain 1 N mappings between the backbone and the hypothesis whereas 1 1 mappings are required in order to build a confusion network .", "Second , if hypothesis words are aligned to a null in the backbone or vice versa , we need to insert actual nulls into the right places in the hypothesis and the backbone , respectively .", "Therefore , we need to normalize the alignment produced by Viterbi search .", "First , whenever more than one hypothesis words are aligned to one backbone word , we keep the link which gives the highest occupation probability computed via the forward backward algorithm .", "The other hypothesis words originally aligned to the backbone word will be aligned to the null associated with that backbone word .", "Second , for the hypothesis words that are aligned to a particular null on the backbone side , a set of nulls are inserted around that backbone word associated with the null such that no links cross each other .", "As illustrated in Fig .", "3 a , if a hypothesis word e2 is aligned to the backbone word e2 , a null is inserted in front of the backbone word e2 linked to the hypothesis word e1 that comes before e2 .", "Nulls are also inserted for other hypothesis words such as e3 and e4 after the backbone word e2 .", "If there is no hypothesis word aligned to that backbone word , all nulls are inserted after that backbone word . 3 For a backbone word that is aligned to no hypothesis word , a null is inserted on the hypothesis side , right after the hypothesis word which is aligned to the immediately preceding backbone word .", "An example is shown in Fig .", "3 b .", "The two main hypothesis alignment methods for system combination in the previous literature are GIZA and TER based methods .", "Matusov et al . 2006 proposed using GIZA to align words between different MT hypotheses , where all hypotheses of the test corpus are collected to create hypothesis pairs for GIZA training .", "This approach uses the conventional HMM model bootstrapped from IBM Model 1 as implemented in GIZA , and heuristically combines results from aligning in both directions .", "System combination based on this approach gives an improvement over the best single system .", "However , the number of hypothesis pairs for training is limited by the size of the test corpus .", "Also , MT hypotheses from the same source sentence are correlated with each other and these hypothesis pairs are not i . i . d . data samples .", "Therefore , GIZA training on such a data set may be unreliable .", "Bangalore et al . 2001 used a multiple stringmatching algorithm based on Levenshtein edit distance , and later Sim et al .", "2007 and Rosti et al . 2007 extended it to a TER based method for hypothesis alignment .", "TER Snover et al . , 2006 measures the minimum number of edits , including substitution , insertion , deletion , and shift of blocks of words , that are needed to modify a hypothesis so that it exactly matches the other hypothesis .", "The best alignment is the one that gives the minimum number of translation edits .", "TER based confusion network construction and system combination has demonstrated superior performance on various large scale MT tasks Rosti . et al , 2007 .", "However , when searching for the optimal alignment , the TER based method uses a strict surface hard match for counting edits .", "Therefore , it is not able to handle synonym matching well .", "Moreover , although TER based alignment allows phrase shifts to accommodate the non monotonic word ordering , all non monotonic shifts are penalized equally no matter how short or how long the move is , and this penalty is set to be the same as that for substitution , deletion , and insertion edits .", "Therefore , its modeling of non monotonic word ordering is very coarse grained .", "In contrast to the GIZA based method , our IHMM based method has a similarity model estimated using bilingual word alignment HMMs that are trained on a large amount of bi text data .", "Moreover , the surface similarity information is explicitly incorporated in our model , while it is only used implicitly via parameter initialization for IBM Model 1 training by Matusov et al . 2006 .", "On the other hand , the TER based alignment model is similar to a coarse grained , nonnormalized version of our IHMM , in which the similarity model assigns no penalty to an exact surface match and a fixed penalty to all substitutions , insertions , and deletions , and the distortion model simply assigns no penalty to a monotonic jump , and a fixed penalty to all other jumps , equal to the non exact match penalty in the similarity model .", "There have been other hypothesis alignment methods .", "Karakos , et al . 2008 proposed an ITGbased method for hypothesis alignment , Rosti et al .", "2008 proposed an incremental alignment method , and a heuristic based matching algorithm was proposed by Jayaraman and Lavie 2005 .", "In this section , we evaluate our IHMM based hypothesis alignment method on the Chinese toEnglish C2E test in the constrained training track of the 2008 NIST Open MT Evaluation NIST , 2008 .", "We compare to the TER based method used by Rosti et al . 2007 .", "In the following experiments , the NIST BLEU score is used as the evaluation metric Papineni et al . , 2002 , which is reported as a percentage in the following sections .", "In our implementation , the backbone is selected with MBR .", "Only the top hypothesis from each single system is considered as a backbone .", "A uniform posteriori probability is assigned to all hypotheses .", "TER is used as loss function in the MBR computation .", "Similar to Rosti et al . , 2007 , each word in the confusion network is associated with a word posterior probability .", "Given a system S , each of its hypotheses is assigned with a rank based score of 1 1 r \u03b7 , where r is the rank of the hypothesis , and \u03b7 is a rank smoothing parameter .", "The system specific rank based score of a word w for a given system S is the sum of all the rank based scores of the hypotheses in system S that contain the word w at the given position after hypothesis alignment .", "This score is then normalized by the sum of the scores of all the alternative words at the same position and from the same system S to generate the system specific word posterior .", "Then , the total word posterior of w over all systems is a sum of these system specific posteriors weighted by system weights .", "Beside the word posteriors , we use language model scores and a word count as features for confusion network decoding .", "Therefore , for an M way system combination that uses N LMs , a total of M N 1 decoding parameters , including M 1 system weights , one rank smoothing factor , N language model weights , and one weight for the word count feature , are optimized using Powell s method Brent , 1973 to maximize BLEU score on a development set4 .", "Two language models are used in our experiments .", "One is a trigram model estimated from the English side of the parallel training data , and the other is a 5 gram model trained on the English GigaWord corpus from LDC using the MSRLM toolkit Nguyen et al , 2007 .", "4 The parameters of IHMM are not tuned by maximum BLEU training .", "In order to reduce the fluctuation of BLEU scores caused by the inconsistent translation output length , an unsupervised length adaptation method has been devised .", "We compute an expected length ratio between the MT output and the source sentences on the development set after maximumBLEU training .", "Then during test , we adapt the length of the translation output by adjusting the weight of the word count feature such that the expected output source length ratio is met .", "In our experiments , we apply length adaptation to the system combination output at the level of the whole test corpus .", "The development dev set used for system combination parameter training contains 1002 sentences sampled from the previous NIST MT Chinese to English test sets 35 from MT04 , 55 from MT05 , and 10 from MT06 newswire .", "The test set is the MT08 Chinese to English current test set , which includes 1357 sentences from both newswire and web data genres .", "Both dev and test sets have four references per sentence .", "As inputs to the system combination , 10 best hypotheses for each source sentence in the dev and test sets are collected from each of the eight single systems .", "All outputs on the MT08 test set were true cased before scoring using a log linear conditional Markov model proposed by Toutanova et al . 2008 .", "However , to save computation effort , the results on the dev set are reported in case insensitive BLEU ciBLEU score instead .", "In our main experiments , outputs from a total of eight single MT systems were combined .", "As listed in Table 1 , Sys 1 is a tree to string system proposed by Quirk et al . , 2005 ; Sys 2 is a phrasebased system with fast pruning proposed by Moore and Quirk 2008 ; Sys 3 is a phrase based system with syntactic source reordering proposed by Wang et al . 2007a ; Sys 4 is a syntax based preordering system proposed by Li et . al .", "2007 ; Sys5 is a hierarchical system proposed by Chiang 2007 ; Sys 6 is a lexicalized re ordering system proposed by Xiong et al . 2006 ; Sys 7 is a twopass phrase based system with adapted LM proposed by Foster and Kuhn 2007 ; and Sys 8 is a hierarchical system with two pass rescoring using a parser based LM proposed by Wang et al . , 2007b .", "All systems were trained within the confines of the constrained training condition of NIST MT08 evaluation .", "These single systems are optimized with maximum BLEU training on different subsets of the previous NIST MT test data .", "The bilingual translation models used to compute the semantic similarity are from the worddependent HMMs proposed by He 2007 , which are trained on two million parallel sentence pairs selected from the training corpus allowed by the constrained training condition of MT08 .", "In the IHMM based method , the smoothing factor for surface similarity model is set to \u03c1 3 , the interpolation factor of the overall similarity model is set to \u03b1 0 . 3 , and the controlling factor of the distance based distortion parameters is set to K 2 .", "These settings are optimized on the dev set .", "Individual system results and system combination results using both IHMM and TER alignment , on both the dev and test sets , are presented in Table 1 .", "The TER based hypothesis alignment tool used in our experiments is the publicly available TER Java program , TERCOM Snover et al . , 2006 .", "Default settings of TERCOM are used in the following experiments .", "On the dev set , the case insensitive BLEU score of the IHMM based 8 way system combination output is about 5 . 8 points higher than that of the best single system .", "Compared to the TER based method , the IHMM based method is about 1 . 5 BLEU points better .", "On the MT08 test set , the IHMM based system combination gave a case sensitive BLEU score of 30 . 89 .", "It outperformed the best single system by 4 . 7 BLEU points and the TER based system combination by 1 . 0 BLEU points .", "Note that the best single system on the dev set and the test set are different .", "The different single systems are optimized on different tuning sets , so this discrepancy between dev set and test set results is presumably due to differing degrees of mismatch between the dev and test sets and the various tuning sets .", "In order to evaluate how well our method performs when we combine more systems , we collected MT outputs on MT08 from seven additional single systems as summarized in Table 2 .", "These systems belong to two groups .", "Sys 9 to Sys 12 are in the first group .", "They are syntaxaugmented hierarchical systems similar to those described by Shen et al . 2008 using different Chinese word segmentation and language models .", "The second group has Sys 13 to Sys 15 .", "Sys 13 is a phrasal system proposed by Koehn et al . 2003 , Sys 14 is a hierarchical system proposed by Chiang 2007 , and Sys 15 is a syntax based system proposed by Galley et al .", "All seven systems were trained within the confines of the constrained training condition of NIST MT08 evaluation .", "We collected 10 best MT outputs only on the MT08 test set from these seven extra systems .", "No MT outputs on our dev set are available from them at present .", "Therefore , we directly adopt system combination parameters trained for the previous 8way system combination , except the system weights , which are re set by the following heuristics First , the total system weight mass 1 . 0 is evenly divided among the three groups of single systems Sys 1 8 , Sys 9 12 , and Sys13 15 .", "Each group receives a total system weight mass of 1 3 .", "Then the weight mass is further divided in each group in the first group , the original weights of systems 1 8 are multiplied by 1 3 ; in the second and third groups , the weight mass is evenly distributed within the group , i . e . , 1 12 for each system in group 2 , and 1 9 for each system in group 35 .", "Length adaptation is applied to control the final output length , where the same expected length ratio of the previous 8 way system combination is adopted .", "The results of the 15 way system combination are presented in Table 3 .", "It shows that the IHMMbased method is still about 1 BLEU point better than the TER based method .", "Moreover , combining 15 single systems gives an output that has a NIST BLEU score of 34 . 82 , which is 3 . 9 points better than the best submission to the NIST MT08 constrained training track NIST , 2008 .", "To our knowledge , this is the best result reported on this task .", "In this section , we evaluate the effect of the semantic similarity model and the surface similarity model by varying the interpolation weight \u03b1 of 2 .", "The results on both the dev and test sets are reported in Table 4 .", "In one extreme case , \u03b1 1 , the overall similarity model is based only on semantic similarity .", "This gives a case insensitive BLEU score of 41 . 70 and a case sensitive BLEU score of 28 . 92 on the dev and test set , respectively .", "The accuracy is significantly improved to 43 . 62 on the dev set and 30 . 89 on test set when \u03b1 0 . 3 .", "In another extreme case , \u03b1 0 , in which only the surface similarity model is used for the overall similarity model , the performance degrades by about 0 . 2 point .", "Therefore , the surface similarity information seems more important for monolingual hypothesis alignment , but both sub models are useful .", "We investigate the effect of the distance based distortion model by varying the controlling factor K in 6 .", "For example , setting K 1 . 0 gives a lineardecay distortion model , and setting K 2 . 0 gives a quadratic smoothed distance based distortion model .", "As shown in Table 5 , the optimal result can be achieved using a properly smoothed distancebased distortion model .", "Synonym matching and word ordering are two central issues for hypothesis alignment in confusion network based MT system combination .", "In this paper , an IHMM based method is proposed for hypothesis alignment .", "It uses a similarity model for synonym matching and a distortion model for word ordering .", "In contrast to previous methods , the similarity model explicitly incorporates both semantic and surface word similarity , which is critical to monolingual word alignment , and a smoothed distance based distortion model is used to model the first order dependency of word ordering , which is shown to be better than simpler approaches .", "Our experimental results show that the IHMMbased hypothesis alignment method gave superior results on the NIST MT08 C2E test set compared to the TER based method .", "Moreover , we show that our system combination method can scale up to combining more systems and produce a better output that has a case sensitive BLEU score of 34 . 82 , which is 3 . 9 BLEU points better than the best official submission of MT08 .", "The authors are grateful to Chris Quirk , Arul Menezes , Kristina Toutanova , William Dolan , Mu Li , Chi Ho Li , Dongdong Zhang , Long Jiang , Ming Zhou , George Foster , Roland Kuhn , Jing Zheng , Wen Wang , Necip Fazil Ayan , Dimitra Vergyri , Nicolas Scheffer , Andreas Stolcke , Kevin Knight , Jens Soenke Voeckler , Spyros Matsoukas , and Antti Veikko Rosti for assistance with the MT systems and or for the valuable suggestions and discussions ."], "summary_lines": ["Indirect-HMM-based Hypothesis Alignment for Combining Outputs from Machine Translation Systems\n", "This paper presents a new hypothesis alignment method for combining outputs of multiple machine translation (MT) systems.\n", "An indirect hidden Markov model (IHMM) is proposed to address the synonym matching and word ordering issues in hypothesis alignment.\n", "Unlike traditional HMMs whose parameters are trained via maximum likelihood estimation (MLE), the parameters of the IHMM are estimated indirectly from a variety of sources including word semantic similarity, word surface similarity, and a distance-based distortion penalty.\n", "The IHMM-based method significantly outperforms the state-of-the-art TER-based alignment model in our experiments on NIST benchmark datasets.\n", "Our combined SMT system using the proposed method achieved the best Chinese-to-English translation result in the constrained training track of the 2008 NIST Open MT Evaluation.\n", "we propose using an indirect hidden Markov model (IHMM) for pairwise alignment of system outputs.\n"]}
{"article_lines": ["Learning Extraction Patterns For Subjective Expressions", "This paper presents a bootstrapping process that learns linguistically rich extraction patterns for subjective opinionated expressions .", "High precision classifiers label unannotated data to automatically create a large training set , which is then given to an extraction pattern learning algorithm .", "The learned patterns are then used to identify more subjective sentences .", "The bootstrapping process learns many subjective patterns and increases recall while maintaining high precision .", "Many natural language processing applications could benefit from being able to distinguish between factual and subjective information .", "Subjective remarks come in a variety of forms , including opinions , rants , allegations , accusations , suspicions , and speculations .", "Ideally , information extraction systems should be able to distinguish between factual information which should be extracted and non factual information which should be discarded or labeled as uncertain .", "Question answering systems should distinguish between factual and speculative answers .", "Multi perspective question answering aims to present multiple answers to the user based upon speculation or opinions derived from different sources .", "Multidocument summarization systems need to summarize different opinions and perspectives .", "Spam filtering systems must recognize rants and emotional tirades , among other things .", "In general , nearly any system that seeks to identify information could benefit from being able to separate factual and subjective information .", "Some existing resources contain lists of subjective words e . g . , Levin s desire verbs 1993 , and some empirical methods in NLP have automatically identified adjectives , verbs , and N grams that are statistically associated with subjective language e . g . , Turney , 2002 ; Hatzivassiloglou and McKeown , 1997 ; Wiebe , 2000 ; Wiebe et al . , 2001 .", "However , subjective language can be exhibited by a staggering variety of words and phrases .", "In addition , many subjective terms occur infrequently , such as strongly subjective adjectives e . g . , preposterous , unseemly and metaphorical or idiomatic phrases e . g . , dealt a blow , swept off one s feet .", "Consequently , we believe that subjectivity learning systems must be trained on extremely large text collections before they will acquire a subjective vocabulary that is truly broad and comprehensive in scope .", "To address this issue , we have been exploring the use of bootstrapping methods to allow subjectivity classifiers to learn from a collection of unannotated texts .", "Our research uses high precision subjectivity classifiers to automatically identify subjective and objective sentences in unannotated texts .", "This process allows us to generate a large set of labeled sentences automatically .", "The second emphasis of our research is using extraction patterns to represent subjective expressions .", "These patterns are linguistically richer and more flexible than single words or N grams .", "Using the automatically labeled sentences as training data , we apply an extraction pattern learning algorithm to automatically generate patterns representing subjective expressions .", "The learned patterns can be used to automatically identify more subjective sentences , which grows the training set , and the entire process can then be bootstrapped .", "Our experimental results show that this bootstrapping process increases the recall of the highprecision subjective sentence classifier with little loss in precision .", "We also find that the learned extraction patterns capture subtle connotations that are more expressive than the individual words by themselves .", "This paper is organized as follows .", "Section 2 discusses previous work on subjectivity analysis and extraction pattern learning .", "Section 3 overviews our general approach , describes the high precision subjectivity classifiers , and explains the algorithm for learning extraction patterns associated with subjectivity .", "Section 4 describes the data that we use , presents our experimental results , and shows examples of patterns that are learned .", "Finally , Section 5 summarizes our findings and conclusions .", "Much previous work on subjectivity recognition has focused on document level classification .", "For example , Spertus , 1997 developed a system to identify inflammatory texts and Turney , 2002 ; Pang et al . , 2002 developed methods for classifying reviews as positive or negative .", "Some research in genre classification has included the recognition of subjective genres such as editorials e . g . , Karlgren and Cutting , 1994 ; Kessler et al . , 1997 ; Wiebe et al . , 2001 .", "In contrast , the goal of our work is to classify individual sentences as subjective or objective .", "Document level classification can distinguish between subjective texts , such as editorials and reviews , and objective texts , such as newspaper articles .", "But in reality , most documents contain a mix of both subjective and objective sentences .", "Subjective texts often include some factual information .", "For example , editorial articles frequently contain factual information to back up the arguments being made , and movie reviews often mention the actors and plot of a movie as well as the theatres where it s currently playing .", "Even if one is willing to discard subjective texts in their entirety , the objective texts usually contain a great deal of subjective information in addition to facts .", "For example , newspaper articles are generally considered to be relatively objective documents , but in a recent study Wiebe et al . , 2001 44 of sentences in a news collection were found to be subjective after editorial and review articles were removed .", "One of the main obstacles to producing a sentencelevel subjectivity classifier is a lack of training data .", "To train a document level classifier , one can easily find collections of subjective texts , such as editorials and reviews .", "For example , Pang et al . , 2002 collected reviews from a movie database and rated them as positive , negative , or neutral based on the rating e . g . , number of stars given by the reviewer .", "It is much harder to obtain collections of individual sentences that can be easily identified as subjective or objective .", "Previous work on sentence level subjectivity classification Wiebe et al . , 1999 used training corpora that had been manually annotated for subjectivity .", "Manually producing annotations is time consuming , so the amount of available annotated sentence data is relatively small .", "The goal of our research is to use high precision subjectivity classifiers to automatically identify subjective and objective sentences in unannotated text corpora .", "The high precision classifiers label a sentence as subjective or objective when they are confident about the classification , and they leave a sentence unlabeled otherwise .", "Unannotated texts are easy to come by , so even if the classifiers can label only 30 of the sentences as subjective or objective , they will still produce a large collection of labeled sentences .", "Most importantly , the high precision classifiers can generate a much larger set of labeled sentences than are currently available in manually created data sets .", "Information extraction IE systems typically use lexicosyntactic patterns to identify relevant information .", "The specific representation of these patterns varies across systems , but most patterns represent role relationships surrounding noun and verb phrases .", "For example , an IE system designed to extract information about hijackings might use the pattern hijacking of x , which looks for the noun hijacking and extracts the object of the preposition of as the hijacked vehicle .", "The pattern x was hijacked would extract the hijacked vehicle when it finds the verb hijacked in the passive voice , and the pattern x hijacked would extract the hijacker when it finds the verb hijacked in the active voice .", "One of our hypotheses was that extraction patterns would be able to represent subjective expressions that have noncompositional meanings .", "For example , consider the common expression drives someone up the wall , which expresses the feeling of being annoyed with something .", "The meaning of this expression is quite different from the meanings of its individual words drives , up , wall .", "Furthermore , this expression is not a fixed word sequence that could easily be captured by N grams .", "It is a relatively flexible construction that may be more generally represented as x drives y up the wall , where x and y may be arbitrary noun phrases .", "This pattern would match many different sentences , such as George drives me up the wall , She drives the mayor up the wall , or The nosy old man drives his quiet neighbors up the wall . We also wondered whether the extraction pattern representation might reveal slight variations of the same verb or noun phrase that have different connotations .", "For example , you can say that a comedian bombed last night , which is a subjective statement , but you can t express this sentiment with the passive voice of bombed .", "In Section 3 . 2 , we will show examples of extraction patterns representing subjective expressions which do in fact exhibit both of these phenomena .", "A variety of algorithms have been developed to automatically learn extraction patterns .", "Most of these algorithms require special training resources , such as texts annotated with domain specific tags e . g . , AutoSlog Riloff , 1993 , CRYSTAL Soderland et al . , 1995 , RAPIER Califf , 1998 , SRV Freitag , 1998 , WHISK Soderland , 1999 or manually defined keywords , frames , or object recognizers e . g . , PALKA Kim and Moldovan , 1993 and LIEP Huffman , 1996 .", "AutoSlog TS Riloff , 1996 takes a different approach , requiring only a corpus of unannotated texts that have been separated into those that are related to the target domain the relevant texts and those that are not the irrelevant texts .", "Most recently , two bootstrapping algorithms have been used to learn extraction patterns .", "Metabootstrapping Riloff and Jones , 1999 learns both extraction patterns and a semantic lexicon using unannotated texts and seed words as input .", "ExDisco Yangarber et al . , 2000 uses a bootstrapping mechanism to find new extraction patterns using unannotated texts and some seed patterns as the initial input .", "For our research , we adopted a learning process very similar to that used by AutoSlog TS , which requires only relevant texts and irrelevant texts as its input .", "We describe this learning process in more detail in the next section .", "We have developed a bootstrapping process for subjectivity classification that explores three ideas 1 highprecision classifiers can be used to automatically identify subjective and objective sentences from unannotated texts , 2 this data can be used as a training set to automatically learn extraction patterns associated with subjectivity , and 3 the learned patterns can be used to grow the training set , allowing this entire process to be bootstrapped .", "Figure 1 shows the components and layout of the bootstrapping process .", "The process begins with a large collection of unannotated text and two high precision subjectivity classifiers .", "One classifier searches the unannotated corpus for sentences that can be labeled as subjective with high confidence , and the other classifier searches for sentences that can be labeled as objective with high confidence .", "All other sentences in the corpus are left unlabeled .", "The labeled sentences are then fed to an extraction pattern learner , which produces a set of extraction patterns that are statistically correlated with the subjective sentences we will call these the subjective patterns .", "These patterns are then used to identify more sentences within the unannotated texts that can be classified as subjective .", "The extraction pattern learner can then retrain using the larger training set and the process repeats .", "The subjective patterns can also be added to the highprecision subjective sentence classifier as new features to improve its performance .", "The dashed lines in Figure 1 represent the parts of the process that are bootstrapped .", "In this section , we will describe the high precision sentence classifiers , the extraction pattern learning process , and the details of the bootstrapping process .", "The high precision classifiers HP Subj and HP Obj use lists of lexical items that have been shown in previous work to be good subjectivity clues .", "Most of the items are single words , some are N grams , but none involve syntactic generalizations as in the extraction patterns .", "Any data used to develop this vocabulary does not overlap with the test sets or the unannotated data used in this paper .", "Many of the subjective clues are from manually developed resources , including entries from Levin , 1993 ; Ballmer and Brennenstuhl , 1981 , Framenet lemmas with frame element experiencer Baker et al . , 1998 , adjectives manually annotated for polarity Hatzivassiloglou and McKeown , 1997 , and subjectivity clues listed in Wiebe , 1990 .", "Others were derived from corpora , including subjective nouns learned from unannotated data using bootstrapping Riloff et al . , 2003 .", "The subjectivity clues are divided into those that are strongly subjective and those that are weakly subjective , using a combination of manual review and empirical results on a small training set of manually annotated data .", "As the terms are used here , a strongly subjective clue is one that is seldom used without a subjective meaning , whereas a weakly subjective clue is one that commonly has both subjective and objective uses .", "The high precision subjective classifier classifies a sentence as subjective if it contains two or more of the strongly subjective clues .", "On a manually annotated test set , this classifier achieves 91 . 5 precision and 31 . 9 recall that is , 91 . 5 of the sentences that it selected are subjective , and it found 31 . 9 of the subjective sentences in the test set .", "This test set consists of 2197 sentences , 59 of which are subjective .", "The high precision objective classifier takes a different approach .", "Rather than looking for the presence of lexical items , it looks for their absence .", "It classifies a sentence as objective if there are no strongly subjective clues and at most one weakly subjective clue in the current , previous , and next sentence combined .", "Why doesn t the objective classifier mirror the subjective classifier , and consult its own list of strongly objective clues ?", "There are certainly lexical items that are statistically correlated with the objective class examples are cardinal numbers Wiebe et al . , 1999 , and words such as per , case , market , and total , but the presence of such clues does not readily lead to high precision objective classification .", "Add sarcasm or a negative evaluation to a sentence about a dry topic such as stock prices , and the sentence becomes subjective .", "Conversely , add objective topics to a sentence containing two strongly subjective words such as odious and scumbag , and the sentence remains subjective .", "The performance of the high precision objective classifier is a bit lower than the subjective classifier 82 . 6 precision and 16 . 4 recall on the test set mentioned above that is , 82 . 6 of the sentences selected by the objective classifier are objective , and the objective classifier found 16 . 4 of the objective sentences in the test set .", "Although there is room for improvement , the performance proved to be good enough for our purposes .", "To automatically learn extraction patterns that are associated with subjectivity , we use a learning algorithm similar to AutoSlog TS Riloff , 1996 .", "For training , AutoSlogTS uses a text corpus consisting of two distinct sets of texts relevant texts in our case , subjective sentences and irrelevant texts in our case , objective sentences .", "A set of syntactic templates represents the space of possible extraction patterns .", "The learning process has two steps .", "First , the syntactic templates are applied to the training corpus in an exhaustive fashion , so that extraction patterns are generated for literally every possible instantiation of the templates that appears in the corpus .", "The left column of Figure 2 shows the syntactic templates used by AutoSlog TS .", "The right column shows a specific extraction pattern that was learned during our subjectivity experiments as an instantiation of the syntactic form on the left .", "For example , the pattern subj was satisfied' will match any sentence where the verb satisfied appears in the passive voice .", "The pattern subj dealt blow represents a more complex expression that will match any sentence that contains a verb phrase with head dealt followed by a direct object with head blow .", "This would match sentences such as The experience dealt a stiff blow to his pride . It is important to recognize that these patterns look for specific syntactic constructions produced by a shallow parser , rather than exact word sequences .", "The second step of AutoSlog TS s learning process applies all of the learned extraction patterns to the training corpus and gathers statistics for how often each pattern occurs in subjective versus objective sentences .", "AutoSlog TS then ranks the extraction patterns using a metric called RlogF Riloff , 1996 and asks a human to review the ranked list and make the final decision about which patterns to keep .", "In contrast , for this work we wanted a fully automatic process that does not depend on a human reviewer , and we were most interested in finding patterns that can identify subjective expressions with high precision .", "So we ranked the extraction patterns using a conditional probability measure the probability that a sentence is subjective given that a specific extraction pattern appears in it .", "The exact formula is where subjfreq patterni is the frequency of patterni in subjective training sentences , and freq patterni is the frequency of patterni in all training sentences .", "This may also be viewed as the precision of the pattern on the training data .", "Finally , we use two thresholds to select extraction patterns that are strongly associated with subjectivity in the training data .", "We choose extraction patterns for which freq patterni 01 and Pr subjective patterni 02 .", "Figure 3 shows some patterns learned by our system , the frequency with which they occur in the training data FREQ and the percentage of times they occur in subjective sentences SUBJ .", "For example , the first two rows show the behavior of two similar expressions using the verb asked .", "100 of the sentences that contain asked in the passive voice are subjective , but only 63 of the sentences that contain asked in the active voice are subjective .", "A human would probably not expect the active and passive voices to behave so differently .", "To understand why this is so , we looked in the training data and found that the passive voice is often used to query someone about a specific opinion .", "For example , here is one such sentence from our training set Ernest Bai Koroma of RITCORP was asked to address his supporters on his views relating to full blooded Temne to head APC . In contrast , many of the sentences containing asked in the active voice are more general in nature , such as The mayor asked a newly formed JR about his petition . Figure 3 also shows that expressions using talk as a noun e . g . , Fred is the talk of the town are highly correlated with subjective sentences , while talk as a verb e . g . , The mayor will talk about . . . are found in a mix of subjective and objective sentences .", "Not surprisingly , longer expressions tend to be more idiomatic and subjective than shorter expressions e . g . , put an end to vs . put ; is going to be vs . is going ; was expectedfrom vs . was expected .", "Finally , the last two rows of Figure 3 show that expressions involving the noun fact are highly correlated with subjective expressions !", "These patterns match sentences such as The fact is . . . and . . . is a fact , which apparently are often used in subjective contexts .", "This example illustrates that the corpus based learning method can find phrases that might not seem subjective to a person intuitively , but that are reliable indicators of subjectivity .", "The text collection that we used consists of Englishlanguage versions of foreign news documents from FBIS , the U . S . Foreign Broadcast Information Service .", "The data is from a variety of countries .", "Our system takes unannotated data as input , but we needed annotated data to evaluate its performance .", "We briefly describe the manual annotation scheme used to create the gold standard , and give interannotator agreement results .", "In 2002 , a detailed annotation scheme Wilson and Wiebe , 2003 was developed for a government sponsored project .", "We only mention aspects of the annotation scheme relevant to this paper .", "The scheme was inspired by work in linguistics and literary theory on subjectivity , which focuses on how opinions , emotions , etc . are expressed linguistically in context Banfield , 1982 .", "The goal is to identify and characterize expressions ofprivate states in a sentence .", "Private state is a general covering term for opinions , evaluations , emotions , and speculations Quirk et al . , 1985 .", "For example , in sentence 1 the writer is expressing a negative evaluation .", "1 The time has come , gentlemen , for Sharon , the assassin , to realize that injustice cannot last long . Sentence 2 reflects the private state of Western countries .", "Mugabe s use of overwhelmingly also reflects a private state , his positive reaction to and characterization of his victory .", "2 Western countries were left frustrated and impotent after Robert Mugabe formally declared that he had overwhelmingly won Zimbabwe s presidential election . Annotators are also asked to judge the strength of each private state .", "A private state may have low , medium , high or extreme strength .", "To allow us to measure interannotator agreement , three annotators who are not authors of this paper independently annotated the same 13 documents with a total of 210 sentences .", "We begin with a strict measure of agreement at the sentence level by first considering whether the annotator marked any private state expression , of any strength , anywhere in the sentence .", "If so , the sentence is subjective .", "Otherwise , it is objective .", "The average pairwise percentage agreement is 90 and the average pairwise rc value is 0 . 77 .", "One would expect that there are clear cases of objective sentences , clear cases of subjective sentences , and borderline sentences in between .", "The agreement study supports this .", "In terms of our annotations , we define a sentence as borderline if it has at least one private state expression identified by at least one annotator , and all strength ratings of private state expressions are low .", "On average , 11 of the corpus is borderline under this definition .", "When those sentences are removed , the average pairwise percentage agreement increases to 95 and the average pairwise r . value increases to 0 . 89 .", "As expected , the majority of disagreement cases involve low strength subjectivity .", "The annotators consistently agree about which are the clear cases of subjective sentences .", "This leads us to define the gold standard that we use when evaluating our results .", "A sentence is subjective if it contains at least one private state expression of medium or higher strength .", "The second class , which we call objective , consists of everything else .", "Our pool of unannotated texts consists of 302 , 163 individual sentences .", "The BP Subj classifier initially labeled roughly 44 , 300 of these sentences as subjective , and the BP Obj classifier initially labeled roughly 17 , 000 sentences as objective .", "In order to keep the training set relatively balanced , we used all 17 , 000 objective sentences and 17 , 000 of the subjective sentences as training data for the extraction pattern learner .", "17 , 073 extraction patterns were learned that have frequency 2 and Pr subjective patterni . 60 on the training data .", "We then wanted to determine whether the extraction patterns are , in fact , good indicators of subjectivity .", "To evaluate the patterns , we applied different subsets of them to a test set to see if they consistently occur in subjective sentences .", "This test set consists of 3947 sentences , 54 of which are subjective .", "Figure 4 shows sentence recall and pattern instancelevel precision for the learned extraction patterns on the test set .", "In this figure , precision is the proportion of pattern instances found in the test set that are in subjective sentences , and recall is the proportion of subjective sentences that contain at least one pattern instance .", "We evaluated 18 different subsets of the patterns , by selecting the patterns that pass certain thresholds in the training data .", "We tried all combinations of 01 2 , 10 and 02 . 60 , . 65 , . 70 , . 75 , . 80 , . 85 , . 90 , . 95 , 1 . 0 .", "The data points corresponding to 01 2 are shown on the upper line in Figure 4 , and those corresponding to 01 10 are shown on the lower line .", "For example , the data point corresponding to 01 10 and 02 . 90 evaluates only the extraction patterns that occur at least 10 times in the training data and with a probability . 90 i . e . , at least 90 of its occurrences are in subjective training sentences .", "Overall , the extraction patterns perform quite well .", "The precision ranges from 71 to 85 , with the expected tradeoff between precision and recall .", "This experiment confirms that the extraction patterns are effective at recognizing subjective expressions .", "In our second experiment , we used the learned extraction patterns to classify previously unlabeled sentences from the unannotated text collection .", "The new subjective sentences were then fed back into the Extraction Pattern Learner to complete the bootstrapping cycle depicted by the rightmost dashed line in Figure 1 .", "The Patternbased Subjective Sentence Classifier classifies a sentence as subjective if it contains at least one extraction pattern with 01 5 and 02 1 . 0 on the training data .", "This process produced approximately 9 , 500 new subjective sentences that were previously unlabeled .", "Since our bootstrapping process does not learn new objective sentences , we did not want to simply add the new subjective sentences to the training set , or it would become increasingly skewed toward subjective sentences .", "Since HP Obj had produced roughly 17 , 000 objective sentences used for training , we used the 9 , 500 new subjective sentences along with 7 , 500 of the previously identified subjective sentences as our new training set .", "In other words , the training set that we used during the second bootstrapping cycle contained exactly the same objective sentences as the first cycle , half of the same subjective sentences as the first cycle , and 9 , 500 brand new subjective sentences .", "On this second cycle of bootstrapping , the extraction pattern learner generated many new patterns that were not discovered during the first cycle .", "4 , 248 new patterns were found that have 01 2 and 02 . 60 .", "If we consider only the strongest most subjective extraction patterns , 308 new patterns were found that had 01 10 and 02 1 . 0 .", "This is a substantial set of new extraction patterns that seem to be very highly correlated with subjectivity .", "An open question was whether the new patterns provide additional coverage .", "To assess this , we did a simple test we added the 4 , 248 new patterns to the original set of patterns learned during the first bootstrapping cycle .", "Then we repeated the same analysis that we depict in Figure 4 .", "In general , the recall numbers increased by about 2 4 while the precision numbers decreased by less , from 0 . 5 2 .", "In our third experiment , we evaluated whether the learned patterns can improve the coverage of the highprecision subjectivity classifier HP Subj , to complete the bootstrapping loop depicted in the top most dashed line of Figure 1 .", "Our hope was that the patterns would allow more sentences from the unannotated text collection to be labeled as subjective , without a substantial drop in precision .", "For this experiment , we selected the learned extraction patterns that had 01 10 and 02 1 . 0 on the training set , since these seemed likely to be the most reliable high precision indicators of subjectivity .", "We modified the HP Subj classifier to use extraction patterns as follows .", "All sentences labeled as subjective by the original HP Subj classifier are also labeled as subjective by the new version .", "For previously unlabeled sentences , the new version classifies a sentence as subjective if 1 it contains two or more of the learned patterns , or 2 it contains one of the clues used by the original HPSubj classifier and at least one learned pattern .", "Table 1 shows the performance results on the test set mentioned in Section 3 . 1 2197 sentences for both the original HPSubj classifier and the new version that uses the learned extraction patterns .", "The extraction patterns produce a 7 . 2 percentage point gain in coverage , and only a 1 . 1 percentage point drop in precision .", "This result shows that the learned extraction patterns do improve the performance ofthe high precision subjective sentence classifier , allowing it to classify more sentences as subjective with nearly the same high reliability .", "HP Subj classifier which do not overlap in non function words with any of the clues already known by the original system .", "For each pattern , we show an example sentence from our corpus that matches the pattern .", "This research explored several avenues for improving the state of the art in subjectivity analysis .", "First , we demonstrated that high precision subjectivity classification can be used to generate a large amount of labeled training data for subsequent learning algorithms to exploit .", "Second , we showed that an extraction pattern learning technique can learn subjective expressions that are linguistically richer than individual words or fixed phrases .", "We found that similar expressions may behave very differently , so that one expression may be strongly indicative of subjectivity but the other may not .", "Third , we augmented our original high precision subjective classifier with these newly learned extraction patterns .", "This bootstrapping process resulted in substantially higher recall with a minimal loss in precision .", "In future work , we plan to experiment with different configurations of these classifiers , add new subjective language learners in the bootstrapping process , and address the problem of how to identify new objective sentences during bootstrapping .", "We are very grateful to Theresa Wilson for her invaluable programming support and help with data preparation ."], "summary_lines": ["Learning Extraction Patterns For Subjective Expressions\n", "This paper presents a bootstrapping process that learns linguistically rich extraction patterns for subjective (opinionated) expressions.\n", "High-precision classifiers label unannotated data to automatically create a large training set, which is then given to an extraction pattern learning algorithm.\n", "The learned patterns are then used to identify more subjective sentences.\n", "The bootstrapping process learns many subjective patterns and increases recall while maintaining high precision.\n", "We construct a high precision classifier for contiguous sentences using the number of strong and weak subjective words in current and nearby sentences.\n", "We introduce a bootstrapping method to learn subjective extraction patterns that match specific syntactic templates using a high-precision sentence-level subjectivity classifier and a large unannotated corpus.\n"]}
{"article_lines": ["An Empirical Study of Semi supervised Structured Conditional Models for Dependency Parsing", "This paper describes an empirical study of high performance dependency parsers based on a semi supervised learning approach .", "We describe an extension of semisupervised structured conditional models SS SCMs to the dependency parsing problem , whose framework is originally proposed in Suzuki and Isozaki , 2008 .", "Moreover , we introduce two extensions related to dependency parsing The first extension is to combine SS SCMs with another semi supervised approach , described in Koo et al . , 2008 .", "The second extension is to apply the approach to secondorder parsing models , such as those described in Carreras , 2007 , using a twostage semi supervised learning approach .", "We demonstrate the effectiveness of our proposed methods on dependency parsing experiments using two widely used test collections the Penn Treebank for English , and the Prague Dependency Treebank for Czech .", "Our best results on test data in the above datasets achieve 93 . 79 parent prediction accuracy for En", "Recent work has successfully developed dependency parsing models for many languages using supervised learning algorithms Buchholz and Marsi , 2006 ; Nivre et al . , 2007 .", "Semi supervised learning methods , which make use of unlabeled data in addition to labeled examples , have the potential to give improved performance over purely supervised methods for dependency parsing .", "It is often straightforward to obtain large amounts of unlabeled data , making semi supervised approaches appealing ; previous work on semisupervised methods for dependency parsing includes Smith and Eisner , 2007 ; Koo et al . , 2008 ; Wang et al . , 2008 .", "In particular , Koo et al . 2008 describe a semi supervised approach that makes use of cluster features induced from unlabeled data , and gives state of the art results on the widely used dependency parsing test collections the Penn Treebank PTB for English and the Prague Dependency Treebank PDT for Czech .", "This is a very simple approach , but provided significant performance improvements comparing with the stateof the art supervised dependency parsers such as McDonald and Pereira , 2006 .", "This paper introduces an alternative method for semi supervised learning for dependency parsing .", "Our approach basically follows a framework proposed in Suzuki and Isozaki , 2008 .", "We extend it for dependency parsing , which we will refer to as a Semi supervised Structured Conditional Model SS SCM .", "In this framework , a structured conditional model is constructed by incorporating a series of generative models , whose parameters are estimated from unlabeled data .", "This paper describes a basic method for learning within this approach , and in addition describes two extensions .", "The first extension is to combine our method with the cluster based semi supervised method of Koo et al . , 2008 .", "The second extension is to apply the approach to second order parsing models , more specifically the model of Carreras , 2007 , using a two stage semi supervised learning approach .", "We conduct experiments on dependency parsing of English on Penn Treebank data and Czech on the Prague Dependency Treebank .", "Our experiments investigate the effectiveness of 1 the basic SS SCM for dependency parsing ; 2 a combination of the SS SCM with Koo et al . 2008 s semisupervised approach even in the case we used the same unlabeled data for both methods ; 3 the twostage semi supervised learning approach that inIn this model v1 , . . . , vk are scalar parameters that may be positive or negative ; q1 . . . qk are functions in fact , generative models , that are trained on unlabeled data .", "The vj parameters will dictate the relative strengths of the functions q1 . . . qk , and will be trained on labeled data .", "For convenience , we will use v to refer to the vector of parameters v1 . . . vk , and q to refer to the set of generative models q1 . . . qk .", "The full model is specified by values for w , v , and q .", "We will write p y x ; w , v , q to refer to the conditional distribution under parameter values w , v , q .", "We will describe a three step parameter estimation method that 1 initializes the q functions generative models to be uniform distributions , and estimates parameter values w and v from labeled data ; 2 induces new functions q1 . . . qk from unlabeled data , based on the distribution defined by the w , v , q values from step 1 ; 3 re estimates w and v on the labeled examples , keeping the q1 .", ". qk from step 2 fixed .", "The end result is a model that combines supervised training with generative models induced from unlabeled data .", "We now describe how the generative models q1 .", ". qk are defined , and how they are induced from unlabeled data .", "These models make direct use of the feature vector definition f x , y used in the original , fully supervised , dependency parser .", "The first step is to partition the d features in f x , y into k separate feature vectors , r1 x , y . . . rk x , y with the result that f is the concatenation of the k feature vectors r1 . . . rk .", "In our experiments on dependency parsing , we partitioned f into up to over 140 separate feature vectors corresponding to different feature types .", "For example , one feature vector rj might include only those features corresponding to word bigrams involved in dependencies i . e . , indicator functions tied to the word bigram xm , xh involved in a dependency x , h , m , l .", "We then define a generative model that assigns a probability corporates a second order parsing model .", "In addition , we evaluate the SS SCM for English dependency parsing with large amounts up to 3 . 72 billion tokens of unlabeled data .", "Throughout this paper we will use x to denote an input sentence , and y to denote a labeled dependency structure .", "Given a sentence x with n words , a labeled dependency structure y is a set of n dependencies of the form h , m , l , where h is the index of the head word in the dependency , m is the index of the modifier word , and l is the label of the dependency .", "We use h 0 for the root of the sentence .", "We assume access to a set of labeled training examples , xz , yz Z_'1 , and in addition a set of unlabeled examples , xz M1 .", "In conditional log linear models for dependency parsing which are closely related to conditional random fields Lafferty et al . , 2001 , a distribution over dependency structures for a sentence x is defined as follows Here f x , h , m , l is a feature vector representing the dependency h , m , l in the context of the sentence x see for example McDonald et al . , 2005a .", "In this paper we extend the definition of g x , y to include features that are induced from unlabeled data .", "Specifically , we define to the dj dimensional feature vector rj x , h , m , l .", "The parameters of this model are \u03b8j , 1 . . . \u03b8j , dj ; they form a multinomial distribution , with the constraints that \u03b8j , a 0 , and Pa \u03b8j , a 1 .", "This model can be viewed as a very simple naiveBayes model that defines a distribution over feature vectors rj E Rdj .", "The next section describes how the parameters \u03b8j , a are trained on unlabeled data .", "Given parameters \u03b8j , a , we can simply define the functions q1 . . . qk to be log probabilities under the generative model We modify this definition slightly , be introducing scaling factors cj , a 0 , and defining In our experiments , cj , a is simply a count of the number of times the feature indexed by j , a appears in unlabeled data .", "Thus more frequent features have their contribution down weighted in the model .", "We have found this modification to be beneficial .", "We now describe the method for estimating the parameters \u03b8j , a of the generative models .", "We assume initial parameters w , v , q , which define a distribution p y x0i ; w , v , q over dependency structures for each unlabeled example x0i .", "We will re estimate the generative models q , based on unlabeled examples .", "The likelihood function on unlabeled data is defined as where q0 j is as defined in Eq .", "This function resembles the Q function used in the EM algorithm , where the hidden labels in our case , dependency structures , are filled in using the conditional distribution p y x0i ; w , v , q .", "It is simple to show that the estimates \u03b8j , a that maximize the function in Eq .", "5 can be defined as follows .", "First , define a vector of expected counts based on w , v , q as Note that it is straightforward to calculate these expected counts using a variant of the inside outside algorithm Baker , 1979 applied to the Eisner , 1996 dependency parsing data structures Paskin , 2001 for projective dependency structures , or the matrix tree theorem Koo et al . , 2007 ; Smith and Smith , 2007 ; McDonald and Satta , 2007 for nonprojective dependency structures .", "The estimates that maximize Eq .", "5 are then In a slight modification , we employ the following estimates in our model , where \u03b7 1 is a parameter of the model This corresponds to a MAP estimate under a Dirichlet prior over the \u03b8j , a parameters .", "This section describes the full parameter estimation method .", "The input to the algorithm is a set of labeled examples xi , yi Ni 1 , a set of unlabeled examples x0i Mi 1 , a feature vector definition f x , y , and a partition of f into k feature vectors r1 . . . rk which underly the generative models .", "The output from the algorithm is a parameter vector w , a set of generative models q1 . . . qk , and parameters v1 . . . vk , which define a probabilistic dependency parsing model through Eqs .", "1 and 2 .", "The learning algorithm proceeds in three steps Step 1 Estimation of a Fully Supervised Model .", "We choose the initial value q0 of the generative models to be the uniform distribution , i . e . , we set \u03b8j , a 1 dj for all j , a .", "We then define the regularized log likelihood function for the labeled examples , with the generative model fixed at q0 , to be This is a conventional regularized log likelihood function , as commonly used in CRF models .", "The parameter C 0 dictates the level of regularization in the model .", "We define the initial parameters w0 , v0 arg max , , v L w , v ; q0 .", "These parameters can be found using conventional methods for estimating the parameters of regularized log likelihood functions in our case we use LBFGS Liu and Nocedal , 1989 .", "Note that the gradient of the log likelihood function can be calculated using the inside outside algorithm applied to projective dependency parse structures , or the matrix tree theorem applied to non projective structures .", "Step 2 Estimation of the Generative Models .", "In this step , expected count vectors r1 . . . rk are first calculated , based on the distribution p y x ; w0 , v0 , q0 .", "Generative model parameters Oj , a are calculated through the definition in Eq .", "6 ; these estimates define updated generative models q1j for j 1 . . . k through Eq .", "We refer to the new values for the generative models as q1 .", "Step 3 Re estimation of w and v . In the final step , w1 and v1 are estimated as arg max , , v L w , v ; q1 where L w , v ; q1 is defined in an analogous way to L w , v ; q0 .", "Thus w and v are re estimated to optimize log likelihood of the labeled examples , with the generative models q1 estimated in step 2 .", "The final output from the algorithm is the set of parameters w1 , v1 , q1 .", "Note that it is possible to iterate the method steps 2 and 3 can be repeated multiple times Suzuki and Isozaki , 2008 but in our experiments we only performed these steps once .", "Koo et al . 2008 describe a semi supervised approach that incorporates cluster based features , and that gives competitive results on dependency parsing benchmarks .", "The method is a two stage approach .", "First , hierarchical word clusters are derived from unlabeled data using the Brown et al . clustering algorithm Brown et al . , 1992 .", "Second , a new feature set is constructed by representing words by bit strings of various lengths , corresponding to clusters at different levels of the hierarchy .", "These features are combined with conventional features based on words and part of speech tags .", "The new feature set is then used within a conventional discriminative , supervised approach , such as the averaged perceptron algorithm .", "The important point is that their approach uses unlabeled data only for the construction of a new feature set , and never affects to learning algorithms .", "It is straightforward to incorporate clusterbased features within the SS SCM approach described in this paper .", "We simply use the clusterbased feature vector representation f x , y introduced by Koo et al . , 2008 as the basis of our approach .", "Previous work McDonald and Pereira , 2006 ; Carreras , 2007 has shown that second order parsing models , which include information from sibling or grandparent relationships between dependencies , can give significant improvements in accuracy over first order parsing models .", "In principle it would be straightforward to extend the SS SCM approach that we have described to second order parsing models .", "In practice , however , a bottleneck for the method would be the estimation of the generative models on unlabeled data .", "This step requires calculation of marginals on unlabeled data .", "Second order parsing models generally require more costly inference methods for the calculation of marginals , and this increased cost may be prohibitive when large quantities of unlabeled data are employed .", "We instead make use of a simple two stage approach for extending the SS SCM approach to the second order parsing model of Carreras , 2007 .", "In the first stage , we use a first order parsing model to estimate generative models q1 . . . qk from unlabeled data .", "In the second stage , we incorporate these generative models as features within a second order parsing model .", "More precisely , in our approach , we first train a first order parsing model by Step 1 and 2 , exactly as described in Section 2 . 4 , to estimate w0 , v0 and q1 .", "Then , we substitute Step 3 as a supervised learning such as MIRA with a second order parsing model McDonald et al . , 2005a , which incorporates q1 as a real values features .", "We refer this two stage approach to as two stage SS SCM .", "In our experiments we use the 1 best MIRA algorithm McDonald and Pereira , 2006 1 as a labeled data sets and unlabeled data used in our experiments parameter estimation method for the second order parsing model .", "In particular , we perform the following optimizations on each update t 1 , . . . , T for re estimating w and v where L yi , y represents the loss between correct output of i th sample yi and y .", "Then , the scoring function S for each y can be defined as follows where B represents a tunable scaling factor , and f1 and f2 represent the feature vectors of first and second order parsing parts , respectively .", "We now describe experiments investigating the effectiveness of the SS SCM approach for dependency parsing .", "The experiments test basic , firstorder parsing models , as well as the extensions to cluster based features and second order parsing models described in the previous section .", "We conducted experiments on both English and Czech data .", "We used the Wall Street Journal sections of the Penn Treebank PTB III Marcus et al . , 1994 as a source of labeled data for English , and the Prague Dependency Treebank PDT 1 . 0 Haji\u02c7c , 1998 for Czech .", "To facilitate comparisons with previous work , we used exactly the same training , development and test sets as those described in McDonald et al . , 2005a ; McDonald et al . , 2005b ; McDonald and Pereira , 2006 ; Koo et al . , 2008 .", "The English dependencyparsing data sets were constructed using a standard set of head selection rules Yamada and Matsumoto , 2003 to convert the phrase structure syntax of the Treebank to dependency tree representations .", "We split the data into three parts sections 02 21 for training , section 22 for development and section 23 for test .", "The Czech data sets were obtained from the predefined training development test partition in the PDT .", "The unlabeled data for English was derived from the Brown Laboratory for Linguistic Information Processing BLLIP Corpus LDC2000T43 2 , giving a total of 1 , 796 , 379 sentences and 43 , 380 , 315 tokens .", "The raw text section of the PDT was used for Czech , giving 2 , 349 , 224 sentences and 39 , 336 , 570 tokens .", "These data sets are identical to the unlabeled data used in Koo et al . , 2008 , and are disjoint from the training , development and test sets .", "The datasets used in our experiments are summarized in Table 1 .", "In addition , we will describe experiments that make use of much larger amounts of unlabeled data .", "Unfortunately , we have no data available other than PDT for Czech , this is done only for English dependency parsing .", "Table 2 shows the detail of the larger unlabeled data set used in our experiments , where we eliminated sentences that have more than 128 tokens for computational reasons .", "Note that the total size of the unlabeled data reaches 3 . 72G billion tokens , which is approximately 4 , 000 times larger than the size of labeled training data .", "In general we will assume that the input sentences include both words and part of speech POS tags .", "Our baseline features baseline are very similar to those described in McDonald et al . , 2005a ; Koo et al . , 2008 these features track word and POS bigrams , contextual features surrounding dependencies , distance features , and so on .", "English POS tags were assigned by MXPOST Ratnaparkhi , 1996 , which was trained on the training data described in Section 4 . 1 .", "Czech POS tags were obtained by the following two steps First , we used feature based tagger included with the PDT3 , and then , we used the method described in Collins et al . , 1999 to convert the assigned rich POS tags into simplified POS tags .", "In a second set of experiments , we make use of the feature set used in the semi supervised approach of Koo et al . , 2008 .", "We will refer to this as the cluster based feature set CL .", "The BLLIP 43M tokens and PDT 39M tokens unlabeled data sets shown in Table 1 were used to construct the hierarchical clusterings used within the approach .", "Note that when this feature set is used within the SSSCM approach , the same set of unlabeled data is used to both induce the clusters , and to estimate the generative models within the SS SCM model .", "As described in section 2 . 2 , the generative models in the SS SCM approach are defined through a partition of the original feature vector f x , y into k feature vectors r1 x , y . . . rk x , y .", "We follow a similar approach to that of Suzuki and Isozaki , 2008 in partitioning f x , y , where the k different feature vectors correspond to different feature types or feature templates .", "Note that , in general , we are not necessary to do as above , this is one systematic way of a feature design for this approach .", "All results presented in our experiments are given in terms of parent prediction accuracy on unla3Training , development , and test data in PDT already contains POS tags assigned by the feature based tagger . beled dependency parsing .", "We ignore the parentpredictions of punctuation tokens for English , while we retain all the punctuation tokens for Czech .", "These settings match the evaluation setting in previous work such as McDonald et al . , 2005a ; Koo et al . , 2008 .", "We used the method proposed by Carreras , 2007 for our second order parsing model .", "Since this method only considers projective dependency structures , we projectivized the PDT training data in the same way as Koo et al . , 2008 .", "We used a non projective model , trained using an application of the matrix tree theorem Koo et al . , 2007 ; Smith and Smith , 2007 ; McDonald and Satta , 2007 for the first order Czech models , and projective parsers for all other models .", "As shown in Section 2 , SS SCMs with 1st order parsing models have two tunable parameters , C and q , corresponding to the regularization constant , and the Dirichlet prior for the generative models .", "We selected a fixed value q 2 , which was found to work well in preliminary experiments . 4 The value of C was chosen to optimize performance on development data .", "Note that C for supervised SCMs were also tuned on development data .", "For the two stage SS SCM for incorporating second order parsing model , we have additional one tunable parameter B shown in Eq .", "This was also chosen by the value that provided the best performance on development data .", "In addition to providing results for models trained on the full training sets , we also performed experiments with smaller labeled training sets .", "These training sets were either created through random sampling or by using a predefined subset of document IDs from the labeled training data .", "Table 3 gives results for the SS SCM method under various configurations for first and secondorder parsing models , with and without the cluster features of Koo et al . , 2008 , and for varying amounts of labeled data .", "The remainder of this section discusses these results in more detail .", "We can see from the results in Table 3 that our semi supervised approach consistently gives gains data .", "Supervised SCM 1od and Supervised MIRA 2od are the baseline first and second order approaches ; SS SCM 1od and 2 stage SS SCM MIRA 2od are the first and second order approaches described in this paper .", "Baseline refers to models without cluster based features , CL refers to models which make use of cluster based features . in performance under various sizes of labeled data .", "Note that the baseline methods that we have used in these experiments are strong baselines .", "It is clear that the gains from our method are larger for smaller labeled data sizes , a tendency that was also observed in Koo et al . , 2008 .", "One important observation from the results in Table 3 is that SS SCMs can successfully improve the performance over a baseline method that uses the cluster based feature set CL .", "This is in spite of the fact that the generative models within the SS SCM approach were trained on the same unlabeled data used to induce the cluster based features .", "Table 3 also shows the effectiveness of the twostage approach described in Section 3 . 2 that integrates the SS SCM method within a second order parser .", "This suggests that the SS SCM method can be effective in providing features generative models used within a separate learning algorithm , providing that this algorithm can make use of realvalued features .", "Figure 1 shows the dependency parsing accuracy on English as a function of the amount of unlabeled data used within the SS SCM approach .", "As described in Section 4 . 1 , we have no unlabeled data other than PDT for Czech , hence this section only considers English dependency parsing .", "We can see that performance does improve as more unlabeled data is added ; this trend is seen both with and without cluster based features .", "In addition , Table 4 shows the performance of our proposed method using 3 . 72 billion tokens of unlabeled data .", "Note , however , that the gain in performance as unlabeled data is added is not as sharp as might be hoped , with a relatively modest difference in performance for 43 . 4 million tokens vs . 3 . 72 billion tokens of unlabeled data .", "The main computational challenge in our approach is the estimation of the generative models q qi . . . qk from unlabeled data , particularly when the amount of unlabeled data used is large .", "In our implementation , on the 43M token BLLIP corpus , using baseline features , it takes about 5 hours to compute the expected counts required to estimate the parameters of the generative models on a single 2 . 93GHz Xeon processor .", "It takes roughly 18 days of computation to estimate the generative models from the larger 3 . 72 billion word corpus .", "Fortunately it is simple to parallelize this step ; our method takes a few hours on the larger data set when parallelized across around 300 separate processes .", "Note that once the generative models have been estimated , decoding with the model , or training the model on labeled data , is relatively inexpensive , essentially taking the same amount of computation as standard dependency parsing approaches .", "Finally , Table 5 displays the final results on test data .", "There results are obtained using the best setting in terms of the development data performance .", "Note that the English dependency parsing results shown in the table were achieved using 3 . 72 billion tokens of unlabeled data .", "The improvements on test data are similar to those observed on the development data .", "To determine statistical significance , we tested the difference of parent prediction error rates at the sentence level using a paired Wilcoxon signed rank test .", "All eight comparisons shown in Table 5 are significant with p 0 . 01 .", "Table 6 shows the performance of a number of state of the art approaches on the English and Czech data sets .", "For both languages our approach gives the best reported figures on these datasets .", "Our results yield relative error reductions of roughly 27 English and 20 Czech over McDonald and Pereira 2006 s second order supervised dependency parsers , and roughly 9 English and 7 Czech over the previous best results provided by Koo et . al .", "2008 s secondorder semi supervised dependency parsers .", "Note that there are some similarities between our two stage semi supervised learning approach and the semi supervised learning method introduced by Blitzer et al . , 2006 , which is an extension of the method described by Ando and Zhang , 2005 .", "In particular , both methods use a two stage approach ; They first train generative models or auxiliary problems from unlabeled data , and then , they incorporate these trained models into a supervised learning algorithm as real valued features .", "Moreover , both methods make direct use of existing feature vector definitions f x , y in inducing representations from unlabeled data .", "This paper has described an extension of the semi supervised learning approach of Suzuki and Isozaki , 2008 to the dependency parsing problem .", "In addition , we have described extensions that incorporate the cluster based features of Koo et al . 2008 , and that allow the use of second order parsing models .", "We have described experiments that show that the approach gives significant improvements over state of the art methods for dependency parsing ; performance improves when the amount of unlabeled data is increased from 43 . 8 million tokens to 3 . 72 billion tokens .", "The approach should be relatively easily applied to languages other than English or Czech .", "We stress that the SS SCM approach requires relatively little hand engineering it makes direct use of the existing feature vector representation f x , y used in a discriminative model , and does not require the design of new features .", "The main choice in the approach is the partitioning of f x , y into components r1 x , y . . . rk x , y , which in our experience is straightforward ."], "summary_lines": ["An Empirical Study of Semi-supervised Structured Conditional Models for Dependency Parsing\n", "This paper describes an empirical study of high-performance dependency parsers based on a semi-supervised learning approach.\n", "We describe an extension of semi-supervised structured conditional models (SS-SCMs) to the dependency parsing problem, whose framework is originally proposed in (Suzuki and Isozaki, 2008).\n", "Moreover, we introduce two extensions related to dependency parsing: The first extension is to combine SS-SCMs with another semi-supervised approach, described in (Koo et al., 2008).\n", "The second extension is to apply the approach to second-order parsing models, such as those described in (Carreras, 2007), using a two-stage semi-supervised learning approach.\n", "We demonstrate the effectiveness of our proposed methods on dependency parsing experiments using two widely used test collections: the Penn Treebank for English, and the Prague Dependency Treebank for Czech.\n", "Our best results on test data in the above datasets achieve 93.79% parent-prediction accuracy for English, and 88.05% for Czech.\n", "We present a very effective semi-supervised approach in which features from multiple generative models estimated on unlabeled data are combined in a discriminative system for structured prediction.\n"]}
{"article_lines": ["Multiple Aspect Ranking Using the Good Grief Algorithm", "We address the problem of analyzing multiple related opinions in a text .", "For instance , in a restaurant review such opinions may include food , ambience and service .", "We formulate this task as a multiple aspect ranking problem , where the goal is to produce a set of numerical scores , one for each aspect .", "We present an algorithm that jointly learns ranking models for individual aspects by modeling the dependencies between assigned ranks .", "This algorithm guides the prediction of individual rankers by analyzing meta relations between opinions , such as agreement and contrast .", "We prove that our agreementbased joint model is more expressive than individual ranking models .", "Our empirical results further confirm the strength of the model the algorithm provides significant improvement over both individual rankers and a state of the art joint ranking model .", "Previous work on sentiment categorization makes an implicit assumption that a single score can express the polarity of an opinion text Pang et al . , 2002 ; Turney , 2002 ; Yu and Hatzivassiloglou , 2003 .", "However , multiple opinions on related matters are often intertwined throughout a text .", "For example , a restaurant review may express judgment on food quality as well as the service and ambience of the restaurant .", "Rather than lumping these aspects into a single score , we would like to capture each aspect of the writer s opinion separately , thereby providing a more fine grained view of opinions in the review .", "To this end , we aim to predict a set of numeric ranks that reflects the user s satisfaction for each aspect .", "In the example above , we would assign a numeric rank from 1 5 for each of food quality , service , and ambience .", "A straightforward approach to this task would be to rank' the text independently for each aspect , using standard ranking techniques such as regression or classification .", "However , this approach fails to exploit meaningful dependencies between users judgments across different aspects .", "Knowledge of these dependencies can be crucial in predicting accurate ranks , as a user s opinions on one aspect can influence his or her opinions on others .", "The algorithm presented in this paper models the dependencies between different labels via the agreement relation .", "The agreement relation captures whether the user equally likes all aspects of the item or whether he or she expresses different degrees of satisfaction .", "Since this relation can often be determined automatically for a given text Marcu and Echihabi , 2002 , we can readily use it to improve rank prediction .", "The Good Grief model consists of a ranking model for each aspect as well as an agreement model which predicts whether or not all rank aspects are 'In this paper , ranking refers to the task of assigning an integer from 1 to k to each instance .", "This task is sometimes referred to as ordinal regression Crammer and Singer , 2001 and rating prediction Pang and Lee , 2005 . equal .", "The Good Grief decoding algorithm predicts a set of ranks one for each aspect which maximally satisfy the preferences of the individual rankers and the agreement model .", "For example , if the agreement model predicts consensus but the individual rankers select ranks 5 , 5 , 4 , then the decoder decides whether to trust the the third ranker , or alter its prediction and output 5 , 5 , 5 to be consistent with the agreement prediction .", "To obtain a model well suited for this decoding , we also develop a joint training method that conjoins the training of multiple aspect models .", "We demonstrate that the agreement based joint model is more expressive than individual ranking models .", "That is , every training set that can be perfectly ranked by individual ranking models for each aspect can also be perfectly ranked with our joint model .", "In addition , we give a simple example of a training set which cannot be perfectly ranked without agreement based joint inference .", "Our experimental results further confirm the strength of the Good Grief model .", "Our model significantly outperforms individual ranking models as well as a stateof the art joint ranking model .", "Sentiment Classification Traditionally , categorization of opinion texts has been cast as a binary classification task Pang et al . , 2002 ; Turney , 2002 ; Yu and Hatzivassiloglou , 2003 ; Dave et al . , 2003 .", "More recent work Pang and Lee , 2005 ; Goldberg and Zhu , 2006 has expanded this analysis to the ranking framework where the goal is to assess review polarity on a multi point scale .", "While this approach provides a richer representation of a single opinion , it still operates on the assumption of one opinion per text .", "Our work generalizes this setting to the problem of analyzing multiple opinions or multiple aspects of an opinion .", "Since multiple opinions in a single text are related , it is insufficient to treat them as separate single aspect ranking tasks .", "This motivates our exploration of a new method for joint multiple aspect ranking .", "Ranking The ranking , or ordinal regression , problem has been extensivly studied in the Machine Learning and Information Retrieval communities .", "In this section we focus on two online ranking methods which form the basis of our approach .", "The first is a model proposed by Crammer and Singer 2001 .", "The task is to predict a rank y E I1 , . . . , k for every input x E R' .", "Their model stores a weight vector w E R' and a vector of increasing boundaries b0 00 b1 . . . bk 1 bk 00 which divide the real line into k segments , one for each possible rank .", "The model first scores each input with the weight vector score x w x .", "Finally , the model locates score x on the real line and returns the appropriate rank as indicated by the boundaries .", "Formally , the model returns the rank r such that br 1 score x br .", "The model is trained with the Perceptron Ranking algorithm or PRank algorithm , which reacts to incorrect predictions on the training set by updating the weight and boundary vectors .", "The PRanking model and algorithm were tested on the EachMovie dataset with a separate ranking model learned for each user in the database .", "An extension of this model is provided by Basilico and Hofmann 2004 in the context of collaborative filtering .", "Instead of training a separate model for each user , Basilico and Hofmann train a joint ranking model which shares a set of boundaries across all users .", "In addition to these shared boundaries , userspecific weight vectors are stored .", "To compute the score for input x and user i , the weight vectors for all users are employed where 0 sim i , j 1 is the cosine similarity between users i and j , computed on the entire training set .", "Once the score has been computed , the prediction rule follows that of the PRanking model .", "The model is trained using the PRank algorithm , with the exception of the new definition for the scoring function . 2 While this model shares information between the different ranking problems , it fails to explicitly model relations between the rank predictions .", "In contrast , our algorithm uses an agreement model to learn such relations and inform joint predictions .", "The goal of our algorithm is to find a rank assignment that is consistent with predictions of individual rankers and the agreement model .", "To this end , we develop the Good Grief decoding procedure that minimizes the dissatisfaction grief of individual components with a joint prediction .", "In this section , we formally define the grief of each component , and a mechanism for its minimization .", "We then describe our method for joint training of individual rankers that takes into account the Good Grief decoding procedure .", "In an m aspect ranking problem , we are given a training sequence of instance label pairs x1 , y1 , . . . , xt , yt , . . . . Each instance xt is a feature vector in Rn and the label yt is a vector of m ranks in Ym , where Y 11 , . . , k is the set of possible ranks .", "The ith component of yt is the rank for the ith aspect , and will be denoted by y i t . The goal is to learn a mapping from instances to rank sets , H X Ym , which minimizes the distance between predicted ranks and true ranks .", "Our m aspect ranking model contains m 1 components w 1 , b 1 , . . . , w m , b m , a .", "The first m components are individual ranking models , one for each aspect , and the final component is the agreement model .", "For each aspect i E 1 . . . m , w i E Rn is a vector of weights on the input features , and b i E Rk 1 is a vector of boundaries which divide the real line into k intervals , corresponding to the k possible ranks .", "The default prediction of the aspect ranking model simply uses the ranking rule of the PRank algorithm .", "This rule predicts the rank r such that b i r 1 scorei x b i r . 3 The value scorei x can be defined simply as the dot product w i x , or it can take into account the weight vectors for other aspects weighted by a measure of interaspect similarity .", "We adopt the definition given in equation 1 , replacing the user specific weight vectors with our aspect specific weight vectors .", "3More precisely taking into account the possibility of ties y i min1E 1 , . . , k r scorei x b i , .", "01 The agreement model is a vector of weights a E Rn .", "A value of a x 0 predicts that the ranks of all m aspects are equal , and a value of a x 0 indicates disagreement .", "The absolute value Ja xJ indicates the confidence in the agreement prediction .", "The goal of the decoding procedure is to predict a joint rank for the m aspects which satisfies the individual ranking models as well as the agreement model .", "For a given input x , the individual model for aspect i predicts a default rank y i based on its feature weight and boundary vectors w i , b i .", "In addition , the agreement model makes a prediction regarding rank consensus based on a x .", "However , the default aspect predictions 9 1 . . . y m may not accord with the agreement model .", "For example , if a x 0 , but 9 i y j for some i , j E 1 . . . m , then the agreement model predicts complete consensus , whereas the individual aspect models do not .", "We therefore adopt a joint prediction criterion which simultaneously takes into account all model components individual aspect models as well as the agreement model .", "For each possible prediction r r 1 , . . . , r m this criterion assesses the level of grief associated with the ith aspect ranking model , gi x , r i .", "Similarly , we compute the grief of the agreement model with the joint prediction , ga x , r both gi and ga are defined formally below .", "The decoder then predicts the m ranks which minimize the overall grief 2 If the default rank predictions for the aspect models , y 0 1 , . . . , y m , are in accord with the agreement model both indicating consensus or both indicating contrast , then the grief of all model components will be zero , and we simply output y .", "On the other hand , if y indicates disagreement but the agreement model predicts consensus , then we have the option of predicting y and bearing the grief of the agreement model .", "Alternatively , we can predict some consensus y0 i . e . with y0 i y0 j , Vi , j and bear the grief of the component ranking models .", "The decoder H chooses the option with lowest overall grief . 4 m Now we formally define the measures of grief used in this criterion .", "Aspect Model Grief We define the grief of the ithaspect ranking model with respect to a rank r to be the smallest magnitude correction term which places the input s score into the rth segment of the real line Agreement Model Grief Similarly , we define the grief of the agreement model with respect to a joint rank r r 1 , . . . , r m as the smallest correction needed to bring the agreement score into accord with the agreement relation between the individual ranks r 1 , . . . , r m Ranking models Pseudo code for Good Grief training is shown in Figure 1 .", "This training algorithm is based on PRanking Crammer and Singer , 2001 , an online perceptron algorithm .", "The training is performed by iteratively ranking each training input x and updating the model .", "If the predicted rank y is equal to the true rank y , the weight and boundaries vectors remain unchanged .", "On the other hand , if y y , then the weights and boundaries are updated to improve the prediction for x step 4 . c in Figure 1 .", "See Crammer and Singer , 2001 for explanation and analysis of this update rule .", "Our algorithm departs from PRanking by conjoining the updates for the m ranking models .", "We achieve this by using Good Grief decoding at each step throughout training .", "Our decoder H x from equation 2 uses all the aspect component models ponent models are comparable .", "In practice , we take an uncalibrated agreement model a' and reweight it with a tuning parameter a \u03b1a' .", "The value of \u03b1 is estimated using the development set .", "We assume that the griefs of the ranking models are comparable since they are jointly trained . as well as the previously trained agreement model to determine the predicted rank for each aspect .", "In concrete terms , for every training instance x , we predict the ranks of all aspects simultaneously step 2 in Figure 1 .", "Then , for each aspect we make a separate update based on this joint prediction step 4 in Figure 1 , instead of using the individual models predictions .", "Agreement model The agreement model a is assumed to have been previously trained on the same training data .", "An instance is labeled with a positive label if all the ranks associated with this instance are equal .", "The rest of the instances are labeled as negative .", "This model can use any standard training algorithm for binary classification such as Perceptron or SVM optimization .", "Ranking Models Following previous work on sentiment classification Pang et al . , 2002 , we represent each review as a vector of lexical features .", "More specifically , we extract all unigrams and bigrams , discarding those that appear fewer than three times .", "This process yields about 30 , 000 features .", "Agreement Model The agreement model also operates over lexicalized features .", "The effectiveness of these features for recognition of discourse relations has been previously shown by Marcu and Echihabi 2002 .", "In addition to unigrams and bigrams , we also introduce a feature that measures the maximum contrastive distance between pairs of words in a review .", "For example , the presence of delicious and dirty indicate high contrast , whereas the pair expensive and slow indicate low contrast .", "The contrastive distance for a pair of words is computed by considering the difference in relative weight assigned to the words in individually trained PRanking models .", "In this section , we prove that our model is able to perfectly rank a strict superset of the training corpora perfectly rankable by m ranking models individually .", "We first show that if the independent ranking models can individually rank a training set perfectly , then our model can do so as well .", "Next , we show that our model is more expressive by providing a simple illustrative example of a training set which can only be perfectly ranked with the inclusion of an agreement model .", "First we introduce some notation .", "For each training instance xt , yt , each aspect i 1 . . . m , and each rank r 1 . . . k , define an auxiliary variable y i t r with y i t r 1 if y i t r and y i t r 1 if y i t r . In words , y i t r indicates whether the true rank y i t is to the right or left of a potential rank r . Now suppose that a training set x1 , y1 , . . . , xT , yT is perfectly rankable for each aspect independently .", "That is , for each aspect i 1 . . . m , there exists some ideal model v i w i , b i such that the signed distance from the prediction to the rth boundary w i xt b i r has the same sign as the auxiliary variable y i tr .", "In other words , the minimum margin over all training instances and ranks , \u03b3 minr t w i xt b i r y i tr , is no less than zero .", "Now for the tth training instance , define an agreement auxiliary variable at , where at 1 when all aspects agree in rank and at 1 when at least two aspects disagree in rank .", "First consider the case where the agreement model a perfectly classifies all training instances a xt at 0 , t .", "It is clear that Good Grief decoding with the ideal joint model w 1 , b 1 , . . . , w m , b m , a will produce the same output as the component ranking models run separately since the grief will always be zero for the default rank predictions .", "Now consider the case where the training data is not linearly separable with regard to agreement classification .", "Define the margin of the worst case error to be \u03b2 maxt a xt a xt at 0 .", "If \u03b2 \u03b3 , then again Good Grief decoding will always produce the default results since the grief of the agreement model will be at most \u03b2 in cases of error , whereas the grief of the ranking models for any deviation from their default predictions will be at least \u03b3 .", "On the other hand , if \u03b2 \u03b3 , then the agreement model errors could potentially disrupt the perfect ranking .", "However , we need only rescale w w 0 and b b E to ensure that the grief of the ranking models will always exceed the grief of the agreement model in cases where the latter is in error .", "Thus whenever independent ranking models can perfectly rank a training set , a joint ranking model with Good Grief decoding can do so as well .", "Now we give a simple example of a training set which can only be perfectly ranked with the addition of an agreement model .", "Consider a training set of four instances with two rank aspects We can interpret these inputs as feature vectors corresponding to the presence of good , bad , and but not in the following four sentences The food was good , but not the ambience .", "The food was good , and so was the ambience .", "The food was bad , but not the ambience .", "The food was bad , and so was the ambience .", "We can further interpret the first rank aspect as the quality of food , and the second as the quality of the ambience , both on a scale of 1 2 .", "A simple ranking model which only considers the words good and bad perfectly ranks the food aspect .", "However , it is easy to see that no single model perfectly ranks the ambience aspect .", "Consider any model w , b b .", "Note that w x1 b and w x2 b together imply that w3 0 , whereas w x3 b and w x4 b together imply that w3 0 .", "Thus independent ranking models cannot perfectly rank this corpus .", "The addition of an agreement model , however , can easily yield a perfect ranking .", "With a 0 , 0 , 5 which predicts contrast with the presence of the words but not and a ranking model for the ambience aspect such as w 1 , 1 , 0 , b 0 , the Good Grief decoder will produce a perfect rank .", "We evaluate our multi aspect ranking algorithm on a corpus of restaurant reviews available on the website http www . we8there . com .", "Reviews from this website have been previously used in other sentiment analysis tasks Higashinaka et al . , 2006 .", "Each review is accompanied by a set of five ranks , each on a scale of 1 5 , covering food , ambience , service , value , and overall experience .", "These ranks are provided by consumers who wrote original reviews .", "Our corpus does not contain incomplete data points since all the reviews available on this website contain both a review text and the values for all the five aspects .", "Training and Testing Division Our corpus contains 4 , 488 reviews , averaging 115 words .", "We randomly select 3 , 488 reviews for training , 500 for development and 500 for testing .", "Parameter Tuning We used the development set to determine optimal numbers of training iterations for our model and for the baseline models .", "Also , given an initial uncalibrated agreement model a' , we define our agreement model to be a \u03b1a' for an appropriate scaling factor \u03b1 .", "We tune the value of \u03b1 on the development set .", "Corpus Statistics Our training corpus contains 528 among 55 3025 possible rank sets .", "The most frequent rank set 5 , 5 , 5 , 5 , 5 accounts for 30 . 5 of the training set .", "However , no other rank set comprises more than 5 of the data .", "To cover 90 of occurrences in the training set , 227 rank sets are required .", "Therefore , treating a rank tuple as a single label is not a viable option for this task .", "We also find that reviews with full agreement across rank aspects are quite common in our corpus , accounting for 38 of the training data .", "Thus an agreementbased approach is natural and relevant .", "A rank of 5 is the most common rank for all aspects and thus a prediction of all 5 s gives a MAJORITY baseline and a natural indication of task difficulty .", "Evaluation Measures We evaluate our algorithm and the baseline using ranking loss Crammer and Singer , 2001 ; Basilico and Hofmann , 2004 .", "Ranking loss measures the average distance between the true rank and the predicted rank .", "Formally , given N test instances x1 , y1 , . . . , xN , yN of an m aspect ranking problem and the corresponding predictions \u02c6y1 , . . . , \u02c6yN , ranking loss is defined as Et , i y i t_\u02c6y i t .", "Lower values of this measure cormN respond to a better performance of the algorithm .", "Comparison with Baselines Table 1 shows the performance of the Good Grief training algorithm GG TRAIN DECODE along with various baselines , including the simple MAJORITY baseline mentioned in section 5 .", "The first competitive baseline , PRANK , learns a separate ranker for each aspect using the PRank algorithm .", "The second competitive baseline , SIM , shares the weight vectors across aspects using a similarity measure Basilico and Hofmann , 2004 .", "Both of these methods are described in detail in Section 2 .", "In addition , we consider two variants of our algorithm GG DECODE employs the PRank training algorithm to independently train all component ranking models and only applies Good Grief decoding at test time .", "GG ORACLE uses Good Grief training and decoding but in both cases is given perfect knowledge of whether or not the true ranks all agree instead of using the trained agreement model .", "Our model achieves a rank error of 0 . 632 , compared to 0 . 675 for PRANK and 0 . 663 for SIM .", "Both of these differences are statistically significant at p 0 . 002 by a Fisher Sign Test .", "The gain in performance is observed across all five aspects .", "Our model also yields significant improvement p 0 . 05 over the decoding only variant GG DECODE , confirming the importance of joint training .", "As shown in Figure 2 , our model demonstrates consistent improvement over the baselines across all the training rounds .", "Model Analysis We separately analyze our percomputed separately on cases of actual consensus and actual disagreement . formance on the 210 test instances where all the target ranks agree and the remaining 290 instances where there is some contrast .", "As Table 2 shows , we outperform the PRANK baseline in both cases .", "However on the consensus instances we achieve a relative reduction in error of 21 . 8 compared to only a 1 . 1 reduction for the other set .", "In cases of consensus , the agreement model can guide the ranking models by reducing the decision space to five rank sets .", "In cases of disagreement , however , our model does not provide sufficient constraints as the vast majority of ranking sets remain viable .", "This explains the performance of GG ORACLE , the variant of our algorithm with perfect knowledge of agreement disagreement facts .", "As shown in Table 1 , GG ORACLE yields substantial improvement over our algorithm , but most of this gain comes from consensus instances see Table 2 .", "We also examine the impact of the agreement model accuracy on our algorithm .", "The agreement model , when considered on its own , achieves classification accuracy of 67 on the test set , compared to a majority baseline of 58 .", "However , those instances with high confidence a x exhibit substantially higher classification accuracy .", "Figure 3 shows the performance of the agreement model as a function of the confidence value .", "The 10 of the data with highest confidence values can be classified by the agreement model with 90 accuracy , and the third of the data with highest confidence can be classified at 80 accuracy .", "This property explains why the agreement model helps in joint ranking even though its overall accuracy may seem low .", "Under the Good Grief criterion , the agreement model s prediction will only be enforced when its grief outweighs that of the ranking models .", "Thus in cases where the prediction confidence a x is relatively low , 6 the agreement model will essentially be ignored .", "We considered the problem of analyzing multiple related aspects of user reviews .", "The algorithm presented jointly learns ranking models for individual aspects by modeling the dependencies between assigned ranks .", "The strength of our algorithm lies in its ability to guide the prediction of individual rankers using rhetorical relations between aspects such as agreement and contrast .", "Our method yields significant empirical improvements over individual rankers as well as a state of the art joint ranking model .", "Our current model employs a single rhetorical relation agreement vs . contrast to model dependencies between different opinions .", "As our analy6What counts as relatively low will depend on both the value of the tuning parameter \u03b1 and the confidence of the component ranking models for a particular input x . sis shows , this relation does not provide sufficient constraints for non consensus instances .", "An avenue for future research is to consider the impact of additional rhetorical relations between aspects .", "We also plan to theoretically analyze the convergence properties of this and other joint perceptron algorithms .", "The authors acknowledge the support of the National Science Foundation CAREER grant IIS 0448168 and grant IIS0415865 and the Microsoft Research Faculty Fellowship .", "Thanks to Michael Collins , Pawan Deshpande , Jacob Eisenstein , Igor Malioutov , Luke Zettlemoyer , and the anonymous reviewers for helpful comments and suggestions .", "Thanks also to Vasumathi Raman for programming assistance .", "Any opinions , findings , and conclusions or recommendations expressed above are those of the authors and do not necessarily reflect the views of the NSF ."], "summary_lines": ["Multiple Aspect Ranking Using the Good Grief Algorithm\n", "We address the problem of analyzing multiple related opinions in a text.\n", "For instance, in a restaurant review such opinions may include food, ambience and service.\n", "We formulate this task as a multiple aspect ranking problem, where the goal is to produce a set of numerical scores, one for each aspect.\n", "We present an algorithm that jointly learns ranking models for individual aspects by modeling the dependencies between assigned ranks.\n", "This algorithm guides the prediction of individual rankers by analyzing meta-relations between opinions, such as agreement and contrast.\n", "We prove that our agreement-based joint model is more expressive than individual ranking models.\n", "Our empirical results further confirm the strength of the model: the algorithm provides significant improvement over both individual rankers and a state-of-the-art joint ranking model.\n", "We combine an agreement model based on contrastive RST relations with a local aspect (or target) model to make a more informed overall decision for sentiment classification.\n"]}
{"article_lines": ["Cues And Control In Expert Client Dialogues", "We conducted an empirical analysis into the relation between control and discourse structure .", "We applied control criteria to four dialogues and identified 3 levels of discourse structure .", "We investigated the mechanism for changing control between these structures and found that utterance type and not cue words predicted shifts of control .", "Participants used certain types of signals when discourse goals were proceeding successfully but resorted to interruptions when they were not .", "A number of researchers have shown that there is organisation in discourse above the level of the individual utterance 5 , 8 , 9 , 10 , The current exploratory study uses control as a parameter for identifying these higher level structures .", "We then go on to address how conversational participants co ordinate moves between these higher level units , in particular looking at the ways they use to signal the beginning and end of such high level units .", "Previous research has identified three means by which speakers signal information about discourse structure to listeners Cue words and phrases 5 , 10 ; Intonation 7 ; Pronominalisation 6 , 2 .", "In the cue words approach , Reichman 10 has claimed that phrases like quot ; because quot ; , quot ; so quot ; , and quot ; but quot ; offer explicit information to listeners about how the speaker's current contribution to the discourse relates to what has gone previously .", "For example a speaker might use the expression quot ; so quot ; to signal that s he is about to conclude what s he has just said .", "Grosz and Sidner 5 relate the use of such phrases to changes in attentional state .", "An example would be that quot ; and quot ; or quot ; but quot ; signal to the listener that a new topic and set of referents is being introduced whereas quot ; anyway quot ; and quot ; in any case quot ; indicate a return to a previous topic and referent set .", "A second indirect way of signalling discourse structure is intonation .", "Hirschberg and Pierrehumbert 7 showed that intonational contour is closely related to discourse segmentation with new topics being signalled by changes in intonational contour .", "A final more indirect cue to discourse structure is the speaker's choice of referring expressions and grammatical structure .", "A number of researchers 4 , 2 , 6 , 10 have given accounts of how these relate to the continuing , retaining or shifting of focus .", "The above approaches have concentrated on particular surface linguistic phenomena and then investigated what a putative cue serves to signal in a number of dialogues .", "The problem with this approach is that the cue may only be an infrequent indicator of a particular type of shift .", "If we want to construct a general theory of discourse than we want to know about the whole range of cues serving this function .", "This study therefore takes a different approach .", "We begin by identifying all shifts of control in the dialogue and then look at how each shift was signalled by the speakers .", "A second problem with previous research is that the criteria for identifying discourse structure are not always made explicit .", "In this study explicit criteria are given we then go on to analyse the relation between cues and this structure .", "The data were recordings of telephone conversations between clients and an expert concerning problems with software .", "The tape recordings from four dialogues were then transcribed and the analysis conducted on the typewritten transcripts rather than the raw recordings .", "There was a total of 450 turns in the dialogues . types .", "Each utterance in the dialogue was classified into one of four categories a Assertions declarative utterances which were used to state facts .", "Yes or no answers to questions were also classified as assertions on the grounds that they were supplying the listener with factual information ; b Commands utterances which were intended to instigate action in their audience .", "These included various utterances which did not have imperative form , e . g .", "quot ; What I would do if I were you is to relink X quot ; but were intended to induce some action ; c Questions utterances which were intended to elicit information from the audience .", "These included utterances which did not have interrogative form . e . g .", "quot ; So my question is . . . . quot ; They also included paraphrases , in which the speaker reformulated or repeated part or all of what had just been said .", "Paraphrases were classified as questions on the grounds that the effect was to induce the listener to confirm or deny what had just been stated ; d Prompts These were utterances which did not express propositional content .", "Examples of prompts were things like quot ; Yes quot ; and quot ; Uhu quot ; . logues .", "We devised several rules to determine the location of control in the dialogues .", "Each of these rules related control to utterance type a For questions , the speaker was defined as being in control unless the question directly followed a question or command by the other conversant .", "The reason for this is that questions uttered following questions or commands are normally attempts to clarify the preceding utterance and as such are elicited by the previous speaker's utterance rather than directing the conversation in their own right .", "b For assertions , the speaker was defined as being in control unless the assertion was made in response to a question , for the same reasons as those given for questions ; an assertion which is a response to a question could not be said to be controlling the discourse ; c For commands , the speaker was defined as controlling the conversation .", "Indirect commands i . e . utterances which did not have imperative form but served to elicit some actions were also classified in this way ; d For prompts , the listener was defined as controlling the conversation , as the speaker was clearly abdicating his her turn .", "In cases where a turn consisted of several utterances , the control rules were only applied to the final utterance .", "We applied the control rules and found that control did not alternate from speaker to speaker on a turn by turn basis , but that there were long sequences of turns in which control remained with one speaker .", "This seemed to suggest that the dialogues were organised above the level of individual turns into phases where control was located with one speaker .", "The mean number of turns in each phase was 6 . 63 .", "We then went on to analyse how control was exchanged between participants at the boundaries of these phases .", "We first examined the last utterance of each phase on the grounds that one mechanism for indicating the end of a phase would be for the speaker controlling the phase to give some cue that he both participants in the dialogues were always male no longer wished to control the discourse .", "There was a total of 56 shifts of control over the 4 dialogues and we identified 3 main classes of cues used to signal control shifts These were prompts , repetitions and summaries .", "We also looked at when no signal was given interruptions .", "3 . 1 Prompts .", "On 21 of the 56 shifts 38 , the utterance immediately prior to the control shift was a prompt .", "We might therefore explain these shifts as resulting from the person in control explicitly indicating that he had nothing more to say .", "In the following examples a line indicates a control shift Example 1 Prompt Dialogue C further 15 occasions 27 , we found that the person in control of the dialogue signalled that they had no new information to offer .", "They did this either by repeating what had just been said 6 occasions , or by giving a summary of what they had said in the preceding utterances of the phase 9 occasions .", "We defined a repetition as an assertion which expresses part or all of the propositional content of a previous assertion but which contains no new information .", "A summary consisted of concise reference to the entire set of information given about the client's problem or the solution plan .", "Example 2 Repetition .", "Dialogue C Half the repetitions were accompanied by cue words .", "These were quot ; and quot ; , quot ; well quot ; and quot ; so quot ; , which prefixed the assertion .", "What are the linguistic characteristics of summaries ?", "Reichman 10 suggests that quot ; so quot ; might be a summary cue on the part of the speaker but we found only one example of this , although there were 3 instances of quot ; and quot ; , one quot ; now quot ; one quot ; but quot ; and one quot ; so quot ; .", "In our dialogues the summaries seemed to be characterised by the concise reference to objects or entities which had earlier been described in detail , e . g .", "a quot ; Now , I'm wondering how the two are related quot ; in which quot ; the two quot ; refers to the two error messages which it had taken several utterances to describe previously .", "The other characteristic of summaries is that they contrast strongly with the extremely concrete descriptions elsewhere in the dialogues , e . g .", "quot ; err the system program standard call file doesn't complete this means that the file does not have a tail record quot ; followed by quot ; And I've no clue at all how to get out of the situation quot ; .", "Example 3 also illustrates this change from specific 1 , 3 , 5 to general 7 .", "How then do repetitions and summaries operate as cues ?", "In summarising , the speaker is indicating a natural breakpoint in the dialogue and they also indicate that they have nothing more to add at that stage .", "Repetitions seem to work in a similar way the fact that a speaker reiterates indicates that he has nothing more to say on a topic .", "3 . 3 Interruptions .", "In the previous cases , the person controlling the dialogue gave a signal that control might be exchanged .", "There were 20 further occasions 36 of shifts on which no such indication is given .", "We therefore went on to analyse the conditions in which such interruptions occurred .", "These seem to fall into 3 categories a vital facts ; b responses to vital facts ; c clarifications .", "3 . 3 . 1 Vital facts .", "On a total of 6 occasions 11 of shifts the client interrupted to contradict the speaker or to supply what seemed to be relevant information that he believed the expert did not know .", "Example 4 Dialogue C Two of these 6 interjections were to supply extra information and one was marked with the cue quot ; as well quot ; .", "The other four were to contradict what had just been said and two had explicit markers quot ; though quot ; and quot ; well actually quot ; the remaining two being direct denials . vital facts .", "The next class of interruptions occur after the client has made some interjection to supply a missing fact or when the client has blocked a plan or rejected an explanation that the expert has produced .", "There were 8 such occasions 14 of shifts .", "The interruption in the previous example illustrates the reversion of control to the expert after the client has supplied information which he the client believes to be highly relevant to the expert .", "In the following example , the client is already in control .", "Example 5 Dialogue B On five occasions the expert explicitly signified his acceptance or rejection of what the client had said , e . g . quot ; Ah quot ; , quot ; Right quot ; , quot ; indeed quot ; , quot ; that's right quot ; , quot ; No quot ; , quot ; Yeah but quot ; .", "On three occasions there were no markers .", "3 . 3 . 3 Clarifications .", "Participants can also interrupt to clarify what has just been said .", "This happened on 6 occasions 11 of shifts .", "Example 6 Dialogue C On two occasions clarifications were prefixed by quot ; now quot ; and twice by quot ; so quot ; .", "On the final two occasions there was no such marker , and a direct question was used .", "We have just described the circumstances in which interruptions occur , but can we now explain why they occur ?", "We suggest the following two principles might account for interruptions these principles concern a the information upon which the participants are basing their plans , and b the plans themselves .", "A .", "Information quality Both expert and client must believe that the information that the expert has about the problem is true and that this information is sufficient to solve the problem .", "This can be expressed by the following two rules which concern the truth of the information and the ambiguity of the information Al if the speaker believes a fact P and believes that fact to be relevant and either believes that the speaker believes not P or that the speaker does not know P then interrupt ; A2 If the listener believes that the speaker's assertion is relevant but ambiguous then interrupt .", "B .", "Plan quality Both expert and client must believe that the plan that the expert has generated is adequate to solve the problem and it must be comprehensible to the client .", "The two rules which express this principle concern the effectiveness of the plan and the ambiguity of the plan B1 If the listener believes P and either believes that P presents an obstacle to the proposed plan or believes that part of the proposed plan has already been satisfied , then interrupt ; B2 If the listener believes that an assertion about the proposed plan is ambiguous , then interrupt .", "In this framework , interruptions can be seen as strategies produced by either conversational participant when they perceive that a either principle is not being adhered to .", "3 . 4 Cue reliability .", "We also investigated whether there were occasions when prompts , repetitions and summaries failed to elicit the control shifts we predicted .", "We considered two possible types of failure either the speaker could give a cue and continue or the speaker could give a cue and the listener fail to respond .", "We found no instances of the first case ; although speakers did produce phrases like quot ; OK quot ; and then continue , the quot ; OK quot ; was always part of the same intonational contour as that further information and there was no break between the two , suggesting the phrase was a prefix and not a cue .", "We did , however , find instances of the second case twice following prompts and once following a summary , there was a long pause , indicating that the speaker was not ready to respond .", "We conducted a similar analysis for those cue words that have been identified in the literature .", "Only 21 of the 35 repetitions , summaries and interruptions had cue words associated with them and there were also 19 instances of the cue words quot ; now quot ; , quot ; and quot ; , quot ; so quot ; , quot ; but quot ; and quot ; well quot ; occurring without a control shift .", "The analysis so far has been concerned with control shifts where shifts were identified from a series of rules which related utterance type and control .", "Examination of the dialogues indicated that there seemed to be different types of control shifts after some shifts there seemed to be a change of topic , whereas for others the topic remained the same .", "We next went on to examine the relationship between topic shift and the different types of cues and interruptions described earlier .", "To do this it was necessary first to classify control shifts according to whether they resulted in shifts of topic .", "4 . 1 Identifying topic shifts .", "We identified topic shifts in the following way Five judges were presented with the four dialogues and in each of the dialogues we had marked where control shifts occurred .", "The judges were asked to state for each control shift whether it was accompanied by a topic shift .", "All five judges agreed on 24 of the 56 shifts , and 4 agreed for another 22 of the shifts .", "Where there was disagreement , the majority judgment was taken . shift .", "Analysing each type of control shift , it is clear that there are differences between the cues used for the topic shift and the no shift cases .", "For interruptions , 90 occur within topic , i . e . they do not result in topic shifts .", "The pattern is not as obvious for prompts and repetitions summaries , with 57 of prompts occurring within topic and 67 of repetitions summaries occurring within topic .", "This suggests that change of topic is a carefully negotiated process .", "The controlling participant signals that he is ready to close the topic by producing either a prompt or a repetition summary and this may or may not be accepted by the other participant .", "What is apparent is that it is highly unusual for a participant to seize control and change topic by interruption .", "It seems that on the majority of occasions 63 participants wait for the strongest possible cue the prompt before changing topic . control .", "We also looked at more general aspects of control within and between topics .", "We investigated the number of utterances for which each participant was in control and found that there seemed to be organisation in the dialogues above the level of topic .", "We found that each dialogue could be divided into two parts separated by a topic shift which we labelled the central shift .", "The two parts of the dialogue were very different in terms of who controlled and initiated each topic .", "Before the central shift , the client had control for more turns per topic and after it , the expert had control for more turns per topic .", "The respective numbers of turns client and expert are in control before and after the central shift are Before 11 7 , 22 8 , 12 6 , 21 6 ; After 1233 , 16 23 , 2 11 , 0 5 for the four dialogues .", "With the exception of the first topic in Dialogues 1 and 4 , the client has control of more turns in every topic before the central shift , whereas after it , the expert has control for more turns in every topic .", "In addition we looked at who initiated each topic , i . e . who produced the first utterance of each topic .", "We found that in each dialogue , the client initiates all the topics before the central shift , whereas the expert initiates the later ones .", "We also discovered a close relationship between topic initiation and topic dominance .", "In 19 of the 21 topics , the person who initiated the topic also had control of more turns .", "As we might expect , the point at which the expert begins to have control over more turns per topic is also the point at which the expert begins to initiate new topics .", "The main result of this exploratory study is the finding that control is a useful parameter for identifying discourse structure .", "Using this parameter we identified three levels of structure in the dialogues a control phases ; b topic ; and c global organisation .", "For the control phases , we found that three types of utterances prompts , repetitions and summaries were consistently used to signal control shifts .", "For the low level structures we identified , i . e . control phases , cue words and phrases were not as reliable in predicting shifts .", "This result challenges the claims of recent discourse theories 5 , 10 which argue for a the close relation between cue words and discourse structure .", "We also examined how utterance type related to topic shift and found that few interruptions introduced a new topic .", "Finally there was evidence for high level structures in these dialogues as evidenced by topic initiation and control , with early topics being initiated and dominated by the client and the opposite being true for the later parts .", "Another focus of current research has been the modelling of speaker and listener goals 1 , 3 but there has been little research on real dialogues investigating how goals are communicated and inferred .", "This study identifies surface linguistic phenomena which reflect the fact that participants are continuously monitoring their goals .", "When plans are perceived as succeeding , participants use explicit cues such as prompts , repetitions and summaries to signal their readiness to move to the next stage of the plan .", "In other cases , where participants perceive obstacles to their goals being achieved , they resort to interruptions and we have tried to make explicit the rules by which they do this .", "In addition our methodology is different from other studies because we have attempted to provide an explanation for whole dialogues rather than fragments of dialogues , and used explicit criteria in a bottom up manner to identify discourse structures .", "The number of dialogues was small and taken from a single problem domain .", "It seems likely therefore that some of our findings e . g the central shift will be specific to the diagnostic dialogues we studied .", "Further research applying the same techniques to a broader set of data should establish the generality of the control rules suggested here ."], "summary_lines": ["Cues And Control In Expert-Client Dialogues\n", "We conducted an empirical analysis into the relation between control and discourse structure.\n", "We applied control criteria to four dialognes and identified 3 levels of discourse structure.\n", "We investigated the mechanism for changing control between these structures and found that utterance type and not cue words predicted shifts of control.\n", "Participants used certain types of signals when discourse goals were proceeding successfully but resorted to interruptions when they were not.\n", "We define initiative as being held by the speaker who is driving the conversation at any point in the conversation.\n", "We propose rules for tracking initiative based on utterance types: for example, statements, proposals and questions show initiative while answers and acknowledgements do not.\n"]}
{"article_lines": ["Better Word Alignments with Supervised ITG Models", "This work investigates supervised word alignment methods that exploit inversion transduction grammar ITG constraints .", "We consider maximum margin and conditional likelihood objectives , including the presentation of a new normal form grammar for canonicalizing derivations .", "Even for non ITG sentence pairs , we show that it is possible learn ITG alignment models by simple relaxations of structured discriminative learning objectives .", "For efficiency , we describe a set of pruning techniques that together allow us to align sentences two orders of magnitude faster than naive bitext CKY parsing .", "Finally , we introduce many to one block alignment features , which significantly improve our ITG models .", "Altogether , our method results in the best reported AER numbers for Chinese English and a performance improvement of 1 . 1 BLEU over GIZA alignments .", "Inversion transduction grammar ITG constraints Wu , 1997 provide coherent structural constraints on the relationship between a sentence and its translation .", "ITG has been extensively explored in unsupervised statistical word alignment Zhang and Gildea , 2005 ; Cherry and Lin , 2007a ; Zhang et al . , 2008 and machine translation decoding Cherry and Lin , 2007b ; Petrov et al . , 2008 .", "In this work , we investigate large scale , discriminative ITG word alignment .", "Past work on discriminative word alignment has focused on the family of at most one to one matchings Melamed , 2000 ; Taskar et al . , 2005 ; Moore et al . , 2006 .", "An exception to this is the work of Cherry and Lin 2006 , who discriminatively trained one to one ITG models , albeit with limited feature sets .", "As they found , ITG approaches offer several advantages over general matchings .", "First , the additional structural constraint can result in superior alignments .", "We confirm and extend this result , showing that one toone ITG models can perform as well as , or better than , general one to one matching models , either using heuristic weights or using rich , learned features .", "A second advantage of ITG approaches is that they admit a range of training options .", "As with general one to one matchings , we can optimize margin based objectives .", "However , unlike with general matchings , we can also efficiently compute expectations over the set of ITG derivations , enabling the training of conditional likelihood models .", "A major challenge in both cases is that our training alignments are often not one to one ITG alignments .", "Under such conditions , directly training to maximize margin is unstable , and training to maximize likelihood is ill defined , since the target alignment derivations don t exist in our hypothesis class .", "We show how to adapt both margin and likelihood objectives to learn good ITG aligners .", "In the case of likelihood training , two innovations are presented .", "The simple , two rule ITG grammar exponentially over counts certain alignment structures relative to others .", "Because of this , Wu 1997 and Zens and Ney 2003 introduced a normal form ITG which avoids this over counting .", "We extend this normal form to null productions and give the first extensive empirical comparison of simple and normal form ITGs , for posterior decoding under our likelihood models .", "Additionally , we show how to deal with training instances where the gold alignments are outside of the hypothesis class by instead optimizing the likelihood of a set of minimum loss alignments .", "Perhaps the greatest advantage of ITG models is that they straightforwardly permit blockstructured alignments i . e . phrases , which general matchings cannot efficiently do .", "The need for block alignments is especially acute in ChineseEnglish data , where oracle AERs drop from 10 . 2 without blocks to around 1 . 2 with them .", "Indeed , blocks are the primary reason for gold alignments being outside the space of one to one ITG alignments .", "We show that placing linear potential functions on many to one blocks can substantially improve performance .", "Finally , to scale up our system , we give a combination of pruning techniques that allows us to sum ITG alignments two orders of magnitude faster than naive inside outside parsing .", "All in all , our discriminatively trained , block ITG models produce alignments which exhibit the best AER on the NIST 2002 Chinese English alignment data set .", "Furthermore , they result in a 1 . 1 BLEU point improvement over GIZA alignments in an end to end Hiero Chiang , 2007 machine translation system .", "In order to structurally restrict attention to reasonable alignments , word alignment models must constrain the set of alignments considered .", "In this section , we discuss and compare alignment families used to train our discriminative models .", "Initially , as in Taskar et al . 2005 and Moore et al .", "2006 , we assume the score a of a potential alignment a decomposes as where sij are word to word potentials and siE and sEj represent English null and foreign null potentials , respectively .", "We evaluate our proposed alignments a against hand annotated alignments , which are marked with sure s and possible p alignments .", "The alignment error rate AER is given by , The class of at most 1 to 1 alignment matchings , A1 1 , has been considered in several works Melamed , 2000 ; Taskar et al . , 2005 ; Moore et al . , 2006 .", "The alignment that maximizes a set of potentials factored as in Equation 1 can be found in O n3 time using a bipartite matching algorithm Kuhn , 1955 . 1 On the other hand , summing over A1 1 is P hard Valiant , 1979 .", "Initially , we consider heuristic alignment potentials given by Dice coefficients where Cef is the joint count of words e , f appearing in aligned sentence pairs , and Ce and Cf are monolingual unigram counts .", "We extracted such counts from 1 . 1 million French English aligned sentence pairs of Hansards data see Section 6 . 1 .", "For each sentence pair in the Hansards test set , we predicted the alignment from A1 1 which maximized the sum of Dice potentials .", "This yielded 30 . 6 AER .", "Wu 1997 s inversion transduction grammar ITG is a synchronous grammar formalism in which derivations of sentence pairs correspond to alignments .", "In its original formulation , there is a single non terminal X spanning a bitext cell with an English and foreign span .", "There are three rule types Terminal unary productions X e , f , where e and f are an aligned English and foreign word pair possibly with one being null ; normal binary rules X _ X L X R , where the English and foreign spans are constructed from the children as X L X R , X L X R i ; and inverted binary rules X X L X R , where the foreign span inverts the order of the children X L X R , X R X L i . 2 In general , we will call a bitext cell a normal cell if it was constructed with a normal rule and inverted if constructed with an inverted rule .", "Each ITG derivation yields some alignment .", "The set of such ITG alignments , AITG , are a strict subset of A1 1 Wu , 1997 .", "Thus , we will view ITG as a constraint on A1 1 which we will argue is generally beneficial .", "The maximum scoring alignment from AITG can be found in O n6 time with synchronous CFG parsing ; in practice , we can make ITG parsing efficient using a variety of pruning techniques .", "One computational advantage of AITG over A1 1 alignments is that summation over AITG is tractable .", "The corresponding dynamic program allows us to utilize likelihoodbased objectives for learning alignment models see Section 4 .", "Using the same heuristic Dice potentials on the Hansards test set , the maximal scoring alignment from AITG yields 28 . 4 AER 2 . 4 better than A1 1 indicating that ITG can be beneficial as a constraint on heuristic alignments .", "An important alignment pattern disallowed by A1 1 is the many to one alignment block .", "While not prevalent in our hand aligned French Hansards dataset , blocks occur frequently in our handaligned Chinese English NIST data .", "Figure 1 contains an example .", "Extending A1 1 to include blocks is problematic , because finding a maximal 1 1 matching over phrases is NP hard DeNero and Klein , 2008 .", "With ITG , it is relatively easy to allow contiguous many to one alignment blocks without added complexity . 3 This is accomplished by adding additional unary terminal productions aligning a foreign phrase to a single English terminal or vice versa .", "We will use BITG to refer to this block ITG variant and ABITG to refer to the alignment family , which is neither contained in nor contains A1 1 .", "For this alignment family , we expand the alignment potential decomposition in Equation 1 to incorporate block potentials sef and sef which represent English and foreign many to one alignment blocks , respectively .", "One way to evaluate alignment families is to consider their oracle AER .", "In the 2002 NIST Chinese English hand aligned data see Section 6 . 2 , we constructed oracle alignment potentials as follows sij is set to 1 if i , j is a sure or possible alignment in the hand aligned data , 1 otherwise .", "All null potentials si , and s j are set to 0 .", "A max matching under these potentials is generally a minimal loss alignment in the family .", "The oracle AER computed in this was is 10 . 1 for A1 1 and 10 . 2 for AITG .", "The ABITG alignment family has an oracle AER of 1 . 2 .", "These basic experiments show that AITG outperforms A1 1 for heuristic alignments , and ABITG provide a much closer fit to true Chinese English alignments than A1 1 .", "In this and the next section , we discuss learning alignment potentials .", "As input , we have a training set D x1 , a 1 , . . . , x , a of hand aligned data , where x refers to a sentence pair .", "We will assume the score of a alignment is given as a linear function of a feature vector \u03c6 x , a .", "We will further assume the feature representation of an alignment , \u03c6 x , a decomposes as in Equation 1 , In the framework of loss augmented margin learning , we seek a w such that w \u03c6 x , a is larger than w \u03c6 x , a L a , a for all a in an alignment family , where L a , a is the loss between a proposed alignment a and the gold alignment a .", "As in Taskar et al . 2005 , we utilize a loss that decomposes across alignments .", "Specifically , for each alignment cell i , j which is not a possible alignment in a , we incur a loss of 1 when azo 6 a zo ; note that if i , j is a possible alignment , our loss is indifferent to its presence in the proposal alignment .", "A simple loss augmented learning procedure is the margin infused relaxed algorithm MIRA Crammer et al . , 2006 .", "MIRA is an online procedure , where at each time step s . t . w O x , a w O x , a L a , a where a arg max aEA In our data sets , many a are not in A1 1 and thus not in AITG , implying the minimum infamily loss must exceed 0 .", "Since MIRA operates in an online fashion , this can cause severe stability problems .", "On the Hansards data , the simple averaging technique described by Collins 2002 yields a reasonable model .", "On the Chinese NIST data , however , where almost no alignment is in A1 1 , the update rule from Equation 2 is completely unstable , and even the averaged model does not yield high quality results .", "We instead use a variant of MIRA similar to Chiang et al . 2008 .", "First , rather than update towards the hand labeled alignment a , we update towards an alignment which achieves minimal loss within the family . 4 We call this bestin class alignment a .", "Second , we perform lossaugmented inference to obtain a .", "This yields the modified QP , where a arg max aEA wt O x , a AL a , a By setting A 0 , we recover the MIRA update from Equation 2 .", "As A grows , we increase our preference that a have high loss relative to a rather than high model score .", "With this change , MIRA is stable , but still performs suboptimally .", "The reason is that initially the score for all alignments is low , so we are biased toward only using very high loss alignments in our constraint .", "This slows learning and prevents us from finding a useful weight vector .", "Instead , in all the experiments we report here , we begin with A 0 and slowly increase it to A 0 . 5 .", "An alternative to margin based training is a likelihood objective , which learns a conditional alignment distribution Pw a x parametrized as follows , where the log denominator represents a sum over the alignment family A .", "This alignment probability only places mass on members of A .", "The likelihood objective is given by , Optimizing this objective with gradient methods requires summing over alignments .", "For AITG and ABITG , we can efficiently sum over the set of ITG derivations in 0 n6 time using the inside outside algorithm .", "However , for the ITG grammar presented in Section 2 . 2 , each alignment has multiple grammar derivations .", "In order to correctly sum over the set of ITG alignments , we need to alter the grammar to ensure a bijective correspondence between alignments and derivations .", "There are two ways in which ITG derivations double count alignments .", "First , n ary productions are not binarized to remove ambiguity ; this results in an exponential number of derivations for diagonal alignments .", "This source of overcounting is considered and fixed by Wu 1997 and Zens and Ney 2003 , which we briefly review here .", "The resulting grammar , which does not handle null alignments , consists of a symbol N to represent a bitext cell produced by a normal rule and I for a cell formed by an inverted rule ; alignment terminals can be either N or I .", "In order to ensure unique derivations , we stipulate that a N cell can be constructed only from a sequence of smaller inverted cells I . Binarizing the rule N I2 introduces the intermediary symbol N see Figure 2 a .", "Similarly for inverse cells , we insist an I cell only be built by an inverted combination of N cells ; binarization of I N2 requires the introduction of the intermediary symbol I see Figure 2 b .", "Null productions are also a source of double counting , as there are many possible orders in which to attach null alignments to a bitext cell ; we address this by adapting the grammar to force a null attachment order .", "We introduce symbols N00 , N10 , and N11 to represent whether a normal cell has taken no nulls , is accepting foreign nulls , or is accepting English nulls , respectively .", "We also introduce symbols I00 , I10 , and I11 to represent inverse cells at analogous stages of taking nulls .", "As Figures 2 c and d illustrate , the directions in which nulls are attached to normal and inverse cells differ .", "The N00 symbol is constructed by one or more complete inverted cells I11 terminated by a no null I00 .", "By placing I00 in the lower right hand corner , we allow the larger N00 to unambiguously attach nulls .", "N00 transitions to the N10 symbol and accepts any number of e , English terminal alignments .", "Then N10 transitions to N11 and accepts any number of , f foreign terminal alignments .", "An analogous set of grammar rules exists for the inverted case see Figure 2 d for an illustration .", "Given this normal form , we can efficiently compute model expectations over ITG alignments without double counting . 5 To our knowledge , the alteration of the normal form to accommodate null emissions is novel to this work .", "A crucial obstacle for using the likelihood objective is that a given a may not be in the alignment family .", "As in our alteration to MIRA Section 3 , we could replace a with a minimal loss in class alignment a .", "However , in contrast to MIRA , the likelihood objective will implicitly penalize proposed alignments which have loss equal to a .", "We opt instead to maximize the probability of the set of alignments M a which achieve the same optimal in class loss .", "Concretely , let m be the minimal loss achievable relative to a in A .", "Then , When a is an ITG alignment i . e . , m is 0 , M a consists only of alignments which have all the sure alignments in a , but may have some subset of the possible alignments in a .", "See Figure 3 for a specific example where m 1 .", "Our modified likelihood objective is given by , Note that this objective is no longer convex , as it involves a logarithm of a summation , however we still utilize gradient based optimization .", "Summing and obtaining feature expectations over M a can be done efficiently using a constrained variant of the inside outside algorithm where sure alignments not present in a are disallowed , and the number of missing sure alignments is appended to the state of the bitext cell . 6 One advantage of the likelihood based objective is that we can obtain posteriors over individual alignment cells , We obtain posterior ITG alignments by including all alignment cells i , j such that PIV i , j x exceeds a fixed threshold t . Posterior thresholding allows us to easily trade off precision and recall in our alignments by raising or lowering t .", "Both discriminative methods require repeated model inference MIRA depends upon lossaugmented Viterbi parsing , while conditional likelihood uses the inside outside algorithm for computing cell posteriors .", "Exhaustive computation of these quantities requires an O n6 dynamic program that is prohibitively slow even on small supervised training sets .", "However , most of the search space can safely be pruned using posterior predictions from a simpler alignment models .", "We use posteriors from two jointly estimated HMM models to make pruning decisions during ITG inference Liang et al . , 2006 .", "Our first pruning technique is broadly similar to Cherry and Lin 2007a .", "We select high precision alignment links from the HMM models those word pairs that have a posterior greater than 0 . 9 in either model .", "Then , we prune all bitext cells that would invalidate more than 8 of these high precision alignments .", "Our second pruning technique is to prune all one by one word to word bitext cells that have a posterior below 10 4 in both HMM models .", "Pruning a one by one cell also indirectly prunes larger cells containing it .", "To take maximal advantage of this indirect pruning , we avoid explicitly attempting to build each cell in the dynamic program .", "Instead , we track bounds on the spans for which we have successfully built ITG cells , and we only iterate over larger spans that fall within those bounds .", "The details of a similar bounding approach appear in DeNero et al . 2009 .", "In all , pruning reduces MIRA iteration time from 175 to 5 minutes on the NIST ChineseEnglish dataset with negligible performance loss .", "Likelihood training time is reduced by nearly two orders of magnitude .", "We present results which measure the quality of our models on two hand aligned data sets .", "Our first is the English French Hansards data set from the 2003 NAACL shared task Mihalcea and Pedersen , 2003 .", "Here we use the same 337 100 train test split of the labeled data as Taskar et al . 2005 ; we compute external features from the same unlabeled data , 1 . 1 million sentence pairs .", "Our second is the Chinese English hand aligned portion of the 2002 NIST MT evaluation set .", "This dataset has 491 sentences , which we split into a training set of 150 and a test set of 191 .", "When we trained external Chinese models , we used the same unlabeled data set as DeNero and Klein 2007 , including the bilingual dictionary .", "For likelihood based models , we set the L2 regularization parameter , U2 , to 100 and the threshold for posterior decoding to 0 . 33 .", "We report results using the simple ITG grammar ITG S , Section 2 . 2 where summing over derivations double counts alignments , as well as the normal form ITG grammar ITG N , Section 4 . 1 which does not double count .", "We ran our annealed lossaugmented MIRA for 15 iterations , beginning with A at 0 and increasing it linearly to 0 . 5 .", "We compute Viterbi alignments using the averaged weight vector from this procedure .", "The French Hansards data are well studied data sets for discriminative word alignment Taskar et al . , 2005 ; Cherry and Lin , 2006 ; Lacoste Julien et al . , 2006 .", "For this data set , it is not clear that improving alignment error rate beyond that of GIZA is useful for translation Ganchev et al . , 2008 .", "Table 1 illustrates results for the Hansards data set .", "The first row uses dice and the same distance features as Taskar et al . 2005 .", "The first two rows repeat the experiments of Taskar et al . 2005 and Cherry and Lin 2006 , but adding ITG models that are trained to maximize conditional likelihood .", "The last row includes the posterior of the jointly trained HMM of Liang et al . 2006 as a feature .", "This model alone achieves an AER of 5 . 4 .", "No model significantly improves over the HMM alone , which is consistent with the results of Taskar et al . 2005 .", "Chinese English alignment is a much harder task than French English alignment .", "For example , the HMM aligner achieves an AER of 20 . 7 when using the competitive thresholding heuristic of DeNero and Klein 2007 .", "On this data set , our block ITG models make substantial performance improvements over the HMM , and moreover these results do translate into downstream improvements in BLEU score for the Chinese English language pair .", "Because of this , we will briefly describe the features used for these models in detail .", "For features on one by one cells , we consider Dice , the distance features from Taskar et al . , 2005 , dictionary features , and features for the 50 most frequent lexical pairs .", "We also trained an HMM aligner as described in DeNero and Klein 2007 and used the posteriors of this model as features .", "The first two columns of Table 2 illustrate these features for ITG and one to one matchings .", "For our block ITG models , we include all of these features , along with variants designed for many to one blocks .", "For example , we include the average Dice of all the cells in a block .", "In addition , we also created three new block specific features types .", "The first type comprises bias features for each block length .", "The second type comprises features computed from N gram statistics gathered from a large monolingual corpus .", "These include features such as the number of occurrences of the phrasal multi word side of a many to one block , as well as pointwise mutual information statistics for the multi word parts of many to one blocks .", "These features capture roughly how coherent the multi word side of a block is .", "The final block feature type consists of phrase shape features .", "These are designed as follows For each word in a potential many to one block alignment , we map an individual word to X if it is not one of the 25 most frequent words .", "Some example features of this type are , For English blocks , for example , these features capture the behavior of phrases such as in spite of or in front of that are rendered as one word in Chinese .", "For Chinese blocks , these features capture the behavior of phrases containing classifier phrases like or , which are rendered as English indefinite determiners .", "The right hand three columns in Table 2 present supervised results on our Chinese English data set using block features .", "We note that almost all of our performance gains relative to both the HMM and 1 1 matchings come from BITG and block features .", "The maximum likelihood trained normal form ITG model outperforms the HMM , even without including any features derived from the unlabeled data .", "Once we include the posteriors of the HMM as a feature , the AER decreases to 14 . 4 .", "The previous best AER result on this data set is 15 . 9 from Ayan and Dorr 2006 , who trained stacked neural networks based on GIZA alignments .", "Our results are not directly comparable they used more labeled data , but did not have the HMM posteriors as an input feature .", "We further evaluated our alignments in an end toend Chinese to English translation task using the publicly available hierarchical pipeline JosHUa Li and Khudanpur , 2008 .", "The pipeline extracts a Hiero style synchronous context free grammar Chiang , 2007 , employs suffix array based rule extraction Lopez , 2007 , and tunes model parameters with minimum error rate training Och , 2003 .", "We trained on the FBIS corpus using sentences up to length 40 , which includes 2 . 7 million English words .", "We used a 5 gram language model trained on 126 million words of the Xinhua section of the English Gigaword corpus , estimated with SRILM Stolcke , 2002 .", "We tuned on 300 sentences of the NIST MT04 test set .", "Results on the NIST MT05 test set appear in Table 3 .", "We compared four sets of alignments .", "The GIZA alignments7 are combined across directions with the grow diag final heuristic , which outperformed the union .", "The joint HMM alignments are generated from competitive posterior thresholding DeNero and Klein , 2007 .", "The ITG Viterbi alignments are the Viterbi output of the ITG model with all features , trained to maximize log likelihood .", "The ITG Posterior alignments result from applying competitive thresholding to alignment posteriors under the ITG model .", "Our supervised ITG model gave a 1 . 1 BLEU increase over GIZA .", "This work presented the first large scale application of ITG to discriminative word alignment .", "We empirically investigated the performance of conditional likelihood training of ITG word aligners under simple and normal form grammars .", "We showed that through the combination of relaxed learning objectives , many to one block alignment potential , and efficient pruning , ITG models can yield state of the art word alignments , even when the underlying gold alignments are highly nonITG .", "Our models yielded the lowest published error for Chinese English alignment and an increase in downstream translation performance ."], "summary_lines": ["Better Word Alignments with Supervised ITG Models\n", "This work investigates supervised word alignment methods that exploit inversion transduction grammar (ITG) constraints.\n", "We consider maximum margin and conditional likelihood objectives, including the presentation of a new normal form grammar for canonicalizing derivations.\n", "Even for non-ITG sentence pairs, we show that it is possible learn ITG alignment models by simple relaxations of structured discriminative learning objectives.\n", "For efficiency, we describe a set of pruning techniques that together allow us to align sentences two orders of magnitude faster than naive bitext CKY parsing.\n", "Finally, we introduce many-to-one block alignment features, which significantly improve our ITG models.\n", "Altogether, our method results in the best reported AER numbers for Chinese-English and a performance improvement of 1.1 BLEU over GIZA++ alignments.\n", "We describe a pruning heuristic that results in average case runtime of O (n 3).\n"]}
{"article_lines": ["A Maximum Entropy Inspired Parser", "We present a new parser for parsing down to Penn tree bank style parse trees that achieves 90 . 1 average precision recall for sentences of 40 and less , and for of length 100 and less when trained and tested on the previously established 5 , 9 , 10 , 15 , 17 quot ; standard quot ; sections of the Wall Street Journal treebank .", "This represents a 13 decrease in error rate over the best single parser results on this corpus 9 .", "The major technical innovation is the use of a quot ; maximum entropy inspired quot ; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events .", "We also present some partial results showing the effects of different conditioning information , including a surprising 2 improvement due to guessing the lexical head's pre terminal before guessing the lexical head .", "We present a new parser for parsing down to Penn tree bank style parse trees 16 that achieves 90 . 1 average precision recall for sentences of length 40 , and 89 . 5 for sentences of length 100 , when trained and tested on the previously established 5 , 9 , 10 , 15 , 17 quot ; standard quot ; sections of the Wall Street Journal tree bank .", "This represents a 13 decrease in error rate over the best single parser results on this corpus 9 .", "Following 5 , 10 , our parser is based upon a probabilistic generative model .", "That is , for all sentences s and all parses 7r , the parser assigns a probability p s , 7r p r , the equality holding when we restrict consideration to 7r whose yield This research was supported in part by NSF grant LIS SBR 9720368 .", "The author would like to thank Mark Johnson and all the rest of the Brown Laboratory for Linguistic Information Processing . is s . Then for any s the parser returns the parse ir that maximizes this probability .", "That is , the parser implements the function arg maxrp 7r s arg maxirp 7r , s arg maxrp w .", "What fundamentally distinguishes probabilistic generative parsers is how they compute p r , and it is to that topic we turn next .", "The model assigns a probability to a parse by a top down process of considering each constituent c in Ir and for each c first guessing the pre terminal of c , t c t for quot ; tag quot ; , then the lexical head of c , h c , and then the expansion of c into further constituents e c .", "Thus the probability of a parse is given by the equation where 1 c is the label of c e . g . , whether it is a noun phrase np , verb phrase , etc . and H c is the relevant history of c information outside c that our probability model deems important in determining the probability in question .", "Much of the interesting work is determining what goes into H c .", "Whenever it is clear to which constituent we are referring we omit the c in , e . g . , h c .", "In this notation the above equation takes the following form Next we describe how we assign a probability to the expansion e of a constituent .", "In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree bank grammar 3 from the training corpus .", "The method that gives the best results , however , uses a Markov grammar a method for assigning probabilities to any possible expansion using statistics gathered from the training corpus 6 , 10 , 15 .", "The method we use follows that of 10 .", "In this scheme a traditional probabilistic context free grammar PCFG rule can be thought of as consisting of a left hand side with a label 1 c drawn from the non terminal symbols of our grammar , and a right hand side that is a sequence of one or more such symbols .", "We assume that all terminal symbols are generated by rules of the form quot ; preterm word quot ; and we treat these as a special case .", "For us the non terminal symbols are those of the tree bank , augmented by the symbols aux and auxg , which have been assigned deterministically to certain auxiliary verbs such as quot ; have quot ; or quot ; having quot ; .", "For each expansion we distinguish one of the right hand side labels as the quot ; middle quot ; or quot ; head quot ; symbol M c .", "M c is the constituent from which the head lexical item h is obtained according to deterministic rules that pick the head of a constituent from among the heads of its children .", "To the left of M is a sequence of one or more left labels Li c including the special termination symbol A , which indicates that there are no more symbols to the left , and similarly for the labels to the right , Ri c .", "Thus an expansion e c looks like The expansion is generated by guessing first M , then in order L1 through L , . 1 A , and similarly for RI through In a pure Markov PCFG we are given the left hand side label 1 and then probabilistically generate the right hand side conditioning on no information other than 1 and possibly previously generated pieces of the right hand side itself .", "In the simplest of such models , a zeroorder Markov grammar , each label on the righthand side is generated conditioned only on that is , according to the distributions p Li j1 , p M I 1 , and p Ri I 1 .", "More generally , one can condition on the m previously generated labels , thereby obtaining an mth order Markov grammar .", "So , for example , in a second order Markov PCFG , L2 would be conditioned on L1 and M . In our complete model , of course , the probability of each label in the expansions is also conditioned on other material as specified in Equation 1 , e . g . , p e t , h , H .", "Thus we would use p L2 I L1 , M , 1 , t , h , H .", "Note that the As on both ends of the expansion in Expression 2 are conditioned just like any other label in the expansion .", "The major problem confronting the author of a generative parser is what information to use to condition the probabilities required in the model , and how to smooth the empirically obtained probabilities to take the sting out of the sparse data problems that are inevitable with even the most modest conditioning .", "For example , in a second order Markov grammar we conditioned the L2 label according to the distribution p L2 I L1 , M , 1 , t , h , H .", "Also , remember that H is a pla , ceholder for any other information beyond the constituent c that may be useful in assigning c a probability .", "In the past few years the maximum entropy , or log linear , approach has recommended itself to probabilistic model builders for its flexibility and its novel approach to smoothing 1 , 17 .", "A complete review of log linear models is beyond the scope of this paper .", "Rather , we concentrate on the aspects of these models that most directly influenced the model presented here .", "To compute a probability in a log linear model one first defines a set of quot ; features quot ; , functions from the space of configurations over which one is trying to compute probabilities to integers that denote the number of times some pattern occurs in the input .", "In our work we assume that any feature can occur at most once , so features are boolean valued 0 if the pattern does not occur , 1 if it does .", "In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub features .", "For example , in computing the probability of the head's pre terminal t we might want a feature schema f t , 1 that returns 1 if the observed pre terminal of c t and the label of c 1 , and zero otherwise .", "This feature is obviously composed of two sub features , one recognizing t , the other 1 .", "If both return 1 , then the feature returns 1 .", "Now consider computing a conditional probability p a H with a set of features h that connect a to the history H . In a log linear model the probability function takes the following form Here the Ai are weights between negative and positive infinity that indicate the relative importance of a feature the more relevant the feature to the value of the probability , the higher the absolute value of the associated A .", "The function Z H , called the partition function , is a normalizing constant for fixed H , so the probabilities over all a sum to one .", "Now for our purposes it is useful to rewrite this as a sequence of multiplicative functions gi a , H for 0 i j Here go a , H 11Z H and gi a , H eAi a , H fi , 11 .", "The intuitive idea is that each factor gi is larger than one if the feature in question makes the probability more likely , one if the feature has no effect , and smaller than one if it makes the probability less likely .", "Maximum entropy models have two benefits for a parser builder .", "First , as already implicit in our discussion , factoring the probability computation into a sequence of values corresponding to various quot ; features quot ; suggests that the probability model should be easily changeable just change the set of features used .", "This point is emphasized by Ratnaparkhi in discussing his parser 17 .", "Second , and this is a point we have not yet mentioned , the features used in these models need have no particular independence of one another .", "This is useful if one is using a loglinear model for smoothing .", "That is , suppose we want to compute a conditional probability p a b , c , but we are not sure that we have enough examples of the conditioning event b , c in the training corpus to ensure that the empirically obtained probability P a I b , c is accurate .", "The traditional way to handle this is also to compute P a b , and perhaps P a c as well , and take some combination of these values as one's best estimate for p a I b , c .", "This method is known as quot ; deleted interpolation quot ; smoothing .", "In max entropy models one can simply include features for all three events f1 a , b , c , f2 a , b , and f3 a , c and combine them in the model according to Equation 3 , or equivalently , Equation 4 .", "The fact that the features are very far from independent is not a concern .", "Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate .", "As it stands , this last equation is pretty much content free .", "But let us look at how it works for a particular case in our parsing scheme .", "Consider the probability distribution for choosing the pre terminal for the head of a constituent .", "In Equation 1 we wrote this as p t I 1 , H .", "As we discuss in more detail in Section 5 , several different features in the context surrounding c are useful to include in H the label , head pre terminal and head of the parent of c denoted as lp , tp , hp , the label of c's left sibling lb for quot ; before quot ; , and the label of the grandparent of c la .", "That is , we wish to compute p t 1 , lp , tp , lb , lg , hp .", "We can now rewrite this in the form of Equation 5 as follows Here we have sequentially conditioned on steadily increasing portions of c's history .", "In many cases this is clearly warranted .", "For example , it does not seem to make much sense to condition on , say , hp without first conditioning on ti , .", "In other cases , however , we seem to be conditioning on apples and oranges , so to speak .", "For example , one can well imagine that one might want to condition on the parent's lexical head without conditioning on the left sibling , or the grandparent label .", "One way to do this is to modify the simple version shown in Equation 6 to allow this Note the changes to the last three terms in Equation 7 .", "Rather than conditioning each term on the previous ones , they are now conditioned only on those aspects of the history that seem most relevant .", "The hope is that by doing this we will have less difficulty with the splitting of conditioning events , and thus somewhat less difficulty with sparse data .", "We make one more point on the connection of Equation 7 to a maximum entropy formulation .", "Suppose we were , in fact , going to compute a true maximum entropy model based upon the features used in Equation 7 , Ii t , 1 , f2 t , 1 , 1p , f3 t , 1 , lp .", "This requires finding the appropriate Ais for Equation 3 , which is accomplished using an algorithm such as iterative scaling II in which values for the Ai are initially quot ; guessed quot ; and then modified until they converge on stable values .", "With no prior knowledge of values for the Ai one traditionally starts with Ai 0 , this being a neutral assumption that the feature has neither a positive nor negative impact on the probability in question .", "With some prior knowledge , non zero values can greatly speed up this process because fewer iterations are required for convergence .", "We comment on this because in our example we can substantially speed up the process by choosing values picked so that , when the maximum entropy equation is expressed in the form of Equation 4 , the gi have as their initial values the values of the corresponding terms in Equation 7 .", "Our experience is that rather than requiring 50 or so iterations , three suffice .", "Now we observe that if we were to use a maximum entropy approach but run iterative scaling zero times , we would , in fact , just have Equation 7 .", "The major advantage of using Equation 7 is that one can generally get away without computing the partition function Z H .", "In the simple content free form Equation 6 , it is clear that Z H 1 .", "In the more interesting version , Equation 7 , this is not true in general , but one would not expect it to differ much from one , and we assume that as long as we are not publishing the raw probabilities as we would be doing , for example , in publishing perplexity results the difference from one should be unimportant .", "As partition function calculation is typically the major on line computational problem for maximum entropy models , this simplifies the model significantly .", "Naturally , the distributions required by Equation 7 cannot be used without smoothing .", "In a pure maximum entropy model this is done by feature selection , as in Ratnaparkhi's maximum entropy parser 17 .", "While we could have smoothed in the same fashion , we choose instead to use standard deleted interpolation .", "Actually , we use a minor variant described in 4 .", "We created a parser based upon the maximumentropy inspired model of the last section , smoothed using standard deleted interpolation .", "As the generative model is top down and we use a standard bottom up best first probabilistic chart parser 2 , 7 , we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model .", "For runs with the generative model based upon Markov grammar statistics , the first pass uses the same statistics , but conditioned only on standard PCFG information .", "This allows the second pass to see expansions not present in the training corpus .", "We use the gathered statistics for all observed words , even those with very low counts , though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words .", "We guess the preterminals of words that are not observed in the training data using statistics on capitalization , hyphenation , word endings the last two letters , and the probability that a given pre terminal is realized using a previously unobserved word .", "As noted above , the probability model uses five smoothed probability distributions , one each for Li , M , Ri , t , and h . The equation for the unsmoothed conditional probability distribution for t is given in Equation 7 .", "The other four equations can be found in a longer version of this paper available on the author's website www . cs . brown . eduhiec .", "L and R are conditioned on three previous labels so we are using a third order Markov grammar .", "Also , the label of the parent constituent lp is conditioned upon even when it is not obviously related to the further conditioning events .", "This is due to the importance of this factor in parsing , as noted in , e . g . , 14 .", "In keeping with the standard methodology 5 , 9 , 10 , 15 , 17 , we used the Penn Wall Street Journal tree bank 16 with sections 2 21 for training , section 23 for testing , and section 24 for development debugging and tuning .", "Performance on the test corpus is measured using the standard measures from 5 , 9 , 10 , 17 .", "In particular , we measure labeled precision LP and recall LR , average number of crossbrackets per sentence CB , percentage of sentences with zero cross brackets OCB , and percentage of sentences with 2 cross brackets 2CB .", "Again as standard , we take separate measurements for all sentences of length 40 and all sentences of length 100 .", "Note that the definitions of labeled precision and recall are those given in 9 and used in all of the previous work .", "As noted in 5 , these definitions typically give results about 0 . 4 higher than the more obvious ones .", "The results for the new parser as well as for the previous top three individual parsers on this corpus are given in Figure 1 .", "As is typical , all of the standard measures tell pretty much the same story , with the new parser outperforming the other three parsers .", "Looking in particular at the precision and recall figures , the new parser's give us a 13 error reduction over the best of the previous work , Co1199 9 .", "In the previous sections we have concentrated on the relation of the parser to a maximumentropy approach , the aspect of the parser that is most novel .", "However , we do not think this aspect is the sole or even the most important reason for its comparative success .", "Here we list what we believe to be the most significant contributions and give some experimental results on how well the program behaves without them .", "We take as our starting point the parser labled Char97 in Figure 1 5 , as that is the program from which our current parser derives .", "That parser , as stated in Figure 1 , achieves an average precision recall of 87 . 5 .", "As noted in 5 , that system is based upon a quot ; tree bank grammar quot ; a grammar read directly off the training corpus .", "This is as opposed to the quot ; Markovgrammar quot ; approach used in the current parser .", "Also , the earlier parser uses two techniques not employed in the current parser .", "First , it uses a clustering scheme on words to give the system a quot ; soft quot ; clustering of heads and sub heads .", "It is quot ; soft quot ; clustering in that a word can belong to more than one cluster with different weights the weights express the probability of producing the word given that one is going to produce a word from that cluster .", "Second , Char97 uses unsupervised learning in that the original system was run on about thirty million words of unparsed text , the output was taken as quot ; correct quot ; , and statistics were collected on the resulting parses .", "Without these enhancements Char97 performs at the 86 . 6 level for sentences of length 40 .", "In this section we evaluate the effects of the various changes we have made by running various versions of our current program .", "To avoid repeated evaluations based upon the testing corpus , here our evaluation is based upon sentences of length 40 from the development corpus .", "We note here that this corpus is somewhat more difficult than the quot ; official quot ; test corpus .", "For example , the final version of our system achieves an average precision recall of 90 . 1 on the test corpus but an average precision recall of only 89 . 7 on the development corpus .", "This is indicated in Figure 2 , where the model labeled quot ; Best quot ; has precision of 89 . 8 and recall of 89 . 6 for an average of 89 . 7 , 0 . 4 lower than the results on the official test corpus .", "This is in accord with our experience that developmentcorpus results are from 0 . 3 to 0 . 5 lower than those obtained on the test corpus .", "The model labeled quot ; Old quot ; attempts to recreate the Char97 system using the current program .", "It makes no use of special maximum entropyinspired features though their presence made it much easier to perform these experiments , it does not guess the pre terminal before guessing the lexical head , and it uses a tree bank grammar rather than a Markov grammar .", "This parser achieves an average precision recall of 86 . 2 .", "This is consistent with the average precision recall of 86 . 6 for 5 mentioned above , as the latter was on the test corpus and the former on the development corpus .", "Between the Old model and the Best model , Figure 2 gives precision recall measurements for several different versions of our parser .", "One of the first and without doubt the most significant change we made in the current parser is to move from two stages of probabilistic decisions at each node to three .", "As already noted , Char97 first guesses the lexical head of a constituent and then , given the head , guesses the PCFG rule used to expand the constituent in question .", "In contrast , the current parser first guesses the head's pre terminal , then the head , and then the expansion .", "It turns out that usefulness of this process had already been discovered by Collins 10 , who in turn notes personal communication that it was previously used by Eisner 12 .", "However , Collins in 10 does not stress the decision to guess the head's pre terminal first , and it might be lost on the casual reader .", "Indeed , it was lost on the present author until he went back after the fact and found it there .", "In Figure 2 we show that this one factor improves performance by nearly 2 .", "It may not be obvious why this should make so great a difference , since most words are effectively unambiguous .", "For example , part ofspeech tagging using the most probable preterminal for each word is 90 accurate 8 .", "We believe that two factors contribute to this performance gain .", "The first is simply that if we first guess the pre terminal , when we go to guess the head the first thing we can condition upon is the pre terminal , i . e . , we compute p h I t .", "This quantity is a relatively intuitive one as , for example , it is the quantity used in a PCFG to relate words to their pre terminals and it seems particularly good to condition upon here since we use it , in effect , as the unsmoothed probability upon which all smoothing of p h is based .", "This one quot ; fix quot ; makes slightly over a percent difference in the results .", "The second major reason why first guessing the pre terminal makes so much difference is that it can be used when backing off the lexical head in computing the probability of the rule expansion .", "For example , when we first guess the lexical head we can move from computing p r I 1 , 1p , h to p r I , t , p , h .", "So , e . g . , even if the word quot ; conflating quot ; does not appear in the training corpus and it does not , the quot ; ng quot ; ending allows our program to guess with relative security that the word has the vbg pre terminal , and thus the probability of various rule expansions can be considerable sharpened .", "For example , the tree bank PCFG probability of the rule quot ; VP vbg np quot ; is 0 . 0145 , whereas once we condition on the fact that the lexical head is a vbg we get a probability of 0 . 214 .", "The second modification is the explicit marking of noun and verb phrase coordination .", "We have already noted the importance of conditioning on the parent label p .", "So , for example , information about an np is conditioned on the parent e . g . , an s , vp , pp , etc .", "Note that when an np is part of an np coordinate structure the parent will itself be an np , and similarly for a vp .", "But nps and vps can occur with np and vp parents in non coordinate structures as well .", "For example , in the Penn Treebank a vp with both main and auxiliary verbs has the structure shown in Figure 3 .", "Note that the subordinate vp has a vp parent .", "Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure .", "A vp coordinate structure is defined here as a constituent with two or more vp children , one or more of the constituents comma , cc , conjp conjunctive phrase , and nothing else ; coordinate np phrases are defined similarly .", "Something very much like this is done in 15 .", "As shown in Figure 2 , conditioning on this information gives a 0 . 6 improvement .", "We believe that this is mostly due to improvements in guessing the sub constituent's pre terminal and head .", "Given we are already at the 88 level of accuracy , we judge a 0 . 6 improvement to be very much worth while .", "Next we add the less obvious conditioning events noted in our previous discussion of the final model grandparent label lg and left sibling label b .", "When we do so using our maximum entropy inspired conditioning , we get another 0 . 45 improvement in average precision recall , as indicated in Figure 2 on the line labeled quot ; MaxEnt Inspired' .", "Note that we also tried including this information using a standard deleted interpolation model .", "The results here are shown in the line quot ; Standard Interpolation quot ; .", "Including this information within a standard deleted interpolation model causes a 0 . 6 decrease from the results using the less conventional model .", "Indeed , the resulting performance is worse than not using this information at all .", "Up to this point all the models considered in this section are tree bank grammar models .", "That is , the PCFG grammar rules are read directly off the training corpus .", "As already noted , our best model uses a Markov grammar approach .", "As one can see in Figure 2 , a firstorder Markov grammar with all the aforementioned improvements performs slightly worse than the equivalent tree bank grammar parser .", "However , a second order grammar does slightly better and a third order grammar does significantly better than the tree bank parser .", "We have presented a lexicalized Markov grammar parsing model that achieves using the now standard training testing development sections of the Penn treebank an average precision recall of 91 . 1 on sentences of length 40 and 89 . 5 on sentences of length 100 .", "This corresponds to an error reduction of 13 over the best previously published single parser results on this test set , those of Collins 9 .", "That the previous three best parsers on this test 5 , 9 , 17 all perform within a percentage point of each other , despite quite different basic mechanisms , led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training , and to conjecture that perhaps we were at it .", "The results reported here disprove this conjecture .", "The results of 13 achieved by combining the aforementioned three best parsers also suggest that the limit on tree bank trained parsers is much higher than previously thought .", "Indeed , it may be that adding this new parser to the mix may yield still higher results .", "From our perspective , perhaps the two most important numbers to come out of this research are the overall error reduction of 13 over the results in 9 and the intermediateresult improvement of nearly 2 on labeled precision recall due to the simple idea of guessing the head's pre terminal before guessing the head .", "Neither of these results were anticipated at the start of this research .", "As noted above , the main methodological innovation presented here is our quot ; maximumentropy inspired quot ; model for conditioning and smoothing .", "Two aspects of this model deserve some comment .", "The first is the slight , but important , improvement achieved by using this model over conventional deleted interpolation , as indicated in Figure 2 .", "We expect that as we experiment with other , more semantic conditioning information , the importance of this aspect of the model will increase .", "More important in our eyes , though , is the flexibility of the maximum entropy inspired model .", "Though in some respects not quite as flexible as true maximum entropy , it is much simpler and , in our estimation , has benefits when it comes to smoothing .", "Ultimately it is this flexibility that let us try the various conditioning events , to move on to a Markov grammar approach , and to try several Markov grammars of different orders , without significant programming .", "Indeed , we initiated this line of work in an attempt to create a parser that would be flexible enough to allow modifications for parsing down to more semantic levels of detail .", "It is to this project that our future parsing work will be devoted ."], "summary_lines": ["A Maximum-Entropy-Inspired Parser\n", "We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trained and tested on the previously established [5,9,10,15,17] \"standard\" sections of the Wall Street Journal tree- bank.\n", "This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].\n", "The major technical innovation is the use of a \"maximum-entropy-inspired\" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.\n", "We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.\n", "As an alternative to hard coded heuristics, we proposed to recover the Penn functional tags automatically.\n", "Our parser is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents.\n"]}
{"article_lines": ["Identifying Word Translations In Non Parallel Texts", "Common algorithms for sentence and word alignment allow the automatic identification of word translations from parallel texts .", "This study suggests that the identification of word translations should also be possible with non parallel and even unrelated texts .", "The method proposed is based on the assumption that there is a correlation between the patterns of word cooccurrences in texts of different languages .", "In a number of recent studies it has been shown that word translations can be automatically derived from the statistical distribution of words in bilingual parallel texts e . g . Catizone , Russell Warwick , 1989 ; Brown et al . , 1990 ; Dagan , Church Gale , 1993 ; Kay ROscheisen , 1993 .", "Most of the proposed algorithms first conduct an alignment of sentences , i . e . those pairs of sentences are located that are translations of each other .", "In a second step a word alignment is performed by analyzing the correspondences of words in each pair of sentences .", "The results achieved with these algorithms have been found useful for the compilation of dictionaries , for checking the consistency of terminological usage in translations , and for assisting the terminological work of translators and interpreters .", "However , despite serious efforts in the compilation of corpora Church Mercer , 1993 ; Armstrong Thompson , 1995 the availability of a large enough parallel corpus in a specific field and for a given pair of languages will always be the exception , not the rule .", "Since the acquisition of non parallel texts is usually much easier , it would be desirable to have a program that can determine the translations of words from comparable or even unrelated texts .", "It is assumed that there is a correlation between the co occurrences of words which are translations of each other .", "If for example in a text of one language two words A and B co occur more often than expected from chance , then in a text of another language those words which are translations of A and B should also co occur more frequently than expected .", "This assumption is reasonable for parallel texts .", "However , in this paper it is further assumed that the co occurrence patterns in original texts are not fundamentally different from those in translated texts .", "Starting from an English vocabulary of six words and the corresponding German translations , table la and b show an English and a German co occurrence matrix .", "In these matrices the entries belonging to those pairs of words that in texts co occur more frequently than expected have been marked with a dot .", "In general , word order in the lines and columns of a co occurrence matrix is independent of each other , but for the purpose of this paper can always be assumed to be equal without loss of generality .", "If now the word order of the English matrix is permuted until the resulting pattern of dots is most similar to that of the German matrix see table lc , then this increases the likelihood that the English and German words are in corresponding order .", "Word a in the English matrix is then the translation of word a in the German matrix .", "A simulation experiment was conducted in order to see whether the above assumptions concerning the similarity of co occurrence patterns actually hold .", "In this experiment , for an equivalent English and German vocabulary two co occurrence matrices were computed and then compared .", "As the English vocabulary a list of 100 words was used , which had been suggested by Kent Rosanoff 1910 for association experiments .", "The German vocabulary consisted of one by one translations of these words as chosen by Russell 1970 .", "The word co occurrences were computed on the basis of an English corpus of 33 and a German corpus of 46 million words .", "The English corpus consists of the Brown Corpus , texts from the Wall Street Journal , Grolier 's Electronic Encyclopedia and scientific abstracts from different fields .", "The German corpus is a compilation of mainly newspaper texts from Frankfurter Rundschau , Die Zeit and Mannheimer Morgen .", "To the knowledge of the author , the English and German corpora contain no parallel passages .", "For each pair of words in the English vocabulary its frequency of common occurrence in the English corpus was counted .", "The common occurrence of two words was defined as both words being separated by at most 11 other words .", "The co occurrence frequencies obtained in this way were used to build up the English matrix .", "Equivalently , the German co occurrence matrix was created by counting the co occurrences of German word pairs in the German corpus .", "As a starting point , word order in the two matrices was chosen such that word n in the German matrix was the translation of word n in the English matrix .", "Co occurrence studies like that conducted by Wettler St Rapp 1993 have shown that for many purposes it is desirable to reduce the influence of word frequency on the co occurrence counts .", "For the prediction of word associations they achieved best results when modifying each entry in the cooccurrence matrix using the following formula Hereby f i j is the frequency of common occurrence of the two words i and j , and f i is the corpus frequency of word i .", "However , for comparison , the simulations described below were also conducted using the original co occurrence matrices formula 2 and a measure similar to mutual information formula 3 . 1 Regardless of the formula applied , the English and the German matrix where both normalized . 2 Starting from the normalized English and German matrices , the aim was to determine how far the similarity of the two matrices depends on the correspondence of word order .", "As a measure for matrix similarity the sum of the absolute differences of the values at corresponding matrix positions was used .", "This similarity measure leads to a value of zero for identical matrices , and to a value of 20 000 in the case that a non zero entry in one of the 100 100 matrices always corresponds to a zero value in the other .", "The simulation was conducted by randomly permuting the word order of the German matrix and then computing the similarity s to the English matrix .", "For each permutation it was determined how many words c had been shifted to positions different from those in the original German matrix .", "The simulation was continued until for each value of c a set of 1000 similarity values was available . '", "Figure 1 shows for the three formulas how the average similarity a between the English and the German matrix depends on the number of non corresponding word positions c . Each of the curves increases monotonically , with formula 1 having the steepest , i . e . best discriminating characteristic .", "The dotted curves in figure 1 are the minimum and maximum values in each set of 1000 similarity values for formula 1 .", "'The logarithm has been removed from the mutual information measure since it is not defined for zero cooccurrences .", "2 Normalization was conducted in such a way that the sum of all matrix entries adds up to the number of fields in the matrix . of the English and the German matrix and the number of non corresponding word positions c for 3 formulas .", "The dotted lines are the minimum and maximum values of each sample of 1000 for formula 1 .", "It could be shown that even for unrelated English and German texts the patterns of word cooccurrences strongly correlate .", "The monotonically increasing character of the curves in figure 1 indicates that in principle it should be possible to find word correspondences in two matrices of different languages by randomly permuting one of the matrices until the similarity function s reaches a minimum and thus indicates maximum similarity .", "However , the minimum curve in figure 1 suggests that there are some deep minima of the similarity function even in cases when many word correspondences are incorrect .", "An algorithm currently under construction therefore searches for many local minima , and tries to find out what word correspondences are the most reliable ones .", "In order to limit the search space , translations that are known beforehand can be used as anchor points .", "Future work will deal with the following as yet unresolved problems Computational limitations require the vocabularies to be limited to subsets of all word types in large corpora .", "With criteria like the corpus frequency of a word , its specificity for a given domain , and the salience of its co occurrence patterns , it should be possible to make a selection of corresponding vocabularies in the two languages .", "If morphological tools and disambiguators are available , preliminary lemmatization of the corpora would be desirable .", "Ambiguities in word translations can be taken into account by working with continuous probabilities to judge whether a word translation is correct instead of making a binary decision .", "Thereby , different sizes of the two matrices could be allowed for .", "It can be expected that with such a method the quality of the results depends on the thematic comparability of the corpora , but not on their degree of parallelism .", "As a further step , even with non parallel corpora it should be possible to locate comparable passages of text .", "I thank Susan Armstrong and Manfred Wettler for their support of this project .", "Thanks also to Graham Russell and three anonymous reviewers for valuable comments on the manuscript ."], "summary_lines": ["Identifying Word Translations In Non-Parallel Texts\n", "Common algorithms for sentence and word-alignment allow the automatic identification of word translations from parallel texts.\n", "This study suggests that the identification of word translations should also be possible with non-parallel and even unrelated texts.\n", "The method proposed is based on the assumption that there is a correlation between the patterns of word co-occurrences in texts of different languages.\n", "We propose a computationally demanding matrix purmutation method which maximizes a similarity between co-occurence matrices in two languages.\n", "An underlying assumption in our work is that translations of words that are related in one language are also related in the other language.\n"]}
{"article_lines": ["Building A Large Scale Annotated Chinese Corpus", "In this paper we address issues related to building a large scale Chinese corpus .", "We try to answer four questions i how to speed up annotation , ii how to maintain high annotation quality , iii for what purposes is the corpus applicable , and finally iv what future work we anticipate .", "The Penn Chinese Treebank CTB is an ongoing project , with its objective being to create a segmented Chinese corpus annotated with POS tags and syntactic brackets .", "The first installment of the project CTB I consists of Xinhua newswire between the years 1994 and 1998 , totaling 100 , 000 words , fully segmented , POS tagged and syntactically bracketed and it has been released to the public via the Penn Linguistic Data Consortium LDC .", "The preliminary results of this phase of the project have been reported in Xia et al 2000 .", "Currently the second installment of the project , the 400 , 000 word CTB II is being developed and is expected to be completed early in the year 2003 .", "CTB II will follow the standards set up in the segmentation Xia 2000b , POS tagging Xia 2000a and bracketing guidelines Xue and Xia 2000 and it will use articles from Peoples' Daily , Hong Kong newswire and material translated into Chinese from other languages in addition to the Xinhua newswire used in CTB I in an effort to diversify the sources .", "The availability of CTB I changed our approach to CTB II considerably .", "Due to the existence of CTB I , we were able to train new automatic Chinese language processing CLP tools , which crucially use annotated corpora as training material .", "These tools are then used for preprocessing in the development of the CTB II .", "We also developed tools to control the quality of the corpus .", "In this paper , we will address three issues in the development of the Chinese Treebank annotation speed , annotation accuracy and usability of the corpus .", "Specifically , we attempt to answer four questions i how do we speed up the annotation process , ii how do we maintain high quality , i . e . annotation accuracy and inter annotator consistency during the annotation process , and iii for what purposes is the corpus applicable , and iv what are our future plans ?", "Although we will touch upon linguistic problems that are specific to Chinese , we believe these issues are general enough for the development of any single language corpus .", "1 Annotation Speed .", "There are three main factors that affect the annotation speed annotators ?", "background , guideline design and more importantly , the availability of preprocessing tools .", "We will discuss how each of these three factors affects annotation speed .", "1 . 1 Annotator Background .", "Even with the best sets of guidelines , it is important that annotators have received considerable training in linguistics , particularly in syntax .", "In both the segmentation POS tagging phase and the syntactic bracketing phase , understanding the structure of the sentences is essential for correct annotation with reasonable speed .", "For example ,"], "summary_lines": ["Building A Large-Scale Annotated Chinese Corpus\n", "In this paper we address issues related to building a large-scale Chinese corpus.\n", "We try to answer four questions: (i) how to speed up annotation, (ii) how to maintain high annotation quality, (iii) for what purposes is the corpus applicable, and finally (iv) what future work we anticipate.\n"]}
{"article_lines": ["A Statistical Parser For Czech", "This paper considers statistical parsing of Czech , which differs radically from English in at least two 1 it is a inflected and it has relatively word order . differences are likely to pose new problems for techniques that have been developed on English .", "We describe our experience in building on the parsing model of Collins 97 .", "Our final results 80 dependency accuracy represent good progress towards the 91 accuracy of the parser on English Wall Street Journal text .", "Much of the recent research on statistical parsing has focused on English ; languages other than English are likely to pose new problems for statistical methods .", "This paper considers statistical parsing of Czech , using the Prague Dependency Treebank PDT Haji , 1998 as a source of training and test data the PDT contains around 480 , 000 words of general news , business news , and science articles Other Slavic languages such as Polish , Russian , Slovak , Slovene , Serbo croatian , Ukrainian also show these characteristics .", "Many European languages exhibit FWO and HI phenomena to a lesser extent .", "Thus the techniques and results found for Czech should be relevant to parsing several other languages .", "This paper first describes a baseline approach , based on the parsing model of Collins 97 , which recovers dependencies with 72 accuracy .", "We then describe a series of refinements to the model , giving an improvement to 80 accuracy , with around 82 accuracy on newspaper business articles .", "As a point of comparison , the parser achieves 91 dependency accuracy on English Wall Street Journal text .", "The Prague Dependency Treebank PDT Hap , 1998 has been modeled after the Penn Treebank Marcus et al . 93 , with one important exception following the Praguian linguistic tradition , the syntactic annotation is based on dependencies rather than phrase structures .", "Thus instead of quot ; nonterminal quot ; symbols used at the non leaves of the tree , the PDT uses so called analytical functions capturing the type of relation between a dependent and its governing node .", "Thus the number of nodes is equal to the number of tokens words punctuation plus one an artificial root node with rather technical function is added to each sentence .", "The PDT contains also a traditional morpho syntactic annotation tags at each word position together with a lemma , uniquely representing the underlying lexical unit .", "As Czech is a HI language , the size of the set of possible tags is unusually high more than 3 , 000 tags may be assigned by the Czech morphological analyzer .", "The PDT also contains machine assigned tags and lemmas for each word using a tagger described in Haji 6 and Hladka , 1998 .", "For evaluation purposes , the PDT has been divided into a training set 19k sentences and a development evaluation test set pair about 3 , 500 sentences each .", "Parsing accuracy is defined as the ratio of correct dependency links vs . the total number of dependency links in a sentence which equals , with the one artificial root node added , to the number of tokens in a sentence .", "As usual , with the development test set being available during the development phase , all final results has been obtained on the evaluation test set , which nobody could see beforehand .", "The parsing model builds on Model 1 of Collins 97 ; this section briefly describes the model .", "The parser uses a lexicalized grammar each nonterminal has an associated head word and part ofspeech POS .", "We write non terminals as X x X is the non terminal label , and x is a w , t pair where w is the associated head word , and t as the POS tag .", "See figure 1 for an example lexicalized tree , and a list of the lexicalized rules that it contains .", "Each rule has the fonnl 'With the exception of the top rule in the tree , which has the form TOP H h .", "H is the head child of the phrase , which inherits the head word h from its parent P . L1 . . . Ln and RI are left and right modifiers of H . Either n or m may be zero , and n The model can be considered to be a variant of Probabilistic Context Free Grammar PCFG .", "In PCFGs each rule a in the CFG underlying the PCFG has an associated probability P 31a .", "In Collins 97 , P 01a is defined as a product of terms , by assuming that the right hand side of the rule is generated in three steps probability I P , h , H , where Ln i in 1 STOP .", "The STOP symbol is added to the vocabulary of nonterminals , and the model stops generating left modifiers when it is generated .", "Other rules in the tree contribute similar sets of probabilities .", "The probability for the entire tree is calculated as the product of all these terms .", "Collins 97 describes a series of refinements to this basic model the addition of quot ; distance quot ; a conditioning feature indicating whether or not a modifier is adjacent to the head ; the addition of subcategorization parameters Model 2 , and parameters that model wh movement Model 3 ; estimation techniques that smooth various levels of back off in particular using POS tags as word classes , allowing the model to learn generalizations about POS classes of words .", "Search for the highest probability tree for a sentence is achieved using a CKY style parsing algorithm .", "Many statistical parsing methods developed for English use lexicalized trees as a representation e . g . , Jelinek et al . 94 ; Magerman 95 ; Ratnaparkhi 97 ; Charniak 97 ; Collins 96 ; Collins 97 ; several e . g . , Eisner 96 ; Collins 96 ; Collins 97 ; Charniak 97 emphasize the use of parameters associated with dependencies between pairs of words .", "The Czech PDT contains dependency annotations , but no tree structures .", "For parsing Czech we considered a strategy of converting dependency structures in training data to lexicalized trees , then running the parsing algorithms originally developed for English .", "A key point is that the mapping from lexicalized trees to dependency structures is many to one .", "As an example , figure 2 shows an input dependency structure , and three different lexicalized trees with this dependency structure .", "The choice of tree structure is crucial in determining the independence assumptions that the parsing model makes .", "There are at least 3 degrees of freedom when deciding on the tree structures To provide a baseline result we implemented what is probably the simplest possible conversion scheme The baseline approach gave a result of 71 . 9 accuracy on the development test set .", "While the baseline approach is reasonably successful , there are some linguistic phenomena that lead to clear problems .", "This section describes some tree transformations that are linguistically motivated , and lead to improvements in parsing accuracy .", "In the PDT the verb is taken to be the head of both sentences and relative clauses .", "Figure 4 illustrates how the baseline transformation method can lead to parsing errors in relative clause cases .", "Figure 4 c shows the solution to the problem the label of the relative clause is changed to SBAR , and an additional VP level is added to the right of the relative pronoun .", "Similar transformations were applied for relative clauses involving Wh PPs e . g . , quot ; the man to whom I gave a book quot ; , Wh NPs e . g . , quot ; the man whose book I read quot ; and Wh Adverbials e . g . , quot ; the place where I live quot ; .", "The PDT takes the conjunct to be the head of coordination structures for example , and would be the head of the NP dogs and cats .", "In these cases the baseline approach gives tree structures such as that in figure 5 a .", "The non terminal label for the phrase is JP because the head of the phrase , the conjunct and , is tagged as J .", "This choice of non terminal is problematic for two reasons 1 the JP label is assigned to all coordinated phrases , for example hiding the fact that the constituent in figure 5 a is an NP ; 2 the model assumes that left and right modifiers are generated independently of each other , and as it stands will give unreasonably high probability to two unlike phrases being coordinated .", "To fix these problems , the non terminal label in coordination cases was altered to be the same as that of the second conjunct the phrase directly to the right of the head of the phrase .", "See figure 5 .", "A similar transformation was made for cases where a comma was the head of a phrase .", "Figure 6 shows an additional change concerning commas .", "This change increases the sensitivity of the model to punctuation .", "This section describes some modifications to the parameterization of the model . guish main clauses from relative clauses both have a verb as the head , so both are labeled VP .", "b A typical parsing error due to relative and main clauses not being distinguished .", "note that two main clauses can be coordinated by a comma , as in John likes Mary , Mary likes Tim .", "c The solution to the problem a modification to relative clause structures in training data .", "The model of Collins 97 had conditioning variables that allowed the model to learn a preference for dependencies which do not cross verbs .", "From the results in table 3 , adding this condition improved accuracy by about 0 . 9 on the development set .", "The parser of Collins 96 used punctuation as an indication of phrasal boundaries .", "It was found that if a constituent Z . . . XY . . . has two children X and Y separated by a punctuation mark , then Y is generally followed by a punctuation mark or the end of sentence marker .", "The parsers of Collins 96 , 97 encoded this as a hard constraint .", "In the Czech parser we added a cost of 2 . 5 log probability 2 to structures that violated this constraint .", "The model of section 3 made the assumption that modifiers are generated independently of each other .", "This section describes a hi gram model , where the context is increased to consider the previously generated modifier Eisner 96 also describes use of bigram statistics .", "The right hand side of a rule is now assumed to be generated in the following three step process where Lo is defined as a special NULL symbol .", "Thus the previous modifier , Li_1 , is added to the conditioning context in the previous model the left modifiers had probability Introducing bigram dependencies into the parsing model improved parsing accuracy by about 0 . 9 as shown in Table 3 .", "Part of speech POS tags serve an important role in statistical parsing by providing the model with a level of generalization as to how classes of words tend to behave , what roles they play in sentences , and what other classes they tend to combine with .", "Statistical parsers of English typically make use of the roughly 50 POS tags used in the Penn Treebank corpus , but the Czech PDT corpus provides a much richer set of POS tags , with over 3000 possible tags defined by the tagging system and over 1000 tags actually found in the corpus .", "Using that large a tagset with a training corpus of only 19 , 000 sentences would lead to serious sparse data problems .", "It is also clear that some of the distinctions being made by the tags are more important than others for parsing .", "We therefore explored different ways of extracting smaller but still maximally informative POS tagsets .", "The POS tags in the Czech PDT corpus Haji 6 and Hladka , 1997 are encoded in 13 character strings .", "Table 1 shows the role of each character .", "For example , the tag NNMP1 A would be used for a word that had quot ; noun quot ; as both its main and detailed part of speech , that was masculine , plural , nominative case 1 , and whose negativeness value was quot ; affirmative quot ; .", "Within the corpus , each word was annotated with all of the POS tags that would be possible given its spelling , using the output of a morphological analysis program , and also with the single one of those tags that a statistical POS tagging program had predicted to be the correct tag Hake and Hladka , 1998 .", "Table 2 shows a phrase from the corpus , with the alternative possible tags and machine selected tag for each word .", "In the training portion of the corpus , the correct tag as judged by human annotators was also provided .", "In the baseline approach , the first letter , or quot ; main part of speech quot ; , of the full POS strings was used as the tag .", "This resulted in a tagset with 13 possible values .", "A number of alternative , richer tagsets were explored , using various combinations of character positions from the tag string .", "The most successful alternative was a two letter tag whose first letter was always the main POS , and whose second letter was the case field if the main POS was one that displays case , while otherwise the second letter was the detailed POS .", "The detailed POS was used for the main POS values D , J , V , and X ; the case field was used for the other possible main POS values .", "This two letter scheme resulted in 58 tags , and provided about a 1 . 1 parsing improvement over the baseline on the development set .", "Even richer tagsets that also included the person , gender , and number values were tested without yielding any further improvement , presumably because the damage from sparse data outweighed the value of the additional information present .", "An entirely different approach , rather than searching by hand for effective tagsets , would be to use clustering to derive them automatically .", "We explored two different methods , bottom up and topdown , for automatically deriving POS tag sets based on counts of governing and dependent tags extracted from the parse trees that the parser constructs from the training data .", "Neither tested approach resulted in any improvement in parsing performance compared to the hand designed quot ; two letter quot ; tagset , but the implementations of each were still only preliminary , and a clustered tagset more adroitly derived might do better .", "One final issue regarding POS tags was how to deal with the ambiguity between possible tags , both in training and test .", "In the training data , there was a choice between using the output of the POS tagger or the human annotator's judgment as to the correct tag .", "In test data , the correct answer was not available , but the POS tagger output could be used if desired .", "This turns out to matter only for unknown words , as the parser is designed to do its own tagging , for words that it has seen in training at least 5 times , ignoring any tag supplied with the input .", "For quot ; unknown quot ; words seen less than 5 times , the parser can be set either to believe the tag supplied by the POS tagger or to allow equally any of the dictionary derived possible tags for the word , effectively allowing the parse context to make the choice .", "Note that the rich inflectional morphology of Czech leads to a higher rate of quot ; unknown quot ; word forms than would be true in English ; in one test , 29 . 5 of the words in test data were quot ; unknown quot ; Our tests indicated that if unknown words are treated by believing the POS tagger's suggestion , then scores are better if the parser is also trained on the POS tagger's suggestions , rather than on the human annotator's correct tags .", "Training on the correct tags results in 1 worse performance .", "Even though the POS tagger's tags are less accurate , they are more like what the parser will be using in the test data , and that turns out to be the key point .", "On the other hand , if the parser allows all possible dictionary tags for unknown words in test material , then it pays to train on the actual correct tags .", "In initial tests , this combination of training on the correct tags and allowing all dictionary tags for unknown test words somewhat outperformed the alternative of using the POS tagger's predictions both for training and for unknown test words .", "When tested with the final version of the parser on the full development set , those two strategies performed at the same level .", "We ran three versions of the parser over the final test set the baseline version , the full model with all additions , and the full model with everything but the bigram model .", "The baseline system on the fithat although the Science section only contributes 25 of the sentences in test data , it contains much longer sentences than the other sections and therefore accounts for 38 of the dependencies in test data . nal test set achieved 72 . 3 accuracy .", "The final system achieved 80 . 0 accuracy3 a 7 . 7 absolute improvement and a 27 . 8 relative improvement .", "The development set showed very similar results a baseline accuracy of 71 . 9 and a final accuracy of 79 . 3 .", "Table 3 shows the relative improvement of each component of the mode14 .", "Table 4 shows the results on the development set by genre .", "It is interesting to see that the performance on newswire text is over 2 better than the averaged performance .", "The Science section of the development set is considerably harder to parse presumably because of longer sentences and more open vocabulary .", "The main piece of previous work on parsing Czech that we are aware of is described in Kuboli 99 .", "This is a rule based system which is based on a manually designed set of rules .", "The system's accuracy is not evaluated on a test corpus , so it is difficult to compare our results to theirs .", "We can , however , make some comparison of the results in this paper to those on parsing English .", "Collins 99 describes results of 91 accuracy in recovering dependencies on section 0 of the Penn Wall Street Journal Treebank , using Model 2 of Collins 97 .", "This task is almost certainly easier for a number of reasons there was more training data 40 , 000 sentences as opposed to 19 , 000 ; Wall Street Journal may be an easier domain than the PDT , as a reasonable proportion of sentences come from a sub domain , financial news , which is relatively restricted .", "Unlike model 1 , model 2 of the parser takes subcategorization information into account , which gives some improvement on English and might well also improve results on Czech .", "Given these differences , it is difficult to make a direct comparison , but the overall conclusion seems to be that the Czech accuracy is approaching results on English , although it is still somewhat behind .", "The 80 dependency accuracy of the parser represents good progress towards English parsing performance .", "A major area for future work is likely to be an improved treatment of morphology ; a natural approach to this problem is to consider more carefully how POS tags are used as word classes by the model .", "We have begun to investigate this issue , through the automatic derivation of POS tags through clustering or quot ; splitting quot ; approaches .", "It might also be possible to exploit the internal structure of the POS tags , for example through incremental prediction of the POS tag being generated ; or to exploit the use of word lemmas , effectively splitting word word relations into syntactic dependencies POS tag POS tag relations and more semantic lemma lemma dependencies ."], "summary_lines": ["A Statistical Parser For Czech\n", "This paper considers statistical parsing of Czech, which differs radically from English in at least two respects: (1) it is a highly inflected language, and (2) it has relatively free word order.\n", "These differences are likely to pose new problems for techniques that have been developed on English.\n", "We describe our experience in building on the parsing model of (Collins 97).\n", "Our final results - 80% dependency accuracy - represent good progress towards the 91% accuracy of the parser on English (Wall Street Journal) text.\n", "We use a transformed tree bank from the Prague Dependency Treebank for constituent parsing on Czech.\n"]}
{"article_lines": ["GermaNet A Lexical Semantic Net For German", "We present the lexical semantic net for German quot ; GermaNet quot ; which integrates conceptual ontological information with lexical semantics , within and across word classes .", "It is compatible with the Princeton WordNet but integrates principlebased modifications on the constructional and organizational level as well as on the level of lexical and conceptual relations .", "GermaNet includes a new treatment of regular polysemy , artificial concepts and of particle verbs .", "It furthermore encodes cross classification and basic syntactic information , constituting an interesting tool in exploring the interacof syntax and development of such a large scale resource is particularly important as German up to now lacks basic online tools for the semantic exploration of very large corpora .", "GermaNet is a broad coverage lexical semantic net for German which currently contains some 16 . 000 words and aims at modeling at least the base vocabulary of German .", "It can be thought of as an online ontology in which meanings associated with words so called synsets are grouped according to their semantic relatedness .", "The basic framework of GermaNet is similar to the Princeton WordNet Miller et al . , 1993 , guaranteeing maximal compatibility .", "Nevertheless some principle based modifications have been applied .", "GermaNet is built from scratch , which means that it is neither a translation of the English WordNet nor is it based on a single dictionary or thesaurus .", "The development of a German wordnet has the advantage that the applications developed for English using WordNet as a resource can be used for German with only minor modifications .", "This affects for example information extraction , automatic sense disambiguation and intelligent document retrieval .", "Furthermore , GermaNet can serve as a training source for statistical methods in natural language processing NLP and it makes future integration of German in multilingual resources such as EuroWordNet Bloksma et al . , 1996 possible .", "This paper gives an overview of the resource situation , followed by sections on the coverage of the net and the basic relations used for linkage of lexical and conceptual items .", "The main part of the paper is concerned with the construction principles of GermaNet and particular features of each of the word classes .", "In English a variety of large scale online linguistic resources are available .", "The application of these resources is essential for various NLP tasks in reducing time effort and error rate , as well as guaranteeing a broader and more domain independent coverage .", "The resources are typically put to use for the creation of consistent and large lexical databases for parsing and machine translation as well as for the treatment of lexical , syntactic and semantic ambiguity .", "Furthermore , linguistic resources are becoming increasingly important as training and evaluation material for statistical methods .", "In German , however , not many large scale monolingual resources are publically available which can aid the building of a semantic net .", "The particular resource situation for German makes it necessary to rely to a large extent on manual labour for the creation process of a wordnet , based on monolingual general and specialist dictionaries and literature , as well as comparisons with the English WordNet .", "However , we take a strongly corpus based approach by determining the base vocabulary modeled in GermaNet by lemmatized frequency lists from text corporal .", "This list is further tuned by using other available sources such as the CELEX German database .", "Clustering methods , which in principle can apply to large corpora without requiring any further information in order to give similar words as output , proved to be interesting but not helpful for the construction of the core net .", "Selectional restrictions of verbs for nouns will , however , be automatically extracted by clustering methods .", "We use the Princeton WordNet technology for the database format , database compilation , as well as the Princeton WordNet interface , applying extensions only where necessary .", "This results in maximal compatibility .", "Gertna . Net shares the basic database division into the four word classes noun , adjective , verb , and adverb with WordNet , although adverbs are not implemented in the current working phase .", "For each of the word classes the semantic space is divided into some 15 semantic fields .", "The purpose of this division is mainly of an organizational nature it allows to split the work into packages .", "Naturally , the semantic fields are closely related to major nodes in the semantic network .", "However , they do not have to agree completely with the net's top level ontology , since a lexicographer can always include relations across these fields and the division into fields is normally not shown to the user by the interface software .", "GermaNet only implements lemmas .", "We assume that inflected forms are mapped to base forms by an external morphological analyzer which might be integrated into an interface to GermaNet .", "In general , proper names and abbreviations are not integrated , even though the lexicographer may do so for important and frequent cases .", "Frequency counts from text corpora serve as a guideline for the inclusion of lemmas .", "In the current version of the database multi word expressions are only covered occasionaly for proper names Olympische Spiele and terminological expressions weifles Blutkorperchen .", "Derivates and a large number of high frequent German compounds are coded manually , making frequent use 1We have access to a large tagged and lemmatized online corpus of 60 , 000 . 000 words , comprising the EC1 corpus 1994 Frankfurter Rundschau , Donau Kuner , VDI Nachrichten and the Tibinger NewsKorpus , consisting of texts collected in T\u00fcbingen from electronic newsgroups . of cross classification .", "An implementation of a more suitable rule based classification of derivates and the unlimited number of semantically transparent compounds fails due to the lack of algorithms for their sound semantic classification .", "The amount of polysemy is kept to a minimunt in GermaNet , an additional sense of a word is only introduced if it conflicts with the coordinates of other senses of the word in the network .", "When in doubt , GermaNet refers to the degree of polysemy given in standard monolingual print dictionaries .", "Additionally , GermaNet makes use of systematic crossclassification .", "Two basic types of relations can be distinguished lexical relations which hold between different lexical realizations of concepts , and conceptual relations which hold between different concepts in all their particular realizations .", "Synonymy and antonymy are bidirectional lexical relations holding for all word classes .", "All other relations except for the 'pertains to' relation are conceptual relations .", "An example for synonymy are torkeln and taumeln , which both express the concept of the same particular lurching motion .", "An example for antonymy are the adjectives kalt cold and warm warm .", "These two relations are implemented and interpreted in GerniaNet as in WordNet .", "The relation pertains to relates denominal adjectives with their nominal base finanztell 'financial' with Finanzen 'finances' , deverbal nominalizations with their verbal base Entdeckung 'discovery' with entdeeken 'discover' and deadjectival nominalizations with their respective adjectival base Miidigkeit 'tiredness' with raids 'tired' .", "This pointer is semantic and not morphological in nature because different morphological realizations can be used to denote derivations from different meanings of the same lemma e . g . konventionell is related to Konvention Regein des Umgangs social rule , while konventzonal is related to Konvention juristischer Text agreement .", "The relation of hyponymy 'is a' holds for all word classes and is implemented in GermaNet as in WordNet , so for example Rotkehlchen robin is a hyponym of Vogel bird .", "Meronymy 'has a' , the part whole relation , holds only for nouns and is subdivided into three relations in WordNet componentrelation , member relation , stuff relation .", "GermaNet , however , currently assumes only one basic meronymy relation .", "An example for meronytny is Arm arm standing in the naeronymy relation to Korper body .", "For verbs , WordNet makes the assumption that the relation of entailment holds in two different situations .", "1 In cases of 'temporal inclusion' of two events as in schnarchen snoring entailing schlafen sleeping .", "ii In cases without temporal inclusion as in what Fellbaum 1993 , 19 calls 'backward presupposition' , holding between grimyen succeed succeed and versuchen try .", "However , these two cases are quite distinct from each other , justifying their separation into two different relations in GermaNet .", "The relation of entailment is kept for the case of backward presupposition .", "Following a suggestion made in EuroWordNet Alonge , 1996 , 43 , we distinguish temporal inclusion by its characteristics that the first event is always a subevent of the second , and thus the relation is called subevent relation .", "The cause relation in WordNet is restricted to hold between verbs .", "We extend its coverage to account for resultative verbs by connecting the verb to its adjectival resultative state .", "For example offnen to open causes offen open .", "Seleetional restrictions , giving information about typical nominal arguments for verbs and adjectives , are additionally implemented .", "They do not exist in WordNet even though their existence is claimed to be important to fully characterize a verbs lexical behavior Fellbauna , 1993 , 28 .", "These selectional properties will be generated automatically by clustering methods once a sense tagged corpus with GermaNet classes is available .", "Another additional pointer is created to account for regular polysemy in an elegant and efficient way , marking potential regular polysemy at a very high level and thus avoiding duplication of entries and time consuming work c . f . section 5 . 1 .", "As opposed to WordNet , connectivity between word classes is a strong point of GermaNet .", "This is achieved in different ways The cross class relations 'pertains to' of WordNet are used more frequently , Certain WordNet relations are modified to cross word classes verbs are allowed to 'cause' adjectives and new cross class relations are introduced e . g .", "'selectional restrictions' .", "Cross class relations are particularly important as the expression of one concept is often not restricted to a single word class .", "Additionally , the final version will contain examples for each concept which are to be automatically extracted from the corpus .", "Some of the guiding principles of the GermaNet ontology creation are different from WordNet and therefore now explained .", "WordNet does contain artificial concepts , that is non lexicalized concepts .", "However , they are neither marked nor put to systematic use nor even exactly defined .", "In contrast , GermaNet enforces the systematic usage of artificial concepts and especially marks them by a quot ; r . Thus they can be cut out on the interface level if the user wishes so .", "We encode two different sorts of artificial concepts i lexical gaps which are of a conceptual nature , meaning that they can be expected to be expressed in other languages see figure 2 and ii proper artificial concepts see figure 3 . 2 Advantages of artificial concepts are the avoidance of unmotivated co hyponyms and a systematic structuring of the data .", "See the following examples In figure 1 noble man is a co hyponym to the other three hyponyms of human , even though the first three are related to a certain education and noble man refers to a state a person is in from birth on .", "This intuition is modeled in figure 2 with the additional artificial concept ? educated human .", "In figure 3 , all concepts except for the leaves are proper artificial concepts .", "That is , one would not expect any language to explicitly verbalize the concept of for example manner of motion verbs which specify the specific instrument used .", "Nevertheless such a structuring is important because it captures semantic intuitions every speaker of German has and it groups verbs according to their semantic relatedness .", "Contrary to WordNet , GertnaNet enforces the use of cross classification whenever two conflicting hierarchies apply .", "This becomes important for example in the classification of animals , where folk and specialized biological hierarchy compete on a large scale .", "By cross classifying between these two hierarchies the taxonomy becomes more accessible and integrates different semantic components which are essential to the meaning of the concepts .", "For example , in figure 4 the concept of a cat is shown to biologically be a vertebrate , and a pet in the folk hierarchy , whereas a whale is only a vertebrate and not a pet .", "The concept of cross classification is of great importance in the verbal domain as well , where most concepts have several meaning components according to which they could be classified .", "However , relevant information would be lost if only one particular aspect was chosen with respect to hyponymy .", "Verbs of sound for example form a distinct semantic class Levin et at . , in press , the members of which differ with respect to additional verb classes with which they cross classify , in English as in German .", "According to Levin in press , 7 , some can be used as verbs of motion accompanied by sound A train rumbled across the loopline bridge .", ", others as verbs of introducing direct speech Annabel squeaked , quot ; Why can't you stay with us ? quot ; or verbs expressing the causation of the emission of a sound He crackled the newspaper , folding it carelessly .", "Systematic crossclassification allows to capture this fine grained distinction easily and in a principle based way .", "With respect to nouns the treatment of regular polysemy in GermsNet deserves special attention .", "A number of proposals have been made for the representation of regular polysemy in the lexicon .", "It is generally agreed that a pure sense enumeration approach is not sufficient .", "Instead , the different senses of a regularly polysemous word need to be treated in a more principle based manner see for example Pustejovsky 1996 .", "GermaNet is facing the problem that lexical entries are integrated in an ontology with strict inheritance rules .", "This implies that any notion of regular polysemy must obey the rules of inheritance .", "It furthermore prohibits joint polysemous entries with dependencies from applying for only one aspect of a polysemous entry .", "A familiar type of regular polysemy is the quot ; organization building it occupies quot ; polyserny .", "GermaNet lists synonyms along with each concept .", "Therefore it is not possible to merge such a type of polysemy into one concept and use crossclassification to point to both , institution and building as in figure 5 .", "This is only possible if all synonyms of both senses and all their dependent nodes in the hierarchy share the same regular polysemy , which is hardly ever the case .", "To allow for regular polysemy , GermaNet introduces a special bidirectional relator which is placed to the top concepts for which the regular polysemy holds c . f . figure 6 .", "In figure 6 the entry bank a financial institution that accepts deposits and channels the money into lending activities may have the synonyms depository financial institution , banking concern , banking company , which are not synonyms of bank2 a building in which commercial banking is transacted .", "In addition , banki may have hyponyms such as credit union , agent bank , commercial bank , full service bank , which do not share the regular polysemy of baraki and bank2 .", "Statistically frequent cases of regular polysemy are manually and explicitly encoded in the net .", "This is necessary because they often really are two separate concepts as in pork , pig and each sense may have different synonyms pork meat is only synonym to pork .", "However , the polysemy pointer additionally allows the recognition of statistically infrequent uses of a word sense created by regular polysemy .", "So for example the sentence I had crocodile for hutch is very infrequent in that crocodile is not commonly perceived as meat but only as animal .", "Nevertheless we know that a regular polysemy exists between meat and animal .", "Therefore we can reconstruct via the regular polysemy pointer that the meat sense is referred to in this particular sentence even though it is not explicitly encoded .", "Thus the pointer can be conceived of as an implementation of a simple default via which the net can account for language productivity and regularity in an effective manner .", "Adjectives in GermaNet are modeled in a taxonornical manner making heavy use of the hyponymy relation , which is very different from the satellite approach taken in WordNet .", "Our approach avoids the rather fuzzy concept of indirect antonyms introduced by WordNet .", "Additionally we do not introduce artificial antonyms as WordNet does pregnant , unpregnant .", "The taxonomical classes follow Hundsnurscher and Splett , 1982 with an additional class for pertainym83 .", "Syntactic frames and particle verbs deserve special attention in the verbal domain .", "The frames used in GermaNet differ from those in WordNet , and particle verbs as such are treated in WordNet at all .", "Each verb sense is linked to one or more syntactic frames which are encoded on a lexical rather than on a conceptual level .", "The frames used in GermaNet are based on the complementation codes provided by CELEX Burnage , 1995 .", "The notation in GermaNet differs from the CELEX database in providing a notation for the subject and a complementation code for obligatory reflexive phrases .", "GermaNet provides frames for verb senses , rather than for lemmas , implying a full disambiguation of the CELEX complementation codes for GermaNet .", "Syntactic information in GerrnaNet differs from that given in WordNet in several ways .", "It marks expletive subjects and reflexives explicitly , encodes case information , which is especially important in German , distinguishes between different realizations of prepositional and adverbial phrases and marks to infinitival as well as pure infinitival complements explicitly .", "Particles pose a particular problem in German .", "They are very productive , which would lead to an explosion of entries if each particle verb was explicitly encoded .", "Some particles establish a regular semantic pattern which can not be accounted for by a simple enumeration approach , whereas others are very irregular and ambiguous .", "We therefore propose a mixed approach , treating irregular particle verbs by enumeration and regular particle verbs in a compositional manner .", "Composition can be thought of as a default which can be overwritten by explicit entries in the database .", "We assume a morphological component such as GERTWOL 1996 to apply before the compositional process starts .", "Composition itself is implemented as follows , relying on a separate lexicon for particles .", "The particle lexicon is hierarchically structured and lists selectional restrictions with respect to the base verb selected .", "An example for the hierarchical structure is given in figure 7 without selectional restrictions for matters of simplicity , where heraus is a hyponym of her and am .", "SAdjectives pertaining to a noun from which they derive their meaning financial , finances .", "Selectional restrictions for particles include Aktionsart , a particular semantic verb field , deictic orientation and directional orientation of the base verb .", "The evaluation of a particle verb takes the following steps .", "First , GermaNet is searched for an explicit entry of the particle verb , If no such entry exists the verb is morphologically analyzed and its semantics is compositionally determined .", "For example the particle verb herauslaufen in figurer is a hyponym to Millen walk as well as to heraus .", "Criteria for a compositional treatment are separability , productivity and a regular semantics of the particle see Fleischer and Bars 1992 , Stiebels 1994 , Stegmann 1996 .", "A wordnet for German has been described which , compared with the Princeton WordNet , integrates principle based modifications and extensions on the constructional and organizational level as well as on the level of lexical and conceptual relations .", "Innovative features of GermaNet are a new treatment of regular polysemy and of particle verbs , as well as a principle based encoding of crossclassification and artificial concepts .", "As compatibility with the Princeton WordNet and EuroWordNet is a major construction criteria of GermaNet , German can now , finally , be integrated into multilingual large scale projects based on ontological and conceptual information .", "This constitutes an important step towards the design of truly multilingual tools applicable in key areas such as information retrieval and intelligent Internet search engines ."], "summary_lines": ["GermaNet - A Lexical-Semantic Net For German\n", "We present the lexical-semantic net for German \"GermaNet\" which integrates conceptual ontological information with lexical semantics, within and across word classes.\n", "It is compatible with the Princeton WordNet but integrates principle-based modifications on the constructional and organizational level as well as on the level of lexical and conceptual relations.\n", "GermaNet includes a new treatment of regular polysemy, artificial concepts and of particle verbs.\n", "It furthermore encodes cross-classification and basic syntactic information, constituting an interesting tool in exploring the interaction of syntax and semantics.\n", "The development of such a large scale resource is particularly important as German up to now lacks basic online tools for the semantic exploration of very large corpora.\n", "GermaNet is a large lexical database, where words are associated with POS in formation and semantic sorts, which are organized in a fine-grained hierarchy.\n"]}
{"article_lines": ["Moses Open Source Toolkit for Statistical Machine Translation", "where many sources of information about the project can be found .", "Moses was the subject of this year s Johns Hopkins University Workshop on Machine Translation Koehn et al . 2006 .", "The decoder is the core component of Moses .", "To minimize the learning curve for many researchers , the decoder was developed as a drop in replacement for Pharaoh , the popular phrase based decoder .", "In order for the toolkit to be adopted by the community , and to make it easy for others to contribute to the project , we kept to the following principles when developing the decoder Accessibility Easy to Maintain Flexibility Easy for distributed team development Portability It was developed in C for efficiency and followed modular , object oriented design .", "3 Factored Translation Model Non factored SMT typically deals only with the surface form of words and has one phrase table , as shown in Figure 1 .", "Translate i am buying you a green cat vous ach\u00e8te un vert using phrase dictionary i am buying vous green chat Figure 1 .", "Non factored translation In factored translation models , the surface forms may be augmented with different factors , such as POS tags or lemma .", "This creates a factored representation of each word , Figure 2 .", "vous achet chat PRO PRO VB ART NN je vous acheter chat st present masc buy you a cat VB PRO ART NN tobuy you a cat Figure 2 .", "Factored translation Mapping of source phrases to target phrases may be decomposed into several steps .", "Decomposition of the decoding process into various steps means that different factors can be modeled separately .", "Modeling factors in isolation allows for flexibility in their application .", "It can also increase accuracy and reduce sparsity by minimizing the number dependencies for each step .", "For example , we can decompose translating from surface forms to surface forms and lemma , as shown in Figure 3 . je ach\u00e8te you a un a une vert cat 178 Figure 3 .", "Example of graph of decoding steps By allowing the graph to be user definable , we can experiment to find the optimum configuration for a given language pair and available data .", "The factors on the source sentence are considered fixed , therefore , there is no decoding step which create source factors from other source factors .", "However , Moses can have ambiguous input in the form of confusion networks .", "This input type has been used successfully for speech to text translation Shen et al . 2006 .", "Every factor on the target language can have its own language model .", "Since many factors , like lemmas and POS tags , are less sparse than surface forms , it is possible to create a higher order language models for these factors .", "This may encourage more syntactically correct output .", "In Figure 3 we apply two language models , indicated by the shaded arrows , one over the words and another over the lemmas .", "Moses is also able to integrate factored language models , such as those described in Bilmes and Kirchhoff 2003 and Axelrod 2006 .", "4 Confusion Network Decoding Machine translation input currently takes the form of simple sequences of words .", "However , there are increasing demands to integrate machine translation technology into larger information processing systems with upstream NLP speech processing tools such as named entity recognizers , speech recognizers , morphological analyzers , etc . .", "These upstream processes tend to generate multiple , erroneous hypotheses with varying confidence .", "Current MT systems are designed to process only one input hypothesis , making them vulnerable to errors in the input .", "In experiments with confusion networks , we have focused so far on the speech translation case , where the input is generated by a speech recognizer .", "Namely , our goal is to improve performance of spoken language translation by better integrating speech recognition and machine translation models .", "Translation from speech input is considered more difficult than translation from text for several reasons .", "Spoken language has many styles and genres , such as , formal read speech , unplanned speeches , interviews , spontaneous conversations ; it produces less controlled language , presenting more relaxed syntax and spontaneous speech phenomena .", "Finally , translation of spoken language is prone to speech recognition errors , which can possibly corrupt the syntax and the meaning of the input .", "There is also empirical evidence that better translations can be obtained from transcriptions of the speech recognizer which resulted in lower scores .", "This suggests that improvements can be achieved by applying machine translation on a large set of transcription hypotheses generated by the speech recognizers and by combining scores of acoustic models , language models , and translation models .", "Recently , approaches have been proposed for improving translation quality through the processing of multiple input hypotheses .", "We have implemented in Moses confusion network decoding as discussed in Bertoldi and Federico 2005 , and developed a simpler translation model and a more efficient implementation of the search algorithm .", "Remarkably , the confusion network decoder resulted in an extension of the standard text decoder .", "5 Efficient Data Structures for Translation Model and Language Models With the availability of ever increasing amounts of training data , it has become a challenge for machine translation systems to cope with the resulting strain on computational resources .", "Instead of simply buying larger machines with , say , 12 GB of main memory , the implementation of more efficient data structures in Moses makes it possible to exploit larger data resources with limited hardware infrastructure .", "A phrase translation table easily takes up gigabytes of disk space , but for the translation of a single sentence only a tiny fraction of this table is needed .", "Moses implements an efficient representation of the phrase translation table .", "Its key properare a tree for source words and demand i . e . only the fraction of the phrase table that is needed to translate a sentence is loaded into the working memory of the decoder .", "179 For the Chinese English NIST task , the memory requirement of the phrase table is reduced from 1 . 7 gigabytes to less than 20 mega bytes , with no loss in translation quality and speed Zens and Ney 2007 .", "The other large data resource for statistical machine translation is the language model .", "Almost unlimited text resources can be collected from the Internet and used as training data for language modeling .", "This results in language models that are too large to easily fit into memory .", "The Moses system implements a data structure for language models that is more efficient than the canonical SRILM Stolcke 2002 implementation used in most systems .", "The language model on disk is also converted into this binary format , resulting in a minimal loading time during start up of the decoder .", "An even more compact representation of the model is the result of the the word prediction and back off probabilities of the language model .", "Instead of representing these probabilities with 4 byte or 8 byte floats , they are sorted into bins , resulting in typically 256 bins which can be referenced with a single 1 byte index .", "This quantized language model , albeit being less accurate , has only minimal impact on translation performance Federico and Bertoldi 2006 .", "6 Conclusion and Future Work This paper has presented a suite of open source tools which we believe will be of value to the MT research community .", "We have also described a new SMT decoder which can incorporate some linguistic features in a consistent and flexible framework .", "This new direction in research opens up many possibilities and issues that require further research and experimentation .", "Initial results show the potential benefit of factors for statistical machine translation , Koehn et al . 2006 and Koehn and Hoang 2007 .", "Phrase based statistical machine translation Koehn et al . 2003 has emerged as the dominant paradigm in machine translation research .", "However , until now , most work in this field has been carried out on proprietary and in house research systems .", "This lack of openness has created a high barrier to entry for researchers as many of the components required have had to be duplicated .", "This has also hindered effective comparisons of the different elements of the systems .", "By providing a free and complete toolkit , we hope that this will stimulate the development of the field .", "For this system to be adopted by the community , it must demonstrate performance that is comparable to the best available systems .", "Moses has shown that it achieves results comparable to the most competitive and widely used statistical machine translation systems in translation quality and run time Shen et al . 2006 .", "It features all the capabilities of the closed sourced Pharaoh decoder Koehn 2004 .", "Apart from providing an open source toolkit for SMT , a further motivation for Moses is to extend phrase based translation with factors and confusion network decoding .", "The current phrase based approach to statistical machine translation is limited to the mapping of small text chunks without any explicit use of linguistic information , be it morphological , syntactic , or semantic .", "These additional sources of information have been shown to be valuable when integrated into pre processing or post processing steps .", "Moses also integrates confusion network decoding , which allows the translation of ambiguous input .", "This enables , for instance , the tighter integration of speech recognition and machine translation .", "Instead of passing along the one best output of the recognizer , a network of different word choices may be examined by the machine translation system .", "Efficient data structures in Moses for the memory intensive translation model and language model allow the exploitation of much larger data resources with limited hardware .", "Proceedings of the ACL 2007 Demo and Poster Sessions , pages 177 180 , Prague , June 2007 . c 2007 Association for Computational Linguistics", "The toolkit is a complete out of the box translation system for academic research .", "It consists of all the components needed to preprocess data , train the language models and the translation models .", "It also contains tools for tuning these models using minimum error rate training Och 2003 and evaluating the resulting translations using the BLEU score Papineni et al . 2002 .", "Moses uses standard external tools for some of the tasks to avoid duplication , such as GIZA Och and Ney 2003 for word alignments and SRILM for language modeling .", "Also , since these tasks are often CPU intensive , the toolkit has been designed to work with Sun Grid Engine parallel environment to increase throughput .", "In order to unify the experimental stages , a utility has been developed to run repeatable experiments .", "This uses the tools contained in Moses and requires minimal changes to set up and customize .", "The toolkit has been hosted and developed under sourceforge . net since inception .", "Moses has an active research community and has reached over 1000 downloads as of 1st March 2007 .", "The main online presence is at http www . statmt . org moses where many sources of information about the project can be found .", "Moses was the subject of this year s Johns Hopkins University Workshop on Machine Translation Koehn et al . 2006 .", "The decoder is the core component of Moses .", "To minimize the learning curve for many researchers , the decoder was developed as a drop in replacement for Pharaoh , the popular phrase based decoder .", "In order for the toolkit to be adopted by the community , and to make it easy for others to contribute to the project , we kept to the following principles when developing the decoder It was developed in C for efficiency and followed modular , object oriented design .", "Non factored SMT typically deals only with the surface form of words and has one phrase table , as shown in Figure 1 .", "In factored translation models , the surface forms may be augmented with different factors , such as POS tags or lemma .", "This creates a factored representation of each word , Figure 2 .", "Mapping of source phrases to target phrases may be decomposed into several steps .", "Decomposition of the decoding process into various steps means that different factors can be modeled separately .", "Modeling factors in isolation allows for flexibility in their application .", "It can also increase accuracy and reduce sparsity by minimizing the number dependencies for each step .", "For example , we can decompose translating from surface forms to surface forms and lemma , as shown in Figure 3 .", "By allowing the graph to be user definable , we can experiment to find the optimum configuration for a given language pair and available data .", "The factors on the source sentence are considered fixed , therefore , there is no decoding step which create source factors from other source factors .", "However , Moses can have ambiguous input in the form of confusion networks .", "This input type has been used successfully for speech to text translation Shen et al . 2006 .", "Every factor on the target language can have its own language model .", "Since many factors , like lemmas and POS tags , are less sparse than surface forms , it is possible to create a higher order language models for these factors .", "This may encourage more syntactically correct output .", "In Figure 3 we apply two language models , indicated by the shaded arrows , one over the words and another over the lemmas .", "Moses is also able to integrate factored language models , such as those described in Bilmes and Kirchhoff 2003 and Axelrod 2006 .", "Machine translation input currently takes the form of simple sequences of words .", "However , there are increasing demands to integrate machine translation technology into larger information processing systems with upstream NLP speech processing tools such as named entity recognizers , speech recognizers , morphological analyzers , etc . .", "These upstream processes tend to generate multiple , erroneous hypotheses with varying confidence .", "Current MT systems are designed to process only one input hypothesis , making them vulnerable to errors in the input .", "In experiments with confusion networks , we have focused so far on the speech translation case , where the input is generated by a speech recognizer .", "Namely , our goal is to improve performance of spoken language translation by better integrating speech recognition and machine translation models .", "Translation from speech input is considered more difficult than translation from text for several reasons .", "Spoken language has many styles and genres , such as , formal read speech , unplanned speeches , interviews , spontaneous conversations ; it produces less controlled language , presenting more relaxed syntax and spontaneous speech phenomena .", "Finally , translation of spoken language is prone to speech recognition errors , which can possibly corrupt the syntax and the meaning of the input .", "There is also empirical evidence that better translations can be obtained from transcriptions of the speech recognizer which resulted in lower scores .", "This suggests that improvements can be achieved by applying machine translation on a large set of transcription hypotheses generated by the speech recognizers and by combining scores of acoustic models , language models , and translation models .", "Recently , approaches have been proposed for improving translation quality through the processing of multiple input hypotheses .", "We have implemented in Moses confusion network decoding as discussed in Bertoldi and Federico 2005 , and developed a simpler translation model and a more efficient implementation of the search algorithm .", "Remarkably , the confusion network decoder resulted in an extension of the standard text decoder .", "5 Efficient Data Structures for Translation Model and Language Models With the availability of ever increasing amounts of training data , it has become a challenge for machine translation systems to cope with the resulting strain on computational resources .", "Instead of simply buying larger machines with , say , 12 GB of main memory , the implementation of more efficient data structures in Moses makes it possible to exploit larger data resources with limited hardware infrastructure .", "A phrase translation table easily takes up gigabytes of disk space , but for the translation of a single sentence only a tiny fraction of this table is needed .", "Moses implements an efficient representation of the phrase translation table .", "Its key properties are a prefix tree structure for source words and on demand loading , i . e . only the fraction of the phrase table that is needed to translate a sentence is loaded into the working memory of the decoder .", "For the Chinese English NIST task , the memory requirement of the phrase table is reduced from 1 . 7 gigabytes to less than 20 mega bytes , with no loss in translation quality and speed Zens and Ney 2007 .", "The other large data resource for statistical machine translation is the language model .", "Almost unlimited text resources can be collected from the Internet and used as training data for language modeling .", "This results in language models that are too large to easily fit into memory .", "The Moses system implements a data structure for language models that is more efficient than the canonical SRILM Stolcke 2002 implementation used in most systems .", "The language model on disk is also converted into this binary format , resulting in a minimal loading time during start up of the decoder .", "An even more compact representation of the language model is the result of the quantization of the word prediction and back off probabilities of the language model .", "Instead of representing these probabilities with 4 byte or 8 byte floats , they are sorted into bins , resulting in typically 256 bins which can be referenced with a single 1 byte index .", "This quantized language model , albeit being less accurate , has only minimal impact on translation performance Federico and Bertoldi 2006 .", "This paper has presented a suite of open source tools which we believe will be of value to the MT research community .", "We have also described a new SMT decoder which can incorporate some linguistic features in a consistent and flexible framework .", "This new direction in research opens up many possibilities and issues that require further research and experimentation .", "Initial results show the potential benefit of factors for statistical machine translation , Koehn et al . 2006 and Koehn and Hoang 2007 ."], "summary_lines": ["Moses: Open Source Toolkit for Statistical Machine Translation\n", "We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models.\n", "In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks.\n", "Our Moses decoder implements the factored phrase-based translation model.\n"]}

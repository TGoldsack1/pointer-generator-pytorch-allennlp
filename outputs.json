{"ground_truth": "a unigram orientation model for statistical machine translation in this paper, we present a unigram segmentation model for statistical machine translation where the segmentation units are blocks: pairs of phrases without internal structure. the segmentation model uses a novel orientation component to handle swapping of neighbor blocks. during training, we collect block unigram counts with orientation: we count how often a block occurs to the left or to the right of some predecessor block. the orientation model is shown to improve translation performance over two models: 1) no block re-ordering is used, and 2) the block swapping is controlled only", "prediction": "a unigram orientation model for statistical machine translation lebanese violate warplanes airspace we present a abstract in we present a abstract in we present a abstract in we present a abstract segmentation model for statistical machine where the segmentation units are pairs of phrases without internal internal the segmentation model uses a novel orientation component to handle swapping of neighbor during collect block uncounts with we count how often a block occurs to the left or to the right of some predecessor the orientation model is shown to improve to performance over two models over two"}
{"ground_truth": "disambiguation of proper names in text identifying the occurrences of proper names in text and the entities they refer to can be a difficult task because of the many-to-many mapping between names and their referents. we analyze the types of ambiguity -- structural and semantic -- that make the discovery of proper names difficult in text, and describe the heuristics used to disambiguate names in nominator, a fully-implemented module for proper name recognition developed at the ibm t.j. watson research center. we use hand-written rules and knowledge bases to classify proper names into broad categories.", "prediction": "disambiguation of proper in text trieving information from full we present the trieving of the the fifteenth of the fifteenth of the new of the translation of the translation of the systems of the language of the need of the to of the refer of the most of the new of the system of a of the system"}
{"ground_truth": "models of translational equivalence among words parallel texts (bitexts) have properties that distinguish them from other kinds of parallel data. first, most words translate to only one other word. second, bitext correspondence is typically only partial - many words in each text have no clear equivalent in the other text. this article presents methods for biasing statistical translation models to reflect these properties. evaluation with respect to independent human judgments has confirmed that translation models biased in this fashion are significantly more accurate than a baseline knowledge-free model. this article also shows how a statistical translation model can take advantage", "prediction": "models of translational equivalence among words parallel texts bitexts have properties properties that distinguish them from other kinds of parallel parallel most words translate to only one other other bitext correspondence correspondence is typically only partial partial many words in each text have no text equivalent in the other this article presents methods this article biasing statistical translation models to reflect these evaluation with respect respect to independent human judgments has judgments has confirmed that translation models biased in this fashion are significantly more accurate than a baseline knowledge free knowledge free this article also shows how a statistical translation model"}
{"ground_truth": "comlex syntax: building a computational lexicon we describe the design of complex syntax, a computational lexicon providing detailed syntactic information for approximately 38,000 english headwords. we consider the types of errors which arise in creating such a lexicon, and how such errors can be measured and controlled. our comlex syntax dictionary provides verb subategorization information and syntactic paraphrases, but they are indexed by words thus not suitable to use in generation directly.", "prediction": "comlex comlex ld ing a computat icon lph paper shm ine des tile tile tile design of comlex a nputa tional lexicon providing detailed syntactic iuformation ff r approximately approximately english we con sider the types of errors which arise in creating such a lexicon and how such errors can be measured and 1 1 goal the goal of the omlex syntax project is to create to create a moderately broad coverage lexicon recording the syn tactic features of gnglist for purposes cou of cou cou of anguage"}
{"ground_truth": "japanese dependency structure analysis based on support vector machines this paper presents a method of japanese dependency structure analysis based on support vector machines (svms). conventional parsing techniques based on machine learning framework, such as decision trees and maximum entropy models, have difficulty in selecting useful features as well as finding appropriate combination of selected features. on the other hand, it is well-known that svms achieve high generalization performance even with input data of very high dimensional feature space. furthermore, by introducing the kernel principle, svms can carry out the training in high-dimensional spaces with a smaller computational cost independent", "prediction": "japanese dependency structure analysis based on support vector machines this paper presents a method of japanese dependency structure analysis based on support vector machines conventional parsing techniques based on learning machine learning such as decision trees and maximum entropy have difficulty in useful features as as finding appropriate combination of selected selected on the other other it is well that svms achieve high generalization performance even with input data of very high dimensional feature by introducing the kernel svms svms can carry the training training in high dimensional spaces with a smaller computational cost independent of their their we apply"}
{"ground_truth": "towards terascale semantic acquisition although vast amounts of textual data are freely available, many nlp algorithms exploit only a minute percentage of it. in this paper, we study the challenges of working at the terascale. we present an algorithm, designed for the terascale, for mining is-a relations that achieves similar performance to a state-of-the-art linguistically-rich method. we focus on the accuracy of these two systems as a function of processing time and corpus size. we propose a similar, highly scalable approach, based on an edit-distance technique, to learn lexico-pos patterns, showing both good performance and efficiency. we extend is-a relation", "prediction": "towards semantic acquisition although acquisition although vast amounts of textual data are freely many many nlp algorithms exploit only a minute percentage of it in this paper, we study the challenges of working at present an designed an designed for the terascale for mining is a relations that achieves similar performance to a state of the art linguistically rich we focus on the accuracy of these two systems a func tion of processing and corpus corpus the natural processing nlp the natural munity munity a growth munity seen a growth in we propose"}
{"ground_truth": "on coreference resolution performance metrics the paper proposes a constrained entity-alignment f-measure (ceaf) for evaluating coreference resolution. the metric is computed by aligning reference and system entities (or coreference chains) with the constraint that a system (reference) entity is aligned with at most one reference (system) entity. we show that the best alignment is a maximum bipartite matching problem which can be solved by the kuhn-munkres algorithm. comparative experiments are conducted to show that the widely-known muc f-measure has serious flaws in evaluating a coreference system. the proposed metric is also compared with the ace-value, the official evaluation metric in", "prediction": "on coreference resolution performance metrics metrics the paper proposes a constrained entity alignment f measure ceaf for evaluatingcoreference evaluatingcoreference the metric is com puted by aligning reference and system entities or coreference chains with the that a system reference entity is aligned with at most most one system we show that the best alignment is we show a maximum bipartite matching problem which can be solved by thekuhn munkres comparative experiments are conducted to show that the widely known muc f measure has serious in evaluating a coreference in a coreference coreference a coreference the official"}
{"ground_truth": "semeval-2007 task 02: evaluating word sense induction and discrimination systems the goal of this task is to allow for comparison across sense-induction and discrimination systems, and also to compare these systems to other supervised and knowledge-based systems. in total there were 6 participating systems. we reused the semeval-2007 english lexical sample subtask of task 17, and set up both clustering-style unsupervised evaluation (using onto notes senses as gold-standard) and a supervised evaluation (using the part of the dataset for mapping). we provide a comparison to the results of the systems participating in the lexical sample subtask of task 17. the", "prediction": "semeval task evaluating evaluating word sense induction and discrimination systems the goal of this task is to task is to allow for comparison across induction and discrim ination and also thesesystems to other supervised supervised and knowledgebased total there were participating we reused the semeval english we reused the semeval sample subtask of both set up both clustering style unsuper vised evaluation using ontonotes senses as gold standard and a supervised evaluation using a part of the dataset for we provide a the dataset for we provide a the ofthe systems in the lexical sam ple of task"}
{"ground_truth": "named entity transliteration with comparable corpora in this paper we investigate chinese-english name transliteration using comparable corpora, corpora where texts in the two languages deal in some of the same topics - and therefore share references to named entities - but are not translations of each other. we present two distinct methods for transliteration, one approach using phonetic transliteration, and the second using the temporal distribution of candidate pairs. each of these approaches works quite well, but by combining the approaches one can achieve even better results. we then propose a novel score propagation method that utilizes the co-occurrence of", "prediction": "named entity with comparable corpora in this paper we investigate chinesename transliteration using compacorpora where texts in the two languages in some some of the same topics and therefore share references to named entities but are not translations of each other we present two distinct methods for transliteration one approach using phonetic and the second using the temporal distribution of candidate candidate each each of these approaches works quite but by combining the approaches approaches one can achieve even better better we then propose a novel score propagation method that utilizes the co occurrence of pairs within propagation pairs achieves further further improvement over the method"}
{"ground_truth": "a study on similarity and relatedness using distributional and wordnet-based approaches this paper presents and compares wordnet-based and distributional similarity approaches. the strengths and weaknesses of each approach regarding similarity and relatedness tasks are discussed, and a combination is presented. each of our methods independently provide the best results in their class on the rg and wordsim353 datasets, and a supervised combination of them yields the best published results on all datasets. finally, we pioneer cross-lingual similarity, showing that our methods are easily adapted for a cross-lingual task with minor losses. we derive a wordnet-based measure using pagerank and combined", "prediction": "a on similarity similarity and relatedness using distributional and wordnet based approaches this paper presents and wordnetbased and distributional similarity similarity the strengths and weaknesses of each approach regarding similarity and relatedness are discussed and a combination is each of our methods independently provide the best results in their class class on the rg and wordsim353 and a supervised combination of them yields them yields the best published results on all we pioneer pioneer showing that our pioneer showing that our methods are easily for a cross lingual task with minor a cross lingual semantic and measuring"}
{"ground_truth": "global thresholding and multiple-pass parsing we present a variation on classic beam thresholding techniques that is up to an order of magnitude faster than the traditional method, at the same performance level. we also present a new thresholding technique, global thresholding, which, combined with the new beam thresholding, gives an additional factor of two improvement, and a novel technique, multiple pass parsing, that can be combined with the others to yield yet another 50% improvement. we use a new search algorithm to simultaneously optimize the thresholding parameters of the various algorithms. we describe a method for producing a simple but", "prediction": "global thresholding and multiple parsing we present a variation on classic beam thresholding techniques that is up to an order of magnitude magnitude faster than the traditional at the same performance we also present a new thresholding global thresholding combined with the new beam with the new beam gives an additional factor of two improvement and a novel multiple pass multiple that can be combined with the others to yield yet another 50 we use a new search algorithm to algorithm to the thresholding parameters of the thresholding of the various in we examine thresholding this"}
{"ground_truth": "minimum cut model for spoken lecture segmentation we consider the task of unsupervised lecture segmentation. we formalize segmentation as a graph-partitioning task that optimizes the normalized cut criterion. our approach moves beyond localized comparisons and takes into account long-range cohesion dependencies. our results demonstrate that global analysis improves the segmentation accuracy and is robust in the presence of speech recognition errors. we optimize a normalized minimum-cut criteria based on a variation of the cosine similarity between sentences. our problem is to find topical boundaries in transcripts of course lectures. we create a corpus of course lectures segmented by four annotators,", "prediction": "minimum cut model for spoken lecture segmentation we consider the task of unsupervised lecture we formalize segmentation as a graph partitioning task that optimizes optimizes the normalized cut our approach moves beyond localized comparisons and takes into account cohesion our results demonstrate that global analysis improves the segmentation accuracy and is robust in the presence of speech recognition errors the development of computational models of text structure is a central in natural language text segmentation segmentation is an important instance of such the task is the task is a text a linear sequence of topically topically segments and thereby"}
{"ground_truth": "a tale of two parsers: investigating and combining graph-based and transition-based dependency parsing graph-based and transition-based approaches to dependency parsing adopt very different views of the problem, each view having its own strengths and limitations. we study both approaches under the framework of beam-search. by developing a graph-based and a transition-based dependency parser, we show that a beam-search decoder is a competitive choice for both methods. more importantly, we propose a beam-search-based parser that combines both graph-based and transition-based parsing into a single system for training and decoding, showing that it outperforms both the pure graph-based and the pure transition-based", "prediction": "a tale of two parsers investigating and combining based and based dependency parsing based and transition to dependency parsing adopt very different views of the problem each view having own strengths and strengths we study we study both approaches under the framework of beamsearch by developing a graph based and a transition based dependency we show that a beam search decoder is a competitive choice for both we propose propose a beam beam based parser that combines both graph based and transitionbased parsing into a single system for training and showing that it outperforms both the pure"}
{"ground_truth": "forest reranking: discriminative parsing with non-local features conventional n-best reranking techniques often suffer from the limited scope of the nbest list, which rules out many potentially good alternatives. we instead propose forest reranking, a method that reranks a packed forest of exponentially many parses. since exact inference is intractable with non-local features, we present an approximate algorithm inspired by forest rescoring that makes discriminative training practical over the whole treebank. our final result, an f-score of 91.7, outperforms both 50-best and 100-best reranking baselines, and is better than any previously reported systems trained on the treebank. we show that the", "prediction": "forest discriminative parsing with local features features reranking techniques ofsuffer from the limited scope of the best best which rules out many potentially potentially we instead propose a method reranks a packed forest of a packed forest of exponentially many since exact inference is local local we present an approximate algorithm inspired by forest rescoring that makes discriminative training practical over practical the whole whole our final our final final of f outperforms both 50 best and 100 best best and better better than any any previously we propose"}
{"ground_truth": "a clustering approach for nearly unsupervised recognition of nonliteral language in this paper we present trofi (trope finder), a system for automatically classifying literal and nonliteral usages of verbs through nearly unsupervised word-sense disambiguation and clustering techniques. trofi uses sentential context instead of selectional constraint violations or paths in semantic hierarchies. it also uses literal and nonliteral seed sets acquired and cleaned without human supervision in order to bootstrap learning. we adapt a word-sense disambiguation algorithm to our task and augment it with multiple seed set learners, a voting schema, and additional features like supertags and extra-sentential context. detailed experiments", "prediction": "a clustering approach for nearly unsupervised recognition nonliteral nonliteral language in this paper we present trofi trope a system for automatically classifying literal and nonliteral usages of verbs through nearly unsupervised word sense disambiguation and clustering clustering sentential context instead of selectional constraint violations or paths paths in semantic it also uses literal and nonliteral seed sets acquired and cleaned without human supervision in order to bootstrap learning we adapt adapt a word sense disambiguation disambiguation algorithm to our task and augment it with multiple seed set a voting voting and additional additional features supertags and extrasentential detailed experiments supertags detailed experiments on hand"}
{"ground_truth": "exploring content models for multi-document summarization we present an exploration of generative probabilistic models for multi-document summarization. beginning with a simple word frequency based model (nenkova and vanderwende, 2005), we construct a sequence of models each injecting more structure into the representation of document set content and exhibiting rouge gains along the way. our final model, hiersum, utilizes a hierarchical lda-style model (blei et al., 2004) to represent content specificity as a hierarchy of topic vocabulary distributions. at the task of producing generic duc-style summaries, hiersum yields state-of-the-art rouge performance and in pairwise user evaluation strongly outperforms toutanova et al.", "prediction": "exploring content models for multi document document summarization we present an exploration of generative probabilistic models for multi document beginning with a simple word frequency based model nenkova and nenkova we construct a sequence of models each models each injecting more structure into the representation of document set content and exhibiting rouge gains along along the way our a hierarchical lda style model blei et to represent to represent a hierarchy of topic a hierarchy of topic the hierarchy task of the task of the task of the rouge"}
{"ground_truth": "better evaluation for grammatical error correction we present a novel method for evaluating grammatical error correction. the core of our method, which we call maxmatch (m2), is an algorithm for efficiently computing the sequence of phrase-level edits between a source sentence and a system hypothesis that achieves the highest overlap with the gold-standard annotation. this optimal edit sequence is subsequently scored using f1 measure. we test our m2 scorer on the helping our own (hoo) shared task data and show that our method results in more accurate evaluation for grammatical error correction. we propose an alternative evaluation scheme which operates", "prediction": "better evaluation for grammatical error correction we present a novel method for evaluating grammatical error error the core of method which we call is an algorithm for efficiently computing computing the sequence of phrase level edits between a source sentence and a system hypothesis that achieves the highest overlap with the goldstandard this optimal edit seis subsequently scored using mea seis we test our on the helping our hoo task data and show that hoo task data and show that our results in more accurate evaluation for grammatical we propose we propose"}
{"ground_truth": "a joint model of text and aspect ratings for sentiment summarization online reviews are often accompanied with numerical ratings provided by users for a set of service or product aspects. we propose a statistical model which is able to discover corresponding topics in text and extract textual evidence from reviews supporting each of these aspect ratings \u2013 a fundamental problem in aspect-based sentiment summarization (hu and liu, 2004a). our model achieves high accuracy, without any explicitly labeled data except the user provided opinion ratings. the proposed approach is general and can be used for segmentation in other applications where sequential", "prediction": "a joint model of text and aspect ratings for sentiment summarization reviews are often accompanied with numerical ratings provided by users for a set of service or or we propose propose a statistical model which is able to discover corresponding topics in text and extract textual evidence from reviews supporting each of these aspect ratings a fundamental problem in aspect based sentiment summarization hu summarization and high high without any explicitly labeled labeled data except the user user provided opinion the proposed proposed approach is is general and can be for segmentation in other applications where sequential data is accompanied with correlated"}
{"ground_truth": "learning dependency-based compositional semantics compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms. in this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs. in tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms. on two standard semantic parsing benchmarks (geo and jobs), our system obtains the highest published accuracies, despite requiring no annotated logical", "prediction": "learning compositional compositional semantics compositional question answering begins by mapping questions to logical but training a semantic parser parser to perform this mapping typically requires the costly annotation of the target logical logical in this paper, we learn to map questions to answers via latent logical which are induced are induced automatically from question answer in tackling challenging learning learning we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient efficient evaluation of logical logical on two stansemantic parsing benchmarks our system obtains the highest published despite requiring no no annotated logical what is"}
{"ground_truth": "learning non-isomorphic tree mappings for machine translation often one may wish to learn a tree-to-tree mapping, training it on unaligned pairs of trees, or on a mixture of trees and strings. unlike previous statistical formalisms (limited to isomorphic trees), synchronous tsg allows local distortion of the tree topology. we reformulate it to permit dependency trees, and sketch em/viterbi algorithms for alignment, training, and decoding. we argue that if the parse tree of source sentence is provided, decoding (for tree-to-string and tree-to-tree models) can also be cast as a tree-parsing problem. we consider synchronous tree substitution grammar, a formalism that can", "prediction": "learning tree mappings for machine translation often often one may wish wish to learn a tree to tree training it on unaligned pairs of unaligned or or on a mixture of trees and unlike previous statistical formalisms to isomorphic tsg local distortion of the tree local we reformulate it it to permit dependency em and sketch em viterbi algorithms for machine translation systems and translation systems are trained on sentences that are sentences that are for the are beaucoup enfants a kids baiser a kids kiss a kids kiss translation"}
{"ground_truth": "nltk: the natural language toolkit nltk, the natural language toolkit, is a suite of open source program modules, tutorials and problem sets, providing ready-to-use computational linguistics courseware. nltk covers symbolic and statistical natural language processing, and is interfaced to annotated corpora. students augment and replace existing components, learn structured programming by example, and manipulate sophisticated models from the outset. nltk, the natural language toolkit, is a suite of python modules providing many nlp data types, processing tasks, corpus samples and readers, together with animated algorithms, tutorials and problem sets.", "prediction": "nltk nltk the natural language toolkit toolkit nltk the natural language language is a suite of open open source program tutorials and problem ready to providing ready to use computational linguistics nltk covers symbolic and statistical natural is interfaced to annotated and is interfaced to replace existing existing augment and existing learn structured programming by by sophisticated and manipulate sophisticated models from the outset teachers of introductory courses on computational linguistics are often with the challenge of setting a practical programming component for student assignments and this is a difficult task because a difficult difficult task because a difficult"}
{"ground_truth": "automatically labeling semantic classes systems that automatically discover semantic classes have emerged in part to address the limitations of broad-coverage lexical resources such as wordnet and cyc. the current state of the art discovers many semantic classes but fails to label their concepts. we propose an algorithm labeling semantic classes and for leveraging them to extract is-a relationships using a top-down approach. the relationships automatically learned in our system include appositions, nominal subjects, such as relationships, and like relationships. our syntactical co-occurrence approach has worst-case time complexity o (n2k), where n is the number of words in the corpus and", "prediction": "automatically labeling semantic classes systems that automatically discover discover semantic classes have in part to address the limitations of broad coverage resources such as wordnet and the art discovers semantic classes but fails fails to label their their their we propose propose an algorithm labeling semantic and for leveraging them to extract relationships using a top down down the natural language literature is rich in theories of perry theories of perry and perry schank and miller 1990 and cyc lenat we propose"}
{"ground_truth": "advances in domain independent linear text segmentation this paper describes a method for linear text segmentation which is twice as accurate and over seven times as fast as the state-of-the-art (reynar, 1998). inter-sentence similarity is replaced by rank in the local context. boundary locations are discovered by divisive clustering. we design an artificial dataset, built by concatenating short pieces of texts that have been extracted from the brown corpus.", "prediction": "advances in domain linear text segmentation this paper describes a method for linear text segmentation which is twice as accurate and over times seven as fast as the state similarity sentence similarity sentence similarity is by in the local the local local the local locations discovered divisive the new address several topics aspects of the aim of same the aim of the aim of text segmentation is to the topic of the topic the topic of this we use a of the uses"}
{"ground_truth": "reading level assessment using support vector machines and statistical language models reading proficiency is a fundamental component of language competency. however, finding topical texts at an appropriate reading level for foreign and second language learners is a challenge for teachers. this task can be addressed with natural language processing technology to assess reading level. existing measures of reading level are not well suited to this task, but previous work and our own pilot experiments have shown the benefit of using statistical language models. in this paper, we also use support vector machines to combine features from traditional reading level measures,", "prediction": "reading assessment using support support vector machines and statistical statistical language models reading proficiency is a fundamental component component of language competency finding finding topical texts at at an appropriate reading level for foreign and second language learners is a challenge for this task can be addressed with natural language processing technology to existing measures of reading level are not well well suited to this but this work and our pilot experiments have shown the benefit of using statistical language statistical language in this we also use support vector machines to combine features combine features from traditional reading"}
{"ground_truth": "intricacies of collins parsing model this article documents a large set of heretofore unpublished details collins used in his parser, such that, along with collins\u2019 (1999) thesis, this article contains all information necessary to duplicate collins\u2019 benchmark results. indeed, these as-yet-unpublished details account for an 11% relative increase in error from an implementation including all details to a clean-room implementation of collins\u2019 model. we also show a cleaner and equally well-performing method for the handling of punctuation and conjunction and reveal certain other probabilistic oddities about collins\u2019 parser. we not only analyze the effect of the unpublished details, but also", "prediction": "intricacies of parsing model model university of pennsylvania this article documents documents a large set of heretofore unpublished details used in his such collins collins 1999 this article contains all information necessary to duplicate collins benchmark benchmark benchmark benchmark indeed these as unpublished account for an 11 relative increase in error from an implementation including all to a clean room implementation of collins collins we also show a cleaner and well performing method for the handling of conjunction and reveal certain other probabilistic oddities we not the unpublished collins we not only the effect of the unpublished reanalyze the effect of certain"}
{"ground_truth": "phrase dependency parsing for opinion mining in this paper, we present a novel approach for mining opinions from product reviews, where it converts opinion mining task to identify product features, expressions of opinions and relations between them. by taking advantage of the observation that a lot of product features are phrases, a concept of phrase dependency parsing is introduced, which extends traditional dependency parsing to phrase level. this concept is then implemented for extracting relations between product features and expressions of opinions. experimental evaluations show that the mining task can benefit from phrase dependency parsing. we utilize the dependency parser", "prediction": "phrase dependency parsing for opinion mining in this we present a novel approach for mining opinions from product where it it converts opinion mining task to identify identify expressions of opinions and relations between by taking advantage of the observation observation that a lot of product features are phrases a concept of phrase dependency parsing is which extends traditional dependency parsing to phrase this parsing to phrase this concept is implemented for extracting features and expressions of opinions experimental evaluations show that the mining task can benefit from phrase dependency of users contribute rich information to the internet an internet an enormous number of reviews are written"}
{"ground_truth": "improving a statistical mt system with automatically learned rewrite patterns current clump-based statistical mt systems have two limitations with respect to word ordering: first, they lack a mechanism for expressing and using generalization that accounts for reorderings of linguistic phrases. second, the ordering of target words in such systems does not respect linguistic phrase boundaries. to address these limitations, we propose to use automatically learned rewrite patterns to preprocess the source sentences so that they have a word order similar to that of the target langauge. our system is a hybrid one. the basic model is statistical, but we use", "prediction": "identifying we present the system we present the system we present the system we present the system we present the system we present the system of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the system of the system of the system of the system of the system of the system of the system of the system of the system of the system of the system of the system of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the inclusion of the"}
{"ground_truth": "tense as discourse anaphor in this paper, i consider a range of english expressions and show that their context-dependency can be characterized in terms of two properties: 1. they specify entities in an evolving model of the discourse that the listener is constructing; 2. the particular entity specified depends on another entity in that part of the evolving \"discourse model\" that the listener is currently attending to. such expressions have been called anaphors. i show how tensed clauses share these characteristics, usually just attributed to anaphoric noun phrases. this not only allows us to capture in a simple way the", "prediction": "as as discourse in this paper i i consider a range of english expressions and show that their context dependency can be characterized in terms terms of two properties they specify entities in an evolving model of the discourse that the listener is constructing the particular entity specified specified depends on another entity in that part of the evolving is discourse model quot that the listener is attending have been called show how tensed clauses share these these usually just attributed attributed to anaphoric noun noun this not only allows in to capture in a simple simple way a that prove intuition that prove"}
{"ground_truth": "exploiting syntactic structure for language modeling the paper presents a language model that develops syntactic structure and uses it to extract meaningful information from the word history, thus enabling the use of long distance dependencies. the model assigns probability to every joint sequence of words-binary-parse-structure with headword annotation and operates in a left-to-right manner -- therefore usable for automatic speech recognition. the model, its probabilistic parameterization, and a set of experiments meant to evaluate its predictive power are presented; an improvement over standard trigram modeling is achieved. we choose the lexical heads of the two previous constituents as determined by", "prediction": "exploiting syntactic structure for language modeling modeling modeling the paper presents a language model that develops syntactic structure and uses it it to extract meaningful information from the word history thus enabling the use of long distance distance the model assigns probability to every joint sequence of words binary parse structure with headword annotation and operates in a left left to right manner therefore usable for automatic speech the its probabilistic and a set of experiments meant to set of experiments meant to evaluate predictive power are power an improvement over trigram modeling modeling modeling modeling modeling the main main goal of"}
{"ground_truth": "entity-based cross-document core f erencing using the vector space model cross-document coreference occurs when the same person, place, event, or concept is discussed in more than one text source. computer recognition of this phenomenon is important because it helps break \"the document boundary\" by allowing a user to examine information about a particular entity from multiple text sources at the same time. in this paper we describe a cross-document coreference resolution algorithm which uses the vector space model to resolve ambiguities between people having the same name. in addition, we also describe a scoring algorithm for evaluating the cross-document coreference", "prediction": "entity cross document core f erencing using the vector space model cross document coreference occurs when the same person or or concept is discussed is discussed in more than one text computer recognition one of this phenomenon is important because it helps break the document boundary quot by allowing a user examine information about a particular entity from multiple text sources at the same in this paper we describe a cross document algorithm which uses the vector space model to resolve ambiguities between people having the same in we also also describe a scoring algorithm for evaluating evaluating evaluating we compare our algorithm to the the document coreference produced by our system and"}
{"ground_truth": "why doesn't em find good hmm pos-taggers? this paper investigates why the hmms estimated by expectation-maximization (em) produce such poor results as part-of-speech (pos) taggers. we find that the hmms estimated by em generally assign a roughly equal number of word tokens to each hidden state, while the empirical distribution of tokens to pos tags is highly skewed. this motivates a bayesian approach using a sparse prior to bias the estimator toward such a skewed distribution. we investigate gibbs sampling (gs) and variational bayes (vb) estimators and show that vb converges faster than gs for this task and that vb", "prediction": "why doesn't em find good hmm pos taggers this paper investigates investigates why the hmms es timated by expectation maximization em poor such poor results as of speech we find that the hmms es timated by em generally a roughlyequal number of word tokens to each hid den while the empirical distribution of tokens to pos tags is highly this motivates motivates a bayesian approach using a bayesian the estimator approach using a the estimator toward such a skewed skewed we investigate gibbs sampling variational bayes vb estimators and show that vb con verges gs gs task and thatvb significantly improves"}
{"ground_truth": "an unsupervised approach to recognizing discourse relations we present an unsupervised approach to recognizing discourse relations of contrast, explanation-evidence, condition and elaboration that hold between arbitrary spans of texts. we show that discourse relation classifiers trained on examples that are automatically extracted from massive amounts of text can be used to distinguish between some of these relations with accuracies as high as 93%, even when the relations are not explicitly marked by cue phrases. we use a pattern based approach to extract instances of discourse relations such as contrast and elaboration from unlabeled corpora. we propose a method to identify", "prediction": "an unsupervised approach to recognizing discourse relations we present an unsupervised approach to discourse relations of hold between arbitrary spans of we show that discourse relation classifiers trained on examples that are automatically extracted from massive amounts of text from text can be used to distinguish between some of relations with accuracies as as even when the relations are the relations not relations not explicitly cue by cue the field of it we propose we propose we propose we use a of the set of the most of"}
{"ground_truth": "edge-based best-first chart parsing best-first probabilistic chart parsing attempts to parse efficiently by working on edges that are judged \"best\" by some probabilistic figure of merit (fom). recent work has used probabilistic context-free grammars (pcfgs) to assign probabilities to constituents, and to use these probabilities as the starting point for the fom. this paper extends this approach to using a probabilistic fom to judge edges (incomplete constituents), thereby giving a much finer-grained control over parsing effort. we show how this can be accomplished in a particularly simple way using the common idea of binarizing the pcfg. the results obtained are", "prediction": "based best best first parsing best first probabilistic chart parsing attempts to parse efficiently by working on edges that are judged best best probabilistic figure of merit recent work has used probabilistic grammars pcfgs to assign probabilities to and to these probabilities as the point for the fom this paper extends this approach to using a probabilistic fom to judge edges incomplete thereby giving a much finergrained control over parsing edges show how over we show how this can be accomplished in a particularly simple way using the common idea of binarizing the a facof twenty over the facof are over the twenty"}
{"ground_truth": "annealing structural bias in multilingual weighted grammar induction we first show how a structural locality bias can improve the accuracy of state-of-the-art dependency grammar induction models trained by em from unannotated examples (klein and manning, 2004). next, by annealing the free parameter that controls this bias, we achieve further improvements. we then describe an alternative kind of structural bias, toward \"broken\" hypotheses consisting of partial structures over segmented sentences, and show a similar pattern of improvement. we relate this approach to contrastive estimation (smith and eisner, 2005a), apply the latter to grammar induction in six languages, and show that our", "prediction": "annealing structural bias in multilingual weighted grammar induction first show how a structural bias improve the accuracy of state of the art grammar induction models trained by em from unannotated klein and annealing annealing the free parameter parameter that controls this we by annealing the controls this further we an alternative kind of structural structural toward hypotheses consisting of partial structures over segmented and show a similar pattern of pattern of we relate this approach approach to contrastive estimation smith and apply the latter latter to grammar induction in six six languages and show that our new approach improves accuracy over absolute over ce and"}
{"ground_truth": "assessing agreement on classification tasks: the kappa statistic currently, computational linguists and cognitive scientists working in the area of discourse and dialogue argue that their subjective judgments are reliable using several different statistics, none of which are easily interpretable or comparable to each other. meanwhile, researchers in content analysis have already experienced the same difficulties and come up with a solution in the kappa statistic. we discuss what is wrong with reliability measures as they are currently used for discourse and dialogue work in computational linguistics and cognitive science, and argue that we would be better off as afield adopting", "prediction": "assessing agreement on classification tasks the kappa statistic statistic currently computational linguists and cognitive scientists working in the area of discourse and dialogue argue that their subjective judgments are reliable using several several different none of which are easily interpretable comparable comparable to each other in content analysis content content analysis have already the same difficulties and come up with a solution in the kappa statistic we discuss what is wrong with reliability measures as they are currently used for discourse discourse and dialogue work in computational linguistics and we computational linguistics and argue that we would as afield content"}
{"ground_truth": "domain-specific sense distributions and predominant sense acquisition distributions of the senses of words are often highly skewed. this fact is exploited by word sense disambiguation (wsd) systems which back off to the predominant sense of a word when contextual clues are not strong enough. the domain of a document has a strong influence on the sense distribution of words, but it is not feasible to produce large manually annotated corpora for every domain of interest. in this paper we describe the construction of three sense annotated corpora in different domains for a sample of english words. we apply an existing", "prediction": "domain specific sense distributions and sense acquisition distributions of the senses of words are often highly this fact is exploitedby word word sense disambiguation wsd sys which off to the predominant sense of a word when contextual clues arenot strong the domain of a doc ument has a strong influence on the sensedistribution of it is not feasi ble to produce large manually annotated corpora for every for every of annotated corpora in this paper we describe the construction of corpora in different domains for a sample of english english for an existing method for acquiring predominant sense automatically from raw automatically and for our sam ple sam"}
{"ground_truth": "named entity recognition using an hmm-based chunk tagger this paper proposes a hidden markov model (hmm) and an hmm-based chunk tagger, from which a named entity (ne) recognition (ner) system is built to recognize and classify names, times and numerical quantities. through the hmm, our system is able to apply and integrate four types of internal and external evidences: 1) simple deterministic internal feature of the words, such as capitalization and digitalization; 2) internal semantic feature of important triggers; 3) internal gazetteer feature; 4) external macro context feature. in this way, the ner problem can be resolved effectively. evaluation of", "prediction": "named entity recognition using an hmm based chunk tagger this paper proposes a hidden markov model hmm and an hmm based chunk from which a named entity ne recognition ner system is built to recognize and classify classify names times and numerical the hmm our system is through the our system is to apply and internal types internal and external evidences simple simple deterministic such as such as capitalization and as capitalization semantic feature of important triggers semantic feature of important 3 internal macro context in context in this paper, the ner"}
{"ground_truth": "learning subjective nouns using extraction pattern bootstrapping we explore the idea of creating a subjectivity classifier that uses lists of subjective nouns learned by bootstrapping algorithms. the goal of our research is to develop a system that can distinguish subjective sentences from objective sentences. first, we use two bootstrapping algorithms that exploit extraction patterns to learn sets of subjective nouns. then we train a naive bayes classifier using the subjective nouns, discourse features, and subjectivity clues identified in prior research. the bootstrapping algorithms learned over 1000 subjective nouns, and the subjectivity classifier performed well, achieving 77% recall with 81% precision.", "prediction": "learning subjective nouns using extraction extraction pattern bootstrapping we explore the idea of creating a subjectivity classifier that uses lists of subjective nouns learned by bootstrapping the goal of our research is to develop a system that can subjective sentences subjective sentences from objective we use two bootstrapping algorithms that exploit extraction patterns to learn sets of subjective then we train a subjective naive bayes classifier using the subjective subjective clues identified in prior the subjective subjectivity clues the bootstrapping algorithms learned over the subjectivity over"}
{"ground_truth": "unsupervised coreference resolution in a nonparametric bayesian model we present an unsupervised, nonparametric bayesian approach to coreference resolution which models both global entity identity across a corpus as well as the sequential anaphoric structure within each document. while most existing coreference work is driven by pairwise decisions, our model is fully generative, producing each mention from a combination of global entity properties and local attentional state. despite being unsupervised, our system achieves a 70.3 muc f1 measure on the muc-6 test set, broadly in the range of some recent supervised results. in our model, we use the distinction bewtween pronouns,", "prediction": "unsupervised coreference resolution in a nonparametric bayesian model we present an nonparametric bayesian approach to coreference resolution which models both global entity identity across a corpus as well as as well the sequential anaphoric structure within each most existing coreference work by driven our model is fully fully each mention from a combination of global entity properties and local attentional attentional despite being being achieves a our system achieves a 3 on the muc test test in the range of some in the range of some recent supervised in to an entity in can broadly be decomposed two"}
{"ground_truth": "a trainable rule-based algorithm for word segmentation this paper presents a trainable rule-based algorithm for performing word segmentation. the algorithm provides a simple, language-independent alternative to large-scale lexicai-based segmenters requiring large amounts of knowledge engineering. as a stand-alone segmenter, we show our algorithm to produce high performance chinese segmentation. in addition, we show the transformation-based algorithm to be effective in improving the output of several existing word segmentation algorithms in three different languages. our chinese segmenter makes use of only a manually segmented corpus without referring to any lexicon.", "prediction": "a trainable rule for word word segmentation this paper presents a trainable rule algorithm for performing word word the algorithm provides a simple language language independent alternative to large lexical based segmenters requiring large amounts of knowledge our knowledge our algorithm to produce high performance in we show the transformation performance in we show to be improving the output of several existing word segmentation algorithms in three different this paper presents a trainable rule algorithm word for performing word our algorithm is effective both as a high accuracy alone segmenter and as a postprocessor that improves the output of existing"}
{"ground_truth": "bidirectional inference with the easiest-first strategy for tagging sequence data this paper presents a bidirectional inference algorithm for sequence labeling problems such as part-of-speech tagging, named entity recognition and text chunking. the algorithm can enumerate all possible decomposition structures and find the highest probability sequence together with the corresponding decomposition structure in polynomial time. we also present an efficient decoding algorithm based on the easiest-first strategy, which gives comparably good performance to full bidirectional inference with significantly lower computational cost. experimental results of part-of-speech tagging and text chunking show that the proposed bidirectional inference methods consistently outperform unidirectional inference methods", "prediction": "bidirectional inference with the easiest first strategy for tagging sequence data this paper presents a bidirectional inference algorithm for sequence labeling problems such as part of speech tag named entity recognition and text the algorithm can enumerate enumerate all possible decomposition structures andfind the highest sequence together with the corresponding decomposi tion structure in polynomial we also present an efficient decoding algorithm algorithm based on the easiest first gives comparably good performance tofull bidirectional inference with significantly lower computational significantly lower computational speech tagging and text chunking text chunking text the proposedbidirectional methods proposedbidirectional methods tently outperform unidirectional inference methods and bidirectional memms give give performance to that achievedby inference methods and"}
{"ground_truth": "translating named entities using monolingual and bilingual resources named entity phrases are some of the most difficult phrases to translate because new phrases can appear from nowhere, and because many are domain specific, not to be found in bilingual dictionaries. we present a novel algorithm for translating named entity phrases using easily obtainable monolingual and bilingual resources. we report on the application and evaluation of this algorithm in translating arabic named entities to english. we also compare our results with the results obtained from human translations and a commercial system for the same task. we show that use of outside", "prediction": "translating named entities using monolingual and bilingual resources named entity phrases are some of the most difficult phrases to translate because new phrases can appear from nowhere and because many are domain not to be to be found in bilingual we present a novel for translating named entity phrases using easily obtainable monolingual and bilingual monolingual we report on the application and evaluation of this algorithm in translating arabic named entities to english we also compare our results with the results obtained from human translations and a commercial system for the same named entity phrases are in news"}
{"ground_truth": "articles: recognizing contextual polarity: an exploration of features for phrase-level sentiment analysis many approaches to automatic sentiment analysis begin with a large lexicon of words marked with their prior polarity (also called semantic orientation). however, the contextual polarity of the phrase in which a particular instance of a word appears may be quite different from the word\u2019s prior polarity. positive words are used in phrases expressing negative sentiments, or vice versa. also, quite often words that are positive or negative out of context are neutral in context, meaning they are not even being used to express a sentiment. the goal", "prediction": "articles recognizing contextual polarity an exploration of features for phrase level sentiment analysis approaches to automatic to automatic sentiment analysis begin with a large lexicon of words marked with their prior polarity also also called semantic however the contextual polarity of the phrase in which a particular instance of a word appears may be quite different from the word prior positive words words are used in phrases expressing negative negative negative or vice quite often words that are positive or negative out of context are neutral in context meaning they are not even being used to express a even used express a even"}
{"ground_truth": "a bootstrapping method for learning semantic lexicons using extraction pattern contexts this paper describes a bootstrapping algorithm called basilisk that learns high-quality semantic lexicons for multiple categories. basilisk begins with an unannotated corpus and seed words for each semantic category, which are then bootstrapped to learn new words for each category. basilisk hypothesizes the semantic class of a word based on collective information over a large body of extraction pattern contexts. we evaluate basilisk on six semantic categories. the semantic lexicons produced by basilisk have higher precision than those produced by previous techniques, with several categories showing substantial improvement. we", "prediction": "a method for semantic lexicons lexicons using extraction pattern contexts contexts this paper describes a algorithm called basilisk that learns highquality semantic lexicons for multiple basilisk begins basilisk with an unannotated corpus and seed words for each semantic semantic which are then bootstrapped to learn new words for each category basilisk hypothesizes the semantic class of a word based on collective information over a large body of pattern of pattern we evaluate basilisk on six semantic semantic categories the semantic lexicons produced by basilisk higher precision than produced by previous with categories showing substantial"}
{"ground_truth": "chinese segmentation with a word-based perceptron algorithm standard approaches to chinese word segmentation treat the problem as a tagging task, assigning labels to the characters in the sequence indicating whether the character marks a word boundary. discriminatively trained models based on local character features are used to make the tagging decisions, with viterbi decoding finding the highest scoring segmentation. in this paper we propose an alternative, word-based segmentor, which uses features based on complete words and word sequences. the generalized perceptron algorithm is used for discriminative training, and we use a beam-search decoder. closed tests on the first and second", "prediction": "chinese segmentation with a word based perceptron standard approaches to chinese word segmentation treat treat the problem as a tagging task assigning labels to to the characters in the sequence indicating whether the character marks a word discriminatively trained models based on local character features are used to make the tagging with viterbi finding decoding finding the highest scoring segmentation in this paper we propose an alternative word which uses features based on complete words and word word the generalized perceptron algorithm is used for discriminative we use a beamsearch closed tests on the first and show that our system is competitive with the best"}
{"ground_truth": "a stochastic japanese morphological analyzer using a forward-dp backward-a* n-best search algorithm we present a novel method for segmenting the input sentence into words and assigning parts of speech to the words. it consists of a statistical language model and an efficient two-pass n-best search algorithm. the algorithm does not require delimiters between words. thus it is suitable for written japanese. the proposed japanese morphological analyzer achieved 95.l% recall and 94.6% precision for open text when it was trained and tested on the atr corpus. we propose a method to search for the n best sets.", "prediction": "a stochastic japanese using a stochastic we a dp backward a algor thm aki we present fax abstract we present a 59 81 468 l abstract abstract we present a novel method sentence into words and assigning parts of speech to the words it consists of a statistical language model and an efficient wo pa qs n best search the algorithm algorithm does not require between between thus it for written qhe for written qhe proposed japanese nalyzer achieved nalyzer recall"}
{"ground_truth": "a multi-pass sieve for coreference resolution most coreference resolution models determine if two mentions are coreferent using a single function over a set of constraints or features. this approach can lead to incorrect decisions as lower precision features often overwhelm the smaller number of high precision ones. to overcome this problem, we propose a simple coreference architecture based on a sieve that applies tiers of deterministic coreference models one at a time from highest to lowest precision. each tier builds on the previous tier\u2019s entity cluster output. further, our model propagates global information by sharing attributes (e.g., gender and number)", "prediction": "multi a sieve for coreference resolution most coreference resolution models determine if two mentions are coreferent using a single function over a set of constraints or constraints this approach can lead to incorrect to incorrect decisions lower as precision features often overwhelm the number of high precision to overcome this we propose propose a simple this we propose a simple coreference architecture based on a sieve that applies tiers of deterministic coreference from highest to lowest each each tier builds on the previous tier entity cluster cluster propagates our model propagates propagates global information by sharing attributes"}
{"ground_truth": "a semantic concordance a semantic concordance is a textual corpus and a lexicon so combined that every substantive word in the text is linked to its appropriate sense in the lexicon. thus it can be viewed either as a corpus in which words have been tagged syntactically and semantically, or as a lexicon in which example sentences can be found for many definitions. a semantic concordance is being constructed to use in studies of sense resolution in context (semantic disambiguation). the brown corpus is the text and wordnet is the lexicon. semantic tags (pointers to wordnet synsets) are inserted in", "prediction": "a semantic concordance concordance oncordance is a semantic oncordance is a textual corpus and a bined lexicon that every substantive substantive word in the text is linked nse in the lexicon thus it can be viewed either as a corpus in which words have been tagged syntactically and semanti or or as a lexicon in which example sentences can be found for many many a semantic oncordance is being constructed to use in studies of sense in context context semantic disambigua the brown corpus semantic disambigua the brown"}
{"ground_truth": "a smorgasbord of features for statistical machine translation we describe a methodology for rapid experimentation in statistical machine translation which we use to add a large number of features to a baseline system exploiting features from a wide range of levels of syntactic representation. feature values were combined in a log-linear model to select the highest scoring candidate translation from an n-best list. feature weights were optimized directly against the bleu evaluation metric on held-out data. we present results for a small selection of features at each level of syntactic representation. at the 2003 johns hopkins summer workshop on statistical", "prediction": "a smorgasbord of features for statistical machine translation we describe a methodology for experimentation in statistical machine translation which we use to add a large number of features to a system exploiting features from a wide range of levels of syntactic values feature were combined in a log model to select the highest scoring candidate from an feature weights were optimized directly against the bleu evaluation metric on held out out we present results for a small selection of features at each level of syntactic representation despite the enormous progress in machine translation mt due to the use of statistical techniques in recent state of the statistical"}
{"ground_truth": "a fully bayesian approach to unsupervised part-of-speech tagging unsupervised learning of linguistic structure is a difficult problem. a common approach is to define a generative model and maximize the probability of the hidden structure given the observed data. typically, this is done using maximum-likelihood estimation (mle) of the model parameters. we show using part-of-speech tagging that a fully bayesian approach can greatly improve performance. rather than estimating a single set of parameters, the bayesian approach integrates over all possible parameter values. this difference ensures that the learned structure will have high probability over a range of possible parameters, and permits", "prediction": "a fully bayesian approach to unsupervised part of speech unsupervised learning of linguistic structure linguistic structure difficult is a difficult a common approach is to define a generative model and maximize the probability of the hidden structure given the observed data typically this is done using maximum likelihood estimation mle of the model we show using of speech tagging tagging that a fully bayesian approach can greatly improve improve rather than estimating a single set of the bayesian approach integrates over all parameter possible parameter this difference ensures that the learned structure will have over a range of"}
{"ground_truth": "applying many-to-many alignments and hidden markov models to letter-to-phoneme conversion letter-to-phoneme conversion generally requires aligned training data of letters and phonemes. typically, the alignments are limited to one-to-one alignments. we present a novel technique of training with many-to-many alignments. a letter chunking bigram prediction manages double letters and double phonemes automatically as opposed to preprocessing with fixed lists. we also apply an hmm method in conjunction with a local classification model to predict a global phoneme sequence given a word. the many-to-many alignments result in significant improvements over the traditional one-to-one approach. our system achieves state-of-the-art performance on several languages", "prediction": "applying many to many alignments and hidden models to models to letter to phoneme conversion letter to phoneme conversion generally requires training data of letters and phonemes the alignments are limited limited limited to are to one to one alignments we present a novel technique of training with many to many many a letter chunking bigram prediction manages double letters and double automatically automatically as opposed to preprocessing with fixed we also apply an hmm method in conjunction with a local classification model to a a local classification predict a sequence given the many to many alignments improvements improvements over the"}
{"ground_truth": "espresso: leveraging generic patterns for automatically harvesting semantic relations in this paper, we present espresso, a weakly-supervised, general-purpose, and accurate algorithm for harvesting semantic relations. the main contributions are: i) a method for exploiting generic patterns by filtering incorrect instances using the web; and ii) a principled measure of pattern and instance reliability enabling the filtering algorithm. we present an empirical comparison of espresso with various state of the art systems, on different size and genre corpora, on extracting various general and specific relations. experimental results show that our exploitation of generic patterns substantially increases system recall with small effect", "prediction": "espresso leveraging generic for automatically harvesting semantic relations this we present a weakly weakly general general and accurate algorithm for harvesting semantic semantic the main contributions are i a method for exploiting generic patterns by filtering incorrect instances using the web ; and ii a principled measure of pattern and instance reliability enabling the filtering filtering we present an empirical comof various state of the art on different size genre on extracting various general general and specific specific experimental results show that our exploitation of generic patterns patterns substantially increases system recall with small effect on overall"}
{"ground_truth": "self-training for biomedical parsing parser self-training is the technique of taking an existing parser, parsing extra data and then creating a second parser by treating the extra data as further training data. here we apply this technique to parser adaptation. in particular, we self-train the standard charniak/johnson penn-treebank parser using unlabeled biomedical abstracts. this achieves an f-score of 84.3% on a standard test set of biomedical abstracts from the genia corpus. this is a 20% error reduction over the best previous result on biomedical data (80.2% on the same test set).", "prediction": "training for biomedical parsing parser training training is the technique of taking an existing parsing extra extra data and then creating a second parser parser by treating the extra data as data as further training we apply apply this technique to parser parser in we self train the standard charniak johnson penn treebank treebank parser using unlabeled biomedical this an this an 3 on a standard test set of biomedical abstracts from the genia genia this is a 20 error reduction over the best previous result on biomedical data 2 on the test"}
{"ground_truth": "combining distributional and morphological information for part of speech induction in this paper we discuss algorithms for clustering words into classes from unlabelled text using unsupervised algorithms, based on distributional and morphological information. we show how the use of morphological information can improve the performance on rare words, and that this is robust across a wide range of languages. we propose a perplexity based test for the quality of the pos induction algorithm. we find that many-to-1 accuracy has several defects.", "prediction": "combining distributional and morphological information for part for in this paper we discuss for clustering words into classes from unlabelled unlabelled text using unsupervised algorithms based on distributional morphological and morphological we show how the use of morphological information can improve the performance on rare and that this and that this is robust across a range of the task studied in paper the task in this is the unsupervised learning of that is to to corresponding we propose we propose we present corresponding we use a of the corresponding of"}
{"ground_truth": "automatic identification of non-compositional multi-word expressions using latent semantic analysis making use of latent semantic analysis, we explore the hypothesis that local linguistic context can serve to identify multi-word expressions that have non-compositional meanings. we propose that vector-similarity between distribution vectors associated with an mwe as a whole and those associated with its constitutent parts can serve as a good measure of the degree to which the mwe is compositional. we present experiments that show that low (cosine) similarity does, in fact, correlate with non-compositionality. we devise a supervised method in which they compute the meaning vectors for the literal", "prediction": "automatic automatic identification of multi word expressions using latent latent semantic analysis making use of latent semantic semantic we explore the hypothesis that local linguistic context can serve to identify multi expressions that have noncompositional we propose that vector vectors associated with an mwe as a whole and those associated with its constitutent parts can serve as a good measure of the degree to which the mwe is is experiments present show that low cosine similarity in we use the low in we propose the correlate we use a"}
{"ground_truth": "the berkeley framenet project framenet is a three-year nsf-supported project in corpus-based computational lexicography, now in its second year (nsf iri-9618838, \"tools for lexicon building\"). the project's key features are (a) a commitment to corpus evidence for semantic and syntactic generalizations, and (b) the representation of the valences of its target words (mostly nouns, adjectives, and verbs) in which the semantic portion makes use of frame semantics. the resulting database will contain (a) descriptions of the semantic frames underlying the meanings of the words described, and (b) the valence representation (semantic and syntactic) of several thousand words and phrases, each", "prediction": "the berkeley framenet project is a project we present in this paper presents the berkeley of the tools the tools project's for the project's for the project's of the berkeley of the new of the valences of the valences of the valences of the valences of the the of semantic the of semantic the semantic of the semantic of a"}
{"ground_truth": "a maximum entropy approach to chinese word segmentation we participated in the second international chinese word segmentation bakeoff. specifically, we evaluated our chinese word segmenter in the open track, on all four corpora, namely academia sinica (as), city university of hong kong (cityu), microsoft research (msr), and peking university (pku). based on a maximum entropy approach, our word segmenter achieved the highest f measure for as, cityu, and pku, and the second highest for msr. we found that the use of an external dictionary and additional training corpora of different segmentation standards helped to further improve segmentation accuracy. we present", "prediction": "a maximum approach to chinese word kiat kiat and hwee participated in in in the second we participated in the second national we our chinese word segmenter in the open on all four four namely academia sinica microsoft search search and peking university peking university based on a maximum entropy approach our word segmenter achieved the highest f measure for as and and the second highest for we found that the use of an ex ternal dictionary dictionary and additional training corpora of different segmentation stan dards to further improve to further"}
{"ground_truth": "building deep dependency structures using a wide-coverage ccg parser this paper describes a wide-coverage statistical parser that uses combinatory categorial grammar (ccg) to derive dependency structures. the parser differs from most existing wide-coverage treebank parsers in capturing the long-range dependencies inherent in constructions such as coordination, extraction, raising and control, as well as the standard local predicate-argument dependencies. a set of dependency structures used for training and testing the parser is obtained from a treebank of ccg normal-form derivations, which have been derived (semi-) automatically from the penn treebank. the parser correctly recovers over 80% of labelled dependencies, and around", "prediction": "building dependency structures using a wide coverage ccg parser this paper describes a wide coverage that uses combinatory categorial grammar ccg to derive dependency dependency the parser differs from most most existing wide treebank parsers in capturing the long range dependencies constructions such as as raising and as well as the standard local predicate argument a set of dependency structures used for training training and testing the parser is obtained from a treebank of ccg normal have been derived semi automatically from the penn the parser correctly recovers over 80 of labelled and labelled around 90 of unlabelled"}
{"ground_truth": "a maximum entropy model for prepositional phrase attachment we construct a benchmark dataset of 27,937 pp-attachment quadruples extracted from the wall street journal corpus. we train a maximum entropy model on (v, n1, p, n2) quadruples extracted from the wall street journal corpus and achieve 81.6% accuracy. our maximum entropy approach uses the mutual information clustering algorithm.", "prediction": "maximum maximum entropy model for prepositional phrase attachment adwait a parser for natural natural language must often choose between between two or more equally grammatical parses for the same often the correct parse can be determined from the lexical properties of certain key words or from the context in which the sentence sentence for example in the the environmental protection agency imposed agrad ual ban on virtually agrad virtually all uses of asbestos the prepositional phrase on asbestos can attach to either the noun noun either the imposed"}
{"ground_truth": "anaphora for everyone: pronominal anaphora resolution without a parser we present an algorithm for anaphora resolution which is a modified and extended version of that developed by (lappin and leass, 1994). in contrast to that work, our algorithm does not require in-depth, full, syntactic parsing of text. instead, with minimal compromise in output quality, the modifications enable the resolution process to work from tile output of a part of speech tagger, enriched only with annotations of grammatical function of lexical items in the in-put text stream. evaluation of the results of our implementation demonstrates that accurate anaphora resolution can be", "prediction": "anaphora for everyone pronominal anaphora resolution without a parser we present an algorithm for anaphora for anaphora res olutkm which is a modified and extended version of that developed by lappin and in contrast to that work our al gorithm does not require in with minimal compromise in output the modifica tions enable the resolution process to work from tile output of a part of speech of gram matica functkm of gram matica functkm of lexical items in the in put text evaluation of the results of our in tplementation demonstrates that ac anaphora resolution can be within natural anguage natural anguage works"}
{"ground_truth": "learning a translation lexicon from monolingual corpora this paper presents work on the task of constructing a word-level translation lexicon purely from unrelated mono-lingual corpora. we combine various clues such as cognates, similar context, preservation of word similarity, and word frequency. experimental results for the construction of a german-english noun lexicon are reported. noun translation accuracy of 39% scored against a parallel test corpus could be achieved. we automatically induce the initial seed bilingual dictionary by using identical spelling features such as cognates and similar contexts.", "prediction": "learning learning a translation lexicon from monolingual corpora this paper presents work on the task of constructing a word level translation lexicon purely from unrelated monolingual we combine various clues clues such as preservation preservation of word word and word word experimental experimental the construction of results for the construction of are noun translation accuracy of 39 scored against a parallel parallel test corpus could be there has been a surge surge in research in machine translation that is based on empirical the seminal the seminal work by brown et"}
{"ground_truth": "coreference resolution using competition learning approach in this paper we propose a competition learning approach to coreference resolution. traditionally, supervised machine learning approaches adopt the single-candidate model. nevertheless the preference relationship between the antecedent candidates cannot be determined accurately in this model. by contrast, our approach adopts a twin-candidate learning model. such a model can present the competition criterion for antecedent candidates reliably, and ensure that the most preferred candidate is selected. furthermore, our approach applies a candidate filter to reduce the computational cost and data noises during training and resolution. the experimental results on muc-6 and muc-7 data set", "prediction": "coreference resolution using competition approach approach in this paper we propose a competition learning approach to coreference resolution supervised machine learning machine learning approaches adopt the singlecandidate adopt the singlecandidate model nevertheless nevertheless the preference relationship between the antecedent candidates cannot be determined accurately in this by our approach adopts a twin candidate learning learning such a model can present the competition criterion for antecedent candidates candidates and ensure that the most preferred candidate is is approach applies a the most candidate our approach applies a the computational cost and cost and data noises during training and the experimental results on muc 6"}
{"ground_truth": "experiments using stochastic search for text planning marcu has characterised an important and difficult problem in text planning: given a set of facts to convey and a set of rhetorical relations that can be used to link them together, how can one arrange this material so as to yield the best possible text? we describe experiments with a number of heuristic search methods for this task. we investigate the problem of determining a discourse tree for a set of elementary speech acts which are partially constrained by rhetorical relations. we advocate genetic algorithms as an alternative to exhaustively searching for", "prediction": "experiments using stochastic for text planning marcu has has characterised an important and difficult problem in text planning given a set of facts facts to convey and a set of rhetorical relations that can be used to link them can one arrange this material so as to yield to yield the best possible text we describe experiments with a number of heuristic search methods for this this paper presents some initial this paper initial experiments using stochastic search methods for aspects of text the work was motivated by the needs of the ilex for museum descriptions of in"}
{"ground_truth": "rouge: a package for automatic evaluation of summaries rouge stands for recall-oriented understudy for gisting evaluation. it includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. the measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. this paper introduces four different rouge measures: rouge-n, rouge-l, rouge-w, and rouge-s included in the rouge summarization evaluation package and their evaluations. three of them have been used in the document understanding conference", "prediction": "rouge a package for automatic evaluation of summaries for recall for gisting gisting it includes measures to automatically automatically determine the quality of a summary by comparing it to other comparing to ideal summaries created by the measures of the number of overlapping such as word and pairs between between the computer generated summary summary to be evaluated and the ideal summaries created by this paper introduces four different included the summarization evaluation package and in the summarization evaluation three of them have used in we propose we use"}
{"ground_truth": "recognizing text genres with simple metrics using discriminant analysis a simple method for categorizing texts into pre-determined text genre categories using the statistical standard technique of discriminant analysis is demonstrated with application to the brown corpus. discriminant analysis makes it possible to use a large number of parameters taht may be specific for a certain corpus or information streatm, and combine them into a small number of function, with the parameters weighted on bais of how useful they are for discriminating text genres. an application to information retrieval is discussed. we word length as an indicator of formality for applications", "prediction": "ecogni xt xt genii s paper presents the ecogni ys we describe the siml of a smnc of the smnc of the smnc of the smnc of the new of the resulting of the resulting of the resulting of the motivation of the resulting of the resulting of the motivation of the resulting of the resulting of the resulting of the resulting of the resulting of the resulting of the resulting of the resulting of the resulting of the resulting of the resulting of the resulting of the resulting of the resulting of the resulting of the resulting of the resulting of the resulting of the resulting of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the motivation of the , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,"}
{"ground_truth": "language model adaptation for statistical machine translation via structured query models we explore unsupervised language model adaptation techniques for statistical machine translation. the hypotheses from the machine translation output are converted into queries at different levels of representation power and used to extract similar sentences from very large monolingual text collection. specific language models are then build from the retrieved data and interpolated with a general background model. experiments show significant improvements when translating with these adapted language models. we apply a slightly different sentence-level strategy to language model adaptation, first generating an nbest list with a baseline system, then", "prediction": "language model adaptation for statistical machine translation structured via structured query models we explore unsupervised language model adaptation techniques for statistical machine the hypotheses from the output output are converted into queries at different levels of representation power and used to extract similar sentences from very text sentences from monolingual text specific language models are then build from the retrieved data and with a general background background experiments show significant improvements when translating with these adapted language language models lm are applied in many language processing such as speech recognition and machine"}
{"ground_truth": "unsupervised part-of-speech tagging employing efficient graph clustering an unsupervised part-of-speech (pos) tagging system that relies on graph clustering methods is described. unlike in current state-of-the-art approaches, the kind and number of different tags is generated by the method itself. we compute and merge two partitionings of word graphs: one based on context similarity of high frequency words, another on log-likelihood statistics for words of lower frequencies. using the resulting word clusters as a lexicon, a viterbi pos tagger is trained, which is refined by a morphological component. the approach is evaluated on three different languages by measuring agreement with existing", "prediction": "part of speech tagging employing efficient graph clustering an unsupervised of speech pos tagging tagging system that relies on graph clustering methods is unlike in current state of the art art the kind and number of different tags is generated by generated by the method we compute compute compute and merge two partitionings of word graphs one based on context similarity of high frequency another on log likelihood statistics for words of lower lower using the resulting word clusters as a a viterbi pos tagger is is a which is refined by a the approach is evaluated on different languages by with existing"}
{"ground_truth": "a hierarchical phrase-based model for statistical machine translation we present a statistical phrase-based translation model that uses hierarchical phrases - phrases that contain subphrases. the model is formally a synchronous context-free grammar but is learned from a bitext without any syntactic information. thus it can be seen as a shift to the formal machinery of syntax-based translation systems without any linguistic commitment. in our experiments using bleu as a metric, the hierarchical phrase-based model achieves a relative improvement of 7.5% over pharaoh, a state-of-the-art phrase-based system. we use the k-best parsing algorithm in a cfg-based log-linear translation model in order", "prediction": "a hierarchical phrase model for statistical machine translation we present a statistical phrase translamodel that uses phrases that contain the model is formally a synchronous context free grammar but is learned from a bitext without any without any thus it can be seen as shift to the of syntaxtranslation systems without any lin in our experiments using bleu as a the hierarchical phrasebased model achieves a relative improvement of 5 over a state 5 over a state of the art phrase based the alignment template translation model och and"}
{"ground_truth": "pseudo-projectivity a polynomially parsable non-projective dependency grammar the pseudo-projective grammar we propose can be parsed in polynomial time and captures non-local dependencies through a form of gap-threading, but the structures generated by the grammar are strictly projective.", "prediction": "pseudo pseudo a polynomially parsable non dependency dependency grammar has a long tradition in syntactic the polynomially to tesniere's to tesniere's empirical as methods in parsing as empirical methods in parsing are discovering discovering the importance which is what dependency is what dependency grammars model explicitly explicitly phrasestructure but grammars do one do one problem that has posed an impediment to more wide spread acceptance grammars dependency dependency grammars is the fact that there is no computationally tractable version of dependency grammar which which is not restricted to known that there"}
{"ground_truth": "automatic sense prediction for implicit discourse relations in text we present a series of experiments on automatically identifying the sense of implicit discourse relations, i.e. relations that are not marked with a discourse connective such as \u201cbut\u201d or \u201cbecause\u201d. we work with a corpus of implicit relations present in newspaper text and report results on a test set that is representative of the naturally occurring distribution of senses. we use several linguistically informed features, including polarity tags, levin verb classes, length of verb phrases, modality, context, and lexical features. in addition, we revisit past approaches using lexical pairs from unannotated", "prediction": "automatic sense prediction for implicit discourse relations in text we present a series of experiments on auidentifying the sense of imrelations relations that are that are not marked with a discourse connective such as but or of implicit relations present a corpus of implicit relations newspaper text and report results on a test set that is representative of the naturally occurring distribution of senses we use several linguistically informed informed including polarity levin verb verb of verb verb lexical lexical lexical in we revisit past past approaches using lexical pairs using lexical pairs from unannotated text as explain some some of propose"}
{"ground_truth": "incremental integer linear programming for non-projective dependency parsing integer linear programming has recently been used for decoding in a number of probabilistic models in order to enforce global constraints. however, in certain applications, such as non-projective dependency parsing and machine translation, the complete formulation of the decoding problem as an integer linear program renders solving intractable. we present an approach which solves the problem incrementally, thus we avoid creating intractable integer linear programs. this approach is applied to dutch dependency parsing and we show how the addition of linguistically motivated constraints can yield a significant improvement over state-of-the-art. for dependency", "prediction": "incremental integer linear programming for non projective dependency parsing integer linear programming has recently been used for decoding in a number of probabilistic models in order to enforce global in certain such as non projective dependency parsing and machine the complete formulation of the decoding problem integer linear program renders solving we present an approach which solves the problem thus we avoid creating intractable integer linear linear linear this approach approach is applied to dutch dependency parsing and we show how the addition of linguistically motivated constraints can yield a significant improvement over stateof the many inference require models to make assumptions assumptions"}
{"ground_truth": "corpus-based identification of non-anaphoric noun phrases coreference resolution involves finding antecedents for anaphoric discourse entities, such as definite noun phrases. but many definite noun phrases are not anaphoric because their meaning can be understood from general world knowledge (e.g., \"the white house\" or \"the news media\"). we have developed a corpus-based algorithm for automatically identifying definite noun phrases that are non-anaphoric, which has the potential to improve the efficiency and accuracy of coreference resolution systems. our algorithm generates lists of non-anaphoric noun phrases and noun phrase patterns from a training corpus and uses them to recognize non-anaphoric noun phrases in", "prediction": "corpus based identification of noun noun phrases coreference resolution involves finding for involves finding for anaphoric discourse discourse such as definite as definite many definite phrases are not phrases are not anaphoric because their their meaning can be understood from general world knowledge or or or the news media phrase-based algorithm for automatically identifying definite noun phrases that are that are non which has the potential to improve the efficiency and accuracy of coreference resolution algorithm generates lists of nonanaphoric phrases and noun phrase patterns from a training corpus corpus and uses them to recognize in new"}
{"ground_truth": "word-sense disambiguation using decomposable models most probabilistic classifiers used for word-sense disambiguation have either been based on only one contextual feature or have used a model that is simply assumed to characterize the interdependencies among multiple contextual features. in this paper, a different approach to formulating a probabilistic model is presented along with a case study of the performance of models produced in this manner for the disambiguation of the noun interest. we describe a method for formulating probabilistic models that use multiple contextual features for word-sense disambiguation, without requiring untested assumptions regarding the form of the model. using this", "prediction": "word sense disambiguation disambiguation using using decomposable models is composed of interdependent the test used to used to evaluate a model gives preference to those that have the fewest number of thereby selecting models expressing only the most systematic variable variable summarize to summarize the method of first informative contextual and the one of the most in we propose the decomposable possible decomposable decomposable models characterizing of all characterizing interdependency relationships among the selected interdependency relationships the selected are that are found that good produce approximations to to the data are identified using the test above and one of those those using"}
{"ground_truth": "learning semantic correspondences with less supervision a central problem in grounded language acquisition is learning the correspondences between a rich world state and a stream of text which references that world state. to deal with the high degree of ambiguity present in this setting, we present a generative model that simultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state. we show that our model generalizes across three domains of increasing difficulty--robocup sportscasting, weather forecasts (a new domain), and nfl recaps. we propose a probabilistic generative approach to produce a viterbi", "prediction": "learning semantic correspondences with less less supervision a central problem in grounded language acquisition is learning the correspondences between a rich state and a stream of text which references text references that world the high degree of ambiguity present in present in this we present a generative model that simultaneously segments the text into utterances and maps each each utterance to a meaning representation grounded in the world world model we show that our model generalizes across three domains of difficulty robocup weather forecasts a new and nfl nfl a in"}
{"ground_truth": "manual and automatic evaluation of machine translation between european languages we evaluated machine translation performance for six european language pairs that participated in a shared task: translating french, german, spanish texts to english and back. evaluation was done automatically using the bleu score and manually on fluency and adequacy. the results of the workshop further suggested that bleu systematically underestimated the quality of rule-based machine translation systems. we report and analyze several cases of strong disagreement between system rankings provided by human assessors and those produced by the bleu metric.", "prediction": "manual and automatic evaluation of translation between languages languages rank rank rank rank jmc 1 7 rali 7 8 8 rali cmu systran 9 10 9 10 10 10 9 the 9 spanish english rank rank rank 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1"}
{"ground_truth": "an algorithm for finding noun phrase correspondences in bilingual corpora the paper describes an algorithm that employs english and french text taggers to associate noun phrases in an aligned bilingual corpus. the taggets provide part-of-speech categories which are used by finite-state recognizers to extract simple noun phrases for both languages. noun phrases are then mapped to each other using an iterative re-estimation algorithm that bears similarities to the baum-welch algorithm which is used for training the taggers. the algorithm provides an alternative to other approaches for finding word correspondences, with the advantage that linguistic structure is incorporated. improvements to the", "prediction": "an algorithm for finding noun phrase correspondences in bilingual corpora the paper describes an algorithm that employs english and french text taggers to noun phrases in an aligned bilingual the taggers provide part of speech categories which are used by finite state recognizers to extract simple noun phrases for both noun phrases are then mapped mapped to each other using an iterative re re algorithm that bears similarities to the baum welch algorithm which is used for training the taggers the algorithm provides an alternative to other approaches for finding word with the advantage that linguistic structure"}
{"ground_truth": "domain adaptation with active learning for word sense disambiguation when a word sense disambiguation (wsd) system is trained on one domain but applied to a different domain, a drop in accuracy is frequently observed. this highlights the importance of domain adaptation for word sense disambiguation. in this paper, we first show that an active learning approach can be successfully used to perform domain adaptation of wsd systems. then, by using the predominant sense predicted by expectation-maximization (em) and adopting a count-merging technique, we improve the effectiveness of the original adaptation process achieved by the basic active learning approach. we perform", "prediction": "domain adaptation with active learning for word sense disambiguation disambiguation when a word sense disambiguation wsd system is trained on one domain but applied to a different different a drop in accuracy is frequently frequently this highlights this highlights the importance of domain adaptation for word sense in we first show that an active learning approach approach can be successfully used to perform domain adaptation of wsd systems by using the predominant sense predicted by expectation maximization em and adopting adopting a count merging merging we improve the effectiveness of the original adaptation achieved by the basic active"}
{"ground_truth": "noun phrase coreference as clustering this paper introduces a new, unsupervised algorithm for noun phrase coreference resolution. it differs from existing methods in that it views coreference resolution as a clustering task. in an evaluation on the muc-6 coreference resolution corpus, the algorithm achieves an f-measure of 53.6%, placing it firmly between the worst (40%) and best (65%) systems in the muc-6 evaluation. more importantly, the clustering approach outperforms the only muc-6 system to treat coreference resolution as a learning problem. the clustering algorithm appears to provide a flexible mechanism for coordinating the application of context-independent and context-dependent constraints and", "prediction": "noun phrase coreference as clustering this paper introduces a unsupervised algorithm for noun phrase coreference coreference it it differs from existing existing methods in that it views it views coreference as a clustering task in an evaluation on the muc 6 coreference resolution resolution the algorithm achieves an f measure of placing it firmly between the worst 40 40 and best 65 systems in the muc 6 more the clustering approach more the clustering approach outperforms the only muc 6 to treat coreference a learning a learning a learning a learning the clustering appears to the application of for to provide a for provide a for coordinating provide a of context and context dependent and dependent and dependent and dependent and context constraints"}
{"ground_truth": "domain adaptation for statistical machine translation with monolingual resources domain adaptation has recently gained interest in statistical machine translation to cope with the performance drop observed when testing conditions deviate from training conditions. the basic idea is that in-domain training data can be exploited to adapt all components of an already developed system. previous work showed small performance gains by adapting from limited in-domain bilingual data. here, we aim instead at significant performance gains by exploiting large but cheap monolingual in-domain data, either in the source or in the target language. we propose to synthesize a bilingual corpus by translating", "prediction": "adaptation for statistical statistical machine translation with monolingual domain adaptation has has recently gained interest in statistical machine translation to cope with the performance drop observed when testing conditions deviate from training the basic idea is that in domain training data can be exploited to adapt all components of an already developed previous work showed small performance gains by adapting from limited in domain bilingual we aim instead at significant performance gains by exploiting large but cheap monolingual in domain either in in the source or in the target source in propose to synthesize a bilingual corpus by translating the monolingual adaptation the monolingual"}
{"ground_truth": "a discriminative matching approach to word alignment we present a discriminative, large-margin approach to feature-based matching for word alignment. in this framework, pairs of word tokens receive a matching score, which is based on features of that pair, including measures of association between the words, distortion between their positions, similarity of the orthographic form, and so on. even with only 100 labeled training examples and simple features which incorporate counts from a large unlabeled corpus, we achieve aer performance close to ibm model 4, in much less time. including model 4 predictions as features, we achieve a relative aer reduction", "prediction": "a discriminative matching approach to word word alignment we present a alignment large margin approach to margin feature matching for word word in word ceive ceive a matching basedon features features of that which including mea sures mea sures of association between the words between between their sim ilarity of the orthographic and even even with only 100 labeled train ing examples and simple features whichincorporate counts from a large unlabeled corpus we achieve aer perfor mance close to ibm model muchless muchless predic tions model 4 as we achieve a relativeaer reduction of 22 in"}
{"ground_truth": "statistical sentence condensation using ambiguity packing and stochastic disambiguation methods for lexical-functional grammar we present an application of ambiguity pack- ing and stochastic disambiguation techniques for lexical-functional grammars (lfg) to the domain of sentence condensation. our system incorporates a linguistic parser/generator for lfg, a transfer component for parse reduction operating on packed parse forests, and a maximum-entropy model for stochastic output selection. furthermore, we propose the use of standard parser evaluation methods for automatically evaluating the summarization quality of sentence condensation systems. an experimental evaluation of summarization quality shows a close correlation between the automatic parse-based evaluation and a manual", "prediction": "statistical statistical sentence condensation using and stochastic disambiguation disambiguation methods for lexical functional grammar we present an application of ambiguity packing and stochastic disambiguation techniques for lexical grammars lfg to the domain of sentence sentence our system incorporates a linguistic parser generator for lfg a transfer component for parse reduction on packed parse parse and a maximum entropy model for stochastic output selection we propose the use of standard parser evaluation methods for automatically the summarization quality of sentence condensation experimental an experimental evaluation of summarization quality shows a correlation between the automatic parse based evaluation and a manual evaluation of generated"}
{"ground_truth": "dependency parsing by belief propagation we formulate dependency parsing as a graphical model with the novel ingredient of global constraints. we show how to apply loopy belief propagation (bp), a simple and effective tool for approximate learning and inference. as a parsing algorithm, bp is both asymptotically and empirically efficient. even with second-order features or latent variables, which would make exact parsing considerably slower or np-hard, bp needs only o(n3) time with a small constant factor. furthermore, such features significantly improve parse accuracy over exact first-order methods. incorporating additional features would increase the runtime additively rather than multiplicatively. we can", "prediction": "dependency parsing by belief propagation we formulate dependency parsing as a graphical model with the novel ingredient of global constraints we show how to apply loopy belief propagation belief propagation belief a simple and tool for and inference as a parsing algorithm bp is both asymptotically and empirically even with second order features or would make exact parsing considerslower or np bp needs needs only with a small constant constant such features such features significantly improve parse parse accuracy over exact first incorporating additional increase the additively increase the additively than increase the additively computational"}
{"ground_truth": "textrunner: open information extraction on the web traditional information extraction systems have focused on satisfying precise, narrow, pre-specified requests from small, homogeneous corpora. in contrast, the textrunner system demonstrates a new kind of information extraction, called open information extraction (oie), in which the system makes a single, data-driven pass over the entire corpus and extracts a large set of relational tuples, without requiring any human input. (banko et al., 2007) textrunner is a fully-implemented, highly scalable example of oie. textrunner's extractions are indexed, allowing a fast query mechanism. our first public demonstration of the textrunner system shows the results of", "prediction": "textrunner open information extraction on the web traditional information the web on the requests of the requests from system demonstrates demonstrates the textrunner of the kind of called information the system of the system of the system of the system of the data the entire of a of the of of a of the of of a of the of of a of a of a of the et of"}
{"ground_truth": "what's in a translation rule? we propose a theory that gives formal semantics to word-level alignments defined over parallel corpora. we use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data. we describe how to learn hundreds of millions of tree transformation rules from a parsed, aligned chinese/english corpus, and we describe probability estimators for those rules. our translation rules are learned from word-aligned bilingual texts whose source side has been parsed by using a syntactic parser.", "prediction": "what's in a translation rule we propose propose a gives formal semantics to word level alignments defined over over parallel we use our theory to introduce a linear linear algorithm that can be used to derive from word word corpora the minimal set of syntactically motivated transformation rules that explain human translation very in a very interesting study of syntax in statistical machine 2002 fox 2002 how how well how translation models fit one we propose we use a translation of the translation of"}
{"ground_truth": "online large-margin training of dependency parsers we present an effective training algorithm for linearly-scored dependency parsers that implements online large-margin multi-class training (crammer and singer, 2003; crammer et al., 2003) on top of efficient parsing techniques for dependency trees (eisner, 1996). the trained parsers achieve a competitive dependency accuracy for both english and czech with no language specific enhancements. we have achieved parsers with o(n3) time complexity without the grammar constant. we use the prefix of each word form instead of word form itself as features. our dependency parser achieves accuracy as good as charniak (2000) with speed ten times", "prediction": "online margin training of dependency parsers we present an effective training algorithm for linearly scored dependency that implements online largemargin multi class training crammer and et of et of top of et of efficient parsing techniques for dependency trees dependency the parsers parsers achieve a competitive dependency accuracy for both english and czech with no language specific research on training parsers from annotated data has for the most focused on models and training algorithms for phrase structure the best parsing the best parsing models represent generatively we use the joint of"}
{"ground_truth": "findings of the 2012 workshop on statistical machine translation this paper presents the results of the wmt12 shared tasks, which included a translation task, a task for machine translation evaluation metrics, and a task for run-time estimation of machine translation quality. we conducted a large-scale manual evaluation of 103 machine translation systems submitted by 34 teams. we used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 12 evaluation metrics. we introduced a new quality estimation task this year, and evaluated submissions from 11 teams. we report for several automatic", "prediction": "findings findings of the 2012 workshop on statistical machine translation this paper presents the results of the wmt12 shared which included a translation task translation a task for machine translation evaluation and a task for run time estimation of machine translation quality we conducted a large scale manual evaluation of 103 machine translation systems submitted by 34 we used the ranking of these systems to measure how strongly automatic metrics correlate with human human human judgments of translation quality for 12 evaluation we introduced a new quality estimation task this paper, and submissions submissions from 11 this paper presents the results of"}
{"ground_truth": "question answering based on semantic structures the ability to answer complex questions posed in natural language depends on (1) the depth of the available semantic representations and (2) the inferential mechanisms they support. in this paper we describe a qa architecture where questions are analyzed and candidate answers generated by 1) identifying predicate argument structures and semantic frames from the input and 2) performing structured probabilistic inference using the extracted relations in the context of a domain and scenario model. a novel aspect of our system is a scalable and expressive representation of actions and events based on coordinated probabilistic", "prediction": "identifying we present the system we present the system we present the system we present the system we present the system of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference of the coreference"}
{"ground_truth": "dependency-based syntactic&#x2013;semantic analysis with propbank and nombank this paper presents our contribution in the closed track of the 2008 conll shared task (surdeanu et al., 2008). to tackle the problem of joint syntactic-semantic analysis, the system relies on a syntactic and a semantic subcomponent. the syntactic model is a bottom-up projective parser using pseudo-projective transformations, and the semantic model uses global inference mechanisms on top of a pipeline of classifiers. the complete syntactic-semantic output is selected from a candidate pool generated by the subsystems. the system achieved the top score in the closed challenge: a labeled syntactic accuracy of 89.32%,", "prediction": "2008 2008 proceedings of the 12th on computational we present 187 this paper, paper presents this paper, paper presents our our contribution in the se pierre closed track in the closed track of the 2008 conll shared task et task to tackle the problem of joint syntactic ? semantic anal the system relies on a syntactic and a semantic the syntactic model is a bottom up parser us ing pseudo and the semantic model uses global inference mechanisms on top of a pipeline of clas"}
{"ground_truth": "reliable measures for aligning japanese-english news articles and sentences we have aligned japanese and english news articles and sentences to make a large parallel corpus. we first used a method based on cross-language information retrieval (clir) to align the japanese and english articles and then used a method based on dynamic programming (dp) matching to align the japanese and english sentences in these articles. however, the results included many incorrect alignments. to remove these, we propose two measures (scores) that evaluate the validity of alignments. the measure for article alignment uses similarities in sentences aligned by dp matching and that", "prediction": "reliable measures for aligning japanese english news articles and sentences we have aligned japanese and english news articles and sentences to make a large parallel parallel we first used a method based on cross language information retrieval clir to align the japanese and english articles and then used a method based on dynamic programming dp matching to align align the japanese and english sentences in these the results included many incorrect to remove remove we propose two measures that evaluate that evaluate the validity of alignment uses similarities the measure for article alignment uses similarities in sentences dp and that for uses similarities"}
{"ground_truth": "indirect-hmm-based hypothesis alignment for combining outputs from machine translation systems this paper presents a new hypothesis alignment method for combining outputs of multiple machine translation (mt) systems. an indirect hidden markov model (ihmm) is proposed to address the synonym matching and word ordering issues in hypothesis alignment. unlike traditional hmms whose parameters are trained via maximum likelihood estimation (mle), the parameters of the ihmm are estimated indirectly from a variety of sources including word semantic similarity, word surface similarity, and a distance-based distortion penalty. the ihmm-based method significantly outperforms the state-of-the-art ter-based alignment model in our experiments on nist benchmark", "prediction": "indirect hypothesis alignment for combining outputs from machine translation systems this paper presents a new hypothesis alignment method for combining outputs of multiple machine translation mt an indirect indirect hidden markov model ihmm is proposed to address the synonym matching and word ordering issues in hypothesis unlike traditional hmms whose parameters are trained via estimation the ihmm of estimation the ihmm of the ihmm are estimated a variety of sources including semantic word surface and a distance based distortion a the ihmm based method significantly outperforms the state of the art ter based model in our experiments on nist"}
{"ground_truth": "learning extraction patterns for subjective expressions this paper presents a bootstrapping process that learns linguistically rich extraction patterns for subjective (opinionated) expressions. high-precision classifiers label unannotated data to automatically create a large training set, which is then given to an extraction pattern learning algorithm. the learned patterns are then used to identify more subjective sentences. the bootstrapping process learns many subjective patterns and increases recall while maintaining high precision. we construct a high precision classifier for contiguous sentences using the number of strong and weak subjective words in current and nearby sentences. we introduce a bootstrapping method to learn subjective", "prediction": "learning extraction patterns for subjective subjective expressions this paper presents a bootstrapping process that learns linguistically rich extraction patterns for subjective opinionated high precision classifiers label unannotated data to automatically create a which large training which is then given to an extraction pattern learning the learned patterns are then used to then used to identify more subjective subjective sentences the bootstrapping process learns subjective patterns and increases recall while maintaining high many many natural language processing could benefit from being able to distinguish between factual and subjective subjective in we propose"}
{"ground_truth": "an empirical study of semi-supervised structured conditional models for dependency parsing this paper describes an empirical study of high-performance dependency parsers based on a semi-supervised learning approach. we describe an extension of semi-supervised structured conditional models (ss-scms) to the dependency parsing problem, whose framework is originally proposed in (suzuki and isozaki, 2008). moreover, we introduce two extensions related to dependency parsing: the first extension is to combine ss-scms with another semi-supervised approach, described in (koo et al., 2008). the second extension is to apply the approach to second-order parsing models, such as those described in (carreras, 2007), using a two-stage", "prediction": "an study of semi supervised structured structured conditional models for dependency parsing this paper describes an empirical study of high performance dependency parsers based on a semi supervised learning we describe an extension of semisupervised structured conditional models ss scms to the dependency parsing whose framework is originally proposed in suzuki and isozaki 2008 we introduce two extensions related to dependency dependency is parsing the first extension is to combine ss scms with another supervised supervised in koo et et extension is the second extension is to the approach to secondorder parsing as such as those described such as those described"}
{"ground_truth": "multiple aspect ranking using the good grief algorithm we address the problem of analyzing multiple related opinions in a text. for instance, in a restaurant review such opinions may include food, ambience and service. we formulate this task as a multiple aspect ranking problem, where the goal is to produce a set of numerical scores, one for each aspect. we present an algorithm that jointly learns ranking models for individual aspects by modeling the dependencies between assigned ranks. this algorithm guides the prediction of individual rankers by analyzing meta-relations between opinions, such as agreement and contrast. we prove that our", "prediction": "multiple aspect ranking using the good grief algorithm we address the problem of analyzing multiple related opinions in a text for instance in a restaurant review such opinions may opinions include may include formulate formulate this task as a multiple aspect ranking ranking where the goal is to produce a set of numerical one for each for each we present an algorithm that jointly learns models for individual aspects aspects by modeling the dependencies between assigned this algorithm guides the prediction of individual rankers by analyzing meta relations between as agreement agreement and agreementbased we more expressive than individual"}
{"ground_truth": "cues and control in expert-client dialogues we conducted an empirical analysis into the relation between control and discourse structure. we applied control criteria to four dialognes and identified 3 levels of discourse structure. we investigated the mechanism for changing control between these structures and found that utterance type and not cue words predicted shifts of control. participants used certain types of signals when discourse goals were proceeding successfully but resorted to interruptions when they were not. we define initiative as being held by the speaker who is driving the conversation at any point in the conversation. we propose rules for", "prediction": "cues and control in expert client dialogues we conducted an empirical analysis into the relation control and discourse we applied control criteria to four dialogues and identified 3 levels of discourse we investigated the mechanism for changing control between these structures and found that utterance type and not cue cue words predicted of participants used participants used certain types of signals when discourse discourse goals were proceeding successfully but to interruptions when they interruptions were they a number of researchers have shown that there is organisation in above above the level of the utterance utterance utterance utterance"}
{"ground_truth": "better word alignments with supervised itg models this work investigates supervised word alignment methods that exploit inversion transduction grammar (itg) constraints. we consider maximum margin and conditional likelihood objectives, including the presentation of a new normal form grammar for canonicalizing derivations. even for non-itg sentence pairs, we show that it is possible learn itg alignment models by simple relaxations of structured discriminative learning objectives. for efficiency, we describe a set of pruning techniques that together allow us to align sentences two orders of magnitude faster than naive bitext cky parsing. finally, we introduce many-to-one block alignment features, which significantly improve", "prediction": "better word alignments with supervised supervised itg models this work investigates investigates word alignment methods that exploit inversion transduction grammar itg itg itg we consider maximum margin and likelihood including the presentation of a new normal form grammar for canonicalizing even for sentence we show that it is possible learn itg alignment models by simple relaxations of structured discriminative learning learning for efficiency we describe a set of pruning techniques that together allow us to align align sentences two orders of magnitude faster than naive we introduce many many to one alignment"}
{"ground_truth": "a maximum-entropy-inspired parser we present a new parser for parsing down to penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trained and tested on the previously established [5,9,10,15,17] \"standard\" sections of the wall street journal tree- bank. this represents a 13% decrease in error rate over the best single-parser results on this corpus [9]. the major technical innovation is the use of a \"maximum-entropy-inspired\" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.", "prediction": "maximum maximum inspired parser we present a new parser for parsing down to penn tree bank style parse trees that achieves 1 average precision recall for sentences of 40 and for of length 100 and less when trained and tested on the previously established established model. however, 17 17 quot standard quot sections of the wall street journal this represents represents a 13 decrease in error rate over the best single parser results major technical innovation is the use technical innovation is the use of a quot maximum entropy inspired quot model for conditioning and smoothing that let us successfully to test and combine many"}
{"ground_truth": "identifying word translations in non-parallel texts common algorithms for sentence and word-alignment allow the automatic identification of word translations from parallel texts. this study suggests that the identification of word translations should also be possible with non-parallel and even unrelated texts. the method proposed is based on the assumption that there is a correlation between the patterns of word co-occurrences in texts of different languages. we propose a computationally demanding matrix purmutation method which maximizes a similarity between co-occurence matrices in two languages. an underlying assumption in our work is that translations of words that are related in one language", "prediction": "identifying word word in non parallel texts common algorithms for sentence and word alignment allow the automatic identification of word translations from parallel suggests this study suggests that the identification of word translations should also be non parallel and even unrelated texts the method proposed is based on the assumption that there is a correlation between the patterns of word cooccurrences in texts of different different in a number of recent studies it been shown that word translations can be automatically derived from the statistical distribution distribution of in we propose"}
{"ground_truth": "building a large-scale annotated chinese corpus in this paper we address issues related to building a large-scale chinese corpus. we try to answer four questions: (i) how to speed up annotation, (ii) how to maintain high annotation quality, (iii) for what purposes is the corpus applicable, and finally (iv) what future work we anticipate.", "prediction": "large large scale annotated chinese corpus in this paper we address issues related to building a large chinese chinese we try to answer four four questions i how to speed to speed up ii how to maintain high annotation iii for what purposes is the corpus corpus finally is the corpus finally iv what future work we the penn chinese treebank ctb is an ongoing its objective create to create to create to create a chinese corpus tags and syntactic the first annotated with the first of first of the project"}
{"ground_truth": "a statistical parser for czech this paper considers statistical parsing of czech, which differs radically from english in at least two respects: (1) it is a highly inflected language, and (2) it has relatively free word order. these differences are likely to pose new problems for techniques that have been developed on english. we describe our experience in building on the parsing model of (collins 97). our final results - 80% dependency accuracy - represent good progress towards the 91% accuracy of the parser on english (wall street journal) text. we use a transformed tree bank from the prague dependency", "prediction": "a statistical parser for czech this paper considers statistical parsing of czech radically which differs radically from english in at least two 1 it is a inflected and it has relatively word word differences are likely to pose new problems for techniques that have been developed we describe in building on the parsing model of collins our final results dependency accuracy represent good progress towards the 91 accuracy of the parser on english wall street journal much much of the recent research on statistical parsing has focused on english languages other than english are likely to pose new problems for statistical"}
{"ground_truth": "germanet - a lexical-semantic net for german we present the lexical-semantic net for german \"germanet\" which integrates conceptual ontological information with lexical semantics, within and across word classes. it is compatible with the princeton wordnet but integrates principle-based modifications on the constructional and organizational level as well as on the level of lexical and conceptual relations. germanet includes a new treatment of regular polysemy, artificial concepts and of particle verbs. it furthermore encodes cross-classification and basic syntactic information, constituting an interesting tool in exploring the interaction of syntax and semantics. the development of such a large scale resource is particularly", "prediction": "a lexical semantic lexical semantic net for german we present the lexical semantic net for german germanet which conceptual ontological information with lexical within and across word word it is compatible with the princeton wordnet but integrates principlebased modifications on the constructional and organizational level as well as on the level of lexical and conceptual relations germanet includes a new treatment of regular regular artificial concepts and of particle particle it furthermore encodes cross classification and basic syntactic constituting an interesting tool in exploring the interacof syntax and development development of such a large resource resource is particularly important as up up to now basic online"}
{"ground_truth": "moses: open source toolkit for statistical machine translation we describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. in addition to the smt decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks. our moses decoder implements the factored phrase-based translation model.", "prediction": "moses open source toolkit for statistical machine translation where many sources of information about the project can be moses moses was the subject of this year johns hopkins university workshop on machine translation et decoder is the core of moses to minimize the learning curve for many many the decoder was developed as a drop in for the drop in for the popular phrase based for the toolkit to be adopted by the be and to make it for others to contribute to the project we to to the following principles we the principles"}
